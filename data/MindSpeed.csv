name,title,body,Created At,Tags,State,Reactions,Comments_count,Link,Comments
Klaiber,Qwen2.5VL希望能增加LoRA支持,一、需求场景&价值 Qwen2.5VL系列模型训练，显存较小时，增加LoRA训练的支持，包括：LoRA训练，LoRA训练的ckpt合并到原始mm_models。 二、需求建议实现的规格 支持全系列昇腾卡。 三、竞品比较（选填） swift/transformers 原生支持Qwen2.5VL系列的LoRA训练。,2025-04-07T16:59:07+08:00,,open,0,1,https://gitee.com/ascend/MindSpeed/issues/IBZ414,规划中，可关注 MindSpeedMM https://gitee.com/ascend/MindSpeedMM
阿郎小哥,MegatronCore cpu offloading的功能,一、需求场景&价值 MegatronCore是支持cpu offloading的，那么在基于MindSpeed的训练中，该如何配置cpu offloading的功能？ 二、需求建议实现的规格 三、竞品比较（选填）,2025-04-02T14:27:09+08:00,,open,0,1,https://gitee.com/ascend/MindSpeed/issues/IBY945,针对优化器的 CPU offload 功能已在规划中
阿郎小哥,支持FSDP,一、需求场景&价值 是否支持FSDP加速训练，如果支持，如何配置？如果不支持，会考虑支持嘛，什么时候支持？ FSDP是基于pytorch原生提供的，按道理，应该是直接就支持的，只要配置一下参数就行吧。 二、需求建议实现的规格 三、竞品比较（选填）,2025-03-28T17:08:48+08:00,,open,0,4,https://gitee.com/ascend/MindSpeed/issues/IBX3GM,已在规划中,在哪个分支开发呢？,在 master 上，对应 core 0.10.0 或者 0.11.0,全局搜了一下MindSpeed项目master分支的代码，fsdp关键字，只有在UnitTest中才有出现， !输入图片说明 FSDP训练的代码还没合进去嘛？
阿郎小哥,如何配置出  类似   deepspeed zero+cpu并行的效果？,"一、需求场景&价值 目前在ascend npu 4 * 64GB上，全参数微调训练qwen32B，显存不足，已经配置了tp=8; pp=2，但还是不行。 想要配置deepspeed的zero并行，或是Pytorch FSDP，该如何配置？ 增加 `enablezero3` 参数，设置  `enablezero3 True` 或  `enablezero3`  都没用 ，直接报错： !输入图片说明 脚本如下： ``` !/bin/bash source /usr/local/Ascend/ascendtoolkit/set_env.sh export CUDA_DEVICE_MAX_CONNECTIONS=1 export PYTORCH_NPU_ALLOC_CONF=expandable_segments:True export HCCL_CONNECT_TIMEOUT=1800  Change for multinode config NPUS_PER_NODE=4 MASTER_ADDR=localhost MASTER_PORT=7006 NNODES=1 NODE_RANK=0 WORLD_SIZE=$(($NPUS_PER_NODE*$NNODES))  please fill these path configurations CKPT_LOAD_DIR=""/home/hwtest/hzy/MindSpeedLLM/model_weights/ds_qwen2.5_32"" CKPT_SAVE_DIR=""/home/hwtest/hzy/MindSpeedLLM/output/full_ds_qwen2.5_32b"" DATA_PATH=""/home/hwtest/hzy/MindSpeedLLM/finetune_dataset/sharegpt"" TOKENIZER_PATH=""/home/hwtest/DeepSeekR1DistillQwen32B""  full参数训练，调整模型并行度 TP=8 PP=2 SEQ_LEN=4096 MBS=1 GBS=16 TRAIN_ITERS=500 DISTRIBUTED_ARGS=""     nproc_per_node $NPUS_PER_NODE \     nnodes $NNODES \     node_rank $NODE_RANK \     master_addr $MASTER_ADDR \     master_port $MASTER_PORT "" TUNE_ARGS=""     finetune \     stage sft \     isinstructiondataset \     tokenizernotusefast \     prompttype deepseek3 \     variableseqlengths \     reusefp32param \ "" GPT_ARGS=""     usemcoremodels \     enable_zero3 \     tensormodelparallelsize ${TP} \     pipelinemodelparallelsize ${PP} \     usedistributedoptimizer \     microbatchsize ${MBS} \     globalbatchsize ${GBS} \     sequenceparallel \     numlayers 64 \     hiddensize 5120 \     ffnhiddensize 27648 \     numattentionheads 40 \     groupqueryattention \     numquerygroups 8 \     tokenizertype PretrainedFromHF \     tokenizernameorpath ${TOKENIZER_PATH} \     seqlength ${SEQ_LEN} \     maxpositionembeddings ${SEQ_LEN} \     makevocabsizedivisibleby 1 \     paddedvocabsize 152064 \     rotarybase 1000000 \     lr 1.25e6 \     trainiters ${TRAIN_ITERS} \     lrdecaystyle cosine \     untieembeddingsandoutputweights \     disablebiaslinear \     attentiondropout 0.0 \     initmethodstd 0.01 \     hiddendropout 0.0 \     positionembeddingtype rope \     normalization RMSNorm \     usefusedrmsnorm \     normepsilon 1e5 \     swiglu \     useflashattn \     usefusedrotaryposemb \     userotarypositionembeddings \     usefusedswiglu \     nomaskedsoftmaxfusion \     attentionsoftmaxinfp32 \     minlr 1.25e7 \     weightdecay 1e1 \     lrwarmupfraction 0.01 \     clipgrad 1.0 \     adambeta1 0.9 \     adambeta2 0.95 \     addqkvbias \     initiallossscale 4096 \     nogradientaccumulationfusion \     noloadoptim \     noloadrng \     seed 42 \     bf16 "" DATA_ARGS=""     datapath $DATA_PATH \     split 100,0,0 "" OUTPUT_ARGS=""     loginterval 1 \     saveinterval ${TRAIN_ITERS} \     evalinterval ${TRAIN_ITERS} \     evaliters 0 \ "" torchrun $DISTRIBUTED_ARGS posttrain_gpt.py \     $GPT_ARGS \     $DATA_ARGS \     $OUTPUT_ARGS \     $TUNE_ARGS \     load ${CKPT_LOAD_DIR} \     save ${CKPT_SAVE_DIR} \     distributedbackend nccl \     | tee logs/pretrain_mcore_distill_qwen_32b_full_8k.log ``` 期望： 能配置出来deepspeed zero系列或pytorch fsdp 加速策略的数据并行的效果。 二、需求建议实现的规格 从目前来看，官方放出来的数据并行策略是默认的pytorch DP、DDP（async）；但是对于优化加速后的数据并行策略支持是不够的。正常来说deepspeed是支持NPU的，是可以无缝集成介入，而且pytorchnpu也支持FSDP策略。建议官方早日实现接入这两种dp策略，而不仅仅更偏向于MegatronLM的tp、pp。 三、竞品比较（选填）",2025-03-28T10:11:39+08:00,,open,0,1,https://gitee.com/ascend/MindSpeed/issues/IBWXBX,usedistributedoptimizer 分布式优化器一定程度上可以看做是 Zero2，FSDP 支持也在规划中。 TP * PP * DP 需等于卡数，4张卡无法开启 TP8，PP2。
MXE,910A是否能使用该库的flashattention or flashattention_v2,"一、问题现象（附报错日志上下文）： ``` Creating extension directory /home/mauser/.cache/torch_extensions/py310_cpu/fusion_attention_v2... Emitting ninja build file /home/mauser/.cache/torch_extensions/py310_cpu/fusion_attention_v2/build.ninja... Building extension module fusion_attention_v2... Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N) [1/3] c++ MMD MF flop_counter.o.d DTORCH_EXTENSION_NAME=fusion_attention_v2 DTORCH_API_INCLUDE_EXTENSION_H DPYBIND11_COMPILER_TYPE=\""_gcc\"" DPYBIND11_STDLIB=\""_libstdcpp\"" DPYBIND11_BUILD_ABI=\""_cxxabi1011\"" I/usr/local/Ascend/ascendtoolkit/latest/include I/home/mauser/.conda/envs/MindSpeed/lib/python3.10/sitepackages/torch_npu/include I/home/mauser/.conda/envs/MindSpeed/lib/python3.10/sitepackages/torch_npu/third_party I/home/mauser/.conda/envs/MindSpeed/lib/python3.10/sitepackages/torch_npu/acl I/home/mauser/.conda/envs/MindSpeed/lib/python3.10/sitepackages/torch_npu/inc I/home/mauser/work/framework/MindSpeedMM/MindSpeed/mindspeed/ops/csrc/cann/inc isystem /home/mauser/.conda/envs/MindSpeed/lib/python3.10/sitepackages/torch/include isystem /home/mauser/.conda/envs/MindSpeed/lib/python3.10/sitepackages/torch/include/torch/csrc/api/include isystem /home/mauser/.conda/envs/MindSpeed/lib/python3.10/sitepackages/torch/include/TH isystem /home/mauser/.conda/envs/MindSpeed/lib/python3.10/sitepackages/torch/include/THC isystem /home/mauser/.conda/envs/MindSpeed/include/python3.10 D_GLIBCXX_USE_CXX11_ABI=0 fPIC std=c++17 fstackprotectorall Wl,z,relro,z,now,z,noexecstack fPIC pie s fvisibility=hidden D_FORTIFY_SOURCE=2 O2 Wnosigncompare Wnodeprecateddeclarations Wnoreturntype D__FILENAME__='""$(notdir $(abspath $ npu_fusion_attention_backward_v2(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int64_t, const std::string&, const c10::optional&, const c10::optional&, const c10::optional&, const c10::optional&, const c10::optional&, const c10::optional&, const c10::optional&, const c10::optional&, double, double, int64_t, int64_t, int64_t, const c10::optional >&, const c10::optional >&, const c10::optional >&, const c10::optional >&, const c10::optional >&, int64_t, int64_t)’: /home/mauser/work/framework/MindSpeedMM/MindSpeed/mindspeed/ops/csrc/cann/inc/aclnn_common.h:622:48: warning: narrowing conversion of ‘workspace_size’ from ‘uint64_t’ {aka ‘long unsigned int’} to ‘long int’ [Wnarrowing]   622                                                 ^~~~~~~~~~~~~~ [3/3] c++ fusion_attention_v2.o flop_counter.o shared L/usr/local/Ascend/ascendtoolkit/latest/lib64 lascendcl L/home/mauser/.conda/envs/MindSpeed/lib/python3.10/sitepackages/torch_npu/lib ltorch_npu L/home/mauser/.conda/envs/MindSpeed/lib/python3.10/sitepackages/torch/lib lc10 ltorch_cpu ltorch ltorch_python o fusion_attention_v2.so Loading extension module fusion_attention_v2... [E compiler_depend.ts:280] call aclnnFlashAttentionScoreV2 failed, detail:EZ9999: Inner Error! EZ9999: 2025032210:34:22.141.584  Op FlashAttentionScore does not has any binary.         TraceBack (most recent call last):         Kernel Run failed. opType: 3, FlashAttentionScore         launch failed for FlashAttentionScore, errno:561000. Exception raised from operator() at /home/mauser/work/framework/MindSpeedMM/MindSpeed/mindspeed/ops/csrc/cann/fusion_attention_v2.cpp:422 (most recent call first): frame 0: c10::Error::Error(c10::SourceLocation, std::string) + 0x68 (0xffff877a4898 in /home/mauser/.conda/envs/MindSpeed/lib/python3.10/sitepackages/torch/lib/libc10.so) frame 1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x6c (0xffff8775d2a8 in /home/mauser/.conda/envs/MindSpeed/lib/python3.10/sitepackages/torch/lib/libc10.so) frame 2:  + 0xce44 (0xffff83d7fe44 in /home/mauser/.cache/torch_extensions/py310_cpu/fusion_attention_v2/fusion_attention_v2.so) frame 3:  + 0x136f32c (0xfffda159d32c in /home/mauser/.conda/envs/MindSpeed/lib/python3.10/sitepackages/torch_npu/lib/libtorch_npu.so) frame 4:  + 0x632164 (0xfffda0860164 in /home/mauser/.conda/envs/MindSpeed/lib/python3.10/sitepackages/torch_npu/lib/libtorch_npu.so) frame 5:  + 0x6326cc (0xfffda08606cc in /home/mauser/.conda/envs/MindSpeed/lib/python3.10/sitepackages/torch_npu/lib/libtorch_npu.so) frame 6:  + 0x62fb44 (0xfffda085db44 in /home/mauser/.conda/envs/MindSpeed/lib/python3.10/sitepackages/torch_npu/lib/libtorch_npu.so) frame 7:  + 0x946ec (0xffff877cb6ec in /home/mauser/.conda/envs/MindSpeed/lib/python3.10/sitepackages/torch/lib/libc10.so) frame 8:  + 0x7624 (0xffff910a1624 in /lib/aarch64linuxgnu/libpthread.so.0) frame 9:  + 0xd162c (0xffff90f2562c in /lib/aarch64linuxgnu/libc.so.6) [W compiler_depend.ts:286] Warning:  (function ExecFunc) Traceback (most recent call last):   File ""/home/mauser/work/test/ms_fa.py"", line 33, in      result = npu_fusion_attention(query.npu(), key.npu(), value.npu(), head_num, input_layout, atten_mask=atten_mask.npu(), scale=scale_value, keep_prob=keep_prob, pre_tokens=pre_tokens, next_tokens=next_tokens, pse_type=1)   File ""/home/mauser/work/framework/MindSpeedMM/MindSpeed/mindspeed/ops/fusion_attention_v2.py"", line 87, in npu_fusion_attention     return FusionAttentionV2Function.apply(query, key, value, head_num,   File ""/home/mauser/.conda/envs/MindSpeed/lib/python3.10/sitepackages/torch/autograd/function.py"", line 539, in apply     return super().apply(*args, **kwargs)   type: ignore[misc]   File ""/home/mauser/work/framework/MindSpeedMM/MindSpeed/mindspeed/ops/fusion_attention_v2.py"", line 29, in forward     outputs = mindspeed_ops.npu_fusion_attention_v2(query, key, value, head_num, RuntimeError: The Inner error is reported as above. The process exits for this inner error, and the current working operator name is aclnnFlashAttentionScoreV2. Since the operator is called asynchronously, the stacktrace may be inaccurate. If you want to get the accurate stacktrace, pleace set the environment variable ASCEND_LAUNCH_BLOCKING=1. [ERROR] 2025032210:34:22 (PID:63469, Device:0, RankID:1) ERR00100 PTA call acl api failed ``` 二、软件版本:  CANN 版本 (e.g., CANN 3.0.x，5.x.x):  8.0.0RC3 Tensorflow/Pytorch/MindSpore 版本: pytorch==2.1.0 Python 版本 (e.g., Python 3.7.5): 3.10.16  MindStudio版本 (e.g., MindStudio 2.0.0 (beta3)): 操作系统版本 (e.g., Ubuntu 18.04):ubuntu 20.04 硬件信息: NPU910A 三、测试步骤： 运行 https://gitee.com/ascend/MindSpeed/blob/master/docs/ops/fusion_attention.md 中的flash_attention_v2类的代码 四、问题： 910A是否能使用该库的flashattention or flashattention_v2",2025-03-22T10:46:52+08:00,,open,0,1,https://gitee.com/ascend/MindSpeed/issues/IBVEV6,您好，910A 不支持 FA 算子，可使用小算子替代
pika2024,开启自适应选择性重计算报错，无法转移到CPU上,"一、问题现象（附报错日志上下文）： Need tensor size is : 54022046208 SWAP Failed SWAP Failed Traceback (most recent call last):   File ""/data01/ssy/MegatronLM/pretrain_gpt.py"", line 264, in      pretrain(   File ""/data01/ssy/MindSpeed/mindspeed/core/training.py"", line 514, in wrapper     pretrain(*args, **kwargs)   File ""/data01/ssy/MindSpeed/mindspeed/training.py"", line 237, in pretrain     model, optimizer, opt_param_scheduler = setup_model_and_optimizer(   File ""/data01/ssy/MindSpeed/mindspeed/core/memory/adaptive_recomputing/adaptive_recompute.py"", line 751, in wrapper     models, optimizer, opt_param_scheduler = setup_model_and_optimizer(*args, **kargs)   File ""/data01/ssy/MindSpeed/mindspeed/core/training.py"", line 537, in wrapper     model, optimizer, opt_param_scheduler = setup_model_and_optimizer(*args, **kwargs)   File ""/data01/ssy/MegatronLM/megatron/training/training.py"", line 617, in setup_model_and_optimizer     model = get_model(model_provider_func, model_type)   File ""/data01/ssy/MegatronLM/megatron/training/training.py"", line 534, in get_model     model = [DDP(config,   File ""/data01/ssy/MegatronLM/megatron/training/training.py"", line 534, in      model = [DDP(config,   File ""/data01/ssy/MegatronLM/megatron/core/distributed/distributed_data_parallel.py"", line 225, in __init__     self.buffers, self.bucket_groups = _allocate_buffers_for_parameters(   File ""/data01/ssy/MegatronLM/megatron/core/distributed/distributed_data_parallel.py"", line 167, in _allocate_buffers_for_parameters     _ParamAndGradBuffer(   File ""/data01/ssy/MegatronLM/megatron/core/distributed/param_and_grad_buffer.py"", line 505, in __init__     self.grad_data = torch.zeros(   File ""/data01/ssy/condaEnv/gpt2/lib/python3.8/sitepackages/torch_npu/contrib/transfer_to_npu.py"", line 56, in decorated     return fn(*args, **kwargs) RuntimeError: NPU out of memory. Tried to allocate 50.31 GiB (NPU 0; 60.97 GiB total capacity; 25.16 GiB already allocated; 25.16 GiB current active; 34.44 GiB free; 25.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.",2025-03-17T22:13:16+08:00,,open,0,2,https://gitee.com/ascend/MindSpeed/issues/IBU3B8,你好！这里报错显示OOM问题，可能是参数设置过大所致，需要更详细的问题描述（如脚本参数配置等）。,部分信息如下： GPT_MODEL_ARGS=(     numlayers 15     hiddensize 4096     numattentionheads 32      seqlength 2048      maxpositionembeddings 2048  ) TRAINING_ARGS=(     microbatchsize 1      globalbatchsize 2      rampupbatchsize 16 16 5859375      trainiters 50 ） 两张卡，2路数据并行，无张量并行，流水线并行
Zach M,ModuleNotFoundError: No module named 'mindspeed.features_manager',"一、问题现象（附报错日志上下文）： /opt/conda/lib/python3.10/sitepackages/torch_npu/contrib/transfer_to_npu.py:301: ImportWarning:      *************************************************************************************************************     The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..     The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..     The backend in torch.distributed.init_process_group set to hccl now..     The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..     The device parameters have been replaced with npu in the function below:     torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.autocast, torch.load, torch.Generator, torch.set_default_device, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.nn.Module.to, torch.nn.Module.to_empty     *************************************************************************************************************   warnings.warn(msg, ImportWarning) /opt/conda/lib/python3.10/sitepackages/torch_npu/contrib/transfer_to_npu.py:260: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.   warnings.warn(msg, RuntimeWarning) Traceback (most recent call last):   File ""/workspace/testcode/ModelLink/./preprocess_data.py"", line 36, in      from mindspeed_llm.training.tokenizer import build_tokenizer   File ""/workspace/testcode/ModelLink/mindspeed_llm/__init__.py"", line 16, in      from mindspeed_llm.tasks import megatron_adaptor   File ""/workspace/testcode/ModelLink/mindspeed_llm/tasks/megatron_adaptor.py"", line 23, in      from mindspeed.features_manager import FEATURES_LIST ModuleNotFoundError: No module named 'mindspeed.features_manager' [ERROR] 2025030720:18:44 (PID:17427, Device:1, RankID:1) ERR99999 UNKNOWN application exception /opt/conda/lib/python3.10/tempfile.py:869: ResourceWarning: Implicitly cleaning up    _warnings.warn(warn_message, ResourceWarning) 二、软件版本:  CANN 版本 (CANN 8.0 RC3):   Tensorflow/Pytorch/MindSpore 版本: pytorch2.1.0; torch_npu2.1.0.post8  MegatronLM core_r0.6.0  MindSpeed 969686ff   Linux version 4.19.902107.6.0.0192.8.oe1.bclinux.aarch64 (mockbuild) Python 版本 (Python 3.10.14): 三、测试步骤： 按照USER_GUIDE处理预训练数据集过程中  python ./preprocess_data.py \     input ./dataset/train00000of00042d964455e17e96d5a.parquet \     tokenizernameorpath ./model_from_hf/llama2hf \     tokenizertype PretrainedFromHF \     handlername GeneralPretrainHandler \     outputprefix ./dataset/enwiki \     jsonkeys text \     workers 4 \     loginterval 1000  ",2025-03-07T20:31:28+08:00,,open,0,2,https://gitee.com/ascend/MindSpeed/issues/IBRP1H,你好，可能需要检查下配套版本。对应MegatronLM core_r0.6.0的较早版本MindSpeed core_r0.6.0中不应当有features_manager。 features_manager在该PR引入https://gitee.com/ascend/MindSpeed/pulls/1797/files。,用 core_r0.6.0 也解决不了这个问题啊， 所以该用哪个配套呢
Luo-Jinyan,RuntimeError: Error building extension 'quant_grouped_matmul',"一、问题现象（附报错日志上下文）：     from mindspeed.ops import gmm, quant_gmm   File ""/MindSpeed/mindspeed/ops/quant_gmm.py"", line 8, in      mindspeed_ops = QuantGMMOpBuilder().load()   File ""MindSpeed/mindspeed/op_builder/builder.py"", line 70, in load     op_module = load(name=self.name,   File ""/lib/python3.10/sitepackages/torch/utils/cpp_extension.py"", line 1309, in load     return _jit_compile(   File ""/lib/python3.10/sitepackages/torch/utils/cpp_extension.py"", line 1719, in _jit_compile     _write_ninja_file_and_build_library(   File ""/lib/python3.10/sitepackages/torch/utils/cpp_extension.py"", line 1832, in _write_ninja_file_and_build_library     _run_ninja_build(   File ""/lib/python3.10/sitepackages/torch/utils/cpp_extension.py"", line 2123, in _run_ninja_build     raise RuntimeError(message) from e RuntimeError: Error building extension 'quant_grouped_matmul' 二、软件版本:  CANN 版本 : CANN 8.0.0   Pytorch 版本: 2.3.1  Python 版本: Python 3.10.12 三、MindSpeed安装方式： git clone https://gitee.com/ascend/MindSpeed.git cd MindSpeed git checkout a956b907ef3b0787d2a38577eb5b702f5b7e715d 推荐commit pip install e . 四、问题： 按上述的安装方式执行后，MindSpeed Version: 0.7.0，从库导包报了这个错，这是什么原因呢？",2025-02-28T11:08:11+08:00,,closed,0,0,https://gitee.com/ascend/MindSpeed/issues/IBPMEM
李旭,开启AutoTuning时报错,"一、问题现象（附报错日志上下文）： 开启AutoTuning时报如下错误： Traceback (most recent call last):   File ""/tmp/MegatronLM/pretrain_gpt.py"", line 265, in      pretrain(   File ""/tmp/MindSpeed/mindspeed/core/training.py"", line 442, in wrapper     auto_tuning(global_args, working_dir=working_dir_root)   File ""/tmp/MindSpeed/mindspeed/auto_tuning/auto_tuning.py"", line 70, in auto_tuning     MemoryModeling.modeling(working_dir)   File ""/tmp/MindSpeed/mindspeed/auto_tuning/module/memory/memory_modeling.py"", line 35, in modeling     cls._dynamic_modeling.model_dynamic_mem(working_dir)   File ""/tmp/MindSpeed/mindspeed/auto_tuning/module/memory/dynamic_mem_modeling.py"", line 138, in model_dynamic_mem     tp4seq4k_prof = _get_profiling(baseline_cfg)   File ""/tmp/MindSpeed/mindspeed/auto_tuning/module/memory/dynamic_mem_modeling.py"", line 133, in _get_profiling     return profiling_node_parse.fuse_node_pkl()   File ""/tmp/MindSpeed/mindspeed/auto_tuning/module/parse/profiling_parse/profiling_node_parse.py"", line 42, in fuse_node_pkl     pkl_files = sorted(os.listdir(pkl_path)) FileNotFoundError: [Errno 2] No such file or directory: '/xxxxx/auto_tuning_dir/auto_tuning_profiling_4tp_4dp_1pp_1cp_1mbs_4096seq/pkl_path'",2025-02-21T12:14:24+08:00,,open,0,2,https://gitee.com/ascend/MindSpeed/issues/IBNUFY,这里的报错在上面，需要把log附全 是否正确开启了全重计算和rsmo算子,normalization RMSNorm usefusedrmsnorm 最好附上.SH脚本，大概率问题出在训练脚本配置里
slyviacassell,Bump up the MegatronLM from core_r0.7.0 to core_r0.8.0 or newest MegatronLM version,"一、需求场景&价值 Currently, the MegatronLM has bump up to core_r0.10.0 and the APIs have changed a lot. For instance, the token_dispatcher in MoE now supports the dynamic expert choosing. The dependency of the most latest MindSpeed is MegatronLM core_r0.7.0 which is developed for fixed expert choosing. So, how about bump up the MegatronLM version? 二、需求建议实现的规格 三、竞品比较（选填）",2025-01-16T11:19:24+08:00,,open,0,2,https://gitee.com/ascend/MindSpeed/issues/IBI579,"Currently, we have already started integrating the Megatron 0.9.0 and 0.10.0 version, and we will ensure support for both existing features and new features.",I am very glad to know that. Thank you for your help!
JinXiaozhao,Norm重计算是否支持Legacy分支？,"https://gitee.com/ascend/MindSpeed/blob/master/docs/features/normrecompute.md页面中注明：Norm重计算仅支持core分支 https://gitee.com/ascend/MindSpeed:~:text=%E2%9C%85,Ascend%20Norm%E9%87%8D%E8%AE%A1%E7%AE%97,link 特性页面中显示：Norm重计算支持core和Legacy两个分支 请问Norm重计算是否支持Legacy分支？",2025-01-03T16:25:26+08:00,,open,0,1,https://gitee.com/ascend/MindSpeed/issues/IBFIHW,不支持。首页描述有误，即将修复。:persevere: 
JinqiHuang,开启zero3功能后出现‘data_parallel_group’报错,一、问题现象（附报错日志上下文）： mindspeed开启zero3后，出现TypeError: distributed_data_parallel_init_zero3() got multiple values for argument的报错 二、软件版本:  Mindspeed 版本: 070  Megatron_LM版本: 070 三、测试步骤： 在运行脚本中开启'enablezero3'进行测试 四、日志信息: xxxx 请根据自己的运行环境参考以下方式搜集日志信息，如果涉及到算子开发相关的问题，建议也提供UT/ST测试和单算子集成测试相关的日志。 日志提供方式: 将日志打包后作为附件上传。若日志大小超出附件限制，则可上传至外部网盘后提供链接。 获取方法请参考wiki： https://gitee.com/ascend/modelzoo/wikis/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E6%97%A5%E5%BF%97%E5%92%8C%E8%AE%A1%E7%AE%97%E5%9B%BE?sort_id=4097825,2025-01-02T14:38:01+08:00,,open,0,1,https://gitee.com/ascend/MindSpeed/issues/IBF66Q,070版本暂不支持，可以尝试060版本
心若无尘,请问MC2通算融合目前还不支持吗？,一、需求场景&价值 目前MindSpeed的TP并行能够看到支持通算融合，但是仅支持默认参数为0。请问MindSpeed预计什么时候实现TP并行参数可配置？此外，目前通算融合的瓶颈是什么，是Scale Up域内的带宽和时延不能满足吗？从理论上看，使用通算融合，对于集群性能提升还是很大。 二、需求建议实现的规格 通算融合在实际使用中对集群性能提升大吗？目前实现通算融合主要困难是什么，是使用通算融合导致GPU计算性能的降低，比使用通算融合带来的收益更大？还是别的困难？ 三、竞品比较（选填） 目前Megatroncore能够支持tp_comm_overlap，虽然未指定底层通信库和硬件。但好像仅使用NCCL和英伟达的卡才能够正常运行。,2024-12-26T17:15:40+08:00,,closed,0,1,https://gitee.com/ascend/MindSpeed/issues/IBDZ87,https://gitee.com/MindSpeed/blob/core_r0.7.0/docs/features/mc2.md 可以尝试使用
韩宇,请问是否有MindSpeed和DeepSpeed的对比介绍，两者是否可切换迁移,请问是否有MindSpeed和DeepSpeed的对比介绍，两者是否可切换迁移,2024-12-23T18:49:24+08:00,,closed,0,1,https://gitee.com/ascend/MindSpeed/issues/IBD6YU,MindSpeed是类megatron的训练框架，针对昇腾设备进行亲和性优化。DeepSpeed也已经原生支持昇腾，两者无法自动迁移切换
hexu,openmind开源实习第三期模型适配中遇到的一个bug,"一、问题现象（附报错日志上下文）： 执行模型转换，hf2legacy时报错： !报错代码 可以看到从该文件中import几个函数后执行了该py导致报错。 !报错文件 args中不需要那几个属性 !权重转换脚本 换成mindspeed0.7版本就没有上述问题。 二、软件版本: Python 版本 (e.g., Python 3.7.5): 3.8 mindspeed 版本: 0.8.0 三、测试步骤： ```bash git clone https://gitee.com/ascend/MindSpeedLLM.git  git clone https://github.com/NVIDIA/MegatronLM.git cd MegatronLM git checkout core_r0.7.0 cp r megatron ../MindSpeedLLM/ cd .. git clone https://gitee.com/ascend/MindSpeed.git cd MindSpeed pip install r requirements.txt  pip3 install e . cd.. cd MindSpeedLLM mkdir logs mkdir model_from_hf mkdir dataset mkdir ckpt  再执行一个hf2legacy脚本 ```",2024-12-21T19:04:25+08:00,,open,0,1,https://gitee.com/ascend/MindSpeed/issues/IBCUGF,您好，MindSpeedLLM与MindSpeed有配套关系，当前MindSpeedLLM配套MindSpeed core_r0.7.0分支，请先使用配套分支。core_r0.8.0正在切换当中
zhaohuihui,mindspeed支持zero3➕cpu offload吗？,,2024-12-18T18:56:28+08:00,,closed,0,1,https://gitee.com/ascend/MindSpeed/issues/IBC7ST,暂不支持，在规划中
atmosphere_aloha,是否考虑支持CosyVoice?,一、需求场景&价值 二、需求建议实现的规格 三、竞品比较（选填）,2024-12-17T18:11:49+08:00,,closed,0,1,https://gitee.com/ascend/MindSpeed/issues/IBBX7O,暂无计划
JayNine,急急急，当数据路径存在多个文件时，会报错显示 no sample to consume: 0,"一、问题现象（附报错日志上下文）： 如果data path存在多个文件，或者写明权重，就会出现报错。（实测单独使用任意一个数据集文件都能正常运行） 单个数据集文件，声明权重，报相同的错。 多个数据集文件，无论是否声明权重，都会报错，且错误相同。 任意单个数据集文件，不声明权重，可以正常运行。 `  data_path ....................................... ['1', '/home/mauser/work/dnadata/gene_HG00103_text_document', '1', '/home/mauser/work/dnadata/gene_HG00113_text_document']` 二、软件版本: Megatron版本：0.8.0 MindSpeed版本：0.8.0 CANN套件版本：8.0rc3 已安装NNCL并设置环境变量 ``` > finished creating GPT datasets ... Traceback (most recent call last):   File ""/home/mauser/CusEnv/MegatronLM/pretrain_gpt.py"", line 246, in      pretrain(   File ""/home/mauser/CusEnv/MindSpeed/mindspeed/core/training.py"", line 512, in wrapper     pretrain(*args, **kwargs)   File ""/home/mauser/CusEnv/MindSpeed/mindspeed/training.py"", line 251, in pretrain     = build_train_valid_test_data_iterators(   File ""/home/mauser/CusEnv/MegatronLM/megatron/training/training.py"", line 1515, in build_train_valid_test_data_iterators     build_train_valid_test_data_loaders(   File ""/home/mauser/CusEnv/MegatronLM/megatron/training/training.py"", line 1484, in build_train_valid_test_data_loaders     valid_dataloader = build_pretraining_data_loader(   File ""/home/mauser/CusEnv/MindSpeed/mindspeed/core/performance/auto_pipeline_perf/data_samplers.py"", line 28, in wrapper     dataloader = build_pretraining_data_loader(*args, **kwargs)   File ""/home/mauser/CusEnv/MegatronLM/megatron/legacy/data/data_samplers.py"", line 23, in build_pretraining_data_loader     batch_sampler = MegatronPretrainingSampler(   File ""/home/mauser/CusEnv/MegatronLM/megatron/legacy/data/data_samplers.py"", line 68, in __init__     assert self.total_samples > 0, \ AssertionError: no sample to consume: 0 ```",2024-12-14T20:13:16+08:00,,open,0,3,https://gitee.com/ascend/MindSpeed/issues/IBBB66,完整报错日志： 通过百度网盘分享的文件：output.log 链接：https://pan.baidu.com/s/1sTxAMkP8u0w2A0wywGYb9A?pwd=k3lb  提取码：k3lb,试下改megatron/legacy/data/data_samplers.py  以下代码     if dataset is None:         return None        变成     if dataset is None or len(dataset) == 0:         return None,请问问题是否解决
JayNine,编译ATB算子失败 Building extension module npu_matmul_add_fp32,"Megatron版本：0.7.0 MindSpeed版本：0.7.0 CANN套件版本：8.0rc2  已安装NNCL并设置环境变量 ``` Building extension module npu_matmul_add_fp32... Using envvar MAX_JOBS (16) as the number of workers... [1/4] c++ MMD MF flop_counter.o.d DTORCH_EXTENSION_NAME=npu_matmul_add_fp32 DTORCH_API_INCLUDE_EXTENSION_H DPYBIND11_COMPILER_TYPE=\""_gcc\"" DPYBIND11_STDLIB=\""_libstdcpp\"" DPYBIND11_BUILD_ABI=\""_cxxabi1011\"" I/home/mauser/Ascend/ascendtoolkit/latest/include I/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/include I/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/third_party I/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/acl I/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/inc I/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/include/third_party/acl/inc I/home/mauser/CusEnv/MindSpeed/mindspeed/ops/csrc/atb/extensions/inc I/home/mauser/Ascend/nnal/atb/latest/atb/cxx_abi_0/include isystem /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/include isystem /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/include/torch/csrc/api/include isystem /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/include/TH isystem /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/include/THC isystem /home/mauser/anaconda3/envs/PyTorch2.1.0/include/python3.9 D_GLIBCXX_USE_CXX11_ABI=0 fPIC std=c++17 fstackprotectorall Wl,z,relro,z,now,z,noexecstack fPIC pie s fvisibility=hidden D_FORTIFY_SOURCE=2 O2 Wnosigncompare Wnodeprecateddeclarations Wnoreturntype D__FILENAME__='""$(notdir $(abspath $<))""' D ENABLE_ATB fstackprotectorall Wl,z,relro,z,now,z,noexecstack fPIC pie s c /home/mauser/CusEnv/MindSpeed/mindspeed/ops/csrc/flop_counter/flop_counter.cpp o flop_counter.o  [2/4] c++ MMD MF atb_adapter.o.d DTORCH_EXTENSION_NAME=npu_matmul_add_fp32 DTORCH_API_INCLUDE_EXTENSION_H DPYBIND11_COMPILER_TYPE=\""_gcc\"" DPYBIND11_STDLIB=\""_libstdcpp\"" DPYBIND11_BUILD_ABI=\""_cxxabi1011\"" I/home/mauser/Ascend/ascendtoolkit/latest/include I/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/include I/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/third_party I/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/acl I/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/inc I/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/include/third_party/acl/inc I/home/mauser/CusEnv/MindSpeed/mindspeed/ops/csrc/atb/extensions/inc I/home/mauser/Ascend/nnal/atb/latest/atb/cxx_abi_0/include isystem /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/include isystem /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/include/torch/csrc/api/include isystem /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/include/TH isystem /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/include/THC isystem /home/mauser/anaconda3/envs/PyTorch2.1.0/include/python3.9 D_GLIBCXX_USE_CXX11_ABI=0 fPIC std=c++17 fstackprotectorall Wl,z,relro,z,now,z,noexecstack fPIC pie s fvisibility=hidden D_FORTIFY_SOURCE=2 O2 Wnosigncompare Wnodeprecateddeclarations Wnoreturntype D__FILENAME__='""$(notdir $(abspath $<))""' D ENABLE_ATB fstackprotectorall Wl,z,relro,z,now,z,noexecstack fPIC pie s c /home/mauser/CusEnv/MindSpeed/mindspeed/ops/csrc/atb/utils/atb_adapter.cpp o atb_adapter.o  [3/4] c++ MMD MF matmul_add.o.d DTORCH_EXTENSION_NAME=npu_matmul_add_fp32 DTORCH_API_INCLUDE_EXTENSION_H DPYBIND11_COMPILER_TYPE=\""_gcc\"" DPYBIND11_STDLIB=\""_libstdcpp\"" DPYBIND11_BUILD_ABI=\""_cxxabi1011\"" I/home/mauser/Ascend/ascendtoolkit/latest/include I/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/include I/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/third_party I/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/acl I/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/inc I/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/include/third_party/acl/inc I/home/mauser/CusEnv/MindSpeed/mindspeed/ops/csrc/atb/extensions/inc I/home/mauser/Ascend/nnal/atb/latest/atb/cxx_abi_0/include isystem /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/include isystem /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/include/torch/csrc/api/include isystem /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/include/TH isystem /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/include/THC isystem /home/mauser/anaconda3/envs/PyTorch2.1.0/include/python3.9 D_GLIBCXX_USE_CXX11_ABI=0 fPIC std=c++17 fstackprotectorall Wl,z,relro,z,now,z,noexecstack fPIC pie s fvisibility=hidden D_FORTIFY_SOURCE=2 O2 Wnosigncompare Wnodeprecateddeclarations Wnoreturntype D__FILENAME__='""$(notdir $(abspath $<))""' D ENABLE_ATB fstackprotectorall Wl,z,relro,z,now,z,noexecstack fPIC pie s c /home/mauser/CusEnv/MindSpeed/mindspeed/ops/csrc/atb/matmul_add.cpp o matmul_add.o  FAILED: matmul_add.o  c++ MMD MF matmul_add.o.d DTORCH_EXTENSION_NAME=npu_matmul_add_fp32 DTORCH_API_INCLUDE_EXTENSION_H DPYBIND11_COMPILER_TYPE=\""_gcc\"" DPYBIND11_STDLIB=\""_libstdcpp\"" DPYBIND11_BUILD_ABI=\""_cxxabi1011\"" I/home/mauser/Ascend/ascendtoolkit/latest/include I/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/include I/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/third_party I/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/acl I/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/inc I/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/include/third_party/acl/inc I/home/mauser/CusEnv/MindSpeed/mindspeed/ops/csrc/atb/extensions/inc I/home/mauser/Ascend/nnal/atb/latest/atb/cxx_abi_0/include isystem /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/include isystem /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/include/torch/csrc/api/include isystem /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/include/TH isystem /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/include/THC isystem /home/mauser/anaconda3/envs/PyTorch2.1.0/include/python3.9 D_GLIBCXX_USE_CXX11_ABI=0 fPIC std=c++17 fstackprotectorall Wl,z,relro,z,now,z,noexecstack fPIC pie s fvisibility=hidden D_FORTIFY_SOURCE=2 O2 Wnosigncompare Wnodeprecateddeclarations Wnoreturntype D__FILENAME__='""$(notdir $(abspath $<))""' D ENABLE_ATB fstackprotectorall Wl,z,relro,z,now,z,noexecstack fPIC pie s c /home/mauser/CusEnv/MindSpeed/mindspeed/ops/csrc/atb/matmul_add.cpp o matmul_add.o  /home/mauser/CusEnv/MindSpeed/mindspeed/ops/csrc/atb/matmul_add.cpp: In function ‘void {anonymous}::matmul_add_fp32(const at::Tensor&, const at::Tensor&, at::Tensor&)’: /home/mauser/CusEnv/MindSpeed/mindspeed/ops/csrc/atb/matmul_add.cpp:48:15: error: ‘struct atb::infer::LinearParam’ has no member named ‘enAccum’          param.enAccum = true;                ^~~~~~~ ninja: build stopped: subcommand failed. ```",2024-12-11T14:01:48+08:00,,closed,0,1,https://gitee.com/ascend/MindSpeed/issues/IBAKVF,算子在rc3版本支持，可以先开启nogradientaccumulationfusion选择不开启此融合
Fragilele,开启ulysses CP并行训练报错,"一、问题现象（附报错日志上下文）： File ""/home/atlas/y00880253/modellink1.0.rc3/megatron/core/transformer/attention.py"", line 315, in forward   File ""/root/anaconda3/envs/mlrc3/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/root/anaconda3/envs/mlrc3/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1568, in _call_impl     core_attn_out = self.core_attention(   File ""/root/anaconda3/envs/mlrc3/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/root/anaconda3/envs/mlrc3/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1568, in _call_impl     result = forward_call(*args, **kwargs)   File ""/home/atlas/y00880253/MindSpeed1.0.RC3_core_r0.6.0/mindspeed/core/context_parallel/ulysses_context_parallel.py"", line 92, in forward     return self._call_impl(*args, **kwargs)   File ""/root/anaconda3/envs/mlrc3/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1568, in _call_impl     key_layer = _SeqAllToAll.apply(self.spg, key, self.scatter_idx, self.gather_idx)   File ""/root/anaconda3/envs/mlrc3/lib/python3.9/sitepackages/torch/autograd/function.py"", line 539, in apply     result = forward_call(*args, **kwargs)   File ""/home/atlas/y00880253/MindSpeed1.0.RC3_core_r0.6.0/mindspeed/core/context_parallel/ulysses_context_parallel.py"", line 92, in forward     return super().apply(*args, **kwargs)   type: ignore[misc]   File ""/home/atlas/y00880253/MindSpeed1.0.RC3_core_r0.6.0/mindspeed/core/context_parallel/ulysses_context_parallel.py"", line 49, in forward     key_layer = _SeqAllToAll.apply(self.spg, key, self.scatter_idx, self.gather_idx)   File ""/root/anaconda3/envs/mlrc3/lib/python3.9/sitepackages/torch/autograd/function.py"", line 539, in apply         return single_all_to_all(input_, scatter_idx, gather_idx, group)result = forward_call(*args, **kwargs)   File ""/home/atlas/y00880253/MindSpeed1.0.RC3_core_r0.6.0/mindspeed/core/context_parallel/ulysses_context_parallel.py"", line 23, in single_all_to_all   File ""/home/atlas/y00880253/MindSpeed1.0.RC3_core_r0.6.0/mindspeed/core/context_parallel/ulysses_context_parallel.py"", line 92, in forward     input_t = input_.reshape( RuntimeError: shape '[1, 8, 0, 128]' is invalid for input of size 1048576 二、软件版本:  CANN 版本 (e.g., CANN 3.0.x，5.x.x): 8.0.RC3 Tensorflow/Pytorch/MindSpore 版本: 2.1 Python 版本 (e.g., Python 3.7.5): 3.9 操作系统版本 (e.g., Ubuntu 18.04): eulerosv2r12 三、测试步骤： 使用默认mcore训练脚本 bash examples/mcore/chatglm3/pretrain_chatglm3_6B_32K.sh",2024-12-05T16:49:39+08:00,,open,0,2,https://gitee.com/ascend/MindSpeed/issues/IB9CGL,megatron的examples目录下无此测试脚本，麻烦发送一下完整脚本,请确认kv head是否够cp切分
heyanyu,我也遇到这个问题了，请问如何解决这个问题？,"一、问题现象（附报错日志上下文）： 数据集转换和权重转换都报ModuleNotFoundError: No module named 'mindspeed.core.context_parallel.adaptive_context_parallel' 二、软件版本:  CANN 版本 (e.g., CANN 3.0.x，5.x.x): 8.0.RC3 Tensorflow/Pytorch/MindSpore 版本:2.1.0 Python 版本 (e.g., Python 3.7.5):3.8  MindStudio版本 (e.g., MindStudio 2.0.0 (beta3)):无 操作系统版本 (e.g., Ubuntu 18.04):麒麟v10sp2 三、测试步骤： python ./preprocess_data.py input ./dataset/my_data.parquet tokenizernameorpath ./ model_from_hf/Qwen272BInstruct tokenizertype PretrainedFromHF handlername GeneralPretrainHandler outputprefix ./dataset/enwiki jsonkeys text workers 4 loginterval 1000",2024-11-25T14:50:11+08:00,,open,0,0,https://gitee.com/ascend/MindSpeed/issues/IB6TF3
JoeyLin,tockens拼写错误,"一、问题现象（附报错日志上下文）： mindspeed/model/transformer.py 中 部分 tokens 被写成 tockens，拼写错误望修正 二、软件版本:  CANN 版本 (e.g., CANN 3.0.x，5.x.x):  不涉及 Tensorflow/Pytorch/MindSpore 版本: 不涉及 Python 版本 (e.g., Python 3.7.5):不涉及  MindStudio版本 (e.g., MindStudio 2.0.0 (beta3)):不涉及 操作系统版本 (e.g., Ubuntu 18.04):不涉及 三、测试步骤： 不涉及 四、日志信息: 不涉及 请根据自己的运行环境参考以下方式搜集日志信息，如果涉及到算子开发相关的问题，建议也提供UT/ST测试和单算子集成测试相关的日志。 日志提供方式: 将日志打包后作为附件上传。若日志大小超出附件限制，则可上传至外部网盘后提供链接。 获取方法请参考wiki： https://gitee.com/ascend/modelzoo/wikis/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E6%97%A5%E5%BF%97%E5%92%8C%E8%AE%A1%E7%AE%97%E5%9B%BE?sort_id=4097825",2024-11-23T09:43:35+08:00,,open,0,0,https://gitee.com/ascend/MindSpeed/issues/IB6I38
flynn,MindSpeed 是否支持fp8的训练,直接使用megatron的fp8的训练会对transformer engine存在依赖，导致报错,2024-11-15T14:18:37+08:00,,closed,0,1,https://gitee.com/ascend/MindSpeed/issues/IB4M1F,暂不支持
Bryan,profile功能对meagtron的patch和modelink冲突,对megatron的train_step和train方法打的patch和modellink冲突，modelink触发的是modellink仓的train方法，mindspeed对meagtron的patch只有train_step生效，train无效，导致profile_npu属性不存在 !输入图片说明,2024-11-12T16:28:45+08:00,,closed,0,1,https://gitee.com/ascend/MindSpeed/issues/IB3RQJ,已修改，参考PR：https://gitee.com/ascend/MindSpeed/pulls/1392
zmmtt,参数复用实现,请问参数复用中的静态内存是怎么测试的？ 以及开启分布式优化器之后，对应的megatron代码中的loadparameterstaet和getparameterstate怎么修改呢？,2024-11-11T10:45:16+08:00,,open,0,0,https://gitee.com/ascend/MindSpeed/issues/IB3CUK
铁伟业,数据集转换和权重转换都报ModuleNotFoundError,"一、问题现象（附报错日志上下文）： 数据集转换和权重转换都报ModuleNotFoundError: No module named 'mindspeed.core.context_parallel.adaptive_context_parallel' 二、软件版本:  CANN 版本 (e.g., CANN 3.0.x，5.x.x):  8.0.RC3 Tensorflow/Pytorch/MindSpore 版本:2.1.0 Python 版本 (e.g., Python 3.7.5):3.8  MindStudio版本 (e.g., MindStudio 2.0.0 (beta3)):无 操作系统版本 (e.g., Ubuntu 18.04):麒麟v10sp2 三、测试步骤： python ./preprocess_data.py \     input ./dataset/my_data.parquet \     tokenizernameorpath ./ model_from_hf/Qwen272BInstruct \     tokenizertype PretrainedFromHF \     handlername GeneralPretrainHandler \     outputprefix ./dataset/enwiki \     jsonkeys text \     workers 4 \     loginterval 1000 ",2024-11-07T09:43:28+08:00,,closed,0,2,https://gitee.com/ascend/MindSpeed/issues/IB2LPV,我也是这个问题，请问解决了吗,RC4版本没有这个问题
铁伟业,权重转换报错ImportError: cannot import name '_get_batch_on_this_cp_rank_in_adaptive_cp' from 'mindspeed.utils' ,"一、问题现象（附报错日志上下文）： (modellink) [root ModelLink] python convert_ckpt.py \ >        modeltype GPT \ >        loadmodeltype hf \ >        savemodeltype mg \ >        targettensorparallelsize 1 \ >        targetpipelineparallelsize 1 \ >        loaddir ./model_from_hf/Qwen272BInstruct \ >        savedir ./model_weights/qwen2_mcore/ \ >        tokenizermodel  ./model_from_hf/Qwen272BInstruct/tokenizer.json \ >        modeltypehf qwen2 \ >        paramsdtype bf16 /root/miniconda3/envs/modellink/lib/python3.8/sitepackages/torch_npu/dynamo/__init__.py:18: UserWarning: Register eager implementation for the 'npu' backend of dynamo, as torch_npu was not compiled with torchair.   warnings.warn( /root/miniconda3/envs/modellink/lib/python3.8/sitepackages/torch_npu/contrib/transfer_to_npu.py:164: ImportWarning:     *************************************************************************************************************     The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..     The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..     The backend in torch.distributed.init_process_group set to hccl now..     The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..     The device parameters have been replaced with npu in the function below:     torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.nn.Module.to, torch.nn.Module.to_empty     *************************************************************************************************************   warnings.warn(msg, ImportWarning) /root/miniconda3/envs/modellink/lib/python3.8/sitepackages/torch_npu/contrib/transfer_to_npu.py:124: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.   warnings.warn(msg, RuntimeWarning) /root/miniconda3/envs/modellink/lib/python3.8/sitepackages/torch_npu/contrib/transfer_to_npu.py:124: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.   warnings.warn(msg, RuntimeWarning) /root/miniconda3/envs/modellink/lib/python3.8/sitepackages/torch_npu/contrib/transfer_to_npu.py:124: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.   warnings.warn(msg, RuntimeWarning) Traceback (most recent call last):   File ""convert_ckpt.py"", line 23, in      from modellink import megatron_adaptor   File ""/home/openlab/ModelLink/modellink/__init__.py"", line 16, in      from modellink.tasks import megatron_adaptor   File ""/home/openlab/ModelLink/modellink/tasks/megatron_adaptor.py"", line 690, in      MegatronAdaptation.execute()   File ""/home/openlab/ModelLink/modellink/tasks/megatron_adaptor.py"", line 36, in execute     adaptation.execute()   File ""/home/openlab/ModelLink/modellink/tasks/megatron_adaptor.py"", line 221, in execute     self.patch_core_models()   File ""/home/openlab/ModelLink/modellink/tasks/megatron_adaptor.py"", line 260, in patch_core_models     from ..training.utils import get_batch_on_this_cp_rank, get_batch_on_this_tp_rank, get_device_wrapper   File ""/home/openlab/ModelLink/modellink/training/__init__.py"", line 16, in      from .training import (get_model_wrapper, is_profile_enabled, get_profiler, setup_model_and_optimizer_wrapper,   File ""/home/openlab/ModelLink/modellink/training/training.py"", line 56, in      from modellink.training.initialize import set_jit_fusion_options   File ""/home/openlab/ModelLink/modellink/training/initialize.py"", line 32, in      from modellink.training.arguments import parse_args_decorator   File ""/home/openlab/ModelLink/modellink/training/arguments.py"", line 19, in      from modellink.training.utils import print_rank0_by_args   File ""/home/openlab/ModelLink/modellink/training/utils.py"", line 30, in      from mindspeed.utils import (set_actual_seq_len, set_position_ids, ImportError: cannot import name '_get_batch_on_this_cp_rank_in_adaptive_cp' from 'mindspeed.utils' (/home/openlab/ModelLink/MindSpeed/mindspeed/utils.py) 二、软件版本:  CANN 版本 (e.g., CANN 3.0.x，5.x.x):  8.0.R3 Tensorflow/Pytorch/MindSpore 版本:pytorch2.1.0 Python 版本 (e.g., Python 3.7.5):3.8  MindStudio版本 (e.g., MindStudio 2.0.0 (beta3)):无 操作系统版本 (e.g., Ubuntu 18.04):麒麟v10sp2 三、测试步骤： python convert_ckpt.py \        modeltype GPT \        loadmodeltype hf \        savemodeltype mg \        targettensorparallelsize 1 \        targetpipelineparallelsize 1 \        loaddir ./model_from_hf/Qwen272BInstruct \        savedir ./model_weights/qwen2_mcore/ \        tokenizermodel  ./model_from_hf/Qwen272BInstruct/tokenizer.json \        modeltypehf qwen2 \        paramsdtype bf16",2024-11-06T19:05:28+08:00,,open,0,2,https://gitee.com/ascend/MindSpeed/issues/IB2JS1,我切换了 Mindspeed 的版本可以奏效（虽然也有遇到其他问题），如果是 `pip install e .` 安装的 Mindspeed 直接在目录 `git checkout core_r0.6.0` 应该就行，如果没有 e 可能需要重新安装。 `core_r0.6.0` 分支是有这个函数的。,RC4使用正常
JinXiaozhao,PP自动并行训练是否支持core模块？,使用MegatronLM中的core模块构造模型，是否可以使用PP自动并行模块？,2024-10-31T17:31:37+08:00,,open,0,0,https://gitee.com/ascend/MindSpeed/issues/IB16S7
Jack,npu_matmul_add 编译失败‘struct atb::infer::LinearParam’ has no member named ‘enAccum’,"一、问题现象（附报错日志上下文）： 使用华为云ModelArts的Notebook环境，安装好NNAL相关库之后，运行MindSpeed报错 ``` Building extension module npu_matmul_add... Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N) [1/3] c++ MMD MF atb_adapter.o.d DTORCH_EXTENSION_NAME=npu_matmul_add DTORCH_API_INCLUDE_EXTENSION_H DPYBIND11_COMPILER_TYPE=\""_gcc\"" DPYBIND11_STDLIB=\""_libstdcpp\"" DPYBIND11_BUILD_ABI=\""_cxxabi1011\"" I/usr/local/Ascend/ascendtoolkit/latest/include I/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/include I/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/third_party I/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/acl I/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/inc I/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/include/third_party/acl/inc I/home/mauser/work/MindSpeed/mindspeed/ops/csrc/atb/extensions/inc I/usr/local/Ascend/nnal/atb/latest/atb/cxx_abi_0/include isystem /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/include isystem /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/include/torch/csrc/api/include isystem /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/include/TH isystem /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/include/THC isystem /home/mauser/anaconda3/envs/PyTorch2.1.0/include/python3.9 D_GLIBCXX_USE_CXX11_ABI=0 fPIC std=c++17 fstackprotectorall Wl,z,relro,z,now,z,noexecstack fPIC pie Wl,disablenewdtags,rpath s Wnosigncompare Wnodeprecateddeclarations Wnoreturntype D__FILENAME__='""$(notdir $(abspath $<))""' D ENABLE_ATB fstackprotectorall Wl,z,relro,z,now,z,noexecstack fPIC pie Wl,disablenewdtags,rpath s c /home/mauser/work/MindSpeed/mindspeed/ops/csrc/atb/utils/atb_adapter.cpp o atb_adapter.o  [2/3] c++ MMD MF matmul_add.o.d DTORCH_EXTENSION_NAME=npu_matmul_add DTORCH_API_INCLUDE_EXTENSION_H DPYBIND11_COMPILER_TYPE=\""_gcc\"" DPYBIND11_STDLIB=\""_libstdcpp\"" DPYBIND11_BUILD_ABI=\""_cxxabi1011\"" I/usr/local/Ascend/ascendtoolkit/latest/include I/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/include I/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/third_party I/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/acl I/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/inc I/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/include/third_party/acl/inc I/home/mauser/work/MindSpeed/mindspeed/ops/csrc/atb/extensions/inc I/usr/local/Ascend/nnal/atb/latest/atb/cxx_abi_0/include isystem /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/include isystem /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/include/torch/csrc/api/include isystem /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/include/TH isystem /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/include/THC isystem /home/mauser/anaconda3/envs/PyTorch2.1.0/include/python3.9 D_GLIBCXX_USE_CXX11_ABI=0 fPIC std=c++17 fstackprotectorall Wl,z,relro,z,now,z,noexecstack fPIC pie Wl,disablenewdtags,rpath s Wnosigncompare Wnodeprecateddeclarations Wnoreturntype D__FILENAME__='""$(notdir $(abspath $<))""' D ENABLE_ATB fstackprotectorall Wl,z,relro,z,now,z,noexecstack fPIC pie Wl,disablenewdtags,rpath s c /home/mauser/work/MindSpeed/mindspeed/ops/csrc/atb/matmul_add.cpp o matmul_add.o  FAILED: matmul_add.o  c++ MMD MF matmul_add.o.d DTORCH_EXTENSION_NAME=npu_matmul_add DTORCH_API_INCLUDE_EXTENSION_H DPYBIND11_COMPILER_TYPE=\""_gcc\"" DPYBIND11_STDLIB=\""_libstdcpp\"" DPYBIND11_BUILD_ABI=\""_cxxabi1011\"" I/usr/local/Ascend/ascendtoolkit/latest/include I/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/include I/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/third_party I/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/acl I/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/inc I/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/include/third_party/acl/inc I/home/mauser/work/MindSpeed/mindspeed/ops/csrc/atb/extensions/inc I/usr/local/Ascend/nnal/atb/latest/atb/cxx_abi_0/include isystem /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/include isystem /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/include/torch/csrc/api/include isystem /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/include/TH isystem /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/include/THC isystem /home/mauser/anaconda3/envs/PyTorch2.1.0/include/python3.9 D_GLIBCXX_USE_CXX11_ABI=0 fPIC std=c++17 fstackprotectorall Wl,z,relro,z,now,z,noexecstack fPIC pie Wl,disablenewdtags,rpath s Wnosigncompare Wnodeprecateddeclarations Wnoreturntype D__FILENAME__='""$(notdir $(abspath $<))""' D ENABLE_ATB fstackprotectorall Wl,z,relro,z,now,z,noexecstack fPIC pie Wl,disablenewdtags,rpath s c /home/mauser/work/MindSpeed/mindspeed/ops/csrc/atb/matmul_add.cpp o matmul_add.o  /home/mauser/work/MindSpeed/mindspeed/ops/csrc/atb/matmul_add.cpp: In function ‘void {anonymous}::matmul_add(const at::Tensor&, const at::Tensor&, at::Tensor&)’: /home/mauser/work/MindSpeed/mindspeed/ops/csrc/atb/matmul_add.cpp:47:15: error: ‘struct atb::infer::LinearParam’ has no member named ‘enAccum’          param.enAccum = true;                ^~~~~~~ ninja: build stopped: subcommand failed. ``` 二、软件版本:  CANN 版本 CANN8.0.RC1  Pytorch 版本: 2.1.0  Python 3.9.10  MindSpeed 0.7.0 操作系统版本: EulerOS 2.0 (SP10) 三、测试步骤： ```  arguments    accumulate_allreduce_grads_in_fp32 .............. False   adam_beta1 ...................................... 0.9   adam_beta2 ...................................... 0.999   adam_eps ........................................ 1e08   adaptive_recompute_device_size .................. 1   adaptive_recompute_device_swap .................. False   adaptive_recompute_profiling_step ............... 10   add_bias_linear ................................. True   add_dense_bias .................................. False   add_position_embedding .......................... True   add_qkv_bias .................................... False   additional_config ............................... None   adlr_autoresume ................................. False   adlr_autoresume_interval ........................ 1000   alibi_diagonal_opposite ......................... False   alibi_fusion_attn_type .......................... None   ampipe_degree ................................... 1   ampipe_tp_sp_comm_overlap ....................... False   apply_layernorm_1p .............................. False   apply_query_key_layer_scaling ................... False   apply_residual_connection_post_layernorm ........ False   apply_rope_fusion ............................... True   async_save ...................................... None   async_tensor_model_parallel_allreduce ........... True   attention_dropout ............................... 0.1   attention_softmax_in_fp32 ....................... False   auto_detect_ckpt_format ......................... False   auto_parallel ................................... False   automated_pipeline .............................. False   automated_pipeline_perf ......................... False   barrier_with_L1_time ............................ True   bert_binary_head ................................ True   bert_embedder_type .............................. megatron   bert_load ....................................... None   bf16 ............................................ False   bias_dropout_fusion ............................. True   bias_gelu_fusion ................................ True   bias_swiglu_fusion .............................. True   biencoder_projection_dim ........................ 0   biencoder_shared_query_context_model ............ False   block_data_path ................................. None   calculate_per_token_loss ........................ False   check_for_nan_in_loss_and_grad .................. False   check_weight_hash_across_dp_replicas_interval ... None   ckpt_assume_constant_structure .................. False   ckpt_fully_parallel_load ........................ False   ckpt_fully_parallel_save ........................ False   ckpt_step ....................................... None   classes_fraction ................................ 1.0   clip_grad ....................................... 1.0   clone_scatter_output_in_embedding ............... True   coc_fused_kernel ................................ False   coc_mode ........................................ 1   coc_parallel_num ................................ 1   consumed_train_samples .......................... 0   consumed_valid_samples .......................... 0   context_parallel_algo ........................... ulysses_cp_algo   context_parallel_size ........................... 1   cp_attention_mask_type .......................... causal   cp_window_size .................................. 1   create_attention_mask_in_dataloader ............. False   data_cache_path ................................. None   data_parallel_random_init ....................... False   data_parallel_size .............................. 2   data_path ....................................... ['/home/mauser/work/obs/miniyulan/mini_data_ids/yulan_try_text_document/yulan_try_text_document']   data_per_class_fraction ......................... 1.0   data_sharding ................................... True   dataloader_type ................................. single   ddp_bucket_size ................................. None   decoder_num_layers .............................. None   decoder_seq_length .............................. None   decoupled_lr .................................... None   decoupled_min_lr ................................ None   delay_grad_reduce ............................... True   delay_param_gather .............................. False   dino_bottleneck_size ............................ 256   dino_freeze_last_layer .......................... 1   dino_head_hidden_size ........................... 2048   dino_local_crops_number ......................... 10   dino_local_img_size ............................. 96   dino_norm_last_layer ............................ False   dino_teacher_temp ............................... 0.07   dino_warmup_teacher_temp ........................ 0.04   dino_warmup_teacher_temp_epochs ................. 30   disable_straggler_on_startup .................... False   dist_ckpt_format ................................ torch_dist   distribute_saved_activations .................... False   distributed_backend ............................. nccl   distributed_timeout_minutes ..................... 10   embedding_path .................................. None   empty_unused_memory_level ....................... 0   enable_one_logger ............................... False   enable_recompute_layers_per_pp_rank ............. False   enable_token_rearrange_opt ...................... False   enable_zero3 .................................... False   encoder_num_layers .............................. 2   encoder_seq_length .............................. 4096   end_weight_decay ................................ 0.01   eod_mask_loss ................................... False   eval_interval ................................... 1000   eval_iters ...................................... 10   evidence_data_path .............................. None   exit_duration_in_mins ........................... None   exit_interval ................................... None   exit_on_missing_checkpoint ...................... False   exit_signal_handler ............................. False   expert_interval ................................. 1   expert_model_parallel_size ...................... 1   ffn_hidden_size ................................. 7680   fill_neg_inf .................................... False   finetune ........................................ False   fp16 ............................................ True   fp16_lm_cross_entropy ........................... False   fp32_residual_connection ........................ False   fp8 ............................................. None   fp8_amax_compute_algo ........................... most_recent   fp8_amax_history_len ............................ 1   fp8_interval .................................... 1   fp8_margin ...................................... 0   fp8_wgrad ....................................... True   global_batch_size ............................... 2   gradient_accumulation_fusion .................... True   group_query_attention ........................... False   head_lr_mult .................................... 1.0   hidden_dropout .................................. 0.1   hidden_size ..................................... 1920   hysteresis ...................................... 2   ict_head_size ................................... None   ict_load ........................................ None   img_h ........................................... 224   img_w ........................................... 224   indexer_batch_size .............................. 128   indexer_log_interval ............................ 1000   inference_batch_times_seqlen_threshold .......... 512   init_method_std ................................. 0.02   init_method_xavier_uniform ...................... False   initial_loss_scale .............................. 4294967296   iter_per_epoch .................................. 1250   jit_compile ..................................... False   kv_channels ..................................... 64   kv_lora_rank .................................... None   lazy_mpu_init ................................... None   load ............................................ None   local_rank ...................................... None   log_batch_size_to_tensorboard ................... False   log_interval .................................... 100   log_learning_rate_to_tensorboard ................ True   log_loss_scale_to_tensorboard ................... True   log_memory_to_tensorboard ....................... False   log_num_zeros_in_grad ........................... False   log_params_norm ................................. False   log_progress .................................... False   log_straggler ................................... False   log_throughput .................................. False   log_timers_to_tensorboard ....................... False   log_validation_ppl_to_tensorboard ............... False   log_world_size_to_tensorboard ................... False   loss_scale ...................................... None   loss_scale_window ............................... 1000   lr .............................................. 0.01   lr_decay_iters .................................. 320000   lr_decay_samples ................................ None   lr_decay_style .................................. cosine   lr_warmup_fraction .............................. 0.01   lr_warmup_init .................................. 0.0   lr_warmup_iters ................................. 0   lr_warmup_samples ............................... 0   make_vocab_size_divisible_by .................... 128   manual_gc ....................................... False   manual_gc_eval .................................. True   manual_gc_interval .............................. 0   mask_factor ..................................... 1.0   mask_prob ....................................... 0.15   mask_type ....................................... random   masked_softmax_fusion ........................... True   master_addr ..................................... None   master_port ..................................... None   max_position_embeddings ......................... 4096   max_tokens_to_oom ............................... 12000   mbs_idx ......................................... None   memory_fragmentation ............................ False   merge_file ...................................... /home/mauser/work/pretrainmini/tokenizer_1731/merges.txt   micro_batch_size ................................ 1   min_loss_scale .................................. 1.0   min_lr .......................................... 1e05   mmap_bin_files .................................. True   mock_data ....................................... False   moe_adaptive_recompute_activation ............... False   moe_adaptive_recompute_activation_scale ......... 2.0   moe_aux_loss_coeff .............................. 0.0   moe_dynamic_padding ............................. False   moe_expert_capacity_factor ...................... None   moe_extended_tp ................................. False   moe_grouped_gemm ................................ False   moe_input_jitter_eps ............................ None   moe_layer_recompute ............................. False   moe_model_type .................................. megatron_moe   moe_no_drop ..................................... False   moe_pad_expert_input_to_capacity ................ False   moe_per_layer_logging ........................... False   moe_permutation_async_comm ...................... False   moe_router_load_balancing_type .................. aux_loss   moe_router_topk ................................. 2   moe_token_dispatcher_type ....................... allgather   moe_token_drop_policy ........................... probs   moe_train_capacity_factor ....................... 1.0   moe_use_sinkhorn ................................ False   moe_z_loss_coeff ................................ None   multi_head_latent_attention ..................... False   n_shared_experts ................................ None   nccl_communicator_config_path ................... None   nd1_dim1_size ................................... 1   nd2_dim1_size ................................... 1   next_tockens .................................... 0   nnodes .......................................... 1   no_load_optim ................................... None   no_load_rng ..................................... None   no_persist_layer_norm ........................... False   no_save_optim ................................... None   no_save_rng ..................................... None   node_rank ....................................... 0   noisy_gate_policy ............................... None   noop_layers ..................................... None   norm_epsilon .................................... 1e05   normalization ................................... LayerNorm   nproc_per_node .................................. 8   num_attention_heads ............................. 30   num_channels .................................... 3   num_classes ..................................... 1000   num_dataset_builder_threads ..................... 1   num_experts ..................................... None   num_layer_list .................................. None   num_layers ...................................... 2   num_layers_per_virtual_pipeline_stage ........... None   num_query_groups ................................ 1   num_workers ..................................... 2   one_logger_entity ............................... hwinf_dcm   one_logger_project .............................. e2etracking   one_logger_run_name ............................. None   onnx_safe ....................................... None   op_cal_tflops ................................... False   openai_gelu ..................................... False   optimize_recomp_communication_level ............. 0   optimize_recomp_communication_status ............ 0   optimize_send_recv_comm ......................... False   optimized_mbs_list .............................. None   optimized_mbs_mode .............................. True   optimizer ....................................... adam   output_bert_embeddings .......................... False   overlap_grad_reduce ............................. False   overlap_p2p_comm ................................ False   overlap_param_gather ............................ False   override_opt_param_scheduler .................... False   params_dtype .................................... torch.float16   patch_dim ....................................... 16   perform_initialization .......................... True   pipe_experts_multi_data ......................... 1   pipe_experts_multi_stream ....................... False   pipeline_model_parallel_size .................... 1   pipeline_model_parallel_split_rank .............. None   position_embedding_type ......................... learned_absolute   pp_schedule_list ................................ None   pre_tockens ..................................... 65536   pretrained_checkpoint ........................... None   prof_file ....................................... None   profile ......................................... False   profile_level ................................... level0   profile_memory .................................. False   profile_operator ................................ False   profile_ranks ................................... [0]   profile_record_shapes ........................... False   profile_save_path ............................... ./profile_dir   profile_step_end ................................ 12   profile_step_start .............................. 10   profile_with_cpu ................................ False   profile_with_memory ............................. False   profile_with_stack .............................. False   q_lora_rank ..................................... None   qk_layernorm .................................... False   qk_nope_head_dim ................................ None   qk_rope_head_dim ................................ None   query_in_block_prob ............................. 0.1   rampup_batch_size ............................... None   rank ............................................ 0   recompute_activation_function ................... False   recompute_activation_function_num_layers ........ None   recompute_granularity ........................... None   recompute_in_advance ............................ False   recompute_in_bubble ............................. False   recompute_method ................................ None   recompute_module_list ........................... None   recompute_num_layers ............................ None   recompute_type .................................. 2   reduce_recompute_for_last_chunk ................. False   reset_attention_mask ............................ False   reset_position_ids .............................. False   retriever_report_topk_accuracies ................ []   retriever_score_scaling ......................... False   retriever_seq_length ............................ 256   retro_add_retriever ............................. False   retro_attention_gate ............................ 1   retro_cyclic_train_iters ........................ None   retro_encoder_attention_dropout ................. 0.1   retro_encoder_hidden_dropout .................... 0.1   retro_encoder_layers ............................ 2   retro_num_neighbors ............................. 2   retro_num_retrieved_chunks ...................... 2   retro_project_dir ............................... None   retro_verify_neighbor_count ..................... True   reuse_fp32_param ................................ False   rope_scaling_beta_fast .......................... 32   rope_scaling_beta_slow .......................... 1   rope_scaling_factor ............................. 1.0   rope_scaling_mscale ............................. 1.0   rope_scaling_mscale_all_dim ..................... 0.0   rope_scaling_original_max_position_embeddings ... None   rope_scaling_type ............................... None   rotary_base ..................................... None   rotary_interleaved .............................. False   rotary_percent .................................. 1.0   rotary_seq_len_interpolation_factor ............. None   sample_rate ..................................... 1.0   save ............................................ /home/mauser/work/obs/miniyulan/model/checkpoint236000   save_interval ................................... 10000   save_memory_ratio ............................... 0.2   scatter_gather_tensors_in_pipeline .............. True   seed ............................................ 1234   seq_length ...................................... 4096   sequence_parallel ............................... False   sgd_momentum .................................... 0.9   shape_order ..................................... SBH   short_seq_prob .................................. 0.1   skip_bias_add ................................... True   skip_train ...................................... False   sparse_mode ..................................... 0   spec ............................................ None   split ........................................... 949,50,1   square_alibi_mask ............................... False   squared_relu .................................... False   standalone_embedding_stage ...................... False   start_weight_decay .............................. 0.01   straggler_ctrlr_port ............................ 65535   straggler_minmax_count .......................... 1   swap_attention .................................. False   swiglu .......................................... False   swin_backbone_type .............................. tiny   tensor_model_parallel_size ...................... 1   tensorboard_dir ................................. None   tensorboard_log_interval ........................ 1   tensorboard_queue_size .......................... 1000   test_data_path .................................. None   test_mode ....................................... False   timing_log_level ................................ 0   timing_log_option ............................... minmax   titles_data_path ................................ None   tokenizer_model ................................. None   tokenizer_name_or_path .......................... None   tokenizer_not_use_fast .......................... True   tokenizer_type .................................. GPT2BPETokenizer   tp_comm_bulk_dgrad .............................. True   tp_comm_bulk_wgrad .............................. True   tp_comm_overlap ................................. False   tp_comm_overlap_ag .............................. True   tp_comm_overlap_cfg ............................. None   tp_comm_overlap_rs .............................. True   tp_comm_overlap_rs_dgrad ........................ False   tp_comm_split_ag ................................ True   tp_comm_split_rs ................................ True   train_data_path ................................. None   train_iters ..................................... 500000   train_samples ................................... None   transformer_impl ................................ local   transformer_pipeline_model_parallel_size ........ 1   ulysses_degree_in_cp ............................ None   untie_embeddings_and_output_weights ............. False   use_ascend_coc .................................. False   use_ascend_mc2 .................................. False   use_checkpoint_args ............................. False   use_checkpoint_opt_param_scheduler .............. False   use_cp_send_recv_overlap ........................ False   use_cpu_initialization .......................... None   use_dist_ckpt ................................... False   use_distributed_optimizer ....................... True   use_ema ......................................... False   use_flash_attn .................................. False   use_fused_moe_token_permute_and_unpermute ....... False   use_fused_ring_attention_update ................. False   use_fused_rmsnorm ............................... False   use_fused_rotary_pos_emb ........................ False   use_fused_swiglu ................................ False   use_fusion_attn_v2 .............................. False   use_mcore_models ................................ False   use_multiparameter_pipeline_model_parallel ...... False   use_nanopipe .................................... False   use_nanopipe_swap ............................... False   use_nd_matmul ................................... False   use_one_sent_docs ............................... False   use_pipe_experts ................................ False   use_ring_exchange_p2p ........................... False   use_rotary_position_embeddings .................. False   use_rts ......................................... False   use_tp_pp_dp_mapping ............................ False   v_head_dim ...................................... None   valid_data_path ................................. None   variable_seq_lengths ............................ False   virtual_pipeline_model_parallel_size ............ None   vision_backbone_type ............................ vit   vision_pretraining .............................. False   vision_pretraining_type ......................... classify   vocab_extra_ids ................................. 0   vocab_file ...................................... /home/mauser/work/pretrainmini/tokenizer_1731/vocab.json   vocab_size ...................................... None   wandb_exp_name ..................................    wandb_project ...................................    wandb_save_dir ..................................    weight_decay .................................... 0.01   weight_decay_incr_style ......................... constant   world_size ...................................... 2   yaml_cfg ........................................ None  end of arguments  ``` ``` !/bin/bash  Runs the ""345M"" parameter model export CUDA_DEVICE_MAX_CONNECTIONS=1 export PYTORCH_NPU_ALLOC_CONF=expandable_segments:True GPUS_PER_NODE=2  Change for multinode config MASTER_ADDR=localhost MASTER_PORT=6000 NNODES=1 NODE_RANK=0 WORLD_SIZE=$(($GPUS_PER_NODE*$NNODES)) CHECKPOINT_PATH=/home/mauser/work/obs/miniyulan/model/checkpoint236000 VOCAB_FILE=/home/mauser/work/pretrainmini/tokenizer_1731/vocab.json MERGE_FILE=/home/mauser/work/pretrainmini/tokenizer_1731/merges.txt DATA_PATH=/home/mauser/work/obs/miniyulan/mini_data_ids/yulan_try_text_document/yulan_try_text_document DISTRIBUTED_ARGS=""     nproc_per_node $GPUS_PER_NODE \     nnodes $NNODES \     node_rank $NODE_RANK \     master_addr $MASTER_ADDR \     master_port $MASTER_PORT "" GPT_ARGS=""     numlayers 2 \     hiddensize 1920 \     numattentionheads 30 \     seqlength 4096 \     maxpositionembeddings 4096 \     microbatchsize 1 \     globalbatchsize 2 \     lr 0.01 \     trainiters 500000 \     lrdecayiters 320000 \     lrdecaystyle cosine \     minlr 1.0e5 \     weightdecay 1e2 \     lrwarmupfraction .01 \     clipgrad 1.0 \     fp16 usedistributedoptimizer "" DATA_ARGS=""     datapath $DATA_PATH \     vocabfile $VOCAB_FILE \     mergefile $MERGE_FILE \     split 949,50,1 "" OUTPUT_ARGS=""     loginterval 100 \     saveinterval 10000 \     evalinterval 1000 \     evaliters 10 "" torchrun $DISTRIBUTED_ARGS pretrain_gpt.py \     $GPT_ARGS \     $DATA_ARGS \     $OUTPUT_ARGS \     distributedbackend nccl \     save $CHECKPOINT_PATH \  load $CHECKPOINT_PATH ``` 四、日志信息: xxxx 请根据自己的运行环境参考以下方式搜集日志信息，如果涉及到算子开发相关的问题，建议也提供UT/ST测试和单算子集成测试相关的日志。 日志提供方式: 将日志打包后作为附件上传。若日志大小超出附件限制，则可上传至外部网盘后提供链接。 获取方法请参考wiki： https://gitee.com/ascend/modelzoo/wikis/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E6%97%A5%E5%BF%97%E5%92%8C%E8%AE%A1%E7%AE%97%E5%9B%BE?sort_id=4097825",2024-10-29T20:56:51+08:00,,closed,0,2,https://gitee.com/ascend/MindSpeed/issues/IB0NOC,镜像是pytorch_2.1.0cann_8.0.rc1py_3.9euler_2.10.7aarch64snt9b，用Ascendcannnnal_8.0.RC2.2_linuxaarch64.run安装的NNAL,解决问题了，CANN的版本对不上
邓佳,[BUG] Learning rate not overrided when set overrideopt_paramscheduler,"原MegatronLM Bug， override_opt_param_scheduler 配置不生效 https://github.com/NVIDIA/MegatronLM/issues/1138 Describe the bug When setting overrideopt_paramscheduler (but still load optimizer and load rng) and setting new learning rate scheduler (including max lr, min lr, decay style, etc.), the learning rate still persists its original scheduler. A related issue could be 963 To Reproduce Set max_lr as 6e4 and constant learning rate. Training some steps and save checkpoint. Load the checkpoint (including optimizer params) and override the scheduler (e.g. cosine 6e4 to 6e5). Then the bug shows: the learning rate is still constantly 6e4. Expected behavior The learning rate scheduler should be overrided. Environment (please complete the following information): MegatronLM commit ID [9bcd417] Proposed fix The following code (https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/optimizer_param_scheduler.pyL121) leads to reloading the optimizer's learning rate when setting overrideopt_paramscheduler def get_lr(self, param_group: dict) > float:         """"""Learning rate decay functions from:         https://openreview.net/pdf?id=BJYwwY9ll pg. 4         Args:             param_group (dict): parameter group from the optimizer.         """"""         max_lr = param_group.get('max_lr', self.max_lr)         min_lr = param_group.get('min_lr', self.min_lr) A possible solution could be def get_lr(self, param_group: dict) > float:         """"""Learning rate decay functions from:         https://openreview.net/pdf?id=BJYwwY9ll pg. 4         Args:             param_group (dict): parameter group from the optimizer.         """"""         max_lr = self.max_lr         min_lr = self.min_lr",2024-10-15T21:09:46+08:00,,open,0,0,https://gitee.com/ascend/MindSpeed/issues/IAXCHJ
Zhao mb,"是否只支持MegatronLM，其他训练框架是否可以用mindspeed,就如deepspeed","是否只支持MegatronLM，其他训练框架是否可以用mindspeed,就如deepspeed",2024-10-14T15:58:45+08:00,,closed,0,1,https://gitee.com/ascend/MindSpeed/issues/IAWZAK,deepspeed目前已经原生支持
LWC,llama factory是否可以使用mindspeed进行训练，可以给出一个操作实例吗？,,2024-10-11T15:33:33+08:00,,closed,0,1,https://gitee.com/ascend/MindSpeed/issues/IAWBA3,暂不支持。llamafactory可使用deepspeed，已原生支持昇腾
zhenyuzhang_83dd,如何使用MindSpeed Profiling工具,问题现象：  描述（请详细描述你的需求或遇到的问题） 我想用MindSpeed Profiling工具对训练的程序进行调试和追踪，但是不知道在哪里插入doc中提到的flag。 我的使用案例是示例里的MegatronLM训练GPT2345M，按照doc要求修改了部分文件，在训练脚本中加入了 `profile`，但发现只打开了MegatronLM原生的profiling（只支持CUDA），请问是在哪里修改呢。,2024-10-04T20:21:26+08:00,,closed,0,2,https://gitee.com/ascend/MindSpeed/issues/IAUYKW,跑的mindspeed代码版本是新的吗？ 在训练脚本中加入`profile`可以打开mindspeed profiling功能，不会打开MegatronLM原生的profiling，如果有报错可以发下。,基于mindspeed网页上Profiling采集命令介绍，把采集命令加在训练脚本中就可以了
MrCangyan,该适配的MegatronLM 是否支持和华为以外的硬件进行训练？,,2024-09-29T14:30:14+08:00,,closed,0,1,https://gitee.com/ascend/MindSpeed/issues/IAUHQ4,暂不支持
nullnull,删除Sharp相关代码,NPU不支持Sharp，建议删除相关代码。 https://gitee.com/ascend/MindSpeed/blob/master/mindspeed/core/parallel_state.pyL679,2024-09-20T10:58:34+08:00,,closed,0,1,https://gitee.com/ascend/MindSpeed/issues/IASE7G,尽量保持与megatron原生代码一致，不进行删除
nullnull,MOE模型shared experts场景的模型初始化问题,一、问题现象（附报错日志上下文）： shared_experts的ffn_hidden_size有bug： https://gitee.com/ascend/MindSpeed/tree/6bf8049ef05126a5c42f5e202a558ab168bffc62/mindspeed/core/transformer/moe/moe_layer.pyL19 每个moe layer的__init__都会更新`self.config.ffn_hidden_size`，导致这个值越来越大。,2024-09-13T14:01:45+08:00,,closed,0,1,https://gitee.com/ascend/MindSpeed/issues/IAR3VS,已修复，对应PR: 【master】: https://gitee.com/ascend/MindSpeed/pulls/814 【core_r0.6.0】: https://gitee.com/ascend/MindSpeed/pulls/815
jxl,dot_product_attention_forward_wrapper()函数入参冗余,函数位置：MindSpeed/mindspeed/core/transformer/dot_product_attention.py dot_product_attention_forward_wrapper()函数的入参 attention_mask 会被函数中生成的 attention_mask 覆盖，相当于冗余参数，需要优化。 !输入图片说明,2024-09-03T11:45:55+08:00,,closed,0,1,https://gitee.com/ascend/MindSpeed/issues/IAOEF8,已经和 wangzhiwei 对齐，待修改。
fward,npu_gmm_v2 代码错误,在 PyBind11 代码中未定义 npu_gmm_v2 的情况下，在单元测试中使用了这个函数。,2024-09-02T13:54:43+08:00,,closed,0,2,https://gitee.com/ascend/MindSpeed/issues/IAO3VR,麻烦描述详细一些，具体问题是？,> 麻烦描述详细一些，具体问题是？ 抱歉，后来发现是CANN版本不匹配。这里忘记关闭issue了。
JayNine,单卡训练正常，但8卡并行预训练时，报错collective operation timeout," 使用镜像：pytorch_2.1.0cann_8.0.rc1py_3.9euler_2.10.7aarch64snt9b  Megatron版本：core_r0.7.0  配置：8*910B4  并行参数：GPUS_PER_NODE=8，NUM_NODES=1，tensormodelparallelsize 1，pipelinemodelparallelsize 1，sequenceparallel  主要报错如下 ``` [E ProcessGroupHCCL.cpp:414] [Rank 7] Watchdog caught collective operation timeout: WorkHCCL(SeqNum=1, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 600445 milliseconds before timing out. [E ProcessGroupHCCL.cpp:414] [Rank 5] Watchdog caught collective operation timeout: WorkHCCL(SeqNum=1, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 600649 milliseconds before timing out. [E ProcessGroupHCCL.cpp:414] [Rank 3] Watchdog caught collective operation timeout: WorkHCCL(SeqNum=1, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 600650 milliseconds before timing out. [E ProcessGroupHCCL.cpp:414] [Rank 6] Watchdog caught collective operation timeout: WorkHCCL(SeqNum=1, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 600653 milliseconds before timing out. [E ProcessGroupHCCL.cpp:414] [Rank 1] Watchdog caught collective operation timeout: WorkHCCL(SeqNum=1, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 600663 milliseconds before timing out. [E ProcessGroupHCCL.cpp:414] [Rank 4] Watchdog caught collective operation timeout: WorkHCCL(SeqNum=1, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 600741 milliseconds before timing out. [E ProcessGroupHCCL.cpp:414] [Rank 0] Watchdog caught collective operation timeout: WorkHCCL(SeqNum=1, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 600840 milliseconds before timing out. [E ProcessGroupHCCL.cpp:434] Some HCCL operations have failed or timed out. Due to the asynchronous nature of ASCEND kernels, subsequent NPU operations might run on corrupted/incomplete data. [E ProcessGroupHCCL.cpp:440] To avoid data inconsistency, we are taking the entire process down. [E ProcessGroupHCCL.cpp:725] [Rank 7] HCCL watchdog thread terminated with exception: [Rank 7] Watchdog caught collective operation timeout: WorkHCCL(SeqNum=1, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 600445 milliseconds before timing out. terminate called after throwing an instance of 'std::runtime_error'   what():  [Rank 7] HCCL watchdog thread terminated with exception: [Rank 7] Watchdog caught collective operation timeout: WorkHCCL(SeqNum=1, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 600445 milliseconds before timing out. [E ProcessGroupHCCL.cpp:434] Some HCCL operations have failed or timed out. Due to the asynchronous nature of ASCEND kernels, subsequent NPU operations might run on corrupted/incomplete data. [E ProcessGroupHCCL.cpp:440] To avoid data inconsistency, we are taking the entire process down. [E ProcessGroupHCCL.cpp:725] [Rank 1] HCCL watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkHCCL(SeqNum=1, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 600663 milliseconds before timing out. terminate called after throwing an instance of 'std::runtime_error'   what():  [Rank 1] HCCL watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkHCCL(SeqNum=1, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 600663 milliseconds before timing out. [E ProcessGroupHCCL.cpp:434] Some HCCL operations have failed or timed out. Due to the asynchronous nature of ASCEND kernels, subsequent NPU operations might run on corrupted/incomplete data. [E ProcessGroupHCCL.cpp:440] To avoid data inconsistency, we are taking the entire process down. [E ProcessGroupHCCL.cpp:434] Some HCCL operations have failed or timed out. Due to the asynchronous nature of ASCEND kernels, subsequent NPU operations might run on corrupted/incomplete data. [E ProcessGroupHCCL.cpp:440] To avoid data inconsistency, we are taking the entire process down. [E ProcessGroupHCCL.cpp:725] [Rank 5] HCCL watchdog thread terminated with exception: [Rank 5] Watchdog caught collective operation timeout: WorkHCCL(SeqNum=1, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 600649 milliseconds before timing out. terminate called after throwing an instance of 'std::runtime_error'   what():  [Rank 5] HCCL watchdog thread terminated with exception: [Rank 5] Watchdog caught collective operation timeout: WorkHCCL(SeqNum=1, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 600649 milliseconds before timing out. [E ProcessGroupHCCL.cpp:725] [Rank 4] HCCL watchdog thread terminated with exception: [Rank 4] Watchdog caught collective operation timeout: WorkHCCL(SeqNum=1, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 600741 milliseconds before timing out. terminate called after throwing an instance of 'std::runtime_error'   what():  [Rank 4] HCCL watchdog thread terminated with exception: [Rank 4] Watchdog caught collective operation timeout: WorkHCCL(SeqNum=1, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 600741 milliseconds before timing out. [E ProcessGroupHCCL.cpp:434] Some HCCL operations have failed or timed out. Due to the asynchronous nature of ASCEND kernels, subsequent NPU operations might run on corrupted/incomplete data. [E ProcessGroupHCCL.cpp:440] To avoid data inconsistency, we are taking the entire process down. [E ProcessGroupHCCL.cpp:725] [Rank 3] HCCL watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkHCCL(SeqNum=1, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 600650 milliseconds before timing out. [E ProcessGroupHCCL.cpp:434] Some HCCL operations have failed or timed out. Due to the asynchronous nature of ASCEND kernels, subsequent NPU operations might run on corrupted/incomplete data. terminate called after throwing an instance of 'std::runtime_error[E ProcessGroupHCCL.cpp:440] To avoid data inconsistency, we are taking the entire process down. '   what():  [Rank 3] HCCL watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkHCCL(SeqNum=1, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 600650 milliseconds before timing out. [E ProcessGroupHCCL.cpp:725] [Rank 6] HCCL watchdog thread terminated with exception: [Rank 6] Watchdog caught collective operation timeout: WorkHCCL(SeqNum=1, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 600653 milliseconds before timing out. terminate called after throwing an instance of 'std::runtime_error'   what():  [Rank 6] HCCL watchdog thread terminated with exception: [Rank 6] Watchdog caught collective operation timeout: WorkHCCL(SeqNum=1, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 600653 milliseconds before timing out. [E ProcessGroupHCCL.cpp:434] Some HCCL operations have failed or timed out. Due to the asynchronous nature of ASCEND kernels, subsequent NPU operations might run on corrupted/incomplete data. [E ProcessGroupHCCL.cpp:440] To avoid data inconsistency, we are taking the entire process down. [E ProcessGroupHCCL.cpp:725] [Rank 0] HCCL watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkHCCL(SeqNum=1, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 600840 milliseconds before timing out. terminate called after throwing an instance of 'std::runtime_error'   what():  [Rank 0] HCCL watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkHCCL(SeqNum=1, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 600840 milliseconds before timing out. [E ProcessGroupHCCL.cpp:414] [Rank 2] Watchdog caught collective operation timeout: WorkHCCL(SeqNum=1, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 600921 milliseconds before timing out. [E ProcessGroupHCCL.cpp:434] Some HCCL operations have failed or timed out. Due to the asynchronous nature of ASCEND kernels, subsequent NPU operations might run on corrupted/incomplete data. [E ProcessGroupHCCL.cpp:440] To avoid data inconsistency, we are taking the entire process down. [E ProcessGroupHCCL.cpp:725] [Rank 2] HCCL watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkHCCL(SeqNum=1, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 600921 milliseconds before timing out. terminate called after throwing an instance of 'std::runtime_error'   what():  [Rank 2] HCCL watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkHCCL(SeqNum=1, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 600921 milliseconds before timing out. [20240825 11:43:55,563] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 6) local_rank: 0 (pid: 106588) of binary: /home/mauser/anaconda3/envs/PyTorch2.1.0/bin/python3.9 Traceback (most recent call last):   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/bin/torchrun"", line 8, in      sys.exit(main())   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper     return f(*args, **kwargs)   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/distributed/run.py"", line 806, in main     run(args)   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/distributed/run.py"", line 797, in run     elastic_launch(   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/distributed/launcher/api.py"", line 134, in __call__     return launch_agent(self._config, self._entrypoint, list(args))   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/distributed/launcher/api.py"", line 264, in launch_agent     raise ChildFailedError( torch.distributed.elastic.multiprocessing.errors.ChildFailedError:  ```",2024-08-25T13:44:13+08:00,,closed,0,3,https://gitee.com/ascend/MindSpeed/issues/IAMAAZ,npusmi info  检查下超时期间，多卡是不是都在正常运营，有没有卡被占用的情况，导致某张卡上的计算一直没完成,仅仅执行了一个指令，但是确实NPU0有多个进程： ``` ++++  +===========================+===============+====================================================+ ```,请问问题是否解决，如未解决麻烦提供对应plog日志
JayNine,预训练GPT模型时，无法启用RoPE旋转位置嵌入Rotary Position Embedding。," 使用镜像：pytorch_2.1.0cann_8.0.rc1py_3.9euler_2.10.7aarch64snt9b  Megatron版本：core_r0.7.0   apex版本：0.1ascend20240413 **设置`positionembeddingtype rope`，不设置`usefusedrotaryposemb`时的报错如下**  ``` [before the start of training step] datetime: 20240824 20:54:45  WARNING:megatron.core.models.common.embeddings.rotary_pos_embedding:Setting apply_rope_fusion to false because its implementation is not included in Apex. Try upgrading to the latest version ```  **若同时设置设置`positionembeddingtype rope`和`usefusedrotaryposemb`，报错如下：**  ``` [before the start of training step] datetime: 20240824 22:53:30  WARNING:megatron.core.models.common.embeddings.rotary_pos_embedding:Setting apply_rope_fusion to false because its implementation is not included in Apex. Try upgrading to the latest version Using /home/mauser/.cache/torch_extensions/py39_cpu as PyTorch extensions root... Emitting ninja build file /home/mauser/.cache/torch_extensions/py39_cpu/npu_rotary_position_embedding/build.ninja... Building extension module npu_rotary_position_embedding... Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N) ninja: no work to do. Loading extension module npu_rotary_position_embedding... Traceback (most recent call last):   File ""/home/mauser/work/MegatronLM/pretrain_gpt.py"", line 178, in forward_step     output_tensor = model(tokens, position_ids, attention_mask,   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/mauser/work/MegatronLM/megatron/core/distributed/distributed_data_parallel.py"", line 180, in forward     return self.module(*inputs, **kwargs)   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/mauser/work/MegatronLM/megatron/legacy/model/module.py"", line 190, in forward     outputs = self.module(*inputs, **kwargs)   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/mauser/work/MegatronLM/megatron/legacy/model/gpt_model.py"", line 82, in forward     lm_output = self.language_model(   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/mauser/work/MegatronLM/megatron/legacy/model/language_model.py"", line 494, in forward     encoder_output = self.encoder(   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/mauser/work/MindSpeed/mindspeed/model/transformer.py"", line 331, in wrapper     return fn(self, hidden_states, attention_mask, **kwargs)   File ""/home/mauser/work/MegatronLM/megatron/legacy/model/transformer.py"", line 1777, in forward     hidden_states = layer(   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/mauser/work/MindSpeed/mindspeed/core/transformer/transformer.py"", line 35, in row_parallel_forward     output = forward_func(*args, **kwargs)   File ""/home/mauser/work/MegatronLM/megatron/legacy/model/transformer.py"", line 1164, in forward     self.self_attention(   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/mauser/work/MindSpeed/mindspeed/model/transformer.py"", line 825, in parallel_attention_forward     query_layer = apply_rotary_pos_emb(query_layer, q_pos_emb, self.config)   File ""/home/mauser/work/MegatronLM/megatron/core/models/common/embeddings/rotary_pos_embedding.py"", line 247, in apply_rotary_pos_emb     return apply_rotary_pos_emb_bshd(t, freqs, rotary_interleaved=config.rotary_interleaved)   File ""/home/mauser/work/MindSpeed/mindspeed/core/fusions/rotary_pos_embedding.py"", line 18, in wrapper     t = npu_rotary_position_embedding(t.contiguous(), cos_, sin_).to(t.dtype)   File ""/home/mauser/work/MindSpeed/mindspeed/ops/npu_rotary_position_embedding.py"", line 8, in npu_rotary_position_embedding     return rope_ops.npu_rotary_position_embedding(x, cos, sin, mode) RuntimeError: aclnnRotaryPositionEmbedding or aclnnRotaryPositionEmbeddingGetWorkspaceSize not in libopapi.so, or libopapi.sonot found. ```",2024-08-24T22:54:23+08:00,,open,0,5,https://gitee.com/ascend/MindSpeed/issues/IAM9DQ,还没解决,是不是因该调用这个算子 aclnnApplyRotaryPosEmb https://www.hiascend.com/document/detail/zh/canncommercial/80RC1/apiref/appdevgapi/context/aclnnApplyRotaryPosEmb.md,报错为版本内无该融合算子，请升级镜像使用cann 8.0.rc2或更高版本。若仍有问题请进一步反馈,官方终于更新了8.0.rc2的镜像，但是没有更高版本了。实测8.0.rc2仍然无法使用rope，不会报算子的错了，但是仍然有第一个报错： `[before the start of training step] datetime: 20241207 19:43:14 WARNING:megatron.core.models.common.embeddings.rotary_pos_embedding:Setting apply_rope_fusion to false because its implementation is not included in Apex. Try upgrading to the latest version`,该warning为megatron打印，实际上已经启用了融合算子，可进一步通过profiling确认
JayNine,gpt2345M模型推理测试时，注意力Mask维度经常出问题," 使用镜像：pytorch_2.1.0cann_8.0.rc1py_3.9euler_2.10.7aarch64snt9b  Megatron版本：core_r0.7.0  出现错误的脚本（wikitext103困惑度测试：https://github.com/NVIDIA/MegatronLM/tree/core_r0.7.0?tab=readmeovfilewikitextperplexityevaluation） ``` export NUMEXPR_MAX_THREADS=192 export CUDA_DEVICE_MAX_CONNECTIONS=1 export MASTER_ADDR=localhost export MASTER_PORT=12345 TASK=""WIKITEXT103"" VALID_DATA=/home/mauser/work/MegatronLM/work/wikitext103validation.txt CHECKPOINT_PATH=/home/mauser/work/MegatronLM/work/checkpoint/gpt2_345m VOCAB_FILE=/home/mauser/work/MegatronLM/work/gpt2vocab.json MERGE_FILE=/home/mauser/work/MegatronLM/work/gpt2merges.txt COMMON_TASK_ARGS=""numlayers 24 \                   hiddensize 1024 \                   numattentionheads 16 \                   seqlength 1024 \                   maxpositionembeddings 1024 \                   fp16 \                   vocabfile $VOCAB_FILE"" python tasks/main.py \        task $TASK \        $COMMON_TASK_ARGS \        validdata $VALID_DATA \        tokenizertype GPT2BPETokenizer \        mergefile $MERGE_FILE \        load $CHECKPOINT_PATH \        microbatchsize 8 \        loginterval 10 \        noloadoptim \        noloadrng ```  报错log： ``` (PyTorch2.1.0) [mauser MegatronLM]$./examples/test/wiki_gpt2.sh /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/utils/path_manager.py:79: UserWarning: Warning: The /usr/local/Ascend/ascendtoolkit/latest owner does not match the current user.   warnings.warn(f""Warning: The {path} owner does not match the current user."") /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/utils/path_manager.py:79: UserWarning: Warning: The /usr/local/Ascend/ascendtoolkit/8.0.RC1/aarch64linux/ascend_toolkit_install.info owner does not match the current user.   warnings.warn(f""Warning: The {path} owner does not match the current user."") /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/contrib/transfer_to_npu.py:211: ImportWarning:      *************************************************************************************************************     The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..     The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..     The backend in torch.distributed.init_process_group set to hccl now..     The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..     The device parameters have been replaced with npu in the function below:     torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.autocast, torch.load, torch.Generator, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.nn.Module.to, torch.nn.Module.to_empty     *************************************************************************************************************   warnings.warn(msg, ImportWarning) /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.   warnings.warn(msg, RuntimeWarning) /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.   warnings.warn(msg, RuntimeWarning) /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/gpytorch/lazy/__init__.py:70: DeprecationWarning: GPyTorch will be replacing all LazyTensor functionality with the linear operator package. Replace all references to gpytorch.lazy.*LazyTensor with linear_operator.operators.*LinearOperator.   warnings.warn( using world size: 1, dataparallel size: 1, contextparallel size: 1 tensormodelparallel size: 1, pipelinemodelparallel size: 1  setting global batch size to 8 WARNING: Setting args.overlap_p2p_comm to False since noninterleaved schedule does not support overlapping p2p communication WARNING: Setting args.check_for_nan_in_loss_and_grad to False since dynamic loss scaling is being used using torch.float16 for parameters ...  arguments    accumulate_allreduce_grads_in_fp32 .............. False   adam_beta1 ...................................... 0.9   adam_beta2 ...................................... 0.999   adam_eps ........................................ 1e08   adaptive_recompute_device_size .................. 1   adaptive_recompute_device_swap .................. False   adaptive_recompute_profiling_step ............... 10   add_bias_linear ................................. True   add_dense_bias .................................. False   add_position_embedding .......................... True   add_qkv_bias .................................... False   additional_config ............................... None   adlr_autoresume ................................. False   adlr_autoresume_interval ........................ 1000   alibi_diagonal_opposite ......................... False   alibi_fusion_attn_type .......................... None   apply_layernorm_1p .............................. False   apply_query_key_layer_scaling ................... False   apply_residual_connection_post_layernorm ........ False   apply_rope_fusion ............................... True   async_save ...................................... None   async_tensor_model_parallel_allreduce ........... True   attention_dropout ............................... 0.1   attention_softmax_in_fp32 ....................... False   auto_detect_ckpt_format ......................... False   auto_parallel ................................... False   automated_pipeline .............................. False   automated_pipeline_perf ......................... False   barrier_with_L1_time ............................ True   bert_binary_head ................................ True   bert_embedder_type .............................. megatron   bert_load ....................................... None   bf16 ............................................ False   bias_dropout_fusion ............................. True   bias_gelu_fusion ................................ True   bias_swiglu_fusion .............................. True   biencoder_projection_dim ........................ 0   biencoder_shared_query_context_model ............ False   block_data_path ................................. None   calculate_per_token_loss ........................ False   check_for_nan_in_loss_and_grad .................. False   check_weight_hash_across_dp_replicas_interval ... None   ckpt_assume_constant_structure .................. False   ckpt_fully_parallel_load ........................ False   ckpt_fully_parallel_save ........................ False   ckpt_step ....................................... None   classes_fraction ................................ 1.0   clip_grad ....................................... 1.0   clone_scatter_output_in_embedding ............... True   coc_fused_kernel ................................ False   coc_mode ........................................ 1   coc_parallel_num ................................ 1   consumed_train_samples .......................... 0   consumed_valid_samples .......................... 0   context_parallel_algo ........................... ulysses_cp_algo   context_parallel_size ........................... 1   cp_attention_mask_type .......................... causal   create_attention_mask_in_dataloader ............. False   data_cache_path ................................. None   data_parallel_random_init ....................... False   data_parallel_size .............................. 1   data_path ....................................... None   data_per_class_fraction ......................... 1.0   data_sharding ................................... True   dataloader_type ................................. single   ddp_bucket_size ................................. None   decoder_num_layers .............................. None   decoder_seq_length .............................. None   decoupled_lr .................................... None   decoupled_min_lr ................................ None   delay_grad_reduce ............................... True   delay_param_gather .............................. False   dino_bottleneck_size ............................ 256   dino_freeze_last_layer .......................... 1   dino_head_hidden_size ........................... 2048   dino_local_crops_number ......................... 10   dino_local_img_size ............................. 96   dino_norm_last_layer ............................ False   dino_teacher_temp ............................... 0.07   dino_warmup_teacher_temp ........................ 0.04   dino_warmup_teacher_temp_epochs ................. 30   disable_straggler_on_startup .................... False   dist_ckpt_format ................................ torch_dist   distribute_saved_activations .................... False   distributed_backend ............................. nccl   distributed_timeout_minutes ..................... 10   embedding_path .................................. None   empty_unused_memory_level ....................... 0   enable_one_logger ............................... False   enable_recompute_layers_per_pp_rank ............. False   enable_token_rearrange_opt ...................... False   enable_zero3 .................................... False   encoder_num_layers .............................. 24   encoder_seq_length .............................. 1024   end_weight_decay ................................ 0.01   eod_mask_loss ................................... False   epochs .......................................... None   eval_interval ................................... 1000   eval_iters ...................................... 100   eval_micro_batch_size ........................... None   evidence_data_path .............................. None   exit_duration_in_mins ........................... None   exit_interval ................................... None   exit_on_missing_checkpoint ...................... False   exit_signal_handler ............................. False   expert_interval ................................. 1   expert_model_parallel_size ...................... 1   faiss_match ..................................... string   faiss_topk_retrievals ........................... 100   faiss_use_gpu ................................... False   ffn_hidden_size ................................. 4096   fill_neg_inf .................................... False   finetune ........................................ False   fp16 ............................................ True   fp16_lm_cross_entropy ........................... False   fp32_residual_connection ........................ False   fp8 ............................................. None   fp8_amax_compute_algo ........................... most_recent   fp8_amax_history_len ............................ 1   fp8_interval .................................... 1   fp8_margin ...................................... 0   fp8_wgrad ....................................... True   global_batch_size ............................... 8   gradient_accumulation_fusion .................... False   group_query_attention ........................... False   head_lr_mult .................................... 1.0   hidden_dropout .................................. 0.1   hidden_size ..................................... 1024   hysteresis ...................................... 2   ict_head_size ................................... None   ict_load ........................................ None   img_h ........................................... 224   img_w ........................................... 224   indexer_batch_size .............................. 128   indexer_log_interval ............................ 1000   inference_batch_times_seqlen_threshold .......... 512   init_method_std ................................. 0.02   init_method_xavier_uniform ...................... False   initial_loss_scale .............................. 4294967296   iter_per_epoch .................................. 1250   jit_compile ..................................... False   keep_last ....................................... False   kv_channels ..................................... 64   lazy_mpu_init ................................... None   load ............................................ /home/mauser/work/MegatronLM/work/checkpoint/gpt2_345m   local_rank ...................................... None   log_batch_size_to_tensorboard ................... False   log_interval .................................... 10   log_learning_rate_to_tensorboard ................ True   log_loss_scale_to_tensorboard ................... True   log_memory_to_tensorboard ....................... False   log_num_zeros_in_grad ........................... False   log_params_norm ................................. False   log_progress .................................... False   log_straggler ................................... False   log_throughput .................................. False   log_timers_to_tensorboard ....................... False   log_validation_ppl_to_tensorboard ............... False   log_world_size_to_tensorboard ................... False   loss_scale ...................................... None   loss_scale_window ............................... 1000   lr .............................................. None   lr_decay_iters .................................. None   lr_decay_samples ................................ None   lr_decay_style .................................. linear   lr_warmup_fraction .............................. None   lr_warmup_init .................................. 0.0   lr_warmup_iters ................................. 0   lr_warmup_samples ............................... 0   make_vocab_size_divisible_by .................... 128   manual_gc ....................................... False   manual_gc_eval .................................. True   manual_gc_interval .............................. 0   mask_factor ..................................... 1.0   mask_prob ....................................... 0.15   mask_type ....................................... random   masked_softmax_fusion ........................... True   master_addr ..................................... None   master_port ..................................... None   max_position_embeddings ......................... 1024   max_tokens_to_oom ............................... 12000   mbs_idx ......................................... None   memory_fragmentation ............................ False   merge_file ...................................... /home/mauser/work/MegatronLM/work/gpt2merges.txt   micro_batch_size ................................ 8   min_loss_scale .................................. 1.0   min_lr .......................................... 0.0   mmap_bin_files .................................. True   mock_data ....................................... False   moe_adaptive_recompute_activation ............... False   moe_adaptive_recompute_activation_scale ......... 2.0   moe_aux_loss_coeff .............................. 0.0   moe_dynamic_padding ............................. False   moe_expert_capacity_factor ...................... None   moe_extended_tp ................................. False   moe_grouped_gemm ................................ False   moe_input_jitter_eps ............................ None   moe_layer_recompute ............................. False   moe_model_type .................................. megatron_moe   moe_no_drop ..................................... False   moe_pad_expert_input_to_capacity ................ False   moe_per_layer_logging ........................... False   moe_permutation_async_comm ...................... False   moe_router_load_balancing_type .................. aux_loss   moe_router_topk ................................. 2   moe_token_dispatcher_type ....................... allgather   moe_token_drop_policy ........................... probs   moe_train_capacity_factor ....................... 1.0   moe_use_sinkhorn ................................ False   moe_z_loss_coeff ................................ None   nccl_communicator_config_path ................... None   nd1_dim1_size ................................... 1   nd2_dim1_size ................................... 1   next_tockens .................................... 0   nnodes .......................................... 1   no_load_optim ................................... True   no_load_rng ..................................... True   no_persist_layer_norm ........................... False   no_save_optim ................................... None   no_save_rng ..................................... None   node_rank ....................................... 0   noisy_gate_policy ............................... None   norm_epsilon .................................... 1e05   normalization ................................... LayerNorm   nproc_per_node .................................. 8   num_attention_heads ............................. 16   num_channels .................................... 3   num_classes ..................................... 1000   num_dataset_builder_threads ..................... 1   num_experts ..................................... None   num_layer_list .................................. None   num_layers ...................................... 24   num_layers_per_virtual_pipeline_stage ........... None   num_query_groups ................................ 1   num_workers ..................................... 2   one_logger_entity ............................... hwinf_dcm   one_logger_project .............................. e2etracking   one_logger_run_name ............................. None   onnx_safe ....................................... None   openai_gelu ..................................... False   optimize_recomp_communication_level ............. 0   optimize_recomp_communication_status ............ 0   optimize_send_recv_comm ......................... False   optimized_mbs_list .............................. None   optimized_mbs_mode .............................. True   optimizer ....................................... adam   output_bert_embeddings .......................... False   overlap_grad_reduce ............................. False   overlap_p2p_comm ................................ False   overlap_param_gather ............................ False   overlapping_eval ................................ 32   override_opt_param_scheduler .................... False   params_dtype .................................... torch.float16   patch_dim ....................................... 16   perform_initialization .......................... True   pipe_experts_multi_data ......................... 1   pipe_experts_multi_stream ....................... False   pipeline_model_parallel_size .................... 1   pipeline_model_parallel_split_rank .............. None   position_embedding_type ......................... learned_absolute   pp_schedule_list ................................ None   pre_tockens ..................................... 65536   pretrained_checkpoint ........................... None   prof_file ....................................... None   profile ......................................... False   profile_level ................................... level0   profile_memory .................................. False   profile_operator ................................ False   profile_ranks ................................... [0]   profile_record_shapes ........................... False   profile_save_path ............................... ./profile_dir   profile_step_end ................................ 12   profile_step_start .............................. 10   profile_with_cpu ................................ False   profile_with_memory ............................. False   profile_with_stack .............................. False   qa_data_dev ..................................... None   qa_data_test .................................... None   qk_layernorm .................................... False   query_in_block_prob ............................. 0.1   rampup_batch_size ............................... None   rank ............................................ 0   recompute_activation_function ................... False   recompute_activation_function_num_layers ........ None   recompute_granularity ........................... None   recompute_in_advance ............................ False   recompute_in_bubble ............................. False   recompute_method ................................ None   recompute_module_list ........................... None   recompute_num_layers ............................ None   recompute_type .................................. 2   reduce_recompute_for_last_chunk ................. False   reset_attention_mask ............................ False   reset_position_ids .............................. False   retriever_report_topk_accuracies ................ []   retriever_score_scaling ......................... False   retriever_seq_length ............................ 256   retro_add_retriever ............................. False   retro_attention_gate ............................ 1   retro_cyclic_train_iters ........................ None   retro_encoder_attention_dropout ................. 0.1   retro_encoder_hidden_dropout .................... 0.1   retro_encoder_layers ............................ 2   retro_num_neighbors ............................. 2   retro_num_retrieved_chunks ...................... 2   retro_project_dir ............................... None   retro_verify_neighbor_count ..................... True   reuse_fp32_param ................................ False   rotary_base ..................................... None   rotary_interleaved .............................. False   rotary_percent .................................. 1.0   rotary_seq_len_interpolation_factor ............. None   sample_rate ..................................... 1.0   save ............................................ None   save_interval ................................... None   save_memory_ratio ............................... 0.2   scatter_gather_tensors_in_pipeline .............. True   seed ............................................ 1234   seq_length ...................................... 1024   sequence_parallel ............................... False   sgd_momentum .................................... 0.9   shape_order ..................................... SBH   short_seq_prob .................................. 0.1   skip_bias_add ................................... True   skip_train ...................................... False   sparse_mode ..................................... 0   spec ............................................ None   split ........................................... 969, 30, 1   square_alibi_mask ............................... False   squared_relu .................................... False   standalone_embedding_stage ...................... False   start_weight_decay .............................. 0.01   straggler_ctrlr_port ............................ 65535   straggler_minmax_count .......................... 1   strict_lambada .................................. False   swap_attention .................................. False   swiglu .......................................... False   swin_backbone_type .............................. tiny   task ............................................ WIKITEXT103   tensor_model_parallel_size ...................... 1   tensorboard_dir ................................. None   tensorboard_log_interval ........................ 1   tensorboard_queue_size .......................... 1000   test_data_path .................................. None   test_mode ....................................... False   timing_log_level ................................ 0   timing_log_option ............................... minmax   titles_data_path ................................ None   tokenizer_model ................................. None   tokenizer_name_or_path .......................... None   tokenizer_not_use_fast .......................... True   tokenizer_type .................................. GPT2BPETokenizer   tp_comm_bulk_dgrad .............................. True   tp_comm_bulk_wgrad .............................. True   tp_comm_overlap ................................. False   tp_comm_overlap_ag .............................. True   tp_comm_overlap_cfg ............................. None   tp_comm_overlap_rs .............................. True   tp_comm_overlap_rs_dgrad ........................ False   tp_comm_split_ag ................................ True   tp_comm_split_rs ................................ True   train_data ...................................... None   train_data_path ................................. None   train_hard_neg .................................. 0   train_iters ..................................... None   train_samples ................................... None   train_with_neg .................................. False   transformer_impl ................................ local   transformer_pipeline_model_parallel_size ........ 1   ulysses_degree_in_cp ............................ None   untie_embeddings_and_output_weights ............. False   use_ascend_coc .................................. False   use_ascend_mc2 .................................. False   use_checkpoint_args ............................. False   use_checkpoint_opt_param_scheduler .............. False   use_cp_send_recv_overlap ........................ False   use_cpu_initialization .......................... None   use_dist_ckpt ................................... False   use_distributed_optimizer ....................... False   use_ema ......................................... False   use_flash_attn .................................. False   use_fused_rmsnorm ............................... False   use_fused_rotary_pos_emb ........................ False   use_fused_swiglu ................................ False   use_fusion_attn_v2 .............................. False   use_mcore_models ................................ False   use_multiparameter_pipeline_model_parallel ...... False   use_nanopipe .................................... False   use_nd_matmul ................................... False   use_one_sent_docs ............................... False   use_pipe_experts ................................ False   use_ring_exchange_p2p ........................... False   use_rotary_position_embeddings .................. False   use_rts ......................................... False   use_tp_pp_dp_mapping ............................ False   val_av_rank_hard_neg ............................ 30   val_av_rank_other_neg ........................... 30   valid_data ...................................... ['/home/mauser/work/MegatronLM/work/wikitext103validation.txt']   valid_data_path ................................. None   variable_seq_lengths ............................ False   virtual_pipeline_model_parallel_size ............ None   vision_backbone_type ............................ vit   vision_pretraining .............................. False   vision_pretraining_type ......................... classify   vocab_extra_ids ................................. 0   vocab_file ...................................... /home/mauser/work/MegatronLM/work/gpt2vocab.json   vocab_size ...................................... None   wandb_exp_name ..................................    wandb_project ...................................    wandb_save_dir ..................................    weight_decay .................................... 0.01   weight_decay_incr_style ......................... constant   world_size ...................................... 1   yaml_cfg ........................................ None  end of arguments  setting number of microbatches to constant 1 > building GPT2BPETokenizer tokenizer ... /home/mauser/work/MegatronLM/megatron/training/tokenizer/gpt2_tokenization.py:159: ResourceWarning: unclosed file    self.encoder = json.load(open(vocab_file)) ResourceWarning: Enable tracemalloc to get the object allocation traceback /home/mauser/work/MegatronLM/megatron/training/tokenizer/gpt2_tokenization.py:164: ResourceWarning: unclosed file    bpe_data = open(merges_file, encoding='utf8').read().split('\n')[1:1] ResourceWarning: Enable tracemalloc to get the object allocation traceback  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304) > initializing torch distributed ... /usr/local/Ascend/ascendtoolkit/8.0.RC1/python/sitepackages/tbe/tvm/contrib/ccec.py:792: DeprecationWarning: invalid escape sequence \L   if not dirpath.find(""AppData\Local\Temp""): /usr/local/Ascend/ascendtoolkit/latest/python/sitepackages/tbe/dsl/classifier/transdata/transdata_classifier.py:222: DeprecationWarning: invalid escape sequence \B   """""" /usr/local/Ascend/ascendtoolkit/latest/python/sitepackages/tbe/dsl/unify_schedule/vector/transdata/common/graph/transdata_graph_info.py:143: DeprecationWarning: invalid escape sequence \c   """""" [W ProcessGroupHCCL.cpp:639] Warning: The HCCL execution timeout 1800000ms is bigger than watchdog timeout 600000ms which is set by init_process_group! The plog may not be recorded. (function ProcessGroupHCCL) [W ProcessGroupHCCL.cpp:639] Warning: The HCCL execution timeout 1800000ms is bigger than watchdog timeout 600000ms which is set by init_process_group! The plog may not be recorded. (function ProcessGroupHCCL) [W ProcessGroupHCCL.cpp:639] Warning: The HCCL execution timeout 1800000ms is bigger than watchdog timeout 600000ms which is set by init_process_group! The plog may not be recorded. (function ProcessGroupHCCL) [W ProcessGroupHCCL.cpp:639] Warning: The HCCL execution timeout 1800000ms is bigger than watchdog timeout 600000ms which is set by init_process_group! The plog may not be recorded. (function ProcessGroupHCCL) [W ProcessGroupHCCL.cpp:639] Warning: The HCCL execution timeout 1800000ms is bigger than watchdog timeout 600000ms which is set by init_process_group! The plog may not be recorded. (function ProcessGroupHCCL) [W ProcessGroupHCCL.cpp:639] Warning: The HCCL execution timeout 1800000ms is bigger than watchdog timeout 600000ms which is set by init_process_group! The plog may not be recorded. (function ProcessGroupHCCL) [W ProcessGroupHCCL.cpp:639] Warning: The HCCL execution timeout 1800000ms is bigger than watchdog timeout 600000ms which is set by init_process_group! The plog may not be recorded. (function ProcessGroupHCCL) [W ProcessGroupHCCL.cpp:639] Warning: The HCCL execution timeout 1800000ms is bigger than watchdog timeout 600000ms which is set by init_process_group! The plog may not be recorded. (function ProcessGroupHCCL) [W ProcessGroupHCCL.cpp:639] Warning: The HCCL execution timeout 1800000ms is bigger than watchdog timeout 600000ms which is set by init_process_group! The plog may not be recorded. (function ProcessGroupHCCL) [W ProcessGroupHCCL.cpp:639] Warning: The HCCL execution timeout 1800000ms is bigger than watchdog timeout 600000ms which is set by init_process_group! The plog may not be recorded. (function ProcessGroupHCCL) [W ProcessGroupHCCL.cpp:639] Warning: The HCCL execution timeout 1800000ms is bigger than watchdog timeout 600000ms which is set by init_process_group! The plog may not be recorded. (function ProcessGroupHCCL) [W ProcessGroupHCCL.cpp:639] Warning: The HCCL execution timeout 1800000ms is bigger than watchdog timeout 600000ms which is set by init_process_group! The plog may not be recorded. (function ProcessGroupHCCL) [W ProcessGroupHCCL.cpp:639] Warning: The HCCL execution timeout 1800000ms is bigger than watchdog timeout 600000ms which is set by init_process_group! The plog may not be recorded. (function ProcessGroupHCCL) [W ProcessGroupHCCL.cpp:639] Warning: The HCCL execution timeout 1800000ms is bigger than watchdog timeout 600000ms which is set by init_process_group! The plog may not be recorded. (function ProcessGroupHCCL) [W ProcessGroupHCCL.cpp:639] Warning: The HCCL execution timeout 1800000ms is bigger than watchdog timeout 600000ms which is set by init_process_group! The plog may not be recorded. (function ProcessGroupHCCL) all tp gourps [[0]] all ep groups [[0]] all dp groups [[0]] all_dp_modulo_exp_group_ranks [[0]] all_tensor_and_expert_group_ranks [[0]] all_data_parallel_group_ranks_with_cp [[0]] > initialized tensor model parallel with size 1 > initialized pipeline model parallel with size 1 > setting random seeds to 1234 ... > compiling dataset index builder ... make: Entering directory '/home/mauser/work/MegatronLM/megatron/core/datasets' make: Nothing to be done for 'default'. make: Leaving directory '/home/mauser/work/MegatronLM/megatron/core/datasets' >>> done with dataset index builder. Compilation time: 0.174 seconds building GPT model ...  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 354871296  loading release checkpoint from /home/mauser/work/MegatronLM/work/checkpoint/gpt2_345m Warning: since the loaded file is not a zipfile, only ""torch.device"" and ""str"" type parameters are currently supported for parameter types of map_locationIf parameter types of map_location is ""Callable[[torch.Tensor, str], torch.Tensor]"" or ""Dict[str, str]"", which is only support for zipfile,all tensors are currently loaded onto the CPU, which may introduce problems could not find arguments in the checkpoint ...  checkpoint version 0  succesfully fixed querykeyvalues ordering for checkpoint version 0   successfully loaded checkpoint from /home/mauser/work/MegatronLM/work/checkpoint/gpt2_345m [ t 0, p 0 ] at iteration 0  > number of original tokens: 216348, number of detokenized tokens: 242241 > working on iteration: 0 /home/mauser/work/MegatronLM/megatron/core/tensor_parallel/layers.py:584: UserWarning: async_grad_allreduce is deprecated and will be removed in a future release. use allreduce_dgrad instead.   warnings.warn( > working on iteration: 10 > working on iteration: 20 > working on iteration: 30 > working on iteration: 40 > working on iteration: 50 > working on iteration: 60 > working on iteration: 70 > working on iteration: 80 > working on iteration: 90 > working on iteration: 100 > working on iteration: 110 > working on iteration: 120 > working on iteration: 130 > working on iteration: 140 > working on iteration: 150 > working on iteration: 160 > working on iteration: 170 > working on iteration: 180 > working on iteration: 190 > working on iteration: 200 > working on iteration: 210 > working on iteration: 220 > working on iteration: 230 > working on iteration: 240 > working on iteration: 250 > working on iteration: 260 > working on iteration: 270 > working on iteration: 280 > working on iteration: 290 > working on iteration: 300 > working on iteration: 310 > working on iteration: 320 > working on iteration: 330 > working on iteration: 340 > working on iteration: 350 > working on iteration: 360 > working on iteration: 370 > working on iteration: 380 > working on iteration: 390 > working on iteration: 400 > working on iteration: 410 > working on iteration: 420 > working on iteration: 430 > working on iteration: 440 > working on iteration: 450 > working on iteration: 460 > working on iteration: 470 > working on iteration: 480 > working on iteration: 490 > working on iteration: 500 > working on iteration: 510 > working on iteration: 520 > working on iteration: 530 > working on iteration: 540 > working on iteration: 550 > working on iteration: 560 > working on iteration: 570 > working on iteration: 580 > working on iteration: 590 > working on iteration: 600 > working on iteration: 610 > working on iteration: 620 > working on iteration: 630 > working on iteration: 640 > working on iteration: 650 > working on iteration: 660 > working on iteration: 670 > working on iteration: 680 > working on iteration: 690 > working on iteration: 700 > working on iteration: 710 > working on iteration: 720 > working on iteration: 730 > working on iteration: 740 > working on iteration: 750 > working on iteration: 760 > working on iteration: 770 > working on iteration: 780 > working on iteration: 790 > working on iteration: 800 > working on iteration: 810 > working on iteration: 820 > working on iteration: 830 > working on iteration: 840 > working on iteration: 850 > working on iteration: 860 > working on iteration: 870 > working on iteration: 880 > working on iteration: 890 > working on iteration: 900 > working on iteration: 910 > working on iteration: 920 > working on iteration: 930 > working on iteration: 940 E89999: Inner Error! E89999: 2024082017:07:22.887.402  op[ScaledMaskedSoftmax1], The first 2 dims of mask [8, 1] must be able to be broadcasted to the first 2 dims of x [2, 16].[FUNC:SetMaskShape][FILE:scaled_masked_softmax.cc][LINE:195]         TraceBack (most recent call last):         op[ScaledMaskedSoftmax1], set mask shape failed.[FUNC:Tiling4ScaledMaskedSoftmax][FILE:scaled_masked_softmax.cc][LINE:268]         [Exec][Op]Execute op failed. op type = ScaledMaskedSoftmax, ge result = 4294967295[FUNC:ReportCallError][FILE:log_inner.cpp][LINE:161] Traceback (most recent call last):   File ""/home/mauser/work/MegatronLM/tasks/main.py"", line 100, in      main()   File ""/home/mauser/work/MegatronLM/tasks/zeroshot_gpt/evaluate.py"", line 208, in main     evaluate_and_print_results(args.task, dataloader, model, eval_metric)   File ""/home/mauser/work/MegatronLM/tasks/zeroshot_gpt/evaluate.py"", line 145, in evaluate_and_print_results     output = evaluate(data_loader, model, eval_metric)   File ""/home/mauser/work/MegatronLM/tasks/zeroshot_gpt/evaluate.py"", line 129, in evaluate     output = forward_step(batch, model, eval_metric, config)   File ""/home/mauser/work/MegatronLM/tasks/zeroshot_gpt/evaluate.py"", line 88, in forward_step     output = model(tokens, position_ids, attention_mask)   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/mauser/work/MegatronLM/megatron/legacy/model/module.py"", line 190, in forward     outputs = self.module(*inputs, **kwargs)   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/mauser/work/MegatronLM/megatron/legacy/model/gpt_model.py"", line 82, in forward     lm_output = self.language_model(   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/mauser/work/MegatronLM/megatron/legacy/model/language_model.py"", line 494, in forward     encoder_output = self.encoder(   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/mauser/work/MindSpeed/mindspeed/model/transformer.py"", line 331, in wrapper     return fn(self, hidden_states, attention_mask, **kwargs)   File ""/home/mauser/work/MegatronLM/megatron/legacy/model/transformer.py"", line 1777, in forward     hidden_states = layer(   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/mauser/work/MindSpeed/mindspeed/core/transformer/transformer.py"", line 35, in row_parallel_forward     output = forward_func(*args, **kwargs)   File ""/home/mauser/work/MegatronLM/megatron/legacy/model/transformer.py"", line 1160, in forward     norm_output = self.input_norm(hidden_states)   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/mauser/work/MegatronLM/megatron/legacy/model/fused_layer_norm.py"", line 84, in forward     return fused_layer_norm_affine(input, weight, self.bias, self.normalized_shape, eps=self.eps)   File ""/home/mauser/work/MindSpeed/mindspeed/core/fusions/fused_layer_norm.py"", line 25, in fused_layer_norm_affine     return torch.nn.functional.layer_norm(input_, normalized_shape, weight, bias, eps)   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/functional.py"", line 2543, in layer_norm     return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled) RuntimeError: The Inner error is reported as above.  Since the operator is called asynchronously, the stacktrace may be inaccurate. If you want to get the accurate stacktrace, pleace set the environment variable ASCEND_LAUNCH_BLOCKING=1. [ERROR] 2024082017:07:22 (PID:35668, Device:0, RankID:1) ERR00005 PTA internal error ```",2024-08-20T17:52:38+08:00,,closed,0,3,https://gitee.com/ascend/MindSpeed/issues/IALAT8,"设置ASCEND_LAUNCH_BLOCKING=1之后，报错如下（有区别的部分） ``` > working on iteration: 940 Traceback (most recent call last):   File ""/home/mauser/work/MegatronLM/tasks/main.py"", line 100, in      main()   File ""/home/mauser/work/MegatronLM/tasks/zeroshot_gpt/evaluate.py"", line 208, in main     evaluate_and_print_results(args.task, dataloader, model, eval_metric)   File ""/home/mauser/work/MegatronLM/tasks/zeroshot_gpt/evaluate.py"", line 145, in evaluate_and_print_results     output = evaluate(data_loader, model, eval_metric)   File ""/home/mauser/work/MegatronLM/tasks/zeroshot_gpt/evaluate.py"", line 129, in evaluate     output = forward_step(batch, model, eval_metric, config)   File ""/home/mauser/work/MegatronLM/tasks/zeroshot_gpt/evaluate.py"", line 88, in forward_step     output = model(tokens, position_ids, attention_mask)   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/mauser/work/MegatronLM/megatron/legacy/model/module.py"", line 190, in forward     outputs = self.module(*inputs, **kwargs)   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/mauser/work/MegatronLM/megatron/legacy/model/gpt_model.py"", line 82, in forward     lm_output = self.language_model(   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/mauser/work/MegatronLM/megatron/legacy/model/language_model.py"", line 494, in forward     encoder_output = self.encoder(   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/mauser/work/MindSpeed/mindspeed/model/transformer.py"", line 331, in wrapper     return fn(self, hidden_states, attention_mask, **kwargs)   File ""/home/mauser/work/MegatronLM/megatron/legacy/model/transformer.py"", line 1777, in forward     hidden_states = layer(   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/mauser/work/MindSpeed/mindspeed/core/transformer/transformer.py"", line 35, in row_parallel_forward     output = forward_func(*args, **kwargs)   File ""/home/mauser/work/MegatronLM/megatron/legacy/model/transformer.py"", line 1164, in forward     self.self_attention(   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/mauser/work/MindSpeed/mindspeed/model/transformer.py"", line 833, in parallel_attention_forward     context_layer = self.core_attention(   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/mauser/work/MindSpeed/mindspeed/model/transformer.py"", line 212, in core_attention_forward     attention_probs = self.scale_mask_softmax(attention_scores,   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/mauser/work/MegatronLM/megatron/legacy/model/fused_softmax.py"", line 148, in forward     return self.forward_fused_softmax(input, mask)   File ""/home/mauser/work/MindSpeed/mindspeed/core/fusions/fused_softmax.py"", line 50, in forward_fused_softmax     return torch_npu.npu_scaled_masked_softmax(input_, mask, scale, False)   File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch/_ops.py"", line 692, in __call__     return self._op(*args, **kwargs or {}) RuntimeError: InnerRun:torch_npu/csrc/framework/OpParamMaker.cpp:197 NPU error, error code is 500002 [ERROR] 2024082220:41:22 (PID:19166, Device:0, RankID:1) ERR01100 OPS call acl api failed [Error]: A GE error occurs in the system.         Rectify the fault based on the error information in the ascend log. E89999: Inner Error! E89999: 2024082220:41:22.534.311  op[ScaledMaskedSoftmax1], The first 2 dims of mask [8, 1] must be able to be broadcasted to the first 2 dims of x [2, 16].[FUNC:SetMaskShape][FILE:scaled_masked_softmax.cc][LINE:195]         TraceBack (most recent call last):         op[ScaledMaskedSoftmax1], set mask shape failed.[FUNC:Tiling4ScaledMaskedSoftmax][FILE:scaled_masked_softmax.cc][LINE:268]         [Exec][Op]Execute op failed. op type = ScaledMaskedSoftmax, ge result = 4294967295[FUNC:ReportCallError][FILE:log_inner.cpp][LINE:161] ```","  File ""/home/mauser/work/MindSpeed/mindspeed/core/fusions/fused_softmax.py"", line 50, in forward_fused_softmax     return torch_npu.npu_scaled_masked_softmax(input_, mask, scale, False) 打印下输入input_的shape看看 npu_scaled_masked_softmax 要求input_的后两维的长度在[32,4096]并且 能被32整除",该融合算子当前存在此限制，可增加nomaskedsoftmaxfusion参数关闭该融合算子进行调试
Zheng Shoujian,all_reduce 报错,"一、问题现象（附报错日志上下文）： `dist.all_reduce(start_time_tensor, op=dist.ReduceOp.MIN)` 该操作执行失败 ``` [20240819 20:42:09,564] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified. [20240819 20:42:09,564] torch.distributed.run: [WARNING] [20240819 20:42:09,564] torch.distributed.run: [WARNING] ***************************************** [20240819 20:42:09,564] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. [20240819 20:42:09,564] torch.distributed.run: [WARNING] ***************************************** /opt/rh/rhpython38/root/usr/local/lib64/python3.8/sitepackages/torch_npu/contrib/transfer_to_npu.py:299: ImportWarning:     *************************************************************************************************************     The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..     The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..     The backend in torch.distributed.init_process_group set to hccl now..     The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..     The device parameters have been replaced with npu in the function below:     torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.autocast, torch.load, torch.Generator, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.nn.Module.to, torch.nn.Module.to_empty     *************************************************************************************************************   warnings.warn(msg, ImportWarning) /opt/rh/rhpython38/root/usr/local/lib64/python3.8/sitepackages/torch_npu/contrib/transfer_to_npu.py:260: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.   warnings.warn(msg, RuntimeWarning) /opt/rh/rhpython38/root/usr/local/lib64/python3.8/sitepackages/torch_npu/contrib/transfer_to_npu.py:260: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.   warnings.warn(msg, RuntimeWarning) /opt/rh/rhpython38/root/usr/local/lib/python3.8/sitepackages/gpytorch/lazy/__init__.py:70: DeprecationWarning: GPyTorch will be replacing all LazyTensor functionality with the linear operator package. Replace all references to gpytorch.lazy.*LazyTensor with linear_operator.operators.*LinearOperator.   warnings.warn( /opt/rh/rhpython38/root/usr/local/lib/python3.8/sitepackages/gpytorch/lazy/__init__.py:70: DeprecationWarning: GPyTorch will be replacing all LazyTensor functionality with the linear operator package. Replace all references to gpytorch.lazy.*LazyTensor with linear_operator.operators.*LinearOperator.   warnings.warn( [W NPUCachingAllocator.cpp:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator()) [W NPUCachingAllocator.cpp:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator()) Traceback (most recent call last):   File ""/opt/rh/rhpython38/root/usr/local/lib64/python3.8/sitepackages/torch/distributed/c10d_logger.py"", line 47, in wrapper     return func(*args, **kwargs)   File ""/opt/rh/rhpython38/root/usr/local/lib64/python3.8/sitepackages/torch/distributed/distributed_c10d.py"", line 2050, in all_reduce     work = group.allreduce([tensor], opts) RuntimeError: create:torch_npu/csrc/distributed/ProcessGroupHCCL.cpp:80 HCCL function error: HcclCommInitRootInfo(numRanks, &rootInfo, rank, &(comm>hcclComm_)), error code is 7 [ERROR] 2024081920:42:18 (PID:107212, Device:0, RankID:1) ERR02200 DIST call hccl api failed. EJ0001: [PID: 107212] 2024081920:42:18.103.684 Failed to initialize the HCCP process. Reason: Maybe the last training process is running.         Solution: Wait for 10s after killing the last training process and try again.         TraceBack (most recent call last):         tsd client wait response fail, hostpid:77509, device response code[1]. unknown device error.[FUNC:WaitRsp][FILE:process_mode_manager.cpp][LINE:336] During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""all_reduce_test.py"", line 34, in      test_all_reduce()   File ""all_reduce_test.py"", line 27, in test_all_reduce     dist.all_reduce(start_time_tensor, op=dist.ReduceOp.MIN)   File ""/opt/rh/rhpython38/root/usr/local/lib64/python3.8/sitepackages/torch/distributed/c10d_logger.py"", line 52, in wrapper     ""args"": f""{args}, {kwargs}"",   File ""/opt/rh/rhpython38/root/usr/local/lib64/python3.8/sitepackages/torch/_tensor.py"", line 431, in __repr__     return torch._tensor_str._str(self, tensor_contents=tensor_contents)   File ""/opt/rh/rhpython38/root/usr/local/lib64/python3.8/sitepackages/torch/_tensor_str.py"", line 664, in _str     return _str_intern(self, tensor_contents=tensor_contents)   File ""/opt/rh/rhpython38/root/usr/local/lib64/python3.8/sitepackages/torch/_tensor_str.py"", line 595, in _str_intern     tensor_str = _tensor_str(self, indent)   File ""/opt/rh/rhpython38/root/usr/local/lib64/python3.8/sitepackages/torch/_tensor_str.py"", line 347, in _tensor_str     formatter = _Formatter(get_summarized_data(self) if summarize else self)   File ""/opt/rh/rhpython38/root/usr/local/lib64/python3.8/sitepackages/torch/_tensor_str.py"", line 138, in __init__     tensor_view, torch.isfinite(tensor_view) & tensor_view.ne(0) RuntimeError: Sync:torch_npu/csrc/framework/OpCommand.cpp:183 NPU function error: c10_npu::acl::AclrtSynchronizeStreamWithTimeout(stream), error code is 507018 [ERROR] 2024081920:42:25 (PID:107212, Device:0, RankID:1) ERR00100 PTA call acl api failed [Error]: The aicpu execution is abnormal.         Rectify the fault based on the error information in the ascend log. EZ9999: Inner Error! EZ9999: [PID: 107212] 2024081920:42:25.305.850 Kernel task happen error, retCode=0x2a, [aicpu exception].[FUNC:PreCheckTaskErr][FILE:davinic_kernel_task.cc][LINE:1221]         TraceBack (most recent call last):        AICPU Kernel task happen error, retCode=0x2a.[FUNC:GetError][FILE:stream.cc][LINE:1067]        Aicpu kernel execute failed, device_id=0, stream_id=5, task_id=2, errorCode=2a.[FUNC:PrintAicpuErrorInfo][FILE:davinic_kernel_task.cc][LINE:1024]        Aicpu kernel execute failed, device_id=0, stream_id=5, task_id=2, fault op_name=[FUNC:GetError][FILE:stream.cc][LINE:1067]        rtStreamSynchronizeWithTimeout execute failed, reason=[aicpu exception][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:53]        synchronize stream failed, runtime result = 507018[FUNC:ReportCallError][FILE:log_inner.cpp][LINE:161]        Fail to get sq reg virtual addr, deviceId=0, sqId=9.[FUNC:Setup][FILE:stream.cc][LINE:639]        stream setup failed, retCode=0x7020010.[FUNC:SyncGetDevMsg][FILE:api_impl.cc][LINE:5199]        Sync get device msg failed, retCode=0x7020010.[FUNC:GetDevErrMsg][FILE:api_impl.cc][LINE:5255]        rtGetDevMsg execute failed, reason=[driver error:internal error][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:53] EJ0001: [PID: 107212] 2024081920:42:18.103.684 Failed to initialize the HCCP process. Reason: Maybe the last training process is running.         Solution: Wait for 10s after killing the last training process and try again.         TraceBack (most recent call last):         tsd client wait response fail, hostpid:77509, device response code[1]. unknown device error.[FUNC:WaitRsp][FILE:process_mode_manager.cpp][LINE:336] [W NPUStream.cpp:409] Warning: NPU warning, error code is 507018[Error]: [Error]: The aicpu execution is abnormal.         Rectify the fault based on the error information in the ascend log. EH9999: Inner Error!         rtDeviceSynchronize execute failed, reason=[aicpu exception][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:53] EH9999: [PID: 107212] 2024081920:42:25.321.721 wait for compute device to finish failed, runtime result = 507018.[FUNC:ReportCallError][FILE:log_inner.cpp][LINE:161]         TraceBack (most recent call last):        Fail to get sq reg virtual addr, deviceId=0, sqId=10.[FUNC:Setup][FILE:stream.cc][LINE:639]        stream setup failed, retCode=0x7020010.[FUNC:SyncGetDevMsg][FILE:api_impl.cc][LINE:5199]        Sync get device msg failed, retCode=0x7020010.[FUNC:GetDevErrMsg][FILE:api_impl.cc][LINE:5255]         rtGetDevMsg execute failed, reason=[driver error:internal error][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:53] EZ9999: Inner Error! EZ9999: [PID: 107212] 2024081920:42:25.305.850 Kernel task happen error, retCode=0x2a, [aicpu exception].[FUNC:PreCheckTaskErr][FILE:davinic_kernel_task.cc][LINE:1221]         TraceBack (most recent call last):        AICPU Kernel task happen error, retCode=0x2a.[FUNC:GetError][FILE:stream.cc][LINE:1067]        Aicpu kernel execute failed, device_id=0, stream_id=5, task_id=2, errorCode=2a.[FUNC:PrintAicpuErrorInfo][FILE:davinic_kernel_task.cc][LINE:1024]        Aicpu kernel execute failed, device_id=0, stream_id=5, task_id=2, fault op_name=[FUNC:GetError][FILE:stream.cc][LINE:1067]        rtStreamSynchronizeWithTimeout execute failed, reason=[aicpu exception][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:53]        synchronize stream failed, runtime result = 507018[FUNC:ReportCallError][FILE:log_inner.cpp][LINE:161]        Fail to get sq reg virtual addr, deviceId=0, sqId=9.[FUNC:Setup][FILE:stream.cc][LINE:639]        stream setup failed, retCode=0x7020010.[FUNC:SyncGetDevMsg][FILE:api_impl.cc][LINE:5199]        Sync get device msg failed, retCode=0x7020010.[FUNC:GetDevErrMsg][FILE:api_impl.cc][LINE:5255]        rtGetDevMsg execute failed, reason=[driver error:internal error][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:53] EJ0001: [PID: 107212] 2024081920:42:18.103.684 Failed to initialize the HCCP process. Reason: Maybe the last training process is running.         Solution: Wait for 10s after killing the last training process and try again.         TraceBack (most recent call last):         tsd client wait response fail, hostpid:77509, device response code[1]. unknown device error.[FUNC:WaitRsp][FILE:process_mode_manager.cpp][LINE:336]  (function npuSynchronizeUsedDevices) [W NPUStream.cpp:392] Warning: NPU warning, error code is 507018[Error]: [Error]: The aicpu execution is abnormal.         Rectify the fault based on the error information in the ascend log. EH9999: Inner Error!         rtDeviceSynchronize execute failed, reason=[aicpu exception][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:53] EH9999: [PID: 107212] 2024081920:42:25.322.195 wait for compute device to finish failed, runtime result = 507018.[FUNC:ReportCallError][FILE:log_inner.cpp][LINE:161]         TraceBack (most recent call last):        Fail to get sq reg virtual addr, deviceId=0, sqId=11.[FUNC:Setup][FILE:stream.cc][LINE:639]        stream setup failed, retCode=0x7020010.[FUNC:SyncGetDevMsg][FILE:api_impl.cc][LINE:5199]        Sync get device msg failed, retCode=0x7020010.[FUNC:GetDevErrMsg][FILE:api_impl.cc][LINE:5255]         rtGetDevMsg execute failed, reason=[driver error:internal error][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:53] EH9999: Inner Error!         rtDeviceSynchronize execute failed, reason=[aicpu exception][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:53] EH9999: [PID: 107212] 2024081920:42:25.321.721 wait for compute device to finish failed, runtime result = 507018.[FUNC:ReportCallError][FILE:log_inner.cpp][LINE:161]         TraceBack (most recent call last):        Fail to get sq reg virtual addr, deviceId=0, sqId=10.[FUNC:Setup][FILE:stream.cc][LINE:639]        stream setup failed, retCode=0x7020010.[FUNC:SyncGetDevMsg][FILE:api_impl.cc][LINE:5199]        Sync get device msg failed, retCode=0x7020010.[FUNC:GetDevErrMsg][FILE:api_impl.cc][LINE:5255]         rtGetDevMsg execute failed, reason=[driver error:internal error][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:53] EZ9999: Inner Error! EZ9999: [PID: 107212] 2024081920:42:25.305.850 Kernel task happen error, retCode=0x2a, [aicpu exception].[FUNC:PreCheckTaskErr][FILE:davinic_kernel_task.cc][LINE:1221]         TraceBack (most recent call last):        AICPU Kernel task happen error, retCode=0x2a.[FUNC:GetError][FILE:stream.cc][LINE:1067]        Aicpu kernel execute failed, device_id=0, stream_id=5, task_id=2, errorCode=2a.[FUNC:PrintAicpuErrorInfo][FILE:davinic_kernel_task.cc][LINE:1024]        Aicpu kernel execute failed, device_id=0, stream_id=5, task_id=2, fault op_name=[FUNC:GetError][FILE:stream.cc][LINE:1067]        rtStreamSynchronizeWithTimeout execute failed, reason=[aicpu exception][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:53]        synchronize stream failed, runtime result = 507018[FUNC:ReportCallError][FILE:log_inner.cpp][LINE:161]        Fail to get sq reg virtual addr, deviceId=0, sqId=9.[FUNC:Setup][FILE:stream.cc][LINE:639]        stream setup failed, retCode=0x7020010.[FUNC:SyncGetDevMsg][FILE:api_impl.cc][LINE:5199]        Sync get device msg failed, retCode=0x7020010.[FUNC:GetDevErrMsg][FILE:api_impl.cc][LINE:5255]        rtGetDevMsg execute failed, reason=[driver error:internal error][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:53] EJ0001: [PID: 107212] 2024081920:42:18.103.684 Failed to initialize the HCCP process. Reason: Maybe the last training process is running.         Solution: Wait for 10s after killing the last training process and try again.         TraceBack (most recent call last):         tsd client wait response fail, hostpid:77509, device response code[1]. unknown device error.[FUNC:WaitRsp][FILE:process_mode_manager.cpp][LINE:336]  (function npuSynchronizeDevice) [W NPUStream.cpp:392] Warning: NPU warning, error code is 507018[Error]: [Error]: The aicpu execution is abnormal.         Rectify the fault based on the error information in the ascend log. EH9999: Inner Error!         rtDeviceSynchronize execute failed, reason=[aicpu exception][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:53] EH9999: [PID: 107212] 2024081920:42:25.322.526 wait for compute device to finish failed, runtime result = 507018.[FUNC:ReportCallError][FILE:log_inner.cpp][LINE:161]         TraceBack (most recent call last):        Fail to get sq reg virtual addr, deviceId=0, sqId=12.[FUNC:Setup][FILE:stream.cc][LINE:639]        stream setup failed, retCode=0x7020010.[FUNC:SyncGetDevMsg][FILE:api_impl.cc][LINE:5199]        Sync get device msg failed, retCode=0x7020010.[FUNC:GetDevErrMsg][FILE:api_impl.cc][LINE:5255]         rtGetDevMsg execute failed, reason=[driver error:internal error][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:53] EH9999: Inner Error!         rtDeviceSynchronize execute failed, reason=[aicpu exception][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:53] EH9999: [PID: 107212] 2024081920:42:25.322.195 wait for compute device to finish failed, runtime result = 507018.[FUNC:ReportCallError][FILE:log_inner.cpp][LINE:161]         TraceBack (most recent call last):        Fail to get sq reg virtual addr, deviceId=0, sqId=11.[FUNC:Setup][FILE:stream.cc][LINE:639]        stream setup failed, retCode=0x7020010.[FUNC:SyncGetDevMsg][FILE:api_impl.cc][LINE:5199]        Sync get device msg failed, retCode=0x7020010.[FUNC:GetDevErrMsg][FILE:api_impl.cc][LINE:5255]         rtGetDevMsg execute failed, reason=[driver error:internal error][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:53] EH9999: Inner Error!         rtDeviceSynchronize execute failed, reason=[aicpu exception][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:53] EH9999: [PID: 107212] 2024081920:42:25.321.721 wait for compute device to finish failed, runtime result = 507018.[FUNC:ReportCallError][FILE:log_inner.cpp][LINE:161]         TraceBack (most recent call last):        Fail to get sq reg virtual addr, deviceId=0, sqId=10.[FUNC:Setup][FILE:stream.cc][LINE:639]        stream setup failed, retCode=0x7020010.[FUNC:SyncGetDevMsg][FILE:api_impl.cc][LINE:5199]        Sync get device msg failed, retCode=0x7020010.[FUNC:GetDevErrMsg][FILE:api_impl.cc][LINE:5255]         rtGetDevMsg execute failed, reason=[driver error:internal error][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:53] EZ9999: Inner Error! EZ9999: [PID: 107212] 2024081920:42:25.305.850 Kernel task happen error, retCode=0x2a, [aicpu exception].[FUNC:PreCheckTaskErr][FILE:davinic_kernel_task.cc][LINE:1221]         TraceBack (most recent call last):        AICPU Kernel task happen error, retCode=0x2a.[FUNC:GetError][FILE:stream.cc][LINE:1067]        Aicpu kernel execute failed, device_id=0, stream_id=5, task_id=2, errorCode=2a.[FUNC:PrintAicpuErrorInfo][FILE:davinic_kernel_task.cc][LINE:1024]        Aicpu kernel execute failed, device_id=0, stream_id=5, task_id=2, fault op_name=[FUNC:GetError][FILE:stream.cc][LINE:1067]        rtStreamSynchronizeWithTimeout execute failed, reason=[aicpu exception][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:53]        synchronize stream failed, runtime result = 507018[FUNC:ReportCallError][FILE:log_inner.cpp][LINE:161]        Fail to get sq reg virtual addr, deviceId=0, sqId=9.[FUNC:Setup][FILE:stream.cc][LINE:639]        stream setup failed, retCode=0x7020010.[FUNC:SyncGetDevMsg][FILE:api_impl.cc][LINE:5199]        Sync get device msg failed, retCode=0x7020010.[FUNC:GetDevErrMsg][FILE:api_impl.cc][LINE:5255]        rtGetDevMsg execute failed, reason=[driver error:internal error][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:53] ``` 二、软件版本:  CANN 版本 (e.g., CANN 3.0.x，5.x.x):  8.0.RC3 Tensorflow/Pytorch/MindSpore 版本: torchnpu 2.1.0.post6  torch 2.1.0+cpu Python 版本 (e.g., Python 3.7.5): 3.8  MindStudio版本 (e.g., MindStudio 2.0.0 (beta3)): 操作系统版本 (e.g., Ubuntu 18.04):  三、测试步骤： ``` import torch import mindspeed.megatron_adaptor import torch_npu import torch.distributed as dist def test_all_reduce():     dist.init_process_group(""nccl"")     rank = dist.get_rank()     print(f""Start running basic DDP example on rank {rank}."")     import time     start_time = time.time()     start_time_tensor = torch.tensor([start_time], dtype=torch.double, device='npu')     dist.all_reduce(start_time_tensor, op=dist.ReduceOp.MIN)     print(start_time_tensor.item())     dist.destroy_process_group() if __name__ == ""__main__"":     test_all_reduce() ``` 保存代码为 `all_reduce_test.py` ``` torchrun nnodes=1 nproc_per_node=2 rdzv_id=100 rdzv_backend=c10d rdzv_endpoint=localhost:29411 all_reduce_test.py ``` 四、日志信息: xxxx 请根据自己的运行环境参考以下方式搜集日志信息，如果涉及到算子开发相关的问题，建议也提供UT/ST测试和单算子集成测试相关的日志。 日志提供方式: 将日志打包后作为附件上传。若日志大小超出附件限制，则可上传至外部网盘后提供链接。 获取方法请参考wiki： https://gitee.com/ascend/modelzoo/wikis/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E6%97%A5%E5%BF%97%E5%92%8C%E8%AE%A1%E7%AE%97%E5%9B%BE?sort_id=4097825",2024-08-19T20:49:09+08:00,,closed,0,1,https://gitee.com/ascend/MindSpeed/issues/IAL298,EJ0001: [PID: 107212] 2024081920:42:18.103.684 Failed to initialize the HCCP process. Reason: Maybe the last training process is running.         Solution: Wait for 10s after killing the last training process and try again. 应该是有残留进程，清理后再重试
jiangjiang,gptmoe2t 在256/1024节点规模运行，跑了几步就出现nan,"一、问题现象（附报错日志上下文）： gptmoe2t 在256/1024节点规模运行，跑了几步就出现nan 二、软件版本:  CANN 版本 (e.g., CANN 3.0.x，5.x.x):   runtime_running_version=[7.3.T10.0.B528:8.0.T16] compiler_running_version=[7.3.T10.0.B528:8.0.T16] hccl_running_version=[7.3.T10.0.B528:8.0.T16] opp_running_version=[7.3.T10.0.B528:8.0.T16] toolkit_running_version=[7.3.T10.0.B528:8.0.T16] aoe_running_version=[7.3.T10.0.B528:8.0.T16] ncs_running_version=[7.3.T10.0.B528:8.0.T16] opp_kernel_running_version=[7.3.T10.0.B528:8.0.T16] runtime_upgrade_version=[7.3.T10.0.B528:8.0.T16] compiler_upgrade_version=[7.3.T10.0.B528:8.0.T16] hccl_upgrade_version=[7.3.T10.0.B528:8.0.T16] opp_upgrade_version=[7.3.T10.0.B528:8.0.T16] toolkit_upgrade_version=[7.3.T10.0.B528:8.0.T16] aoe_upgrade_version=[7.3.T10.0.B528:8.0.T16] ncs_upgrade_version=[7.3.T10.0.B528:8.0.T16] opp_kernel_upgrade_version=[7.3.T10.0.B528:8.0.T16] runtime_installed_version=[7.3.T10.0.B528:8.0.T16] compiler_installed_version=[7.3.T10.0.B528:8.0.T16] hccl_installed_version=[7.3.T10.0.B528:8.0.T16] opp_installed_version=[7.3.T10.0.B528:8.0.T16] toolkit_installed_version=[7.3.T10.0.B528:8.0.T16] aoe_installed_version=[7.3.T10.0.B528:8.0.T16] ncs_installed_version=[7.3.T10.0.B528:8.0.T16] opp_kernel_installed_version=[7.3.T10.0.B528:8.0.T16] Tensorflow/Pytorch/MindSpore 版本:2.1.0 Python 版本 (e.g., Python 3.7.5):  MindStudio版本 (e.g., MindStudio 2.0.0 (beta3)):不涉及 操作系统版本 (e.g., Ubuntu 18.04): Ubuntu 20.04.6 LTS 三、测试步骤： 确保集合通信正常后拉起集群训练 四、日志信息: 获取方法请参考wiki： https://gitee.com/ascend/modelzoo/wikis/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E6%97%A5%E5%BF%97%E5%92%8C%E8%AE%A1%E7%AE%97%E5%9B%BE?sort_id=4097825",2024-08-14T11:09:38+08:00,,closed,0,0,https://gitee.com/ascend/MindSpeed/issues/IAJW92
JayNine,运行脚本，提示依赖transformer_engine。,"尝试运行MegatronLM/examples/run_text_generation_server_345M.sh失败。 主要就是在 MegatronLM/tools/run_text_generation_server.py 文件中`import torch`下面加了一行：`import mindspeed.megatron_adaptor`然后尝试运行脚本。 失败报错如下： ``` > `(PyTorch2.1.0) [mauser MegatronLM]$./examples/run_text_generation_server_345M.sh` > /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/utils/path_manager.py:79: UserWarning: Warning: The /usr/local/Ascend/ascendtoolkit/latest owner does not match the current user. >   warnings.warn(f""Warning: The {path} owner does not match the current user."") > /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages/torch_npu/utils/path_manager.py:79: UserWarning: Warning: The /usr/local/Ascend/ascendtoolkit/8.0.RC1/aarch64linux/ascend_toolkit_install.info owner does not match the current user. >   warnings.warn(f""Warning: The {path} owner does not match the current user."") > Traceback (most recent call last): >   File ""/home/mauser/work/MegatronLM/tools/run_text_generation_server.py"", line 8, in  >     from megatron.training import get_args >   File ""/home/mauser/work/MegatronLM/megatron/training/__init__.py"", line 16, in  >     from .initialize  import initialize_megatron >   File ""/home/mauser/work/MegatronLM/megatron/training/initialize.py"", line 18, in  >     from megatron.training.arguments import parse_args, validate_args >   File ""/home/mauser/work/MegatronLM/megatron/training/arguments.py"", line 13, in  >     from megatron.core.models.retro.utils import ( >   File ""/home/mauser/work/MegatronLM/megatron/core/models/retro/__init__.py"", line 12, in  >     from .decoder_spec import get_retro_decoder_block_spec >   File ""/home/mauser/work/MegatronLM/megatron/core/models/retro/decoder_spec.py"", line 9, in  >     from megatron.core.models.gpt.gpt_layer_specs import ( >   File ""/home/mauser/work/MegatronLM/megatron/core/models/gpt/__init__.py"", line 1, in  >     from .gpt_model import GPTModel >   File ""/home/mauser/work/MegatronLM/megatron/core/models/gpt/gpt_model.py"", line 17, in  >     from megatron.core.transformer.transformer_block import TransformerBlock >   File ""/home/mauser/work/MegatronLM/megatron/core/transformer/transformer_block.py"", line 16, in  >     from megatron.core.transformer.custom_layers.transformer_engine import ( >   File ""/home/mauser/work/MegatronLM/megatron/core/transformer/custom_layers/transformer_engine.py"", line 9, in  >     import transformer_engine as te > ModuleNotFoundError: No module named 'transformer_engine' ```",2024-08-13T23:36:08+08:00,,closed,0,2,https://gitee.com/ascend/MindSpeed/issues/IAJTGX,建议将 import torch import mindspeed.megatron_adaptor 移动到import megatron 相关内容的上方,确实可以
lyn,是用nanopipe报错IndexError: pop from empty list,"一、问题现象（附报错日志上下文）： Traceback (most recent call last):   File ""pretrain_gpt.py"", line 267, in      main()   File ""pretrain_gpt.py"", line 260, in main     pretrain(train_valid_test_datasets_provider,   File ""/home/l00515014/model_test/ModelLink/modellink/training.py"", line 325, in pretrain     iteration, num_floating_point_operations_so_far = train(*train_args)   File ""/home/l00515014/model_test/ModelLink/modellink/training.py"", line 469, in train     train_step(forward_step_func,   File ""/home/l00515014/model_test/ModelLink/megatron/megatron/training/training.py"", line 543, in train_step     losses_reduced = forward_backward_func(   File ""/home/l00515014/model_test/MindSpeedmaster/mindspeed/core/pipeline_parallel/flexible_schedules.py"", line 1119, in forward_backward_pipelining_with_interleaving_nano_pipe     WeightGradStore.pop()   File ""/home/l00515014/model_test/MindSpeedmaster/mindspeed/core/weight_grad_store.py"", line 96, in pop     input_slice, grad_output_slice, weight, sequence_parallel, in_row, pipe_experts = cls.stored_grads.pop(0) IndexError: pop from empty list 二、配置项：   sgd_momentum .................................... 0.9   shape_order ..................................... SBH   short_seq_prob .................................. 0.1   skip_bias_add ................................... True   skip_train ...................................... False   sliding_window .................................. None   sparse_mode ..................................... 0   spec ............................................ None   split ........................................... 949,50,1   square_alibi_mask ............................... False   squared_relu .................................... False   standalone_embedding_stage ...................... False   start_weight_decay .............................. 0.1   swiglu .......................................... True   swin_backbone_type .............................. tiny   tensor_model_parallel_size ...................... 1   tensorboard_dir ................................. None   tensorboard_log_interval ........................ 1   tensorboard_queue_size .......................... 1000   test_data_path .................................. None   test_mode ....................................... False   timing_log_level ................................ 0   timing_log_option ............................... minmax   titles_data_path ................................ None   tokenizer_kwargs ................................ None   tokenizer_model ................................. /home/ModelLink_0412_data_build/dataset/llama27bhf/tokenizer/tokenizer.model   tokenizer_name_or_path .......................... None   tokenizer_not_use_fast .......................... True   tokenizer_padding_side .......................... right   tokenizer_type .................................. Llama2Tokenizer   tp_comm_bulk_dgrad .............................. True   tp_comm_bulk_wgrad .............................. True   tp_comm_overlap ................................. False   tp_comm_overlap_ag .............................. True   tp_comm_overlap_cfg ............................. None   tp_comm_overlap_rs .............................. True   tp_comm_split_ag ................................ True   tp_comm_split_rs ................................ True   train_data_path ................................. None   train_iters ..................................... 10   train_samples ................................... None   transformer_impl ................................ local   transformer_pipeline_model_parallel_size ........ 4   ulysses_degree_in_cp ............................ None   untie_embeddings_and_output_weights ............. True   use_ascend_coc .................................. False   use_ascend_mc2 .................................. False   use_checkpoint_args ............................. False   use_checkpoint_opt_param_scheduler .............. False   use_cp_send_recv_overlap ........................ False   use_cpu_initialization .......................... None   use_deter_comp .................................. False   use_dist_ckpt ................................... False   use_distributed_optimizer ....................... True   use_flash_attn .................................. True   use_fused_rmsnorm ............................... True   use_fused_rotary_pos_emb ........................ False   use_fused_swiglu ................................ False   use_fusion_attn_v2 .............................. False   use_mc2 ......................................... False   use_mcore_models ................................ False   use_nanopipe .................................... True   use_nd_matmul ................................... False   use_one_sent_docs ............................... False   use_partial_rope ................................ False   use_pipe_experts ................................ False   use_ring_exchange_p2p ........................... False   use_rotary_position_embeddings .................. False   use_rts ......................................... False   valid_data_path ................................. None   variable_seq_lengths ............................ False   virtual_pipeline_model_parallel_size ............ 4   vision_backbone_type ............................ vit   vision_pretraining .............................. False   vision_pretraining_type ......................... classify   vocab_extra_ids ................................. 0   vocab_file ...................................... None   vocab_size ...................................... None   wandb_exp_name ..................................   wandb_project ...................................   wandb_save_dir ..................................   weight_decay .................................... 0.1   weight_decay_incr_style ......................... constant   world_size ...................................... 8   yaml_cfg ........................................ None",2024-08-13T16:38:37+08:00,,closed,0,1,https://gitee.com/ascend/MindSpeed/issues/IAJQJX,最新版本mindspeed用一样的参数跑起来没问题，建议更新最新版本。
邓彬彬,MindSpeed安装报错,"一、问题现象（附报错日志上下文）： 277	      Requirement already satisfied: pluggy=0.12 in /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages (from pytest>r requirements.txt (line 15)) (1.5.0)    278	      Requirement already satisfied: py>=1.8.2 in /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages (from pytest>r requirements.txt (line 15)) (1.11.0)    279	      Requirement already satisfied: tomli>=1.0.0 in /home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/sitepackages (from pytest>r requirements.txt (line 15)) (2.0.1)    280	      WARNING: Error parsing dependencies of apex: Invalid version: '0.1ascend20240413'    281	      WARNING: Error parsing dependencies of moxingframework: Invalid version: '2.1.16.2ae09d45'    282	      Installing collected packages: sentencepiece, ninja, regex, pybind11, einops    283	      ERROR: Exception:    284	      Traceback (most recent call last):    285	        File ""/opt/huawei/dataset/env_dir/sitepackages/pip/_internal/cli/base_command.py"", line 105, in _run_wrapper    286	          status = _inner_run()    287	        File ""/opt/huawei/dataset/env_dir/sitepackages/pip/_internal/cli/base_command.py"", line 96, in _inner_run    288	          return self.run(options, args)    289	        File ""/opt/huawei/dataset/env_dir/sitepackages/pip/_internal/cli/req_command.py"", line 67, in wrapper    290	          return func(self, options, args)    291	        File ""/opt/huawei/dataset/env_dir/sitepackages/pip/_internal/commands/install.py"", line 483, in run    292	          installed_versions[distribution.canonical_name] = distribution.version    293	        File ""/opt/huawei/dataset/env_dir/sitepackages/pip/_internal/metadata/pkg_resources.py"", line 192, in version   294	          return parse_version(self._dist.version)    295	        File ""/opt/huawei/dataset/env_dir/sitepackages/pip/_vendor/packaging/version.py"", line 56, in parse    296	          return Version(version)    297	        File ""/opt/huawei/dataset/env_dir/sitepackages/pip/_vendor/packaging/version.py"", line 202, in __init__    298	          raise InvalidVersion(f""Invalid version: '{version}'"")    299	      pip._vendor.packaging.version.InvalidVersion: Invalid version: '0.1ascend20240413'    300	      Traceback (most recent call last):    301	        File """", line 2, in     302	        File """", line 34, in     303	        File ""/opt/huawei/scheduletrain/algorithm/MindSpeed/setup.py"", line 118, in     304	          subprocess.check_call([sys.executable, 'm', 'pip', 'install', 'r', 'requirements.txt'])    305	        File ""/home/mauser/anaconda3/envs/PyTorch2.1.0/lib/python3.9/subprocess.py"", line 373, in check_call    306	          raise CalledProcessError(retcode, cmd)    307	      subprocess.CalledProcessError: Command '['/home/mauser/anaconda3/envs/PyTorch2.1.0/bin/python', 'm', 'pip', 'install', 'r', 'requirements.txt']' returned nonzero exit status 2.    308	      [end of output] 二、软件版本:  CANN 版本 (e.g., CANN 3.0.x，5.x.x):  8.0.RC2 Tensorflow/Pytorch/MindSpore 版本: Pytorch 2.1.0 Python 版本 (e.g., Python 3.7.5): 3.9.10  MindStudio版本 (e.g., MindStudio 2.0.0 (beta3)): N/A 操作系统版本 (e.g., Ubuntu 18.04): 欧拉2.10 三、测试步骤： pip install e MindSpeed 四、日志信息: xxxx 请根据自己的运行环境参考以下方式搜集日志信息，如果涉及到算子开发相关的问题，建议也提供UT/ST测试和单算子集成测试相关的日志。 日志提供方式: 将日志打包后作为附件上传。若日志大小超出附件限制，则可上传至外部网盘后提供链接。 获取方法请参考wiki： https://gitee.com/ascend/modelzoo/wikis/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E6%97%A5%E5%BF%97%E5%92%8C%E8%AE%A1%E7%AE%97%E5%9B%BE?sort_id=4097825",2024-08-13T15:14:15+08:00,,closed,0,0,https://gitee.com/ascend/MindSpeed/issues/IAJOVU
刘田亮,MindSpeed在依赖torchair，未在文档或requirment中标注，且torchair无发行版本,MindSpeed在依赖torchair为啥依赖里面没有呢，torchair还没有发行版，还需要用户自己打包，是不是学习成本太高了？,2024-08-09T17:32:03+08:00,,closed,0,1,https://gitee.com/ascend/MindSpeed/issues/IAJ1XV,torchair 为 torch_npu 内置包，如有问题，可尝试更新 torch_npu。
wangtongyu6,开启profiling之后单个step会多次更新权重,"一、问题现象（附报错日志上下文）： xxxx 二、软件版本:  CANN 版本 (e.g., CANN 3.0.x，5.x.x):  8.0.RC1 Tensorflow/Pytorch/MindSpore 版本: Pytorch 2.1.0 Python 版本 (e.g., Python 3.7.5): python.8  MindStudio版本 (e.g., MindStudio 2.0.0 (beta3)): 无 操作系统版本 (e.g., Ubuntu 18.04): 欧拉 三、测试步骤： 两机跑llama13b，单机的脚本如下： ```bash python m torch.distributed.launch nproc_per_node 8 nnodes 2 node_rank 1 master_addr 110.3.241.19 master_port 60015 pretrain_gpt.py tensormodelparallelsize 8 pipelinemodelparallelsize 1 sequenceparallel numlayers 40 hiddensize 5120 ffnhiddensize 13824 numattentionheads 40 tokenizertype Llama2Tokenizer tokenizermodel /home/ModelLink_0412_data_build/dataset/llama27bhf/tokenizer/tokenizer.model seqlength 4096 maxpositionembeddings 4096 microbatchsize 2 globalbatchsize 128 makevocabsizedivisibleby 1 lr 1e6 trainiters 4 lrdecaystyle cosine untieembeddingsandoutputweights disablebiaslinear attentiondropout 0.0 initmethodstd 0.01 hiddendropout 0.0 positionembeddingtype rope normalization RMSNorm usefusedrmsnorm swiglu useflashattn nomaskedsoftmaxfusion attentionsoftmaxinfp32 minlr 1e8 weightdecay 1e1 lrwarmupfraction 0.01 clipgrad 1.0 adambeta1 0.9 initiallossscale 4096 adambeta2 0.95 nogradientaccumulationfusion load /home/l00515014/model_test/checkpoint_llama2_13b noloadoptim noloadrng bf16 datapath /home/ModelLink_0412_data_build/dataset/llama27bhf/alpaca_text_document split 949,50,1 loginterval 1 saveinterval 1000 evalinterval 1000 evaliters 10 profile profilestepstart 0 profilestepend 2 profileranks 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 profilelevel level1 profilesavepath /home/hwtest/model_test/llama13b_profile_iter_0_2 profilewithcpu profilerecordshapes distributedbackend nccl ``` profiling收集的单个step中跑了两个相同的操作，更新了两次权重。怀疑是bug !输入图片说明 四、日志信息: xxxx 请根据自己的运行环境参考以下方式搜集日志信息，如果涉及到算子开发相关的问题，建议也提供UT/ST测试和单算子集成测试相关的日志。 日志提供方式: 将日志打包后作为附件上传。若日志大小超出附件限制，则可上传至外部网盘后提供链接。 获取方法请参考wiki： https://gitee.com/ascend/modelzoo/wikis/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E6%97%A5%E5%BF%97%E5%92%8C%E8%AE%A1%E7%AE%97%E5%9B%BE?sort_id=4097825",2024-08-08T20:06:49+08:00,,closed,0,2,https://gitee.com/ascend/MindSpeed/issues/IAITCZ,图中的 allreduce 看起来并非 optimizer 中的 allreduce，可以再确认下此处 allreduce 是哪里的,这个allreduce是梯度桶的allreduce，optimizer时间非常短，但是跑了两次。
maryhh,modellink 调用Ascend出现 import name 'initialize_model_parallel'错误,我在转换模型的时候出现ImportError: cannot import name 'initialize_model_parallel' from 'mindspeed.core.parallel_state'  命令 python tools/checkpoint/convert_ckpt.py  modeltype GPT  loader llama2_hf  saver megatron  targettensorparallelsize 8  targetpipelineparallelsize 1  loaddir ./modellink/model_from_hf/Baichuan7B/  savedir ./model_weights/Baichuan7Bv0.1tp8pp1/  tokenizermodel ./modellink/model_from_hf/Baichuan7B/tokenizer.model  wpack True !输入图片说明,2024-08-07T15:18:13+08:00,,closed,0,1,https://gitee.com/ascend/MindSpeed/issues/IAIH3R,请确认modellink与mindspeed配套版本，当前modellink对应megatron core0.6.0版本。 建议使用mindspeed 1.1分支（630 release），或者core_r0.6.0分支（930 core_r0.6.0主线）
谢国敏,MindIE多卡推理internlm27bchat报错,"一、问题现象（附报错日志上下文）：  MindIE多卡推理internlm27bchat报错 二、软件版本:  scendcannkernels310p_8.0.T16  Ascendcanntoolkit_8.0.T16                         Ascendmindie_1.0.T60 Ascendcannnnal_8.0.T16  三、测试步骤： xxxx 四、日志信息: scendcannkernels310p_8.0.T16  Ascendcanntoolkit_8.0.T16                         Ascendmindie_1.0.T60 Ascendcannnnal_8.0.T16  单卡推理internlm27bchat没什么问题。。。 多卡出现报错，报错情况如下： 20240731 16:13:57,967 [INFO] [pid: 216308] logging.py53: begin inference /root/mindie/MindIELLM/examples/server/request.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).   input_ids = torch.tensor(input_ids, dtype=torch.int64) 20240731 16:13:57,973 [INFO] [pid: 216308] logging.py53: total req num: 1, infer start Traceback (most recent call last):   File ""/root/miniconda3/envs/mindie2/lib/python3.10/runpy.py"", line 196, in _run_module_as_main     return _run_code(code, main_globals, None,   File ""/root/miniconda3/envs/mindie2/lib/python3.10/runpy.py"", line 86, in _run_code     exec(code, run_globals)   File ""/root/mindie/MindIELLM/examples/run_pa.py"", line 367, in      generate_texts, token_nums, _ = pa_runner.infer(**infer_params)   File ""/root/mindie/MindIELLM/examples/run_pa.py"", line 191, in infer     generate_req(req_list, self.model, self.max_batch_size, self.max_prefill_tokens, self.cache_manager)   File ""/root/mindie/MindIELLM/examples/server/generate.py"", line 124, in generate_req     req_finished = generate_token(model, cache_manager, batch)   File ""/root/mindie/MindIELLM/examples/server/generate.py"", line 24, in generate_token     logits = model.forward(   File ""/root/mindie/MindIELLM/atb_llm/runner/model_runner.py"", line 98, in forward     return self.model.forward(**kwargs)   File ""/root/mindie/MindIELLM/atb_llm/models/base/flash_causal_lm.py"", line 227, in forward     logits = self.execute_ascend_operator(acl_inputs, acl_param, is_prefill)   File ""/root/mindie/MindIELLM/atb_llm/models/base/flash_causal_lm.py"", line 198, in execute_ascend_operator     acl_model_out = self.acl_encoder_operation.execute(acl_inputs, acl_param) RuntimeError: execute失败 详细信息见Ascend官方文档，请开启日志进一步定位问题。 /root/mindie/MindIELLM/examples/server/request.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).   input_ids = torch.tensor(input_ids, dtype=torch.int64) Traceback (most recent call last):   File ""/root/miniconda3/envs/mindie2/lib/python3.10/runpy.py"", line 196, in _run_module_as_main     return _run_code(code, main_globals, None,   File ""/root/miniconda3/envs/mindie2/lib/python3.10/runpy.py"", line 86, in _run_code     exec(code, run_globals)   File ""/root/mindie/MindIELLM/examples/run_pa.py"", line 367, in      generate_texts, token_nums, _ = pa_runner.infer(**infer_params)   File ""/root/mindie/MindIELLM/examples/run_pa.py"", line 191, in infer     generate_req(req_list, self.model, self.max_batch_size, self.max_prefill_tokens, self.cache_manager)   File ""/root/mindie/MindIELLM/examples/server/generate.py"", line 124, in generate_req     req_finished = generate_token(model, cache_manager, batch)   File ""/root/mindie/MindIELLM/examples/server/generate.py"", line 24, in generate_token     logits = model.forward(   File ""/root/mindie/MindIELLM/atb_llm/runner/model_runner.py"", line 98, in forward     return self.model.forward(**kwargs)   File ""/root/mindie/MindIELLM/atb_llm/models/base/flash_causal_lm.py"", line 227, in forward     logits = self.execute_ascend_operator(acl_inputs, acl_param, is_prefill)   File ""/root/mindie/MindIELLM/atb_llm/models/base/flash_causal_lm.py"", line 198, in execute_ascend_operator     acl_model_out = self.acl_encoder_operation.execute(acl_inputs, acl_param) RuntimeError: execute失败 详细信息见Ascend官方文档，请开启日志进一步定位问题。 [ERROR] 2024073116:13:59 (PID:216308, Device:0, RankID:1) ERR99999 UNKNOWN application exception [ERROR] 2024073116:13:59 (PID:216309, Device:1, RankID:1) ERR99999 UNKNOWN application exception [20240731 16:14:13,215] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 216308) of binary: /root/miniconda3/envs/mindie2/bin/python Traceback (most recent call last):   File ""/root/miniconda3/envs/mindie2/bin/torchrun"", line 8, in      sys.exit(main())   File ""/root/miniconda3/envs/mindie2/lib/python3.10/sitepackages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper     return f(*args, **kwargs)   File ""/root/miniconda3/envs/mindie2/lib/python3.10/sitepackages/torch/distributed/run.py"", line 806, in main     run(args)   File ""/root/miniconda3/envs/mindie2/lib/python3.10/sitepackages/torch/distributed/run.py"", line 797, in run     elastic_launch(   File ""/root/miniconda3/envs/mindie2/lib/python3.10/sitepackages/torch/distributed/launcher/api.py"", line 134, in __call__     return launch_agent(self._config, self._entrypoint, list(args))   File ""/root/miniconda3/envs/mindie2/lib/python3.10/sitepackages/torch/distributed/launcher/api.py"", line 264, in launch_agent     raise ChildFailedError( torch.distributed.elastic.multiprocessing.errors.ChildFailedError: ============================================================ examples.run_pa FAILED  Failures: [1]:   time      : 20240731_16:14:13   host      : 95097d23bb4c   rank      : 1 (local_rank: 1)   exitcode  : 1 (pid: 216309)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html  Root Cause (first observed failure): [0]:   time      : 20240731_16:14:13   host      : 95097d23bb4c   rank      : 0 (local_rank: 0)   exitcode  : 1 (pid: 216308)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html ============================================================",2024-07-31T17:46:53+08:00,,closed,0,1,https://gitee.com/ascend/MindSpeed/issues/IAGY8E,MindSpeed 为训练加速库，MindIE 非 MindSpeed 旗下组件
Titan,arg_parser不支持extra_args_provider与core_transformer_config_from_args_wrapper报错,"一、问题现象（附报错日志上下文）： 问题1：arg_parser不支持extra_args_provider !输入图片说明 !输入图片说明 会在解析额外参数时报错 问题2： !输入图片说明 !输入图片说明 megatron 原始代码有两个输入 二、软件版本:  CANN 版本 8.0.RC2 Tensorflow/Pytorch/MindSpore 版本: torch 2.1.0 Python 版本 (e.g., Python 3.7.5): python3.9 操作系统版本 (e.g., Ubuntu 18.04): 4.19.90vhulk2211.3.0.h1543.eulerosv2r10.aarch64 三、测试步骤： 无 四、日志信息: 无",2024-07-27T11:00:18+08:00,,closed,0,1,https://gitee.com/ascend/MindSpeed/issues/IAFX90,bug已修复
glhyy,apply_patch存在潜在报错风险,"代码位置：https://gitee.com/ascend/MindSpeed/blob/master/mindspeed/patch_utils.pyL67 对sys.modules进行完整循环，并调用hasattr，可能存在潜在报错风险：当value里面的__getattr__里面有import新的模块时，会改变sys.modules，导致字典循环报错。 ``` for key, value in sys.modules.items():     if self.orig_func_name is not None and hasattr(value, self.orig_func_name) \             and id(getattr(value, self.orig_func_name)) == self.orig_func_id:         setattr(value, self.orig_func_name, self.patch_func) ``` 举例：我的环境中有beartype包，会引发`RuntimeError: dictionary changed size during iteration`错误",2024-07-24T17:47:49+08:00,,closed,0,1,https://gitee.com/ascend/MindSpeed/issues/IAFBTS,感谢反馈，已提pr修复
zy_zhang,如何能够复用相同参数的atb 算子,"看到底层算子调用时是通过atb::CreateOperation(param, &op);来创建的算子，想问下如果输入相同，算子参数相同，能否把这个构建的算子个cache住达到复用的效果，形式上类似这种： ``` // 全局 op atb::Operation* op = nullptr; class NPURmsNormFunction : public torch::autograd::Function { public:     static at::Tensor forward(         AutogradContext *ctx, const at::Tensor &x, const at::Tensor &gamma, float epsilon)     {         at::AutoNonVariableTypeMode g;         c10::SmallVector tensor_rstd_shape;         CheckRmsNorm(x, gamma);         InferShapeRmsNorm(tensor_rstd_shape, x, gamma);         // apply tensor         at::Tensor tensor_rstd = at::empty(at::IntArrayRef(tensor_rstd_shape), x.options().dtype(at::ScalarType::Float));         at::Tensor tensor_y = at::empty(x.sizes(), x.options());         atb::infer::RmsNormParam param;         param.layerType = (atb::infer::RmsNormParam::RmsNormType)RMSNORM_LAYERTYPE;         param.normParam.epsilon = epsilon;         param.normParam.rstd = true;         // set input and output         ParamSetter paramsetter;         paramsetter.Input(x)                    .Input(gamma)                    .Output(tensor_y)                    .Output(tensor_rstd);         // atb::Operation* op = nullptr;         // atb::CreateOperation(param, &op);         if(!op) {                         // 复用第一次op的结果             atb::CreateOperation(param, &op);         }         TORCH_CHECK(op != nullptr, ""RmsNorm get op failed!"");         RunAtbCmd(op, paramsetter, ""RmsNorm_forward"");         ctx>save_for_backward({x, tensor_rstd, gamma})         ....... ``` 但是我实际跑个例子重复调用rms_norm算子两次，第一次可以正常跑，第二次就会 报错 segment fault，挂在uint64_t workspaceSize = OperationSetup(paramsetter.variantPack, op, contextPtr)中， 感觉像是第二次运行时第一次的op的指针失效或者析构了导致非法访问的segment fault。 package_name=Ascendcanntoolkit version=8.0.T13 innerversion=V100R001C18B521 compatible_version=[V100R001C15,V100R001C18],[V100R001C30],[V100R001C13],[V100R003C11],[V100R001C29],[V100R001C10] arch=x86_64 os=linux path=/usr/local/Ascend/ascendtoolkit/8.0.T13/x86_64linux 芯片：910b torch：2.1.0.post3 torch_npu: 2.1.0",2024-07-24T15:06:20+08:00,,closed,0,0,https://gitee.com/ascend/MindSpeed/issues/IAF929
王智伟,开启usedistckpt保存权重时报错,开启usedistckpt保存权重时报错,2024-07-19T11:16:06+08:00,,closed,0,2,https://gitee.com/ascend/MindSpeed/issues/IAE3XD,开启usedistckpt保存权重时，使用fork启动多进程，在CANN时被限制使用的。 修改megatron/core/dist_checkpointing/strategies/filesystem_async.py +143 当前为core.0.6.0版本位置 mp.get_contest('fork')改成mp.get_contest('spawn') ,限制说明文档https://www.hiascend.com/document/detail/zh/Atlas200IDKA2DeveloperKit/23.0.RC2/Application%20Development%20Guide/aadgc/aclcppdevg_0152.html
dingdingdyp,mc2.md中单词错误,`docs/features/mc2.md`中matmum单词错误 !输入图片说明,2024-07-15T20:23:27+08:00,,closed,0,1,https://gitee.com/ascend/MindSpeed/issues/IAD5SH,thanks，已修正
marsmeng1994,如何用mindspeed 做类似vllm的推理,场景是一个 长上下文的 推理任务,2024-07-10T14:43:20+08:00,,closed,0,4,https://gitee.com/ascend/MindSpeed/issues/IABY28,当前 MindSpeed 定位是训练加速库，暂无推理内容。但训练前向便是推理，如果您是想做在线推理，可参考各特性的文档，一些特性对推理也是有收益的，例如计算通信并行，各类融合算子等。,page attention算子后续会支持么,暂无计划，推理建议使用MindIE https://www.hiascend.com/document/detail/zh/mindie/1.0.RC1/releasenote/releasenote_0001.html,issue先关闭，后续有需求可进一步讨论
Hi20240217,后续VocabParallelCrossEntropy会开发融合算子吗,后续VocabParallelCrossEntropy会开发融合算子吗,2024-07-03T17:27:43+08:00,,closed,0,4,https://gitee.com/ascend/MindSpeed/issues/IAABR2,当前内部暂无计划，若有需求可向华为客户经理提出，也可学习 Ascend C 开发教程自行开发。,是因为 就算开发了VocabParallelCrossEntropy融合算子 对整体性能的贡献也不大吗,评估对整体贡献不大，因为耗时占比小。若有场景该处耗时占比大，可以提供profiling数据、模型场景，进一步分析,issue关闭，后续有需求可进一步交流
nullnull,grouped_gemm报错，请在文档中写明哪个版本支持该算子," 报错 ``` Traceback (most recent call last):   File ""grouped_gemm.py"", line 30, in      result = gmm.npu_gmm(x.npu(), weight.npu(), bias=None, group_list=group_list, group_type=group_type)   File ""/tmp/MindSpeed_eef859a70d/mindspeed/ops/gmm.py"", line 75, in npu_gmm     return torch.ops.mindspeed.npu_gmm(*args, **kwargs)   File ""/opt/rh/rhpython38/root/usr/local/lib64/python3.8/sitepackages/torch/_ops.py"", line 692, in __call__     return self._op(*args, **kwargs or {})   File ""/tmp/MindSpeed_eef859a70d/mindspeed/ops/gmm.py"", line 71, in _npu_gmm     return GMMFunction.apply(x, weight, bias, group_list, group_type)   File ""/opt/rh/rhpython38/root/usr/local/lib64/python3.8/sitepackages/torch/autograd/function.py"", line 539, in apply     return super().apply(*args, **kwargs)   type: ignore[misc]   File ""/tmp/MindSpeed_eef859a70d/mindspeed/ops/gmm.py"", line 23, in forward     outputs = GMMFunction.mindspeed_ops.npu_gmm([x], [weight], bias, group_list, group_type) RuntimeError: aclnnGroupedMatmulV2 or aclnnGroupedMatmulV2GetWorkspaceSize not in libopapi.so, or libopapi.sonot found. ```  复现脚本 脚本来自：https://gitee.com/ascend/MindSpeed/blob/master/docs/ops/gmm.md `python3 grouped_gemm.py` ``` !/usr/bin/python ****************************************************************  ScriptName: grouped_gemm.py *************************************************************** import os import torch import torch_npu import numpy as np import math from mindspeed.ops import gmm num_expert, seq_len, hidden_dim = 8, 32, 256 group_list = [1, 3, 6, 10, 15, 21, 28, 32] group_type = 0 x_shape = (seq_len, hidden_dim) weight_shape = (num_expert, hidden_dim, seq_len) dtype = torch.float16 x = (torch.rand(x_shape).to(dtype)  0.5) weight = (torch.rand(weight_shape).to(dtype)  0.5)  正向接口案例 x.requires_grad = True weight.requires_grad = True result = gmm.npu_gmm(x.npu(), weight.npu(), bias=None, group_list=group_list, group_type=group_type)  反向接口案例 result.backward(torch.ones(result.shape).npu()) ```  环境 torch版本 ``` torch                     2.1.0+cpu torchnpu                 2.1.0.post3 ``` CANN版本 ```  version: 1.0 runtime_running_version=[7.2.0.1.235:8.0.RC1] compiler_running_version=[7.2.0.1.235:8.0.RC1] hccl_running_version=[7.2.0.1.235:8.0.RC1] opp_running_version=[7.2.0.1.235:8.0.RC1] toolkit_running_version=[7.2.0.1.235:8.0.RC1] aoe_running_version=[7.2.0.1.235:8.0.RC1] ncs_running_version=[7.2.0.1.235:8.0.RC1] opp_kernel_running_version=[7.2.0.1.235:8.0.RC1] runtime_upgrade_version=[7.2.0.1.235:8.0.RC1] compiler_upgrade_version=[7.2.0.1.235:8.0.RC1] hccl_upgrade_version=[7.2.0.1.235:8.0.RC1] opp_upgrade_version=[7.2.0.1.235:8.0.RC1] toolkit_upgrade_version=[7.2.0.1.235:8.0.RC1] aoe_upgrade_version=[7.2.0.1.235:8.0.RC1] ncs_upgrade_version=[7.2.0.1.235:8.0.RC1] opp_kernel_upgrade_version=[7.2.0.1.235:8.0.RC1] runtime_installed_version=[7.2.0.1.235:8.0.RC1] compiler_installed_version=[7.2.0.1.235:8.0.RC1] hccl_installed_version=[7.2.0.1.235:8.0.RC1] opp_installed_version=[7.2.0.1.235:8.0.RC1] toolkit_installed_version=[7.2.0.1.235:8.0.RC1] aoe_installed_version=[7.2.0.1.235:8.0.RC1] ncs_installed_version=[7.2.0.1.235:8.0.RC1] opp_kernel_installed_version=[7.2.0.1.235:8.0.RC1] ```",2024-07-03T13:38:35+08:00,,closed,0,2,https://gitee.com/ascend/MindSpeed/issues/IAA7GV,请更新最新的 CANN 版本,请使用CANN 8.0.RC2及以上版本
nullnull,支持Megatron Core 0.7.0,Megatron Core 0.7.0对MoE做了优化，请问预计什么时候能支持0.7.0？,2024-07-02T11:41:33+08:00,,closed,0,2,https://gitee.com/ascend/MindSpeed/issues/IA9V1V,已在开发中，可关注此条PR，正式商发为 9月 30 号。,master分支已切换core 0.7.0，随RC3版本商发，issue先关闭，后续有问题可进一步交流
guozhihua,ulysess走mcore使用双机报维度错误，ring attention无此现象,ulysess走mcore使用双机报维度错误，ring attention无此现象 !报错error信息 cp2tp8,2024-06-27T10:51:44+08:00,,closed,0,0,https://gitee.com/ascend/MindSpeed/issues/IA8O5C
guozhihua,mc2在mixtral 8*7B走mcore分支的时候没有生效,使用useascendmc2开启mc2之后，mixtral 8*7B走mcore分支时没走到mindspeed里打patch的函数里面，采集的profile也没有mc的算子出现。,2024-06-26T15:01:49+08:00,,closed,0,1,https://gitee.com/ascend/MindSpeed/issues/IA8G2X,已修改，参见PR:https://gitee.com/ascend/MindSpeed/pulls/499
kimmishi,transformer engine依赖咨询,"你好，我按照文档尝试运行，下载了megatronlm 并切到 core_r0.6.0, 但这个版本在`megatron/core/transformer/custom_layers/transformer_engine.py` 对 `transformer_engine`有依赖，安装transformer_engine比较麻烦，需要cudnn这些依赖；请问有比较简单的运行方式吗？",2024-06-19T10:29:41+08:00,,rejected,0,1,https://gitee.com/ascend/MindSpeed/issues/IA6J71,昇腾上无须安装transformer_engine
guozhihua,小算子不支持atten_mask构建,不开fa情况下，预训练中不能创建atten_mask，导致小算子情况下atten_mask一直是None。,2024-06-19T09:10:13+08:00,,closed,0,2,https://gitee.com/ascend/MindSpeed/issues/IA6HM4,小算子支持attn_mask但有限制，如开启pp后仅首尾stage存在mask，为megatron原生问题。 是否有不用fa的实际训练场景,issue已超一个月，先关闭，仍有问题可进一步交流
guozhihua,仓上的megatroncp无法启动,仓上的megatroncp由于新特性引入，存在bug，无法使用。,2024-06-19T09:09:11+08:00,,closed,0,1,https://gitee.com/ascend/MindSpeed/issues/IA6HLS,可以将启动脚本上传下
guozhihua,ulysses走mcore精度问题,"ulysses走mcore,在开cp和不开co的时候，精度相差比较大。",2024-06-19T09:07:52+08:00,,closed,0,0,https://gitee.com/ascend/MindSpeed/issues/IA6HLB
feifeibear,混合序列并行方法参考他人工作，希望注明出处,尊敬的AscendSpeed开发团队， 您好！我注意到您的项目AscendSpeed在10天前加入了混合序列并行功能，其详细描述可以在以下链接找到： https://gitee.com/ascend/AscendSpeed/blob/master/docs/features/hybridcontextparallel.md 在阅读您的文档时，我发现您对Ulysses和Ring缺陷的分析及解决方案与我的GitHub项目feifeibear/longcontextattention中的内容有相似之处。我的项目早在四月一日就已经开源，并且相关技术报告也已于5月13日发布。以下是我的项目和论文预印本的链接： 1. GitHub项目地址： https://github.com/feifeibear/longcontextattention 2. 论文预印本地址： USP: A Unified Sequence Parallelism Approach for Long Context Generative AI https://arxiv.org/abs/2405.07719 我非常欣赏并支持开源社区的合作精神，相信通过相互学习和引用，我们可以共同推动技术的进步和创新。如果您在开发过程中参考了我的工作，我衷心希望能在您的文档或代码中看到相应的引用和致谢。这不仅是对我个人工作的认可，也是对开源精神的尊重和维护。 期待我们能够继续在开源的道路上携手前行，共同为技术社区贡献力量。,2024-05-26T18:06:12+08:00,,closed,0,2,https://gitee.com/ascend/MindSpeed/issues/I9S8QK,您好，  该特性设计之初，有参考您大模型训练之序列并行双雄：DeepSpeed Ulysses & RingAttention中关于将两种序列并行方案融合的讨论。我们已将您github项目以及论文预印本链接增加到该特性文档中。 我们尊重开源以及每一位开源贡献者，也为您对该方案细致的讨论与严谨的实验点赞，欢迎进一步的讨论和指点。,感谢！希望共同努力，为国产大模型框架添砖加瓦！
那一天,ModelLink调用AscendSpeed出现Import rms_norm_init错误,!Import包错误,2024-05-16T10:52:28+08:00,,closed,0,2,https://gitee.com/ascend/MindSpeed/issues/I9PPWF,更新下ascendspeed,好的 感谢
janelu9,使用fusion_attention_v2时候报错libopapi.so not found,from ascendspeed.ops.fusion_attention_v2 import npu_fusion_attention 导入可以成功 输入tensor计算就报错,2024-05-06T18:54:57+08:00,,closed,0,2,https://gitee.com/ascend/MindSpeed/issues/I9MPJA,是不是没装atb包，可以贴下完整的报错信息,请升级到CANN包到最新版本
janelu9,模型并行时，npu_fusion_attention导致loss不收敛,"一、问题现象（附报错日志上下文）： [W LegacyTypeDispatch.h:74] Warning: AutoNonVariableTypeMode is deprecated and will be removed in 1.10 release. For kernel implementations please use AutoDispatchBelowADInplaceOrView instead, If you are looking for a user facing API to enable running your inferenceonly workload, please use c10::InferenceMode. Using AutoDispatchBelowADInplaceOrView in user code is under risk of producing silent wrong result in some edge cases. See Note [AutoDispatchBelowAutograd] for more details.  这是一个警告 二、软件版本:  CANN 版本 (e.g., CANN 8):   Pytorch版本:2.1.0 Python 版本 (e.g., Python 3.9): 操作系统版本 (e.g., Ubuntu 20.04): 如果只是用deepspeed的pipeline并行就可以收敛，在deepspeed的pipeline中嵌套模型并行（融合算子），在npu上训练模型时候，loss就不收敛。",2024-04-30T10:40:31+08:00,,closed,0,6,https://gitee.com/ascend/MindSpeed/issues/I9KV5C,有使用AscendSpeed么,没有使用ascendspeed 用的deepspeed实现流水线并行，tensor并行使用torch的distributed算子 基本纯torch实现（megatron.core.tensor_parallel.layers的写法），其他全部用的ds，但是训练时候不收敛 loss一动不动，即便tensor并行为1。同并行策略和实现，在gpu上是收敛的，怎么回事，需要改哪些东西呢？有算子不兼容吗,  肯能和梯度重计算有关，我用的torch.utils.checkpoint.checkpoint手动对decoderlayer做的重计算，tensor_parallel.ColumnParallelLinear和tensor_parallel.RowParallelLinear在昇腾上不收敛 但是gpu上loss收敛,目前可以trace到是attention算子里面qkv的ColumnParallelLinear有问题，如果替换成torch.nn.Linear就收敛，但是最后的head层也是用的这个算子就没问题，真搞不懂npu这是什么bug,最新发现是npu_fusion_attention存在bug，在列并行下不收敛,npu_fusion_attention 调用问题 已解决
蒋轲磊,torch_tpu中的torchair报错,"torch_tpu版本是torch_npu2.1.0.post3cp39cp39manylinux_2_17_aarch64.manylinux2014_aarch64 ```     from pretrain_gpt import model_provider   File ""/home/mauser/work/ModelLink/MegatronLM/pretrain_gpt.py"", line 6, in      import ascendspeed.megatron_adaptor   File ""/home/mauser/work/ModelLink/AscendSpeed/ascendspeed/megatron_adaptor.py"", line 292, in      exe_adaptation()   File ""/home/mauser/work/ModelLink/AscendSpeed/ascendspeed/megatron_adaptor.py"", line 283, in exe_adaptation     aspm.apply_patches()   File ""/home/mauser/work/ModelLink/AscendSpeed/ascendspeed/patch_utils.py"", line 99, in apply_patches     patch.apply_patch()   File ""/home/mauser/work/ModelLink/AscendSpeed/ascendspeed/patch_utils.py"", line 57, in apply_patch     if hasattr(value, self.orig_func_name) and id(getattr(value, self.orig_func_name)) == self.orig_func_id:   File ""/home/mauser/anaconda3/envs/MindSpore/lib/python3.9/sitepackages/torch_npu/dynamo/__init__.py"", line 17, in __getattr__     raise self._e   File ""/home/mauser/anaconda3/envs/MindSpore/lib/python3.9/sitepackages/torch_npu/dynamo/__init__.py"", line 36, in _get_default_backend     from . import torchair   File ""/home/mauser/anaconda3/envs/MindSpore/lib/python3.9/sitepackages/torch_npu/dynamo/torchair/__init__.py"", line 5, in      from torchair.npu_fx_compiler import get_compiler   File ""/home/mauser/anaconda3/envs/MindSpore/lib/python3.9/sitepackages/torch_npu/dynamo/torchair/__init__.py"", line 5, in      from torchair.npu_fx_compiler import get_compiler   File ""/home/mauser/anaconda3/envs/MindSpore/lib/python3.9/sitepackages/torch_npu/dynamo/torchair/npu_fx_compiler.py"", line 31, in      from torchair.ge_concrete_graph.fx2ge_converter import GeConcreteGraph as ConcreteGraph   File ""/home/mauser/anaconda3/envs/MindSpore/lib/python3.9/sitepackages/torch_npu/dynamo/torchair/ge_concrete_graph/fx2ge_converter.py"", line 22, in      from torchair.core import _torchair ImportError: /home/mauser/anaconda3/envs/MindSpore/lib/python3.9/sitepackages/torch_npu/dynamo/torchair/core/_abi_compat_ge_apis.so: undefined symbol: _ZN2ge5Graph28LoadFromSerializedModelArrayEPKvm /home/mauser/anaconda3/envs/MindSpore/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up    _warnings.warn(warn_message, ResourceWarning) ```",2024-04-26T18:33:24+08:00,,closed,0,1,https://gitee.com/ascend/MindSpeed/issues/I9K135,请更新 cann 包以及 torch_npu
zzzz886,找不到tools/ckpt_convert/llama/convert_weights_from_huggingface.py,找不到tools/ckpt_convert/llama/convert_weights_from_huggingface.py文件？,2024-04-21T17:46:01+08:00,,closed,0,1,https://gitee.com/ascend/MindSpeed/issues/I9IFGW,https://gitee.com/ascend/ModelLink/blob/bk_origin_23/tools/ckpt_convert/llama/convert_weights_from_huggingface.py 旧版 AscendSpeed 已归档至：https://gitee.com/ascend/ModelLink/tree/bk_origin_23/
line290,moe_top_k为什么写死topk只能是1 或 2呢？,此文件下 > ascendspeed/components/moe/gate.py,2024-04-15T09:43:57+08:00,,closed,0,0,https://gitee.com/ascend/MindSpeed/issues/I9GKEE
liuhc33,推理仓库咨询,您好， 我需要将大模型转为onnx，并在昇腾上进行推理（C/C++），请问我应该参考哪个仓库实现，谢谢！,2024-04-08T17:31:04+08:00,,closed,0,1,https://gitee.com/ascend/MindSpeed/issues/I9EX63,推理加速库当前暂未开源，暂无大模型转onnx推理特性
方建文,出现bug，希望解答,ModuleNotFoundError: No module named 'megatron.legacy',2024-04-08T17:11:43+08:00,,closed,0,3,https://gitee.com/ascend/MindSpeed/issues/I9EWRS,+1,切换到1.0分支后解决,megatron.legacy 为新版 Megatron 路径，请确认 commit id 正确。
weitzban,CANN 文档中使用方法和代码不一致,根据 CANN 中的文档执行大模型迁移 https://www.hiascend.com/document/detail/zh/canncommercial/700/foundmodeldev/foundmodeltrain/PT_LMTMOG_000013.html 其中 ``` from ascendspeed.model import llama_model ``` 现在AscendSpeed中没有这个模块，想问下现在基于AscendSpeed的开发流程是什么，是像Readme里讲的在Megatron中引入ascendspeed.megatron_adaptor之后在Megatron里进行开发还是需要额外操作。,2024-03-22T17:06:32+08:00,,closed,0,2,https://gitee.com/ascend/MindSpeed/issues/I9ASVC,文档为 23 年版本，24 年对 AscendSpeed 进行了大整改。 旧版 AscendSpeed 已归档至 https://gitee.com/ascend/ModelLink/tree/bk_origin_23/,新版 AscendSpeed 以插件化的方式适配各种类 Megatron 框架，在适配昇腾设备的同时并提供各种额外的加速特性，做到轻量化使用。
胡继洪,【Atlas Atlas 200T box】【客户问题】【Ascendspeed】根据资料跑gpt3 demo失败,"Environment Hardware Environment(Ascend/GPU/CPU): Ascend Training Solution 23.0.0.3.B010 驱动和固件:Ascend HDK 23.0.RC1.B100 https://cmcszv.clouddragon.huawei.com/cmcversion/versionDetail/Release/basicData?deltaId=9883836933997952&isSelect=Software Uncomment only one /device  line, hit enter to put that in a new line, and remove leading whitespaces from that line: /device ascend Software Environment:  Python version (e.g., Python 3.7.5):Python 3.7.5  OS platform and distribution (e.g., Linux Ubuntu 16.04):4.19.36vhulk1907.1.0.h475.eulerosv2r8.aarch64 Related testcase gpt3 demo Steps to reproduce the issue 拉起demo !输入图片说明 Describe the current behavior 拉起成功 Describe the expected behavior 拉起失败 Related log / screenshot Special notes for this issue 拉起失败 定位开发：韩俊 00558267",2024-03-12T10:23:29+08:00,,closed,0,1,https://gitee.com/ascend/MindSpeed/issues/I97PKO,已解决，为用户环境问题
xiangsen2,从git直接安装不可用,方式一：从 git 直接安装 pip install git+https://gitee.com/ascend/AscendSpeed.git 在后面import megatron_adaptor时， 会报错module not found +import ascendspeed.megatron_adaptor 希望改正。,2024-03-11T11:10:11+08:00,,closed,0,2,https://gitee.com/ascend/MindSpeed/issues/I97D5G,已对齐，为用户环境问题,至少有两个改进点 1. 如果ascendspeed升级了版本， 版本号要变， 这样安装的时候我知道是否更新了 2. 手册加上force_install选项， 提示下用户存在两个同样版本号的但内容不同的包， 强制用户升级一下。
snsao,flashattention无法load，报错：torch_npu/csrc/core/npu/NPUFormat.h: No such file or directory,"一、问题现象（附报错日志上下文）： 源码安装AscendSpeed后，无法使用flashattention 二、软件版本:  CANN 版本 7.0.0 Tensorflow/Pytorch/MindSpore 版本: torch2.1.0cpu Python 版本 3.9.18: 操作系统版本: CentOS Linux 7 (AltArch) 三、测试步骤： 复现代码： ``` import torch import torch_npu from ascendspeed.op_builder import FusionAttentionOpBuilder ascendspeed_ops = FusionAttentionOpBuilder().load() ``` 四、报错日志信息: ``` indspore2.2.11_py39) root:/data/wangguoteng/AscendSpeed/test python test_fa.py  /root/miniconda3/envs/mindspore2.2.11_py39/lib/python3.9/sitepackages/torch_npu/dynamo/__init__.py:18: UserWarning: Register eager implementation for the 'npu' backend of dynamo, as torch_npu was not compiled with torchair.   warnings.warn( Traceback (most recent call last):   File ""/data/wangguoteng/AscendSpeed/test/test_fa.py"", line 83, in      result = ascendspeed.ops.flash_attention(query.npu(), key.npu(), value.npu(), atten_mask.npu(), None, scale_value, q_scale, head_num, input_layout, keep_prob, pre_tokens, next_tokens) AttributeError: module 'ascendspeed.ops' has no attribute 'flash_attention' (mindspore2.2.11_py39) root:/data/wangguoteng/AscendSpeed/test cd .. (mindspore2.2.11_py39) root:/data/wangguoteng/AscendSpeed cd ..^C (mindspore2.2.11_py39) root:/data/wangguoteng/AscendSpeed mv fa.py ./test (mindspore2.2.11_py39) root:/data/wangguoteng/AscendSpeed cd test (mindspore2.2.11_py39) root:/data/wangguoteng/AscendSpeed/test python fa.py  /root/miniconda3/envs/mindspore2.2.11_py39/lib/python3.9/sitepackages/torch_npu/dynamo/__init__.py:18: UserWarning: Register eager implementation for the 'npu' backend of dynamo, as torch_npu was not compiled with torchair.   warnings.warn( Using /root/.cache/torch_extensions/py39_cpu as PyTorch extensions root... Emitting ninja build file /root/.cache/torch_extensions/py39_cpu/fusion_attention/build.ninja... Building extension module fusion_attention... Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N) [1/2] c++ MMD MF fusion_attention.o.d DTORCH_EXTENSION_NAME=fusion_attention DTORCH_API_INCLUDE_EXTENSION_H DPYBIND11_COMPILER_TYPE=\""_gcc\"" DPYBIND11_STDLIB=\""_libstdcpp\"" DPYBIND11_BUILD_ABI=\""_cxxabi1011\"" I/usr/local/Ascend/ascendtoolkit/latest/include I/root/miniconda3/envs/mindspore2.2.11_py39/lib/python3.9/sitepackages/torch_npu/include I/root/miniconda3/envs/mindspore2.2.11_py39/lib/python3.9/sitepackages/torch_npu/third_party I/root/miniconda3/envs/mindspore2.2.11_py39/lib/python3.9/sitepackages/torch_npu/acl I/root/miniconda3/envs/mindspore2.2.11_py39/lib/python3.9/sitepackages/torch_npu/inc I/root/miniconda3/envs/mindspore2.2.11_py39/lib/python3.9/sitepackages/ascendspeed/ops/csrc/cann/inc isystem /root/miniconda3/envs/mindspore2.2.11_py39/lib/python3.9/sitepackages/torch/include isystem /root/miniconda3/envs/mindspore2.2.11_py39/lib/python3.9/sitepackages/torch/include/torch/csrc/api/include isystem /root/miniconda3/envs/mindspore2.2.11_py39/lib/python3.9/sitepackages/torch/include/TH isystem /root/miniconda3/envs/mindspore2.2.11_py39/lib/python3.9/sitepackages/torch/include/THC isystem /root/miniconda3/envs/mindspore2.2.11_py39/include/python3.9 D_GLIBCXX_USE_CXX11_ABI=0 fPIC std=c++17 fstackprotectorall Wl,z,relro,z,now,z,noexecstack fPIC pie Wl,disablenewdtags,rpath s Wnosigncompare Wnodeprecateddeclarations Wnoreturntype D__FILENAME__='""$(notdir $(abspath $     ascendspeed_ops = FusionAttentionOpBuilder().load()   File ""/root/miniconda3/envs/mindspore2.2.11_py39/lib/python3.9/sitepackages/ascendspeed/op_builder/builder.py"", line 61, in load     op_module = load(name=self.name,   File ""/root/miniconda3/envs/mindspore2.2.11_py39/lib/python3.9/sitepackages/torch/utils/cpp_extension.py"", line 1308, in load     return _jit_compile(   File ""/root/miniconda3/envs/mindspore2.2.11_py39/lib/python3.9/sitepackages/torch/utils/cpp_extension.py"", line 1710, in _jit_compile     _write_ninja_file_and_build_library(   File ""/root/miniconda3/envs/mindspore2.2.11_py39/lib/python3.9/sitepackages/torch/utils/cpp_extension.py"", line 1823, in _write_ninja_file_and_build_library     _run_ninja_build(   File ""/root/miniconda3/envs/mindspore2.2.11_py39/lib/python3.9/sitepackages/torch/utils/cpp_extension.py"", line 2116, in _run_ninja_build     raise RuntimeError(message) from e RuntimeError: Error building extension 'fusion_attention' ```",2024-03-07T13:11:29+08:00,,closed,0,1,https://gitee.com/ascend/MindSpeed/issues/I96L64,请使用 torch_npu 主线版本，确认存在 torch_npu/csrc/core/npu/NPUFormat.h 文件。
wyg1997,编译依赖咨询,请问一下编译依赖的 atb 是哪个版本的？我这里试了 7.0.0 的包，没有 `train_op_params.h` 这个头文件,2024-02-01T19:38:40+08:00,,closed,0,0,https://gitee.com/ascend/MindSpeed/issues/I90JTJ
liuyijiang,咋全删啦，请问啥时候能发布啊？,rt,2024-01-18T19:06:55+08:00,,closed,0,2,https://gitee.com/ascend/MindSpeed/issues/I8X9ZM,新版 AscendSpeed 将解耦各种适配与加速特性，敬请期待。 旧版请查看 https://gitee.com/ascend/ModelLink,感谢
