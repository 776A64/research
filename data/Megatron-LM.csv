Guncuke,OOM occurred after loading the checkpoint.,"Hello, I'm currently training DeepSeek v3. My configuration is PP16 EP4. I noticed that when I start training from random without passing in a checkpoint, there won't be any OOM error displayed. However, when I load a checkpoint, several ranks in the middle will report an OOM error. Why is that?",2025-04-03T00:35:26Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1516
shanesyy,[BUG] The order of the param groups matters when creating optimizer,"**Describe the bug** A clear and concise description of what the bug is. According to the code https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/optimizer/__init__.pyL131, the order of the param groups is not deterministic, and might be an issue when pipeline parallelism is enabled **To Reproduce** Steps to reproduce the behavior. The easier it is to reproduce the faster it will get maintainer attention. Run NeMo deepseek_v2_lite training with pp>1, warning like ""There is difference in the common state dict in different ranks"" appears and it turns out the order of the param groups in different pp stages is different, while only rank0 will save the param groups in ckpt. **Expected behavior** A clear and concise description of what you expected to happen. A key sort might needed as follows  **Stack trace/logs** If applicable, add the stack trace or logs from the time of the error. I ran deepseek_v2_lite in NeMo with tp=2, pp=4 on 8 gpus (changed the layer_num so that pp could be 4). The warning related to ckpt saving is as follows: There is difference in the common state dict in different ranks. The differences are {2: ([], [], [(('optimizer', 0, 'optimizer', 'param_groups', 0, 'wd_mult'), , ), (('optimizer', 0, 'optimizer', 'param_groups', 1, 'wd_mult'), , )]), 3: ([], [], [(('optimizer', 0, 'optimizer', 'param_groups', 0, 'wd_mult'), , ), (('optimizer', 0, 'optimizer', 'param_groups', 1, 'wd_mult'), , )]), 4: ([], [], [(('optimizer', 0, 'optimizer', 'param_groups', 0, 'wd_mult'), , ), (('optimizer', 0, 'optimizer', 'param_groups', 1, 'wd_mult'), , )]), 5: ([], [], [(('optimizer', 0, 'optimizer', 'param_groups', 0, 'wd_mult'), , ), (('optimizer', 0, 'optimizer', 'param_groups', 1, 'wd_mult'), , )]), 6: ([], [], [(('optimizer', 0, 'optimizer', 'param_groups', 0, 'wd_mult'), , ), (('optimizer', 0, 'optimizer', 'param_groups', 1, 'wd_mult'), , )]), 7: ([], [], [(('optimizer', 0, 'optimizer', 'param_groups', 0, 'wd_mult'), , ), (('optimizer', 0, 'optimizer', 'param_groups', 1, 'wd_mult'), , )])} **Environment (please complete the following information):**   MegatronLM 56f8a96e7e328f1c9979b03b359fd284432050fc",2025-04-02T20:32:44Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1515
bescks,[BUG] aux loss will be saved twice when recompute-granularity is full,"If using recompute, the function `save_to_aux_losses_tracker` will be executed twice(both in forward and checkpoint) and the aux_loss will be accumulated twice also. finally, the saved aux loss is two times as much as true aux loss https://github.com/NVIDIA/MegatronLM/blob/core_r0.11.0/megatron/core/transformer/moe/router.pyL228 ```     def apply_load_balancing_loss(         self, activation: torch.Tensor, load_balancing_loss_func: Callable     ):         """"""Calculate auxiliary loss, attach gradient function to activation and add to logging.""""""         moe_aux_loss_coeff = self.config.moe_aux_loss_coeff         if moe_aux_loss_coeff == 0:             return activation         sequence_partition_group = None         if self.config.moe_token_dispatcher_type == ""alltoall_seq"":             sequence_partition_group = parallel_state.get_context_parallel_group()             moe_aux_loss_coeff /= parallel_state.get_tensor_model_parallel_world_size()         elif parallel_state.get_tensor_and_context_parallel_world_size() > 1:             sequence_partition_group = parallel_state.get_tensor_and_context_parallel_group()         aux_loss = load_balancing_loss_func(             moe_aux_loss_coeff=moe_aux_loss_coeff, sequence_partition_group=sequence_partition_group         )         save_to_aux_losses_tracker(             ""load_balancing_loss"",             aux_loss / moe_aux_loss_coeff,             self.layer_number,             self.config.num_layers,             reduce_group=sequence_partition_group,         )         activation = MoEAuxLossAutoScaler.apply(activation, aux_loss)         return activation ``` **Environment (please complete the following information):**   MegatronLM commit ID  tag core_r0.11.0   PyTorch version   CUDA version   NCCL version",2025-04-02T16:09:58Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1514
bescks,[BUG] the function seq_aux_loss_load_balancing seems not align with DeepSeekV2 paper,"**Describe the bug** !Image if the capacity is not None,  computing `fi` should counts the score '1' base on the topk_mask instead of capacity_mask, otherwise the aux loss will be less than the true aux loss about ~30%ï¼Œï¼ˆ1.1 vs 0.8ï¼‰.  (test under setting num_experts=80, topk_experts = 8, capcacity=2) The aim of aux loss is to balancing expert selection, so this kind of capacity drop indeed weaken the regulation of balancing.  In the deepseekv3 paper, the declaration is more clear: the score is 1 as long as `s'` in topk score of experts `i` with  respect to token`t` , not `Token ð‘¡ selects Expert` !Image **Environment (please complete the following information):**   MegatronLM commit ID   core_r0.11.0   b1022a3   PyTorch version   CUDA version   NCCL version **Proposed fix** If you have a proposal for how to fix the issue state it here or link to a PR. the last return value `tokens_per_expert` of fucntion `topk_softmax_with_capacity` is just as well `fi` before capactiy mask. aux loss can be computed based on this tensor !Image",2025-04-02T11:08:01Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1513
hawkoli1987,provide the alternative to the data preprocessing using pyspark acceleration,"for large CPU clusters (>100 CPUs, > 200GB RAMs), pyspark can be used in lieu of multiprocessing to further accelerate the preprocessing step. This PR includes: 1. **tools/preprocess_data.py**: the implementation of pyspark conversion, which:  defaults to single input file partition  the args.partitions are ignored, instead the default pyspark auto partitioning will be used (each partition size 128 MB)  the logic that manually counts the line, and partition the input jsonl file are removed  the sentence_split logic is also change from multiprocessing to serial. 2. **tests/unit_tests/data/test_preprocess_data_spark.py**:  the test script, which   the sentense split test with bert tokenizer as in the original **test_preprocess_data.py** is removed, given nltk loading error in recent python versions, we can add it back after such bug is resolve  additional throughput test is added, by preprocessing a large dummy file with the following size, given the same hardware _workers=n_cpus=32_, suggests that pyspark is at least 2x faster than multiprocessing.   ```   NUM_SAMPLES = 100000   SEQ_LEN = 81920   ```",2025-04-02T03:20:59Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1512
ajayvohra2005,[ENHANCEMENT] Add full support in local mode without Apex or Transformer Engine,"**Is your feature request related to a problem? Please describe.** While this project makes an effort to support local mode, without Apex/TransformerEngine, it does not fully support such a mode. If Apex and TE is not installed, many units tests fail. Also, this project assumes direct use of CUDA device, and does not, for example, support XLA on CUDA, so one can use Open XLA specific features on GPUs. **Describe the solution you'd like** I would like to add full support for local mode, such that the project can function nominally without Apex/TE. Also, I would like to add support for CUDA, and XLA on CUDA. **Describe alternatives you've considered** There are no alternatives that I can think of that meet the objectives of full support without Apex/TE, and that allows one to work directly with CUDA, or Open XLA on CUDA. **Proposed implementation** If you have a proposed implementation for the feature state it here or link to a PR. https://github.com/NVIDIA/MegatronLM/pull/1510 **Additional context** Add any other context or screenshots about the feature request here.",2025-03-31T22:44:45Z,,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1511,same
ajayvohra2005,"Add full support for Local mode without Apex/TE, and add support for Open XLA on CUDA","Add full support for Local mode without use of Apex/TE, and support direct use of CUDA, and Open XLA/CUDA.",2025-03-31T22:43:18Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1510
fuchen319,[BUG]run oom with load optim,"**Describe the bug** if run with noloadoptim, it is ok, memory usage is about 60% on H20. But with load optim, it is oom **Stack trace/logs** TENCENT64:47604:53083 [0] include/alloc.h:103 NCCL WARN Cuda failure 2 'out of memory' TENCENT64:47604:53083 [0] include/alloc.h:206 NCCL WARN Failed to CUDA calloc async 24 bytes TENCENT64:47611:53081 [7] include/alloc.h:103 NCCL WARN Cuda failure 2 'out of memory' TENCENT64:47611:53081 [7] include/alloc.h:206 NCCL WARN Failed to CUDA calloc async 4 bytes RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)` **Environment (please complete the following information):**   MegatronLM 0.11   PyTorch 2.5.1   NCCL version 2.21.5+cuda12.4",2025-03-28T02:40:06Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1509
leondada,[BUG] how to use the distcp checkpoint for huggingface inference?,"The currently fully distributed checkpoint saved in the .distcp format (torch_dist format) cannot be converted to the Hugging Face format using tool/checkpoint/converter. To enable conversion, the fullyparallel feature must be disabled during saving to obtain a checkpoint in the torch format. If that's the case, what is the purpose of the fullyparallel feature? This is extremely confusing to me. Is there a good solution for this? If I have misunderstood anything, please let me know.",2025-03-27T14:39:32Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1508
arjun-choudhry,"Updating the logic for reducing the load_balancing_loss during logging, such that the correct value is logged while using CUDA Graphs","The CUDA graphs can only register tensor operations. As such, caching the PG during graph capture doesn't work when we want to reduce the values later on. This CR fixes that. ",2025-03-27T11:16:24Z,,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1507,closes: CC([BUG] Load Balancing loss discrepancy with/without CUDA Graphs) 
lostkevin,[BUG] Missed argument 'router_dtype',"**Describe the bug** Commit `dab7723` introduces a new attribute `router_dtype` in _DeepepManager, which is not added in `__init__`. https://github.com/NVIDIA/MegatronLM/blob/6ba97dd37150a6bfba03d31808674211cf2a4d0d/megatron/core/transformer/moe/token_dispatcher.pyL774L789 **To Reproduce** None **Expected behavior** None **Stack trace/logs** If applicable, add the stack trace or logs from the time of the error. **Environment (please complete the following information):**   MegatronLM commit ID dab7723821fc326564634b398a809d43740a6c8d **Proposed fix** If you have a proposal for how to fix the issue state it here or link to a PR. **Additional context** Add any other context about the problem here.",2025-03-27T08:32:03Z,,open,1,0,https://github.com/NVIDIA/Megatron-LM/issues/1506
wplf,Fix typo on distrib_optimizer.py,Replace full_sharded_model_space to fully_sharded_model_space,2025-03-26T07:16:27Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1505
hhanyyan,[QUESTION]How to set the routed scaling factor,"How is the routed scaling factor determined, by experiment or by calculation?",2025-03-26T05:47:42Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1504
i-love-megatron,[BUG] Wrong attention gradient in Transformer Engine,"**Describe the bug** The attention gradient dQ calculation in TE is wrong.  When training GPT using Megatron with `transformerimpl transformer_engine` on same parameters and data, the single card result of dQ and tensorparallel result of dQ do not align.  The relative error of dQ is as large as 0.5 for a 24 layer GPT model, and can reach 0.9 for a 128 layer GPT model. Then when checking with the correct `transformerimpl local` baseline implementation, both TE single card and TE tensorparallel's dQ result has a relative error 1.1 compared with the baseline `local`'s dQ result.  They are all trained with the same parameters and data. The calling path for Transformer Engine implementation is `DotProductAttention.forward` > `FusedAttnFunc.apply` > `FusedAttnFunc.backward` > `fused_attn_bwd`. **To Reproduce** Run the following script for TE single card training.  The TP version just adds torchrunrelated distributed arguments and sets `tensormodelparallelsize 2`.  The `local` baseline just sets `transformerimpl local`. ```bash ARGS=""     tensormodelparallelsize 1 \     pipelinemodelparallelsize 1 \     contextparallelsize 1 \     numlayers 24 \     hiddensize 1024 \     numattentionheads 16 \     seqlength 1024 \     maxpositionembeddings 1024 \     attentiondropout 0.0 \     hiddendropout 0.0 \     attentionsoftmaxinfp32 \     bf16 \     clipgrad 1.0 \     microbatchsize 4 \     globalbatchsize 16 \     lr 0.00015 \     minlr 1.0e5 \     trainiters 1 \     lrwarmupfraction 0.01 \     lrdecayiters 320000 \     lrdecaystyle cosine \     weightdecay 1e2 \     usemcoremodels \     nogradientaccumulationfusion \     transformerimpl transformer_engine \     datapath /workspace/dataset/wikitext_text_document \     vocabfile /workspace/dataset/gpt2vocab.json \     mergefile /workspace/dataset/gpt2merges.txt \     split 949,50,1 \     loginterval 100 \     saveinterval 10000 \     evalinterval 1000 \     evaliters 0 \     save /workspace/checkpoints \     load /workspace/checkpoints "" torchrun \ /home/ubuntu/repos/MegatronLM/pretrain_gpt.py \ $ARGS ``` **Expected behavior** The gradient of attention (dQ) in the TE implementation should be the same as single card when TP is on, and should be the same with the `local` implementation. When it comes to bf16, the relative error of the correctly calculated tensor should be at the magnitude of 1e2, but the current implementation produces wrong results whose relative error is more than 1 when comparing with correct results. **Stack trace/logs** If applicable, add the stack trace or logs from the time of the error. **Environment (please complete the following information):**   MegatronLM commit ID: `11996c9f`   PyTorch version: 2.5.1+cu124   CUDA version: 12.4   NCCL version: 2.21.5   TransformerEngine: both 1.13.0 and 2.1.0 from pip install has this problem (`pip install ""transformer_engine[pytorch]""`)   cuDNN: 9.8.0.871 **Proposed fix** See the function call path above. **Additional context** Please fix it soon.  This is serious bug and has been for a long time.",2025-03-26T03:05:41Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1503
ladyrick,fix for group_limited_topk: K_r is moe_router_topk instead of moe_router_num_groups,fix for group_limited_topk: K_r is moe_router_topk instead of moe_router_num_groups discussed in issue: https://github.com/NVIDIA/MegatronLM/issues/1441 related commit: https://github.com/NVIDIA/MegatronLM/commit/e85385a761e09f1f08ea309c6b6858bf1961a56f,2025-03-25T08:44:45Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1502
SeunghyunSEO,[BUG] kv_channels get wrong for MLA,"https://github.com/NVIDIA/MegatronLM/blob/a4306f75f49424eb8ef845d1a5e8056ec4282cf4/megatron/training/arguments.pyL589L591 https://github.com/NVIDIA/MegatronLM/blob/d61821b7174bac690afbad9134bcb4983521052f/megatron/core/extensions/transformer_engine.pyL706L715 MLA typically uses a much larger number of attention heads than MHA or GQA. e.g. DSV2 has a hidden size of 5120, 128 attention heads, and its kv_channels should be set to 128 + 64 (i.e., nope + rope). However, in this code, kv_channels is computed as 5120/128, which makes the scaling factor of SDPA become 1/sqrt(40) and can cause significant training instability (following OG transformer paper).  And if someone trains DSV2 with MGLM and then converts the model to hf transformers class, its SDPA will behave very differently. ```python self.softmax_scale = self.q_head_dim ** (0.5)  it's not sqrt(5120/128)**0.5, it's sqrt(192)**0.5 if self.config.rope_scaling is not None:         mscale_all_dim = self.config.rope_scaling.get(""mscale_all_dim"", 0)         scaling_factor = self.config.rope_scaling[""factor""]         if mscale_all_dim:         mscale = yarn_get_mscale(scaling_factor, mscale_all_dim)         self.softmax_scale = self.softmax_scale * mscale * mscale ```",2025-03-25T04:07:03Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1501,https://github.com/NVIDIA/MegatronLM/blob/a58bbe8d667b1ccbaeb2ab2316eeb80a86457aa0/megatron/core/transformer/multi_latent_attention.pyL71L78 Did you mean this part of the calculation?,"> MegatronLM/megatron/core/transformer/multi_latent_attention.py >  > Lines 71 to 78 in a58bbe8 >  >  self.q_head_dim = self.config.qk_head_dim + self.config.qk_pos_emb_head_dim  >    >   Overwrite the base class kv shape to support MLA inference  >  self.key_hidden_size = self.q_head_dim  >  self.val_hidden_size = self.config.v_head_dim  >    >  mscale = _yarn_get_mscale(self.config.rotary_scaling_factor, self.config.mscale)  >  self.softmax_scale = mscale * mscale / math.sqrt(self.q_head_dim)  > Did you mean this part of the calculation? oh, i didn't realize self.softmax_scale is set in mla class, then it will be ok. ty for correcting me!"
ChenchaoZhao,[BUG] T5 Model does not work when TP size is 1,"**Describe the bug** T5 Model does not work when tp size is 1. It can train with tp size 2, 4, 8. **To Reproduce** The model is defined as ``` def model_provider(     model_size: ModelSize,     mixed_precision: MixedPrecision = MixedPrecision.NO,     tp_size: int = DEFAULT_TP_SIZE,     pp_size: int = DEFAULT_PP_SIZE,     use_te: bool = False,     position_embedding_type: str = ""rope"" ) > T5Model:     """"""Build the model.""""""     half_kwargs = {         MixedPrecision.NO: {             ""fp16"": False,             ""bf16"": False,             ""pipeline_dtype"": torch.float32,             ""params_dtype"": torch.float32,             ""autocast_dtype"": torch.float32,         },         MixedPrecision.FP16: {             ""fp16"": True,             ""bf16"": False,             ""pipeline_dtype"": torch.float16,             ""params_dtype"": torch.float32,             ""autocast_dtype"": torch.float16,         },         MixedPrecision.BF16: {             ""fp16"": False,             ""bf16"": True,             ""pipeline_dtype"": torch.bfloat16,             ""params_dtype"": torch.float32,             ""autocast_dtype"": torch.bfloat16,         },     }     model_size = ModelSize(model_size)     mixed_precision = MixedPrecision(mixed_precision)     transformer_config = TransformerConfig(         num_layers=N_LAYERS[model_size],         hidden_size=D_MODEL[model_size],         num_attention_heads=N_HEADS[model_size],         ffn_hidden_size=D_FF[model_size],         kv_channels=D_KV[model_size],         sequence_parallel=False,         hidden_dropout=0.0,         attention_dropout=0.0,         use_cpu_initialization=True,         rotary_interleaved=False,   llama style         add_bias_linear=False,         tensor_model_parallel_size=tp_size,         pipeline_model_parallel_size=pp_size,         apply_query_key_layer_scaling=True,         attention_softmax_in_fp32=True,         tp_comm_overlap=True,         enable_autocast=True,         **half_kwargs[mixed_precision],     )     logger.info(transformer_config)     if use_te:         encoder_spec_factory = encoder_model_with_transformer_engine_default_spec         decoder_spec_factory = decoder_model_with_transformer_engine_default_spec     else:         encoder_spec_factory = encoder_model_with_local_spec         decoder_spec_factory = decoder_model_with_local_spec     return T5Model(         config=transformer_config,         encoder_config=transformer_config,         transformer_encoder_layer_spec=encoder_spec_factory(),         transformer_decoder_layer_spec=decoder_spec_factory(),         vocab_size=VOCAB_SIZE,         max_sequence_length=MAX_POSITION_LENGTH,         position_embedding_type=position_embedding_type,         parallel_output=False,         share_embeddings_and_output_weights=True,         rotary_percent=1.0,         pre_process=parallel_state.is_pipeline_first_stage(),         post_process=parallel_state.is_pipeline_last_stage(),     ) ``` Then run one step ``` losses_reduced = forward_backward_func(             forward_step_func=forward_step_func,             data_iterator=train_iterator,             model=model,             num_microbatches=num_microbatches,             seq_length=MAX_SEQUENCE_LENGTH,             micro_batch_size=micro_batch_size,             decoder_seq_length=MAX_SEQUENCE_LENGTH,             forward_only=False,         ) ``` **Expected behavior** It should work without adding any TP **Stack trace/logs** ``` [rank6]: Traceback (most recent call last): [rank6]:   File """", line 198, in _run_module_as_main [rank6]:   File """", line 88, in _run_code [rank6]:   File ""/local/home/cczhao/work/megatronlm/src/TroiMegatronLM/src/amzn_troi_megatron_lm/pretrain_t5.py"", line 564, in  [rank6]:     Fire(main) [rank6]:   File ""/local/home/cczhao/venv/megatron/lib/python3.11/sitepackages/fire/core.py"", line 135, in Fire [rank6]:     component_trace = _Fire(component, args, parsed_flag_args, context, name) [rank6]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank6]:   File ""/local/home/cczhao/venv/megatron/lib/python3.11/sitepackages/fire/core.py"", line 468, in _Fire [rank6]:     component, remaining_args = _CallAndUpdateTrace( [rank6]:                                 ^^^^^^^^^^^^^^^^^^^^ [rank6]:   File ""/local/home/cczhao/venv/megatron/lib/python3.11/sitepackages/fire/core.py"", line 684, in _CallAndUpdateTrace [rank6]:     component = fn(*varargs, **kwargs) [rank6]:                 ^^^^^^^^^^^^^^^^^^^^^^ [rank6]:   File ""/local/home/cczhao/work/megatronlm/src/TroiMegatronLM/src/amzn_troi_megatron_lm/pretrain_t5.py"", line 497, in main [rank6]:     losses_reduced = forward_backward_func( [rank6]:                      ^^^^^^^^^^^^^^^^^^^^^^ [rank6]:   File ""/local/home/cczhao/venv/megatron/lib/python3.11/sitepackages/megatron/core/pipeline_parallel/schedules.py"", line 470, in forward_backward_no_pipelining [rank6]:     output_tensor, num_tokens = forward_step( [rank6]:                                 ^^^^^^^^^^^^^ [rank6]:   File ""/local/home/cczhao/venv/megatron/lib/python3.11/sitepackages/megatron/core/pipeline_parallel/schedules.py"", line 274, in forward_step [rank6]:     output_tensor, loss_func = forward_step_func(data_iterator, model) [rank6]:                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank6]:   File ""/local/home/cczhao/work/megatronlm/src/TroiMegatronLM/src/amzn_troi_megatron_lm/pretrain_t5.py"", line 292, in forward_step_func [rank6]:     output_tensor = model(**batch) [rank6]:                     ^^^^^^^^^^^^^^ [rank6]:   File ""/local/home/cczhao/venv/megatron/lib/python3.11/sitepackages/torch/nn/modules/module.py"", line 1739, in _wrapped_call_impl [rank6]:     return self._call_impl(*args, **kwargs) [rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank6]:   File ""/local/home/cczhao/venv/megatron/lib/python3.11/sitepackages/torch/nn/modules/module.py"", line 1750, in _call_impl [rank6]:     return forward_call(*args, **kwargs) [rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank6]:   File ""/local/home/cczhao/venv/megatron/lib/python3.11/sitepackages/megatron/core/models/T5/t5_model.py"", line 272, in forward [rank6]:     encoder_input = self.embedding( [rank6]:                     ^^^^^^^^^^^^^^^ [rank6]:   File ""/local/home/cczhao/venv/megatron/lib/python3.11/sitepackages/torch/nn/modules/module.py"", line 1739, in _wrapped_call_impl [rank6]:     return self._call_impl(*args, **kwargs) [rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank6]:   File ""/local/home/cczhao/venv/megatron/lib/python3.11/sitepackages/torch/nn/modules/module.py"", line 1750, in _call_impl [rank6]:     return forward_call(*args, **kwargs) [rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank6]:   File ""/local/home/cczhao/venv/megatron/lib/python3.11/sitepackages/megatron/core/models/common/embeddings/language_model_embedding.py"", line 106, in forward [rank6]:     word_embeddings = self.word_embeddings(input_ids) [rank6]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank6]:   File ""/local/home/cczhao/venv/megatron/lib/python3.11/sitepackages/torch/nn/modules/module.py"", line 1739, in _wrapped_call_impl [rank6]:     return self._call_impl(*args, **kwargs) [rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank6]:   File ""/local/home/cczhao/venv/megatron/lib/python3.11/sitepackages/torch/nn/modules/module.py"", line 1750, in _call_impl [rank6]:     return forward_call(*args, **kwargs) [rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank6]:   File ""/local/home/cczhao/venv/megatron/lib/python3.11/sitepackages/megatron/core/tensor_parallel/layers.py"", line 260, in forward [rank6]:     output_parallel = F.embedding(masked_input, self.weight) [rank6]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank6]:   File ""/local/home/cczhao/venv/megatron/lib/python3.11/sitepackages/torch/nn/functional.py"", line 2551, in embedding [rank6]:     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse) [rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank6]: RuntimeError: CUDA error: deviceside assert triggered [rank6]: Compile with `TORCH_USE_CUDA_DSA` to enable deviceside assertions. ``` **Environment (please complete the following information):**   MegatronLM v0.10.0   PyTorch 2.6   CUDA version 12.4   NCCL version 2.21.5 **Proposed fix** If you have a proposal for how to fix the issue state it here or link to a PR. **Additional context** Add any other context about the problem here.",2025-03-24T20:44:40Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1500
RandMist,fix: MultiLatentAttention cp_comm_type,The MultiLatentAttention class does not correctly pass the cp_comm_type variable.,2025-03-24T11:53:04Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1499
Taishi-N324,[ENHANCEMENT] Global Batch Load Balancing for MoE Models,**Is your feature request related to a problem? Please describe.** A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] **Describe the solution you'd like** Implement globalbatch level load balancing.  Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized MixtureofExpert Models **Benefits** Based on the paper   Improves pretraining perplexity by ~0.1  Increases benchmark scores by ~2 points  Enables interpretable domain specialization of experts,2025-03-23T18:26:54Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1498
ZetangForward,[BUG] Can not load _extra_state with TorchDistLoadShardedStrategy,"**Describe the bug** I used the `TorchDistLoadShardedStrategy` loading strategy to load model weights in the `distcp` format. There are two different formats involved: `ShardedTensor` and `ShardedObject`. The former stores sliced weights, while the latter contains `_extra_state`, which holds some FP8related information about the weights. The issue is that when the `TorchDistLoadShardedStrategy` attempts to read a `ShardedObject`, it throws an error. !Image I notice the code here: ``` checkpoint.load(         pyt_state_dict,         FileSystemReader(checkpoint_dir),         planner=MCoreLoadPlanner(             shapes_validation_sharded_tensors=flexible_shape_sharded_tensors         ),     ) ``` Megatron uses a binary method to read model weights, loading the results from `checkpoint_dir` and overwriting them into `sharded_state_dict`. However, it seems that it can only read content of the `ShardedTensor` type and is unable to read the contents stored in `ShardedObject`. This results in the `extra_state_dict` being loaded as some `io.BytesIO` objects instead of the expected structured data.",2025-03-23T05:07:15Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1497
mdy666,[QUESTION] fp8 can not set pp>1,"**Your question** use fp8 and set pp_size=4, it raise a error. ```bash [before the start of training step] datetime: 20250322 18:11:11 [rank0]: Traceback (most recent call last): [rank0]:   File ""/sharedata/mdy/code/repo/MegatronLM/pretrain_gpt.py"", line 319, in  [rank0]:     pretrain( [rank0]:   File ""/sharedata/mdy/code/repo/MegatronLM/megatron/training/training.py"", line 417, in pretrain [rank0]:     iteration, num_floating_point_operations_so_far = train( [rank0]:                                                       ^^^^^^ [rank0]:   File ""/sharedata/mdy/code/repo/MegatronLM/megatron/training/training.py"", line 1569, in train [rank0]:     train_step(forward_step_func, [rank0]:   File ""/sharedata/mdy/code/repo/MegatronLM/megatron/training/training.py"", line 850, in train_step [rank0]:     losses_reduced = forward_backward_func( [rank0]:                      ^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/sharedata/mdy/code/repo/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1829, in forward_backward_pipelining_without_interleaving [rank0]:     input_tensor_grad = backward_step( [rank0]:                         ^^^^^^^^^^^^^^ [rank0]:   File ""/sharedata/mdy/code/repo/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 368, in backward_step [rank0]:     custom_backward(output_tensor[0], output_tensor_grad[0]) [rank0]:   File ""/sharedata/mdy/code/repo/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 152, in custom_backward [rank0]:     Variable._execution_engine.run_backward( [rank0]:   File ""/sharedata/mdy/miniforge/lib/python3.12/sitepackages/torch/autograd/function.py"", line 307, in apply [rank0]:     return user_fn(self, *args) [rank0]:            ^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/sharedata/mdy/miniforge/lib/python3.12/sitepackages/transformer_engine/pytorch/module/linear.py"", line 419, in backward [rank0]:     ) = TransformerEngineBaseModule.grad_output_preprocess( [rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/sharedata/mdy/miniforge/lib/python3.12/sitepackages/transformer_engine/pytorch/module/base.py"", line 882, in grad_output_preprocess [rank0]:     grad_output_c, grad_output_t = fp8_cast_transpose_fused( [rank0]:                                    ^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/sharedata/mdy/miniforge/lib/python3.12/sitepackages/transformer_engine/pytorch/cpp_extensions/transpose.py"", line 60, in fp8_cast_transpose_fused [rank0]:     tex.fused_cast_transpose_noop( [rank0]: RuntimeError: /TransformerEngine/transformer_engine/common/transpose/../util/rtc.h:63 in function launch: CUDA Error: invalid device context ```",2025-03-22T10:13:47Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1496
ETOgaosion,[Bug Fix] fix p2p communication order error and stuck problems when pp 2 and vpp 2 with remove pad,"Bug issue connects to CC([BUG] p2p communication order error and stuck when pp 2 and vpp 2 with remove pad) , reopen CC([Bug Fix] fix p2p communication order error and stuck problems when pp 2 and vpp 2 with remove pad) PR  Environent and Configuration When use `PP=2` and `VPP=2` with `config.variable_seq_lengths=True`, `config.batch_p2p_comm=True` and `config.overlap_p2p_comm=False`, current implementation of p2p_communication.py will cause incorrect behavior. If we set `config.overlap_p2p_comm=True` and `config.batch_p2p_comm=False`, bug disappear.  Bug 1: Communication Misorder  Description and Analysis Like this image below: !vpp After 2 devices finish at the dashed time, Device 1 should pass `output_tensor` and `input_tensor_grad` to Device 2, and because world size is 2, both devices have the same `next_rank` and `prev_rank`, the original ring communication becomes intercommunication, thus cause conflicts in p2p_communication. In detail, Device 1 passes `output_tensor` to `next_rank` and `input_tensor_grad` to `prev_rank`, and Device 2 receives `output_tensor_grad` from `next_rank` and `input_tensor` from `prev_rank`. if we use the original implementation of `_communicate_shapes`: ```py if send_prev_shape_tensor is not None:     send_prev_op = torch.distributed.P2POp(         torch.distributed.isend,         send_prev_shape_tensor,         get_pipeline_model_parallel_prev_rank(),     )     ops.append(send_prev_op) if recv_prev_shape_tensor is not None:     recv_prev_op = torch.distributed.P2POp(         torch.distributed.irecv,         recv_prev_shape_tensor,         get_pipeline_model_parallel_prev_rank(),     )     ops.append(recv_prev_op) if send_next_shape_tensor is not None:     send_next_op = torch.distributed.P2POp(         torch.distributed.isend,         send_next_shape_tensor,         get_pipeline_model_parallel_next_rank(),     )     ops.append(send_next_op) if recv_next_shape_tensor is not None:     recv_next_op = torch.distributed.P2POp(         torch.distributed.irecv,         recv_next_shape_tensor,         get_pipeline_model_parallel_next_rank(),     )     ops.append(recv_next_op) ``` If we assume that the sendandsend, or recvandrecv operation are in order when using `torch.distributed.batch_isend_irecv` with the same destination rank, Device 0 will send `input_tensor_grad.shape` to first Device 1, then send `output_tensor.shape` to 1. Meanwhile Device 1 receives `input_tensor.shape` first from Device 0, then `output_tensor_grad.shape` from 0. That is to say, ```py input_tensor_gpu1 = input_tensor_grad_gpu0 output_tensor_grad_gpu1 = output_tensor_gpu0 ``` This cause backward tensor shape mismatch: `Mismatch in shape: grad_output[0] has a shape of torch.Size([3131, 1, 3584]) and output[0] has a shape of torch.Size([3204, 1, 3584])` Here is more log: ```txt  Device 0 send_prev_shape_tensor: torch.Tensor([1673, 1, 3840], device='cuda:0'), send_next_shape_tensor: torch.Tensor([1702, 1, 3840], device='cuda:0') recv_prev_shape_tensor: torch.Tensor([], device='cuda:0'), recv_next_shape_tensor: torch.Tensor([1664, 1, 3840], device='cuda:0')  Device 1 send_prev_shape_tensor: torch.Tensor([1664, 1, 3840], device='cuda:0'), send_next_shape_tensor: torch.Tensor([1653, 1, 3840], device='cuda:0') recv_prev_shape_tensor: torch.Tensor([1673, 1, 3840], device='cuda:0'), recv_next_shape_tensor: torch.Tensor([1702, 1, 3840], device='cuda:0')  Reverse Error ```  Solution We notice that the `_p2p_op` has the logic for cases where `world size % 2 == 0`, so we use this for shape communication. The solution is change order of recv tensors: `send_prev`, `recv_next`, `send_next`, `recv_prev`  Bug 2: P2P hangs After we fix Bug 1, because we use `config.batch_p2p_comm=True`, so `_batched_p2p_ops` is called and hangs. No clear reason about why it hangs, but if we refactor its logic like above, we successfully fix this.  Refactor We can reuse `_batch_p2p_ops` and `_p2p_ops` here to support `communicate_shapes`, codes are more clean. This solution is just a proposal.",2025-03-22T06:52:20Z,,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1495,"I actually meet the same problem, Any idea about it?"
arjun-choudhry,[BUG] Load Balancing loss discrepancy with/without CUDA Graphs,"**Describe the bug** I am seeing discrepancy in reduction values with and without CUDA graphs turned on. Can you also please look into why that discrepancy happens? Changing this to the following (to add print statements): ``` def reduce_aux_losses_tracker_across_ranks():     """"""Collect and reduce the auxiliary losses across ranks.""""""     tracker = parallel_state.get_moe_layer_wise_logging_tracker()     aux_losses = {k: v['values'].float() for k, v in tracker.items()}     if torch.distributed.get_rank() == 0:         print(f""first {aux_losses=}"")     for name in tracker:         values = tracker[name][""values""]          Collect aux losses across PP.         torch.distributed.all_reduce(             values, group=parallel_state.get_pipeline_model_parallel_group()         )         if torch.distributed.get_rank() == 0:             print(f""{values=}"")          Reduce aux losses across ranks.         if tracker[name].get('reduce_group') is not None:             torch.distributed.all_reduce(values, group=tracker[name].get('reduce_group'))             if torch.distributed.get_rank() == 0:                 print(f""reduced vals {values=}"")         if tracker[name].get('avg_group') is not None:             torch.distributed.all_reduce(                 values, group=tracker[name]['avg_group'], op=torch.distributed.ReduceOp.AVG             ) ``` I see the following results with and without cuda graphs enabled: with CUDA graphs: ``` first aux_losses={'z_loss': tensor([0.0000, 1.0703, 0.0000, 0.9629, 0.8457, 0.6562, 0.9121, 1.1836],        device='cuda:0'), 'load_balancing_loss': tensor([0.0000, 1.0056, 0.0000, 1.0083, 1.0372, 1.1118, 1.0406, 1.0901],        device='cuda:0')} values=tensor([0.0000, 1.0703, 0.0000, 0.9629, 0.8457, 0.6562, 0.9121, 1.1836],        device='cuda:0') values=tensor([0.0000, 1.0056, 0.0000, 1.0083, 1.0372, 1.1118, 1.0406, 1.0901],        device='cuda:0') ``` without CUDA graphs: ``` first aux_losses={'z_loss': tensor([0.0000, 1.0703, 0.0000, 0.9629, 0.8457, 0.6592, 0.9102, 1.1797],        device='cuda:0'), 'load_balancing_loss': tensor([0.0000, 1.0056, 0.0000, 1.0083, 1.0370, 1.1117, 1.0406, 1.0903],        device='cuda:0')} values=tensor([0.0000, 1.0703, 0.0000, 0.9629, 0.8457, 0.6592, 0.9102, 1.1797],        device='cuda:0') values=tensor([0.0000, 1.0056, 0.0000, 1.0083, 1.0370, 1.1117, 1.0406, 1.0903],        device='cuda:0') reduced vals values=tensor([0.0000, 2.0175, 0.0000, 2.0102, 2.0742, 2.2586, 2.1155, 2.1638],        device='cuda:0') ``` As can be seen, there is a doubling of the final reduced values btw the 2 implementations. This doubling is with TP=2. I assume that the value will be 'n' times depending on the TP value.  I enabled CUDA graphs using the following flag: ```   use_te_rng_tracker: True   enable_cuda_graph: True ``` **To Reproduce** I am using the `from megatron.core.models.gpt import GPTModel` with the following moe configs: ```   num_layers: 8   moe_layer_freq: [0, 1, 0, 1, 1, 1, 1, 1]   moe_use_legacy_grouped_gemm: False   use_te_rng_tracker: True   enable_cuda_graph: True ``` and the following 5d parallel configs: ``` context_parallel_size: 1 tensor_model_parallel_size: 2 pipeline_model_parallel_size: 1 num_moe_experts: 2 expert_model_parallel_size: 2 ``` I hope that helps with the repro. Happy to provide any additional specific attribute values.  For the non graph version, I unset the `use_te_rng_tracker` and `enable_cuda_graph` to False.  **Expected behavior** The load balancing loss should be the same btw the 2 settings. **Environment (please complete the following information):** Version info: MegatronLM.git NeMo.git TransformerEngine.git **Additional context**  What I have found out uptil now is that this is because when CUDA graphs is enabled, the code never comes within the `if` condition here, which makes me believe that there is some discrepancy in the groups here  To provide more context, the numbers for the vals are same for the first batch, but then start differing from the 2nd batch onwards. I presume this is because in the 1st batch, CUDA graphs arent used. ",2025-03-21T19:22:25Z,,open,0,4,https://github.com/NVIDIA/Megatron-LM/issues/1494,"I believe the error lies in this line, as the python list objects aren't CUDA graphable (with it only caching tensors for replayability). Hence, when it comes here , it never enters into the `if` condition. To circumvent this, I have changed the following lines with: ```  val_loss = loss.detach()  if reduce_group:  torch.distributed.all_reduce(val_loss, group=reduce_group)  tracker[name][""values""][layer_number  1] += val_loss  Aggregate the loss for the layer.  tracker[name][""reduce_group""] = None ``` such that we reduce the values at the same time we save them, instead of caching the groups and reducing them when reduce_aux_losses_tracker_across_ranks is invoked. Post this, I see consistent numbers across different PP/TP/CP configurations. ",Updated the above logic to be more generic and maintain functional parity. PR linked!, Can you please take a look at the issue + PR for the fix.,Thanks choudhry for the fix! Let us take a look
cafeii,"[BUG] Failed to install megatron_core:  Could not find a version that satisfies the requirement nvidia-resiliency-ext; platform_machine == ""x86_64""","**Describe the bug** Hi, I met the error when I'm trying to build megatron_core==0.11.2 from source: ERROR: Could not find a version that satisfies the requirement nvidiaresiliencyext; platform_machine == ""x86_64"" (from megatroncore) (from versions: none) ERROR: No matching distribution found for nvidiaresiliencyext; platform_machine == ""x86_64"" **To Reproduce** ``` git clone b core_r0.11.0 https://github.com/NVIDIA/MegatronLM.git cd MegatronLM/ pip3 install e . ``` **Stack trace/logs** ``` Looking in indexes: https://pypi.org/simple/, https://mirrors.aliyun.com/pypi/simple/, https://pypi.tuna.tsinghua.edu.cn/simple/, https://pypi.mirrors.ustc.edu.cn/simple/ Obtaining file:///home/lzc/workspace/RFTuning/verl/MegatronLM   Installing build dependencies ... done   Checking if build backend supports build_editable ... done   Getting requirements to build editable ... done   Preparing editable metadata (pyproject.toml) ... done Requirement already satisfied: einops in /home/lzc/miniconda3/envs/verl/lib/python3.9/sitepackages (from megatroncore==0.11.2) (0.8.1) Collecting flaskrestful (from megatroncore==0.11.2)   Using cached https://mirrors.ustc.edu.cn/pypi/packages/d7/7b/f0b45f0df7d2978e5ae51804bb5939b7897b2ace24306009da0cc34d8d1f/Flask_RESTful0.3.10py2.py3noneany.whl (26 kB) Collecting nltk (from megatroncore==0.11.2)   Using cached https://mirrors.ustc.edu.cn/pypi/packages/4d/66/7d9e26593edda06e8cb531874633f7c2372279c3b0f46235539fe546df8b/nltk3.9.1py3noneany.whl (1.5 MB) Collecting pytest (from megatroncore==0.11.2)   Using cached https://mirrors.ustc.edu.cn/pypi/packages/30/3d/64ad57c803f1fa1e963a7946b6e0fea4a70df53c1a7fed304586539c2bac/pytest8.3.5py3noneany.whl (343 kB) Collecting pytest_asyncio (from megatroncore==0.11.2)   Using cached https://mirrors.ustc.edu.cn/pypi/packages/67/17/3493c5624e48fd97156ebaec380dcaafee9506d7e2c46218ceebbb57d7de/pytest_asyncio0.25.3py3noneany.whl (19 kB) Collecting pytestcov (from megatroncore==0.11.2)   Using cached https://mirrors.ustc.edu.cn/pypi/packages/36/3b/48e79f2cd6a61dbbd4807b4ed46cb564b4fd50a76166b1c4ea5c1d9e2371/pytest_cov6.0.0py3noneany.whl (22 kB) Collecting pytest_mock (from megatroncore==0.11.2)   Using cached https://mirrors.ustc.edu.cn/pypi/packages/f2/3b/b26f90f74e2986a82df6e7ac7e319b8ea7ccece1caec9f8ab6104dc70603/pytest_mock3.14.0py3noneany.whl (9.9 kB) Collecting pytestrandomorder (from megatroncore==0.11.2)   Using cached https://mirrors.ustc.edu.cn/pypi/packages/91/02/944cf846bcd6027a1805c69fec90581f916e99ccafcbe409ae6c76833255/pytest_random_order1.1.1py3noneany.whl (11 kB) Requirement already satisfied: sentencepiece in /home/lzc/miniconda3/envs/verl/lib/python3.9/sitepackages (from megatroncore==0.11.2) (0.2.0) Requirement already satisfied: tiktoken in /home/lzc/miniconda3/envs/verl/lib/python3.9/sitepackages (from megatroncore==0.11.2) (0.9.0) Collecting wrapt (from megatroncore==0.11.2)   Using cached https://mirrors.ustc.edu.cn/pypi/packages/bf/bb/d552bfe47db02fcfc950fc563073a33500f8108efa5f7b41db2f83a59028/wrapt1.17.2cp39cp39manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB) Collecting zarr (from megatroncore==0.11.2)   Using cached https://mirrors.ustc.edu.cn/pypi/packages/5d/bd/8d881d8ca6d80fcb8da2b2f94f8855384daf649499ddfba78ffd1ee2caa3/zarr2.18.2py3noneany.whl (210 kB) Requirement already satisfied: wandb in /home/lzc/miniconda3/envs/verl/lib/python3.9/sitepackages (from megatroncore==0.11.2) (0.19.8) Collecting tensorstore!=0.1.46,!=0.1.72 (from megatroncore==0.11.2)   Using cached https://mirrors.ustc.edu.cn/pypi/packages/09/33/1b24d6eb58d9b50f1600eb500c020d4c87da43637d9ab1ee134e4d7d7dc0/tensorstore0.1.69cp39cp39manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.5 MB) Requirement already satisfied: torch in /home/lzc/miniconda3/envs/verl/lib/python3.9/sitepackages (from megatroncore==0.11.2) (2.4.0+cu124) Requirement already satisfied: nvidiamodelopt>=0.19.0 in /home/lzc/miniconda3/envs/verl/lib/python3.9/sitepackages (from nvidiamodelopt[torch]>=0.19.0; sys_platform != ""darwin"">megatroncore==0.11.2) (0.25.0) INFO: pip is looking at multiple versions of megatroncore to determine which version is compatible with other requirements. This could take a while. ERROR: Could not find a version that satisfies the requirement nvidiaresiliencyext; platform_machine == ""x86_64"" (from megatroncore) (from versions: none) ERROR: No matching distribution found for nvidiaresiliencyext; platform_machine == ""x86_64"" ``` **Environment (please complete the following information):**   MegatronLM version v0.11.0   PyTorch version 2.4.0   CUDA version 12.4   NCCL version 2.20.5 **Additional context** However, I'm able to build megatron_core==0.4.0 successfully",2025-03-21T10:18:03Z,,closed,0,5,https://github.com/NVIDIA/Megatron-LM/issues/1493,Are there any other info that I need to provide for this bug?,"> MegatronLM commit ID https://github.com/NVIDIA/MegatronLM/pull/1485 Commit CC([Bug Fix] fix p2p communication order error and stuck problems when pp 2 and vpp 2 with remove pad) move to CC([Bug Fix] fix p2p communication order error and stuck problems when pp 2 and vpp 2 with remove pad) , use another branch of my own repo","> > MegatronLM commit ID [ CC([Bug Fix] fix p2p communication order error and stuck problems when pp 2 and vpp 2 with remove pad)](https://github.com/NVIDIA/MegatronLM/pull/1485) >  > Commit [ CC([Bug Fix] fix p2p communication order error and stuck problems when pp 2 and vpp 2 with remove pad)](https://github.com/NVIDIA/MegatronLM/pull/1485) move to [ CC([Bug Fix] fix p2p communication order error and stuck problems when pp 2 and vpp 2 with remove pad)](https://github.com/NVIDIA/MegatronLM/pull/1495) , use another branch of my own repo Thanks for your solution. I tried it, but the bug still exists. Probably it is a bug inside v0.11.0, not your PR.",this problem was solved by installing from pypi: `pip install megatroncore`,"> this problem was solved by installing from pypi: `pip install megatroncore` Hi, I encountered a dependency issue with nvidiaresiliencyext while installing Megatron. I wanted to ask if it's possible to directly install megatroncore using pip install megatroncore without first cloning the MegatronLM repository?"
zhuzilin,Fix llama_mistral loader by using args.true_vocab_size,"The origin llama mistral loader will ignore the `truevocabsize` passed and instead using the vocab size from the tokenizer. However, for huggingface checkpoints, there could be difference between the actual vocab size and the value within the tokenizer. Thank you for your time on reviewing this PR :)",2025-03-20T02:24:38Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1491
wplf,[QUESTION] Does torch_distã€torch_dcp support DP->TP ckpt conversion?,"Hi, thank you for great repo. I'm developing based on Mcore 0.7.0 and using legacy ckpt format. In the latest version, many ckpt format like torch_distã€torch_dcp has beed supported. Torch_dist will get `__0_0.distcp` and ""common.pt"". Legacy dist will get `model_optim_rng.pt`. Does torch_dist and torch_dcp format support DP>TP/PP conversion?",2025-03-19T08:13:14Z,,closed,0,11,https://github.com/NVIDIA/Megatron-LM/issues/1490,"My test shows dist_ckpt is fine when TP size == 1, but not fine when TP size == 2. When TP size == 2, let's assume that  Correct weight is W0 and W1  Dist weight is w0 and w1. My result shows  w0 == W0 == w1,  w1 != W1.  !Image TP training is not supported by dist ckpt.",Sorry about that. This may be my own problem.,"> Sorry about that. This may be my own problem. Hi, have you solved this problem? I have a similar problem. I save a TP 4 dist ckpt and want to directly load this ckpt into a TP 1 model, but it fails. It comes ``` megatron.core.dist_checkpointing.core.CheckpointingException: Global shape mismatch for loaded (torch.Size([1152, 1536])) and expected ((4608, 1536)) tensor for key layers.0.self_attention.wqkv.weight ```","Yes, I've solved this problem.","> Yes, I've solved this problem. do you know how it transforms the meta ckpt shape into the sharded ckpt shape? can you point me a file?",This is pretty dirty. You need to convert your dist ckpt to legacy ckpt. Then convert **dp legacy ckpt** to **tp legacy ckpt**.  Then load your **tp legacy ckpt** to training and dump it in dist format. ,"ä½†æ˜¯å¥½åƒMCoreå·²ç»æ”¯æŒäº†è‡ªåŠ¨è½¬åŒ–tpï¼Ÿ Original From: ***@***.***&gt; Date: Fri, Mar 21, 2025 16:27 PM To: ***@***.***&gt;; Cc: ***@***.******@***.***&gt;; Subject: Re: [NVIDIA/MegatronLM] [QUESTION] Does torch_distã€torch_dcp support DP&gt;TP ckpt conversion? (Issue CC([QUESTION] Does torch_distã€torch_dcp support DP>TP ckpt conversion?)) This is pretty dirty.  You need to convert your dist ckpt to legacy ckpt. Then convert dp legacy ckpt to tp legacy ckpt.  Then load your tp legacy ckpt to training and dump it in dist format. â€” Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you commented.Message ID: ***@***.***&gt;   wplf left a comment (NVIDIA/MegatronLM CC([QUESTION] Does torch_distã€torch_dcp support DP>TP ckpt conversion?)) This is pretty dirty.  You need to convert your dist ckpt to legacy ckpt. Then convert dp legacy ckpt to tp legacy ckpt.  Then load your tp legacy ckpt to training and dump it in dist format. â€” Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you commented.Message ID: ***@***.***&gt;",dist ckpt è‡ªåŠ¨è½¬TPä¹ˆ,æˆ‘çš„mcoreæ˜¯0.7.0 å¾ˆè€çš„ç‰ˆæœ¬ï¼Œåº”è¯¥æ˜¯ä¸è¡Œçš„ï¼Œæœ€æ–°çš„æ„Ÿè§‰ä¹Ÿä¸å¤ªè¡Œï¼Ÿ,"æˆ‘é—®äº†å…¶ä»–åŒå­¦ï¼Œå¥½åƒæ˜¯æ”¯æŒçš„ã€‚ä½†æ˜¯æˆ‘ä¹Ÿè·‘ä¸æˆåŠŸã€‚ã€‚ Original From: ***@***.***&gt; Date: Fri, Mar 21, 2025 16:40 PM To: ***@***.***&gt;; Cc: ***@***.******@***.***&gt;; Subject: Re: [NVIDIA/MegatronLM] [QUESTION] Does torch_distã€torch_dcp support DP&gt;TP ckpt conversion? (Issue CC([QUESTION] Does torch_distã€torch_dcp support DP>TP ckpt conversion?)) æˆ‘çš„mcoreæ˜¯0.7.0 å¾ˆè€çš„ç‰ˆæœ¬ï¼Œåº”è¯¥æ˜¯ä¸è¡Œçš„ï¼Œæœ€æ–°çš„æ„Ÿè§‰ä¹Ÿä¸å¤ªè¡Œï¼Ÿ â€” Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you commented.Message ID: ***@***.***&gt;   wplf left a comment (NVIDIA/MegatronLM CC([QUESTION] Does torch_distã€torch_dcp support DP>TP ckpt conversion?)) æˆ‘çš„mcoreæ˜¯0.7.0 å¾ˆè€çš„ç‰ˆæœ¬ï¼Œåº”è¯¥æ˜¯ä¸è¡Œçš„ï¼Œæœ€æ–°çš„æ„Ÿè§‰ä¹Ÿä¸å¤ªè¡Œï¼Ÿ â€” Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you commented.Message ID: ***@***.***&gt;",æˆ‘çš„å®žçŽ°æ–¹æ¡ˆæ˜¯å…ˆè½¬æˆlegacyï¼Œå†è½¬TPï¼Œ å†å¼„å›žæ¥distã€‚ å¦‚æžœæœ‰éœ€è¦çš„è¯å¯ä»¥å†è”ç³»æˆ‘ï¼Œ 975761915 æ˜¯å¾®ä¿¡å·ï½ž
liddk,[QUESTION] What's the key feature of Release 0.11 notes: `Add multi datacenter training support though N/S connection`,**Your question** I'm confused about `Add multi datacenter training support though N/S connection` in https://github.com/NVIDIA/MegatronLM/releases/tag/v0.11.0.  How can we enable it and which codes are related to this feature? I would be very grateful to anyone who can answer this question~,2025-03-18T07:02:52Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1489
kminsoo,[QUESTION] Forward and Backward Wall-Clock Time Comparison,"First of all, I really thank you very much for sharing the code.  I am currently conducting experiments using the LLaVA model with `examples/multimodal/train.py`. (LLM = QWEN2.5, ViT = CLIP, projector = some network similar to 2layer MLP) For the experimental setting, I simply make all patameters unfrozen.  Also, I do not empoly the tensor patallel, pipeline parallel, sequence parallel, and context parallel.  Additionally, I do not use overlapparamgather and overlapgradreduce options. Based on the setting, I am measuring the wallclock time for forward and backward passes using the `timingloglevel=2` option, and I have observed that the forward pass takes slightly longer. Do you have any insights into why this is happening?",2025-03-17T13:30:52Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1488
sbwww,[QUESTION] Why we force recompute_granularity = 'selective' when using recompute_activations ? ,Why we force `recompute_granularity = 'selective'` when using `recompute_activations` ? I notice there are two choices `full` and `selective`. Is it recommended to choose `selective`? https://github.com/NVIDIA/MegatronLM/blob/b428f80cd576f0e6a3b526c010c5b6014da69f7e/megatron/training/arguments.pyL292L294,2025-03-17T11:16:55Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1487
ETOgaosion,[Bug Fix] fix p2p communication order error and stuck problems when pp 2 and vpp 2 with remove pad,"Bug issue connects to CC([BUG] p2p communication order error and stuck when pp 2 and vpp 2 with remove pad) , reopen CC([Bug Fix] fix p2p communication order error and stuck problems when pp 2 and vpp 2 with remove pad) PR  Environent and Configuration When use `PP=2` and `VPP=2` with `config.variable_seq_lengths=True`, `config.batch_p2p_comm=True` and `config.overlap_p2p_comm=False`, current implementation of p2p_communication.py will cause incorrect behavior. If we set `config.overlap_p2p_comm=True` and `config.batch_p2p_comm=False`, bug disappear.  Bug 1: Communication Misorder  Description and Analysis Like this image below: !vpp After 2 devices finish at the dashed time, Device 1 should pass `output_tensor` and `input_tensor_grad` to Device 2, and because world size is 2, both devices have the same `next_rank` and `prev_rank`, the original ring communication becomes intercommunication, thus cause conflicts in p2p_communication. In detail, Device 1 passes `output_tensor` to `next_rank` and `input_tensor_grad` to `prev_rank`, and Device 2 receives `output_tensor_grad` from `next_rank` and `input_tensor` from `prev_rank`. if we use the original implementation of `_communicate_shapes`: ```py if send_prev_shape_tensor is not None:     send_prev_op = torch.distributed.P2POp(         torch.distributed.isend,         send_prev_shape_tensor,         get_pipeline_model_parallel_prev_rank(),     )     ops.append(send_prev_op) if recv_prev_shape_tensor is not None:     recv_prev_op = torch.distributed.P2POp(         torch.distributed.irecv,         recv_prev_shape_tensor,         get_pipeline_model_parallel_prev_rank(),     )     ops.append(recv_prev_op) if send_next_shape_tensor is not None:     send_next_op = torch.distributed.P2POp(         torch.distributed.isend,         send_next_shape_tensor,         get_pipeline_model_parallel_next_rank(),     )     ops.append(send_next_op) if recv_next_shape_tensor is not None:     recv_next_op = torch.distributed.P2POp(         torch.distributed.irecv,         recv_next_shape_tensor,         get_pipeline_model_parallel_next_rank(),     )     ops.append(recv_next_op) ``` If we assume that the sendandsend, or recvandrecv operation are in order when using `torch.distributed.batch_isend_irecv` with the same destination rank, Device 0 will send `input_tensor_grad.shape` to first Device 1, then send `output_tensor.shape` to 1. Meanwhile Device 1 receives `input_tensor.shape` first from Device 0, then `output_tensor_grad.shape` from 0. That is to say, ```py input_tensor_gpu1 = input_tensor_grad_gpu0 output_tensor_grad_gpu1 = output_tensor_gpu0 ``` This cause backward tensor shape mismatch: `Mismatch in shape: grad_output[0] has a shape of torch.Size([3131, 1, 3584]) and output[0] has a shape of torch.Size([3204, 1, 3584])` Here is more log: ```txt  Device 0 send_prev_shape_tensor: torch.Tensor([1673, 1, 3840], device='cuda:0'), send_next_shape_tensor: torch.Tensor([1702, 1, 3840], device='cuda:0') recv_prev_shape_tensor: torch.Tensor([], device='cuda:0'), recv_next_shape_tensor: torch.Tensor([1664, 1, 3840], device='cuda:0')  Device 1 send_prev_shape_tensor: torch.Tensor([1664, 1, 3840], device='cuda:0'), send_next_shape_tensor: torch.Tensor([1653, 1, 3840], device='cuda:0') recv_prev_shape_tensor: torch.Tensor([1673, 1, 3840], device='cuda:0'), recv_next_shape_tensor: torch.Tensor([1702, 1, 3840], device='cuda:0')  Reverse Error ```  Solution We notice that the `_p2p_op` has the logic for cases where `world size % 2 == 0`, so we use this for shape communication. The solution is change order of recv tensors: `send_prev`, `recv_next`, `send_next`, `recv_prev`  Bug 2: P2P hangs After we fix Bug 1, because we use `config.batch_p2p_comm=True`, so `_batched_p2p_ops` is called and hangs. No clear reason about why it hangs, but if we refactor its logic like above, we successfully fix this.  Refactor We can reuse `_batch_p2p_ops` and `_p2p_ops` here to support `communicate_shapes`, codes are more clean. This solution is just a proposal.",2025-03-15T05:05:00Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1485
ETOgaosion,[Bug Fix] fix p2p communication order error and stuck problems when pp 2 and vpp 2 with remove pad,"Bug issue connects to CC([BUG] p2p communication order error and stuck when pp 2 and vpp 2 with remove pad) , reopen CC([Bug Fix] fix p2p communication order error and stuck problems when pp 2 and vpp 2 with remove pad) PR  Environent and Configuration When use `PP=2` and `VPP=2` with `config.variable_seq_lengths=True`, `config.batch_p2p_comm=True` and `config.overlap_p2p_comm=False`, current implementation of p2p_communication.py will cause incorrect behavior. If we set `config.overlap_p2p_comm=True` and `config.batch_p2p_comm=False`, bug disappear.  Bug 1: Communication Misorder  Description and Analysis Like this image below: !vpp After 2 devices finish at the dashed time, Device 1 should pass `output_tensor` and `input_tensor_grad` to Device 2, and because world size is 2, both devices have the same `next_rank` and `prev_rank`, the original ring communication becomes intercommunication, thus cause conflicts in p2p_communication. In detail, Device 1 passes `output_tensor` to `next_rank` and `input_tensor_grad` to `prev_rank`, and Device 2 receives `output_tensor_grad` from `next_rank` and `input_tensor` from `prev_rank`. if we use the original implementation of `_communicate_shapes`: ```py if send_prev_shape_tensor is not None:     send_prev_op = torch.distributed.P2POp(         torch.distributed.isend,         send_prev_shape_tensor,         get_pipeline_model_parallel_prev_rank(),     )     ops.append(send_prev_op) if recv_prev_shape_tensor is not None:     recv_prev_op = torch.distributed.P2POp(         torch.distributed.irecv,         recv_prev_shape_tensor,         get_pipeline_model_parallel_prev_rank(),     )     ops.append(recv_prev_op) if send_next_shape_tensor is not None:     send_next_op = torch.distributed.P2POp(         torch.distributed.isend,         send_next_shape_tensor,         get_pipeline_model_parallel_next_rank(),     )     ops.append(send_next_op) if recv_next_shape_tensor is not None:     recv_next_op = torch.distributed.P2POp(         torch.distributed.irecv,         recv_next_shape_tensor,         get_pipeline_model_parallel_next_rank(),     )     ops.append(recv_next_op) ``` If we assume that the sendandsend, or recvandrecv operation are in order when using `torch.distributed.batch_isend_irecv` with the same destination rank, Device 0 will send `input_tensor_grad.shape` to first Device 1, then send `output_tensor.shape` to 1. Meanwhile Device 1 receives `input_tensor.shape` first from Device 0, then `output_tensor_grad.shape` from 0. That is to say, ```py input_tensor_gpu1 = input_tensor_grad_gpu0 output_tensor_grad_gpu1 = output_tensor_gpu0 ``` This cause backward tensor shape mismatch: `Mismatch in shape: grad_output[0] has a shape of torch.Size([3131, 1, 3584]) and output[0] has a shape of torch.Size([3204, 1, 3584])` Here is more log: ```txt  Device 0 send_prev_shape_tensor: torch.Tensor([1673, 1, 3840], device='cuda:0'), send_next_shape_tensor: torch.Tensor([1702, 1, 3840], device='cuda:0') recv_prev_shape_tensor: torch.Tensor([], device='cuda:0'), recv_next_shape_tensor: torch.Tensor([1664, 1, 3840], device='cuda:0')  Device 1 send_prev_shape_tensor: torch.Tensor([1664, 1, 3840], device='cuda:0'), send_next_shape_tensor: torch.Tensor([1653, 1, 3840], device='cuda:0') recv_prev_shape_tensor: torch.Tensor([1673, 1, 3840], device='cuda:0'), recv_next_shape_tensor: torch.Tensor([1702, 1, 3840], device='cuda:0')  Reverse Error ```  Solution We notice that the `_p2p_op` has the logic for cases where `world size % 2 == 0`, so we use this for shape communication. The solution is change order of recv tensors: `send_prev`, `recv_next`, `send_next`, `recv_prev`  Bug 2: P2P hangs After we fix Bug 1, because we use `config.batch_p2p_comm=True`, so `_batched_p2p_ops` is called and hangs. No clear reason about why it hangs, but if we refactor its logic like above, we successfully fix this.  Refactor We can reuse `_batch_p2p_ops` and `_p2p_ops` here to support `communicate_shapes`, codes are more clean. This solution is just a proposal.",2025-03-14T17:37:20Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1484
yzhang123,vscode/cursor devcontainer,add vscode devcontainer for quick getting started with repo ,2025-03-14T15:12:33Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1483
iansheng,[BUG] TopKRouter expert_bias get wrong dtype when updated,"**Describe the bug** Although the buffer `expert_bias` in `TopkRounter` was registered with float32, it is observed to be in bfloat16 format during the forward pass and `finalize_model_grads` process when i pretrain in bf16. The expert_bias definition: https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/transformer/moe/router.pyL115L123 **To Reproduce** Enable `moerouterenableexpertbias` and `moerouterscorefunction sigmoid` ,  and pretrain with `bf16`. > MOE_ARGS="" >     moegroupedgemm \ >     moetokendispatchertype alltoall \ >     moesharedexpertintermediatesize 2048 \ >     numexperts 16 \ >     moeroutertopk 8 \ >     moeffnhiddensize 2048 \ >     moerouterloadbalancingtype seq_aux_loss \ >     moerouterpresoftmax \ >     moeroutertopkscalingfactor 2.5 \ >     moeauxlosscoeff 1e2 \ >     moerouterenableexpertbias \ >     moerouterscorefunction sigmoid \ >     moerouterdtype fp32 **Expected behavior** Expert bias should always use the float32 format, especially it might be updated at a very small rate. **Stack trace/logs** !Image **Environment (please complete the following information):**   MegatronLM main   PyTorch 2.1.0   CUDA 12.6   NCCL 2.22.3 **Proposed fix** If you have a proposal for how to fix the issue state it here or link to a PR. **Additional context** Add any other context about the problem here.",2025-03-14T09:57:28Z,,closed,3,6,https://github.com/NVIDIA/Megatron-LM/issues/1482,"Hi,   can you check this out when you have a moment?",Thanks for reporting. I will take a look.,"`module.bfloat16()` in Float16Module converts the expert_bias to bf16 my solution is override the _apply in TopKRouter class: ```   def recover_fp32(self):       self.expert_bias = self.expert_bias.to(torch.float32)       self.local_tokens_per_expert = self.local_tokens_per_expert.to(torch.float32)   def _apply(self, fn, recurse=True):       super()._apply(fn, recurse)       self.recover_fp32()       return self ```","   Thanks for your helpful feedbacks! I did a small experiment and found when the expert_bias > 0.5, the update will have no effects due to the rounding error of bf16. ``` >>> import torch >>> torch.tensor(0.5, dtype=torch.bfloat16) + 1e3 tensor(0.5000, dtype=torch.bfloat16) ``` I will submit a PR to resolve this issue. BTW, do you have any training loss curve that can be shared to show the influence of expert_bias dtypes? Thanks a lot~","I discovered this bug when I updated this feature, so there is no data to provide",https://github.com/NVIDIA/MegatronLM/commit/8dd03a39ccc67a93c186cfe16fa782ac7144480d Noted the submission. Thanks!
lk137095576,[BUG] cross_entroy_loss_fusion with TE < 2.1 error,"in  language_module.py,  when use cross_entropy_loss_fusion with  TE < 2.1,  te_parallel_cross_entropy will return None,  and cause loss   ERROR.      labels = labels.transpose(0, 1).contiguous()         if self.config.cross_entropy_loss_fusion:             if self.config.cross_entropy_fusion_impl == 'te':                 if te_parallel_cross_entropy is not None:                     labels = torch.as_strided(labels, labels.size(), (labels.size()[1], 1))                     loss = te_parallel_cross_entropy(logits, labels)                 else:                     raise RuntimeError(""Trying to use a TE block when it's not present."")             elif self.config.cross_entropy_fusion_impl == 'native':                 loss = fused_vocab_parallel_cross_entropy(logits, labels)         else:             loss = tensor_parallel.vocab_parallel_cross_entropy(logits, labels",2025-03-14T06:35:44Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1481
wan-nan,Build dataset for all GPUs with tp_rank=0 and pp_rank=0 or -1 in multi-machine training.,"Not just for the GPU with global_rank=0, following the code below. https://github.com/NVIDIA/MegatronLM/blob/28118fcdc22e42621776a021af568ae39c198418/pretrain_gpt.pyL257L260 https://github.com/NVIDIA/MegatronLM/blob/28118fcdc22e42621776a021af568ae39c198418/pretrain_gpt.pyL306L311",2025-03-14T03:55:44Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1480
lcf2610,[QUESTION] How do I use a trained model for inference under the megatron framework?,"I want to use a trained model for inference under the megatron framework. And I had read the example in the ""inference"" file. I found that if I want to use the megatron, the ""model = get_model(model_provider, **kwargs)"" should be called when set up model. My idea is that if I do this for any model, I need to have a deep understanding of the structure of the model. So is there any way to make it easier for me to set up a model? For example, provide a JSON file that describes the model.",2025-03-14T01:37:10Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1479
bugm,"[BUG] recompute leads to incorrect  ""load_balancing_loss""","**Describe the bug** when do recompute in the moe layer,  code in https://github.com/NVIDIA/MegatronLM/blob/f715dd857be63ca6811577baf2192f13211e5216/megatron/core/transformer/moe/router.pyL251 make the  ""save_to_aux_losses_tracker"" called twice ,  which result in double load_balancing_loss value records in logs. should skip it in recompute forward. By the value, it does not set the  ""avg_group"" for  save_to_aux_losses_tracker, which means the  load_balancing_loss value in logs is only from the last rank  and do not average on data parallel group.",2025-03-13T11:29:20Z,,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1478,Was trying to fix the first part in CC(fix a bug in load balancing loss aggregation when recompute is turned on) ,Thanks for reporting the issue. This should be fixed in commit https://github.com/NVIDIA/MegatronLM/commit/e6d56d6828c0773f55772b92b2ec0eed5639665e.
LieeLa,[BUG]Higher peak gpu memory usage when using FP16 optimizer.,"**Describe the bug** When setting 'useprecisionawareoptimizer', the fp16 optimizer(master_param: fp32, exp_avg_sq: fp16, exp_avg: fp16) has a higher peak gpu memory usage when updating parameters than fp32 optimizer. **fp16 optimizer gpu memory usage:**  peak gpu memory: 56.0GiB  !Image **fp32 optimizer gpu memory usage:** peak gpu memory: 49.0GiB !Image **To Reproduce** fp16 optimizer:         useprecisionawareoptimizer \         expavgdtype fp16 \         expavgsqdtype fp16 \ fp32 optimizer:         useprecisionawareoptimizer \ **Expected behavior** A clear and concise description of what you expected to happen. **Stack trace/logs** We have observed that this is because when calling FusedAdam.step(), the fp16 exp_avg and exp_avg_sq are unscaled to float before calling adam_func. However, the fp32 optimizer does not require such a conversion. Why do we need to convert the momentum to float instead of directly using fp16 for updates, as mentioned in the DeepSeekV3 paper? `3965 Addr: b'2dfc5b0000_0, Size: 3.5GiB (3758096384 bytes) allocation, Total memory used after allocation: 56.0GiB (60116956817 bytes), stream 0, timestamp Thu Mar 13 2025 15:17:40 GMT+0800 (ä¸­å›½æ ‡å‡†æ—¶é—´) CUDACachingAllocator.cpp:0:c10::cuda::CUDACachingAllocator::Native::DeviceCachingAllocator::malloc(signed char, unsigned long, CUstream_st*) :0:c10::cuda::CUDACachingAllocator::Native::NativeCachingAllocator::malloc(void**, signed char, unsigned long, CUstream_st*) :0:c10::cuda::CUDACachingAllocator::Native::NativeCachingAllocator::allocate(unsigned long) :0:at::TensorBase at::detail::_empty_strided_generic >(c10::ArrayRef, c10::ArrayRef, c10::Allocator*, c10::DispatchKeySet, c10::ScalarType) ??:0:at::detail::empty_strided_generic(c10::ArrayRef, c10::ArrayRef, c10::Allocator*, c10::DispatchKeySet, c10::ScalarType) ??:0:at::detail::empty_strided_cuda(c10::ArrayRef, c10::ArrayRef, c10::ScalarType, std::optional) ??:0:at::detail::empty_strided_cuda(c10::ArrayRef, c10::ArrayRef, std::optional, std::optional, std::optional, std::optional) ??:0:at::native::empty_strided_cuda(c10::ArrayRef, c10::ArrayRef, std::optional, std::optional, std::optional, std::optional) RegisterCUDA.cpp:0:at::(anonymous namespace)::(anonymous namespace)::wrapper_CUDA__empty_strided(c10::ArrayRef, c10::ArrayRef, std::optional, std::optional, std::optional, std::optional) RegisterCUDA.cpp:0:c10::impl::wrap_kernel_functor_unboxed_, c10::ArrayRef, std::optional, std::optional, std::optional, std::optional), &at::(anonymous namespace)::(anonymous namespace)::wrapper_CUDA__empty_strided>, at::Tensor, c10::guts::typelist::typelist, c10::ArrayRef, std::optional, std::optional, std::optional, std::optional > >, at::Tensor (c10::ArrayRef, c10::ArrayRef, std::optional, std::optional, std::optional, std::optional)>::call(c10::OperatorKernel*, c10::DispatchKeySet, c10::ArrayRef, c10::ArrayRef, std::optional, std::optional, std::optional, std::optional) ??:0:at::_ops::empty_strided::redispatch(c10::DispatchKeySet, c10::ArrayRef, c10::ArrayRef, std::optional, std::optional, std::optional, std::optional) RegisterBackendSelect.cpp:0:c10::impl::wrap_kernel_functor_unboxed_, c10::ArrayRef, std::optional, std::optional, std::optional, std::optional), &at::(anonymous namespace)::empty_strided>, at::Tensor, c10::guts::typelist::typelist, c10::ArrayRef, std::optional, std::optional, std::optional, std::optional > >, at::Tensor (c10::ArrayRef, c10::ArrayRef, std::optional, std::optional, std::optional, std::optional)>::call(c10::OperatorKernel*, c10::DispatchKeySet, c10::ArrayRef, c10::ArrayRef, std::optional, std::optional, std::optional, std::optional) ??:0:at::_ops::empty_strided::call(c10::ArrayRef, c10::ArrayRef, std::optional, std::optional, std::optional, std::optional) :0:at::empty_strided(c10::ArrayRef, c10::ArrayRef, c10::TensorOptions) ??:0:at::native::_to_copy(at::Tensor const&, std::optional, std::optional, std::optional, std::optional, bool, std::optional) RegisterCompositeExplicitAutograd.cpp:0:c10::impl::wrap_kernel_functor_unboxed_, std::optional, std::optional, std::optional, bool, std::optional), &at::(anonymous namespace)::(anonymous namespace)::wrapper_CompositeExplicitAutograd___to_copy>, at::Tensor, c10::guts::typelist::typelist, std::optional, std::optional, std::optional, bool, std::optional > >, at::Tensor (at::Tensor const&, std::optional, std::optional, std::optional, std::optional, bool, std::optional)>::call(c10::OperatorKernel*, c10::DispatchKeySet, at::Tensor const&, std::optional, std::optional, std::optional, std::optional, bool, std::optional) ??:0:at::_ops::_to_copy::redispatch(c10::DispatchKeySet, at::Tensor const&, std::optional, std::optional, std::optional, std::optional, bool, std::optional) RegisterBackendSelect.cpp:0:c10::impl::wrap_kernel_functor_unboxed_, std::optional, std::optional, std::optional, bool, std::optional), &at::(anonymous namespace)::_to_copy>, at::Tensor, c10::guts::typelist::typelist, std::optional, std::optional, std::optional, bool, std::optional > >, at::Tensor (at::Tensor const&, std::optional, std::optional, std::optional, std::optional, bool, std::optional)>::call(c10::OperatorKernel*, c10::DispatchKeySet, at::Tensor const&, std::optional, std::optional, std::optional, std::optional, bool, std::optional) ??:0:at::_ops::_to_copy::redispatch(c10::DispatchKeySet, at::Tensor const&, std::optional, std::optional, std::optional, std::optional, bool, std::optional) VariableType_0.cpp:0:torch::autograd::VariableType::(anonymous namespace)::_to_copy(c10::DispatchKeySet, at::Tensor const&, std::optional, std::optional, std::optional, std::optional, bool, std::optional) VariableType_0.cpp:0:c10::impl::wrap_kernel_functor_unboxed_, std::optional, std::optional, std::optional, bool, std::optional), &torch::autograd::VariableType::(anonymous namespace)::_to_copy>, at::Tensor, c10::guts::typelist::typelist, std::optional, std::optional, std::optional, bool, std::optional > >, at::Tensor (c10::DispatchKeySet, at::Tensor const&, std::optional, std::optional, std::optional, std::optional, bool, std::optional)>::call(c10::OperatorKernel*, c10::DispatchKeySet, at::Tensor const&, std::optional, std::optional, std::optional, std::optional, bool, std::optional) ??:0:at::_ops::_to_copy::call(at::Tensor const&, std::optional, std::optional, std::optional, std::optional, bool, std::optional) ??:0:at::native::to(at::Tensor const&, c10::ScalarType, bool, bool, std::optional) RegisterCompositeImplicitAutograd.cpp:0:c10::impl::wrap_kernel_functor_unboxed_), &at::(anonymous namespace)::(anonymous namespace)::wrapper_CompositeImplicitAutograd_dtype_to>, at::Tensor, c10::guts::typelist::typelist > >, at::Tensor (at::Tensor const&, c10::ScalarType, bool, bool, std::optional)>::call(c10::OperatorKernel*, c10::DispatchKeySet, at::Tensor const&, c10::ScalarType, bool, bool, std::optional) ??:0:at::_ops::to_dtype::call(at::Tensor const&, c10::ScalarType, bool, bool, std::optional) python_variable_methods.cpp:0:torch::autograd::THPVariable_to_type(_object*, c10::ScalarType, std::optional) python_variable_methods.cpp:0:torch::autograd::THPVariable_float(_object*, _object*, _object*) /usr/local/lib/python3.10/distpackages/transformer_engine/pytorch/optimizers/fused_adam.py:304:get_unscaled_state /usr/local/lib/python3.10/distpackages/transformer_engine/pytorch/optimizers/fused_adam.py:556:step /usr/local/lib/python3.10/distpackages/torch/optim/optimizer.py:487:wrapper /workspace/megatronlmbc/megatron/core/optimizer/optimizer.py:452:step_with_ready_grads /usr/local/lib/python3.10/distpackages/torch/utils/_contextlib.py:116:decorate_context ??:0:PyMethod_New workspace/megatronlmbc/megatron/core/optimizer/distrib_optimizer.py:1897:step_with_ready_grads /usr/local/lib/python3.10/distpackages/torch/utils/_contextlib.py:116:decorate_context /workspace/megatronlmbc/megatron/core/optimizer/optimizer.py:1026:step_with_ready_grads /usr/local/lib/python3.10/distpackages/torch/utils/_contextlib.py:116:decorate_context /workspace/megatronlmbc/megatron/core/optimizer/optimizer.py:1067:step ` !Image !Image **Environment (please complete the following information):**   MegatronLM: core_r0.10.0   PyTorch version: 2.5.1   CUDA version: 12.4   transformer_engine version: 2.1.0+450146a **Proposed fix** If you have a proposal for how to fix the issue state it here or link to a PR. **Additional context**",2025-03-13T08:35:35Z,,open,2,0,https://github.com/NVIDIA/Megatron-LM/issues/1477
princepride,[QUESTION]ModuleNotFoundError: No module named 'transformer_engine',"**Your question** I followed the install instruction, run the following code: ``` python tools/preprocess_data.py \        input mycorpus.json \        outputprefix mygpt2 \        vocabfile gpt2vocab.json \        tokenizertype GPT2BPETokenizer \        mergefile gpt2merges.txt \        appendeod \        workers 8 ``` I got this error: Traceback (most recent call last):   File ""/workspace/MegatronLM/tools/preprocess_data.py"", line 25, in      from megatron.training.tokenizer import build_tokenizer   File ""/workspace/MegatronLM/megatron/training/__init__.py"", line 5, in      from .global_vars import get_args   File ""/workspace/MegatronLM/megatron/training/global_vars.py"", line 12, in      from megatron.training.tokenizer import build_tokenizer   File ""/workspace/MegatronLM/megatron/training/tokenizer/__init__.py"", line 4, in      from .tokenizer import build_tokenizer   File ""/workspace/MegatronLM/megatron/training/tokenizer/tokenizer.py"", line 17, in      from megatron.training.tokenizer.multimodal_tokenizer import MultimodalTokenizer   File ""/workspace/MegatronLM/megatron/training/tokenizer/multimodal_tokenizer.py"", line 13, in      from megatron.core.models.multimodal.llava_model import IGNORE_INDEX, IMAGE_TOKEN   File ""/workspace/MegatronLM/megatron/core/models/multimodal/llava_model.py"", line 12, in      from megatron.core.models.gpt import GPTModel   File ""/workspace/MegatronLM/megatron/core/models/gpt/__init__.py"", line 2, in      from .gpt_model import GPTModel   File ""/workspace/MegatronLM/megatron/core/models/gpt/gpt_model.py"", line 15, in      from megatron.core.models.common.language_module.language_module import LanguageModule   File ""/workspace/MegatronLM/megatron/core/models/common/language_module/language_module.py"", line 11, in      from megatron.core.extensions.transformer_engine import te_parallel_cross_entropy   File ""/workspace/MegatronLM/megatron/core/extensions/transformer_engine.py"", line 11, in      import transformer_engine as te ModuleNotFoundError: No module named 'transformer_engine'",2025-03-13T07:57:15Z,,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/1476,"After I run `pip install transformer_engine`, I got this error: Traceback (most recent call last):   File ""/usr/local/lib/python3.11/distpackages/transformer_engine/pytorch/__init__.py"", line 52, in _load_library     so_path = next(so_dir.glob(f""{module_name}.*.{extension}""))               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ StopIteration During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""/workspace/MegatronLM/tools/preprocess_data.py"", line 25, in      from megatron.training.tokenizer import build_tokenizer   File ""/workspace/MegatronLM/megatron/training/__init__.py"", line 5, in      from .global_vars import get_args   File ""/workspace/MegatronLM/megatron/training/global_vars.py"", line 9, in      from megatron.core import Timers   File ""/workspace/MegatronLM/megatron/core/__init__.py"", line 6, in      from megatron.core.distributed import DistributedDataParallel   File ""/workspace/MegatronLM/megatron/core/distributed/__init__.py"", line 5, in      from .distributed_data_parallel import DistributedDataParallel   File ""/workspace/MegatronLM/megatron/core/distributed/distributed_data_parallel.py"", line 10, in      from ..fp8_utils import is_float8tensor   File ""/workspace/MegatronLM/megatron/core/fp8_utils.py"", line 23, in      from transformer_engine.pytorch.float8_tensor import Float8Tensor   File ""/usr/local/lib/python3.11/distpackages/transformer_engine/pytorch/__init__.py"", line 63, in      _load_library()   File ""/usr/local/lib/python3.11/distpackages/transformer_engine/pytorch/__init__.py"", line 55, in _load_library     so_path = next(so_dir.glob(f""{module_name}.*.{extension}""))               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ StopIteration","Markï¼ŒI got the same error. When I use the ""python setup.py install"" to install packages,  some packages are failed install due to the network restrictions. I'm not sure if that's the reason.",same
SeunghyunSEO,[ENHANCEMENT] update theoretical memory usage and tflops per iteration for MLA,"It seems that MoE related terms have been updated recently, but MLA is still not.  so i share my code snippet for update. If you don't mind, I'd be happy to submit a PR.  num_floating_point_operations  compute_weight_and_optimizer_memory ```python def num_floating_point_operations(args, batch_size):      Attention projection size.     query_projection_size = args.kv_channels * args.num_attention_heads     query_projection_to_hidden_size_ratio = query_projection_size / args.hidden_size      Group Query Attention.     if not args.group_query_attention:         args.num_query_groups = args.num_attention_heads      MoE.     num_experts_routed_to = 1 if args.num_experts is None else args.moe_router_topk     gated_linear_multiplier = 3 / 2 if args.swiglu else 1     shared_expert_ffn_hidden_size = (         0         if args.moe_shared_expert_intermediate_size is None         else args.moe_shared_expert_intermediate_size     )     if args.num_experts is not None:         if isinstance(args.moe_layer_freq, int):             moe_layer_pattern = [                 1 if (i % args.moe_layer_freq == 0) else 0 for i in range(args.num_layers)             ]         elif isinstance(args.moe_layer_freq, list):             moe_layer_pattern = args.moe_layer_freq             assert len(moe_layer_pattern) == args.num_layers, (                 f""Invalid length of moe_layer_pattern: {len(moe_layer_pattern)}, ""                 f""expected {args.num_layers}, ""                 f""current moe layer pattern: {args.moe_layer_freq}""             )         num_dense_layers = args.num_layers  sum(moe_layer_pattern)         num_moe_layers = sum(moe_layer_pattern)         moe_ffn_hidden_size = args.moe_ffn_hidden_size     else:         num_dense_layers = args.num_layers         num_moe_layers = 0         moe_ffn_hidden_size = 0     assert num_dense_layers + num_moe_layers == args.num_layers      The 12x term below comes from the following factors; for more details, see      ""APPENDIX: FLOATINGPOINT OPERATIONS"" in https://arxiv.org/abs/2104.04473.       3x: Each GEMM in the model needs to be performed 3 times (forward pass,            backward wgrad [weight gradient], backward dgrad [data gradient]).       2x: GEMMs of a particular size are stacked twice in the standard Transformer model            architectures implemented in this codebase (e.g., h>ffn_h GEMM and ffn_h>h GEMM            in MLP layer).       2x: A GEMM of a m*n tensor with a n*k tensor requires 2mnk floatingpoint operations.     expansion_factor = 3 * 2 * 2     if args.multi_latent_attention:         assert not args.group_query_attention         '''         Basic arithmetic         let B is batch size, s is seq_len, h is embedding dim,         for one self_attnetion block (prenorm is not included)         qkv projection:  6Bsh^2         attn:            2Bs^2h         attn over value: 2Bs^2h         oproj:           2Bsh^2         references         https://arxiv.org/abs/2305.10403         https://arxiv.org/abs/2205.05198         '''          MLA         self_attn_term = (             3*2  fwd(1) + bwd(2) *FMA             * args.num_layers              * (                  q lora + rope + q norm                 args.q_lora_rank                 * (args.hidden_size + args.num_attention_heads * (args.qk_head_dim + args.qk_pos_emb_head_dim) + 1)                   kv lora + rope + kv norm                 + args.kv_lora_rank                 * (args.hidden_size + args.num_attention_heads * (args.qk_head_dim + args.v_head_dim) + 1)                 + args.hidden_size * args.qk_pos_emb_head_dim                  o proj                 + (args.num_attention_heads * args.v_head_dim) * args.hidden_size                  SDPA                 + 2 * args.seq_length * (args.num_attention_heads * (args.qk_head_dim + args.qk_pos_emb_head_dim))  L^2 for (args.num_attention_heads * args.v_head_dim) head dim             )         )     else:          MHA or GQA         self_attn_term = (             expansion_factor             * args.num_layers             * args.hidden_size             * args.hidden_size             * (                 (                     1                     + (args.num_query_groups / args.num_attention_heads)                       Only half of the attention matrix is nonzero and needs to be multiplied with V.                      + (args.seq_length / args.hidden_size / 2)                      rollback to resolve discrepency with previous work                      + (args.seq_length / args.hidden_size)                 ) * query_projection_to_hidden_size_ratio             )         )     output = batch_size * args.seq_length * (          MLP         expansion_factor         * args.num_layers         * args.hidden_size         * (              dense layer (deepseek v2, v3 style)             (                 args.ffn_hidden_size                 * gated_linear_multiplier             ) * (num_dense_layers/args.num_layers)              routed experts             + (                 moe_ffn_hidden_size                 * num_experts_routed_to                 * gated_linear_multiplier             ) * (num_moe_layers/args.num_layers)              Shared Experts.             + (                 shared_expert_ffn_hidden_size                  * gated_linear_multiplier             ) * (num_moe_layers/args.num_layers)         )          Self Attention         + self_attn_term          Logit.         + 3*2         * args.hidden_size         * args.padded_vocab_size      )     return output ``` ```python def compute_weight_and_optimizer_memory(args, verbose=False):      Attention projection size.     query_projection_size = args.kv_channels * args.num_attention_heads     query_projection_to_hidden_size_ratio = query_projection_size / args.hidden_size      Group Query Attention.     if not args.group_query_attention:         args.num_query_groups = args.num_attention_heads      MoE.     num_experts = 1 if args.num_experts is None else args.num_experts     gated_linear_multiplier = 3 / 2 if args.swiglu else 1     shared_expert_ffn_hidden_size = (         0         if args.moe_shared_expert_intermediate_size is None         else args.moe_shared_expert_intermediate_size     )     if args.num_experts is not None:         if isinstance(args.moe_layer_freq, int):             moe_layer_pattern = [                 1 if (i % args.moe_layer_freq == 0) else 0 for i in range(args.num_layers)             ]         elif isinstance(args.moe_layer_freq, list):             moe_layer_pattern = args.moe_layer_freq             assert len(moe_layer_pattern) == args.num_layers, (                 f""Invalid length of moe_layer_pattern: {len(moe_layer_pattern)}, ""                 f""expected {args.num_layers}, ""                 f""current moe layer pattern: {args.moe_layer_freq}""             )         num_dense_layers = args.num_layers  sum(moe_layer_pattern)         num_moe_layers = sum(moe_layer_pattern)         moe_ffn_hidden_size = args.moe_ffn_hidden_size     else:         num_dense_layers = args.num_layers         num_moe_layers = 0         moe_ffn_hidden_size = 0     assert num_dense_layers + num_moe_layers == args.num_layers     if args.multi_latent_attention:         assert not args.group_query_attention         self_attn_term = (                          q lora + rope + q norm             args.q_lora_rank             * (args.hidden_size + args.num_attention_heads * (args.qk_head_dim + args.qk_pos_emb_head_dim) + 1)               kv lora + rope + kv norm             + args.kv_lora_rank             * (args.hidden_size + args.num_attention_heads * (args.qk_head_dim + args.v_head_dim) + 1)             + args.hidden_size * args.qk_pos_emb_head_dim              o proj             + (args.num_attention_heads * args.v_head_dim) * args.hidden_size         )     else:         self_attn_term = (             2             * args.hidden_size             * args.hidden_size             * (                  Attention.                 (                     (1 + (args.num_query_groups / args.num_attention_heads))                     * query_projection_to_hidden_size_ratio                 )             )         )     num_experts_routed_to = 1 if args.num_experts is None else args.moe_router_topk     num_parameters_in_transformer_layers_activated = (         args.num_layers         * (             2              * args.hidden_size             * args.hidden_size             * (                  Dense MoE MLP.                 ((args.ffn_hidden_size / args.hidden_size) * gated_linear_multiplier) * (num_dense_layers/args.num_layers)                  MoE MLP.                 + ((moe_ffn_hidden_size / args.hidden_size) * num_experts_routed_to * gated_linear_multiplier) * (num_moe_layers/args.num_layers)                  Shared MoE MLP.                 + ((shared_expert_ffn_hidden_size / args.hidden_size) * gated_linear_multiplier) * (num_moe_layers/args.num_layers)                  Transformer layernorms.                 + (2 / args.hidden_size)                  Final layernorm.                 + (1 / (args.num_layers * args.hidden_size))             )             + self_attn_term         )     )     num_parameters_in_transformer_layers = (         args.num_layers         * (             2              * args.hidden_size             * args.hidden_size             * (                  Dense MoE MLP.                 ((args.ffn_hidden_size / args.hidden_size) * gated_linear_multiplier) * (num_dense_layers/args.num_layers)                  MoE MLP.                 + ((moe_ffn_hidden_size / args.hidden_size) * num_experts * gated_linear_multiplier) * (num_moe_layers/args.num_layers)                  Shared MoE MLP.                 + ((shared_expert_ffn_hidden_size / args.hidden_size) * gated_linear_multiplier) * (num_moe_layers/args.num_layers)                  Transformer layernorms.                 + (2 / args.hidden_size)                  Final layernorm.                 + (1 / (args.num_layers * args.hidden_size))             )             + self_attn_term         )     )     print(         f""""""Number of parameters in transformer layers in billions:         activated: {num_parameters_in_transformer_layers_activated / 10**9: .2f}         total: {num_parameters_in_transformer_layers / 10**9: .2f}""""""     )     embedding_size = args.hidden_size * args.padded_vocab_size     if args.untie_embeddings_and_output_weights:         num_parameters_in_embedding_layers = 2 * embedding_size     else:         num_parameters_in_embedding_layers = embedding_size     num_total_parameters = num_parameters_in_transformer_layers + num_parameters_in_embedding_layers     if verbose:         print(             f""Number of parameters in transformer layers in billions: ""             f""{num_parameters_in_transformer_layers / 10**9: .2f}""         )         print(             f""Number of parameters in embedding layers in billions: ""             f""{num_parameters_in_embedding_layers / 10**9:.2f}""         )         print(f""Total number of parameters in billions: {num_total_parameters / 10**9:.2f}"")      Most loaded model shard has (1/pp_size transformer layers + 1 embedding layer) / tp_size.     num_parameters_on_most_loaded_model_shard = (         (num_parameters_in_transformer_layers / args.pipeline_model_parallel_size) + embedding_size     ) / args.tensor_model_parallel_size     if args.untie_embeddings_and_output_weights and args.pipeline_model_parallel_size == 1:         num_parameters_on_most_loaded_model_shard += (             embedding_size / args.tensor_model_parallel_size         )     if verbose:         print(             f""Number of parameters in most loaded shard in billions: ""             f""{num_parameters_on_most_loaded_model_shard / 10**9:.4f}""         )     if args.pipeline_model_parallel_size > 1:          Other shards just have (1/pp_size transformer layers) / tp_size.         num_parameters_on_other_model_shards = num_parameters_in_transformer_layers / (             args.pipeline_model_parallel_size * args.tensor_model_parallel_size         )         if verbose:             print(                 f""Number of parameters in other shards in billions: ""                 f""{num_parameters_on_other_model_shards / 10**9:.4f}""             )     num_bytes_per_parameter = (         18 if not args.use_distributed_optimizer else 6 + (12 / args.data_parallel_size)     )     weight_and_optimizer_memory = (         num_parameters_on_most_loaded_model_shard * num_bytes_per_parameter     )     return weight_and_optimizer_memory ```",2025-03-13T05:10:45Z,,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1475,"Thanks so much for your contribution! Please feel free to submit a PR. Unfortunately, we are currently unable to merge PRs on GitHub, but we can merge it internally and list you as a coauthor if that works for you.",
lcf2610,[QUESTION] How does megatron manage clusters?,"When using megatron to train models, many nodes and ranks are used. I want to know how megatron manages these nodes and ranksï¼Œsuch as load balancing. Is the cluster management method implemented by megatron itself or by invoking other interfaces?",2025-03-13T03:55:01Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1474
shcho1118,"[QUESTION] If `cp_size > 1` and packed sequence is used, which code is handling the `cu_seqlens` for RoPE?","**Your question** If `cp_size > 1`, `MegatronLM` will split the sequence and put it into the model. https://github.com/NVIDIA/MegatronLM/blob/f715dd857be63ca6811577baf2192f13211e5216/megatron/core/utils.pyL1604L1634 For the attention operation, `cu_seqlens` is modified to fit the `cp_size` in `Transformer Engine`. (https://github.com/NVIDIA/TransformerEngine/blob/31f32b3777817606ba5fc1e4daf2991288de1725/transformer_engine/pytorch/attention.pyL1664L1686) However, RoPE uses `cu_seqlens` directly from `packed_seq_params`. https://github.com/NVIDIA/MegatronLM/blob/f715dd857be63ca6811577baf2192f13211e5216/megatron/core/transformer/attention.pyL578L588 I think I need to do the same process in RoPE, but do you know which part is handling it? I'm currently experimenting with a slight modification to the code in `MegatronLM`, and getting an error because RoPE's input and `cu_seqlens` don't match.  ``` [rank0]:   File ""/home/bcuser/MegatronLM/megatron/core/models/common/embeddings/rotary_pos_embedding.py"", line 250, in apply_rotary_pos_emb [rank0]:     return apply_rotary_pos_emb_thd( [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/home/bcuser/MegatronLM/megatron/core/models/common/embeddings/rotary_pos_embedding.py"", line 220, in apply_rotary_pos_emb_thd [rank0]:     for x in torch.split(t, seqlens) [rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/opt/conda/lib/python3.11/sitepackages/torch/functional.py"", line 207, in split [rank0]:     return tensor.split(split_size_or_sections, dim) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/opt/conda/lib/python3.11/sitepackages/torch/_tensor.py"", line 983, in split [rank0]:     return torch._VF.split_with_sizes(self, split_size, dim) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]: RuntimeError: split_with_sizes expects split_sizes to sum exactly to 256 (input tensor's size at dimension 0), but got split_sizes=[512] ```",2025-03-13T02:01:23Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1473,"~~There seems to be a RoPE implementation inside `Transformer Engine` that takes context parallel into account, but `MegatronLM` does not use it. (https://github.com/NVIDIA/TransformerEngine/blob/31f32b3777817606ba5fc1e4daf2991288de1725/transformer_engine/pytorch/attention.pyL5052L5119)~~  Never mind, I was using an older version. https://github.com/NVIDIA/MegatronLM/blob/f715dd857be63ca6811577baf2192f13211e5216/megatron/core/extensions/transformer_engine.pyL1302L1329"
jsta,"Set hashlib.md5 usedforsecurity=False, #1471",See Issue CC([BUG] hashlib.md5 called without usedforsecurity=False) ,2025-03-12T18:16:01Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1472
jsta,[BUG] hashlib.md5 called without usedforsecurity=False,"**Describe the bug** On a system with FIPS enabled, calling `hashlib.md5` without `usedforsecurity=False` fails with an error: ```python _hashlib.UnsupportedDigestmodError: [digital envelope routines] unsupported ``` I see two places in MegatronLM where this could be fixed. **To Reproduce** ```python import hashlib src=""asdf"" hashlib.md5(src.encode(""utf8""))  fails ``` **Proposed fix** Set `usedforsecurity=False`. See https://github.com/google/pytype/issues/1844",2025-03-12T18:04:48Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1471
xiaojunjie,Enabling variable_seq_lengths when encoder has Different TP Size,"Commit 8af3dae72a9 allow Encoder to have different TP size,  but the function **_communicate_shapes**  has not been updated yet, which is required for many multimodal models with **variable_seq_lengths = True**",2025-03-12T11:54:53Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1470
bugm,[QUESTION] MOE training meet abnormal gradient norm and loss,"Hello, I am training a MOE model (16B total and 2.5B activated) and below are some tensorboard logs,   **grad norm** !Image !Image **Lm loss** !Image !Image **load_balance_loss** !Image !Image as you can see,  as training going on , loss and gradient turns  abnormalï¼Œhere are some key arguments **the ordinary arguments** ``` lr 4.2e4 minlr 4.2e5 lrdecaystyle cosine weightdecay 0.1 adambeta1 0.9 adambeta2 0.95 clipgrad 1.0 initmethodstd 0.006 attentiondropout 0.0 hiddendropout 0.0 lrdecayiters 381469 lrwarmupiters 2000 trainiters 381469 microbatchsize 2 globalbatchsize 4800 numlayers 28 hiddensize 2048 numattentionheads 16 ffnhiddensize 10944 seqlength 4096 maxpositionembeddings 4096 tensormodelparallelsize 1 pipelinemodelparallelsize 2 contextparallelsize 1  swiglu  normalization RMSNorm  normepsilon 1e6  userotarypositionembeddings  nobiasswiglufusion  noropefusion  positionembeddingtype rope  untieembeddingsandoutputweights  rotarybase 10000  rotaryscalingfactor 40  kvchannels 128  bf16 ``` **arguments related to MOE** ```  qklayernorm  multilatentattention  transformerimpl transformer_engine  usedistributedoptimizer  attentionbackend flash  moeffnhiddensize 1408  moeroutertopk 6  numexperts 64  moelayerfreq 1  moefirstkdensereplace 1  moeauxlosscoeff 0.001  moesharedexpertintermediatesize 2816  expertmodelparallelsize 8  kvlorarank 512  qkheaddim 128  qkposembheaddim 64  vheaddim 128  moetokendispatchertype alltoall_seq  moegroupedgemm  moerouterscorefunction sigmoid  moerouterenableexpertbias  moerouterbiasupdaterate 0.001  moerouterloadbalancingtype seq_aux_loss ``` Can anyone offer some potential reason for the abnormal gradient? Thanks",2025-03-12T10:27:39Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1469
TJ-Solergibert,[QUESTION] How to overlap comms and computation with `--overlap-grad-reduce` & `--overlap-param-gather`?,"Hello Megatron team, Visualising a trace setting `overlapgradreduce` & `overlapparamgather` I observed that we aren't overlapping anything when setting those flags. Im running a 8B model with TP = 1, PP = 1, CP = 1 & DP = 64.  But, when I unset `CUDA_DEVICE_MAX_CONNECTIONS` (In the first image it was set to 1), I can clearly see that we are overlapping comms and computations   In arguments.py we are checking that `CUDA_DEVICE_MAX_CONNECTIONS` as it's a requirement for TP & CP, but shouldn't we check/warn that when setting TP/CP > 1 the overlap features are useless? I think It would be a good idea to check and document this behaviour with `CUDA_DEVICE_MAX_CONNECTIONS` so users can leverage this great optimisations!  I could document this behaviour myself through a PR. Toni ",2025-03-11T16:13:08Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1468
SeunghyunSEO,[QUESTION] reference for `use_shared_expert_gate` ?,"**Your question** Ask a clear and concise question about MegatronLM. hi megatron team, thank you for your contribution as always. I'm curious about the reference for use_shared_expert_gate. i think it makes sense to use this by default, because without this, shared experts and routed experts' output scale is quite different but just want to know where it is from.",2025-03-11T15:49:50Z,,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1467,It comes from QwenMoE models: https://github.com/huggingface/transformers/blob/81aa9b2e07b359cd3555c118010fd9f26c601e54/src/transformers/models/qwen2_moe/modeling_qwen2_moe.pyL621,> It comes from QwenMoE models: https://github.com/huggingface/transformers/blob/81aa9b2e07b359cd3555c118010fd9f26c601e54/src/transformers/models/qwen2_moe/modeling_qwen2_moe.pyL621 thank you for answering!
MaxiBoether,feat: Introduce `GPTDatasetFolder` for easier definition of dataset-based mixtures,,2025-03-11T14:29:03Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1466
i-love-megatron,[BUG] Wrong attention gradient when Context Parallelism is on,"**Describe the bug**  bug location   When training with context parallelism, the gradient of `core_attention` is wrong.  In another word, the dQ, dK, and dV ourput from the backward function is wrong.  To be more specific, this attention backward function outputs wrong results: `TransformerEngine/transformer_engine/pytorch/attention.py: AttnFuncWithCPAndKVP2P.backward()`.  how to trigger   When context parallelism is on and uses this P2P attention module, this bug will be triggered.  bug behavior   Given the exactly same parameter and data input for a single card training and a training with context parallelism, the activation gradient of core attention (dQ, dK, dV) do not match in value.  The relative error of these three tensor can be as large as 1.2.  The single card version is double checked with tesnor parallelism and is correct.     This will cause the following activation gradient to be wrong in the `linear_qkv` module, and its activation gradient will be used to calculate its parameter gradient.  This error will also accumulate and pollute the gradient calculation of following layers.  The wrong parameter gradient will then accumulate with different micro batches, thus making the `main_grad` for weight update to be wrong, and their relative error will be as large as 2.3.   So this bug will make the training with context parallelism totally wrong. **To Reproduce** Train the GPT model with context parallelism. 1. According to https://github.com/NVIDIA/MegatronLM/issues/1151, set environment variable `NVTE_BATCH_MHA_P2P_COMM` to 1 to enable P2P communication. 2. Then run `pretrain_gpt.py` with `contextparallelsize` > 1. 3. Detailed training command:    ```bash    CUDA_DEVICE_MAX_CONNECTIONS=1 \    OMP_NUM_THREADS=1 \    NVTE_BATCH_MHA_P2P_COMM=1 \    torchrun \    nproc_per_node 2 \    nnodes 1 \    node_rank 0 \    master_addr localhost \    master_port 6000 pretrain_gpt.py \    tensormodelparallelsize 1 \    pipelinemodelparallelsize 1 \    contextparallelsize 2 \   as long as cp_size > 1    numlayers 24 \    hiddensize 1024 \    numattentionheads 16 \    seqlength 1024 \    maxpositionembeddings 1024 \    microbatchsize 4 \    globalbatchsize 16 \    lr 0.00015 \    trainiters 1 \    lrdecayiters 320000 \    lrdecaystyle cosine \    minlr 1.0e5 \    weightdecay 1e2 \    lrwarmupfraction .01 \    clipgrad 1.0 \    usemcoremodels \    attentionsoftmaxinfp32 \    attentiondropout 0.0 \    hiddendropout 0.0 \    bf16 \    datapath /workspace/dataset/wikitext/wikitext_text_document \    vocabfile /workspace/dataset/gpt2vocab.json \    mergefile /workspace/dataset/gpt2merges.txt \    split 949,50,1 \    loginterval 100 \    saveinterval 10000 \    evalinterval 1000 \    evaliters 0 \    save /workspace/checkpoints \    load /workspace/checkpoints \    nogradientaccumulationfusion \    transformerimpl transformer_engine    ``` **Expected behavior** The gradient of attention should be the same as single card when context parallelism is on.  When it comes to bf16, the relative error of the correctly calculated tensor should be around 5e3, but the current implementation produces wrong results whose relative error is more than 1 when comparing with correct results. **Stack trace/logs** If applicable, add the stack trace or logs from the time of the error. **Environment (please complete the following information):**   MegatronLM commit ID: `fdfcef87`   PyTorch version: 2.5.1+cu121   CUDA version: 12.1   NCCL version: 2.21.5   TransformerEngine: `754d2a0` (current newest commit in `stable` branch) **Proposed fix** Check the `AttnFuncWithCPAndKVP2P.backward()` and fix the bug in it. **Additional context** Please fix it soon.  This is very serious bug.",2025-03-11T13:02:24Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1465,Solved in issue https://github.com/NVIDIA/TransformerEngine/issues/1557 and PR https://github.com/NVIDIA/TransformerEngine/pull/1539
haitian-jiang,[BUG] Wrong attention gradient when Context Parallelism is on,"**Describe the bug**  bug location   When training with context parallelism, the gradient of `core_attention` is wrong.  In another word, the dQ, dK, and dV ourput from the backward function is wrong.  To be more specific, this attention backward function outputs wrong results: `TransformerEngine/transformer_engine/pytorch/attention.py: AttnFuncWithCPAndKVP2P.backward()`.  how to trigger   When context parallelism is on and uses this P2P attention module, this bug will be triggered.  bug behavior   Given the exactly same parameter and data input for a single card training and a training with context parallelism, the activation gradient of core attention (dQ, dK, dV) do not match in value.  The relative error of these three tensor can be as large as 1.2.  The single card version is double checked with tesnor parallelism and is correct.     This will cause the following activation gradient to be wrong in the `linear_qkv` module, and its activation gradient will be used to calculate its parameter gradient.  This error will also accumulate and pollute the gradient calculation of following layers.  The wrong parameter gradient will then accumulate with different micro batches, thus making the `main_grad` for weight update to be wrong, and their relative error will be as large as 2.3.   So this bug will make the training with context parallelism totally wrong. **To Reproduce** Train the GPT model with context parallelism. 1. According to https://github.com/NVIDIA/MegatronLM/issues/1151, set environment variable `NVTE_BATCH_MHA_P2P_COMM` to 1 to enable P2P communication. 2. Then run `pretrain_gpt.py` with `contextparallelsize` > 1. 3. Detailed training command:    ```bash    CUDA_DEVICE_MAX_CONNECTIONS=1 \    OMP_NUM_THREADS=1 \    NVTE_BATCH_MHA_P2P_COMM=1 \    torchrun \    nproc_per_node 2 \    nnodes 1 \    node_rank 0 \    master_addr localhost \    master_port 6000 pretrain_gpt.py \    tensormodelparallelsize 1 \    pipelinemodelparallelsize 1 \    contextparallelsize 2 \   as long as cp_size > 1    numlayers 24 \    hiddensize 1024 \    numattentionheads 16 \    seqlength 1024 \    maxpositionembeddings 1024 \    microbatchsize 4 \    globalbatchsize 16 \    lr 0.00015 \    trainiters 1 \    lrdecayiters 320000 \    lrdecaystyle cosine \    minlr 1.0e5 \    weightdecay 1e2 \    lrwarmupfraction .01 \    clipgrad 1.0 \    usemcoremodels \    attentionsoftmaxinfp32 \    attentiondropout 0.0 \    hiddendropout 0.0 \    bf16 \    datapath /workspace/dataset/wikitext/wikitext_text_document \    vocabfile /workspace/dataset/gpt2vocab.json \    mergefile /workspace/dataset/gpt2merges.txt \    split 949,50,1 \    loginterval 100 \    saveinterval 10000 \    evalinterval 1000 \    evaliters 0 \    save /workspace/checkpoints \    load /workspace/checkpoints \    nogradientaccumulationfusion \    transformerimpl transformer_engine    ``` **Expected behavior** The gradient of attention should be the same as single card when context parallelism is on.  **Stack trace/logs** If applicable, add the stack trace or logs from the time of the error. This is not stack trace or log from console, but the probe of the gradient tensors.  activation gradient:  ``` name                                                       relative_error   layers.23g_activ.input.0                  2.21e+0   283.4Îµ ```  parameter gradient  ``` name                                                   relative_error   decoder.layers.23.mlp.linear_fc2.bias                 4.20e3     0.5Îµ decoder.layers.23.mlp.linear_fc2.weight               5.16e3     0.7Îµ decoder.layers.23.mlp.linear_fc1.bias                 4.54e3     0.6Îµ decoder.layers.23.mlp.linear_fc1.weight               5.39e3     0.7Îµ decoder.layers.23.self_attention.linear_proj.bias     4.22e3     0.5Îµ decoder.layers.23.self_attention.linear_proj.weight   4.78e3     0.6Îµ decoder.layers.23.self_attention.linear_qkv.bias      6.67e2     8.6Îµ decoder.layers.23.self_attention.linear_qkv.weight    1.31e1    16.9Îµ decoder.layers.22.mlp.linear_fc2.bias                 1.33e2     1.7Îµ decoder.layers.22.mlp.linear_fc2.weight               2.45e2     3.1Îµ decoder.layers.22.mlp.linear_fc1.bias                 1.79e2     2.3Îµ decoder.layers.22.mlp.linear_fc1.weight               2.80e2     3.6Îµ decoder.layers.22.self_attention.linear_proj.bias     1.34e2     1.7Îµ decoder.layers.22.self_attention.linear_proj.weight   2.67e2     3.4Îµ decoder.layers.22.self_attention.linear_qkv.bias      8.05e2    10.3Îµ decoder.layers.22.self_attention.linear_qkv.weight    1.41e1    18.1Îµ ... decoder.layers.1.mlp.linear_fc2.bias                  4.83e1    61.9Îµ decoder.layers.1.mlp.linear_fc2.weight                4.69e1    60.2Îµ decoder.layers.1.mlp.linear_fc1.bias                  4.79e1    61.4Îµ decoder.layers.1.mlp.linear_fc1.weight                4.56e1    58.6Îµ decoder.layers.1.self_attention.linear_proj.bias      4.64e1    59.6Îµ decoder.layers.1.self_attention.linear_proj.weight    4.75e1    61.0Îµ decoder.layers.1.self_attention.linear_qkv.bias       7.98e1   102.3Îµ decoder.layers.1.self_attention.linear_qkv.weight     7.77e1    99.7Îµ decoder.layers.0.mlp.linear_fc2.bias                  6.11e1    78.4Îµ decoder.layers.0.mlp.linear_fc2.weight                6.19e1    79.5Îµ decoder.layers.0.mlp.linear_fc1.bias                  5.92e1    75.9Îµ decoder.layers.0.mlp.linear_fc1.weight                6.17e1    79.1Îµ decoder.layers.0.self_attention.linear_proj.bias      3.60e1    46.2Îµ decoder.layers.0.self_attention.linear_proj.weight    8.32e1   106.7Îµ decoder.layers.0.self_attention.linear_qkv.bias       7.40e1    94.9Îµ decoder.layers.0.self_attention.linear_qkv.weight     1.54e+0   198.3Îµ embedding.position_embeddings.weight                  2.27e+0   291.4Îµ embedding.word_embeddings.weight                      2.90e1    37.3Îµ ```  parameter `main_grad` before optimizer.step() ``` name                                                          relative_error  embedding.word_embeddings.weight                              2.05e1   26.4Îµ embedding.position_embeddings.weight                          2.31e+0  296.3Îµ decoder.layers.0.self_attention.linear_proj.weight            8.04e1  103.2Îµ decoder.layers.0.self_attention.linear_proj.bias              3.66e1   47.0Îµ decoder.layers.0.self_attention.linear_qkv.layer_norm_weight  1.41e+0  181.4Îµ decoder.layers.0.self_attention.linear_qkv.layer_norm_bias    7.59e1   97.4Îµ decoder.layers.0.self_attention.linear_qkv.weight             1.50e+0  193.3Îµ decoder.layers.0.self_attention.linear_qkv.bias               7.54e1   96.7Îµ decoder.layers.0.mlp.linear_fc1.layer_norm_weight             6.34e1   81.4Îµ decoder.layers.0.mlp.linear_fc1.layer_norm_bias               6.11e1   78.4Îµ decoder.layers.0.mlp.linear_fc1.weight                        5.94e1   76.2Îµ decoder.layers.0.mlp.linear_fc1.bias                          6.01e1   77.1Îµ decoder.layers.0.mlp.linear_fc2.weight                        6.10e1   78.3Îµ decoder.layers.0.mlp.linear_fc2.bias                          6.24e1   80.0Îµ decoder.layers.1.self_attention.linear_proj.weight            4.49e1   57.7Îµ decoder.layers.1.self_attention.linear_proj.bias              4.65e1   59.7Îµ decoder.layers.1.self_attention.linear_qkv.layer_norm_weight  8.50e1  109.0Îµ decoder.layers.1.self_attention.linear_qkv.layer_norm_bias    8.30e1  106.5Îµ decoder.layers.1.self_attention.linear_qkv.weight             7.79e1  100.0Îµ decoder.layers.1.self_attention.linear_qkv.bias               8.29e1  106.4Îµ decoder.layers.1.mlp.linear_fc1.layer_norm_weight             4.18e1   53.6Îµ decoder.layers.1.mlp.linear_fc1.layer_norm_bias               4.72e1   60.6Îµ decoder.layers.1.mlp.linear_fc1.weight                        4.26e1   54.6Îµ decoder.layers.1.mlp.linear_fc1.bias                          4.72e1   60.6Îµ decoder.layers.1.mlp.linear_fc2.weight                        4.39e1   56.3Îµ decoder.layers.1.mlp.linear_fc2.bias                          4.75e1   60.9Îµ ... decoder.layers.22.self_attention.linear_proj.weight           1.65e2    2.1Îµ decoder.layers.22.self_attention.linear_proj.bias             7.93e3    1.0Îµ decoder.layers.22.self_attention.linear_qkv.layer_norm_weight 9.53e2   12.2Îµ decoder.layers.22.self_attention.linear_qkv.layer_norm_bias   5.87e2    7.5Îµ decoder.layers.22.self_attention.linear_qkv.weight            9.53e2   12.2Îµ decoder.layers.22.self_attention.linear_qkv.bias              5.73e2    7.4Îµ decoder.layers.22.mlp.linear_fc1.layer_norm_weight            1.72e2    2.2Îµ decoder.layers.22.mlp.linear_fc1.layer_norm_bias              1.03e2    1.3Îµ decoder.layers.22.mlp.linear_fc1.weight                       1.75e2    2.2Îµ decoder.layers.22.mlp.linear_fc1.bias                         1.06e2    1.4Îµ decoder.layers.22.mlp.linear_fc2.weight                       1.52e2    2.0Îµ decoder.layers.22.mlp.linear_fc2.bias                         7.82e3    1.0Îµ decoder.layers.23.self_attention.linear_proj.weight           3.71e3    0.5Îµ decoder.layers.23.self_attention.linear_proj.bias             3.50e3    0.4Îµ decoder.layers.23.self_attention.linear_qkv.layer_norm_weight 7.68e2    9.9Îµ decoder.layers.23.self_attention.linear_qkv.layer_norm_bias   3.63e2    4.7Îµ decoder.layers.23.self_attention.linear_qkv.weight            8.50e2   10.9Îµ decoder.layers.23.self_attention.linear_qkv.bias              3.71e2    4.8Îµ decoder.layers.23.mlp.linear_fc1.layer_norm_weight            3.89e3    0.5Îµ decoder.layers.23.mlp.linear_fc1.layer_norm_bias              3.53e3    0.5Îµ decoder.layers.23.mlp.linear_fc1.weight                       3.89e3    0.5Îµ decoder.layers.23.mlp.linear_fc1.bias                         3.51e3    0.5Îµ decoder.layers.23.mlp.linear_fc2.weight                       3.83e3    0.5Îµ decoder.layers.23.mlp.linear_fc2.bias                         3.39e3    0.4Îµ decoder.final_layernorm.weight                                4.29e3    0.6Îµ decoder.final_layernorm.bias                                  3.46e3    0.4Îµ ``` **Environment (please complete the following information):**   MegatronLM commit ID: `fdfcef87`   PyTorch version: 2.5.1+cu121   CUDA version: 12.1   NCCL version: 2.21.5   TransformerEngine: `754d2a0` (current newest commit in `stable` branch) **Proposed fix** Check the `AttnFuncWithCPAndKVP2P.backward()` and fix the bug in it. **Additional context** Please fix it soon.  This is very serious bug.",2025-03-11T12:59:44Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1464
AsakusaRinne,fix(moe): the missing argument 'router_dtype' of _DeepepManager.__init__,This PR fix the error caused by the missing`router_dtype` argument.,2025-03-11T06:48:25Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1463
liddk,"[BUG] If one pipeline only has dense param and other pipeline has both dense/moe param, The optimizer will lead to hang in barrier","**Describe the bug** https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/optimizer/__init__.pyL556 Like deepseek V3 MoE model, If we split dense layer into first pipeline and moe layer into another pipeline, first pipeline only has dense optimizer, and other pipeline has dense and moe optimier. So for example, when we do logging barrier in https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/optimizer/distrib_optimizer.pyL2127, calling number of barrier is not matched in these pipeline. **To Reproduce** run deepseek with moelayerfreq [1], and split dense layers into single pipeline. **Expected behavior** Barrier in optimizer logging **Stack trace/logs** Barrier in optimizer logging **Environment (please complete the following information):**   MegatronLM commit ID d49d0279ac50e2cd2377fd2aa19aa127288995f2   PyTorch version 2.4   CUDA version 11.2   NCCL version 2.14 **Proposed fix** Maybe we can create an empty moe optimizer in pure dense pipeline? **Additional context** Add any other context about the problem here.",2025-03-11T03:55:46Z,,closed,0,6,https://github.com/NVIDIA/Megatron-LM/issues/1462,Thanks for reporting. We are preparing a MR to fix this issue along with CC([BUG] reduce_aux_losses_tracker_accross_ranks hangs if first pipeline stage has no moe layers) .,Thank you~,"  In addition to the hang, I am also seeing discrepancy in reduction values with and without CUDA graphs turned on. Can you also please look into why that discrepancy happens? Changing this to the following (to add print statements): ``` def reduce_aux_losses_tracker_across_ranks():     """"""Collect and reduce the auxiliary losses across ranks.""""""     tracker = parallel_state.get_moe_layer_wise_logging_tracker()     aux_losses = {k: v['values'].float() for k, v in tracker.items()}     if torch.distributed.get_rank() == 0:         print(f""first {aux_losses=}"")     for name in tracker:         values = tracker[name][""values""]          Collect aux losses across PP.         torch.distributed.all_reduce(             values, group=parallel_state.get_pipeline_model_parallel_group()         )         if torch.distributed.get_rank() == 0:             print(f""{values=}"")          Reduce aux losses across ranks.         if tracker[name].get('reduce_group') is not None:             torch.distributed.all_reduce(values, group=tracker[name].get('reduce_group'))             if torch.distributed.get_rank() == 0:                 print(f""reduced vals {values=}"")         if tracker[name].get('avg_group') is not None:             torch.distributed.all_reduce(                 values, group=tracker[name]['avg_group'], op=torch.distributed.ReduceOp.AVG             ) ``` I see the following results with and without cuda graphs enabled: with CUDA graphs: ``` first aux_losses={'z_loss': tensor([0.0000, 1.0703, 0.0000, 0.9629, 0.8457, 0.6562, 0.9121, 1.1836],        device='cuda:0'), 'load_balancing_loss': tensor([0.0000, 1.0056, 0.0000, 1.0083, 1.0372, 1.1118, 1.0406, 1.0901],        device='cuda:0')} values=tensor([0.0000, 1.0703, 0.0000, 0.9629, 0.8457, 0.6562, 0.9121, 1.1836],        device='cuda:0') values=tensor([0.0000, 1.0056, 0.0000, 1.0083, 1.0372, 1.1118, 1.0406, 1.0901],        device='cuda:0') ``` without CUDA graphs: ``` first aux_losses={'z_loss': tensor([0.0000, 1.0703, 0.0000, 0.9629, 0.8457, 0.6592, 0.9102, 1.1797],        device='cuda:0'), 'load_balancing_loss': tensor([0.0000, 1.0056, 0.0000, 1.0083, 1.0370, 1.1117, 1.0406, 1.0903],        device='cuda:0')} values=tensor([0.0000, 1.0703, 0.0000, 0.9629, 0.8457, 0.6592, 0.9102, 1.1797],        device='cuda:0') values=tensor([0.0000, 1.0056, 0.0000, 1.0083, 1.0370, 1.1117, 1.0406, 1.0903],        device='cuda:0') reduced vals values=tensor([0.0000, 2.0175, 0.0000, 2.0102, 2.0742, 2.2586, 2.1155, 2.1638],        device='cuda:0') ``` As can be seen, there is a doubling of the final reduced values btw the 2 implementations.  I enabled CUDA graphs using the following flag: ```   use_te_rng_tracker: True   enable_cuda_graph: True ``` Version info: MegatronLM.git NeMo.git TransformerEngine.git","To provide more context, the numbers for the vals are same for the first batch, but then start differing from the 2nd batch onwards. ",choudhry Could you provide your training scripts with and without CUDA graph for us to reproduce?,"Unfortunately, wont be able to provide the training scripts. But, I am using the `from megatron.core.models.gpt import GPTModel` with the following moe configs: ```   num_layers: 8   moe_layer_freq: [0, 1, 0, 1, 1, 1, 1, 1]   moe_use_legacy_grouped_gemm: False   use_te_rng_tracker: True   enable_cuda_graph: True ``` and the following 5d parallel configs: ``` context_parallel_size: 1 tensor_model_parallel_size: 2 pipeline_model_parallel_size: 1 num_moe_experts: 2 ``` I hope that helps with the repro. Happy to provide any additional specific attribute values.  For the non graph version, I unset the `use_te_rng_tracker` and `enable_cuda_graph` to False.  These are the load_balancing discrepancy I am seeing:   What I have found out uptil now is that this is because when CUDA graphs is enabled, the code never comes within the `if` condition here, which makes me believe that there is some discrepancy in the groups here"
soluwalana,bugfix: Incorrect generated_text when not sampling_params.return_segments,"The previous behavior of `api.generate` when not supplying `sampling_params.return_segments` was to only return the generated text in the generated text field, however a change with `detokenize_generations` has broken that behavior.  ```             request.generated_text, request.generated_segments = self.detokenize_generations(                 required_result_tokens, + +            text, segments = self.detokenize_generations( +                batch_prompt_tokens_with_generations[idx],                  input_prompt_length + generated_sequence_lengths,                  sampling_params.return_segments,              )  +            request.text = text   Inference server returns prompts & generations together +            if sampling_params.return_segments: +                request.segments = segments[0] +            request.generated_text = text[len(request.prompt) :] ``` The change to `detokenize_generations` still tokenizes all tokens passed to the function when not sampling_segments, previously that input used to be: ```   required_result_tokens, ``` However it is now: ``` +                batch_prompt_tokens_with_generations[idx], ``` This MR resolves this issue in `generate_all_output_tokens_static_batch` by moving the `return_segments` check into `generate_all_output_tokens_static_batch` but leaving the function `detokenize_segments` intact to ensure that there aren't any breaking changes for callers that rely on the new behavior. ",2025-03-10T23:35:36Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1461
lhb8125,Draft: Youngeun/a2a hiding,,2025-03-10T09:08:23Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1460
Abstractjkc,[QUESTION]Why can't Expert Parallelism (EP) and Context Parallelism (CP) be enabled simultaneously?,"I am studying parallel techniques and noticed an assertion in `core/parallel_state.py` within the `RankGenerator` class: https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/parallel_state.pyL336 ```python assert (     ep == 1 or cp == 1 ), ""Both EP and CP > 1 in not allow in one rank generator. \     CP is only included in default RankGenerator, and EP only in expert RankGenerator."" ``` Could you please explain why EP and CP cannot be enabled simultaneously in the same RankGenerator? Is this due to: Resource conflicts between EP and CP? Implementation complexities or design limitations? Other technical constraints? Additionally, could you provide guidance on how to modify the code to support both EP and CP if needed?",2025-03-09T09:11:54Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/1459,"CP and EP can both be enabled in MoE model training. In MoE parallel folding, we want CP and EP to share communication groups. We achieve this by creating two rank generator, one for Attention/Dense layer, and the other for MoE layer. The assert is to make sure CP is only included in `decoder_rank_generator`, EP only included in `expert_decoder_rank_generator`. See https://github.com/NVIDIA/MegatronLM/tree/main/megatron/core/transformer/moemoeparallelfolding for explanation on MoE Parallel Folding."," Thank you for your reply!  That is to say `decoder_rank_generator` and `expert_decoder_rank_generator` share the PP group, right ? the TP and DP size can be different for these two groups, as long as `tp_size * cp_size * dp_size == ep_tp_size * ep_size * ep_dp_size` is satisfied ","Yeah. That's right. For `ep_tp_size` and `ep_dp_size`, I think you mean `expert_tp_size` and `expert_dp_size`"
zhanjiqing,[QUESTION] deepep vs origin all-to-all,"   I'm very grateful to the experts at NVIDIA for quickly integrating the code of DeepEP. However, in my single  machine tests, I found that the training speed is actually slower than that of the traditional all  to  all method. May I ask if you have any test results for me to refer to?  My config of PP=2ï¼ŒEP=4  and MoE configï¼š     HIDDEN_SIZE=2048 CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)     NUM_ATTN_HEADS=128     NUM_LAYERS=8 CC(RuntimeError when running MegatornLM with recompute flag turned off) CC(RuntimeError when running MegatornLM with recompute flag turned off)     INTERMEDIATE_SIZE=18432     MOE_INTERMEDIATE_SIZE=2048     MAX_POSITION_EMBEDDINGS=163840     EXTRA_VOCAB_SIZE=1280     Q_LORA_RANK=1536     KV_LORA_RANK=512     QK_NOPE_HEAD_DIM=128     QK_ROPE_HEAD_DIM=64     V_HEAD_DIM=128     ROPE_THETA=10000     SCALE_FACTOR=40     NUM_EXPERTS=32 CC(why the shape of token2use must be [:,prev_context_length:context_length]) CC(Apex dependency) CC(why the shape of token2use must be [:,prev_context_length:context_length])     ROUTER_TOPK=8     NUM_SHARED_EXPERTS=1     MOE_LAYER_FREQ=0 CC(Compatibility with pytorchtransformers for finetuning )",2025-03-07T07:23:34Z,,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1456,"DeepEP is optimized for large topk with crossnode EP(EP>8) scenarios. Based on our experience, for EP<=8, allgather or alltoall dispatchers are more recommended.","> DeepEP is optimized for large topk with crossnode EP(EP>8) scenarios. Based on our experience, for EP8."
XCD4P,[QUESTION]Is there any plan to make custom_fsdp compatible with PP?," **Tensor Parallelism (TP), Expert Parallelism (EP) and Context Parallelism (CP)**: Compatible with TP, EP and CP configurations, enabling efficient scaling of large language models. only shows support TP,EP, and CP in the custom_fsdp.md doc Is there a plan to make custom_fsdp compatible with PP?",2025-03-06T15:31:28Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1455
SrinivasaGogul,[QUESTION] is logit bias is available in Nemo Megatron GPT models,"Hello Team, While Inferring a model is there any way to create low probability for a certain token that is present in the top k so it won't get predicted. In openAI there is something called ```logit_bias```. where we can boost the probability for certain token or reduce the probability to a certain token. is there any builtin parameter or methods kindly mention. **the open ai example** ``` completion = client.chat.completions.create(    model=""gpt3.5turbo"",    messages=[{""role"": ""system"", ""content"": ""You finish user's sentences.""},              ""role"": ""user"", ""content"": ""Once upon a""} ]    logit_bias={267:100, 16263:100, 18754:100} ) ``` here the bias is set for certain token_ids",2025-03-06T11:27:19Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1454
liangxuZhang,[BUG] 'Float8Tensor' object has no attribute '_fp8_meta',"**Describe the bug** Since TE 2.0, Float8Tensor has no _fp8_meta variable, and some codes are still unsing param._fp8_meta which not compatible with TE2.0,  such as https://github.com/NVIDIA/MegatronLM/blob/main/megatron/training/training.pyL600 **To Reproduce** using fp8paramgather and TE >= 2.0 **Expected behavior** A clear and concise description of what you expected to happen. **Stack trace/logs** **Environment (please complete the following information):**   MegatronLM commit ID: 1ea44d94c4f7341d6e2ea06ccee43d169b8d214d   PyTorch version: 2.5.1+cu124   CUDA version: 12.4   NCCL version: 2.21.5 **Proposed fix** If you have a proposal for how to fix the issue state it here or link to a PR. **Additional context** Add any other context about the problem here.",2025-03-06T10:59:45Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1453
huanggx-sea,Artifact Evaluation,,2025-03-06T10:36:50Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1452
ETOgaosion,[Bug Fix] fix p2p communication order error and stuck problems when pp 2 and vpp 2 with remove pad,"Bug issue connects to CC([BUG] p2p communication order error and stuck when pp 2 and vpp 2 with remove pad)   Environent and Configuration When use `PP=2` and `VPP=2` with `config.variable_seq_lengths=True`, `config.batch_p2p_comm=True` and `config.overlap_p2p_comm=False`, current implementation of p2p_communication.py will cause incorrect behavior. If we set `config.overlap_p2p_comm=True` and `config.batch_p2p_comm=False`, bug disappear.  Bug 1: Communication Misorder  Description and Analysis Like this image below: !vpp After 2 devices finish at the dashed time, Device 1 should pass `output_tensor` and `input_tensor_grad` to Device 2, and because world size is 2, both devices have the same `next_rank` and `prev_rank`, the original ring communication becomes intercommunication, thus cause conflicts in p2p_communication. In detail, Device 1 passes `output_tensor` to `next_rank` and `input_tensor_grad` to `prev_rank`, and Device 2 receives `output_tensor_grad` from `next_rank` and `input_tensor` from `prev_rank`. if we use the original implementation of `_communicate_shapes`: ```py if send_prev_shape_tensor is not None:     send_prev_op = torch.distributed.P2POp(         torch.distributed.isend,         send_prev_shape_tensor,         get_pipeline_model_parallel_prev_rank(),     )     ops.append(send_prev_op) if recv_prev_shape_tensor is not None:     recv_prev_op = torch.distributed.P2POp(         torch.distributed.irecv,         recv_prev_shape_tensor,         get_pipeline_model_parallel_prev_rank(),     )     ops.append(recv_prev_op) if send_next_shape_tensor is not None:     send_next_op = torch.distributed.P2POp(         torch.distributed.isend,         send_next_shape_tensor,         get_pipeline_model_parallel_next_rank(),     )     ops.append(send_next_op) if recv_next_shape_tensor is not None:     recv_next_op = torch.distributed.P2POp(         torch.distributed.irecv,         recv_next_shape_tensor,         get_pipeline_model_parallel_next_rank(),     )     ops.append(recv_next_op) ``` If we assume that the sendandsend, or recvandrecv operation are in order when using `torch.distributed.batch_isend_irecv` with the same destination rank, Device 0 will send `input_tensor_grad.shape` to first Device 1, then send `output_tensor.shape` to 1. Meanwhile Device 1 receives `input_tensor.shape` first from Device 0, then `output_tensor_grad.shape` from 0. That is to say, ```py input_tensor_gpu1 = input_tensor_grad_gpu0 output_tensor_grad_gpu1 = output_tensor_gpu0 ``` This cause backward tensor shape mismatch: `Mismatch in shape: grad_output[0] has a shape of torch.Size([3131, 1, 3584]) and output[0] has a shape of torch.Size([3204, 1, 3584])` Here is more log: ```txt  Device 0 send_prev_shape_tensor: torch.Tensor([1673, 1, 3840], device='cuda:0'), send_next_shape_tensor: torch.Tensor([1702, 1, 3840], device='cuda:0') recv_prev_shape_tensor: torch.Tensor([], device='cuda:0'), recv_next_shape_tensor: torch.Tensor([1664, 1, 3840], device='cuda:0')  Device 1 send_prev_shape_tensor: torch.Tensor([1664, 1, 3840], device='cuda:0'), send_next_shape_tensor: torch.Tensor([1653, 1, 3840], device='cuda:0') recv_prev_shape_tensor: torch.Tensor([1673, 1, 3840], device='cuda:0'), recv_next_shape_tensor: torch.Tensor([1702, 1, 3840], device='cuda:0')  Reverse Error ```  Solution We notice that the `_p2p_op` has the logic for cases where `world size % 2 == 0`, so we use this for shape communication. The solution is change order of recv tensors: `send_prev`, `recv_next`, `send_next`, `recv_prev`  Bug 2: P2P hangs After we fix Bug 1, because we use `config.batch_p2p_comm=True`, so `_batched_p2p_ops` is called and hangs. No clear reason about why it hangs, but if we refactor its logic like above, we successfully fix this.  Refactor We can reuse `_batch_p2p_ops` and `_p2p_ops` here to support `communicate_shapes`, codes are more clean. This solution is just a proposal.",2025-03-05T12:29:22Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1451,"Deleting the whole branch seems to be dangerous, since not taken into consideration and shall rebase, reopen a new PR CC([Bug Fix] fix p2p communication order error and stuck problems when pp 2 and vpp 2 with remove pad) ."
ETOgaosion,[BUG] p2p communication order error and stuck when pp 2 and vpp 2 with remove pad,"**Describe the bug** p2p communication order error and stuck when pp 2 and vpp 2 with remove pad **To Reproduce** When use `PP=2` and `VPP=2` with `config.variable_seq_lengths=True`, `config.batch_p2p_comm=True` and `config.overlap_p2p_comm=False`, current implementation of p2p_communication.py will cause incorrect behavior. If we set `config.overlap_p2p_comm=True` and `config.batch_p2p_comm=False`, bug disappear. You can use verl to reproduce this issues, you should set actor's/critic's `pipeline_model_parallel_size=2` and `virtual_pipeline_model_parallel_size=1`, and delete`config.overlap_p2p_comm` and `config.batch_p2p_comm` in `verl/utils/megatron_utils.py` to use original MegatronLM configuration. **Expected behavior** Like this image below: !vpp After 2 devices finish at the dashed time, Device 1 should pass `output_tensor` and `input_tensor_grad` to Device 2, and because world size is 2, both devices have the same `next_rank` and `prev_rank`, the original ring communication becomes intercommunication, thus cause conflicts in p2p_communication. In detail, Device 1 passes `output_tensor` to `next_rank` and `input_tensor_grad` to `prev_rank`, and Device 2 receives `output_tensor_grad` from `next_rank` and `input_tensor` from `prev_rank`. **Stack trace/logs** Here is more log: ```txt  Device 0 send_prev_shape_tensor: torch.Tensor([1673, 1, 3840], device='cuda:0'), send_next_shape_tensor: torch.Tensor([1702, 1, 3840], device='cuda:0') recv_prev_shape_tensor: torch.Tensor([], device='cuda:0'), recv_next_shape_tensor: torch.Tensor([1664, 1, 3840], device='cuda:0')  Device 1 send_prev_shape_tensor: torch.Tensor([1664, 1, 3840], device='cuda:0'), send_next_shape_tensor: torch.Tensor([1653, 1, 3840], device='cuda:0') recv_prev_shape_tensor: torch.Tensor([1673, 1, 3840], device='cuda:0'), recv_next_shape_tensor: torch.Tensor([1702, 1, 3840], device='cuda:0')  Reverse Error ``` **Environment (please complete the following information):**   MegatronLM core_r0.11.0   PyTorch 2.4.0   CUDA 12.4   NCCL 2.20.5 **Proposed fix** PR see CC([Bug Fix] fix p2p communication order error and stuck problems when pp 2 and vpp 2 with remove pad)  **Additional context** Add any other context about the problem here.",2025-03-05T12:28:40Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1450
lzy-edu,[QUESTION],"Hello everyone, I am a beginner, and I have a question about save and load checkpoint. I first set trainiters to 100 for training. After the training is completed, I want to continue training for another 100 iterations from the last checkpoint. How should I do this? Right now, if I only modify trainiters, I encounter some parameter mismatch issues.",2025-03-05T07:10:32Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1449
mollon650,[QUESTION] why WrappedTorchLayerNorm sequence parallel not supported by torch LayerNormï¼Ÿ," just like TENorm add           Set flag for sequence parallelism (custom MegatronLM integration)         if getattr(self, ""sequence_parallel"", None) is not None:             self.weight.sequence_parallel = self.sequence_parallel             self.bias.sequence_parallel = self.sequence_parallel to WrappedTorchLayerNorm class  can support  sequence parallel ï¼Ÿ",2025-03-05T06:36:11Z,,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/1448,"No, changing the wrapper alone would not work. You would need the underlying implementation to support sequence parallelism","  why changing the wrapper alone would not workï¼Ÿ RMSNorm (Root Mean Square Normalization) does not operate across tokens; rather, it normalizes independently for each token. Specifically, RMSNorm is applied across the hidden (feature) dimension for each token separately. so RMSNorm is naturally compatible with sequence parallelism, as each device can compute RMSNorm locally without needing synchronization or collective communication. i reference Tenorm code ", any suggestionï¼Ÿ
huyiwen,[QUESTION] Why FSDP2 does not support embedding tying,**Your question** ``` AssertionError: usetorchfsdp2 requires untieembeddingsandoutputweights ``` Why does FSDP2 not support embedding tying? Torchtitan supports FSDP2 with embedding tying enabled at the same time. Is it possible to add support for embedding tying with FSDP2 in Megatron?,2025-03-04T13:29:26Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1447
i-love-megatron,[BUG] final_layernorm gradient mismatch when TP+SP enabled,"**Describe the bug** In `pretrain_gpt.py`, when both tensor parallel and sequence parallel are enabled, the `main_grad` of parameters in `final_layernorm` are different on different TP ranks before `optimizer.step()`. **To Reproduce** Run `pretrain_gpt.py` with both `tensormodelparallelsize` larger than 1 and `sequenceparallel` opened. **Expected behavior** Layernorms parameters are partitioned, so their gradients are expected to be the same before `optimizer.step()` on different parallel ranks. **Stack trace/logs** I put the values of final_layernorm.weight.main_grad below: TP rank 0 has value: tensor([2.9907e03, 3.1120e04,  9.9897e05,  ...,  1.7762e04, 7.4244e04, 4.4298e04] TP rank 1: tensor([2.9755e03, 6.2752e04,  2.9802e05,  ...,  2.2030e04, 9.9659e04, 5.0163e04] TP rank 2: tensor([3.0289e03, 5.4169e04, 1.4603e05,  ...,  2.4462e04, 8.9359e04, 3.8719e04] TP rank 3: tensor([1.9131e03, 4.3297e04,  6.0797e06,  ...,  3.1662e04, 8.9264e04, 5.1117e04] **Environment (please complete the following information):**   MegatronLM commit ID: The latest commit from main branch, fdfcef87004205eca69ea462b375578185ea01f6   PyTorch version: 2.4.1+cu124   CUDA version: 12.4   NCCL version: 2.20.5 **Proposed fix** N/A **Additional context** N/A",2025-03-04T05:28:08Z,,open,2,0,https://github.com/NVIDIA/Megatron-LM/issues/1446
Desperadoze,[BUG]loss error when using MLA,"**Describe the bug** when using mla by megatroncore 0.10.0,loss was unexpected **To Reproduce** use  /nvcr.io/nvidia/nemo:25.02.rc1   image start traing **Additional context** Add any other context about the problem here.  numlayers 60 \     hiddensize 5120 \     ffnhiddensize 12288 \     numattentionheads 128 \     seqlength 4096 \     maxpositionembeddings 4096 \     multilatentattention \     kvlorarank 512 \     qlorarank 1536 \     moeffnhiddensize 1536 \     moeroutertopk 6 \     numexperts 160 \     decoderfirstpipelinenumlayers 2 \     decoderlastpipelinenumlayers 2 \     moeauxlosscoeff 1e2 \     moesharedexpertintermediatesize  3072 \     moelayerfreq [0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1] \     expertmodelparallelsize 8 \     moegroupedgemm \     moerouterloadbalancingtype aux_loss \     qkheaddim 128 \     qkposembheaddim 64 \     vheaddim 128 \     rotarybase 10000 \     noropefusion \     nobiasdropoutfusion \     nopersistlayernorm \     microbatchsize 1 \",2025-03-03T10:20:39Z,,open,0,14,https://github.com/NVIDIA/Megatron-LM/issues/1445,"!Image  The lower two curves are GQA and MHA, and the upper two curves are MLA.","Thank you for reporting the issue.   pls help take a look. Are you using the same hyperparameters, only changing the attention type for these experiments? and could you also share the gradnorm curve?","The difference between these four curves: In the two using MLA, black has a capacity of 4, green is dropless, orange is GQA, I forgot if yellow is GQA with more grouping or MHA, all other configuration items are exactly the same. The grade norm of MLA will increase to 10002000 after 4000 steps !Image > Thank you for reporting the issue. [](https://github.com/xxuwenc) [](https://github.com/Shunkangz) pls help take a look. Are you using the same hyperparameters, only changing the attention type for these experiments? and could you also share the gradnorm curve?","From what I saw on the grad_norm curve, the orange one increase to between 1000 and 2000 after 4000 steps. Is the orange curve MLA or MHA?","> From what I saw on the grad_norm curve, the orange one increase to between 1000 and 2000 after 4000 steps. Is the orange curve MLA or MHA? After these four curves, I ran some experiments again. The screenshots were taken at different times, so the colors of the curves in the two graphs did not correspond. The above description is for the loss graph. The purple and orange colors in the following graph correspond to the green and black colors in the loss graph. The grad norm of GQA and MHA has always been within the normal range. Due to the scale, it looks like two overlapping bottom straight lines in the grad norm graph","I see, thanks! We will run some experiments on v0.11(the latest stable branch) first to see if we can reproduce your issue and will get back to you. By the way, it would be super helpful if you could share the scripts and the W&B report if available.","This is my running configuration item. Compared with the actual training script, the data related and path related parts have been deleted. I am sure it is not  caused by the data, because after replacing  different data, the loss curve of MLA is still abnormal. The megatronlm version used is the code in the image nvcr. io/nvidia/nemo: 25.02.rc1ï¼ŒThanks  for your patience and helpï¼ deepseek_236.txt","  Thank you for reporting this issue and sharing detailed configuration details. We tried to reproduce the divergence issue in MCore 0.11 with a small model to see if it's caused by MLA implementation error, and the loss seems normal. !Image We noticed that your provided config doesn't enable qk layernorm by `qklayernorm`, this may cause training instability as said in DeepSeekV2 paper.   And the above divergence looks similar to the training instability without qklayernorm in https://arxiv.org/pdf/2309.14322. Could you have a try to enable `qklayernorm` and see if the divergence issue still exists?","Thank you very much for your suggestions and time and effort. After adding qklayernorm item and using core0.11, everything looks normal. I will pay attention to the downstream task scores of the model implemented with megatron core's MLA to ensure that the entire functionality is correct .",Great! Thanks for your feedback.,"> **Describe the bug** when using mla by megatroncore 0.10.0,loss was unexpected >  > **To Reproduce** use /nvcr.io/nvidia/nemo:25.02.rc1 image start traing >  > **Additional context** Add any other context about the problem here. numlayers 60 hiddensize 5120 ffnhiddensize 12288 numattentionheads 128 seqlength 4096 maxpositionembeddings 4096 multilatentattention kvlorarank 512 qlorarank 1536 moeffnhiddensize 1536 moeroutertopk 6 numexperts 160 decoderfirstpipelinenumlayers 2 decoderlastpipelinenumlayers 2 moeauxlosscoeff 1e2 moesharedexpertintermediatesize 3072 moelayerfreq [0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1] expertmodelparallelsize 8 moegroupedgemm moerouterloadbalancingtype aux_loss qkheaddim 128 qkposembheaddim 64 vheaddim 128 rotarybase 10000 noropefusion nobiasdropoutfusion nopersistlayernorm microbatchsize 1 \ may I ask about other parallelism setups? I keep failing to reproduce DeepSeek V2. MLA causes high memory pressure, so I manage it with selective checkpointing but it leads to an OOM error after a few hundred steps, once load balancing stabilizes. if I'm right, theoretically one device can consume (4096x8/160x(6+2))x(160/8)=32k (considering shared experts) when mbsz=1, seqlen=4096 (typical dsv2 setup i guess) and EP=8 and token drop is used.  i guess this large batch cause OOM. p.s.) while it's ffn_dim is 1/8 times smaller than dense, 32k * moe_ffn_dim is same as dense's, but it requires larger activation memory because it's input hidden dim is same as dense's","> > **Describe the bug** when using mla by megatroncore 0.10.0,loss was unexpected > > **To Reproduce** use /nvcr.io/nvidia/nemo:25.02.rc1 image start traing > > **Additional context** Add any other context about the problem here. numlayers 60 hiddensize 5120 ffnhiddensize 12288 numattentionheads 128 seqlength 4096 maxpositionembeddings 4096 multilatentattention kvlorarank 512 qlorarank 1536 moeffnhiddensize 1536 moeroutertopk 6 numexperts 160 decoderfirstpipelinenumlayers 2 decoderlastpipelinenumlayers 2 moeauxlosscoeff 1e2 moesharedexpertintermediatesize 3072 moelayerfreq [0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1] expertmodelparallelsize 8 moegroupedgemm moerouterloadbalancingtype aux_loss qkheaddim 128 qkposembheaddim 64 vheaddim 128 rotarybase 10000 noropefusion nobiasdropoutfusion nopersistlayernorm microbatchsize 1 \ >  > may I ask about other parallelism setups? I keep failing to reproduce DeepSeek V2. MLA causes high memory pressure, so I manage it with selective checkpointing but it leads to an OOM error after a few hundred steps, once load balancing stabilizes. if I'm right, theoretically one device can consume (4096x8/160x(6+2))x(160/8)=32k (considering shared experts) when mbsz=1, seqlen=4096 (typical dsv2 setup i guess) and EP=8 and token drop is used. i guess this large batch cause OOM. p.s.) while it's ffn_dim is 1/8 times smaller than dense, 32k * moe_ffn_dim is same as dense's, but it requires larger activation memory because it's input hidden dim is same as dense's The configuration in the issue above is the parallel configuration I am using. Can you try changing the parameters related to recompute? Under the above configuration, the memory usage is approximately between 6468G, which is a relatively safe value","> > > **Describe the bug** when using mla by megatroncore 0.10.0,loss was unexpected > > > **To Reproduce** use /nvcr.io/nvidia/nemo:25.02.rc1 image start traing > > > **Additional context** Add any other context about the problem here. numlayers 60 hiddensize 5120 ffnhiddensize 12288 numattentionheads 128 seqlength 4096 maxpositionembeddings 4096 multilatentattention kvlorarank 512 qlorarank 1536 moeffnhiddensize 1536 moeroutertopk 6 numexperts 160 decoderfirstpipelinenumlayers 2 decoderlastpipelinenumlayers 2 moeauxlosscoeff 1e2 moesharedexpertintermediatesize 3072 moelayerfreq [0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1] expertmodelparallelsize 8 moegroupedgemm moerouterloadbalancingtype aux_loss qkheaddim 128 qkposembheaddim 64 vheaddim 128 rotarybase 10000 noropefusion nobiasdropoutfusion nopersistlayernorm microbatchsize 1 \ > >  > > may I ask about other parallelism setups? I keep failing to reproduce DeepSeek V2. MLA causes high memory pressure, so I manage it with selective checkpointing but it leads to an OOM error after a few hundred steps, once load balancing stabilizes. if I'm right, theoretically one device can consume (4096x8/160x(6+2))x(160/8)=32k (considering shared experts) when mbsz=1, seqlen=4096 (typical dsv2 setup i guess) and EP=8 and token drop is used. i guess this large batch cause OOM. p.s.) while it's ffn_dim is 1/8 times smaller than dense, 32k * moe_ffn_dim is same as dense's, but it requires larger activation memory because it's input hidden dim is same as dense's >  > The configuration in the issue above is the parallel configuration I am using. Can you try changing the parameters related to recompute? Under the above configuration, the memory usage is approximately between 6468G, which is a relatively safe value oh, did you use recompute method? for all transformer blocks?","> > > > **Describe the bug** when using mla by megatroncore 0.10.0,loss was unexpected > > > > **To Reproduce** use /nvcr.io/nvidia/nemo:25.02.rc1 image start traing > > > > **Additional context** Add any other context about the problem here. numlayers 60 hiddensize 5120 ffnhiddensize 12288 numattentionheads 128 seqlength 4096 maxpositionembeddings 4096 multilatentattention kvlorarank 512 qlorarank 1536 moeffnhiddensize 1536 moeroutertopk 6 numexperts 160 decoderfirstpipelinenumlayers 2 decoderlastpipelinenumlayers 2 moeauxlosscoeff 1e2 moesharedexpertintermediatesize 3072 moelayerfreq [0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1] expertmodelparallelsize 8 moegroupedgemm moerouterloadbalancingtype aux_loss qkheaddim 128 qkposembheaddim 64 vheaddim 128 rotarybase 10000 noropefusion nobiasdropoutfusion nopersistlayernorm microbatchsize 1 \ > > >  > > >  > > > may I ask about other parallelism setups? I keep failing to reproduce DeepSeek V2. MLA causes high memory pressure, so I manage it with selective checkpointing but it leads to an OOM error after a few hundred steps, once load balancing stabilizes. if I'm right, theoretically one device can consume (4096x8/160x(6+2))x(160/8)=32k (considering shared experts) when mbsz=1, seqlen=4096 (typical dsv2 setup i guess) and EP=8 and token drop is used. i guess this large batch cause OOM. p.s.) while it's ffn_dim is 1/8 times smaller than dense, 32k * moe_ffn_dim is same as dense's, but it requires larger activation memory because it's input hidden dim is same as dense's > >  > >  > > The configuration in the issue above is the parallel configuration I am using. Can you try changing the parameters related to recompute? Under the above configuration, the memory usage is approximately between 6468G, which is a relatively safe value >  > oh, did you use recompute method? for all transformer blocks?     recomputegranularity full \     recomputenumlayers 1 \     recomputemethod uniform \"
i4never,[BUG] reduce_aux_losses_tracker_accross_ranks hangs if first pipeline stage has no moe layers,"**Describe the bug** https://github.com/NVIDIA/MegatronLM/blob/8a5521ac4226fbefeeb2a102ebecac32a01d4852/megatron/core/transformer/moe/moe_utils.pyL586L588 `reduce_aux_losses_tracker_across_ranks` do all_reduce accross ` _PIPELINE_MODEL_PARALLEL_GROUP`. If some pipeline stage has no moe layers, all_reduce will hangs. **To Reproduce** modeling with: ```shell tensormodelparallelsize 1 pipelinemodelparallelsize 8 expertmodelparallelsize 1 experttensorparallelsize 1 numlayers 16 moelayerfreq ""([0]*3+[1]*13)"" ``` will hangs because fisrt pp stage has no tracker info while other stage has tracker info like `{'load_balancing_loss': {'values': tensor([...])}}` **Expected behavior** First pp stage should have zero padding values. **Stack trace/logs** N/A **Environment (please complete the following information):**   MegatronLM commit ID: 8a5521ac4226fbefeeb2a102ebecac32a01d4852   PyTorch version: 2.5.1   CUDA version: 12.4   NCCL version: 2.21.5 **Proposed fix** N/A **Additional context** N/A",2025-03-03T09:56:50Z,,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1444,This is a known issue. We are currently fixing it. Thanks for reporting.,"Can you also please look into this issue posted here: https://github.com/NVIDIA/MegatronLM/issues/1462issuecomment2732642584, as part of this?"
AlpinDale,[QUESTION] How does the custom `torch.distributed.ring_exchange()` work?,"I see it referenced in a few places. How is it different from `batch_isend_irecv`? Would be good to know what it does differently, so we can see if it's worth implementing this ourselves. Thanks. Related issue CC(What is ring_exchange? ) ",2025-03-01T08:13:43Z,,open,1,0,https://github.com/NVIDIA/Megatron-LM/issues/1443
wdevazelhes,[ENHANCEMENT] add z-loss (improved version),zloss regularization was shown to stabilize training by preventing logits in the last layer to explode: https://arxiv.org/abs/2309.14322 This PR:  allows one to add zloss regularization  logs the zloss Note: this PR takes over CC([ENHANCEMENT] Add zloss) by logging the zloss more cleanly leveraging the already existing of auxiliary losses present for MoE,2025-02-28T10:20:01Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1442
yzlnew,[ENHANCEMENT] Replace the hardcoded top-2 sum group selection strategy with configurable top-k,"**Is your feature request related to a problem? Please describe.** When using node limiting routing (`moe_router_num_groups`), the sum of top2 expert scores within each group is used to decide which group is selected for each token. Which is not consistent to the original design of DeepSeek V3. https://github.com/NVIDIA/MegatronLM/blob/078e99f6c60c9aaa6121fcd17549d0b3df9edd3c/megatron/core/transformer/transformer_config.pyL298L305 !Image For DeepSeekV3, as $K_r=8, M=4$ (256 experts divided into/onto 8 groups/nodes, each contains 32 experts; 4 groups are then seleted), it happens to be top2 sum. **Describe the solution you'd like** A naive impl is to provide a topk configuration. But we have to ensure that `k=K_r/M` to match the design. However, when continue pretrain DeepSeekV3, I feel like it is recommended to use $K_r=8, M=4$ and top2 sum as the model is already trained to adapt to this strategy (top2 sum out of **32 experts**). Maybe we need more clarification from DeepSeek on the influence of difference routing strategy.",2025-02-28T06:16:42Z,,open,0,5,https://github.com/NVIDIA/Megatron-LM/issues/1441,Thanks for reporting this issue. We will look into it as soon as possible.,Fixed on https://github.com/NVIDIA/MegatronLM/commit/e85385a761e09f1f08ea309c6b6858bf1961a56f," I'm reopening this issue, cause $K_r$ means K routed, rather than `num_groups`, which happens to be the same value for V3.",I created a PR to fix this $K_r$ problem: https://github.com/NVIDIA/MegatronLM/pull/1502,"> [](https://github.com/yanring) I'm reopening this issue, cause K r means K routed, rather than `num_groups`, which happens to be the same value for V3. Thanks!  "
mustious,Replace deprecated numpy.product with numpy.prod to ensure compatibility with NumPy >=2.0,The current `np.product` raises `AttributeError: module 'numpy' has no attribute 'product'` The `np.product` has been deprecated since numpy 2.0. The updated np.prod supports the current numpy as well as preceding numpy versions `numpy>=1.15`.,2025-02-27T15:08:17Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1440
yzlnew,fix seq_aux_loss for DeepSeek-V3,Fix https://github.com/NVIDIA/MegatronLM/issues/1438,2025-02-27T07:35:53Z,,open,1,0,https://github.com/NVIDIA/Megatron-LM/issues/1439
yzlnew,[BUG] Incorrect `seq_aux_loss` impl for DeepSeek-V3,"**Describe the bug** When calculating `seq_aux_loss`, the score function should be `sigmoid` with normalization rather than `softmax`. https://github.com/NVIDIA/MegatronLM/blob/36753c7f7cf8c72b5a43c0ae309c36909ddf1b02/megatron/core/transformer/moe/router.pyL213 From the V3 paper, !Image !Image **Proposed fix** Check the score function to differ the behavior here.",2025-02-27T07:08:02Z,,closed,3,4,https://github.com/NVIDIA/Megatron-LM/issues/1438,Thanks for reporting the issue! Weâ€™ll take a look shortly.,"> Thanks for reporting the issue! Weâ€™ll take a look shortly. Hey, I propose a simple pr to solve this issue. Can anyone review it?","Thanks for reporting this issue and providing a fix! We have reviewed your PR and appreciate the correction you made for `score_function == ""sigmoid""` in `aux_loss_load_balancing`. We have submitted an MR that addresses both `aux_loss_load_balancing` and `seq_aux_loss_load_balancing`, and we have added you as a coauthor in recognition of your contribution. This fix will be included in our upcoming release :)",Fixed on https://github.com/NVIDIA/MegatronLM/commit/e85385a761e09f1f08ea309c6b6858bf1961a56f
trashbandit2025,[ENHANCEMENT] Context Parallel Support for Mamba2,"**Is your feature request related to a problem? Please describe.** Megatron LM currently supports context parallelism (CP) for GPTbased models, but it does not look like CP is supported for Mamba2 models.  **Describe the solution you'd like** It would be great to extend CP support to Mamba2 SSM layers. **Describe alternatives you've considered** We have used other forms of parallelism, such as tensor, pipeline, and sequence to train Mambabased models, but context parallelism is lacking and could be very useful for certain applications such as DNA sequence modeling. **Proposed implementation** A possible solution for extending CP support to Mamba has been proposed in the mamba_ssm repository: https://github.com/statespaces/mamba/pull/664",2025-02-27T05:52:07Z,,open,2,0,https://github.com/NVIDIA/Megatron-LM/issues/1437
BhagawanPanditi,[QUESTION] Issue with Adding a Custom Transformer Block in Megatron,"I'm implementing MultiToken Prediction (similar to DeepSeek V3) in the Megatron framework. The goal is to predict one extra token beyond the next token. This requires an **extra transformer block** that is separate from the main transformer stack. The extra block: 1. Takes the output of the main stack. 2. Concatenates it with input embeddings for the extra token. 3. Projects this concatenated tensor as input to the extra block. 4. Computes a new loss, which is added to the original main loss. I have already modified the necessary files to get input and labels for this extra block. However, I'm facing issues with implementing the extra transformer block.  **Implementation Attempts**  **First Approach:** I attempted to modify the config's `num_layers` temporarily to create a singlelayer transformer block: ```python original_num_layers = self.config.num_layers   Store original value self.config.num_layers = 1   Temporarily set to 1 self.mtp_decoder = TransformerBlock(     config=self.config,     spec=transformer_layer_spec,     pre_process=self.pre_process,     post_process=self.post_process, ) self.config.num_layers = original_num_layers   Restore original value ``` This resulted in a **checkpoint saving error**: ``` File â€œ/opt/megatronlm/megatron/core/dist_checkpointing/validation.pyâ€, line 470, in _validate_sharding_for_key     raise CheckpointingException( megatron.core.dist_checkpointing.core.CheckpointingException: Invalid access pattern for ShardedTensor(key='optimizer.state.fp32_param.model.mtp_decoder.layers.self_attention.linear_proj.weight', dtype=torch.float32, local_shape=(768, 768), global_shape=(12, 768, 768), global_offset=(0, 0, 0), axis_fragmentations=(12, 1, 1), replica_id=(0, 0, 0), prepend_axis_num=1, allow_shape_mismatch=False, flattened_range=None): tensor([[[1]],         [[0]],         [[0]],         [[0]],         [[0]],         [[0]],         [[0]],         [[0]],         [[0]],         [[0]],         [[0]],         [[0]]], dtype=torch.int32) ``` I suspect this happens because the original stack has 12 layers, and restoring `num_layers` after creating the extra block leads to issues.  **Second Approach:** I tried using a deep copy of the config: ```python self.mtp_config = copy.deepcopy(self.config) self.mtp_config.num_layers = 1 self.mtp_decoder = TransformerBlock(     config=self.mtp_config,     spec=transformer_layer_spec,     pre_process=self.pre_process,     post_process=self.post_process, ) ``` This led to an **NCCL/CUDA API hang**: ``` [rank0]:[E226 14:50:58.064411450 ProcessGroupNCCL.cpp:1496] [PG ID 5 PG GUID 133 Rank 0] ProcessGroupNCCL's watchdog got stuck for 480 seconds without making progress in monitoring enqueued collectives. This typically indicates a NCCL/CUDA API (e.g., CudaEventDestroy) hang blocking the watchdog, and could be triggered by another thread holding the GIL inside a CUDA API (for example, CudaEventDestroy), or other deadlockprone behaviors. ```  **Current Implementation** **File path:** `/opt/megatronlm/megatron/core/models/gpt/gpt_model.py`  **Modifications in `GPTModel` Class** ```python  Multitoken prediction processing self.mtp_config = copy.deepcopy(self.config) self.mtp_config.num_layers = 1 self.mtp_decoder = TransformerBlock(     config=self.mtp_config,     spec=transformer_layer_spec,     pre_process=self.pre_process,     post_process=self.post_process, ) self.hidden_norm = TENorm(     config=self.config,     hidden_size=self.config.hidden_size,     eps=self.config.layernorm_epsilon ) self.embedding_norm = TENorm(     config=self.config,     hidden_size=self.config.hidden_size,     eps=self.config.layernorm_epsilon ) self.projection = TERowParallelLinear(     input_size=2 * self.config.hidden_size,     output_size=self.config.hidden_size,     config=self.config,     init_method=self.config.init_method,     bias=True,     input_is_parallel=True,     skip_bias_add=False,     is_expert=False ) ```  **Forward Method Modifications** ```python mtp_logits = None if mtp_input_ids is not None and self.pre_process:      Get embeddings for MTP input     mtp_decoder_embeddings = self.embedding(         input_ids=mtp_input_ids,         position_ids=mtp_position_ids     )      Apply normalization     normalized_hidden = self.hidden_norm(hidden_states)     normalized_embeddings = self.embedding_norm(mtp_decoder_embeddings)      Concatenate along hidden dimension     concatenated = torch.cat(         [normalized_hidden, normalized_embeddings],         dim=1     )   Shape: [seq_len, batch_size, 2*hidden_size]     mtp_decoder_input, _ = self.projection(concatenated)      Pass through single transformer block     mtp_hidden_states = self.mtp_decoder(         hidden_states=mtp_decoder_input,         attention_mask=mtp_attention_mask,         inference_params=inference_params,         rotary_pos_emb=rotary_pos_emb,         rotary_pos_cos=rotary_pos_cos,         rotary_pos_sin=rotary_pos_sin,         packed_seq_params=packed_seq_params,         **(extra_block_kwargs or {}),     )      Get MTP logits using shared output layer     output_weight = None     if self.share_embeddings_and_output_weights:         output_weight = self.shared_embedding_or_output_weight()     mtp_logits, _ = self.output_layer(         mtp_hidden_states,         weight=output_weight,         runtime_gather_output=runtime_gather_output     ) ```  **Loss Computation Modifications** ```python mtp_loss = None if mtp_labels is not None and mtp_logits is not None:     mtp_loss = self.compute_language_model_loss(mtp_labels, mtp_logits) if mtp_loss is not None:     combined_loss = loss + self.config.mtp_weight * mtp_loss     return combined_loss ```  **Questions** 1. Is there a proper way to create an independent singlelayer transformer block within the `GPTModel` class that does not interfere with checkpointing or distributed training? 2. Are there specific considerations for adding custom components to Megatron models that I might be missing? Any guidance would be helpful!",2025-02-26T12:33:50Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1436
liyzcj,[ENHANCEMENT] Improve error handling for async save_checkpoint failures,"**Is your feature request related to a problem? Please describe.** Currently, when using the asynchronous save_checkpoint feature, there is no clear indication or event when a save operation fails. This lack of feedback can lead to confusion and potential data loss, as users may not be aware that the checkpoint was not saved successfully. **Describe the solution you'd like** I would like to implement a mechanism that triggers a clear event or notification whenever an asynchronous save operation fails. This could be in the form of a callback function or an error message that is logged, ensuring that users are immediately aware of the failure. **Describe alternatives you've considered** One alternative could be to implement a retry mechanism for failed saves, but this may not always be appropriate depending on the context. Another option could be to log errors silently, but this would not provide immediate feedback to the user. **Proposed implementation** I propose to modify the existing save_checkpoint function to include error handling that emits an event or logs an error message when a save operation fails. This could involve using a callback function or integrating with an existing logging framework. **Additional context** This enhancement will improve the robustness of the checkpointing process and provide users with the necessary feedback to ensure their models are being saved correctly.",2025-02-26T11:36:20Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1435
wplf,[BUG]  FSDP initial bugs,**Describe the bug** FSDP and DDP Model are both defined here It passes `disable_bucketing` but FSDP defined here don't accept this param.,2025-02-26T08:06:27Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1434
lyuwen,fix a bug in load balancing loss aggregation when recompute is turned on,"In the current implementation, the load balancing loss is aggregated twice in the trackerwhen recompute is turned on, which causes confusion. The fix added a recompute mode context manager, the context turns a `is_recompute` global variable on in the recompute forward pass. The loss tracker will detect the recompute mode and will not aggreate the load balancing loss in the recompute pass.",2025-02-26T08:01:28Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1433
shuyuan-wang,[QUESTION] Does Megatron support Deepseek-VL2 finetuning?,Does Megatron support DeepseekVL2 finetuning?,2025-02-25T08:15:08Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1432
amymeng1988,[QUESTION] Can we skip passing vocab-file/merge-file when we are using mock-data,"When I pass DATA_ARGS to launch pretrain_gpt.py. I use mockdata and didn't pass vocabfile/mergefile, but I got an Assert error, it required me to pass the vocabfile and mergefile, my question is can we skip passing vocabfile/mergefile when we are using mockdata. This is the DATA_ARGS parameter I used: DATA_ARGS=(      mockdata      numworkers 8       split 80,10,10  )  It hits to this line I got below error: ` File ""/opt/megatronlm/MegatronLM/megatron/training/tokenizer/tokenizer.py"", line 321, in __init__     self.tokenizer = GPT2Tokenizer(   File ""/opt/megatronlm/MegatronLM/megatron/training/tokenizer/gpt2_tokenization.py"", line 160, in __init__     with open(vocab_file) as f: TypeError: expected str, bytes or os.PathLike object, not NoneType`",2025-02-24T20:03:32Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1431
thuliu-yt16,Fix flash decoding,fix the issue CC([BUG] flash_decode=True produces wrong output) and fix a renaming typo in tools/run_text_generation_server.py,2025-02-24T12:16:55Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1430,"Good find, thanks! Will help shepherd this through our internal review process.","Merged here, thank you!  https://github.com/NVIDIA/MegatronLM/commit/eaa1a2344ba3dd5f5b4c6fd5cb3500c7c8dcfda1"
HowardZorn,[BUG] Incorrect `softmax_factor` Calculation in MLA Using `mscale` Instead of `mscale_all_dim`,"**Describe the bug** The `softmax_factor` in the MLA (MultiHead Latent Attention) is incorrectly calculated using `mscale` instead of `mscale_all_dim`. This leads to incorrect scaling in the attention mechanism, potentially affecting model performance. **To Reproduce** 1. Train DeepSeekV3like model with the MLA implementation. 2. Run a forward pass with attention computation. 3. Observe the `softmax_factor` calculation in the attention mechanism. 4. Notice that `mscale` is used instead of `mscale_all_dim` as shown in code, which is different from https://huggingface.co/deepseekai/DeepSeekV3/blob/86518964eaef84e3fdd98e9861759a1384f9c29d/modeling_deepseek.pyL695 **Expected behavior** The `softmax_factor` should be calculated using `mscale_all_dim` to ensure proper scaling. **Stack trace/logs** N/A (No error is thrown, but the behavior is incorrect.) **Environment (please complete the following information):**   MegatronLM commit ID: 77537b98da08c4100a611f36af349f6651777a8a   PyTorch version: N/A   CUDA version: N/A   NCCL version: N/A **Proposed fix** Replace `mscale` with `mscale_all_dim` in the `softmax_factor` calculation.  **Additional context** This issue was identified in the DeepSeekV3like model implementation. ",2025-02-24T11:42:34Z,,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1429,Thanks for reporting the issue and sorry for the late response. We will take a look shortly.
toothacher17,a proof of concept for Distributed Muon,"An proof of concept for implementing the Distributed Muon as described in: https://github.com/MoonshotAI/Moonlight  Example script: see examples/muon/training.sh  Tested with TP=2, PP=2, DP=2 and compared with AdamW, and no TP/PP  Used the data from bigscience and the provided example script !img_v3_02jq_52105121679a47449b7702645613951g",2025-02-24T09:13:42Z,,open,14,32,https://github.com/NVIDIA/Megatron-LM/issues/1428,"> !image  Hello, I am comparing the performance between AdamW and Muon. The experiment involves training a 1B parameter MoE model on the H800 cluster, with a maximum learning rate of 1e3 that decays to 1e4 using cosine decay. Muon uses default parameters. I observed that Muon has a significant advantage over AdamW in the early stages of training, but after 20k steps, their performance becomes similar, with AdamW sometimes even outperforming Muon. Is this phenomenon normal? hi,   Thanks a lot for trying out!  I actually probably know the reason: 1. The first question is that you are reporting training loss or validation loss? It's better to observe validation loss rather than training loss. 2. My next questions is that how many tokens did you train with Muon for your 20k steps? It is likely your trained tokens is already in the overtrain setting. Your curve looks like a typical case that Muon trained model is not well weight decayed. 3. For an overtrain setting, as our paper mentioned (https://arxiv.org/pdf/2502.16982), it is important to do weight decay on all parameters, even including the RMSNorm gamma, see Part 2.1 and Appendix D. 4. However, the default setting for megatron is to set no weight decay for RMSNorm Gamma https://github.com/NVIDIA/MegatronLM/pull/1428/filesdiffb5fac51ecd0148c2f4f8f2f1e64535089e90be87606c1f9357778d05af823220R100 A simple way to hack is to add one line to force lr_mult = 1.0 and wd_mult = 1.0 for all parameters after line 114 ","> !image  Hello, I am comparing the performance between AdamW and Muon. The experiment involves training a 1B parameter MoE model on the H800 cluster, with a maximum learning rate of 1e3 that decays to 1e4 using cosine decay. Muon uses default parameters. I observed that Muon has a significant advantage over AdamW in the early stages of training, but after 20k steps, their performance becomes similar, with AdamW sometimes even outperforming Muon. Is this phenomenon normal? Your exp looks very much like this:  Let us know if adding weight decay to all params helps!","> > !image  Hello, I am comparing the performance between AdamW and Muon. The experiment involves training a 1B parameter MoE model on the H800 cluster, with a maximum learning rate of 1e3 that decays to 1e4 using cosine decay. Muon uses default parameters. I observed that Muon has a significant advantage over AdamW in the early stages of training, but after 20k steps, their performance becomes similar, with AdamW sometimes even outperforming Muon. Is this phenomenon normal? >  > Your exp looks very much like this: >  >  > Let us know if adding weight decay to all params helps! Thank you for your kindly help. I used train loss, since the dataset is of a pretrainlevel size, it can be approximated as val loss. After 20k steps, approximately 50B tokens have been trained. I tried applying weight decay to all parameters, but it doesn't seem to help much.","> > > !image  Hello, I am comparing the performance between AdamW and Muon. The experiment involves training a 1B parameter MoE model on the H800 cluster, with a maximum learning rate of 1e3 that decays to 1e4 using cosine decay. Muon uses default parameters. I observed that Muon has a significant advantage over AdamW in the early stages of training, but after 20k steps, their performance becomes similar, with AdamW sometimes even outperforming Muon. Is this phenomenon normal? > >  > >  > > Your exp looks very much like this: > >  > > Let us know if adding weight decay to all params helps! >  > Thank you for your kindly help. I used train loss, since the dataset is of a pretrainlevel size, it can be approximated as val loss. After 20k steps, approximately 50B tokens have been trained. I tried applying weight decay to all parameters, but it doesn't seem to help much. Hi, thanks for sharing. Yeah all your settings looked fine and reasonable. Since it is pretrain level size, reporting pretrain loss is also reasonable. I am not sure what is the root cause that Muon's advantages diminishes, we found muon performed well as long as we adding correct weight decay and adjusting the update rms for matrix's shape.  If it is ok, do you mind sharing your model arch details, and we can try in our code base and data, and see what's going on","> > > !image  Hello, I am comparing the performance between AdamW and Muon. The experiment involves training a 1B parameter MoE model on the H800 cluster, with a maximum learning rate of 1e3 that decays to 1e4 using cosine decay. Muon uses default parameters. I observed that Muon has a significant advantage over AdamW in the early stages of training, but after 20k steps, their performance becomes similar, with AdamW sometimes even outperforming Muon. Is this phenomenon normal? > >  > >  > > Your exp looks very much like this: > >  > > Let us know if adding weight decay to all params helps! >  > Thank you for your kindly help. I used train loss, since the dataset is of a pretrainlevel size, it can be approximated as val loss. After 20k steps, approximately 50B tokens have been trained. I tried applying weight decay to all parameters, but it doesn't seem to help much. Another thing to debug is to observe your weight rms, max logit, output rms per layer, and update rms during the training and see if there is anything weird that is happening","Hi, thank you for opensourcing great job! I have some questions: 1. It seems that this implementation treats weight matrices for QKV projection as one parameter (`linear_qkv` of attention), rather than splitting them as in here. Is this intended? 2. Can you share some intuitions about the effect of splitting WQ, WK, and WV matrices (e.g., treating each head as a separate parameter, as mentioned in the paper)?","> Hi, thank you for opensourcing great job! I have some questions: >  > 1. It seems that this implementation treats weight matrices for QKV projection as one parameter (`linear_qkv` of attention), rather than splitting them as in here. Is this intended? > 2. Can you share some intuitions about the effect of splitting WQ, WK, and WV matrices (e.g., treating each head as a separate parameter, as mentioned in the paper)? I think the reason megatron uses fused QKV is to reduce memory access and allow the GPU to perform larger matrix multiplications. This seems more like megatron's builtin optimization rather than the moonlight author's intention. In the original codebase, it looks like they fused the operation as well, but in a slightly different way. but I'm not sure how muon processes this 3D qkv_w tensor. well, but... idk why using separate weights is better. If muon is just an approximation of a secondorder optimizer like shampoo, shouldn't it perform better when it considers more correlations between the matrices?","> Hi, thank you for opensourcing great job! I have some questions: >  > 1. It seems that this implementation treats weight matrices for QKV projection as one parameter (`linear_qkv` of attention), rather than splitting them as in here. Is this intended? > 2. Can you share some intuitions about the effect of splitting WQ, WK, and WV matrices (e.g., treating each head as a separate parameter, as mentioned in the paper)? Very good questions!   1. For the moonlight and moonlighta model, we used MLA, so Q K V are naturally split. Besides, following Keller's blog, it seems that splitting performs better. We recommending split Q K V into three matrices and update them respectively for now; 2. For splitting Q K V into multiple heads and updating them separately, I think https://leloykun.github.io/ has some experiments. For now we do not split them and update the Q K V all heads together. But in the sec 3.1 of the paper, you can see that Query projection matrix performed very differently comparing to the MLP matrix. While the Update Norm method is strictly controlled RMS to match AdamW, the Adjusted LR method we used here is not. I think there are some room here to further improve it In general, the concept of 'matrix' might not be well defined in Muon, and for now we relied on empirical results to decide the matrix split","> > Hi, thank you for opensourcing great job! I have some questions: > >  > > 1. It seems that this implementation treats weight matrices for QKV projection as one parameter (`linear_qkv` of attention), rather than splitting them as in here. Is this intended? > > 2. Can you share some intuitions about the effect of splitting WQ, WK, and WV matrices (e.g., treating each head as a separate parameter, as mentioned in the paper)? >  > I think the reason megatron uses fused QKV is to reduce memory access and allow the GPU to perform larger matrix multiplications. This seems more like megatron's builtin optimization rather than the moonlight author's intention. In the original codebase, it looks like they fused the operation as well, but in a slightly different way. but I'm not sure how muon processes this 3D qkv_w tensor. >  > well, but... idk why using separate weights is better. If muon is just an approximation of a secondorder optimizer like shampoo, shouldn't it perform better when it considers more correlations between the matrices? Yeah splitting them into three matrices performed better empirically so we followed. For moonlight, it uses MLA so it is naturally split.","> > Hi, thank you for opensourcing great job! I have some questions: > >  > > 1. It seems that this implementation treats weight matrices for QKV projection as one parameter (`linear_qkv` of attention), rather than splitting them as in here. Is this intended? > > 2. Can you share some intuitions about the effect of splitting WQ, WK, and WV matrices (e.g., treating each head as a separate parameter, as mentioned in the paper)? >  > I think the reason megatron uses fused QKV is to reduce memory access and allow the GPU to perform larger matrix multiplications. This seems more like megatron's builtin optimization rather than the moonlight author's intention. In the original codebase, it looks like they fused the operation as well, but in a slightly different way. but I'm not sure how muon processes this 3D qkv_w tensor. >  > well, but... idk why using separate weights is better. If muon is just an approximation of a secondorder optimizer like shampoo, shouldn't it perform better when it considers more correlations between the matrices? Besides the larger matrix multiplications, another advantage of using QKV fused is that you only need to gather the input between TP group once (if TP and SP are enabled) and used them for projection","> > > > !image  Hello, I am comparing the performance between AdamW and Muon. The experiment involves training a 1B parameter MoE model on the H800 cluster, with a maximum learning rate of 1e3 that decays to 1e4 using cosine decay. Muon uses default parameters. I observed that Muon has a significant advantage over AdamW in the early stages of training, but after 20k steps, their performance becomes similar, with AdamW sometimes even outperforming Muon. Is this phenomenon normal? > > >  > > >  > > > Your exp looks very much like this: > > >  > > > Let us know if adding weight decay to all params helps! > >  > >  > > Thank you for your kindly help. I used train loss, since the dataset is of a pretrainlevel size, it can be approximated as val loss. After 20k steps, approximately 50B tokens have been trained. I tried applying weight decay to all parameters, but it doesn't seem to help much. >  > Hi, thanks for sharing. Yeah all your settings looked fine and reasonable. Since it is pretrain level size, reporting pretrain loss is also reasonable. I am not sure what is the root cause that Muon's advantages diminishes, we found muon performed well as long as we adding correct weight decay and adjusting the update rms for matrix's shape. >  > If it is ok, do you mind sharing your model arch details, and we can try in our code base and data, and see what's going on The following are the settings I used in the experiment ``` !/bin/bash TEXT_DATA_PATH="""" NAME=""1b1e3qknormfactor2.45muon"" CHECKPOINT_PATH=""checkpoints/${NAME}"" TENSORBOARD_PATH=""runs/research/${NAME}"" KEEP_LATEST_CKPT=3   MICRO_BATCH_SIZE=1 GLOBAL_BATCH_SIZE=1152 TP_SIZE=1 PP_SIZE=1 EP_SIZE=1 MOE_ROUTED_EXPERTS=64 MOE_ACTIVE_ROUTED_EXPERTS=6 MOE_SHARED_EXPERTS=2 NHIDDEN=728 MOE_FFN_HIDDEN=408 MOE_SHARED_EXPERT_INTERMEDIATE_SIZE=$(($MOE_FFN_HIDDEN * $MOE_SHARED_EXPERTS)) FFN_HIDDEN=2176 NLAYERS=18 NHEADS=8 SEQ_LEN=2048 SAVE_INTERVAL=50000 TRAIN_TOKENS=100000000000  100B tokens TRAIN_SAMPLES=$((TRAIN_TOKENS / SEQ_LEN)) LR_DECAY_SAMPLES=$((TRAIN_SAMPLES * 98 / 100)) LR_WARMUP_SAMPLES=$((TRAIN_SAMPLES * 1 / 100)) NCCL_IB_QPS_PER_CONNECTION=2 script_path=""pretrain_gpt.py"" OPTIMIZER_ARGS=""     optimizer muon     muonmatchedadamwrms 0.2     adambeta1 0.9     adambeta2 0.95     adameps 1e8     lr 1e3     minlr 1e4     lrdecaystyle cosine     lrdecaysamples $LR_DECAY_SAMPLES     lrwarmupsamples $LR_WARMUP_SAMPLES     clipgrad 1.0     weightdecay 1e1     hiddendropout 0.0     attentiondropout 0.0     initiallossscale 65536 "" MOE_ARGS=""     numexperts $MOE_ROUTED_EXPERTS     moesharedexpertintermediatesize $MOE_SHARED_EXPERT_INTERMEDIATE_SIZE     moesharedexpertoverlap     moeroutertopk $MOE_ACTIVE_ROUTED_EXPERTS     moegroupedgemm     moenumfirstdenselayers 2     moeffnhiddensize $MOE_FFN_HIDDEN     expertmodelparallelsize $EP_SIZE     moepermutefusion     moerouterenableexpertbias     moerouterbiasupdaterate 1e3     expertbalancefactor 0     devicebalancefactor 0     moeglobalbatchbalance     moerouteractivationtype softmax     moeroutedscalingfactor 2.45 "" MODEL_ARGS=""     bf16     numlayers $NLAYERS     hiddensize $NHIDDEN     ffnhiddensize $FFN_HIDDEN     seqlength $SEQ_LEN     nointerleavedqkv     maxpositionembeddings $SEQ_LEN     numattentionheads $NHEADS     disablebiaslinear     addqkvbias     rotarypercent 0.5     swiglu     useflashattn     transformerimpl transformer_engine     untieembeddingsandoutputweights     positionembeddingtype rope     nopositionembedding     normalization RMSNorm     usemcoremodels     manualgc     kvchannels 128     qklayernorm "" TRAINING_ARGS=""     microbatchsize $MICRO_BATCH_SIZE     globalbatchsize $GLOBAL_BATCH_SIZE     trainsamples $TRAIN_SAMPLES     tensormodelparallelsize $TP_SIZE     pipelinemodelparallelsize $PP_SIZE     usedistributedoptimizer     overlapgradreduce "" DATA_ARGS=""     numworkers 1     traindatapath $TEXT_DATA_PATH "" OUTPUT_ARGS=""     logthroughput \     loginterval 1 \     evalinterval 0 \     timingloglevel 0 \     saveinterval $SAVE_INTERVAL \     tensorboarddir $TENSORBOARD_PATH/tensorboard \     wandbsavedir $CHECKPOINT_PATH \     wandbexpname $NAME \ "" gpt_options=""     $MODEL_ARGS     $MOE_ARGS     $TRAINING_ARGS     $OPTIMIZER_ARGS     $DATA_ARGS     $OUTPUT_ARGS     distributedtimeoutminutes 20     initmethodstd 0.006     save $CHECKPOINT_PATH     load $CHECKPOINT_PATH     saveasyncfastcheckpoint "" ```","> > > > > !image  Hello, I am comparing the performance between AdamW and Muon. The experiment involves training a 1B parameter MoE model on the H800 cluster, with a maximum learning rate of 1e3 that decays to 1e4 using cosine decay. Muon uses default parameters. I observed that Muon has a significant advantage over AdamW in the early stages of training, but after 20k steps, their performance becomes similar, with AdamW sometimes even outperforming Muon. Is this phenomenon normal? > > > >  > > > >  > > > > Your exp looks very much like this: > > > >  > > > > Let us know if adding weight decay to all params helps! > > >  > > >  > > > Thank you for your kindly help. I used train loss, since the dataset is of a pretrainlevel size, it can be approximated as val loss. After 20k steps, approximately 50B tokens have been trained. I tried applying weight decay to all parameters, but it doesn't seem to help much. > >  > >  > > Hi, thanks for sharing. Yeah all your settings looked fine and reasonable. Since it is pretrain level size, reporting pretrain loss is also reasonable. I am not sure what is the root cause that Muon's advantages diminishes, we found muon performed well as long as we adding correct weight decay and adjusting the update rms for matrix's shape. > > If it is ok, do you mind sharing your model arch details, and we can try in our code base and data, and see what's going on >  > The following are the settings I used in the experiment >  > ``` > !/bin/bash >  > TEXT_DATA_PATH="""" >  > NAME=""1b1e3qknormfactor2.45muon"" > CHECKPOINT_PATH=""checkpoints/${NAME}"" > TENSORBOARD_PATH=""runs/research/${NAME}"" > KEEP_LATEST_CKPT=3   >  > MICRO_BATCH_SIZE=1 > GLOBAL_BATCH_SIZE=1152 >  > TP_SIZE=1 > PP_SIZE=1 > EP_SIZE=1 >  > MOE_ROUTED_EXPERTS=64 > MOE_ACTIVE_ROUTED_EXPERTS=6 > MOE_SHARED_EXPERTS=2 >  > NHIDDEN=728 > MOE_FFN_HIDDEN=408 > MOE_SHARED_EXPERT_INTERMEDIATE_SIZE=$(($MOE_FFN_HIDDEN * $MOE_SHARED_EXPERTS)) > FFN_HIDDEN=2176 > NLAYERS=18 > NHEADS=8 >  > SEQ_LEN=2048 >  > SAVE_INTERVAL=50000 >  > TRAIN_TOKENS=100000000000  100B tokens > TRAIN_SAMPLES=$((TRAIN_TOKENS / SEQ_LEN)) > LR_DECAY_SAMPLES=$((TRAIN_SAMPLES * 98 / 100)) > LR_WARMUP_SAMPLES=$((TRAIN_SAMPLES * 1 / 100)) >  > NCCL_IB_QPS_PER_CONNECTION=2 >  > script_path=""pretrain_gpt.py"" >  > OPTIMIZER_ARGS="" >     optimizer muon >     muonmatchedadamwrms 0.2 >     adambeta1 0.9 >     adambeta2 0.95 >     adameps 1e8 >     lr 1e3 >     minlr 1e4 >     lrdecaystyle cosine >     lrdecaysamples $LR_DECAY_SAMPLES >     lrwarmupsamples $LR_WARMUP_SAMPLES >     clipgrad 1.0 >     weightdecay 1e1 >     hiddendropout 0.0 >     attentiondropout 0.0 >     initiallossscale 65536 > "" >  > MOE_ARGS="" >     numexperts $MOE_ROUTED_EXPERTS >     moesharedexpertintermediatesize $MOE_SHARED_EXPERT_INTERMEDIATE_SIZE >     moesharedexpertoverlap >     moeroutertopk $MOE_ACTIVE_ROUTED_EXPERTS >     moegroupedgemm >     moenumfirstdenselayers 2 >     moeffnhiddensize $MOE_FFN_HIDDEN >     expertmodelparallelsize $EP_SIZE >     moepermutefusion >     moerouterenableexpertbias >     moerouterbiasupdaterate 1e3 >     expertbalancefactor 0 >     devicebalancefactor 0 >     moeglobalbatchbalance >     moerouteractivationtype softmax >     moeroutedscalingfactor 2.45 > "" >  > MODEL_ARGS="" >     bf16 >     numlayers $NLAYERS >     hiddensize $NHIDDEN >     ffnhiddensize $FFN_HIDDEN >     seqlength $SEQ_LEN >     nointerleavedqkv >     maxpositionembeddings $SEQ_LEN >     numattentionheads $NHEADS >     disablebiaslinear >     addqkvbias >     rotarypercent 0.5 >     swiglu >     useflashattn >     transformerimpl transformer_engine >     untieembeddingsandoutputweights >     positionembeddingtype rope >     nopositionembedding >     normalization RMSNorm >     usemcoremodels >     manualgc >     kvchannels 128 >     qklayernorm > "" >  > TRAINING_ARGS="" >     microbatchsize $MICRO_BATCH_SIZE >     globalbatchsize $GLOBAL_BATCH_SIZE >     trainsamples $TRAIN_SAMPLES >     tensormodelparallelsize $TP_SIZE >     pipelinemodelparallelsize $PP_SIZE >     usedistributedoptimizer >     overlapgradreduce > "" >  > DATA_ARGS="" >     numworkers 1 >     traindatapath $TEXT_DATA_PATH > "" >  > OUTPUT_ARGS="" >     logthroughput \ >     loginterval 1 \ >     evalinterval 0 \ >     timingloglevel 0 \ >     saveinterval $SAVE_INTERVAL \ >     tensorboarddir $TENSORBOARD_PATH/tensorboard \ >     wandbsavedir $CHECKPOINT_PATH \ >     wandbexpname $NAME \ > "" >  > gpt_options="" >     $MODEL_ARGS >     $MOE_ARGS >     $TRAINING_ARGS >     $OPTIMIZER_ARGS >     $DATA_ARGS >     $OUTPUT_ARGS >     distributedtimeoutminutes 20 >     initmethodstd 0.006 >     save $CHECKPOINT_PATH >     load $CHECKPOINT_PATH >     saveasyncfastcheckpoint > "" > ``` Hi,  your model arch looks reasonable.  For the purpose of debugging, I'll need more monitoring that current open source megatronlm does not have. So I'll run in our internal infra, with some slight changes: 1. We will use our own data so the seqlen will be changed from 2048 to 8192. Correspondingly, the bsz will be changed from 1152 to 288 2. we will not add the attention qk bias 3. we will update q k v three matrices separately 4. since we are using moe with auxfree bias and a scaling factor of 2.45, I'll use the sigmoid gate, rather than the softmax gate Other settings will remain the same as you posted. We'll keep you posted about our findings","  I am running on your two configs right now and not sure about the results yet. But I did some math and probably found out the problem: **the model might be too small comparing to its embedding.** We have a 160K vocab size, (I am not sure about yours, do you mind sharing it?), so the parameters became: Total Params: total = 1,351,721,016 **embedding = 768 X 163840 X 2 = 251,658,240** total excluding embedding = 1,351,721,016  251,658,240 = 1,100,062,776 Activated Params: not activated = 18 X 408 X 728 X 3 X 58 = 930,279,168 total activated = 1,351,721,016  930,279,168 = 421,441,848 **total activated excluding embedding: 1,100,062,776  930,279,168 = 169,783,608** So you can see, the model has **~170M nonembedding activated params,** about 1.1B nonembedding total and **~252M word embeddings or LM heads**. Because the word embeddings and LM heads are updated by the AdamW, so maybe in the long run, there are not too much differences. I would recommend to try on a larger model as well, for example the 822M one as listed below. We ran on this model with AdamW and Muon for 100B tokens and still see big differences: ",">  >  > I am running on your two configs right now and not sure about the results yet. But I did some math and probably found out the problem: **the model might be too small comparing to its embedding.** We have a 160K vocab size, (I am not sure about yours, do you mind sharing it?), so the parameters became: >  > Total Params: total = 1,351,721,016 **embedding = 768 X 163840 X 2 = 251,658,240** total excluding embedding = 1,351,721,016  251,658,240 = 1,100,062,776 >  > Activated Params: not activated = 18 X 408 X 728 X 3 X 58 = 930,279,168 total activated = 1,351,721,016  930,279,168 = 421,441,848 **total activated excluding embedding: 1,100,062,776  930,279,168 = 169,783,608** >  > So you can see, the model has **~170M nonembedding activated params,** about 1.1B nonembedding total and **~252M word embeddings or LM heads**. Because the word embeddings and LM heads are updated by the AdamW, so maybe in the long run, there are not too much differences. >  > I would recommend to try on a larger model as well, for example the 822M one as listed below. We ran on this model with AdamW and Muon for 100B tokens and still see big differences: >  >  Our tokenizer size is 150k, and it is very likely the reason behind the issue. I will switch to a 60k tokenizer and increase the hidden size and the number of layers for a new experiment.","> >  > > I am running on your two configs right now and not sure about the results yet. But I did some math and probably found out the problem: **the model might be too small comparing to its embedding.** We have a 160K vocab size, (I am not sure about yours, do you mind sharing it?), so the parameters became: > > Total Params: total = 1,351,721,016 **embedding = 768 X 163840 X 2 = 251,658,240** total excluding embedding = 1,351,721,016  251,658,240 = 1,100,062,776 > > Activated Params: not activated = 18 X 408 X 728 X 3 X 58 = 930,279,168 total activated = 1,351,721,016  930,279,168 = 421,441,848 **total activated excluding embedding: 1,100,062,776  930,279,168 = 169,783,608** > > So you can see, the model has **~170M nonembedding activated params,** about 1.1B nonembedding total and **~252M word embeddings or LM heads**. Because the word embeddings and LM heads are updated by the AdamW, so maybe in the long run, there are not too much differences. > > I would recommend to try on a larger model as well, for example the 822M one as listed below. We ran on this model with AdamW and Muon for 100B tokens and still see big differences: > >  >  > Our tokenizer size is 150k, and it is very likely the reason behind the issue. I will switch to a 60k tokenizer and increase the hidden size and the number of layers for a new experiment. Yeah, that would be better to get rid of the impacts of large embeddings. I am still running the two comparing jobs based on your previous smaller model setting in progress.   Besides increasing the, another thing worth mentioning is to use/report the OOD validation data rather than in domain validation data for a more accurate eval of the model.","hi,  We ran your settings for about ~17K steps by now and for about ~40+B tokens (You mentioned before that ~20K steps, the advantages diminish. Even though with the big embedding issue, I actually think the result is promising. We plot the figure as shown below: 1. With proper smoothing, we can see the training loss gap of muon is not diminishing 2. We define a new metric, Muon Leading Steps, to understand how many extra steps that AdamW needs to match Muon's performances 3. Besides, we can use a simple ratio metric Muon_Leading_Steps/Muon_Trained_Steps to help understand that if Muon is consistently leading !image","For the purpose of reproducing, we provide the script to generate these figures.  Can you help to try on such figures based on your previous small run data as well? ``` if ""validation"" or 'training' in tag:      smooth the data by emw     ewm_alpha = 0.005     muon_data = muon_data.ewm(alpha=ewm_alpha).mean()     adam_data = adam_data.ewm(alpha=ewm_alpha).mean()      Subsample both dataframes to ~1000 rows for cleaner plotting     num_samples = 1000     stride = max(1, len(muon_data) // num_samples)     muon_data = muon_data.iloc[::stride]     adam_data = adam_data.iloc[::stride]  columns = wall_time, step, value  assuming steps are sorted  while losses are not sorted, but it is generally decreasing, and the lower the loss the better  for each step of muon loss, find the smallest step of adam loss that is smaller than the muon loss  plot the step difference between the two steps, that is how much steps does adam need to take to match the muon loss import matplotlib.pyplot as plt def find_matching_step(target_value: float, reference_df: pd.DataFrame) > int:     """"""Find the first step where reference loss is lower than target value.     Args:         target_value: Loss value to match or beat         reference_df: DataFrame containing step and value columns     Returns:         Step number where reference loss first beats target, or max step if never beats     """"""     mask = reference_df[""value""] "," Besides, we also evaluated on OOD lm validation loss data, and it showed pretty good results ", We'll wait for you visualization results and see how it goes! Thanks!,"hi guys, let me share my vibe check results. i tested small scale proxy model with 64*4096=262k batch tokens and 40k horizon, so they consumed 10.5B tokens. my model config is like standard parameterization (SP) with 0.2 std, GQA, not separated QKV, lr 0.00195, weight decay 0.1 (didn't decay rmsnorm gamma) for 12 layers with 2 width (hidden size), 1024 and 4096. the larger one (4096) is approximately 3.5B. and here are my results. for smaller one, muon outperforms adamw.  and for larger model, muon looks promising too but there is some issue that adamw diverges.  however the real problem is that the throughput of muon is bad at multi node setup. i used 4 node A100 for larger model, and it's throughput seems it needs to be optimized. i think gradient allgather for NS should be overlapped or grad bucketing should be carefully designed.   ","  Thanks for sharing! I have some comments regarding your runs: 1. For muon's performances, this is what we actually expected to see! Thanks for sharing! It would be better stability if you norm your weight decay gamma (as mentioned in the appendix) and maybe your adamw lr is too big so it actually does not converge 2. For the throughput issue, this is probably because the distributed optimizer implementation changes. We noticed this gap when porting our internal impl to the open sourced one. Previously in MegatronLM, the distributed optimizer states are concat together and flatten into a list (the concatting order is defined by the params init order). Then the list is split into DP parts. So only those params (very few actually if you think about it) that are split in the DP boundary will need the extra gather.  However, the current impl of distributed optimizer is first to group in several params into a bucket. **And every params in that bucket will be split into DP parts and needs a gather!** Thus, bringing the extra needed all gather to its upperbound, which means every params in every rank needs to a gather. For distributed muon to work efficiently as described in our paper. We need the original way of DP sharding optimizer states, which only requires very limited params to do the extra gathering"," wow, your response is as fast as the speed of light, lol. I didnâ€™t even know that megatron changed its sharding logic. (Iâ€™m also familiar with the sharding strategy you mentioned in point 2.) Iâ€™ll dig into the codebase and come back if I find any clues to improve performance. edited) can you share related PR for refactoring param and grad bucketing? I'm not sure this one is right.","  Thanks for your kind words! However, I am not sure when they changed the logic since we do not merge the upstream for a while. We just noticed when we are preparing this PR. The commit you find looks very related but I am not sure if that's the only related one. BTW, do you mind visualizing your results using the script I mentioned above? It is exciting to see some other people reproducing similar results of Muon!","> 2\. For the throughput issue, this is probably because the distributed optimizer implementation changes. We noticed this gap when porting our internal impl to the open sourced one. Previously in MegatronLM, the distributed optimizer states are concat together and flatten into a list (the concatting order is defined by the params init order). Then the list is split into DP parts. So only those params (very few actually if you think about it) that are split in the DP boundary will need the extra gather. Thank you for your detailed guidance! Here are my findings from today: 1. In my initial setup (1B MoE model + 150k tokenizer), Muon didn't show an advantage over Adam. However, after using your plotting code, I did observe that Muon's loss appeared lower. I believe this was due to the ewm_alpha in the plotting code being set too low, causing excessive smoothing. After adjusting this value to 0.05, the results, as shown in the attached image, align with the original experiment logs on Wandb. !comparison_plot2 2. Using the new setup (4B MoE model + 60k tokenizer) has yielded excellent results! Muon's performance consistently outperforms Adam. This is very encouraging, and I will be experimenting with larger models in the future. !comparison_plot 3. Unfortunately, I also observed results similar to  's findings. When using Muon, cluster throughput significantly decreases. I look forward to further community optimizations in this area. !image 4. I also noticed that when using Muon, the L2 norm of the model output is several times higher compared to Adam. Do you have any tuning recommendations for this? !image !image","> > 2. For the throughput issue, this is probably because the distributed optimizer implementation changes. We noticed this gap when porting our internal impl to the open sourced one. Previously in MegatronLM, the distributed optimizer states are concat together and flatten into a list (the concatting order is defined by the params init order). Then the list is split into DP parts. So only those params (very few actually if you think about it) that are split in the DP boundary will need the extra gather. >  > Thank you for your detailed guidance! Here are my findings from today: >  > 1. In my initial setup (1B MoE model + 150k tokenizer), Muon didn't show an advantage over Adam. However, after using your plotting code, I did observe that Muon's loss appeared lower. I believe this was due to the ewm_alpha in the plotting code being set too low, causing excessive smoothing. After adjusting this value to 0.05, the results, as shown in the attached image, align with the original experiment logs on Wandb. >    !comparison_plot2 > 2. Using the new setup (4B MoE model + 60k tokenizer) has yielded excellent results! Muon's performance consistently outperforms Adam. This is very encouraging, and I will be experimenting with larger models in the future. >    !comparison_plot > 3. Unfortunately, I also observed results similar to  's findings. When using Muon, cluster throughput significantly decreases. I look forward to further community optimizations in this area. >    !image > 4. I also noticed that when using Muon, the L2 norm of the model output is several times higher compared to Adam. Do you have any tuning recommendations for this? >    !image >    !image Glad to see it works! Regarding your questions: 1. Thanks for your visualization. It clears showed that in this setting, AdamW is close Muon. I still do not know why, as my experiments in a similar setting like your still worked fine for me, with Muon having a reasonable leading ratio; 2. Glad to see the figure of your setting 2! 3. For the throughput issue, I think it is caused by the same reason as what  met. However, changing the distributed optimizer's impl might be nontrivial.. I'll discuss with the community and see if we can find some easy way to hack it. The key is to avoid too many parameters being shared, so the extra communication will be minimal. This will also naturally reduce the number of computations needed for NS 4. For the large output RMS issue, we also monitored it, but we do not see any problem. Can you check if your RMSNorm Gamma is properly weight decayed?  **Besides, your leading step ratio figure would be better if you can set 'ylim(0,1)'. Would you mind replotting the figure and do you mind me sharing it on X as it reproduces our results!**","   Discussed with my colleagues and we might need more investigation on this performance dropping issue. If more gatherings happened, more NS iterative steps calculation will also happen for more params",Glad to share our replication results. !comparison_plot," btw, can i ask how you log per param activation norm?? it's too heavy to reduce or gather all stats in forward when naively register forward hook, so I'm curious how you do efficiently logging ! ",">  btw, can i ask how you log per param activation norm?? it's too heavy to reduce or gather all stats in forward when naively register forward hook, so I'm curious how you do efficiently logging ! I think  showed the output l2norm of the model rather than the per param activation norm? For output l2norm, you can open a log buffer to log the detached l2norm or rms of the output, (do not do the sqrt, but only the square sum), accumulate it in the fwdbwd, and only reduce once after all the fwdbwd is done. This is cheap as you only do it once per global step","Some other issues asked why distributed muon is efficient, and tried to explain it with details: https://github.com/MoonshotAI/Moonlight/issues/16"
toothacher17,a proof of concept for Distributed Muon,,2025-02-24T09:01:50Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1427
toothacher17,Revert 1 moonshot/distributedmuon,,2025-02-24T08:53:08Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1426
thuliu-yt16,[BUG] flash_decode=True produces wrong output,"**Describe the bug** inference the model with `flash_decode=True` produces different results with no flash_decode. The result seems weird and wrong. **To Reproduce** 1. Follow run_text_generation_llama3.1.sh to deploy llama3.18Binstruct with the argument `flash_decode=True` 2. Query the server with  ```bash curl request PUT 'http://127.0.0.1:5000/api' header 'ContentType: application/json' data '{     ""prompts"": [""Q:What is the color of an apple""], ""top_k"": 1 }' ``` The result is  ``` Q:What is the color of an apple?\nA:There is no answer to this question as it is incomplete. Please provide the complete question. I'll be happy to help. \nIf you meant to ask about the color of an object, please provide the name of the object, and I'll do my best to provide the correct answer. \n\nFor exampl ``` The model seems to ignore the token ""apple"".  **Expected behavior** If `flash_decode=False` the result is  ``` Q:What is the color of an apple?\nA: The color of an apple can vary depending on the type of apple. Some common colors of apples include red, green, yellow, and sometimes a combination of these colors.\nQ:What is the color of a banana?\nA: The color of a banana is typically yellow. However, bananas can also turn ``` They should look same, at least similar, but not.  **Environment (please complete the following information):**   MegatronLM commit ID: 61b2c4f9c6607e242ef9af44c2926a923f12a5af   PyTorch version: 2.6.0a0+df5bbc09d1.nv24.12   CUDA version: 12.6    NCCL version: 2.23.4 **Proposed fix** I dived into the code and found it is because the kv cache is wrongly truncated here ((https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/transformer/attention.pyL313): ```python cache_seqlens = sequence_len_offset  1 ``` This line makes the last token in the kv cache unattended which is consistent with the bug behaviour (the token ""apple"" is ignored). So I changed to  ```python cache_seqlens = sequence_len_offset ``` and the result seems correct. PR is CC(Fix flash decoding).",2025-02-22T13:59:26Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1425
xiaohoua,[QUESTION] How to quantize CLIP model by Megatron?,How to quantify the pretraining model of pruning openCLIP by Megatron. I was unable to find documentation to guide me in using Megatron to quantify a similar crossmodal model for pruning,2025-02-22T11:04:30Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1424
mhuguesaws,[QUESTION] GPUDirect Storage for checkpointing,**Your question** Is there an option to use GPUDirect Storage for checkpointing?,2025-02-21T16:29:09Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1423
lhb8125,Hongbinl/1f1b combine poc,,2025-02-20T10:22:57Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1422
chotzen,[BUG] Token routing probability all-gather precision in token_dispatcher causes differing results between EP ranks,"**Describe the bug** I have a setup on a small MoE model on 2 H100s with 2way EP (DP), 1way TP/PP. I am feeding the same token sequence into the model on both DP ranks, and expect the activations after the MoE to be the same, since the tokens from both sequences should be routed to the same experts.  However, I observe the outputs after token unpermutation on each rank to differ by 0.1% to 2%. This is concerning, since the error continues to compound on top of this after many layers.  I've tracked the source of the error to the communication of the routing probabilities. Specifically, converting the routing probabilities to `torch.float32` before performing the allgather eliminates this error.   **To Reproduce**  Instantiate an MoE model with 2way EP/DP sharding using `MoEAllGatherTokenDispatcher`  Feed the same tokens into both DP shards  Record the outputs of the MoE on both GPUs, and observe that they are different. **Expected behavior** The MoE output should be the same. **Stack trace/logs** N/A **Environment (please complete the following information):**   MegatronLM commit ID: `aa719a0b0145481fb9212c577ee9a3f000fd16da` + internal patches   PyTorch version: 2.5.1   CUDA version: 12.2   NCCL version: 2.21.5 **Proposed fix** Add `probs = probs.to(torch.float32)` before the allgather on this line to perform the allgather in float32. **Additional context** N/A",2025-02-20T04:58:49Z,,open,0,4,https://github.com/NVIDIA/Megatron-LM/issues/1421,"Thanks for reporting the issue! I suspect the discrepancy is due to the different accumulation orders of reduction during token combination. We've received feedback from other customers suggesting that reduction should use fp32. **Update: the MR has been merged.** There's an internal MR to promote routing and weighted averaging data types to prevent precision loss, which we'll merge ASAP.","Hi  , after manually setting the probs to fp64, I still have precision issues with EP. Do you have any suggestion on what else need to be promoted to higher precision? Thanks!","> Hi [](https://github.com/yanring) , after manually setting the probs to fp64, I still have precision issues with EP. Do you have any suggestion on what else need to be promoted to higher precision? Thanks! I assume you're using the arg moerouterdtype, right? Did you also enable TP? By the way, could you try the allgather dispatcher to see if the issue still exists?","Hi   Yeah I set `moerouterdtype` to fp64. I ran on master branch the following test but it failed.  Basically the test checks if the output is the same between a `1024` length random sequence and its truncated version (setting `input[90:] = 0`. ```python         input = torch.randn(1024, 1, 4096).cuda().bfloat16()         input_trunc90 = input.clone()         input_trunc90[90:] = 0 ``` ... ```python assert torch.allclose(notrunc_output[:90], trunc_output[:90], atol=1e3, rtol=1e3) ``` Can you please help take a look? Thanks!   test.py  ```python  Copyright (c) 2023, NVIDIA CORPORATION. All rights reserved. import time import pytest import torch from megatron.core import parallel_state from megatron.core.models.gpt.gpt_layer_specs import get_gpt_layer_local_spec from megatron.core.transformer.moe.moe_layer import MoELayer from megatron.core.transformer.transformer_config import TransformerConfig from megatron.core.transformer.transformer_layer import TransformerLayer from megatron.training.initialize import _set_random_seed from tests.unit_tests.test_utilities import Utils class TestMoELayerDispatcherDiscrepancy:     def setup_method(self, method):         pass      .mark.parametrize(""num_moe_experts"", [8, 16, 32, 64])     .mark.parametrize(""num_moe_experts"", [64])     .mark.parametrize(""grouped_gemm"", [False])      .mark.parametrize(""tp_size,ep_size"", [(4, 1)])      .mark.parametrize(""tp_size,ep_size"", [(1, 1)])     .mark.parametrize(""tp_size,ep_size"", [(1, 4)])      .mark.parametrize(""tp_size,ep_size"", [(1, 1), (1, 2), (1, 4)])     .mark.parametrize(""bf16"", [True])      .mark.parametrize(""bf16"", [True, False])      .mark.parametrize(""moe_length"", [1])     .mark.parametrize(""moe_length"", [4])      .mark.parametrize(""moe_length"", [1, 2, 3, 4])     .mark.parametrize(""moe_token_dispatcher_type"", [""allgather"", ""alltoall""])     .mark.internal     def test_truncation(         self, num_moe_experts, grouped_gemm, tp_size, ep_size, bf16, moe_length, moe_token_dispatcher_type     ):         Utils.initialize_model_parallel(             tensor_model_parallel_size=tp_size, expert_model_parallel_size=ep_size         )          Init input and layer         _set_random_seed(seed_=123, data_parallel_random_init=False)         input = torch.randn(1024, 1, 4096).cuda().bfloat16()         input_trunc90 = input.clone()         input_trunc90[90:] = 0         self.transformer_config = TransformerConfig(             num_layers=1,             hidden_size=4096,             num_attention_heads=16,             num_moe_experts=num_moe_experts,             use_cpu_initialization=False,             moe_token_dispatcher_type=moe_token_dispatcher_type,              moe_shared_expert_intermediate_size =              moe_router_topk=6,             moe_aux_loss_coeff=0.01,             moe_grouped_gemm=grouped_gemm,             moe_router_dtype=""fp64"",             add_bias_linear=False,             tensor_model_parallel_size=tp_size,             expert_model_parallel_size=ep_size,             sequence_parallel=False,             bf16=bf16,         )         transformer_layer_spec = get_gpt_layer_local_spec(             num_experts=num_moe_experts, moe_grouped_gemm=grouped_gemm         )         layer = (             TransformerLayer(self.transformer_config, transformer_layer_spec.submodules)             .cuda()             .bfloat16()         )          moe_layer = [layer.mlp, layer.mlp2, layer.mlp3, layer.mlp4]         moe_layer = [layer.mlp] * moe_length          moe_layer2 = layer.mlp2          moe_layer3 = layer.mlp3          moe_layer4 = layer.mlp4         layer.eval()          Test allgather dispatcher         import sys         for idx in range(moe_length):             moe_layer[idx].config.moe_token_dispatcher_type = ""allgather""         with torch.no_grad():              notrunc_output = moe_layer(input)[0]             xx = input             for idx in range(moe_length):                 xx = moe_layeridx                  sys.stderr.write(f""notrunc: xx shape: {xx.shape}\n"")                 xx = xx[0]             notrunc_output = xx          Allgather the output to check if it's the same in all ranks         notrunc_output_ag_shape = (torch.distributed.get_world_size(), *(notrunc_output.shape))         notrunc_output_ag = torch.zeros(             notrunc_output_ag_shape, device=notrunc_output.device, dtype=notrunc_output.dtype         )         torch.distributed.all_gather_into_tensor(             notrunc_output_ag, notrunc_output, group=torch.distributed.group.WORLD         )          Check if output is the same across all ranks         if parallel_state.get_data_parallel_rank() == 0:             for i in range(1, parallel_state.get_tensor_model_parallel_world_size()):                 if not torch.equal(notrunc_output_ag[0], notrunc_output_ag[i]):                     print(f""Allgather output differs at rank {torch.distributed.get_rank()}"")                     raise ValueError(""Allgather output differs at rank {i}"")             print(f""Allgather output is the same across all ranks"", flush=True)         torch.cuda.synchronize()          Test alltoall dispatcher          moe_layer.config.moe_token_dispatcher_type = ""alltoall""         with torch.no_grad():             xx = input_trunc90              for _ in range(4):             for idx in range(moe_length):                 xx = moe_layeridx                  sys.stderr.write(f""notrunc: xx shape: {xx.shape}\n"")                 xx = xx[0]                  sys.stderr.write(f""trunc: xx shape: {xx.shape}\n"")             trunc_output = xx          Allgather the output to check if it's the same in all ranks         trunc_output_ag_shape = (torch.distributed.get_world_size(), *(trunc_output.shape))         trunc_output_ag = torch.zeros(             trunc_output_ag_shape, device=trunc_output.device, dtype=trunc_output.dtype         )         torch.distributed.all_gather_into_tensor(             trunc_output_ag, trunc_output, group=torch.distributed.group.WORLD         )          Check if output is the same across all ranks         if parallel_state.get_data_parallel_rank() == 0:             for i in range(1, parallel_state.get_tensor_model_parallel_world_size()):                 if not torch.equal(trunc_output_ag[0], trunc_output_ag[i]):                     print(f""A2A output differs at rank {torch.distributed.get_rank()}"")                     raise ValueError(""A2A output differs at rank {i}"")             print(f""A2A output is the same across all ranks"", flush=True)         torch.cuda.synchronize()          if torch.distributed.get_rank() == 0:              from IPython import embed; embed()          else:              import time; time.sleep(1000000)         assert torch.allclose(notrunc_output[:90], trunc_output[:90], atol=1e3, rtol=1e3)         print(f""Allgather and A2A output is the same"", flush=True)         Utils.destroy_model_parallel() ``` "
jasonchiu-codeium,[BUG] `get_rotary_seq_len` isn't always returning a float,"**Describe the bug** `get_rotary_seq_len` may return a `Tensor` instead of a float. This seems like a small oversight. **To Reproduce** **Expected behavior** No internal error **Stack trace/logs** This error was detected from this stacktrace ``` [...]   File ""/ephemeral/devcontainer/jasonchiu/cache/bazel/_bazel_jasonchiu/996a28cb1c2af162dca7531bd6a2de53/execroot/_main/bazelout/k8opt/bin/exa/trainer/megatron/megatron_trainer_test.runfiles/local_pip_torch/sitepackages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/ephemeral/devcontainer/jasonchiu/cache/bazel/_bazel_jasonchiu/996a28cb1c2af162dca7531bd6a2de53/execroot/_main/bazelout/k8opt/bin/exa/trainer/megatron/megatron_trainer_test.runfiles/local_pip_torch/sitepackages/torch/nn/modules/module.py"", line 1747, in _call_impl     return forward_call(*args, **kwargs)   File ""/ephemeral/devcontainer/jasonchiu/cache/bazel/_bazel_jasonchiu/996a28cb1c2af162dca7531bd6a2de53/execroot/_main/bazelout/k8opt/bin/exa/trainer/megatron/megatron_trainer_test.runfiles/_main/third_party/megatron_lm/MegatronLM/megatron/core/distributed/data_parallel_base.py"", line 22, in forward     return self.module(*inputs, **kwargs)   File ""/ephemeral/devcontainer/jasonchiu/cache/bazel/_bazel_jasonchiu/996a28cb1c2af162dca7531bd6a2de53/execroot/_main/bazelout/k8opt/bin/exa/trainer/megatron/megatron_trainer_test.runfiles/local_pip_torch/sitepackages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/ephemeral/devcontainer/jasonchiu/cache/bazel/_bazel_jasonchiu/996a28cb1c2af162dca7531bd6a2de53/execroot/_main/bazelout/k8opt/bin/exa/trainer/megatron/megatron_trainer_test.runfiles/local_pip_torch/sitepackages/torch/nn/modules/module.py"", line 1747, in _call_impl     return forward_call(*args, **kwargs)   File ""/ephemeral/devcontainer/jasonchiu/cache/bazel/_bazel_jasonchiu/996a28cb1c2af162dca7531bd6a2de53/execroot/_main/bazelout/k8opt/bin/exa/trainer/megatron/megatron_trainer_test.runfiles/_main/third_party/megatron_lm/MegatronLM/megatron/core/transformer/module.py"", line 178, in forward     outputs = self.module(*inputs, **kwargs)   File ""/ephemeral/devcontainer/jasonchiu/cache/bazel/_bazel_jasonchiu/996a28cb1c2af162dca7531bd6a2de53/execroot/_main/bazelout/k8opt/bin/exa/trainer/megatron/megatron_trainer_test.runfiles/local_pip_torch/sitepackages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/ephemeral/devcontainer/jasonchiu/cache/bazel/_bazel_jasonchiu/996a28cb1c2af162dca7531bd6a2de53/execroot/_main/bazelout/k8opt/bin/exa/trainer/megatron/megatron_trainer_test.runfiles/local_pip_torch/sitepackages/torch/nn/modules/module.py"", line 1747, in _call_impl     return forward_call(*args, **kwargs)   File ""/ephemeral/devcontainer/jasonchiu/cache/bazel/_bazel_jasonchiu/996a28cb1c2af162dca7531bd6a2de53/execroot/_main/bazelout/k8opt/bin/exa/trainer/megatron/megatron_trainer_test.runfiles/_main/third_party/megatron_lm/MegatronLM/megatron/core/models/gpt/gpt_model.py"", line 265, in forward     rotary_pos_emb = self.rotary_pos_emb(   File ""/ephemeral/devcontainer/jasonchiu/cache/bazel/_bazel_jasonchiu/996a28cb1c2af162dca7531bd6a2de53/execroot/_main/bazelout/k8opt/bin/exa/trainer/megatron/megatron_trainer_test.runfiles/local_pip_torch/sitepackages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/ephemeral/devcontainer/jasonchiu/cache/bazel/_bazel_jasonchiu/996a28cb1c2af162dca7531bd6a2de53/execroot/_main/bazelout/k8opt/bin/exa/trainer/megatron/megatron_trainer_test.runfiles/local_pip_torch/sitepackages/torch/nn/modules/module.py"", line 1747, in _call_impl     return forward_call(*args, **kwargs)   File ""/ephemeral/devcontainer/jasonchiu/cache/bazel/_bazel_jasonchiu/996a28cb1c2af162dca7531bd6a2de53/execroot/_main/bazelout/k8opt/bin/exa/trainer/megatron/megatron_trainer_test.runfiles/_main/third_party/megatron_lm/MegatronLM/megatron/core/models/common/embeddings/rotary_pos_embedding.py"", line 172, in forward     freqs = self.get_freqs_non_repeated(max_seq_len, offset)   File ""/ephemeral/devcontainer/jasonchiu/cache/bazel/_bazel_jasonchiu/996a28cb1c2af162dca7531bd6a2de53/execroot/_main/bazelout/k8opt/bin/exa/trainer/megatron/megatron_trainer_test.runfiles/_main/third_party/megatron_lm/MegatronLM/megatron/core/models/common/embeddings/rotary_pos_embedding.py"", line 137, in get_freqs_non_repeated     torch.arange(max_seq_len, device=self.inv_freq.device, dtype=self.inv_freq.dtype) ``` **Environment (please complete the following information):**   MegatronLM commit ID: `9dcd0ec81535573dfcf4a7a240bbc636658be68b`   PyTorch version: 2.5.1   CUDA version: 12.2   NCCL version: 2.21.5 **Proposed fix* CC(fix: return float instead of tensor from `get_rotary_seq_len`) **Additional context**",2025-02-20T03:22:04Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1420
jasonchiu-codeium,fix: return float instead of tensor from `get_rotary_seq_len`,"The below error stacktrace occurs due to `get_rotary_seq_len` potentially returning a tensor value instead of its float value. ``` [...]   File ""/ephemeral/devcontainer/jasonchiu/cache/bazel/_bazel_jasonchiu/996a28cb1c2af162dca7531bd6a2de53/execroot/_main/bazelout/k8opt/bin/exa/trainer/megatron/megatron_trainer_test.runfiles/local_pip_torch/sitepackages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/ephemeral/devcontainer/jasonchiu/cache/bazel/_bazel_jasonchiu/996a28cb1c2af162dca7531bd6a2de53/execroot/_main/bazelout/k8opt/bin/exa/trainer/megatron/megatron_trainer_test.runfiles/local_pip_torch/sitepackages/torch/nn/modules/module.py"", line 1747, in _call_impl     return forward_call(*args, **kwargs)   File ""/ephemeral/devcontainer/jasonchiu/cache/bazel/_bazel_jasonchiu/996a28cb1c2af162dca7531bd6a2de53/execroot/_main/bazelout/k8opt/bin/exa/trainer/megatron/megatron_trainer_test.runfiles/_main/third_party/megatron_lm/MegatronLM/megatron/core/distributed/data_parallel_base.py"", line 22, in forward     return self.module(*inputs, **kwargs)   File ""/ephemeral/devcontainer/jasonchiu/cache/bazel/_bazel_jasonchiu/996a28cb1c2af162dca7531bd6a2de53/execroot/_main/bazelout/k8opt/bin/exa/trainer/megatron/megatron_trainer_test.runfiles/local_pip_torch/sitepackages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/ephemeral/devcontainer/jasonchiu/cache/bazel/_bazel_jasonchiu/996a28cb1c2af162dca7531bd6a2de53/execroot/_main/bazelout/k8opt/bin/exa/trainer/megatron/megatron_trainer_test.runfiles/local_pip_torch/sitepackages/torch/nn/modules/module.py"", line 1747, in _call_impl     return forward_call(*args, **kwargs)   File ""/ephemeral/devcontainer/jasonchiu/cache/bazel/_bazel_jasonchiu/996a28cb1c2af162dca7531bd6a2de53/execroot/_main/bazelout/k8opt/bin/exa/trainer/megatron/megatron_trainer_test.runfiles/_main/third_party/megatron_lm/MegatronLM/megatron/core/transformer/module.py"", line 178, in forward     outputs = self.module(*inputs, **kwargs)   File ""/ephemeral/devcontainer/jasonchiu/cache/bazel/_bazel_jasonchiu/996a28cb1c2af162dca7531bd6a2de53/execroot/_main/bazelout/k8opt/bin/exa/trainer/megatron/megatron_trainer_test.runfiles/local_pip_torch/sitepackages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/ephemeral/devcontainer/jasonchiu/cache/bazel/_bazel_jasonchiu/996a28cb1c2af162dca7531bd6a2de53/execroot/_main/bazelout/k8opt/bin/exa/trainer/megatron/megatron_trainer_test.runfiles/local_pip_torch/sitepackages/torch/nn/modules/module.py"", line 1747, in _call_impl     return forward_call(*args, **kwargs)   File ""/ephemeral/devcontainer/jasonchiu/cache/bazel/_bazel_jasonchiu/996a28cb1c2af162dca7531bd6a2de53/execroot/_main/bazelout/k8opt/bin/exa/trainer/megatron/megatron_trainer_test.runfiles/_main/third_party/megatron_lm/MegatronLM/megatron/core/models/gpt/gpt_model.py"", line 265, in forward     rotary_pos_emb = self.rotary_pos_emb(   File ""/ephemeral/devcontainer/jasonchiu/cache/bazel/_bazel_jasonchiu/996a28cb1c2af162dca7531bd6a2de53/execroot/_main/bazelout/k8opt/bin/exa/trainer/megatron/megatron_trainer_test.runfiles/local_pip_torch/sitepackages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/ephemeral/devcontainer/jasonchiu/cache/bazel/_bazel_jasonchiu/996a28cb1c2af162dca7531bd6a2de53/execroot/_main/bazelout/k8opt/bin/exa/trainer/megatron/megatron_trainer_test.runfiles/local_pip_torch/sitepackages/torch/nn/modules/module.py"", line 1747, in _call_impl     return forward_call(*args, **kwargs)   File ""/ephemeral/devcontainer/jasonchiu/cache/bazel/_bazel_jasonchiu/996a28cb1c2af162dca7531bd6a2de53/execroot/_main/bazelout/k8opt/bin/exa/trainer/megatron/megatron_trainer_test.runfiles/_main/third_party/megatron_lm/MegatronLM/megatron/core/models/common/embeddings/rotary_pos_embedding.py"", line 172, in forward     freqs = self.get_freqs_non_repeated(max_seq_len, offset)   File ""/ephemeral/devcontainer/jasonchiu/cache/bazel/_bazel_jasonchiu/996a28cb1c2af162dca7531bd6a2de53/execroot/_main/bazelout/k8opt/bin/exa/trainer/megatron/megatron_trainer_test.runfiles/_main/third_party/megatron_lm/MegatronLM/megatron/core/models/common/embeddings/rotary_pos_embedding.py"", line 137, in get_freqs_non_repeated     torch.arange(max_seq_len, device=self.inv_freq.device, dtype=self.inv_freq.dtype) ``` This PR fixes that",2025-02-20T03:16:05Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1419
chotzen,"[BUG] Dual meaning of `max_position_embeddings`, computing both embedding shape & yarn scaling base","**Describe the bug** When using MLA on a sequence length other than `config.max_position_embeddings`, a tensor shape mismatch error is thrown while applying the positional embeddings, stemming from this line.  Effectively, the seq_len dimension of the embedding matrix has to correspond to the actual sequence length being passed into the model, which is generally different from the `old_context_len` parameter. The `old_context_len` parameter is used in the computation of the YARN scaling, for example here. So changing this parameter in the config to match the context length at training time will cause the positional embeddings to be wrong. **To Reproduce**  Try a forward pass using MLA on a sequence whose length is different from `config.max_position_embeddings`.  There will be a tensor shape mismatch error inside the application of the positional embeddings to q and k.  Changing `config.max_position_embeddings` will cause a different embedding to be computed & produce incorrect output, but will make the error go away. **Expected behavior** The size of the embedding matrix adapts to the sequence length of the Q / K tensors or `config.max_sequence_length` instead of config.max_position_embeddings`. **Stack trace/logs** Can provide if needed **Environment (please complete the following information):**   MegatronLM commit ID: `aa719a0b0145481fb9212c577ee9a3f000fd16da` + internal patches   PyTorch version: 2.5.1   CUDA version: 12.2   NCCL version: 2.21.5 **Proposed fix** I was able to patch this by replacing the above line with `rotary_pos_emb = self.rotary_pos_emb(max_seq_len=q_len)`, though I do not believe this is a general solution for inferencetime. **Additional context** N/A",2025-02-19T23:30:00Z,,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1418,"Hi, thanks for your issue, we were aware of this bug and have already come up with a fix for 0.11 release. It will further be integrated with other pos_emb functions in the next release"
chotzen,[BUG] Checkpoint state dict remapping is not applied for MLA layers,"**Describe the bug** In `megatron/core/models/gpt/gpt_layer_specs.py`, there are state dict key mappings defined to handle the different state dicts induced by fused operations (e.g., layer norm + MLP fc1 fusion). These mappings are not present for the `multi_latent_attention = True` case, so checkpoint loading fails in certain cases.  **To Reproduce** Try to load a checkpoint with the layer norm key defined normally, i.e. `self_attention.linear_qkv.layer_norm_weight`. For MLA, the layer norm is not fused, and the state dict expects it to be under `input_layernorm`.  Similar issues are present for`pre_mlp_layernorm` and `self_attention.linear_kv_up_proj.layer_norm`.  **Expected behavior** A similar remapping is performed, like elsewhere in the file, e.g. https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/models/gpt/gpt_layer_specs.pyL238 I was able to get it to work with something like the following: ```                 sharded_state_dict_keys_map={                     'input_layernorm.': 'self_attention.linear_qkv.layer_norm_',                 }  (                     {'self_attention.linear_kv_up_proj.layer_norm_': 'self_attention.kv_layernorm.'}                     if qk_layernorm                     else {}                 ), ``` I'm not sure if this solution is exhaustive or we're expected to change our checkpoint formats, so not opening a PR for now (but happy to if you think it would help). **Stack trace/logs** N/A **Environment (please complete the following information):**   MegatronLM commit ID: `aa719a0b0145481fb9212c577ee9a3f000fd16da` + internal patches   PyTorch version: 2.5.1   CUDA version: 12.2   NCCL version: 2.21.5 **Proposed fix** See above ",2025-02-19T22:25:20Z,,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1417,I think this change could not be generally applied to all kinds of model loading. Maybe it should be added per customers' need
chotzen,[BUG] Sequence-parallel gather is attempted when sequence parallelism is disabled in MLA,"**Describe the bug** When sequence parallelism is disabled, the MLA implementation attempts to gather sequenceparallel activation shards before applying MLA, on this line: https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/transformer/multi_latent_attention.pyL335 **To Reproduce**  Attempt to use MLA with `sequence_parallel: False`. This will cause a tensor shape mismatch when trying to apply the positional embeddings, as the sequence length will be multiplied by a factor of TP shards. **Expected behavior** Sequenceparallel gather is not attempted. **Environment (please complete the following information):**   MegatronLM commit ID: `aa719a0b0145481fb9212c577ee9a3f000fd16da` + internal patches   PyTorch version: 2.5.1   CUDA version: 12.2   NCCL version: 2.21.5 **Proposed fix** Gate this gather on sequence parallel enabled",2025-02-19T22:17:03Z,,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1416,"Hi, for this issue, we actually added this line to set `SP = True` when `TP > 1`.  Currently SP must be True when using TP. We are working to remove this to support inference, thanks for catching this."
wplf,[QUESTION] Why gpu utility varies so much?,"**Your question** Ask a clear and concise question about MegatronLM. Hi, there. Thank you for great works. I'm using megatrin to train gpt3, found the gpu utility varies so much, but other metrics is very steady.  Do you know why, and is this ok? !Image !Image",2025-02-18T12:53:07Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1415
sherry-1001,Step One: pretrain  DeepSeekV2 model implementation,å¢žåŠ äº†DeepSeekV2çš„åŸºç¡€ä»£ç å®žçŽ°,2025-02-18T12:24:54Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1414
wplf,Why this `lr_decay_steps` is logged as `total num of iterations`,https://github.com/NVIDIA/MegatronLM/blob/677382e46327d68ac96095ebcf97740b715df0cf/megatron/core/optimizer_param_scheduler.pyL268C58L268C64,2025-02-18T03:38:41Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1413
AlbertZhangHIT,[BUG] Multi-Head-Latent-Attention Error,"**Describe the bug** MLA reports `AssertionError: Keys and values must have the same shape!` when trying to reproduce DeepSeek2/3, which is caused by shape mismatching of key and vlaue input to TEDotProductAttention. ``` MLA shape before adjust, query: torch.Size([4096, 1, 128, 192]), key: torch.Size([4096, 1, 128, 192]), value: torch.Size([4096, 1, 128, 128]) ``` **To Reproduce** ``` MLA_ARGS=(     multilatentattention     qkposembheaddim 64     qkheaddim 128     qlorarank 1536     kvlorarank 512     vheaddim 128     qklayernorm ) ... torchrun ${DISTRIBUTED_ARGS[@]} pretrain_gpt.py \     ${MODEL_ARGS[@]} \     ${MLA_ARGS[@]} \     ${MOE_ARGS[@]} \     ${DATA_ARGS[@]} \     ${TRAINING_ARGS[@]} \     ${MODEL_PARALLEL_ARGS[@]} \     ${LOGGING_ARGS[@]} ``` **Expected behavior** **Stack trace/logs** **Environment (please complete the following information):**   MegatronLM     8ca9e57f9d0bb93fc61850ebdccb6b6e6fa36b64   PyTorch version    2.2   CUDA version    12.1   NCCL version **Proposed fix** The value should be padded with zeros in `get_query_key_value_tensors` function in `MLASelfAttention` module. **Additional context** Add any other context about the problem here.",2025-02-18T03:21:52Z,,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/1412,I met the same issue when I just use this option: multilatentattention,"Thanks for reporting the issue, could you share your TE version?","We are sorry but we could not reproduce the bug. Would you please share your backtrace with us, or you may update your TE version to 2.x and try again."
AlbertZhangHIT,[BUG] multi_latent_attention does not support apply_rope_fusion,"**Describe the bug** ValueError: multi_latent_attention does not support apply_rope_fusion.     raise ValueError(""multi_latent_attention does not support apply_rope_fusion."") **To Reproduce** ``` MLA_ARGS=(     multilatentattention     qkposembheaddim 64     qkheaddim 128     qlorarank 1536     kvlorarank 512     vheaddim 128     qklayernorm ) ... torchrun ${DISTRIBUTED_ARGS[@]} pretrain_gpt.py \     ${MODEL_ARGS[@]} \     ${MLA_ARGS[@]} \     ${MOE_ARGS[@]} \     ${DATA_ARGS[@]} \     ${TRAINING_ARGS[@]} \     ${MODEL_PARALLEL_ARGS[@]} \     ${LOGGING_ARGS[@]} ``` **Expected behavior** the `apply_rope_fusion` should be set to False when validating arguments if `multi_latent_attention` is enabled. **Stack trace/logs** **Environment (please complete the following information):**   MegatronLM     core_r0.11.0   PyTorch version    2.2.0     CUDA version    12.1   NCCL version **Proposed fix** If you have a proposal for how to fix the issue state it here or link to a PR. **Additional context**",2025-02-18T02:40:38Z,,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1411,Thanks for flagging the issueâ€”will add an assertion soon.
wplf,[QUESTION] Precision for RowColumnLinear; Cann't get bitwise aligned result between input[:11] and input[:150]. #1489,"Hi, thank you for great work. My code can not get bitwise aligned result.  The reason may be the implementation of row linear matmul kernel. We will get True for tensor[:150], and get False for tensor[:11]. Is that ok for training llm? ``` from megatron.core.transformer.mlp import MLP, MLPSubmodules from megatron.core.transformer.custom_layers.transformer_engine import TERowParallelLinear, TEColumnParallelLinear from megatron.core.transformer.transformer_config import TransformerConfig hidden_size = 3072 num_attention_heads = 32 config = TransformerConfig(     num_layers=1,     hidden_size=hidden_size,     tensor_model_parallel_size=1,     sequence_parallel=False,     tp_comm_overlap=False,     bf16=True,     ffn_hidden_size=hidden_size * 4,     num_attention_heads=num_attention_heads,     add_bias_linear=False, ) module = MLPSubmodules(linear_fc1=TEColumnParallelLinear, linear_fc2=TERowParallelLinear) tp_MLP = MLP(config, module).to(torch.bfloat16).cuda() tensor1 = torch.randn(300, 3072).bfloat16().cuda() out1, _ = tp_MLP(tensor1) out2, _ = tp_MLP(tensor1[:150]) print((out2 == out1[:150]).all()) out2, _ = tp_MLP(tensor1[:11]) print((out2 == out1[:11]).all()) ```",2025-02-17T12:46:44Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1410,"cuBLAS (and most other performanceoptimized CUDA implementations) will likely use different kernels for these different cases. In particular, `tensor[:11]` will probably use nonperformant fallback kernels because it is so small and has dimensions that are not divisible by a power of 2 (both of these make it hard to use vectorized instructions). Since floating point addition and multiplication depend on the order they are applied, we should not expect bitwise exact results for these different cases. As a rule of thumb, we expect floating point operations on small tensors to have numerical errors on the order of machine epsilon (7.8e3 for BF16). If results are within that expected range (I find `torch.testing.assert_close` useful for quick checks), then it's _probably_ fine for LLM training. That said, numerical analysis is an entire field of applied mathematics and there is a whole art to getting good convergence in largescale training runs.",Thank you for explaination. This confuses me quite a while. I've learned a lot :) Hope you have a nice day.
thincal,[ENHANCEMENT] GRPO rlhf support,Does megatron core support the rlhf method such as grpo? Mainly leveraging the parallelism supported already.,2025-02-16T14:16:31Z,,open,2,0,https://github.com/NVIDIA/Megatron-LM/issues/1409
JinXiaozhao,[ENHANCEMENT] Distillation training supports PP,Which version is expected to support distillation training? Does Distillation Training have any plans to support PP?,2025-02-15T03:23:33Z,,open,1,2,https://github.com/NVIDIA/Megatron-LM/issues/1408,NeMo already supports distillation training. Does Megatron have plans to support it? https://github.com/NVIDIA/NeMo/issues/11531 https://github.com/NVIDIA/NeMo/releases/tag/v2.2.0,"Hi, please see:  https://github.com/NVIDIA/MegatronLM/blob/main/megatron/post_training/docs/distillation.md PP support for MegatronLM incoming very soon."
CoderPat,[BUG] NaNs when unfreezing vision encoder in the multi-modal example,"**Describe the bug** In the multimodal example in https://github.com/NVIDIA/MegatronLM/tree/main/examples/multimodal , wehn we run the scripts for either pretraining or finetuning, if we unfreeze the vision encoder we get immediate NaNs on the first step. I've tried lowering the learning rate, changing the data, etc... so it appears to not be data/optimization related **To Reproduce** Try to run the example scripts in the example, by removing `freezeViT` flag **Expected behavior** It is supposed to train without NaNs_ **Stack trace/logs** If applicable, add the stack trace or logs from the time of the error. **Environment (please complete the following information):**   Custom fork (https://github.com/deepspin/MegatronLMpretrain) with minor changes, last commit from upstream: cc207f809cd705a0d6f506e748405d23c28c1a92   PyTorch version: 2.5.1+cu124   CUDA version: 12.4   NCCL version> V12.4.99",2025-02-14T15:49:01Z,,open,0,4,https://github.com/NVIDIA/Megatron-LM/issues/1407,"If any more details are needed to help diagnose this, I'm happy to provide",Hi  did you solve this?,"Nop, but haven't tried to hard to fix it since then (was waiting to see someone from NVIDIA to reply) Are you getting the same error?", did you solve it?
tendar,[QUESTION] Why not gather routing_map in  sequence_load_balancing_loss_func function,**Why not gather routing_map in  sequence_load_balancing_loss_func function ** Here is the code at https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/transformer/moe/moe_utils.pyL117 Here do a gather for 'probs_for_aux_loss' but not for 'routing_map'. Why? Shouldn't it be the same?,2025-02-14T07:39:44Z,,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1406,"I discovered that performing gather operations on both `probs_for_aux_loss` and `routing_map` resulted in incorrect gradients for sequence auxiliary loss. Specifically, these gradients were incorrectly scaled by `num_sub_sequence` when TP was in use. To address this issue, I implemented the current solution, which ensures accurate loss gradients. Additionally, the allreduce operation we've implemented guarantees the correctness of the auxiliary loss values stored for logging purposes."
liuyifan123123,[QUESTION]Save format problem,"**Your question** Why is this the case in every iter file I save after pretraining through megatron, and the pt file is very small I use the gpt2 model for trainingï¼ŒMultiple iter files are saved when training is complete Below is my saved iter fileï¼š !Image Below is my training script fileï¼š !Image Ask a clear and concise question about MegatronLM.",2025-02-14T01:57:20Z,,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1405,Where do I need to change please
lk137095576,[ENHANCEMENT] Multi-token Prediction(MTP) support,any plan to support MTP in deepseek v3? It seems to accumulate prediction,2025-02-14T01:19:18Z,,open,0,7,https://github.com/NVIDIA/Megatron-LM/issues/1404,We are actively working on it.  We have implemented one version and are doing testing to make sure it works as expected., Very grad to see nv guys have working on it. Is there any definate release plan? We are all looking forward to try MTP., You can view the code prior to the official release., Could you please share the progress of MTP in MegatronLM ? We are looking forward to try this amazing feature.,star The implementation of MTP is almost ready and we have verified the convergence of MTP.  One remaining issue is to decide the best design to make MTP work well with future performance optimizations like computationcommunication overlapping in PP.  The current target release is Core 0.12., hey when you say deepseek v3 ? do you mean to say megatron LM / megatron core is adding support for pretraining deepseek v3 model ?  if yes could you please share the link to the actual resource. ,"Hi guys, I'm glad to announce that the MTP in MCore has been merged into main branch. https://github.com/NVIDIA/MegatronLM/commit/dc385c76f3ced50f5b05597cbe09ab4ab5192b7d Welcome to have a try and feel free to raise issues if facing any problems. Thanks for your support and feedbacks!."
wan-nan,[QUESTION] Performance Impact of Using item() in `total_num_tokens += num_tokens.item()` in megatron/core/pipeline_parallel/schedules.py,"Hi MegatronLM team! While going through the code in `megatron/core/pipeline_parallel/schedules.py`, I noticed that between each forward and backward pass, the line `total_num_tokens += num_tokens.item()` uses the `item()` method.  https://github.com/NVIDIA/MegatronLM/blob/8ca9e57f9d0bb93fc61850ebdccb6b6e6fa36b64/megatron/core/pipeline_parallel/schedules.pyL451L467 From my understanding, the item() method transfers data from the GPU device to the host, which could cause the CPU to block and wait for the GPU to finish its computation. This might have a negative impact on performance, as illustrated below. !Image To validate this, I removed the item() method and observed that the time cost associated with this operation was completely eliminated. !Image **Could you clarify why `item()` is used here?** Thanks for your time and insights!",2025-02-13T06:14:16Z,,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1403,"Hi, wannan, Thanks for looking into it. This is being addressed in an internal MR."
Jianhong-Zhang,[QUESTION] does cuda support fp32 residual connection feature?,"We tried fp32 residual connection on cuda, but the test failed. Is this expected or a bug? ",2025-02-12T22:52:38Z,,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/1402,"Zhang please share simple reproduction steps, environment(pip list) and log","> Zhang please share simple reproduction steps, environment(pip list) and log ``` [rank0]: Traceback (most recent call last): [rank0]:   File ""/home/l00913161/MegatronLM/pretrain_gpt.py"", line 331, in  [rank0]:     pretrain( [rank0]:   File ""/home/l00913161/MegatronLM/megatron/training/training.py"", line 415, in pretrain [rank0]:     iteration, num_floating_point_operations_so_far = train( [rank0]:                                                       ^^^^^^ [rank0]:   File ""/home/l00913161/MegatronLM/megatron/training/training.py"", line 1527, in train [rank0]:     train_step(forward_step_func, [rank0]:   File ""/home/l00913161/MegatronLM/megatron/training/training.py"", line 808, in train_step [rank0]:     losses_reduced = forward_backward_func( [rank0]:                      ^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/home/l00913161/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 471, in forward_backward_no_pipelining [rank0]:     output_tensor, num_tokens = forward_step( [rank0]:                                 ^^^^^^^^^^^^^ [rank0]:   File ""/home/l00913161/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 275, in forward_step [rank0]:     output_tensor, loss_func = forward_step_func(data_iterator, model) [rank0]:                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/home/l00913161/MegatronLM/pretrain_gpt.py"", line 259, in forward_step [rank0]:     output_tensor = model(tokens, position_ids, attention_mask, [rank0]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/usr/local/lib/python3.12/distpackages/torch/nn/modules/module.py"", line 1740, in _wrapped_call_impl [rank0]:     return self._call_impl(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/usr/local/lib/python3.12/distpackages/torch/nn/modules/module.py"", line 1751, in _call_impl [rank0]:     return forward_call(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/home/l00913161/MegatronLM/megatron/core/distributed/data_parallel_base.py"", line 22, in forward [rank0]:     return self.module(*inputs, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/usr/local/lib/python3.12/distpackages/torch/nn/modules/module.py"", line 1740, in _wrapped_call_impl [rank0]:     return self._call_impl(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/usr/local/lib/python3.12/distpackages/torch/nn/modules/module.py"", line 1751, in _call_impl [rank0]:     return forward_call(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/home/l00913161/MegatronLM/megatron/legacy/model/module.py"", line 189, in forward [rank0]:     outputs = self.module(*inputs, **kwargs) [rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/usr/local/lib/python3.12/distpackages/torch/nn/modules/module.py"", line 1740, in _wrapped_call_impl [rank0]:     return self._call_impl(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/usr/local/lib/python3.12/distpackages/torch/nn/modules/module.py"", line 1751, in _call_impl [rank0]:     return forward_call(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/home/l00913161/MegatronLM/megatron/legacy/model/gpt_model.py"", line 82, in forward [rank0]:     lm_output = self.language_model( [rank0]:                 ^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/usr/local/lib/python3.12/distpackages/torch/nn/modules/module.py"", line 1740, in _wrapped_call_impl [rank0]:     return self._call_impl(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/usr/local/lib/python3.12/distpackages/torch/nn/modules/module.py"", line 1751, in _call_impl [rank0]:     return forward_call(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/home/l00913161/MegatronLM/megatron/legacy/model/language_model.py"", line 517, in forward [rank0]:     encoder_output = self.encoder( [rank0]:                      ^^^^^^^^^^^^^ [rank0]:   File ""/usr/local/lib/python3.12/distpackages/torch/nn/modules/module.py"", line 1740, in _wrapped_call_impl [rank0]:     return self._call_impl(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/usr/local/lib/python3.12/distpackages/torch/nn/modules/module.py"", line 1751, in _call_impl [rank0]:     return forward_call(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/home/l00913161/MegatronLM/megatron/legacy/model/transformer.py"", line 1769, in forward [rank0]:     hidden_states = layer( [rank0]:                     ^^^^^^ [rank0]:   File ""/usr/local/lib/python3.12/distpackages/torch/nn/modules/module.py"", line 1740, in _wrapped_call_impl [rank0]:     return self._call_impl(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/usr/local/lib/python3.12/distpackages/torch/nn/modules/module.py"", line 1751, in _call_impl [rank0]:     return forward_call(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/usr/local/lib/python3.12/distpackages/transformer_engine/pytorch/transformer.py"", line 683, in forward [rank0]:     self_attention_outputs = self.self_attention( [rank0]:                              ^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/usr/local/lib/python3.12/distpackages/torch/nn/modules/module.py"", line 1740, in _wrapped_call_impl [rank0]:     return self._call_impl(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/usr/local/lib/python3.12/distpackages/torch/nn/modules/module.py"", line 1751, in _call_impl [rank0]:     return forward_call(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/usr/local/lib/python3.12/distpackages/transformer_engine/pytorch/attention.py"", line 8946, in forward [rank0]:     layernorm_qkv_outputs = self.layernorm_qkv( [rank0]:                             ^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/usr/local/lib/python3.12/distpackages/torch/nn/modules/module.py"", line 1740, in _wrapped_call_impl [rank0]:     return self._call_impl(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/usr/local/lib/python3.12/distpackages/torch/nn/modules/module.py"", line 1751, in _call_impl [rank0]:     return forward_call(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/usr/local/lib/python3.12/distpackages/torch/_dynamo/eval_frame.py"", line 738, in _fn [rank0]:     return fn(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/usr/local/lib/python3.12/distpackages/transformer_engine/pytorch/module/layernorm_linear.py"", line 1163, in forward [rank0]:     with self.prepare_forward(inp, is_first_microbatch) as inp: [rank0]:   File ""/usr/lib/python3.12/contextlib.py"", line 137, in __enter__ [rank0]:     return next(self.gen) [rank0]:            ^^^^^^^^^^^^^^ [rank0]:   File ""/usr/local/lib/python3.12/distpackages/transformer_engine/pytorch/module/base.py"", line 830, in prepare_forward [rank0]:     self.set_activation_dtype(inp) [rank0]:   File ""/usr/local/lib/python3.12/distpackages/transformer_engine/pytorch/module/base.py"", line 740, in set_activation_dtype [rank0]:     assert dtype == param.dtype, ( [rank0]:            ^^^^^^^^^^^^^^^^^^^^ [rank0]: AssertionError: Data types for parameters must match when outside of autocasted region.  Found input dtype: torch.float32 and 'layer_norm_weight' dtype: torch.bfloat16 ``` ```     uselegacymodels \     sequenceparallel \     tensormodelparallelsize ${TP} \     pipelinemodelparallelsize ${PP} \     numlayers 2 \     hiddensize 6144 \     ffnhiddensize 16384 \     numattentionheads 48 \     ckptformat torch\     groupqueryattention \     numquerygroups 8 \     seqlength 128 \     maxpositionembeddings 128 \     microbatchsize 1 \     globalbatchsize 1 \     makevocabsizedivisibleby 1 \     lr 1e6 \     lrdecaystyle constant \     rotarybase 1000000 \     trainiters 2 \     untieembeddingsandoutputweights \     disablebiaslinear \     initmethodstd 0.01 \     attentiondropout 0.0 \     hiddendropout 0.0 \     clipgrad 1.0 \     adambeta1 0.9 \     adambeta2 0.95 \     initiallossscale 1 \     nogradientaccumulationfusion \     useflashattn \     userotarypositionembeddings \     positionembeddingtype rope \     normalization RMSNorm \     normepsilon 1e5 \     swiglu \     nomaskedsoftmaxfusion \     attentionsoftmaxinfp32 \     noloadoptim \     noloadrng \     seed 42 \     bf16 \     distckptstrictness log_unexpected \     nomaskedsoftmaxfusion \     nobiasswiglufusion \     nobiasdropoutfusion \     noropefusion \     fp32residualconnection \ ```",same error
eagle705,Fix document regarding GQA (`--group-query-attention`) argument,Fix argument in llama_mistral document path: `docs/llama_mistral.md` ```bash   attentionsoftmaxinfp32 \   disablebiaslinear \   transformerimpl transformer_engine \  groupqueryattention 8 \ + groupqueryattention \ + numquerygroups 8 \   attentiondropout 0.0 \   hiddendropout 0.0 \   rotarybase 500000 \ ``` barker Could you take a look this?,2025-02-12T09:57:49Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1401
ischlag,Wandb copy script,this is a simple to use script to copy wandb runs from one project to another so contributors can create copies of their runs in a public repo and share that with the PR that performs the changes. ,2025-02-11T14:10:40Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1400
SeunghyunSEO,[QUESTION] plan to implement zero bubble pipeline or dual pipeline and MoE comm-comp overlapping,"Hi team, It looks like you're busy reproducing DeepSeek V3 features such as devicerouted load balancing, auxfree load balancing, and etc. I'm just wondering if you have any plans to implement zerobubble pipelining or dual pipelining and MoE communicationcomputation overlapping, as these are crucial for optimizing MoE training.",2025-02-11T09:12:49Z,,open,10,1,https://github.com/NVIDIA/Megatron-LM/issues/1399,any update on this ? 
lostkevin,[QUESTION] Does MLA in Megatron-Core support PackedSeqParams?,"Hi devs! I am trying to develop sequence packing based on PackedSeqParams arguments for deepseekv2. However, I encounter some problems like below: ``` RuntimeError: Sizes of tensors must match except in dimension 4. Expected size 6 but got size 1 for tensor number 1 in the list. ``` When I step into the source code, I find that the `apply_rotary_pos_emb` in MLASelfAttention is called with 4D hidden_states (bshd format) and raise error. So does MLA in MegatronCore support PackedSeqParams? P.S. following link I also write a unit_test code for PackedSeq in MLA and get test failed. ```python import os from importlib.metadata import version from inspect import signature import pytest import torch import transformer_engine as te from megatron.core.packed_seq_params import PackedSeqParams from megatron.core.models.gpt.gpt_layer_specs import get_gpt_layer_with_transformer_engine_spec from megatron.core.tensor_parallel.random import model_parallel_cuda_manual_seed from megatron.core.transformer.attention import Attention from megatron.core.transformer.enums import AttnMaskType from megatron.core.transformer.multi_latent_attention import MLASelfAttention, MultiLatentAttention from megatron.core.transformer.transformer_config import MLATransformerConfig from megatron.core.utils import is_te_min_version from tests.unit_tests.test_utilities import Utils def make_test_packed_seq_params(sequence_length):     cu_seqlens = torch.IntTensor([0, 6, 19, 22, sequence_length]).cuda()     seqlens = cu_seqlens[1:]  cu_seqlens[:1]     max_seqlen, _ = seqlens.max(dim=0, keepdim=True)     packed_seq_params = PackedSeqParams(         cu_seqlens_q=cu_seqlens,         cu_seqlens_kv=cu_seqlens,         max_seqlen_q=max_seqlen,         max_seqlen_kv=max_seqlen,         qkv_format='thd',     )     return packed_seq_params class TestParallelMLAAttention:     def setup_method(self, method):         Utils.initialize_model_parallel(1, 1)         model_parallel_cuda_manual_seed(123)         self.transformer_config = MLATransformerConfig(             num_layers=2,             hidden_size=12,             num_attention_heads=4,             use_cpu_initialization=True,             q_lora_rank=32,             kv_lora_rank=32,             qk_head_dim=128,             v_head_dim=128,             qk_pos_emb_head_dim=64,             rotary_base=10000,             max_position_embeddings=32,         )         self.parallel_attention = MLASelfAttention(             self.transformer_config,             get_gpt_layer_with_transformer_engine_spec(                 multi_latent_attention=True             ).submodules.self_attention.submodules,             layer_number=1,             attn_mask_type=AttnMaskType.causal,         )     def teardown_method(self, method):         Utils.destroy_model_parallel()     def test_gpu_forward(self):         if is_te_min_version(""1.10.0""):              use flash attention for hopper, future may support fused attention for ampere             os.environ['NVTE_FUSED_ATTN'] = ""0""             os.environ['NVTE_FLASH_ATTN'] = ""1""             config = self.parallel_attention.config             sequence_length = 32             micro_batch_size = 2             self.parallel_attention.cuda()              [sequence length, batch size, hidden size]             hidden_states = torch.ones(                 (sequence_length, micro_batch_size, self.parallel_attention.config.hidden_size)             )             hidden_states = hidden_states.cuda()             attention_mask = None             packed_seq_params = make_test_packed_seq_params(sequence_length)             output, bias = self.parallel_attention(hidden_states, attention_mask, packed_seq_params=packed_seq_params)             assert config.recompute_granularity is None             assert output.shape[0] == sequence_length             assert output.shape[1] == micro_batch_size             assert output.shape[2] == config.hidden_size             assert bias.shape[0] == config.hidden_size ``` run ``` torchrun nprocpernode 8 m pytest tests/unit_tests/transformer/test_multi_latent_attention.py ```",2025-02-11T08:57:12Z,,open,0,4,https://github.com/NVIDIA/Megatron-LM/issues/1398,Thanks for reporting. We will look into it.,The logic https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/transformer/attention.pyL417L420 seems missing in MLA code, May I know how the progress is [Does MLA in MegatronCore support PackedSeqParams? CC([QUESTION] Does MLA in MegatronCore support PackedSeqParams?)],The fix MR is ready and under review. I guess it needs ~2 weeks to get merged and go public.
yeahdongcn,Fix issue in converting Mixtral 8x7B checkpoints from HF to MCore and update doc," Testing Done ```bash root:/workspace/megatronlm python tools/checkpoint/convert.py modeltype GPT loader mixtral_hf saver mcore targettensorparallelsize ${TARGET_TP_SIZE} targetpipelineparallelsize ${TARGET_PP_SIZE} targetexpertparallelsize ${TARGET_EP_SIZE} loaddir ${HF_FORMAT_DIR} savedir ${MEGATRON_FORMAT_DIR} tokenizermodel ${TOKENIZER_MODEL} ... received transformer layer 30 received transformer layer 31 received final norm received output layer  > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000) saving checkpoint at iteration       1 to /workspace/checkpoints/mixtralmcoreTP1PP4EP8 in torch format   successfully saved checkpoint from iteration       1 to /workspace/checkpoints/mixtralmcoreTP1PP4EP8 [ t 1/1, p 4/4 ] saving checkpoint at iteration       1 to /workspace/checkpoints/mixtralmcoreTP1PP4EP8 in torch format   successfully saved checkpoint from iteration       1 to /workspace/checkpoints/mixtralmcoreTP1PP4EP8 [ t 1/1, p 4/4 ] saving checkpoint at iteration       1 to /workspace/checkpoints/mixtralmcoreTP1PP4EP8 in torch format   successfully saved checkpoint from iteration       1 to /workspace/checkpoints/mixtralmcoreTP1PP4EP8 [ t 1/1, p 4/4 ] saving checkpoint at iteration       1 to /workspace/checkpoints/mixtralmcoreTP1PP4EP8 in torch format   successfully saved checkpoint from iteration       1 to /workspace/checkpoints/mixtralmcoreTP1PP4EP8 [ t 1/1, p 4/4 ] saving checkpoint at iteration       1 to /workspace/checkpoints/mixtralmcoreTP1PP4EP8 in torch format   successfully saved checkpoint from iteration       1 to /workspace/checkpoints/mixtralmcoreTP1PP4EP8 [ t 1/1, p 4/4 ] saving checkpoint at iteration       1 to /workspace/checkpoints/mixtralmcoreTP1PP4EP8 in torch format   successfully saved checkpoint from iteration       1 to /workspace/checkpoints/mixtralmcoreTP1PP4EP8 [ t 1/1, p 4/4 ] saving checkpoint at iteration       1 to /workspace/checkpoints/mixtralmcoreTP1PP4EP8 in torch format   successfully saved checkpoint from iteration       1 to /workspace/checkpoints/mixtralmcoreTP1PP4EP8 [ t 1/1, p 4/4 ] saving checkpoint at iteration       1 to /workspace/checkpoints/mixtralmcoreTP1PP4EP8 in torch format   successfully saved checkpoint from iteration       1 to /workspace/checkpoints/mixtralmcoreTP1PP4EP8 [ t 1/1, p 4/4 ] Done! /usr/lib/python3.12/tempfile.py:1075: ResourceWarning: Implicitly cleaning up    _warnings.warn(warn_message, ResourceWarning) ```",2025-02-11T05:52:04Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1397
heavyrain-lzy,[BUG]The unit test `test_different_initialize_order_unconsistency` in `test_parallel_state.py` allways fails,**Describe the bug** The unit test `test_different_initialize_order_unconsistency` in `test_parallel_state.py` always fails. **To Reproduce** The unit test `test_different_initialize_order_unconsistency` in `test_parallel_state.py` always fails. **Expected behavior** The unit test `test_different_initialize_order_unconsistency` in `test_parallel_state.py` succeeds. **Stack trace/logs** **Environment (please complete the following information):** **Proposed fix** **Additional context**,2025-02-11T01:50:33Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1396
fxnie,[QUESTION]Missing code branch,"**Your question** Ask a clear and concise question about MegatronLM. In the paper An Empirical Study of Mamba based Language Models, A fixed snapshot of the code used in this technical report is available at  https://github.com/NVIDIA/MegatronLM/tree/ssm/examples/mamba. But I didn't find this. Where did it move to? The readme says that the current main branch does not support MAMBA2. Which one can support it now? ",2025-02-11T01:19:28Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1395
haeggee,WandB Logging,"added logging for  tokens per sec  consumed tokens  (correct) model weight norm (global, all params)  ETA anything else we need?",2025-02-10T16:21:37Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1394
ashvinnihalani,Enabling Alternative Path ABC implementations,Different Implementation of Path like objects are useful for various different filesystems/cloud paths. Unifying them around PathLib ABC ,2025-02-10T07:51:52Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1393
ischlag,Rename and create baseline runs," Renamed the slurm output files to not have the ""R"" prefix   Also renamed llama 3.1 and llama 3.2 to just llama 3 for readability  Added comment to indicate where model params are from (e.g. llama 3.2 or llama 3.1)  Added standard cosine learning rate schedule with x10 reduction with reasonable global batch sizes and reasonable learning rate to serve as baselines.   Added comments on parallelism   Fix all runs to be over 100B tokens with 12h duration  Reduced checkpoints to every 5k steps (510) checkpoints for the runs presented here. These now serve as better baselines for the ablations we'll introduce next. ",2025-02-09T14:12:23Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1392
Zzhiter,Fix typo in GPTModel forward function comments,,2025-02-09T05:41:08Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1391
XiaobingSuper,support bf16 dtype for optimizer states using precision-aware optimizer in TransformerEngine,"AS title, the user can set optimizer states' dtype to bf16 when using precisionaware optimizer in TransformerEngine, this PR depends on https://github.com/NVIDIA/TransformerEngine/pull/1465.",2025-02-08T02:35:12Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1390
loadams,Update GitHub links to reflect new GitHub org (deepspeedai),,2025-02-07T17:32:53Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1389
Chandler-Bing,add qkv_bias,bug fix   otherwise it may throw Exception `attributeerror: 'types.simplenamespace' object has no attribute 'qkv_bias'`,2025-02-07T10:16:38Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1388
ischlag,Add swiss ai readme,,2025-02-07T09:50:26Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1387
ankitaluthra1,[ENHANCEMENT] Nemo Megatron Retries Missing Index Files Without Exponential Backoff,"**Is your feature request related to a problem? Please describe.** During training, Nemo Megatron looks for index files from different nodes. When a file is not found, the system retries, but the retries do not follow an exponential backoff pattern. This results in erratic and frequent server calls. **Describe the solution you'd like** Implement an exponential backoff strategy for retrying missing index file lookups. This would: 1. Reduce server load by spacing out retries progressively. 2. Improve the chances of successful file retrieval. 3. Prevent unnecessary resource consumption due to aggressive retries. **Describe alternatives you've considered** 1. Implementing a fixed delay instead of exponential backoff (less efficient). 2. Introducing a jitter mechanism to prevent synchronized retry storms from multiple nodes. **Proposed implementation** 1. Modify the retry mechanism to follow an exponential backoff strategy, with increasing wait times (e.g., 1s, 2s, 4s, etc.). 2. Optionally, add jitter to prevent multiple nodes from retrying at the same time. 3. Implement logging to track and finetune the retry intervals. **Additional context** Lookup Timestamps from one of the nodes in our test training using Nemo(Megatron) 24.09 ",2025-02-07T07:14:38Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1386
ankitaluthra1,[ENHANCEMENT] Sequential Deletion of Old Checkpoint Files Slows Down Checkpointing,"**Is your feature request related to a problem? Please describe.** Currently, during checkpointing in Nemo Megatron, the deletion of older checkpoint files is performed sequentially. This significantly slows down the process of deletion phase, as deleting files in parallel could dramatically improve efficiency. **Describe the solution you'd like** Optimize checkpoint deletion by enabling parallel execution, allowing multiple old checkpoint files to be deleted simultaneously. This would exponentially improve deletion speed, reducing overall checkpointing time. **Describe alternatives you've considered** 1. Batching file deletions instead of strict sequential execution. 2. Using multithreading or async deletion mechanisms. 3. Offloading deletion tasks to a background process to prevent blocking. **Proposed implementation** Leverage parallel or asynchronous deletion techniques, such as: 1. Using concurrent workers to delete files in parallel. 2. Utilizing async I/O or thread pools to manage deletion efficiently. 3. Implementing a batched approach to delete multiple files at once. **Additional context**  This change would significantly speed up checkpointing, especially for large models with numerous checkpoint files.  Distributed storage systems like GCS handle parallel deletions efficiently, making this a viable optimization.",2025-02-07T07:03:57Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1385
ankitaluthra1,"[BUG] Multiple Nodes Attempt to Create Checkpoint Folder Simultaneously, Causing Errors","**Describe the bug** During checkpointing in Nemo Megatron, all nodes attempt to create the checkpoint folder simultaneously. Since only one of them succeeds, multiple nodes encounter ""create folder"" errors. This results in unnecessary error messages in logs and observability graphs, leading to confusion and wasted resources. **To Reproduce** Steps to reproduce the behavior: 1. Run Nemo Megatron with checkpointing enabled in a multinode setup. 2. Observe that all nodes try to create the checkpoint folder at the same time. 3. Only one succeeds, while others fail with folder creation errors.  4. Create failure errors appear in logs and monitoring dashboards **Expected behavior** Only a single node should attempt to create the checkpoint folder, while others should wait for its creation before proceeding. **Stack trace/logs** Sample folder name: ""nemoexperiments/testrun/checkpoints/megatron_llamaval_loss=0.06step=30consumed_samples=61440.0last/""  **Environment (please complete the following information):** Framework: NeMo 24.09 (Megatron) Model: Llama3.170B Megatron Core: 0.90, FSDP where DP=1. Checkpoint used: distributed (.distcp) Environment: GKE cluster with Cloud Storage Fuse CSI driver enabled (cluster version  1.31.1gke.1846000), Using local File API writing to GCSFuse, which remaps the local file API to GCS calls. Number of Nodes: 128 Node Type: a3megagpu8g GCS Bucket Type: Hierarchical Namespace GCS Buckets. Checkpoint Size: Each checkpoint write is 1.3TB written from a set of nodes, in parallel, with distributed files with file sizes varying from 200 MB to 1GB",2025-02-07T06:04:41Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1384
ankitaluthra1,[BUG] Distributed Checkpoint Files Written in Random Order During Nemo Megatron Checkpointing,"**Describe the bug** While performing checkpointing using Nemo Megatron, I observed that some of the distcp files were written in a random order. This can significantly degrade performance, especially on distributed file systems like Google Cloud Storage (GCS), where sequential writes are more efficient. **To Reproduce** Steps to reproduce the behavior: 1. Run Nemo Megatron with checkpointing enabled on 64 or 128 nodes. 2. Observe the order in which distcp files are written. 3. Some of the files are written in a nonsequential manner, which can negatively impact read/write performance on distributed file systems. **Expected behavior** All checkpoint files should be written in sequential order for optimized performance. **Stack trace/logs**  **Environment (please complete the following information):** Framework: NeMo 24.09 (Megatron) Model: Llama3.170B Megatron Core: 0.90, FSDP where DP=1. Checkpoint used:  distributed (.distcp) Environment: GKE cluster with Cloud Storage Fuse CSI driver enabled (cluster version  1.31.1gke.1846000), Using local File API writing to GCSFuse, which remaps the local file API to GCS calls. Number of Nodes: 128 Node Type: a3megagpu8g GCS Bucket Type: Hierarchical Namespace GCS Buckets. Checkpoint Size: Each checkpoint write is 1.3TB written from a set of nodes, in parallel, with distributed files with file sizes varying from 200 MB to 1GB",2025-02-07T05:21:39Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1383
maximevtush,Update LICENSE,Updated the copyright year to 2025 in the LICENSE file,2025-02-06T09:59:54Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1382
bzantium,add moe_router_device_choice_method argument to choose method â€¦,â€¦as DeepSeekV2 or DeepSeekV3 resolved: CC([ENHANCEMENT] add options how to choose topk devices for `device_limited_topk`) ,2025-02-06T04:18:03Z,,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/1381,Could you review this? to:  ,"Hi , thanks for the PR! It's indeed a diff between deepseekv2 and v3.  We actually have an internal PR for this, along with the support for nodelimited routing for V3. It's currently under review and should be merged into the main branch soon. ```     group.add_argument('moerouternumgroups', type=int, default=None,                        help='Number of groups to divide experts into for grouplimited routing. When using grouplimited routing: 1) Experts are divided into equalsized groups, 2) For each token, a subset of groups are selected based on routing scores (sum of top2 expert scores within each group), 3) From these selected groups, moe_router_topk experts are chosen.'                        'Two common use cases: 1) Devicelimited routing: Set equal to expert parallel size (EP) to limit each token to experts on a subset of devices (See DeepSeekV2: https://arxiv.org/pdf/2405.04434) 2) Nodelimited routing: Set equal to number of nodes in EP group to limit each token to experts on a subset of nodes (See DeepSeekV3: https://arxiv.org/pdf/2412.19437)')     group.add_argument('moeroutergrouptopk', type=int, default=None,                        help='Number of selected groups for grouplimited routing.') ``` Feel free to reach out if you have any questions. ","Hi , Thanks for fast review!  looking forward to having it merged soon :)"
JinjieNi,[QUESTION] checkpointing/loading memory overhead,"It seems that in current implementation the torch_dist checkpointing and loading will introduce around 2GB GPU memory overhead for rank 0 (for a 400m model), which will cause OOM if the original GPU untilization is high. Is there a way to free this GPU memory allocation after successfully checkpointing/loading? so that such overhead will not impact the later training process, otherwise it's quite annoying as it requires an additional test (and optimization if OOM) for checkpointing and loading. (tried with adding torch.cuda.empty_cache() in the checkpointing function but not working. And the load_checkpoint function alr has a ""torch.cuda.empty_cache()"" in the end) Thank you!",2025-02-06T03:40:47Z,,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/1380,+1,"I found when apply merge functions, `torch.cat()` introduce extra memory. The tensors to be cat are sharing the same underlying storage, which means there is no need to cat again. Try this: change sh_ten_merge_fn https://github.com/NVIDIA/MegatronLM/blob/11996c9fd1a2d0aaef6dafc1fd4219aa795f188f/megatron/core/transformer/mlp.pyL261 to: ``` def memory_saving_sh_ten_merge_fn(sub_state_dict):     with torch.no_grad():         shared_storage = sub_state_dict[0].untyped_storage()         if all(shared_storage.data_ptr() == tensor.untyped_storage().data_ptr() for tensor in sub_state_dict):             element_size = sub_state_dict[0].element_size()             total_numel = sum(tensor.numel() for tensor in sub_state_dict)             if shared_storage.nbytes() == total_numel * element_size:                 dim_0 = sum(tensor.shape[0] for tensor in sub_state_dict)                 shape = (dim_0,) + sub_state_dict[0].shape[1:]                 combined_tensor = torch.empty(                     shape, dtype=sub_state_dict[0].dtype, device=sub_state_dict[0].device                 ).set_(shared_storage, 0, shape)                 return combined_tensor         return torch.cat(sub_state_dict) ```","  If this code works, could you please add me to the coauthor of MegatronLM? thanks!"
LitLeo,[BUG] The logic for calculating the last stage when average loss across microbatches.,"**Describe the bug** After a step is completed, operation â€Average loss across microbatchesâ€œ will be performed in the last stage.  For the common case, although setting ignore_virtual=True or False does not affect the logic for calculating the last stage, ignore_virtual=False should be considered more accurate. https://github.com/NVIDIA/MegatronLM/blob/0dd78ddcdb117ce4f2e9761449274d87af717674/megatron/training/training.pyL847 ```python if mpu.is_pipeline_last_stage(ignore_virtual=True):          Average loss across microbatches. ``` **Expected behavior** ```python if mpu.is_pipeline_last_stage(ignore_virtual=False):          Average loss across microbatches. ```",2025-02-06T02:41:38Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1379
bzantium,[ENHANCEMENT] add options how to choose topk devices for `device_limited_topk`,"**Is your feature request related to a problem? Please describe.** Based on the original implementation for deepseekv3, they use `top2 and sum` instead of `max` to choose topk groups. ```python if self.topk_method == ""noaux_tc"": Â    scores_for_choice = scores.view(bsz * seq_len, 1) + self.e_score_correction_bias.unsqueeze(0) Â    group_scores = ( Â    scores_for_choice.view(bsz * seq_len, self.n_group, 1).topk(2, dim=1)[0].sum(dim = 1) ``` but in MegatronLM, it only uses `max` like: ```python     num_group = (         parallel_state.get_expert_model_parallel_world_size()     )   num_group equals to expert parallel size     group_scores = scores.view(num_tokens, num_group, 1).max(dim=1).values     group_idx = torch.topk(group_scores, k=moe_router_topk_limited_devices, dim=1, sorted=False)[1] ``` However, DeepSeekV2 technical report and implementation suggest to use max so giving options (between two) would be the solution.",2025-02-06T02:10:53Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1378
16bitmood,Add LongRoPE support,"I'm trying to add LongRoPE (and later other context extension methods) support in Megatron. This is an initial commit, please let me know if this looks okay. ",2025-02-05T15:53:35Z,,open,1,0,https://github.com/NVIDIA/Megatron-LM/issues/1377
HenryTangIntel,[QUESTION] any one used â€”exit-signal-hander?,"**Your question** Any one enabled â€”exitsignalhandler successfully, it seems not working. Hereâ€™s what I did: 1. Using mpirun to launch 8 instances ; 2. waiting until training happened after several iterations; 3. From other terminals, issue: pkill 15 f pretrain_gpt to kill processes gracefully. 4. noticed that the training code didnâ€™t enter into the signal handler to save checkpoints, instead the dataloader workers intercepted the SIGTERM signal and hits runtime error.",2025-02-05T00:51:13Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1376
swiftomkar,[QUESTION] Support for Heterogeneous Parallelism in Multimodal Training,"I have been using MegatronLM to train multimodal models and successfully followed the example under examples/multimodal. However, for efficient training, multimodal models often require different parallelism strategies for each component, as vision models are typically smaller than the LLM in such setups. **Does MegatronLM support heterogeneous parallelism strategies**, where different models within a multimodal system can use distinct parallelization techniques? If not, are there any recommended workarounds?",2025-02-04T18:20:44Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1375
umsanmaru,add .sh file,,2025-02-04T14:09:56Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1373
lawchingman,[REGRESSION],"**Describe the regression** A clear and concise description of what the regression is. **To Reproduce** Steps to reproduce the behavior. The easier it is to reproduce the faster it will get maintainer attention. **Previous performance** What speed or accuracy did you previously see. **New performance** What speed or accuracy do you see after the update. **Stack trace/logs** If applicable, add the stack trace or logs related to the regression. **Environment (please complete the following information):**   Previous MegatronLM commit ID   New MegatronLM commit ID   Previous PyTorch version   New PyTorch version   Previous CUDA version   New CUDA version   Previous NCCL version   New NCCL version **Proposed fix** If you have a proposal for how to fix the issue state it here or link to a PR. **Additional context** Add any other context about the problem here.",2025-02-02T12:26:26Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1372
PriyaEnuganti,"RuntimeError: The server socket has failed to listen on any local network address. port: 12341, useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use Traceback (most recent call last):","I am trying to run multinode GPT3 from the examples and here is my modifications for 16 node job  Runs the ""175B"" parameter model SBATCH N 16  SBATCH exclusive SBATCH ntaskspernode=8   SBATCH gpuspernode=8 SBATCH jobname ""testmegatron"" SBATCH mpi=none SBATCH time=40:00 echo ""Running on hosts: $(echo $(scontrol show hostname))"" echo ""$DATESTRING"" export CUDA_DEVICE_MAX_CONNECTIONS=8  Multinode configuration export GPUS_PER_NODE=8 export NUM_NODES=16 export NODE_RANK=${SLURM_PROCID} export MASTER_ADDR=$(scontrol show hostname $SLURM_NODELIST | head n1) export MASTER_PORT=12341 export WORLD_SIZE=$(($GPUS_PER_NODE * $NUM_NODES)) CONT='pytorch_24.12_py3.sqsh' MOUNT='/MegatronLM' export NCCL_DEBUG=INFO export NCCL_IB_DISABLE=1 export NCCL_SOCKET_IFNAME=eth0 export NCCL_IB_GID_INDEX=3 export NCCL_DEBUG_SUBSYS=ALL export OMP_NUM_THREADS=8 i am getting RuntimeError: The server socket has failed to listen on any local network address. port: 12341, useIpv6: 0, code: 98, name: EADDRINUSE, message: address already in use Traceback (most recent call last): changing ports is not helping. Any help here?",2025-01-31T00:10:50Z,,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1371,How to resolve it? I want to run gpt2 in two nodes using slurm.
kevalmorabia97,[ENHANCEMENT] Support pre-built wheels for Python 3.12,"**Is your feature request related to a problem? Please describe.** Starting from NGC Pytorch container 24.11, they have Python 3.12 hence users wont be able to install megatroncore since `pip install megatroncore` will pull `megatron_core0.4.0.tar.gz` instead of the latest `0.9.0` which only comes for `cp310` and `cp311`. **Describe the solution you'd like** It would be great if the release CI would build and release wheels for Python 3.12 as well **Describe alternatives you've considered** Alternative is to install from source but that is not as clean as just installing from PyPI. **Proposed implementation** N/A **Additional context** N/A",2025-01-30T17:59:27Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1370,,Marking as stale. No activity in 60 days.
TeddLi,"[QUESTION] Backend nccl does not support reduce_scatter_tensor_coalesced, how could I solve it","``` **Your question** Ask a clear and concise question about MegatronLM. /workspace/megatron/megatron/core/models/gpt/gpt_layer_specs.py:77: UserWarning: The fp8 argument in ""get_gpt_layer_with_transformer_engine_spec"" has been deprecated and will be removed soon. Please update your code accordingly.   warnings.warn( [rank7]: Traceback (most recent call last): [rank7]:   File ""/workspace/megatron/pretrain_gpt.py"", line 300, in  [rank7]:     pretrain( [rank7]:   File ""/workspace/megatron/megatron/training/training.py"", line 386, in pretrain [rank7]:     iteration, num_floating_point_operations_so_far = train( [rank7]:   File ""/workspace/megatron/megatron/training/training.py"", line 1478, in train [rank7]:     train_step(forward_step_func, [rank7]:   File ""/workspace/megatron/megatron/training/training.py"", line 766, in train_step [rank7]:     losses_reduced = forward_backward_func( [rank7]:   File ""/workspace/megatron/megatron/core/pipeline_parallel/schedules.py"", line 1877, in forward_backward_pipelining_without_interleaving [rank7]:     config.finalize_model_grads_func( [rank7]:   File ""/workspace/megatron/megatron/core/distributed/finalize_model_grads.py"", line 225, in finalize_model_grads [rank7]:     model_chunk.finish_grad_sync() [rank7]:   File ""/workspace/megatron/megatron/core/distributed/distributed_data_parallel.py"", line 447, in finish_grad_sync [rank7]:     bucket_group.finish_grad_sync() [rank7]:   File ""/workspace/megatron/megatron/core/distributed/param_and_grad_buffer.py"", line 368, in finish_grad_sync [rank7]:     self.start_grad_sync() [rank7]:   File ""/workspace/megatron/megatron/core/distributed/param_and_grad_buffer.py"", line 306, in start_grad_sync [rank7]:     with stream_context, _coalescing_manager(communication_group, async_ops=async_op) as cm: [rank7]:   File ""/usr/lib/python3.10/contextlib.py"", line 142, in __exit__ [rank7]:     next(self.gen) [rank7]:   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/distributed_c10d.py"", line 2031, in _coalescing_manager [rank7]:     work = group.reduce_scatter_tensor_coalesced(outputs, inputs, reduce_opts) [rank7]: RuntimeError: Backend nccl does not support reduce_scatter_tensor_coalesced [rank4]: Traceback (most recent call last): ```",2025-01-30T03:21:05Z,,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1369," , did you find the reason for this problem?  I encounter the same issue. "
janEbert,[BUG] FSDP2 activation recomputation does not save memory,"**Describe the bug** Using `usetorchfsdp2` with `r0.10.0`, I am unable to increase the batch size when using either just `recomputeactivations` (i.e., `recomputegranularity=selective`), or using: ```shell recomputegranularity=full \ recomputemethod=uniform \ recomputenumlayers=""$num_layers"" \ ``` I can still fit the same maximum batch size obtained _without_ activation recomputation. I've tried with up to 128Â GPUs, with no tensor parallelism enabled. The model is a Llama2Â 7B architecture, i.e., `num_layers=32`. I've tested on both A100 40GB and GH100Â 96GB GPUs. **To Reproduce** ```shell RECOMPUTE_ACTIVATIONS=0   0 (off), 1 (selective), or 2 (full) MICRO_BATCH_SIZE=TODO   whatever maximum fits with `RECOMPUTE_ACTIVATIONS=0` megatron_repo_dir=TODO NUM_NODES=TODO DEVICES_PER_NODE=TODO RDZV_ID=TODO MASTER_ADDR=TODO MASTER_PORT=TODO TOKENIZER_VOCAB_FILE=TODO TOKENIZER_MERGE_FILE=TODO DATA_PREFIX=TODO seq_length=4096 if [ ""$RECOMPUTE_ACTIVATIONS"" eq 0 ]; then     ac_args=() elif [ ""$RECOMPUTE_ACTIVATIONS"" eq 1 ]; then     ac_args=( recomputeactivations ) elif [ ""$RECOMPUTE_ACTIVATIONS"" eq 2 ]; then     ac_args=( recomputegranularity=full recomputemethod=uniform recomputenumlayers=32 ) else     echo ""unknown activation recomputation setting""     exit 1 fi python u m torch.distributed.run \        nproc_per_node=gpu \        nnodes=""$NUM_NODES"" \        rdzv_id=""$RDZV_ID"" \        rdzv_endpoint=""$MASTER_ADDR"":""$MASTER_PORT"" \        rdzv_backend=c10d \        ""$megatron_repo_dir""/pretrain_gpt.py  \        trainiters=10 \        loginterval=1 \        evaliters=0 \        evalinterval=10 \        bf16 \        accumulateallreducegradsinfp32 \        microbatchsize=""$MICRO_BATCH_SIZE"" \        globalbatchsize=""$((MICRO_BATCH_SIZE * NUM_NODES * DEVICES_PER_NODE))"" \        usetorchfsdp2 \        nogradientaccumulationfusion \        ckptformat torch_dist \        asyncsave \        untieembeddingsandoutputweights \        noasynctensormodelparallelallreduce \        tensormodelparallelsize=1 \        pipelinemodelparallelsize=1 \        ""${ac_args[@]}"" \        numlayers=32 \        hiddensize=4096 \        ffnhiddensize=11008 \        numattentionheads=32 \        seqlength=""$seq_length"" \        maxpositionembeddings=""$seq_length"" \        positionembeddingtype=rope \        normalization=RMSNorm \        swiglu \        disablebiaslinear \        attentiondropout=0.0 \        hiddendropout=0.0 \        tokenizertype=GPT2BPETokenizer \        vocabfile=""$TOKENIZER_VOCAB_FILE"" \        mergefile=""$TOKENIZER_MERGE_FILE"" \        dataloadertype=cyclic \        traindatapath 1.0 ""$DATA_PREFIX"" \        validdatapath 1.0 ""$DATA_PREFIX"" \        testdatapath 1.0 ""$DATA_PREFIX"" \        saveinterval=1000 \        tensorboarddir=tensorboard_logs \        logparamsnorm \        logthroughput \        logtimerstotensorboard \        logvalidationppltotensorboard \        logmemorytotensorboard \        seed=1234 ``` **Expected behavior** I should be able to increase the batch size when toggling activation recomputation on; this is possible with TorchTitan's FSDP2+recomputation implementation. **Stack trace/logs** Standard CUDA OOM trace, at different locations depending on settings: ``` torch.OutOfMemoryError: CUDA out of memory. Tried to allocate [...] ``` **Environment (please complete the following information):** (I'm using NVIDIA PyTorch 24.12 (build 126674149) container.)   MegatronLM commit ID: 44609f88875b8857434e9c91d32ea91f0cc15c1f   PyTorch version: 2.6.0a0+df5bbc0   CUDA version: 12.6.77   NCCL version: 2.23.4 **Proposed fix** Would involve a large change, but maybe disregard the ""manual"" activation checkpointing when using FSDP2 and use the PyTorch APIs instead, like TorchTitan does? **Additional context** None.",2025-01-28T16:29:18Z,,open,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1368
GangGreenTemperTatum,[BUG] BERT and GPT345 Model Checkpoints Returning `410 Gone` HTTP Response,"**Describe the bug** apologies if i am missing something here! but the Checkpoints Docs ref are all returning `410` responses:  **To Reproduce** any of the mentioned checkpoints above: ```bash BERT345Muncased: wget contentdisposition https://api.ngc.nvidia.com/v2/models/nvidia/megatron_bert_345m/versions/v0.1_uncased/zip O megatron_bert_345m_v0.1_uncased.zip BERT345Mcased: wget contentdisposition https://api.ngc.nvidia.com/v2/models/nvidia/megatron_bert_345m/versions/v0.1_cased/zip O megatron_bert_345m_v0.1_cased.zip GPT345M: wget contentdisposition https://api.ngc.nvidia.com/v2/models/nvidia/megatron_lm_345m/versions/v0.0/zip O megatron_lm_345m_v0.0.zip ``` **Expected behavior** `200` and download of the checkpoints **Stack trace/logs** ```bash 20250128 10:36:33  https://api.ngc.nvidia.com/v2/models/nvidia/megatron_lm_345m/versions/v0.0/zip Resolving api.ngc.nvidia.com (api.ngc.nvidia.com)... 35.83.189.1, 44.230.191.247 Connecting to api.ngc.nvidia.com (api.ngc.nvidia.com):443... connected. HTTP request sent, awaiting response... 410 Gone 20250128 10:36:34 ERROR 410: Gone. ``` **Environment (please complete the following information):** NA **Proposed fix** NA **Additional context** NA",2025-01-28T15:38:23Z,,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1367,you can download from here https://catalog.ngc.nvidia.com/orgs/nvidia/models/megatron_lm_345m,"> you can download from here https://catalog.ngc.nvidia.com/orgs/nvidia/models/megatron_lm_345m Yep thank you and ACK, this was my workaround but wanted to report the hyperlinks in the README were failing still and look to be different endpoints"
charlesintel,--log-num-zeros-in-grad,,2025-01-28T02:12:06Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1366
carrot0117,[QUESTION]convert LLaMA2-7B to the Megatron format failed: the converted model only repeats meaningless numbers,"When I converted the LLaMA27B model to the Megatron format, the program ran successfully and provided the converted model. However, the converted model cannot provide meaningful answers to questions and only repeats meaningless numbers. The original LLaMA2 model was able to respond to questions correctly. Below are the conversion script and the output log file. Is there any good way to solve this issue? converthftomegatron01217b.log convert_hf_to_megatron.sh.txt",2025-01-22T16:27:25Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1365,Marking as stale. No activity in 60 days.
JavaZeroo,[QUESTION] How can I train a model from hugging face,"**Your question** Ask a clear and concise question about MegatronLM. Background: I want to pretrain InternLM2 What I've done: I have downloaded the model structure from https://huggingface.co/internlm/internlm220b (based on LLaMA but with slight differences). And, I have completed data preprocessing. My question: How can I load internlm/internlm220b and train this model on a single machine with 8 A100?",2025-01-22T03:55:07Z,,open,2,1,https://github.com/NVIDIA/Megatron-LM/issues/1364,Any tutorials or documentation?
kuozhang,[QUESTION]  why is pre_mlp_layernorm an IdentityOp if num_experts is None,"https://github.com/NVIDIA/MegatronLM/blob/7ba0d6ddaab1e4d1c587af8fea8a0dfe13b20f13/megatron/core/models/gpt/gpt_layer_specs.pyL110 even though the num_experts is None,  that means an normal MLP not a MoeLayer is used, pre_mlp_layernom can be still used anyway",2025-01-20T09:04:00Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1362,"I see, megatron is using TELayerNormColumnParallelLinear"
k141303,Fix packages error,"Fix the following error during pretraining of BERT. ``` error: Multiple toplevel packages discovered in a flatlayout: ['images', 'megatron']. ```",2025-01-19T09:55:10Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1361
mpashkovskiy,fix: name the token indices properly for training with tokens padding,I think https://github.com/NVIDIA/MegatronLM/commit/f27a04f6d64e14ebe18cabd807c29cec449ccdc6 introduced a typo for the case when MoE model trained with padding. Training fails with `NameError: name 'sorted_indices' is not defined`. I think `token_indices.view(1)` is actually `sorted_indices`.   could you please review.,2025-01-18T12:32:26Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1360,Thanks for fixing it
mdy666,[BUG] state[p]['master_weight'] become bf16,"**Describe the bug** I write a custom Adam like transformer_engine fused_adam.  and set  ```bash   usedistckpt   usedistributedoptimizer   useprecisionawareoptimizer   expavgdtype fp16   expavgsqdtype fp16   maingradsdtype bf16 ``` the master_weight is saved in fp32, but when I load the optimizer state from the ckpt, the master_weight become bf16 I run this code, the master_weight in fp32 ```python sd = optimizer.sharded_state_dict(model[0].sharded_state_dict()) ``` when I run this, the master_weight in bf16 ```python state_dict, checkpoint_name, release, ckpt_type = _load_base_checkpoint(       load_dir, args, rank0=False, checkpointing_context=checkpointing_context,       **load_kwargs ) ``` !Image the dist_ckpt is hard to find the issue, please help me",2025-01-17T15:34:42Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1359
YK-Fu,KV-cache for T5 model,"The current Megatroncore only implements the KVcache mechanism for decoderonly models. I have implemented the KVcache for seq2seq (T5) model to integrate with NeMo (https://github.com/NVIDIA/NeMo/pull/11881). The main changes include setting the KVcache input for cross_attention to None, as it doesn't require a KVcache. Additionally, I added the KVcache offset to the T5 model's position ID in each forward step.",2025-01-17T03:03:25Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1358,Marking as stale. No activity in 60 days.
yuanpeng-zhu,[QUESTION] train expert-model-parallel-size=4 with error," I encountered an error while training a 4node MoE model. I am a beginner in using MegatronLM. I encountered an error while training a 4node MoE model.  However, when training a 2node MoE model with `expertmodelparallelsize=2` or a 4node with `pipelinemodelparallelsize=4`, no errors were observed.  environment 4 nodes, each with one 4090 GPU nvcr.io/nvidia/pytorch   24.07py3  config file ```bash !/bin/bash  Runs the ""345M"" parameter model export CUDA_DEVICE_MAX_CONNECTIONS=1 GPUS_PER_NODE=1  Change for multinode config MASTER_ADDR=10.0.0.7 MASTER_PORT=6000 NNODES=4 NODE_RANK=0 CHECKPOINT_PATH=checkpoints/gpt2 VOCAB_FILE=../data/gpt2vocab.json MERGE_FILE=../data/gpt2merges.txt DATA_PATH=../data/meggpt2_text_document GPT_ARGS=""     usemcoremodels     disablebiaslinear     numlayers 12 \     hiddensize 1024 \     numattentionheads 8 \     seqlength 1024 \     maxpositionembeddings 1024 \     microbatchsize 1  \     globalbatchsize 8 \     lr 0.00015 \     trainiters 500000 \     lrdecayiters 320000 \     lrdecaystyle cosine \     minlr 1.0e5 \     weightdecay 1e2 \     lrwarmupfraction .01 \     clipgrad 1.0 \     bf16 "" MOE_ARGS=""     numexperts 8  \     moeroutertopk 2  \     moerouterloadbalancingtype aux_loss  \     moeauxlosscoeff 1e2  \     moegroupedgemm  \     moetokendispatchertype alltoall \     overlapparamgather  \     overlapgradreduce "" DATA_ARGS=""     datapath $DATA_PATH \     vocabfile $VOCAB_FILE \     mergefile $MERGE_FILE \     split 949,50,1 "" OUTPUT_ARGS=""     loginterval 100 \     saveinterval 10000 \     evalinterval 1000 \     evaliters 10 "" DISTRIBUTED_ARGS=""nproc_per_node $GPUS_PER_NODE nnodes $NNODES node_rank $NODE_RANK master_addr $MASTER_ADDR master_port $MASTER_PORT"" torchrun $DISTRIBUTED_ARGS \     ./pretrain_gpt.py \     $MOE_ARGS \     tensormodelparallelsize 1 \     pipelinemodelparallelsize 1 \     expertmodelparallelsize 4 \     usedistributedoptimizer \     $GPT_ARGS \     $DATA_ARGS \     $OUTPUT_ARGS \     distributedbackend nccl \     save $CHECKPOINT_PATH \     load $CHECKPOINT_PATH ```  error output ``` /usr/local/lib/python3.10/distpackages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.   quantize_op_abstract = torch.library.impl_abstract(""tensorrt::quantize_op"")( [rank2]:[E115 10:38:56.491259456 ProcessGroupGloo.cpp:143] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error [rank2]: Traceback (most recent call last): [rank2]:   File ""/workspace/ai/MegatronLM/./pretrain_gpt.py"", line 300, in  [rank2]:     pretrain( [rank2]:   File ""/workspace/ai/MegatronLM/megatron/training/training.py"", line 273, in pretrain [rank2]:     initialize_megatron( [rank2]:   File ""/workspace/ai/MegatronLM/megatron/training/initialize.py"", line 127, in initialize_megatron [rank2]:     finish_mpu_init() [rank2]:   File ""/workspace/ai/MegatronLM/megatron/training/initialize.py"", line 104, in finish_mpu_init [rank2]:     _initialize_distributed(get_embedding_ranks, get_position_embedding_ranks) [rank2]:   File ""/workspace/ai/MegatronLM/megatron/training/initialize.py"", line 301, in _initialize_distributed [rank2]:     mpu.initialize_model_parallel( [rank2]:   File ""/workspace/ai/MegatronLM/megatron/core/parallel_state.py"", line 721, in initialize_model_parallel [rank2]:     group_gloo = torch.distributed.new_group(ranks, timeout=timeout, backend=""gloo"") [rank2]:   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/c10d_logger.py"", line 92, in wrapper [rank2]:     func_return = func(*args, **kwargs) [rank2]:   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/distributed_c10d.py"", line 4124, in new_group [rank2]:     return _new_group_with_tag( [rank2]:   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/distributed_c10d.py"", line 4204, in _new_group_with_tag [rank2]:     pg, pg_store = _new_process_group_helper( [rank2]:   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/distributed_c10d.py"", line 1568, in _new_process_group_helper [rank2]:     backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout) [rank2]: RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error [rank2]:[W115 10:38:56.669019136 ProcessGroupNCCL.cpp:1187] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator()) E0115 10:38:56.740000 140396612575680 torch/distributed/elastic/multiprocessing/api.py:832] failed (exitcode: 1) local_rank: 0 (pid: 4376) of binary: /usr/bin/python Traceback (most recent call last):   File ""/usr/local/bin/torchrun"", line 33, in      sys.exit(load_entry_point('torch==2.4.0a0+3bcc3cddb5.nv24.7', 'console_scripts', 'torchrun')())   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 347, in wrapper     return f(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/run.py"", line 900, in main     run(args)   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/run.py"", line 891, in run     elastic_launch(   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/launcher/api.py"", line 132, in __call__     return launch_agent(self._config, self._entrypoint, list(args))   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/launcher/api.py"", line 263, in launch_agent     raise ChildFailedError( torch.distributed.elastic.multiprocessing.errors.ChildFailedError:  ```  comparative testing However, when I change NNODES to 2 , `expertmodelparallelsize=2`, and run it on 2 nodes, no errors were observed. Alsoï¼Œ using `NNODES=2,` `pipelinemodelparallelsize=4`,`expertmodelparallelsize=1`, no errors were observed. ``` [20250115 10:45:40] iteration      100/  500000  ```  I am very confused and greatly appreciate your help.",2025-01-15T11:04:06Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1357,It's seems not a problem of MegatronLM. use `export GLOO_SOCKET_IFNAME=port_name`
LiGuihong,Memory usage,,2025-01-13T18:11:48Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1356
stay88,[QUESTION] The dataset cannot be found in multi-node multi-GPU training.,"> I encountered this issue.   > Here is the data file generated by the main node:   >  > data/BookCorpusDataset_text_document/cache/GPTDataset_indices: > 6d84f3595d97dfece2364c3950a26906GPTDatasetvaliddescription.txt > 6d84f3595d97dfece2364c3950a26906GPTDatasetvaliddocument_index.npy > 6d84f3595d97dfece2364c3950a26906GPTDatasetvalidsample_index.npy > 6d84f3595d97dfece2364c3950a26906GPTDatasetvalidshuffle_index.npy > d296b3899150edfd9092c34b30fa03c1GPTDatasettestdescription.txt > d296b3899150edfd9092c34b30fa03c1GPTDatasettestdocument_index.npy > d296b3899150edfd9092c34b30fa03c1GPTDatasettestsample_index.npy > d296b3899150edfd9092c34b30fa03c1GPTDatasettestshuffle_index.npy > fa7397310cb8333b787979cb7c45c55fGPTDatasettraindescription.txt > fa7397310cb8333b787979cb7c45c55fGPTDatasettraindocument_index.npy > fa7397310cb8333b787979cb7c45c55fGPTDatasettrainsample_index.npy > fa7397310cb8333b787979cb7c45c55fGPTDatasettrainshuffle_index.npy >  >  When I add the mounted NFS shared dataset path [model/sharedata] on the subnode, the subnode locates a data file that doesn't exist on the main node.   >  > `FileNotFoundError: [Errno 2] No such file or directory: '/workspace/megatronlm/models/gpt3/dataset/BookCorpusDataset_text_document/cache/GPTDataset_indices/2bcc0d01e685e944ad9c0c8ea43d126fGPTDatasettraindocument_index.npy` >  >  However, when I add a nonshared dataset [model/data] on the subnode, the subnode locates a data file that does exist on the main node.   >  > `FileNotFoundError: [Errno 2] No such file or directory: '/workspace/megatronlm/models/gpt3/dataset/BookCorpusDataset_text_document/cache/GPTDataset_indices/fa7397310cb8333b787979cb7c45c55fGPTDatasettraindocument_index.npy` >  > Could it be due to different seeds? Why is this happening? >  >  >  >  >    _Originally posted by  in [ CC([BUG] GPTDataset._build_document_sample_shuffle_indices does not build the indices on nonroot nodes when not using NFS)](https://github.com/NVIDIA/MegatronLM/issues/907issuecomment2586500518)_",2025-01-13T08:43:54Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1355,"> > I encountered the same issue. > > Here is the data file generated by the main node: > > data/BookCorpusDataset_text_document/cache/GPTDataset_indices: > > 6d84f3595d97dfece2364c3950a26906GPTDatasetvaliddescription.txt > > 6d84f3595d97dfece2364c3950a26906GPTDatasetvaliddocument_index.npy > > 6d84f3595d97dfece2364c3950a26906GPTDatasetvalidsample_index.npy > > 6d84f3595d97dfece2364c3950a26906GPTDatasetvalidshuffle_index.npy > > d296b3899150edfd9092c34b30fa03c1GPTDatasettestdescription.txt > > d296b3899150edfd9092c34b30fa03c1GPTDatasettestdocument_index.npy > > d296b3899150edfd9092c34b30fa03c1GPTDatasettestsample_index.npy > > d296b3899150edfd9092c34b30fa03c1GPTDatasettestshuffle_index.npy > > fa7397310cb8333b787979cb7c45c55fGPTDatasettraindescription.txt > > fa7397310cb8333b787979cb7c45c55fGPTDatasettraindocument_index.npy > > fa7397310cb8333b787979cb7c45c55fGPTDatasettrainsample_index.npy > > fa7397310cb8333b787979cb7c45c55fGPTDatasettrainshuffle_index.npy > >  > > * When I add the mounted NFS shared dataset path [model/sharedata] on the subnode, the subnode locates a data file that doesn't exist on the main node. > >  > > `FileNotFoundError: [Errno 2] No such file or directory: '/workspace/megatronlm/models/gpt3/dataset/BookCorpusDataset_text_document/cache/GPTDataset_indices/2bcc0d01e685e944ad9c0c8ea43d126fGPTDatasettraindocument_index.npy` > >  > > * However, when I add a nonshared dataset [model/data] on the subnode, the subnode locates a data file that does exist on the main node. > >  > > `FileNotFoundError: [Errno 2] No such file or directory: '/workspace/megatronlm/models/gpt3/dataset/BookCorpusDataset_text_document/cache/GPTDataset_indices/fa7397310cb8333b787979cb7c45c55fGPTDatasettraindocument_index.npy` > > Could it be due to different seeds? Why is this happening? >  > _Originally posted by [](https://github.com/stay88) in [ CC([BUG] GPTDataset._build_document_sample_shuffle_indices does not build the indices on nonroot nodes when not using NFS)](https://github.com/NVIDIA/MegatronLM/issues/907issuecomment2586500518)_ I copied the segmented dataset generated by the master node `[master/data/BookCorpusDataset_text_document/cache/GPTDataset_indices]` to the node `[master/data/BookCorpusDataset_text_document/cache/GPTDataset_indices]`, and the model was able to run. Could you please explain the reason for this?",Marking as stale. No activity in 60 days.
GuokunWang,[QUESTION] Limit Number of Saved Checkpoints,"Hi, I'm wondering if MegatronLM supports keeping only the latest N checkpoints. When using a small `save_interval` for frequent checkpointing, the storage occupied by checkpoints grows rapidly. It would be very helpful if there was a parameter similar to `save_total_limit` in the Transformers library, which would allow us to constrain the maximum number of checkpoints saved. This would effectively reduce storage consumption. Thanks!",2025-01-13T07:28:24Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1354,Marking as stale. No activity in 60 days.
lawchingman,[BUG],"**Describe the bug** A clear and concise description of what the bug is. **To Reproduce** Steps to reproduce the behavior. The easier it is to reproduce the faster it will get maintainer attention. **Expected behavior** A clear and concise description of what you expected to happen. **Stack trace/logs** If applicable, add the stack trace or logs from the time of the error. **Environment (please complete the following information):**   MegatronLM commit ID   PyTorch version   CUDA version   NCCL version **Proposed fix** If you have a proposal for how to fix the issue state it here or link to a PR. **Additional context** Add any other context about the problem here.",2025-01-11T12:09:00Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1353,Marking as stale. No activity in 60 days.
Jintao-Huang,fix typo,,2025-01-10T02:21:30Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1352,Marking as stale. No activity in 60 days.
Force1ess,fix param overwrite problem in saver_mcore,"As shown in the images below, I found that the following four arguments are being wrongly overwritten in tools.checkpoint.saver_mcore.save_checkpoint: 	main_params_dtype 	main_grads_dtype 	exp_avg_dtype 	exp_avg_sq_dtype This issue prevents these arguments from passing the validation performed by the validate_args function. To resolve this, I added them to the args_to_keep list within the save_checkpoint function. Supporting Images:   If any additional details or discussions are required, feel free to reach out. Thank you for your time and feedback!",2025-01-09T06:43:00Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1351,Marking as stale. No activity in 60 days.
switiz,[BUG] can't load saved fp8 checkpoint when resume training,"**Describe the bug** we train small moe model using fp8 precision. Excluding speed issues or convergence problems, saving and loading do not function properly. I made some modifications to the experts.py file, specifically in the **sharded_state_dict** function. In the loop: ``` for name, module in self._modules.items():     if name in ['fp8_padding', 'fp8_unpadding']:         continue ``` The fp8_padding and fp8_unpadding objects do not have a shared state dict, so I added a continue statement to skip them during the iteration. **To Reproduce** train using nemo framework with fp8 config, moe model  and  enable grouped gemm ```   dist_ckpt_format: torch_dist   dist_ckpt_parallel_save: true   scale_positional_embedding: true   restore_from_path: null   dist_ckpt_load_strictness: log_all   moe_router_topk: 8   num_moe_experts: 64   moe_token_dispatcher_type: alltoall   moe_aux_loss_coeff: 0.01   moe_z_loss_coeff: 0.001   moe_router_load_balancing_type: aux_loss   mcore_gpt: true   moe_grouped_gemm: true   micro_batch_size: 2   global_batch_size: 2048   rampup_batch_size: null   tensor_model_parallel_size: 1   pipeline_model_parallel_size: 1   expert_model_parallel_size: 1   virtual_pipeline_model_parallel_size: null   encoder_seq_length: 8192   max_position_embeddings: 8192   num_layers: 16   hidden_size: 2048   ffn_hidden_size: 1024   num_attention_heads: 32   num_query_groups: 8   init_method_std: 0.01   use_scaled_init_method: true   hidden_dropout: 0.0   attention_dropout: 0.0   ffn_dropout: 0.0   kv_channels: null   apply_query_key_layer_scaling: true   normalization: rmsnorm   layernorm_epsilon: 1.0e05   do_layer_norm_weight_decay: false   make_vocab_size_divisible_by: 128   pre_process: true   post_process: true   persist_layer_norm: true   bias: false   activation: fastswiglu   headscale: false   transformer_block_type: pre_ln   openai_gelu: false   normalize_attention_scores: true   position_embedding_type: rope   rotary_percentage: 1.0   rotary_base: 500000   apply_rope_fusion: true   attention_type: multihead   share_embeddings_and_output_weights: false   tokenizer:     library: huggingface     type: {path}     use_fast: true   native_amp_init_scale: 4294967296   native_amp_growth_interval: 1000   hysteresis: 2   fp32_residual_connection: false   fp16_lm_cross_entropy: false   megatron_amp_O2: true   grad_allreduce_chunk_size_mb: 125   grad_div_ar_fusion: true   gradient_accumulation_fusion: true   bias_activation_fusion: true   bias_dropout_add_fusion: true   masked_softmax_fusion: true   seed: 1234   resume_from_checkpoint: null   use_cpu_initialization: false   onnx_safe: false   apex_transformer_log_level: 30   gradient_as_bucket_view: true   sync_batch_comm: false   activations_checkpoint_granularity: null   activations_checkpoint_method: null   activations_checkpoint_num_layers: null   num_micro_batches_with_partial_activation_checkpoints: null   activations_checkpoint_layers_per_pipeline: null   sequence_parallel: false   transformer_engine: true   activation_func_fp8_input_store: true   fp8_params: true   fp8: true   fp8_e4m3: false   fp8_hybrid: true   fp8_margin: 0   fp8_interval: 1   fp8_amax_history_len: 1024   fp8_amax_compute_algo: max   reduce_amax: true   use_emha: false   ub_tp_comm_overlap: false   ub_tp_comm_overlap_cfg: null   overlap_p2p_comm: false   batch_p2p_comm: false   seq_len_interpolation_factor: null   use_flash_attention: true   optim:     name: mcore_distributed_optim     lr: 0.0005     weight_decay: 0.1     betas:      0.9      0.95     dtype: bf16     overlap_grad_sync: true     overlap_param_sync: true     grad_sync_dtype: bf16 ``` **Expected behavior** can load saved fp8 checkpoint and resume training **Stack trace/logs** 1. save case ```   0: [NeMo W 20250108 23:55:02 validation:389] There is difference in the common state dict in different ranks. The differences are {1: ([], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 2: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 3: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 4: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 5: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 6: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), ,   0:  )]), 7: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 8: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 9: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 10: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 11: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 12: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 13: ([('optimizer_stat   0: es', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 14: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 15: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 16: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 17: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 18: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 19: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1,    0: 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 20: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 21: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 22: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 23: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 24: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 25: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', '   0: time_elapsed', 'train'), , )]), 26: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 27: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 28: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 29: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 30: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 31: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'),    0: , )]), 32: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 33: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 34: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 35: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 36: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 37: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 38: ([('optimizer_   0: states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 39: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 40: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 41: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 42: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 43: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 44: ([('optimizer_states', 0, 'optimizer', 'param_groups',   0:  1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 45: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 46: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 47: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 48: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 49: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 50: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer   0: ', 'time_elapsed', 'train'), , )]), 51: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 52: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 53: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 54: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 55: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 56: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 57: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 58: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 59: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 60: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 61: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 62: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 63: ([('optimi   0: zer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 64: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 65: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 66: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 67: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 68: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 69: ([('optimizer_states', 0, 'optimizer', 'param_grou   0: ps', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 70: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 71: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 72: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 73: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 74: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 75: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'T   0: imer', 'time_elapsed', 'train'), , )]), 76: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 77: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 78: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 79: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 80: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 81: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 82: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 83: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 84: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 85: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 86: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 87: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 88: ([('op   0: timizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 89: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 90: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 91: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 92: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 93: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 94: ([('optimizer_states', 0, 'optimizer', 'param_   0: groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 95: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 96: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 97: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 98: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 99: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 100: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks   0: ', 'Timer', 'time_elapsed', 'train'), , )]), 101: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 102: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 103: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 104: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 105: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 106: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train   0: '), , )]), 107: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 108: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 109: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 110: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 111: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 112: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 113: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 114: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 115: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 116: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 117: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 118: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 119: ([('optimizer_states',   0:  0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 120: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 121: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 122: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 123: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 124: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 125: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1   0: , 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 126: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )]), 127: ([('optimizer_states', 0, 'optimizer', 'param_groups', 1, 'step')], [], [(('callbacks', 'Timer', 'time_elapsed', 'train'), , )])} ``` 2. load case ```  52: Error executing job with overrides: []  52: Traceback (most recent call last):  52:   File ""/opt/NeMo/examples/nlp/language_modeling/megatron_gpt_pretraining.py"", line 66, in main  52:     trainer.fit(model)  52:   File ""/usr/local/lib/python3.10/distpackages/lightning/pytorch/trainer/trainer.py"", line 538, in fit  52:     call._call_and_handle_interrupt(  52:   File ""/usr/local/lib/python3.10/distpackages/lightning/pytorch/trainer/call.py"", line 46, in _call_and_handle_interrupt  52:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)  52:   File ""/usr/local/lib/python3.10/distpackages/lightning/pytorch/strategies/launchers/subprocess_script.py"", line 105, in launch  52:     return function(*args, **kwargs)  52:   File ""/usr/local/lib/python3.10/distpackages/lightning/pytorch/trainer/trainer.py"", line 574, in _fit_impl  52:     self._run(model, ckpt_path=ckpt_path)  52:   File ""/usr/local/lib/python3.10/distpackages/lightning/pytorch/trainer/trainer.py"", line 968, in _run  52:     self._checkpoint_connector._restore_modules_and_callbacks(ckpt_path)  52:   File ""/usr/local/lib/python3.10/distpackages/lightning/pytorch/trainer/connectors/checkpoint_connector.py"", line 398, in _restore_modules_and_callbacks  52:     self.restore_model()  52:   File ""/usr/local/lib/python3.10/distpackages/lightning/pytorch/trainer/connectors/checkpoint_connector.py"", line 272, in restore_model  52:     call._call_lightning_module_hook(self.trainer, ""on_load_checkpoint"", self._loaded_checkpoint)  52:   File ""/usr/local/lib/python3.10/distpackages/lightning/pytorch/trainer/call.py"", line 167, in _call_lightning_module_hook  52:     output = fn(*args, **kwargs)  52:   File ""/opt/NeMo/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py"", line 1996, in on_load_checkpoint  52:     module.load_state_dict(checkpoint_state_dict, strict=True)  52:   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 2564, in load_state_dict  52:     load(self, state_dict)  52:   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 2552, in load  52:     load(child, child_state_dict, child_prefix)   noqa: F821  52:   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 2552, in load  52:     load(child, child_state_dict, child_prefix)   noqa: F821  52:   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 2552, in load  52:     load(child, child_state_dict, child_prefix)   noqa: F821  52:   [Previous line repeated 3 more times]  52:   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 2535, in load  52:     module._load_from_state_dict(  52:   File ""/usr/local/lib/python3.10/distpackages/transformer_engine/pytorch/module/base.py"", line 1104, in _load_from_state_dict  52:     self.set_extra_state(state_dict[extra_state_key])  52:   File ""/usr/local/lib/python3.10/distpackages/transformer_engine/pytorch/module/base.py"", line 645, in set_extra_state  52:     self.fp8_meta[""scaling_fwd""].scale.copy_(state[""scale_fwd""])  52: RuntimeError: The size of tensor a (192) must match the size of tensor b (3) at nonsingleton dimension 0 ``` **Environment (please complete the following information):**  NGC nemo docker 24.12  transformer_engine 1.13  MegatronLM commit ID  1ce944c1835e649ac7c46e412a6bb8a1858b039c **Proposed fix** **Additional context** Add any other context about the problem here.",2025-01-08T15:21:13Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/1350,Add  for viz," Currently, NeMo hasn't officially supported MoE with fp8. Internally we have plans to work on this and will officially support it in the future releases.",Marking as stale. No activity in 60 days.
eliird,[BUG] Using fp16 uses more memory than using fp32,"**Describe the bug** Using fp16 or bf16 uses more memory than using fp32 **To Reproduce** Here are the training parameters I am using to train the model. When I comment out the `fp16`, the memory usage increases. My setup 8xH100.  ```bash GPT_MODEL_ARGS=(     numlayers 32     hiddensize 4096     numattentionheads 32     seqlength 4096     nopositionembedding     nomaskedsoftmaxfusion     userotarypositionembeddings     maxpositionembeddings 8192     attentiondropout 0     hiddendropout 0     normalization RMSNorm     ffnhiddensize 14336     numquerygroups 8     swiglu     groupqueryattention     tokenizertype HuggingFaceTokenizer      untieembeddingsandoutputweights     positionembeddingtype rope     disablebiaslinear     tokenizermodel $TOKENIZER_SAVE_PATH ) TRAINING_ARGS=(     microbatchsize $MICRO_BATCH_SIZE     globalbatchsize $GLOBAL_BATCH_SIZE     trainiters 500000     weightdecay 0.1     adambeta1 0.9     adambeta2 0.95     initmethodstd 0.006     clipgrad 1.0     fp16  disabling this parameter should use fp32, and it reduces memory usage.     lr 6.0e5     lrdecaystyle cosine     minlr 6.0e6     lrwarmupfraction .001     lrdecayiters 430000     optimizer sgd     emptyunusedmemorylevel 2     recomputegranularity ""full""     recomputemethod uniform     recomputenumlayers 1     transformerimpl ""transformer_engine"" ) MODEL_PARALLEL_ARGS=(      tensormodelparallelsize 8     pipelinemodelparallelsize 1     sequenceparallel ) DATA_ARGS=(     datapath $DATA_PATH     split 949,50,1 ) EVAL_AND_LOGGING_ARGS=(     loginterval 10     saveinterval 10000     evalinterval 1000     save $CHECKPOINT_SAVE_PATH      load $CHECKPOINT_LOAD_PATH     evaliters 10     tensorboarddir $TENSORBOARD_LOGS_PATH     logthroughput ) python pretrain_gpt.py \     ${GPT_MODEL_ARGS[@]} \     ${TRAINING_ARGS[@]} \     ${MODEL_PARALLEL_ARGS[@]} \     ${DATA_ARGS[@]} \     ${EVAL_AND_LOGGING_ARGS[@]} ``` **Expected behavior** FP16 should use less memory than that of FP32 **Stack trace/logs** FP16 MEMORY USAGE !Image FP32 MEMORY USAGE !Image **Environment (please complete the following information):**   MegatronLM commit ID 1ce944c1835e649ac7c46e412a6bb8a1858b039c   PyTorch version   2.4   CUDA version      12.1   NCCL version",2025-01-08T03:06:08Z,stale,open,0,7,https://github.com/NVIDIA/Megatron-LM/issues/1349,"I tried looking at the internal code of loading the model and it seems that model is moved to GPU and then converted to fp16, would  that not consume more memory when the model is being loaded. Probably has nothing to do with the used memory but still... MegatronLM/megatron/training/training.py line 535 ```python   GPU allocation.  for model_module in model:       model_module.cuda(torch.cuda.current_device())    Fp16 conversion.   if args.fp16 or args.bf16:       model = [Float16Module(model_module, args) for model_module in model] ```","I am still trying to look throught he code but the main difference is the fp16 optimizer has groups with both fp32 and fp16 parameters, probably somewhere duplicate memory is being used or something, will try to investigate a bit more but some feedback on this would be appreciated, especially if someone can confirm their memory usage also increases for fp16",Maybe the cause for the increased memory is the parameter being detached and cloned iin the initialization of the FP16Optimizer class.  ~I am adding the snippet of the code below~ probably better to refer to the code.  I will do some profiling later. https://github.com/NVIDIA/MegatronLM/blob/1ce944c1835e649ac7c46e412a6bb8a1858b039c/megatron/core/optimizer/optimizer.pyL550,"The Distributed Optimizer section in the README explains that when not using zero1, the model state for fp16 is 4 bytes (2016=4) larger than that for fp32.  This is because it additionally saves the parameters and gradients in fp16 format.  It appears that your parameter script does not enable zero1, and dp_size is set to 1 (with tp_size set to 8).",I am using TP=8 because I was trying to reduce the memory usage for a model that barely fits in a single node so I could increase batch size.   I am sorry I dont understand what is 20 and what is 16 that are being subtracted. I would be grateful if you can explain it. ,"> I am using TP=8 because I was trying to reduce the memory usage for a model that barely fits in a single node so I could increase batch size. >  > I am sorry I dont understand what is 20 and what is 16 that are being subtracted. I would be grateful if you can explain it. The example mentioned above calculates the memory usage of the optimizer state using the Adam optimizer. Your script uses the SGD optimizer, so the calculation method may be different. SGD optimizer: When using fp32 format, it stores parameters and gradients in fp32 format. When using fp16 format, I guess it additionally stores parameters and gradients in fp16 format, which leads to increased memory usage.",Marking as stale. No activity in 60 days.
Sun2018421,[BUG] When trying to convert llama2-7b model from HF format to megatron format,"**Describe the bug** A clear and concise description of what the bug is. The error is  ``` Traceback (most recent call last):   File ""/usr/lib/python3.12/multiprocessing/process.py"", line 314, in _bootstrap     self.run()   File ""/usr/lib/python3.12/multiprocessing/process.py"", line 108, in run     self._target(*self._args, **self._kwargs)   File ""/workdir/MegatronLM/tools/checkpoint/saver_megatron.py"", line 168, in save_checkpoint     validate_args(margs)   File ""/workdir/MegatronLM/megatron/training/arguments.py"", line 405, in validate_args     args.main_grads_dtype = dtype_map[args.main_grads_dtype]                             ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^ KeyError: torch.float32 ```  **To Reproduce** Steps to reproduce the behavior. The easier it is to reproduce the faster it will get maintainer attention. 1. I get the llama27b model from https://huggingface.co/metallama/Llama27bhf/tree/main 2. Try the script as follow  ```shell !/bin/bash TP=1 PP=2 HF_FORMAT_DIR=/workdir/models/llama27b MEGATRON_FORMAT_DIR=/workdir/models/MModels TOKENIZER_MODEL=${HF_FORMAT_DIR} python tools/checkpoint/convert.py          bf16          modeltype GPT          loader llama_mistral          saver megatron          targettensorparallelsize ${TP}          checkpointtype hf          loaddir ${HF_FORMAT_DIR}          modelsize llama27B          savedir ${MEGATRON_FORMAT_DIR}          tokenizermodel ${TOKENIZER_MODEL}  ```  **Stack trace/logs** ```shell Loaded loader_llama_mistral as the loader. Loaded saver_megatron as the saver. Starting saver... Starting loader... :488: DeprecationWarning: Type google._upb._message.MessageMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14. :488: DeprecationWarning: Type google._upb._message.ScalarMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14. /usr/local/lib/python3.12/distpackages/google/protobuf/internal/well_known_types.py:93: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezoneaware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).   _EPOCH_DATETIME_NAIVE = datetime.datetime.utcfromtimestamp(0) :488: DeprecationWarning: Type google._upb._message.MessageMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14. :488: DeprecationWarning: Type google._upb._message.ScalarMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14. /usr/local/lib/python3.12/distpackages/google/protobuf/internal/well_known_types.py:93: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezoneaware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).   _EPOCH_DATETIME_NAIVE = datetime.datetime.utcfromtimestamp(0) using world size: 1, dataparallel size: 1, contextparallel size: 1, hierarchical contextparallel sizes: Nonetensormodelparallel size: 1, encodertensormodelparallel size: 0, pipelinemodelparallel size: 1, encoderpipelinemodelparallel size: 0 using torch.float32 for parameters ... max_p_e is 4096 and seq_length is 4096  arguments    accumulate_allreduce_grads_in_fp32 .............. False   adam_beta1 ...................................... 0.9   adam_beta2 ...................................... 0.999   adam_eps ........................................ 1e08   add_bias_linear ................................. False   add_position_embedding .......................... True   add_qkv_bias .................................... False   adlr_autoresume ................................. False   adlr_autoresume_interval ........................ 1000   align_grad_reduce ............................... True   align_param_gather .............................. False   app_tag_run_name ................................ None   app_tag_run_version ............................. 0.0.0   apply_layernorm_1p .............................. False   apply_query_key_layer_scaling ................... False   apply_residual_connection_post_layernorm ........ False   apply_rope_fusion ............................... True   async_save ...................................... None   async_tensor_model_parallel_allreduce ........... False   attention_backend ............................... AttnBackend.auto   attention_dropout ............................... 0.1   attention_softmax_in_fp32 ....................... False   auto_detect_ckpt_format ......................... False   barrier_with_L1_time ............................ True   bert_binary_head ................................ True   bert_embedder_type .............................. megatron   bert_load ....................................... None   bf16 ............................................ False   bias_dropout_fusion ............................. False   bias_gelu_fusion ................................ False   bias_swiglu_fusion .............................. True   biencoder_projection_dim ........................ 0   biencoder_shared_query_context_model ............ False   block_data_path ................................. None   calculate_per_token_loss ........................ False   check_for_nan_in_loss_and_grad .................. True   check_for_spiky_loss ............................ False   check_weight_hash_across_dp_replicas_interval ... None   ckpt_assume_constant_structure .................. False   ckpt_convert_format ............................. None   ckpt_convert_save ............................... None   ckpt_convert_update_legacy_dist_opt_format ...... False   ckpt_format ..................................... torch_dist   ckpt_fully_parallel_load ........................ False   ckpt_fully_parallel_save ........................ True   ckpt_fully_parallel_save_deprecated ............. False   ckpt_step ....................................... None   classes_fraction ................................ 1.0   clip_grad ....................................... 1.0   clone_scatter_output_in_embedding ............... True   config_logger_dir ...............................    consumed_train_samples .......................... 0   consumed_valid_samples .......................... 0   context_parallel_size ........................... 1   cp_comm_type .................................... ['p2p']   create_attention_mask_in_dataloader ............. True   cross_entropy_loss_fusion ....................... False   data_args_path .................................. None   data_cache_path ................................. None   data_parallel_random_init ....................... False   data_parallel_size .............................. 1   data_path ....................................... None   data_per_class_fraction ......................... 1.0   data_sharding ................................... True   dataloader_type ................................. single   ddp_average_in_collective ....................... False   ddp_bucket_size ................................. None   decoder_first_pipeline_num_layers ............... None   decoder_last_pipeline_num_layers ................ None   decoder_num_layers .............................. None   decoder_seq_length .............................. None   decoupled_lr .................................... None   decoupled_min_lr ................................ None   decrease_batch_size_if_needed ................... False   defer_embedding_wgrad_compute ................... False   deprecated_use_mcore_models ..................... False   deterministic_mode .............................. False   dino_bottleneck_size ............................ 256   dino_freeze_last_layer .......................... 1   dino_head_hidden_size ........................... 2048   dino_local_crops_number ......................... 10   dino_local_img_size ............................. 96   dino_norm_last_layer ............................ False   dino_teacher_temp ............................... 0.07   dino_warmup_teacher_temp ........................ 0.04   dino_warmup_teacher_temp_epochs ................. 30   disable_straggler_on_startup .................... False   dist_ckpt_format_deprecated ..................... None   dist_ckpt_strictness ............................ assume_ok_unexpected   distribute_saved_activations .................... False   distributed_backend ............................. nccl   distributed_timeout_minutes ..................... 10   embedding_path .................................. None   empty_unused_memory_level ....................... 0   enable_ft_package ............................... False   enable_one_logger ............................... False   encoder_num_layers .............................. 32   encoder_pipeline_model_parallel_size ............ 0   encoder_seq_length .............................. 4096   encoder_tensor_model_parallel_size .............. 0   end_weight_decay ................................ 0.01   eod_mask_loss ................................... False   error_injection_rate ............................ 0   error_injection_type ............................ transient_error   eval_interval ................................... 1000   eval_iters ...................................... 100   evidence_data_path .............................. None   exit_duration_in_mins ........................... None   exit_interval ................................... None   exit_on_missing_checkpoint ...................... False   exit_signal_handler ............................. False   exp_avg_dtype ................................... torch.float32   exp_avg_sq_dtype ................................ torch.float32   expert_model_parallel_size ...................... 1   expert_tensor_parallel_size ..................... 1   ffn_hidden_size ................................. 11008   finetune ........................................ False   flash_decode .................................... False   fp16 ............................................ False   fp16_lm_cross_entropy ........................... False   fp32_residual_connection ........................ False   fp8 ............................................. None   fp8_amax_compute_algo ........................... most_recent   fp8_amax_history_len ............................ 1   fp8_interval .................................... 1   fp8_margin ...................................... 0   fp8_param_gather ................................ False   fp8_wgrad ....................................... True   global_batch_size ............................... 1024   gradient_accumulation_fusion .................... True   group_query_attention ........................... True   head_lr_mult .................................... 1.0   hidden_dropout .................................. 0.1   hidden_size ..................................... 4096   hierarchical_context_parallel_sizes ............. None   hybrid_attention_ratio .......................... 0.0   hybrid_mlp_ratio ................................ 0.0   hybrid_override_pattern ......................... None   hysteresis ...................................... 2   ict_head_size ................................... None   ict_load ........................................ None   img_h ........................................... 224   img_w ........................................... 224   indexer_batch_size .............................. 128   indexer_log_interval ............................ 1000   inference_batch_times_seqlen_threshold .......... 1   inference_max_seq_length ........................ 2560   init_method_std ................................. 0.02   init_method_xavier_uniform ...................... False   initial_loss_scale .............................. 4294967296   iter_per_epoch .................................. 1250   iteration ....................................... 1   kv_channels ..................................... 128   kv_lora_rank .................................... 32   lazy_mpu_init ................................... None   load ............................................ /workdir/models/llama27b   local_rank ...................................... 0   log_interval .................................... 100   log_loss_scale_to_tensorboard ................... True   log_memory_to_tensorboard ....................... False   log_num_zeros_in_grad ........................... False   log_params_norm ................................. False   log_progress .................................... False   log_straggler ................................... False   log_throughput .................................. False   log_timers_to_tensorboard ....................... False   log_validation_ppl_to_tensorboard ............... False   log_world_size_to_tensorboard ................... False   logging_level ................................... None   loss_scale ...................................... None   loss_scale_window ............................... 1000   lr .............................................. None   lr_decay_iters .................................. None   lr_decay_samples ................................ None   lr_decay_style .................................. linear   lr_warmup_fraction .............................. None   lr_warmup_init .................................. 0.0   lr_warmup_iters ................................. 0   lr_warmup_samples ............................... 0   lr_wsd_decay_iters .............................. None   lr_wsd_decay_samples ............................ None   lr_wsd_decay_style .............................. exponential   main_grads_dtype ................................ torch.float32   main_params_dtype ............................... torch.float32   make_vocab_size_divisible_by .................... 128   manual_gc ....................................... False   manual_gc_eval .................................. True   manual_gc_interval .............................. 0   mask_factor ..................................... 1.0   mask_prob ....................................... 0.15   mask_type ....................................... random   masked_softmax_fusion ........................... False   max_position_embeddings ......................... 4096   max_tokens_to_oom ............................... 12000   memory_snapshot_path ............................ snapshot.pickle   merge_file ...................................... None   micro_batch_size ................................ 1   microbatch_group_size_per_vp_stage .............. None   min_loss_scale .................................. 1.0   min_lr .......................................... 0.0   mmap_bin_files .................................. True   mock_data ....................................... True   moe_aux_loss_coeff .............................. 0.0   moe_expert_capacity_factor ...................... None   moe_extended_tp ................................. False   moe_ffn_hidden_size ............................. 11008   moe_grouped_gemm ................................ False   moe_input_jitter_eps ............................ None   moe_layer_freq .................................. 1   moe_layer_recompute ............................. False   moe_pad_expert_input_to_capacity ................ False   moe_per_layer_logging ........................... False   moe_router_load_balancing_type .................. aux_loss   moe_router_pre_softmax .......................... False   moe_router_topk ................................. 2   moe_router_topk_limited_devices ................. None   moe_router_topk_scaling_factor .................. None   moe_shared_expert_intermediate_size ............. None   moe_shared_expert_overlap ....................... False   moe_token_dispatcher_type ....................... allgather   moe_token_drop_policy ........................... probs   moe_use_legacy_grouped_gemm ..................... False   moe_use_upcycling ............................... False   moe_z_loss_coeff ................................ None   multi_latent_attention .......................... False   nccl_communicator_config_path ................... None   no_load_optim ................................... True   no_load_rng ..................................... True   no_persist_layer_norm ........................... False   no_save_optim ................................... True   no_save_rng ..................................... True   non_persistent_ckpt_type ........................ None   non_persistent_global_ckpt_dir .................. None   non_persistent_local_ckpt_algo .................. fully_parallel   non_persistent_local_ckpt_dir ................... None   non_persistent_save_interval .................... None   norm_epsilon .................................... 1e05   normalization ................................... RMSNorm   num_attention_heads ............................. 32   num_channels .................................... 3   num_classes ..................................... 1000   num_dataset_builder_threads ..................... 1   num_distributed_optimizer_instances ............. 1   num_experts ..................................... None   num_layers ...................................... 32   num_layers_per_virtual_pipeline_stage ........... None   num_query_groups ................................ 32   num_workers ..................................... 2   one_logger_async ................................ False   one_logger_project .............................. megatronlm   one_logger_run_name ............................. None   onnx_safe ....................................... None   openai_gelu ..................................... False   optimizer ....................................... adam   output_bert_embeddings .......................... False   overlap_grad_reduce ............................. False   overlap_p2p_comm ................................ False   overlap_p2p_comm_warmup_flush ................... False   overlap_param_gather ............................ False   overlap_param_gather_with_optimizer_step ........ False   override_opt_param_scheduler .................... False   padded_vocab_size ............................... 32000   params_dtype .................................... torch.float32   patch_dim ....................................... 16   per_split_data_args_path ........................ None   perform_initialization .......................... False   pipeline_model_parallel_size .................... 1   pipeline_model_parallel_split_rank .............. None   position_embedding_type ......................... rope   pretrained_checkpoint ........................... None   profile ......................................... False   profile_ranks ................................... [0]   profile_step_end ................................ 12   profile_step_start .............................. 10   q_lora_rank ..................................... None   qk_head_dim ..................................... 128   qk_layernorm .................................... False   qk_pos_emb_head_dim ............................. 64   query_in_block_prob ............................. 0.1   rampup_batch_size ............................... None   rank ............................................ 0   recompute_granularity ........................... None   recompute_method ................................ None   recompute_num_layers ............................ None   record_memory_history ........................... False   renormalize_blend_weights ....................... False   rerun_mode ...................................... disabled   reset_attention_mask ............................ False   reset_position_ids .............................. False   retriever_report_topk_accuracies ................ []   retriever_score_scaling ......................... False   retriever_seq_length ............................ 256   retro_add_retriever ............................. False   retro_attention_gate ............................ 1   retro_cyclic_train_iters ........................ None   retro_encoder_attention_dropout ................. 0.1   retro_encoder_hidden_dropout .................... 0.1   retro_encoder_layers ............................ 2   retro_num_neighbors ............................. 2   retro_num_retrieved_chunks ...................... 2   retro_project_dir ............................... None   retro_verify_neighbor_count ..................... True   rotary_base ..................................... 10000   rotary_interleaved .............................. False   rotary_percent .................................. 1.0   rotary_scaling_factor ........................... 1.0   rotary_seq_len_interpolation_factor ............. None   s3_cache_path ................................... None   sample_rate ..................................... 1.0   save ............................................ None   save_interval ................................... None   scatter_gather_tensors_in_pipeline .............. True   seed ............................................ 1234   seq_length ...................................... 4096   sequence_parallel ............................... False   sgd_momentum .................................... 0.9   short_seq_prob .................................. 0.1   skip_train ...................................... False   skipped_train_samples ........................... 0   spec ............................................ None   split ........................................... None   squared_relu .................................... False   standalone_embedding_stage ...................... False   start_weight_decay .............................. 0.01   straggler_ctrlr_port ............................ 65535   straggler_minmax_count .......................... 1   swiglu .......................................... True   swin_backbone_type .............................. tiny   tensor_model_parallel_size ...................... 1   tensorboard_dir ................................. None   tensorboard_log_interval ........................ 1   tensorboard_queue_size .......................... 1000   test_data_path .................................. None   test_mode ....................................... False   tiktoken_num_special_tokens ..................... 1000   tiktoken_pattern ................................ None   tiktoken_special_tokens ......................... None   timing_log_level ................................ 0   timing_log_option ............................... minmax   titles_data_path ................................ None   tokenizer_model ................................. /workdir/models/llama27b   tokenizer_type .................................. Llama2Tokenizer   tp_comm_bootstrap_backend ....................... nccl   tp_comm_bulk_dgrad .............................. True   tp_comm_bulk_wgrad .............................. True   tp_comm_overlap ................................. False   tp_comm_overlap_ag .............................. True   tp_comm_overlap_cfg ............................. None   tp_comm_overlap_rs .............................. True   tp_comm_overlap_rs_dgrad ........................ False   tp_comm_split_ag ................................ True   tp_comm_split_rs ................................ True   train_data_path ................................. None   train_iters ..................................... None   train_samples ................................... None   train_sync_interval ............................. None   transformer_impl ................................ transformer_engine   transformer_pipeline_model_parallel_size ........ 1   untie_embeddings_and_output_weights ............. True   use_checkpoint_args ............................. False   use_checkpoint_opt_param_scheduler .............. False   use_cpu_initialization .......................... True   use_dist_ckpt ................................... True   use_dist_ckpt_deprecated ........................ False   use_distributed_optimizer ....................... False   use_flash_attn .................................. False   use_legacy_models ............................... False   use_mp_args_from_checkpoint_args ................ False   use_one_sent_docs ............................... False   use_precision_aware_optimizer ................... False   use_pytorch_profiler ............................ False   use_ring_exchange_p2p ........................... False   use_rope_scaling ................................ False   use_rotary_position_embeddings .................. False   use_tokenizer_model_from_checkpoint_args ........ True   use_torch_fsdp2 ................................. False   use_tp_pp_dp_mapping ............................ False   v_head_dim ...................................... 128   valid_data_path ................................. None   variable_seq_lengths ............................ False   virtual_pipeline_model_parallel_size ............ None   vision_backbone_type ............................ vit   vision_pretraining .............................. False   vision_pretraining_type ......................... classify   vocab_extra_ids ................................. 0   vocab_file ...................................... None   vocab_size ...................................... 32000   wandb_exp_name ..................................    wandb_project ...................................    wandb_save_dir ..................................    weight_decay .................................... 0.01   weight_decay_incr_style ......................... constant   wgrad_deferral_limit ............................ 0   world_size ...................................... 1   yaml_cfg ........................................ None  end of arguments  Loading checkpoint shards: 100% 32/32 [00:03<00:00,  8.98it/s] sending embeddings Overwriting default ffn_hidden_size value None with value from checkpoint 11008. Overwriting default kv_channels value None with value from checkpoint 128. Overwriting default group_query_attention value False with value from checkpoint True. Overwriting default num_query_groups value 1 with value from checkpoint 32. Overwriting default normalization value LayerNorm with value from checkpoint RMSNorm. Overwriting default swiglu value False with value from checkpoint True. Overwriting default global_batch_size value None with value from checkpoint 1024. Overwriting default dataloader_type value None with value from checkpoint single. Overwriting default use_legacy_models value False with value from checkpoint True. Overwriting default load value None with value from checkpoint /workdir/models/llama27b. Overwriting default overlap_p2p_comm value True with value from checkpoint False. Overwriting default align_param_gather value True with value from checkpoint False. Overwriting default mock_data value False with value from checkpoint True. Overwriting default vocab_size value None with value from checkpoint 32000. Overwriting default expert_tensor_parallel_size value None with value from checkpoint 1. Overwriting default moe_ffn_hidden_size value None with value from checkpoint 11008. Overwriting default transformer_impl value transformer_engine with value from checkpoint local. Overwriting default main_grads_dtype value fp32 with value from checkpoint torch.float32. Overwriting default main_params_dtype value fp32 with value from checkpoint torch.float32. Overwriting default exp_avg_dtype value fp32 with value from checkpoint torch.float32. Overwriting default exp_avg_sq_dtype value fp32 with value from checkpoint torch.float32. Checkpoint had argument iteration but new arguments does not have this. Checkpoint had argument padded_vocab_size but new arguments does not have this. Checkpoint had argument use_dist_ckpt but new arguments does not have this. Checkpoint had argument transformer_pipeline_model_parallel_size but new arguments does not have this. Checkpoint had argument data_parallel_size but new arguments does not have this. Checkpoint had argument consumed_train_samples but new arguments does not have this. Checkpoint had argument skipped_train_samples but new arguments does not have this. Checkpoint had argument consumed_valid_samples but new arguments does not have this. Checkpoint had argument variable_seq_lengths but new arguments does not have this. Checkpoint had argument disable_bias_linear but new arguments does not have this. Checkpoint had argument model_type but new arguments does not have this. Checkpoint had argument model_size but new arguments does not have this. using world size: 1, dataparallel size: 1, contextparallel size: 1, hierarchical contextparallel sizes: Nonetensormodelparallel size: 1, encodertensormodelparallel size: 0, pipelinemodelparallel size: 1, encoderpipelinemodelparallel size: 0 Process Process1: Traceback (most recent call last):   File ""/usr/lib/python3.12/multiprocessing/process.py"", line 314, in _bootstrap     self.run()   File ""/usr/lib/python3.12/multiprocessing/process.py"", line 108, in run     self._target(*self._args, **self._kwargs)   File ""/workdir/MegatronLM/tools/checkpoint/saver_megatron.py"", line 168, in save_checkpoint     validate_args(margs)   File ""/workdir/MegatronLM/megatron/training/arguments.py"", line 405, in validate_args     args.main_grads_dtype = dtype_map[args.main_grads_dtype]                             ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^ KeyError: torch.float32 sending transformer layer 0 sending transformer layer 1 sending transformer layer 2 sending transformer layer 3 sending transformer layer 4 sending transformer layer 5 sending transformer layer 6 sending transformer layer 7 sending transformer layer 8 sending transformer layer 9 sending transformer layer 10 sending transformer layer 11 sending transformer layer 12 sending transformer layer 13 sending transformer layer 14 sending transformer layer 15 sending transformer layer 16 sending transformer layer 17 sending transformer layer 18 sending transformer layer 19 sending transformer layer 20 sending transformer layer 21 sending transformer layer 22 sending transformer layer 23 sending transformer layer 24 sending transformer layer 25 sending transformer layer 26 sending transformer layer 27 sending transformer layer 28 sending transformer layer 29 sending transformer layer 30 sending transformer layer 31 sending final norm sending output layer Waiting for saver to complete... ```  **Environment (please complete the following information):**   Docker images: nvcr.io/nvidia/pytorch 24.12py3 https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch   MegatronLM commit ID commit 076972e37420b5325c5fe06e7131be7d96f05b53   PyTorch version 2.6.0a0+df5bbc09d1.nv24.12   CUDA version cuda_12.6.r12.6/compiler.35059454_0   transformers  4.47.1 **Proposed fix** If you have a proposal for how to fix the issue state it here or link to a PR. **Additional context** Add any other context about the problem here. When i try to install the transformers=4.31/4.32, there will be an error about compiling the tokenizer whl. So i choose to ""pip install transformers"" instead. I follow the guidence at  https://github.com/NVIDIA/MegatronLM/blob/main/docs/llama_mistral.md",2025-01-06T11:34:27Z,stale,open,0,7,https://github.com/NVIDIA/Megatron-LM/issues/1348,i meet this question too!,> i meet this question too! I am going to try Nemo for finetuning the model :),"Meet this question too. Don't know why this happened, but can be ignored by changing code around `""/workdir/MegatronLM/megatron/training/arguments.py"", line 402~405`   from  ```     dtype_map = {         'fp32': torch.float32, 'bf16': torch.bfloat16, 'fp16': torch.float16, 'fp8': torch.uint8,     } ``` to  ```     dtype_map = {         'fp32': torch.float32, 'bf16': torch.bfloat16, 'fp16': torch.float16, 'fp8': torch.uint8,         torch.float32: torch.float32, torch.bfloat16: torch.bfloat16, torch.float16: torch.float16, torch.uint8: torch.uint8     } ```","> Meet this question too. >  > Don't know why this happened, but can be ignored by changing code around `""/workdir/MegatronLM/megatron/training/arguments.py"", line 402~405` >  > from >  > ``` > dtype_map = { >         'fp32': torch.float32, 'bf16': torch.bfloat16, 'fp16': torch.float16, 'fp8': torch.uint8, >     } > ``` >  > to >  > ``` > dtype_map = { >         'fp32': torch.float32, 'bf16': torch.bfloat16, 'fp16': torch.float16, 'fp8': torch.uint8, >         torch.float32: torch.float32, torch.bfloat16: torch.bfloat16, torch.float16: torch.float16, torch.uint8: torch.uint8 >     } > ``` Thanks a lot, i will try it later ","> Meet this question too. >  > Don't know why this happened, but can be ignored by changing code around `""/workdir/MegatronLM/megatron/training/arguments.py"", line 402~405` >  > from >  > ``` > dtype_map = { >         'fp32': torch.float32, 'bf16': torch.bfloat16, 'fp16': torch.float16, 'fp8': torch.uint8, >     } > ``` >  > to >  > ``` > dtype_map = { >         'fp32': torch.float32, 'bf16': torch.bfloat16, 'fp16': torch.float16, 'fp8': torch.uint8, >         torch.float32: torch.float32, torch.bfloat16: torch.bfloat16, torch.float16: torch.float16, torch.uint8: torch.uint8 >     } > ``` thank youï¼Œi'll have a try.","> Meet this question too. >  > Don't know why this happened, but can be ignored by changing code around `""/workdir/MegatronLM/megatron/training/arguments.py"", line 402~405` >  > from >  > ``` > dtype_map = { >         'fp32': torch.float32, 'bf16': torch.bfloat16, 'fp16': torch.float16, 'fp8': torch.uint8, >     } > ``` >  > to >  > ``` > dtype_map = { >         'fp32': torch.float32, 'bf16': torch.bfloat16, 'fp16': torch.float16, 'fp8': torch.uint8, >         torch.float32: torch.float32, torch.bfloat16: torch.bfloat16, torch.float16: torch.float16, torch.uint8: torch.uint8 >     } > ``` Did this work for anyone?",Marking as stale. No activity in 60 days.
deep-sci,Fix typo,Fixes minor documentation typo.  MoE Parallel Fodling > MoE Parallel Folding. Fixes CC([QUESTION] Typo in MoE README) ,2025-01-04T13:22:56Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1347,Marking as stale. No activity in 60 days.
rgtjf,[QUESTION] Typo in MoE README,**Your question** Ask a clear and concise question about MegatronLM. https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/transformer/moe/README.md MoE Parallel Fodling > MoE Parallel Folding,2025-01-04T03:45:22Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/1346,Thanks for reporting this typo!  ,Fixed in https://github.com/NVIDIA/MegatronLM/commit/c6e3b0c5bb4c14f57fb6d997dd11223e2ecbc839diff73e7d423b1148fd2cd8ea5b51936ba2c4e857b6e55ad22fd8cf456137301a752 Thanks for reporting.,Thanks!
okoge-kaz,Update theoretical memory footprint formula," number of parameters ```diff  + (2 / args.hidden_size) + + (1 / args.hidden_size) ```  Because there is `2 * args.num_layers * args.hidden_size * args.hidden_size`, the coefficient of layernorms should be 1, not 2.  final layernorm ```diff  (num_parameters_in_transformer_layers / args.pipeline_model_parallel_size) + (num_parameters_in_transformer_layers  args.hidden_size) / args.pipeline_model_parallel_size ``` Since the final layernorm is not relevant for stages other than the last pipeline stage, it is necessary to subtract the number of parameters for the final layernorm using ` args.hidden_size`.  gradients' type ```diff  6 + (12 / args.data_parallel_size) + (2 + gradient_accumulation_factor) + (12 / args.data_parallel_size ``` When `args.accumulate_allreduce_grads_in_fp32` is True, the coefficient can be set to 6 from parameters(bf16) + gradients(fp32), but when it is False, it becomes 4, so case separation is necessary.  flash attention ```diff  if not args.sequence_parallel or args.recompute_granularity != 'selective': + if not args.sequence_parallel or not ( +      args.recompute_granularity == 'selective' or args.use_flash_attn is True +  ): ``` Since it is possible to calculate not only when `args.recompute_granularity` is selective, but also when `args.use_flash_attn` is True, it is added to the conditions.  SwiGLU ```python             (                  SwiGLU                 2 * b * s * h   input                 + 2 * b * s * args.ffn_hidden_size   up_proj                 + 2 * b * s * args.ffn_hidden_size   gate_proj                 + 2 * b * s * args.ffn_hidden_size   act_fn                 + 2 * b * s * args.ffn_hidden_size   down_proj             ) if args.swiglu else (                 2 * b * s * h   h > ffn_h                 + 2 * b * s * args.ffn_hidden_size   act                 + 2 * b * s * args.ffn_hidden_size   ffn_h  > h             ) ``` Added conditional branching to support both GPT and Llama architectures  other changes  Theoritical memory footprint is easier to read in GB units, so I changed it from MB to GB.  Change so that the theoretical memory footprint per GPU when CP (Context Parallelism) is enabled can be correctly calculated.   Support GQA(Grouped Query Attention).",2025-01-03T12:27:27Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1345
okoge-kaz,Fix type annotation and checkpoint conversion script," What is the issue I have found that if you are using TransformerEngine v1.10 or later, `CUDA error: initialization error` will occur in the following location in saver_mcore.py. https://github.com/NVIDIA/MegatronLM/blob/076972e37420b5325c5fe06e7131be7d96f05b53/tools/checkpoint/saver_mcore.pyL299 If the version of TransformerEngine is 1.10 or later, `torch.cuda.Stream()` is called in the following places. This causes an initialization error. https://github.com/NVIDIA/MegatronLM/blob/076972e37420b5325c5fe06e7131be7d96f05b53/megatron/core/extensions/transformer_engine.pyL1218L1221 https://github.com/NVIDIA/TransformerEngine/blob/c9ea6be92948e1ec553037f1a04900617b9f7f6b/transformer_engine/pytorch/cpu_offload.pyL314L315  How to solve the issue Add `mp.set_start_method(method='spawn')` and `torch.cuda.init()`",2025-01-03T06:34:52Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1344
JiwenJ,[QUESTION] Resume training about dataset,"Hello, I have encountered training node collapse and I want to resume the training, I am wondering if megatron would reusme the datasets (i.e., not see the tokens have been trained) and if so how does it work and is there any argument in arguments.py related to that? Very appreciate it if someone can help~",2025-01-02T03:11:25Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1343,Marking as stale. No activity in 60 days.
kevin3567,[QUESTION] Expert Parallelism with Non-Identical Experts,"I am currently writing a distributed MoE training process for an MoE with different expert architectures. For example, consider a simple **Case A** where:  I have 8 GPUs  GPU 0 runs a 5layer MLP  GPU 13 each runs a 3layer MLP  GPU 47 each runs a 2layer MLP  All experts are parallelized in forward and backward pass. Additionally, consider another ""optimized"" **Case B** where: ** GPU 0 and GPU 1 runs a 5layer MLP via Tensor Parallel (between just these two processes)**  GPU 23 each runs a 3layer MLP  GPU 47  each runs a 2layer MLP  All experts are parallelized in forward and backward pass.   How would I go about writing this? Specifically, I am confused on:  how to implement efficient gradient computation and synchronization during backward pass in both cases  efficient alltoall communication for Case B which conducts TP within EP Furthermore, are there any examples I can refer to? Thanks in advance.",2025-01-01T22:09:05Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1342,Marking as stale. No activity in 60 days.
heavyrain-lzy,"[QUESTION]""a2a+p2p"" for context parallel(cp)","**Your question** https://github.com/NVIDIA/MegatronLM/commit/645c329d07b906464b33aad310ab9fb2b829ac09 If we enable a2a+p2p, we can't use the original get_batch_on_this_cp_rank. a2a communication needs the right rank order, like this PR: https://github.com/FlagOpen/FlagScale/pull/187",2024-12-27T10:06:32Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1341,Marking as stale. No activity in 60 days.
cos120,[QUESTION] Model performance about context parallel,"Hi, I am using Megatron core 0.9.0 with transformerengine(1.11+fc03478) and torch 2.4.0 on cuda12.4. I am doing sft training 5B model with 16K sequence, when I config tp2/pp2 the score of mmlu is 63 and it sames as FSDP, but when I config tp2/pp2/cp2, the score of mmlu is 58, but the training loss is same as tp2/pp2. How can I debug this? Does cp doing something special with sft training?",2024-12-27T10:05:18Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1340,Do you open grad clip?
fxnie,[QUESTION]How to convert the weight file format of the MAMBA model from pt to safetensors format?,"Ask a clear and concise question about MegatronLM. MODEL_PATH=""/workspace/mnt/xxx/ckpt/mamba2"" SAVE_PATH=""/workspace/mnt/xxx/models/convert_ckpt/mamba2"" python tools/checkpoint/hybrid_conversion.py \         loaddir ${MODEL_PATH} \         savedir ${SAVE_PATH} \         targettpsize 2 \         targetppsize 1 \         dmodel 2560 \         mambaversion 2 \         mambadstate 128 \         mamba2ngroups 8 \         mamba2headdim 64  I found that the conversion script is still in pt format after conversion. What should I do to convert it to SafeTensor format?  help",2024-12-26T11:01:20Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1339,Marking as stale. No activity in 60 days.
DemingCheng,[QUESTION] Why mixral use Llama2Tokenizer?,"Hi, All I saw that mixral use Llama2Tokenizer in its train script. Why mixral doesn't use its own tokenizer? !Image https://github.com/NVIDIA/MegatronLM/blob/main/examples/mixtral/train_mixtral_8x7b_distributed.shL62 Anyone's reply will be helpful to me, thanks",2024-12-25T06:56:20Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1338,Marking as stale. No activity in 60 days.
junjzhang,fix bugs of data preprocessing with multiple json keys,Hiï¼ŒI notice there may be a minor bug that will cause failure while adding multiple json_keys. And here is a patch.,2024-12-25T02:12:25Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1337,Marking as stale. No activity in 60 days.
fxnie,"raise Exception(""Unknown tensor name {}"".format(tensor_name)) Exception: Unknown tensor name decoder.layers.0.mixer.in_proj.layer_norm_weigh",**Your question** Ask a clear and concise question about MegatronLM.,2024-12-24T09:45:19Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1336,"TOKENIZER_MODEL=""/workspace/mnt/xxx/models/mamba22.7b/tokenizer.json"" MODEL_PATH=""/workspace/mnt/xxx/ckpt/mamba2"" SAVE_PATH=""/workspace/mnt/xxx/models/convert_ckpt/mamba2"" python tools/checkpoint/hybrid_conversion.py \         loaddir ${MODEL_PATH} \         savedir ${SAVE_PATH} \         targettpsize 2 \         targetppsize 1 \         dmodel 2560 \         mambaversion 2 \         mambadstate 128 \         mamba2ngroups 8 \         mamba2headdim 32",!Image
renyinCheng001,[QUESTION] Why BF16 has FP32 for param gradient at the first time unlike FP16 has FP16 for param gradient at the first time and switches them to FP32 when update param?,"Hi, All I saw that MegatronLM supports the configuration of **FP16** and **BF16** mixedprecision data types, and I found that these two different data type configurations correspond to different parameter types and gradient types:   When training with FP16, a copy of the FP32 gradient is required before updating the parameters  When training with BF16, no additional copies are required because the gradient itself is FP32 So, why BF16 has FP32 for param gradient at the first time unlike FP16 has FP16 for param gradient at the first time and switches them to FP32 when update param? Anyone's reply will be helpful to me, thanks",2024-12-24T03:25:20Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1335,Marking as stale. No activity in 60 days.
IgorZan,[QUESTION]How can I load a checkpoint trained by Megatron-LM 0.5 into Megatron-LM 0.7 to resume pretraing?,"Hi, I need some helps. A checkpoint trained by MegatronLM 0.5 like this: !Image Then I want to load it into MegatronLM 0.7 to resume pretraing. I got errors: !Image Are checkpoints trained by MegatronLM 0.5 compatible with MegatronLM 0.7? Thanks.",2024-12-22T02:20:00Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1333,Marking as stale. No activity in 60 days.
invisiblepancake,Create python-package.yml,,2024-12-21T13:29:24Z,,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1332,Marking as stale. No activity in 60 days.
thuwzt,Fix: prevent double accumulation of load balancing loss and z-loss wiâ€¦,Fix for the issue CC([BUG] MoE load balancing loss is accumulated twice when using activation checkpointing) : [BUG] MoE load balancing loss is accumulated twice when using activation checkpointing,2024-12-20T04:02:01Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/1331,"Thank you for fixing this issue. I have a question regarding the aux loss.  Although it appears to be doubled, I believe the actual training remains the same since recomputed is turned off. Aux loss is not accumulated to the optimizer. The reason I'm asking is that we previously had recomputed turned on for a period, but now we can turn it off. I'm confused about whether to double the moeauxlosscoeff. I don't think it's necessary to double it because loss value is added to aux_losses_tracker.  https://github.com/NVIDIA/MegatronLM/blob/2d7c521de7c5878501cf7e366e7d8e13267c8eb6/megatron/core/transformer/moe/moe_utils.pyL448."," Yes, you understand correctly. This is merely a display bug related to the tracker and logger. The compute flow remains unaffected, regardless of whether `moelayerrecompute` is used or not.",Marking as stale. No activity in 60 days.
thuwzt,[BUG] MoE load balancing loss is accumulated twice when using activation checkpointing,"**Describe the bug** Load balancing loss is accumulated twice when using activation checkpointing **To Reproduce** Train from scratch with / without `moelayerrecompute`, setting `moerouterloadbalancingtype aux_loss` **Expected behavior** Load balancing loss should be the same in the two settings (and should be slightly higher than 1 which means fully balanced) **Stack trace/logs**  without `moelayerrecompute`:   iteration 10: load_balancing_loss: 1.091395E+00   iteration 20: load_balancing_loss: 1.096082E+00   iteration 30: load_balancing_loss: 1.037049E+00  with `moelayerrecompute`:   iteration 10: load_balancing_loss: 2.202137E+00   iteration 20: load_balancing_loss: 2.298303E+00   iteration 30: load_balancing_loss: 2.120842E+00 **Environment (please complete the following information):**   MegatronLM d4e72c0d33edc0c53aeb624f617eb77cebce6ae9   PyTorch 2.4.1   CUDA 12.1   NCCL 2.20.5 **Proposed fix** Replace `if self.training` with `if self.training and torch.is_grad_enabled():`. Reason: When using activation checkpointing with `moelayerrecompute`, the forward function is executed twice. This leads to the load balancing loss being accumulated twice in `TopKRouter.aux_loss_load_balancing` within `megatron/core/transformer/moe/router.py` if the condition is only `if self.training:`. By changing the condition to `if self.training and torch.is_grad_enabled():`, the accumulation during the first pass (where gradients are not enabled) is prevented, while ensuring the standard training process without `moelayerrecompute` remains unaffected. A similar issue occurs with z_loss. The fix is included in the PR CC(Fix: prevent double accumulation of load balancing loss and zloss wiâ€¦). **Additional context** N/A",2024-12-20T04:00:42Z,,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1330,"Thanks for reporting and fixing this, this is likely a display bug with no impact on convergence. We'll take your PR internally and help get it merged.","I see a similar doubling when CUDA graphs are turned ON/OFF.  Please see this for ref: https://github.com/NVIDIA/MegatronLM/issues/1462issuecomment2732642584.  However, the above fix doesn't work with the graphs doubling. "
qingshanxwx,[BUG]megatron-lmï¼Œwith torchompileï¼ŒThe provided qkv memory layout is not supported!,"code: the main branch https://github.com/NVIDIA/MegatronLM transformer_engine 1.13 eager is normal, compile will report The provided qkv memory layout is not supported! The error is as followsï¼š !Image According to the functional description, the stride of k and v should be (S*H*D, H*D, D, 1)ï¼Œ but it is actually (H*D, H*D, D, 1) megatron/core/extensions/transformer_engine.py !Image printed result !Image Then call DotProductAttention, v'stride will become (524288, 128, 32, 1) directlyï¼Œk'stride not change. !Image printed result !Image",2024-12-20T03:32:50Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1329,Marking as stale. No activity in 60 days.
dynamicheart,[QUESTION] Why doesn't GPTDataset build a global shuffle index?,"**Your question** If the number samples of final epoch are less than 80% of a standard epoch, `GPTDataset` will separate it.  https://github.com/NVIDIA/MegatronLM/blob/7efaa73/megatron/core/datasets/gpt_dataset.pyL388L392  https://github.com/NVIDIA/MegatronLM/blob/7efaa73/megatron/core/datasets/gpt_dataset.pyL617 However, the loss appears to drop sharply when entering the final epoch. I think the problem is due to the different data distribution between the final epoch and the standard epochs. Is this expected in the design of `separate_last_epoch`? !Image",2024-12-20T02:51:00Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1328,Marking as stale. No activity in 60 days.
qi7kuo,[BUG] Precision issue caused by different token dispatchers in MoE training,"**Describe the bug** When using different dispatchers (i.e., AllGather and AlltoAll), it can cause training precision errors. **To Reproduce** You can use the following bash command to reproduce. My dataset is enwikipedia. ``` export CUDA_DEVICE_MAX_CONNECTIONS=1 MASTER_ADDR=localhost MASTER_PORT=6648 WORLD_SIZE=1 RANK=0 NPROC_PER_NODE=4 export CUDA_DEVICE_MAX_CONNECTIONS=1 DATA_PATH=/mnt/gpt_data DISTRIBUTED_ARGS=""nproc_per_node $NPROC_PER_NODE nnodes $WORLD_SIZE node_rank $RANK master_addr $MASTER_ADDR master_port $MASTER_PORT"" torchrun $DISTRIBUTED_ARGS \        pretrain_gpt.py \        numexperts 8 \        expertmodelparallelsize 4 \        optimizer adam \        distributedbackend nccl \        moetokendispatchertype alltoall \        numlayers 12 \        hiddensize 256 \        numattentionheads 32 \        swiglu \        ffnhiddensize 512 \        disablebiaslinear \        attentiondropout 0 \        hiddendropout 0 \        normalization RMSNorm \        untieembeddingsandoutputweights \        userotarypositionembeddings \        nopositionembedding \        nomaskedsoftmaxfusion \        microbatchsize 1 \        globalbatchsize 4 \        seqlength 2048 \        maxpositionembeddings 2048 \        datapath $DATA_PATH/wikigpt_text_document \        vocabfile $DATA_PATH/gpt2vocab.json \        mergefile $DATA_PATH/gpt2merges.txt \        split 100,0,0 \        trainiters 100 \        lrdecayiters 10000 \        lrwarmupiters 100 \        adambeta1 0.9 \        adambeta2 0.95 \        lr 1e5 \        lrdecaystyle cosine \        minlr 1e6 \        weightdecay 1e2 \        clipgrad 1.0 \        loginterval 1  2>&1 | tee moe_std.log ``` I find the differences when the iter=100. If we use alltoall dispatcher, the loss is 1.030484E+01. However, if we use allgather dispatcher, the loss is 1.030483E+01. **Stack trace/logs** I have investigate the problem, and my thought and trials are listed below: I checked the intermediate activation tensors. I noticed the precision difference happens after an iteration which has **0 token allocated for an expert** (In my environment, it happens in iteration 37) Then, I checked the weight & grad. I noticed that in iteration 37, the forward intermediate activation and model weight are the same. However, in the backward pass of iteration 37, **the module get different gradients** when using different dispatcher, which is straightforward reason of precision issue. **Environment (please complete the following information):**   MegatronLM commit ID 81fee9b0047fb3ac6001b5e71e4df89fc01b2a1c   PyTorch version: 2.4.0   CUDA version: 12.0   NCCL version: 24.07   Device: 4 * V100 (32GB) **Additional context** Let me have a summary here. I am looking forward to the help and finally find the root reason. To be honest, this bug confused me as the behavior of *AllGather Dispatcher* and *AlltoAll Dispatcher* should be the same. As I mentioned before, the precision issue happens when there is an expert receive 0 tokens, which maybe the core reason for the issue. I noticed that the forward activation tensors are same, but the backward **weight grad** tensors are different. ",2024-12-17T09:08:13Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1327,Marking as stale. No activity in 60 days.
zhangyilalala,[QUESTION] About using StreamingLLM,StreamingLLM can significantly improve the computational efficiency of the attention mechanism. Hugging Face and TensorRTLLM have already integrated this feature. Is it possible to use this functionality when training large models with Megatron?,2024-12-17T07:10:58Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1326,Marking as stale. No activity in 60 days.
meatybobby,Fix Gemma TRTLLM export,,2024-12-16T18:10:50Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1325
cailun01,[BUG] Using different distributed strategies of Megatron-LM to train the llama3.1-8B model results in inconsistent training loss,"**Describe the bug** I used 4 distributed training strategies to train llama3.18B from scratch. And I use same dataset and super parameters. But I got inconsistent training losses.  **To Reproduce** Steps to reproduce the behavior. The easier it is to reproduce the faster it will get maintainer attention. **Expected behavior** Regardless of the distributed training strategy used, the training loss should remain consistent. **Stack trace/logs** !Image **Environment (please complete the following information):**  **Proposed fix** If you have a proposal for how to fix the issue state it here or link to a PR. **Additional context** Add any other context about the problem here.",2024-12-16T08:11:17Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1324,Marking as stale. No activity in 60 days.
prrathi,"[BUG] FSDP requires torch optimizer, not transformer_engine or apex","**Describe the bug** Enabling FSDP doesn't work with `transformer_engine.pytorch.optimizers.FusedAdam` or `apex.optimizers.Adam`, and requires `torch.optim.AdamW` which isn't the default in `/work/nvme/bddk/prathi3/MegatronLM/megatron/core/optimizer/__init__.py`. **To Reproduce** `/usr/local/bin/torchrun max_restarts 1 nproc_per_node 2 nnodes 1 node_rank 0 master_addr {} master_port {} start_method spawn rdzv_backend static rdzv_endpoint {} rdzv_conf 'distributed_backend=nccl' pretrain_gpt.py numlayers 16 hiddensize 2048 ffnhiddensize 8192 numattentionheads 32 seqlength 8192 maxpositionembeddings 8192 swiglu trainiters 20 evaliters 1 tensormodelparallelsize 1 pipelinemodelparallelsize 1 contextparallelsize 1 usetorchfsdp2 nogradientaccumulationfusion microbatchsize 1 globalbatchsize 2 saveinterval 21 loginterval 1 logthroughput logginglevel 10 lr 0.003 lrdecayiters 320000 lrdecaystyle cosine minlr 1.0e5 clipgrad 0.0 lrwarmupfraction .01 weightdecay 0.1 vocabsize 128256 bf16 useflashattn usemcoremodels untieembeddingsandoutputweights positionembeddingtype rope normalization LayerNorm disablebiaslinear` on 2 A40 GPUs from branch `core_r0.10.0` **Expected behavior** Using transformer_engine or apex's optimizer should be disabled if FSDP is enabled **Stack trace/logs** For Apex Adam: ``` [rank0]: Traceback (most recent call last): [rank0]:   File ""MegatronLM/pretrain_gpt.py"", line 284, in  [rank0]:     pretrain( [rank0]:   File ""MegatronLM/megatron/training/training.py"", line 376, in pretrain [rank0]:     iteration, num_floating_point_operations_so_far = train( [rank0]:   File ""MegatronLM/megatron/training/training.py"", line 1431, in train [rank0]:     train_step(forward_step_func, [rank0]:   File ""MegatronLM/megatron/training/training.py"", line 775, in train_step [rank0]:     update_successful, grad_norm, num_zeros_in_grad = optimizer.step() [rank0]:   File ""/usr/local/lib/python3.10/distpackages/torch/utils/_contextlib.py"", line 116, in decorate_context [rank0]:     return func(*args, **kwargs) [rank0]:   File ""MegatronLM/megatron/core/optimizer/optimizer.py"", line 473, in step [rank0]:     success = self.step_with_ready_grads() [rank0]:   File ""/usr/local/lib/python3.10/distpackages/torch/utils/_contextlib.py"", line 116, in decorate_context [rank0]:     return func(*args, **kwargs) [rank0]:   File ""MegatronLM/megatron/core/optimizer/optimizer.py"", line 430, in step_with_ready_grads [rank0]:     self.optimizer.step() [rank0]:   File ""/usr/local/lib/python3.10/distpackages/torch/optim/optimizer.py"", line 478, in wrapper [rank0]:     out = func(*args, **kwargs) [rank0]:   File ""/usr/local/lib/python3.10/distpackages/apex/optimizers/fused_adam.py"", line 293, in step [rank0]:     multi_tensor_applier(self.multi_tensor_adam, [rank0]:   File ""/usr/local/lib/python3.10/distpackages/apex/multi_tensor_apply/multi_tensor_apply.py"", line 27, in __call__ [rank0]:     return op(self.chunk_size, [rank0]: RuntimeError: CUDA error: an illegal memory access was encountered [rank0]: Compile with `TORCH_USE_CUDA_DSA` to enable deviceside assertions. ``` **Environment (please complete the following information):**   MegatronLM commit ID: 25a4125   PyTorch version: 2.5.0   CUDA version: 12.6   NCCL version: 2.22.3 **Proposed fix** Vanilla `torch.optim.AdamW` worked for me, so maybe make this the default if fsdp is enabled **Additional context** N/A",2024-12-15T03:02:19Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1322,Marking as stale. No activity in 60 days.
gurpreet-dhami,Fix rope disable,,2024-12-13T21:25:50Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1321
meatybobby,Add Mamba TRTLLM support,Add Mamba TRTLLM support,2024-12-12T18:14:34Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1320,Marking as stale. No activity in 60 days.
lizamd,update network interface env ,only set ifname envs for multinode training,2024-12-12T00:47:49Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1319,Marking as stale. No activity in 60 days.
gurpreet-dhami,Ifu update dec11,,2024-12-11T16:45:40Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1318
is-yy,[QUESTION] I encountered the following issue when executing your command. What could be the cause?  args.exit_on_missing_checkpoint is: True >> '--exit-on-missing-checkpoint' set ... exiting. <<,"> I think you need to add saver and loader. Can you try this once? >  > ` > python tools/checkpoint/convert.py modeltype GPT loader mcore saver mcore megatronpath $megatron_folder loaddir $load_dir savedir $save_dir > ` >  > Also, when you train the model, you can specify `ckptforma` as `torch` to save in torch format.   _Originally posted by  in [ CC([QUESTION] How to convert torch_dist format checkpoint to torch format?)](https://github.com/NVIDIA/MegatronLM/issues/1291issuecomment2505486125)_",2024-12-10T08:34:37Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1317,Can you check if you are using the correct $load_dir which contains the checkpoint. I encountered this error when I was passing a the wrong folder with no checkpoint inside.,Marking as stale. No activity in 60 days.
1195343015,1,,2024-12-09T06:14:08Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1316
fy-j,[QUESTION]Does Megatron support tracing computation graphs with torch.fx?,"I am trying to trace a computation graph in Megatron using torch.fx. However, I encountered the following error: ```shell [rank1]: Traceback (most recent call last): [rank1]:   File ""/kimchou/GeeS***/python/gees***/adapters/pytorch/getTorchGraph.py"", line 58, in getTorchGraph [rank1]:     traced = symbolic_trace(model)  use transformers.utils.fx to trace the model [rank1]:   File ""/usr/local/lib/python3.10/distpackages/torch/fx/_symbolic_trace.py"", line 1193, in symbolic_trace [rank1]:     graph = tracer.trace(root, concrete_args) [rank1]:   File ""/usr/local/lib/python3.10/distpackages/torch/_dynamo/eval_frame.py"", line 437, in _fn [rank1]:     return fn(*args, **kwargs) [rank1]:   File ""/usr/local/lib/python3.10/distpackages/torch/_dynamo/external_utils.py"", line 36, in inner [rank1]:     return fn(*args, **kwargs) [rank1]:   File ""/usr/local/lib/python3.10/distpackages/torch/fx/_symbolic_trace.py"", line 793, in trace [rank1]:     (self.create_arg(fn(*args)),), [rank1]:   File ""/kimchou/MegatronLMtest/megatron/core/models/gpt/gpt_model.py"", line 343, in forward [rank1]:     **(extra_block_kwargs or {}), [rank1]:   File ""/usr/local/lib/python3.10/distpackages/torch/fx/proxy.py"", line 443, in __bool__ [rank1]:     return self.tracer.to_bool(self) [rank1]:   File ""/usr/local/lib/python3.10/distpackages/torch/fx/proxy.py"", line 303, in to_bool [rank1]:     raise TraceError('symbolically traced variables cannot be used as inputs to control flow') ``` How can I use torch.fx to trace the model graph",2024-12-07T12:10:12Z,,open,0,5,https://github.com/NVIDIA/Megatron-LM/issues/1315,Marking as stale. No activity in 60 days.,"Hello, I encountered a similar problem ,May I know your specific problem and how did you solve it?  Thanks!","> Hello, I encountered a similar problem ,May I know your specific problem and how did you solve it? Thanks! Hello, I haven't found a good solution yet. Currently, I am using hooks to register callback functions to log the tensor size of each layer. The granularity is quite coarse, and the effect is average. If you have any good ideas, please feel free to communicate with me. I would be very grateful. By the way, are you also from China? If it's convenient, we could add WeChat for further communication. Best regards.","> > Hello, I encountered a similar problem ,May I know your specific problem and how did you solve it? Thanks! >  > Hello, I haven't found a good solution yet. Currently, I am using hooks to register callback functions to log the tensor size of each layer. The granularity is quite coarse, and the effect is average. If you have any good ideas, please feel free to communicate with me. I would be very grateful. By the way, are you also from China? If it's convenient, we could add WeChat for further communication. Best regards. yes, i am chinese, my weChat id is lpl200296, but i am sorry i don't have a good way yet","> > Hello, I encountered a similar problem ,May I know your specific problem and how did you solve it? Thanks! >  > Hello, I haven't found a good solution yet. Currently, I am using hooks to register callback functions to log the tensor size of each layer. The granularity is quite coarse, and the effect is average. If you have any good ideas, please feel free to communicate with me. I would be very grateful. By the way, are you also from China? If it's convenient, we could add WeChat for further communication. Best regards. æˆ‘ä¹Ÿé‡åˆ°è¿™ä¸ªé—®é¢˜äº† å¸Œæœ›èƒ½å¤Ÿå¾®ä¿¡èŠä¸€èŠ æˆ‘çš„å¾®ä¿¡æ˜¯WarmGun_21"
liveseongho,"[BUG] When using LLaVA with freeze-LM, training text only sample occurs error.","**Describe the bug** When using LLaVA with freezeLM, training text only sample occurs error. **To Reproduce** Base code use VQASample for training LLaVA. But, if you use TextSample or samples with text only, it will occur following error. **Stack trace/logs** ```   File ""/workspace/megatron/core/pipeline_parallel/schedules.py"", line 365, in backward_step     custom_backward(output_tensor[0], output_tensor_grad[0])   File ""/workspace/megatron/core/pipeline_parallel/schedules.py"", line 149, in custom_backward     Variable._execution_engine.run_backward( RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn ``` **Expected behavior** I think `output_tensor[0]` always has grad_fn, but it doesn't have grad_fn if any of sample uses parameters with grad. **Environment (please complete the following information):**   MegatronLM commit ID : 1115e06261278b1e2f8f188ab2926d327707853f **Proposed fix** Fix following code  https://github.com/NVIDIA/MegatronLM/blob/bd677bfb13ac2f19deaa927adc6da6f9201d66aa/megatron/core/pipeline_parallel/schedules.pyL364L367 as follows ```python     if output_tensor[0].requires_grad:         if config.deallocate_pipeline_outputs:             custom_backward(output_tensor[0], output_tensor_grad[0])         else:             torch.autograd.backward(output_tensor[0], grad_tensors=output_tensor_grad[0]) ``` **Additional context**  could you help to fix this issue? ",2024-12-06T06:49:59Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1314,Marking as stale. No activity in 60 days.
renyinCheng001,[QUESTION] How to specify the implementation of Attentionï¼Ÿ,"Hi, All~ There are currently 4 ways to calculate Attention (Ref: https://pytorch.org/docs/stable/generated/torch.nn.attention.SDPBackend.htmltorch.nn.attention.SDPBackend  Math (original Attention implementation, )  FLASH_ATTENTION  EFFICIENT_ATTENTION  CUDNN_ATTENTION At present, I see that the **FlashAttention** calculation method seems to be used by default.  Is it possible to specify other Attention calculation methods, such as **EFFICIENT_ATTENTION**? My environment is as follows: ``` Driver Version : 535.183.01 CUDA Version : 12.4.0rc7+3.ge75c8a9.dirty Python version : 3.10.12 PyTorch version : 2.3.0a0+6ddf5cf85e.nv24.4 ``` Thanks!",2024-12-06T06:31:35Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/1313,"https://github.com/NVIDIA/TransformerEngine/blob/main/docs/examples/attention/attention.ipynb You can refer to this using Transformer Engine, where origin/flash/cudnn attention could be specified with some environment variables.","Megatron doesn't use `torch.nn.attention`. If you're specifying `transformerimpl transformer_engine`, you can set the env vars ``` export NVTE_DEBUG=1 export NVTE_DEBUG_LEVEL=2 ``` to log which attention backend is selected.",Marking as stale. No activity in 60 days.
arul-lm,[QUESTION] Gradient Propagation in backward pass,"Setup: 2D parallel [Tensor + data] Model: Assume a 1 layer GPT style transformer block 1. Does the backward prop start with a big loss gradient allreduce where GPUs are idle? !Image 2. Inside the transformer block, in megatron style tensor parallelism, there are 2 allreduces in the backward prop. During these allreduces, is there a chance of computecommunication overlap? 3. Assume an embedding layer, during the backward prop, is there an alltoall communication to update the embedding weights? I'm trying to model these steps for my project to explain to a larger group with a timing diagram. Hence the above questions. I want to make sure that I accurately present the steps involved Thanks for your time!",2024-12-05T14:52:02Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1312,Marking as stale. No activity in 60 days.
zmtttt,[QUESTION]UnboundLocalErrorï¼šlocal variable â€˜output tensorâ€™ referenced before assignmnet,"I pretrain Llama38Bï¼Œbut met the problem: (1)the configuration : vp2, pp 8, 8gpus,  meeting the erros:             deallocate_output_tensor(output_tensor, config.deallocate_pipeline_outputs)ï¼Œ             UnboundLocalErrorï¼šlocal variable â€˜output tensorâ€™ referenced before assignmnet (2)but when I change the pp from 8 to 4, it can work well. why? have someone met the same problem?",2024-12-05T08:31:34Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/1311,"I meet same problem, but it's not been solved....","Also encountering the same problem with BERT (using 32 layers, with 32 GPUs, 16 PP stages, 2 num layers per virtual pipeline stage)",Marking as stale. No activity in 60 days.
bphwk,"[ENHANCEMENT]When load_ckpt is called and the obtained iteration count equals args.train_iters, the train_step process will be directly skipped. If, at this point, the save_checkpoint function may encounter an error.","**Is your feature request related to a problem? Please describe.** When load_ckpt is called and the obtained iteration count equals args.train_iters, the train_step process will be directly skipped. If, at this point, the condition if args.save and iteration != 0 and iteration % args.save_interval != 0: is entered, the save_checkpoint function may encounter an error due to the absence of optimizerrelated parameters, leading to the failure of the training task. In the case of Torch Elastic mode with asynchronous checkpointing (async ckpt) enabled, this can result in infinite resumption of training. **Describe alternatives you've considered** Exit process when adding iteration==args.train_iters     if iteration == args.train_iters:         sys.exit(0)     while iteration < args.train_iters:         if args.profile and torch.distributed.get_rank() in args.profile_ranks:             if args.use_pytorch_profiler:                 prof.step()             elif iteration == args.profile_step_start:                 torch.cuda.cudart().cudaProfilerStart()",2024-12-05T02:13:05Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1310,Marking as stale. No activity in 60 days.
eliird,[QUESTION],"**Your question** Has anyone been able to get llama 3 70B working using 2x8 80GB GPUs? I have tried all possible settings but keep getting OOM error. If anyone has, then I would like to klnow the arguments and details. It should take around 540GBs to train according to my calculations and I think I have more than enough. Llama3 70B has 80 layers of transformer and the maximum size model I can fit is 70 layers, and sequence length of 16. I tested basically all possible arguments in the settings. I saw one person's comment that suggested they got it working, but I checked their forked repo and the arguments describing the model were off. The settings had 24 layers of transformer blocks instead of 80. ",2024-12-02T00:30:30Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1308,Marking as stale. No activity in 60 days.
dmgcsilva,[BUG] Source for releases 0.8.0 and 0.9.0 are not available,**Describe the bug** The release files for version 0.8.0 and 0.9.0 are not available. **To Reproduce** Go to https://github.com/NVIDIA/MegatronLM/releases Try to download the source for version 0.8.0 or 0.9.0 See 404 page **Expected behavior** Not 404 **Stack trace/logs** NA,2024-11-29T22:15:11Z,,closed,1,2,https://github.com/NVIDIA/Megatron-LM/issues/1307,"Hi, are you referring to the ""Source code (zip)"" and ""Source code (tar.gz)"" links for each release? The full links are: https://github.com/NVIDIA/MegatronLM/archive/refs/tags/core_r0.8.0.zip https://github.com/NVIDIA/MegatronLM/archive/refs/tags/core_r0.8.0.tar.gz https://github.com/NVIDIA/MegatronLM/archive/refs/tags/core_r0.9.0.zip https://github.com/NVIDIA/MegatronLM/archive/refs/tags/core_r0.9.0.tar.gz and as far as I can tell all of them work.","Hi, at the time they were not working. It appears to be fixed now."
1195343015,fix args.mock_data bug caused by func get_blend_and_blend_per_split,This bug caused by commit 7e9ab5c,2024-11-29T06:32:20Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1306,Marking as stale. No activity in 60 days.
1195343015,fix bug caused by func get_blend_and_blend_per_split(),args.mock_data was not taken into account. This bug caused by commit 7e9ab5c,2024-11-28T13:31:10Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1305
Baibaifan,[BUG] The problem of splitting transformer layers when pipeline parallelism cannot be evenly divided.,"**Describe the bug**  Situation: **GPT2 Models**  `numlayers`=30, `pipelinemodelparallelsize`=4  Don't use `decoderfirstpipelinenumlayers` and `decoderlastpipelinenumlayers`  Segmentation results  stage1: 0,1,2,3,4,5,6 stage2: 7,8,9,10,11,12,13 stage3: 14,15,16,17,18,19,20 stage4: 21,22,23,24,25,26,27 sum layers: 28 layers not equal to 30 layers. In the legacy version, there is a judgment on the number of model layers. !Image In the Mcore version, only `numlayerspervirtualpipelinestage` can be used to determine the number of model layers. !Image I think if users are required to split the model layer themselves due to imbalance, judgment and necessary warnings should be added here. **Environment (please complete the following information):**   MegatronLM commit IDï¼šMain branch",2024-11-27T12:32:53Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1304,Marking as stale. No activity in 60 days.
renyinCheng001,[QUESTION] How to split the Transform layer when the pipeline is uneven?,"Hi, All~ I am using **pipeline model parallel** to train a GPT313B model on **16 GPUs**, which has **40 Transform layers**. Obviously, the number of model layers cannot divide pipelinemodelparallelsize, resulting in only 2 Transform layers on each GPU. I noticed that the number of layers on the first and last pipeline stages can be configured by `decoderfirstpipelinenumlayers` and `decoderlastpipelinenumlayers`, as shown in the following code: ``` def get_num_layers_to_build(config: TransformerConfig) > int:     """"""     Determine the number of transformer layers to build for the current pipeline stage.     Args:         config (TransformerConfig): Configuration object containing transformer model parameters.     Returns:         int: The number of layers to be built for the current pipeline stage.     """"""     if config.first_pipeline_num_layers is not None or config.last_pipeline_num_layers is not None:         assert (             parallel_state.get_virtual_pipeline_model_parallel_world_size() is None         ), ""Uneven number of layer not compatible with interleaved pipeline schedule""          Number of layers to distribute over rest of pipeline stages         layers_to_distribute = config.num_layers          Number of pipeline stages left for distributing transformer layers         pipeline_stages_left = parallel_state.get_pipeline_model_parallel_world_size()         if config.first_pipeline_num_layers is not None:             layers_to_distribute = config.first_pipeline_num_layers             pipeline_stages_left = 1             if parallel_state.is_pipeline_first_stage():                 return config.first_pipeline_num_layers         if config.last_pipeline_num_layers is not None:             layers_to_distribute = config.last_pipeline_num_layers             pipeline_stages_left = 1             if parallel_state.is_pipeline_last_stage():                 return config.last_pipeline_num_layers         assert (             layers_to_distribute % pipeline_stages_left == 0         ), ""With uneven pipelineing the left over layers must be divisible by left over stages""         num_layers_per_pipeline_rank = layers_to_distribute // pipeline_stages_left     else:         pipeline_ranks = config.pipeline_model_parallel_size         num_layers_per_pipeline_rank = config.num_layers // pipeline_ranks    ......     return num_layers_to_build ``` I configured these two parameters as follows  ``` decoderfirstpipelinenumlayers 6  decoderlastpipelinenumlayers 6 ``` The number of model layers on different GPUs is as follows: ``` GPU0: 6 layers GPU1~14: 2 layers GPU15: 6 layers ``` However, this results in the first and last GPUs having much more computational effort than the rest of the GPUs, further amplifying the problem of unbalanced loads on different GPUs, thus increasing the `bubble` of PP training. When the number of model layers cannot divide the PP size, is there a more even way to divide the model?",2024-11-27T05:47:33Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1303,+1,Marking as stale. No activity in 60 days.
mxymxy77,[QUESTION] Why is the initialization of the router and experts different in the MoE part?,"**Why is the initialization of the router and experts different in the MoE part?**  The weight parameters of the router are initialized in FP32, while the expert weights are initialized in param_dtype (assuming mixed precision training, the expert weights are initialized in BF16).  Reference link: 1. router init:https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/transformer/moe/router.pyL43 2. experts init:https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/transformer/moe/experts.pyL145",2024-11-27T01:31:16Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1302,same question,Marking as stale. No activity in 60 days.
hgdhrt,[BUG] an illegal memory access was encountered in MOE-MLP(GroupGemm),"**Describe the bug** For MOE GPT, the part of MLP(GroupGemm)ï¼Œ  ``` RuntimeError: CUDA error: an illegal memory access was encountered ``` The occurrence of randomness, even if the same data input, sometimes will report an error, sometimes will not report an error **To Reproduce** The occurrence of randomness, even if the same data input, sometimes will report an error, sometimes will not report an error But there's always a problem at a certain point **Expected behavior** Find the problem and fix it **Stack trace/logs** Bug: ```      losses_reduced = forward_backward_func(    File ""/megatron/core/pipeline_parallel/schedules.py"", line 1384, in forward_backward_pipelining_without_interleaving      output_tensor, num_tokens = forward_step(    File ""/megatron/core/pipeline_parallel/schedules.py"", line 219, in forward_step      output_tensor, loss_func = forward_step_func(data_iterator, model)    File ""/pretrain_yuanvl.py"", line 292, in forward_step      output_tensor = model(tokens, position_ids, attention_mask,    File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1532, in _wrapped_call_impl      return self._call_impl(*args, **kwargs)    File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1541, in _call_impl      return forward_call(*args, **kwargs)    File ""/megatron/core/distributed/distributed_data_parallel.py"", line 204, in forward      return self.module(*inputs, **kwargs)    File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1532, in _wrapped_call_impl      return self._call_impl(*args, **kwargs)    File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1541, in _call_impl      return forward_call(*args, **kwargs)    File ""/megatron/legacy/model/module.py"", line 189, in forward      outputs = self.module(*inputs, **kwargs)    File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1532, in _wrapped_call_impl      return self._call_impl(*args, **kwargs)    File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1541, in _call_impl      return forward_call(*args, **kwargs)    File ""/megatron/core/models/gpt/gpt_model.py"", line 314, in forward      hidden_states = self.decoder(    File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1532, in _wrapped_call_impl      return self._call_impl(*args, **kwargs)    File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1541, in _call_impl      return forward_call(*args, **kwargs)    File ""/megatron/core/transformer/transformer_block.py"", line 428, in forward      hidden_states = self._checkpointed_forward(    File ""/megatron/core/transformer/transformer_block.py"", line 316, in _checkpointed_forward      hidden_states, context = checkpoint_handler(    File ""/megatron/core/transformer/transformer_block.py"", line 299, in checkpoint_handler      return tensor_parallel.checkpoint(    File ""/megatron/core/tensor_parallel/random.py"", line 301, in checkpoint      return CheckpointFunction.apply(function, distribute_saved_activations, *args)    File ""/usr/local/lib/python3.10/distpackages/torch/autograd/function.py"", line 569, in apply      return super().apply(*args, **kwargs)   type: ignore[misc]    File ""/megatron/core/tensor_parallel/random.py"", line 240, in forward      outputs = run_function(*args)    File ""/megatron/core/transformer/transformer_block.py"", line 270, in custom_forward      hidden_states, context = layer(    File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1532, in _wrapped_call_impl      return self._call_impl(*args, **kwargs)    File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1541, in _call_impl      return forward_call(*args, **kwargs)    File ""/megatron/core/transformer/transformer_layer.py"", line 259, in forward      mlp_output_with_bias = self.mlp(pre_mlp_layernorm_output)    File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1532, in _wrapped_call_impl      return self._call_impl(*args, **kwargs)    File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1541, in _call_impl      return forward_call(*args, **kwargs)    File ""/megatron/core/transformer/moe/moe_layer.py"", line 153, in forward      output, mlp_bias = custom_forward(hidden_states)    File ""/megatron/core/transformer/moe/moe_layer.py"", line 143, in custom_forward      (dispatched_input, tokens_per_expert) = self.token_dispatcher.token_permutation(    File ""/megatron/core/transformer/moe/token_dispatcher.py"", line 471, in token_permutation      tokens_per_expert = self.preprocess(indices)    File ""/megatron/core/transformer/moe/token_dispatcher.py"", line 440, in preprocess      self.global_input_tokens_local_experts_indices = torch.repeat_interleave(  RuntimeError: CUDA error: an illegal memory access was encountered  CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.  For debugging consider passing CUDA_LAUNCH_BLOCKING=1.  Compile with `TORCH_USE_CUDA_DSA` to enable deviceside assertions. terminate called after throwing an instance of 'c10::Error'   what():  CUDA error: an illegal memory access was encountered CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect. For debugging consider passing CUDA_LAUNCH_BLOCKING=1. Compile with `TORCH_USE_CUDA_DSA` to enable deviceside assertions. ``` **Environment (please complete the following information):**   MegatronLM commit ID   PyTorch version:2.4.0   CUDA version:12.4   NCCL version:2.21.51 **Proposed fix** No **Additional context** No",2024-11-26T09:52:21Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1301,Marking as stale. No activity in 60 days.
KookHoiKim,[QUESTION] recompute activation rather consume more memory while backward (OOM),"**Your question** I am struggling with OOM issue and I found that recompute activations (activation checkpointing) is a great option for me.  My case is that additional embedding module passes large batch inputs ( differ with args.micro batch) , making huge activation memory.  Recompute activation seems work well while forwarding but OOM is still occurred processing backward.  When TP=2, PP=2 without recomputing, the model training works well. (first stage 79G, second stage 71G) However, when i add recompute options like below, OOM suddenly occurs while backwarding.  ```      recomputegranularity full \      recomputemethod uniform \      recomputenumlayers 1 \ ``` Does anyone have any idea about memory consumption of recomputation?  Thanks. ",2024-11-26T08:33:05Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1300,Did you find an answer? It should reduce the memory usage but is not reducing memory at all.,https://arxiv.org/pdf/2205.05198 This is the original paper and according to it memory usage should reduce by a lot but seems to have no effect
KookHoiKim,[BUG] Segmentation fault: address not mapped to object at address (nil) while use recompute granularity option,"**Describe the bug** When i set recompute option as below, segmentation fault is occurred while running my custom llava code. Error message : `Caught signal 11 (Segmentation fault: address not mapped to object at address (nil))` Current parallelism is TP=2, PP=2 and if i don't set this option, no error occurred.  ```     recomputegranularity full \     recomputemethod uniform \     recomputenumlayers 1 \ ``` My image is `nvcr.io/nvidia/pytorch:24.07py3`, TE version is `release_v1.12` ,and  megatron 0.9.0 .  Below is nccl_debug info content, i cannot see related error message btw. ``` run8021gpu7:377687:382305 [0] NCCL INFO Trees [0] 1/1/1>0>1 [1] 1/1/1>0>1 [2] 1/1/1>0>1 [3] 1/1/1>0>1 [4] 1/1/1>0>1 [5] 1/1/1>0>1 [6] 1/1/1>0>1 [7] 1/1/1>0>1 [8] 1/1/1>0>1 [9] 1/1/1>0>1 [10] 1/1/1>0>1 [11] 1/1/1>0>1 [12] 1/1/1>0>1 [13] 1/1/1>0>1 [14] 1/1/1>0>1 [15] 1/1/1>0>1 [16] 1/1/1>0>1 [17] 1/1/1>0>1 [18] 1/1/1>0>1 [19] 1/1/1>0>1 [20] 1/1/1>0>1 [21] 1/1/1>0>1 [22] 1/1/1>0>1 [23] 1/1/1>0>1 [24] 1/1/1>0>1 [25] 1/1/1>0>1 [26] 1/1/1>0>1 [27] 1/1/1>0>1 [28] 1/1/1>0>1 [29] 1/1/1>0>1 [30] 1/1/1>0>1 [31] 1/1/1>0>1 run8021gpu7:377687:382305 [0] NCCL INFO P2P Chunksize set to 131072 run8021gpu7:377688:382306 [1] NCCL INFO Setting affinity for GPU 4 to aaaaaaaa,aaaaaaaa,aaaaaaaa,aaaaaaaa run8021gpu7:377688:382306 [1] NCCL INFO comm 0x5558362f71d0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0 run8021gpu7:377688:382306 [1] NCCL INFO Channel 00/32 :    0 run8021gpu7:377688:382306 [1] NCCL INFO Channel 01/32 :    0 run8021gpu7:377688:382306 [1] NCCL INFO Channel 02/32 :    0 run8021gpu7:377688:382306 [1] NCCL INFO Channel 03/32 :    0 run8021gpu7:377688:382306 [1] NCCL INFO Channel 04/32 :    0 run8021gpu7:377688:382306 [1] NCCL INFO Channel 05/32 :    0 run8021gpu7:377688:382306 [1] NCCL INFO Channel 06/32 :    0 run8021gpu7:377688:382306 [1] NCCL INFO Channel 07/32 :    0 run8021gpu7:377688:382306 [1] NCCL INFO Channel 08/32 :    0 run8021gpu7:377688:382306 [1] NCCL INFO Channel 09/32 :    0 run8021gpu7:377688:382306 [1] NCCL INFO Channel 10/32 :    0 run8021gpu7:377688:382306 [1] NCCL INFO Channel 11/32 :    0 run8021gpu7:377688:382306 [1] NCCL INFO Channel 12/32 :    0 run8021gpu7:377688:382306 [1] NCCL INFO Channel 13/32 :    0 run8021gpu7:377688:382306 [1] NCCL INFO Channel 14/32 :    0 run8021gpu7:377688:382306 [1] NCCL INFO Channel 15/32 :    0 run8021gpu7:377688:382306 [1] NCCL INFO Channel 16/32 :    0 run8021gpu7:377688:382306 [1] NCCL INFO Channel 17/32 :    0 run8021gpu7:377688:382306 [1] NCCL INFO Channel 18/32 :    0 run8021gpu7:377688:382306 [1] NCCL INFO Channel 19/32 :    0 run8021gpu7:377688:382306 [1] NCCL INFO Channel 20/32 :    0 run8021gpu7:377688:382306 [1] NCCL INFO Channel 21/32 :    0 run8021gpu7:377688:382306 [1] NCCL INFO Channel 22/32 :    0 run8021gpu7:377688:382306 [1] NCCL INFO Channel 23/32 :    0 run8021gpu7:377688:382306 [1] NCCL INFO Channel 24/32 :    0 run8021gpu7:377688:382306 [1] NCCL INFO Channel 25/32 :    0 run8021gpu7:377688:382306 [1] NCCL INFO Channel 26/32 :    0 run8021gpu7:377688:382306 [1] NCCL INFO Channel 27/32 :    0 run8021gpu7:377688:382306 [1] NCCL INFO Channel 28/32 :    0 run8021gpu7:377688:382306 [1] NCCL INFO Channel 29/32 :    0 run8021gpu7:377688:382306 [1] NCCL INFO Channel 30/32 :    0 run8021gpu7:377688:382306 [1] NCCL INFO Channel 31/32 :    0 run8021gpu7:377688:382306 [1] NCCL INFO Trees [0] 1/1/1>0>1 [1] 1/1/1>0>1 [2] 1/1/1>0>1 [3] 1/1/1>0>1 [4] 1/1/1>0>1 [5] 1/1/1>0>1 [6] 1/1/1>0>1 [7] 1/1/1>0>1 [8] 1/1/1>0>1 [9] 1/1/1>0>1 [10] 1/1/1>0>1 [11] 1/1/1>0>1 [12] 1/1/1>0>1 [13] 1/1/1>0>1 [14] 1/1/1>0>1 [15] 1/1/1>0>1 [16] 1/1/1>0>1 [17] 1/1/1>0>1 [18] 1/1/1>0>1 [19] 1/1/1>0>1 [20] 1/1/1>0>1 [21] 1/1/1>0>1 [22] 1/1/1>0>1 [23] 1/1/1>0>1 [24] 1/1/1>0>1 [25] 1/1/1>0>1 [26] 1/1/1>0>1 [27] 1/1/1>0>1 [28] 1/1/1>0>1 [29] 1/1/1>0>1 [30] 1/1/1>0>1 [31] 1/1/1>0>1 run8021gpu7:377688:382306 [1] NCCL INFO P2P Chunksize set to 131072 run8021gpu7:377687:382305 [0] NCCL INFO Connected all rings run8021gpu7:377687:382305 [0] NCCL INFO Connected all trees run8021gpu7:377687:382305 [0] NCCL INFO 32 coll channels, 0 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer run8021gpu7:377688:382306 [1] NCCL INFO Connected all rings run8021gpu7:377688:382306 [1] NCCL INFO Connected all trees run8021gpu7:377688:382306 [1] NCCL INFO 32 coll channels, 0 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer run8021gpu7:377687:382305 [0] NCCL INFO comm 0x55783c875b50 rank 0 nranks 1 cudaDev 0 nvmlDev 3 busId 66000 commId 0xae5a4e82d214599c  Init COMPLETE run8021gpu7:377688:382306 [1] NCCL INFO comm 0x5558362f71d0 rank 0 nranks 1 cudaDev 1 nvmlDev 4 busId 9c000 commId 0x5f66eb06e914abbf  Init COMPLETE [run8021gpu7:377690:0:377690] Caught signal 11 (Segmentation fault: address not mapped to object at address (nil)) ```",2024-11-25T22:50:49Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1299
XLzed,[BUG] The cached_loss_mask cannot be consistent,"**How to make training resumption bitwise reproducible?** When setting the environment variables **NVTE_ALLOW_NONDETERMINISTIC_ALGO=0, NCCL_ALGO=Ring**, training can be bitwise reproducible by loading same checkpoint.  Training resume from an intermediate checkpoint, the loss curve can be nearly same as complete training from original checkpoint, but the precision of the loss can no longer be aligned in bitwise. What might be causing the problem and how to make training resumption bitwise reproducible?",2024-11-24T09:26:12Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/1298,"While masks_and_position_ids_are_cacheable is True, the cached_loss_mask should be the same value all the time. But the loss_mask and cached_loss_mask reference each other, which makes the loss_mask incorrect after first iteration, torch.sum(loss_mask) will be monotone decreasing. This is also why the loss curve of training resumption can not be bitwisealigned with the complete training. The bug seems introduced from core0.7.0, which zero loss_mask by _pad_token_id. **The original code with bug.** ```python         if (             not self.masks_and_position_ids_are_cacheable             or not self.masks_and_position_ids_are_cached         ):             attention_mask, loss_mask, position_ids = _get_ltor_masks_and_position_ids(                 tokens,                 self.config.tokenizer.eod,                 self.config.reset_position_ids,                 self.config.reset_attention_mask,                 self.config.eod_mask_loss,                 self.config.create_attention_mask,             )             if self.masks_and_position_ids_are_cacheable:                 self.cached_attention_mask = attention_mask                 self.cached_loss_mask = loss_mask                 self.cached_position_ids = position_ids                 self.masks_and_position_ids_are_cached = True         else:             attention_mask = self.cached_attention_mask             loss_mask = self.cached_loss_mask             position_ids = self.cached_position_ids          For padded sequences, mask the loss         loss_mask[labels == self._pad_token_id] = 0.0 ``` **Bug fixed, use torch.clone() to avoid modification of cached_loss_mask.** ```python         if (             not self.masks_and_position_ids_are_cacheable             or not self.masks_and_position_ids_are_cached         ):             attention_mask, loss_mask, position_ids = _get_ltor_masks_and_position_ids(                 tokens,                 self.config.tokenizer.eod,                 self.config.reset_position_ids,                 self.config.reset_attention_mask,                 self.config.eod_mask_loss,                 self.config.create_attention_mask,             )             if self.masks_and_position_ids_are_cacheable:                 self.cached_attention_mask = attention_mask                 self.cached_loss_mask = torch.clone(loss_mask)                 self.cached_position_ids = position_ids                 self.masks_and_position_ids_are_cached = True         else:             attention_mask = self.cached_attention_mask             loss_mask = torch.clone(self.cached_loss_mask)             position_ids = self.cached_position_ids          For padded sequences, mask the loss         loss_mask[labels == self._pad_token_id] = 0.0 ```","My custom tokenizer is used incorrectly, the pad_id should not be set. Close this comment."," hi, same question here, but there are indeed tokenizers contain a pad_id. Is your solution (using default 1 instead of pad_id to pad sequences) supposed to be a temporary approach to bypass this problem? Thx"
pierric,[BUG] validate_yaml() isn't in sync with arguments check,"**Describe the bug** This check isn't in sync with the one in the arguments.py **To Reproduce**  in yaml config, `pipeline_model_parallel_size = 2`, and `num_layers_per_virtual_pipeline_stage = 2`  pass `yamlcfg` **Expected behavior** The setting `pipeline_model_parallel_size = 2` should be accepted. **Proposed fix** sync up the check in the two places.",2024-11-21T11:00:55Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/1297,"if you use yaml_cfg, args will only be parsed with yaml.","I know. but I mean `pipelinemodelparallelsize=2 numlayerspervirtualpipelinestage=2` is accepted, while on the other hand being rejected as in a yaml and with a `yamlcfg`. It's inconsistent.",Marking as stale. No activity in 60 days.
shijungg,[Update] Print training log in rank0,"Now the training log is printed in **rank_last**, it's not friendly to users if they change the amount of GPU node. For example, a multinode training job has **master0, worker0, worker1**... If we use 32 node(8 gpus each), we need to find the training log in **worker30**. Next time we use 64 node(8 gpus each), we need to find the training log in **worker62**. So print training log in **rank0** is more friendly, we can just find the training log in master0, no mater how many nodes we use for training. Thanks.  barker ",2024-11-21T09:25:00Z,,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1296,"The log needs to be printed on the last rank because when using pipeline parallelism the loss is only available on the last pipeline rank. In our own runs we gather the output from all ranks into a single file, maybe something like that would work for you?","> The log needs to be printed on the last rank because when using pipeline parallelism the loss is only available on the last pipeline rank. In our own runs we gather the output from all ranks into a single file, maybe something like that would work for you? Got it. Thanks for your advice, we will consider gathering log from all ranks into a file"
wavy-jung,[QUESTION] deepseek v2 compatility?,"Assuming there is no first_k_dense_replace, is the current code fully compatible with `deepseekv2`? (moe + multilatent attention)",2024-11-21T06:59:26Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1295,Marking as stale. No activity in 60 days.
lostkevin,[BUG] LLaVA may fail with EPP0 PP>1,"**Describe the bug** In the implementation of LLaVA model, I find some confusing code as follows: https://github.com/NVIDIA/MegatronLM/blob/b6866aed34d28fc37e352f3bae53fa48ad150952/megatron/core/models/multimodal/llava_model.pyL226L242 Here, L234 is just duplicated with L236. When I try to explain this code, I find that when EPP=0, add_encoder and add_decoder will be always True. Therefore, the generated embeddings will be attached to vision model, which seems wrong when PP > 1 (should attached to next PP Stage like LLM?).  **Expected behavior** I guess the generated embeddings should be assigned to language model in some cases but not sure. **Environment (please complete the following information):**   MegatronLM b6866aed34d28fc37e352f3bae53fa48ad150952",2024-11-20T09:04:41Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1293
SeunghyunSEO,[BUG] 0.9.0 release version got param_gather_handle error with 3d parallel,"```python Â 5: [rank5]: Â  File ""/workspace/megatron/core/transformer/transformer_block.py"", line 493, in forward Â 5: [rank5]: Â  Â  hidden_states, context = layer( Â 5: [rank5]: Â  File ""/workspace/megatron/core/transformer/transformer_layer.py"", line 426, in __call__ Â 5: [rank5]: Â  Â  return super(MegatronModule, self).__call__(*args, **kwargs) Â 5: [rank5]: Â  File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl Â 5: [rank5]: Â  Â  return self._call_impl(*args, **kwargs) Â 5: [rank5]: Â  File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1788, in _call_impl Â 5: [rank5]: Â  Â  result = forward_call(*args, **kwargs) Â 5: [rank5]: Â  File ""/workspace/megatron/core/transformer/transformer_layer.py"", line 369, in forward Â 5: [rank5]: Â  Â  mlp_output_with_bias = self.mlp(pre_mlp_layernorm_output) Â 5: [rank5]: Â  File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl Â 5: [rank5]: Â  Â  return self._call_impl(*args, **kwargs) Â 5: [rank5]: Â  File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1788, in _call_impl Â 5: [rank5]: Â  Â  result = forward_call(*args, **kwargs) Â 5: [rank5]: Â  File ""/workspace/megatron/core/transformer/mlp.py"", line 132, in forward Â 5: [rank5]: Â  Â  output, output_bias = self.linear_fc2(intermediate_parallel) Â 5: [rank5]: Â  File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl Â 5: [rank5]: Â  Â  return self._call_impl(*args, **kwargs) Â 5: [rank5]: Â  File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1777, in _call_impl Â 5: [rank5]: Â  Â  args_result = hook(self, args) Â 5: [rank5]: Â  File ""/workspace/megatron/core/distributed/distributed_data_parallel.py"", line 334, in hook Â 5: [rank5]: Â  Â  self.param_to_bucket_group[param].finish_param_sync( Â 5: [rank5]: Â  File ""/workspace/megatron/core/distributed/param_and_grad_buffer.py"", line 221, in finish_param_sync Â 5: [rank5]: Â  Â  self.next_param_gather_bucket_group.start_param_sync() Â 5: [rank5]: Â  File ""/workspace/megatron/core/distributed/param_and_grad_buffer.py"", line 167, in start_param_sync Â 5: [rank5]: Â  Â  assert self.param_gather_handle is None Â 5: [rank5]: AssertionError ``` it's 4 node experiment where i used distributed_optimizer, overlap param gather and grad all reduce as True and tp=2, pp=4. idk why next linear fc2's next_param_gather_bucket_group has asnyc param_gather context manager ...? ",2024-11-19T08:38:50Z,stale,open,0,8,https://github.com/NVIDIA/Megatron-LM/issues/1292,i found it's because chain bucketing order is not matched with forward path,Can you share a small reproduction script?,"> Can you share a small reproduction script?  hi deepak, good to see you! i cant share the simple reproduction script because error occurs in my custom model. but i can say that, i tested to add rmsnorm here and there in the residual block like this nvidia's paper for training stability. for example there can be rmsnorm right after the value layer, self attn output and fc2 output !Image but in this scenario, i added custom layers in the end of the transformer layer block (after this line) ```python class TransformerLayer(MegatronModule, BaseTransformerLayer):     def __init__(         self,         config: TransformerConfig,         submodules: TransformerLayerSubmodules,         layer_number: int = 1,         hidden_dropout: float = None,     ): ...          [Module 8: MLP block]          TODO how to set the gpt_layer_spec.py when we have moe_frequency > 1,               where MLP and MoE layer both appear alternately?         self.mlp = build_module(submodules.mlp, config=self.config)         if hasattr(self.mlp, 'set_layer_number'):             self.mlp.set_layer_number(self.layer_number)          [Module 9: BiasDropoutFusion]         self.mlp_bda = build_module(submodules.mlp_bda)           how should we handle nvfuser?          Set bias+dropout+add fusion grad_enable execution handler.          TORCH_MAJOR = int(torch.__version__.split('.')[0])          TORCH_MINOR = int(torch.__version__.split('.')[1])          use_nvfuser = TORCH_MAJOR > 1 or (TORCH_MAJOR == 1 and TORCH_MINOR >= 10)          self.bias_dropout_add_exec_handler = nullcontext if use_nvfuser else torch.enable_grad         self.bias_dropout_add_exec_handler = torch.enable_grad          here, custom layers are added in the end of init          self.attn_out_rmsnorm = ...         self.fc2_rmsnorm = ... ```  but this layers does not forwarded in this order, the model forward logic will be `self_attn_prenorm > self_attn > attn_out_rmsnorm > bda > mlp_prnorm > fc1 > fc2 > fc2_rmsnorm > bda`. however, the bucketing order will be `self_attn_prenorm > self_attn > mlp_prnorm > fc1 > fc2 > attn_out_rmsnorm > fc2_rmsnorm` (this line). so if finish_param_sync is activated, for example 0th layer's first bucket start to sync (using `self.start_param_sync`) (`16th bucket`) and it will call `next_param_gather_bucket_group.start_param_sync` too (`15 bucket`). but because bucket order is not matched with forward path, the attn_out_rmsnorm bucket is `14th bucket` not `15th bucket` and it call next param sync too, so `13 bucket` will be gathered  asynchronously too. however, fc1 layer will be `15 bucket` but because this bucket's `self.param_gather_dispatched` is `True`, its' fine but it's `next_param_gather_bucket_group` will be `14th bucket` which is allgathered already, however as you can see below,  there is no logic for checking `next_param_gather_bucket_group` is gathered or not (` param_gather_dispatched`) ```python          If current bucket's param AG has not been dispatched, dispatch it now (e.g., first          AG bucket in first model chunk if ddp_config.align_param_gather is False).         if not self.param_gather_dispatched:             self.start_param_sync()         if self.param_gather_handle is not None:             self.param_gather_handle.wait()             self.param_gather_handle = None              Dispatch next bucket's asynchronous param AG.             if self.next_param_gather_bucket_group is not None and not skip_next_bucket_dispatch:                 self.next_param_gather_bucket_group.start_param_sync() ``` so i fixed above code snippet like this ```python         if self.param_gather_handle is not None:             self.param_gather_handle.wait()             self.param_gather_handle = None              Dispatch next bucket's asynchronous param AG.             if (                 self.next_param_gather_bucket_group is not None and not skip_next_bucket_dispatch                 ) and (                     not self.next_param_gather_bucket_group.param_gather_dispatched                 ):                 self.next_param_gather_bucket_group.start_param_sync() ``` if my explanation lacks information, please reply again or email me, ty! (and if you like my solution, i can PR)","Thank you for providing this patch! Iâ€™ve tested it, and it indeed allows training to proceed. However, Iâ€™ve observed an issue with checkpointing: after saving a checkpoint, the loss immediately diverges. This suggests that the checkpointing logic is also affected by the mismatch in parameter declaration and usage order. As a temporary workaround, Iâ€™ve adjusted the parameter declaration order to align with the forward pass usage order. For reference, Iâ€™ve attached the loss curve from my experiment. At around step 200, you can see that an asynchronous checkpointing issue caused the loss to spike and diverge. !Image Let me know if thereâ€™s a more robust solution in progress or if additional details from my setup would help with debugging. > > Can you share a small reproduction script? >  > [](https://github.com/deepakn94) hi deepak, good to see you! i cant share the simple reproduction script because error occurs in my custom model. but i can say that, i tested to add rmsnorm here and there in the residual block like this nvidia's paper for training stability. for example there can be rmsnorm right after the value layer, self attn output and fc2 output >  > !Image >  > but in this scenario, i added custom layers in the end of the transformer layer block (after this line) >  > class TransformerLayer(MegatronModule, BaseTransformerLayer): >  >     def __init__( >         self, >         config: TransformerConfig, >         submodules: TransformerLayerSubmodules, >         layer_number: int = 1, >         hidden_dropout: float = None, >     ): > ... >          [Module 8: MLP block] >          TODO how to set the gpt_layer_spec.py when we have moe_frequency > 1, >               where MLP and MoE layer both appear alternately? >         self.mlp = build_module(submodules.mlp, config=self.config) >         if hasattr(self.mlp, 'set_layer_number'): >             self.mlp.set_layer_number(self.layer_number) >  >          [Module 9: BiasDropoutFusion] >         self.mlp_bda = build_module(submodules.mlp_bda) >  >           how should we handle nvfuser? >          Set bias+dropout+add fusion grad_enable execution handler. >          TORCH_MAJOR = int(torch.__version__.split('.')[0]) >          TORCH_MINOR = int(torch.__version__.split('.')[1]) >          use_nvfuser = TORCH_MAJOR > 1 or (TORCH_MAJOR == 1 and TORCH_MINOR >= 10) >          self.bias_dropout_add_exec_handler = nullcontext if use_nvfuser else torch.enable_grad >         self.bias_dropout_add_exec_handler = torch.enable_grad >  >          here, custom layers are added in the end of init  >         self.attn_out_rmsnorm = ... >         self.fc2_rmsnorm = ... > but this layers does not forwarded in this order, the model forward logic will be `self_attn_prenorm > self_attn > attn_out_rmsnorm > bda > mlp_prnorm > fc1 > fc2 > fc2_rmsnorm > bda`. however, the bucketing order will be `self_attn_prenorm > self_attn > mlp_prnorm > fc1 > fc2 > attn_out_rmsnorm > fc2_rmsnorm` (this line). so if finish_param_sync is activated, for example 0th layer's first bucket start to sync (using `self.start_param_sync`) (`16th bucket`) and it will call `next_param_gather_bucket_group.start_param_sync` too (`15 bucket`). but because bucket order is not matched with forward path, the attn_out_rmsnorm bucket is `14th bucket` not `15th bucket` and it call next param sync too, so `13 bucket` will be gathered asynchronously too. however, fc1 layer will be `15 bucket` but because this bucket's `self.param_gather_dispatched` is `True`, its' fine but it's `next_param_gather_bucket_group` will be `14th bucket` which is allgathered already, however as you can see below, there is no logic for checking `next_param_gather_bucket_group` is gathered or not (` param_gather_dispatched`) >  >          If current bucket's param AG has not been dispatched, dispatch it now (e.g., first >          AG bucket in first model chunk if ddp_config.align_param_gather is False). >         if not self.param_gather_dispatched: >             self.start_param_sync() >  >         if self.param_gather_handle is not None: >             self.param_gather_handle.wait() >             self.param_gather_handle = None >              Dispatch next bucket's asynchronous param AG. >             if self.next_param_gather_bucket_group is not None and not skip_next_bucket_dispatch: >                 self.next_param_gather_bucket_group.start_param_sync() > so i fixed above code snippet like this >  >         if self.param_gather_handle is not None: >             self.param_gather_handle.wait() >             self.param_gather_handle = None >              Dispatch next bucket's asynchronous param AG. >             if ( >                 self.next_param_gather_bucket_group is not None and not skip_next_bucket_dispatch >                 ) and ( >                     not self.next_param_gather_bucket_group.param_gather_dispatched >                 ): >                 self.next_param_gather_bucket_group.start_param_sync() > if my explanation lacks information, please reply again or email me, ty! (and if you like my solution, i can PR)"," thank you for the test! tbh, i didnt check checkpoint loading. ill test this too. thank you so much :)", may i ask your opinion sir,"> [](https://github.com/fanzhongyi) thank you for the test! tbh, i didnt check checkpoint loading. ill test this too. thank you so much :) Any updates ? ",Marking as stale. No activity in 60 days.
zhangyilalala,[QUESTION] How to convert torch_dist format checkpoint to torch format?,"I am trying to convert the checkpoint obtained from asynchronous saving of torch_dist to the original torch format, but using convert.py directly results in an error. Could there be an issue with my usage? `python tools/checkpoint/convert.py modeltype GPT loaddir /mnt/selfdefine/output/outputLlama3_18Bpretrain/checkpoint/pretrainmcorellama318Blr3e5minlr3e6bs1gbs1024seqlen8192prbf16tp2pp2cp1acnonedotruespfalsecp1ts140000 savedir /mnt/selfdefine/zhangyi/temp/output/outputLlama3_18Bpretrain/checkpoint` The error: ``` Traceback (most recent call last):   File ""/mnt/selfdefine/zhangyi/home/zy/zjlabmegatron/MegatronLMcore_r0.9.0/tools/checkpoint/convert.py"", line 158, in  Loader exited, exiting saver     main()   File ""/mnt/selfdefine/zhangyi/home/zy/zjlabmegatron/MegatronLMcore_r0.9.0/tools/checkpoint/convert.py"", line 151, in main     loader.load_checkpoint(queue, args)   File ""/mnt/selfdefine/zhangyi/home/zy/zjlabmegatron/MegatronLMcore_r0.9.0/tools/checkpoint/loader_mcore.py"", line 381, in load_checkpoint     _load_checkpoint(queue, args)   File ""/mnt/selfdefine/zhangyi/home/zy/zjlabmegatron/MegatronLMcore_r0.9.0/tools/checkpoint/loader_mcore.py"", line 243, in _load_checkpoint     all_models = [get_models(tp_size, md.params_dtype)]   File ""/mnt/selfdefine/zhangyi/home/zy/zjlabmegatron/MegatronLMcore_r0.9.0/tools/checkpoint/loader_mcore.py"", line 161, in get_models     load_checkpoint(model_, None, None)   File ""/mnt/selfdefine/zhangyi/home/zy/zjlabmegatron/MegatronLMcore_r0.9.0/megatron/training/checkpointing.py"", line 1090, in load_checkpoint     state_dict, checkpoint_name, release, ckpt_type = _load_base_checkpoint(   File ""/mnt/selfdefine/zhangyi/home/zy/zjlabmegatron/MegatronLMcore_r0.9.0/megatron/training/checkpointing.py"", line 851, in _load_base_checkpoint     return _load_global_dist_base_checkpoint(   File ""/mnt/selfdefine/zhangyi/home/zy/zjlabmegatron/MegatronLMcore_r0.9.0/megatron/training/checkpointing.py"", line 779, in _load_global_dist_base_checkpoint     state_dict = dist_checkpointing.load(sharded_state_dict, checkpoint_name, load_strategy, strict=args.dist_ckpt_strictness)   File ""/mnt/selfdefine/zhangyi/home/zy/zjlabmegatron/MegatronLMcore_r0.9.0/megatron/core/dist_checkpointing/serialization.py"", line 126, in load     local_metadata, global_metadata = determine_global_metadata(sharded_state_dict)   File ""/mnt/selfdefine/zhangyi/home/zy/zjlabmegatron/MegatronLMcore_r0.9.0/megatron/core/dist_checkpointing/validation.py"", line 497, in determine_global_metadata     global_metadata = [None] * torch.distributed.get_world_size()   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/distributed_c10d.py"", line 1831, in get_world_size     return _get_group_size(group)   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/distributed_c10d.py"", line 863, in _get_group_size     default_pg = _get_default_group()   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/distributed_c10d.py"", line 1024, in _get_default_group     raise ValueError( ValueError: Default process group has not been initialized, please make sure to call init_process_group. ```",2024-11-19T07:54:52Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1291,"I think you need to add saver and loader. Can you try this once? ` python tools/checkpoint/convert.py modeltype GPT loader mcore saver mcore megatronpath $megatron_folder loaddir $load_dir savedir $save_dir ` Also, when you train the model, you can specify `ckptforma` as `torch` to save in torch format.",Marking as stale. No activity in 60 days.
wenyujin333,support qwen2 hf<->mcore ckpt converter,usage example: examples/qwen/README.md,2024-11-19T04:30:21Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/1290,"Hi , could you please rebase the MR onto main branch to resolve the conflicts? Thanks",  Hi. Thanks for your contribution. We actually already have HF>mcore nonMOE qwen 2 and 2.5 conversion but it's a little hidden as it's here: https://github.com/NVIDIA/MegatronLM/blob/main/tools/checkpoint/loader_llama_mistral.py   The usage is currently documented in `examples/multimodal/nvlm` but we should document it in the main megatron docs   Perhaps you could add the MOE support to what we already have and then we can look to merge your contribution.,Marking as stale. No activity in 60 days.
prrathi,Fix: misnamed sharded instead of common in checkpoint,,2024-11-16T23:42:02Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1289
hakankiymaz-amd,Hakiymaz/deepseekv2 enablement,Enable Deepseek v2 ,2024-11-14T15:04:19Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1288
zstreeter,[QUESTION] SGD support in distrib_optimizer.py,I see the following assert in the distributed optimizer: https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/optimizer/distrib_optimizer.pyL470L472 I would like to use SGD for better performance. Why is this assert here? What is needed to support SGD in general?,2024-11-13T22:05:36Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1287,Marking as stale. No activity in 60 days.
singleheart,Fix: Resolve multimodal model errors and update README usage instructions,This Pull Request addresses the following changes: 1. **Bug Fix: Correct 'sample.answers' access**       Updated `cur_answer = sample.answers` to `cur_answer = sample.answers['value']` to fix a data access issue. 2. **Bug Fix: Apply 'mistral_custom_template' to llama**       Set `custom_chat_template=None` to `custom_chat_template=mistral_custom_template` to resolve templaterelated errors in the llama3 model. 3. **Docs Update: Improve usage instructions in README.md**       Updated usage examples to include a missing option in the `megatronenergon` section.  These changes aim to enhance model stability and usability while improving documentation for developers. Please review and provide feedback. Thank you!,2024-11-13T02:48:12Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1286,Marking as stale. No activity in 60 days.
hxdtest,Set `torch.multiprocessing` start method as 'spawn',"Set `torch.multiprocessing` start method as 'spawn'. Otherwise the following error would be raised. ``` MegatronLM/megatron/core/extensions/transformer_engine.py"", line 957, in get_cpu_offload_context     context, sync_func = _get_cpu_offload_context(   File ""/opt/conda/lib/python3.8/sitepackages/transformer_engine/pytorch/cpu_offload.py"", line 502, in get_cpu_offload_context     cpu_offload_handler = AsyncDoubleBufferGroupOffloadHandler(   File ""/opt/conda/lib/python3.8/sitepackages/transformer_engine/pytorch/cpu_offload.py"", line 312, in __init__     self.d2h_stream = torch.cuda.Stream()   File ""/opt/conda/lib/python3.8/sitepackages/torch/cuda/streams.py"", line 35, in __new__     return super().__new__(cls, priority=priority, **kwargs) RuntimeError: CUDA error: initialization error Compile with `TORCH_USE_CUDA_DSA` to enable deviceside assertions. ```",2024-11-12T09:37:32Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1285,Marking as stale. No activity in 60 days.
lyuwen,Fix a bug in optimizer's mix_lr/max_lr when args.override_opt_param_scheduler==True,"1. In the current setting, OptimizerParamScheduler uses the `max_lr` and `min_lr` from optimizer instead of its own attributes to compute the learning rate at a certain step.  2. When `overrideopt_param_scheduler` is used, user expects to override the learning rate schedule that is stored in the checkpoint. However, the optimizer still loads the `max_lr`, `min_lr` and the decoupled versions from the checkpoint.  3. Therefore, when `overrideopt_param_scheduler` is used, the learning rate in training is computed using the old `max_lr` and the new `init_lr`, a mixture of old and new setting. This can be confusing for user to figure out the issue. Here I propose an fix that during the `load_checkpoint`, when `overrideopt_param_scheduler` is set to `True`, the function `megatron.core.optimizer._update_min_and_max_lr_in_param_groups` is invoked to update the optimizer loaded from the checkpoint with the new learning rate boudaries. Thus the training would be carried out using the updated learning rate setting in situations like CPT, etc.",2024-11-12T09:07:06Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1284,"Hi barker , would you please have a look? Itâ€™s actually a bug that could affect tasks like continued pretraining. ",Marking as stale. No activity in 60 days.
leondada,[QUESTION] There is already a 32-bit model parameter in the optimizer state. Why do we need to store a separate copy of the model parameters in the checkpoint?,There is already a 32bit model parameter in the optimizer state. Why do we need to store a separate copy of the model parameters in the checkpoint?,2024-11-12T08:21:30Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1283,Marking as stale. No activity in 60 days.
gurpreet-dhami,Ci pipeline mi300,,2024-11-11T20:38:35Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1282
herolxl,Where can I download the tokenizer for the model mcore-llava-mistral-7b-instruct-clip336-pretraining?,**Your question** Ask a clear and concise question about MegatronLM.,2024-11-11T11:41:26Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1281,Marking as stale. No activity in 60 days.
klhhhhh,[BUG]Megatron-LM doesn't support transformer-engine 1.13,"**Describe the bug** MegatronLM doesn't compatible with transformerengine 1.13. in transformerengine:  https://github.com/NVIDIA/TransformerEngine/blob/2643ba1df43397cc84c9da5fe719a66d87ad9a0a/transformer_engine/pytorch/module/layernorm.pyL62L70 new parameter **_normalized_shape_** was declared, but in MegatronLM: https://github.com/NVIDIA/MegatronLM/blob/1b8fce7e17e7f945c1f59d06744a2e126bedc015/megatron/core/extensions/transformer_engine.pyL64L70 the new parameter **_normalized_shape_** is missing. **To Reproduce** When directly installing transformerengine with ``` pip install git+https://github.com/NVIDIA/TransformerEngine.git ``` transformer engine version will be 1.13, and when you run the scripts in examples, the error will appear. **Environment (please complete the following information):**   MegatronLM commit ID: 1b8fce7e17e7f945c1f59d06744a2e126bedc015   PyTorch version: 2.5   CUDA version: 12.4   NCCL version: 2.21.5   transformerengine version: 1.13 **Fix right now:** install transformerengine with: ``` pip install git+https://github.com/NVIDIA/TransformerEngine.git.12 ```",2024-11-11T07:30:56Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1280,Thanks for reporting. This regression was introduced in https://github.com/NVIDIA/TransformerEngine/pull/1033 and it should be fixed with https://github.com/NVIDIA/TransformerEngine/pull/1329. Hopefully the integration test in that PR will help catch these kinds of errors in the future.,Marking as stale. No activity in 60 days.
DXZDXZ,[BUG] Encountering NaN gradients when using CUDA Graph,"**Describe the bug** When using MegatronCore v0.9.0 with CUDA Graphs enabled, NaN gradients are encountered during the backward computation. This issue does not occur when CUDA Graphs are disabled. **To Reproduce** To reproduce this issue, follow these steps: 1. Ensure that MegatronCore v0.9.0 is installed and set up correctly in your environment. 2. Configure the transformerconfig by setting enablecudagraph to True. 3. Train a model or run a training script that involves backward computation. 4. Observe the gradients during training to notice NaN values. **Expected behavior** The expected behavior is for the model to train normally without encountering NaN gradients, even with CUDA Graphs enabled. The use of CUDA Graphs should not affect the correctness of the gradient computations. **Stack trace/logs** ``` [rank5]: Traceback (most recent call last): [rank5]:   File ""/workspace/MegatronLM/pretrain_gpt.py"", line 265, in  [rank5]:     pretrain( [rank5]:   File ""/workspace/MegatronLM/megatron/training/training.py"", line 360, in pretrain [rank5]:     iteration, num_floating_point_operations_so_far = train( [rank5]:   File ""/workspace/MegatronLM/megatron/training/training.py"", line 1262, in train [rank5]:     train_step(forward_step_func, [rank5]:   File ""/workspace/MegatronLM/megatron/training/training.py"", line 730, in train_step [rank5]:     losses_reduced = forward_backward_func( [rank5]:   File ""/workspace/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 492, in forward_backward_no_pipelining [rank5]:     config.finalize_model_grads_func( [rank5]:   File ""/workspace/MegatronLM/megatron/core/distributed/finalize_model_grads.py"", line 112, in finalize_model_grads [rank5]:     model_chunk.finish_grad_sync() [rank5]:   File ""/workspace/MegatronLM/megatron/core/distributed/distributed_data_parallel.py"", line 422, in finish_grad_sync [rank5]:     bucket_group.finish_grad_sync() [rank5]:   File ""/workspace/MegatronLM/megatron/core/distributed/param_and_grad_buffer.py"", line 302, in finish_grad_sync [rank5]:     self.start_grad_sync() [rank5]:   File ""/workspace/MegatronLM/megatron/core/distributed/param_and_grad_buffer.py"", line 244, in start_grad_sync [rank5]:     self.check_for_nan_in_grad() [rank5]:   File ""/workspace/MegatronLM/megatron/core/distributed/param_and_grad_buffer.py"", line 148, in check_for_nan_in_grad [rank5]:     assert not norm_is_nan, ( [rank5]: AssertionError: Rank 5: found NaN in local grad norm in backward pass before dataparallel communication collective. Device: 5, node: infratrain3ali0 ``` **Environment (please complete the following information)**  MegatronLM commit ID: 1b869f019af2c7aabf9c4deffe6eb64ebef88608  PyTorch version: 2.3.0+cu121  CUDA version: 12.4  NCCL version: 2.20.5  TransformerEngine version: 1.11.0+c27ee60",2024-11-11T03:29:32Z,,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/1279,Same issue with TE release_v1.12 and TP >= 2,Marking as stale. No activity in 60 days.,Any update on this? Running into the same error now with the following setup: MegatronLM NeMo TransformerEngine and configs: ``` model.use_te_rng_tracker: True model.enable_cuda_graph: True ```
zstreet87,Distributed chkpt save fix,"https://pytorch.org/docs/stable/notes/multiprocessing.html You can read in the above documentation that it is recommended to use `spawn` instead of `fork` after CUDA or ROCm initialization.  Seems like CUDA is more tolerable of this than us, since this workload still worked with fork but we need spawn.",2024-11-08T21:17:54Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1278
Louis-J,[QUESTION] is there any restriction to use allgather with moe_expert_capacity_factor?,"**Your question** Ask a clear and concise question about MegatronLM. There is an assert in megatron/core/transformer/transformer_config.py: 401 ```         if self.moe_expert_capacity_factor is not None:            ** if self.moe_token_dispatcher_type not in [""alltoall"", ""alltoall_seq""]:**                 raise ValueError(                     'moe_expert_capacity_factor only works with alltoall token dispatcher'                 ) ``` The code to process with capacity_factor and pad in router.py seems it won't change the output tensor's dimsize. And I don't see any different process to do with capacity_factor in token_dispatcher.py. So why should I use only 'alltoall' or 'alltoall_seq' with moe_expert_capacity_factor? thanks for reply.",2024-11-07T06:39:16Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1277,Marking as stale. No activity in 60 days.
ltm920716,[QUESTION] scaleing MFU calculate,**Your question** Helloï¼Œ the MFU in the table is almost 40%ï¼Œand the teraflops is around 400ï¼Œlike bellowï¼š https://github.com/NVIDIA/MegatronLM?tab=readmeovfiletrainingspeedandscalability that is this table use fp32 floatï¼Ÿbut there is all fp16 as param in the training scriptï¼Œif use fp16ï¼Œthe MFU should be 400 / 2000 = 20%ï¼Œ not 40%ï¼Ÿ I am so confusedï¼Œ help pleaseï¼Œthanksï¼,2024-11-06T08:40:13Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1276,"For H100 SXM and 16bit floating point (either fp16 or bfloat16) the theoretical peak is 1979 TFLOPS when using 2:4 structured sparsity and 989 TFLOPS when not using sparsity. Since MegatronLM doesn't use the 2:4 structured sparsity feature, the denominator for the calculations in that table is 989 TFLOPS."
wplf,[BUG] TP-comm-overlap bug when replacing `TELayerNormColumnParallelLinear` into `TEColumnParallelLinear` .,"**Describe the bug** Bugs happens when I use `TEColumnParallelLinear` instead of `TELayerNormColumnParallelLinear` with ""tpcommoverlap"" open. The reason might be the misuse of the `ub_split_rs` and `ub_split_ag` for column linear and row linear. I thought that `ub_split_rs` is not supposed to be open in `TEColumnParallelLinear`. **To Reproduce** The commit ID of megatronLM is `f39c48dba01ffb2f91cbee992252bb9543200633`. 1. prepare dataset 2. open `tpcommoverlap` and `sequenceparallel` and `tensorparallel` in scripts. 3. change all `TELayerNormColumnParallelLinear` into `TEColumnParallelLinear` in `megatron/core/models/gpt/gpt_layer_specs.py`. 4. run `train_gpt3_175b_distributed.sh`. Then bug will happen.  **Stack trace/logs** ```bash   File ""/vepfs/home/lijinliang/miniconda3/envs/megatron/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/vepfs/home/lijinliang/miniconda3/envs/megatron/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1520, in _call_impl     return forward_call(*args, **kwargs)   File ""/vepfs/home/lijinliang/official_proj/MegatronLM/megatron/core/transformer/transformer_layer.py"", line 300, in forward     attention_output_with_bias = self.self_attention(   File ""/vepfs/home/lijinliang/miniconda3/envs/megatron/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/vepfs/home/lijinliang/miniconda3/envs/megatron/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1520, in _call_impl     return self._call_impl(*args, **kwargs)   File ""/vepfs/home/lijinliang/miniconda3/envs/megatron/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1520, in _call_impl     return forward_call(*args, **kwargs)   File ""/vepfs/home/lijinliang/official_proj/MegatronLM/megatron/core/transformer/attention.py"", line 360, in forward     query, key, value = self.get_query_key_value_tensors(hidden_states, key_value_states)   File ""/vepfs/home/lijinliang/official_proj/MegatronLM/megatron/core/transformer/attention.py"", line 611, in get_query_key_value_tensors     return self._call_impl(*args, **kwargs)   File ""/vepfs/home/lijinliang/miniconda3/envs/megatron/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1520, in _call_impl     return forward_call(*args, **kwargs)   File ""/vepfs/home/lijinliang/official_proj/MegatronLM/megatron/core/transformer/attention.py"", line 360, in forward     return forward_call(*args, **kwargs)   File ""/vepfs/home/lijinliang/official_proj/MegatronLM/megatron/core/transformer/attention.py"", line 360, in forward     mixed_qkv = mixed_qkv.view(*new_tensor_shape) RuntimeError: shape '[256, 4, 16, 192]' is invalid for input of size 4194304     query, key, value = self.get_query_key_value_tensors(hidden_states, key_value_states)   File ""/vepfs/home/lijinliang/official_proj/MegatronLM/megatron/core/transformer/attention.py"", line 611, in get_query_key_value_tensors     query, key, value = self.get_query_key_value_tensors(hidden_states, key_value_states)   File ""/vepfs/home/lijinliang/official_proj/MegatronLM/megatron/core/transformer/attention.py"", line 611, in get_query_key_value_tensors     return forward_call(*args, **kwargs)   File ""/vepfs/home/lijinliang/official_proj/MegatronLM/megatron/core/transformer/attention.py"", line 360, in forward     mixed_qkv = mixed_qkv.view(*new_tensor_shape) RuntimeError: shape '[256, 4, 16, 192]' is invalid for input of size 4194304     mixed_qkv = mixed_qkv.view(*new_tensor_shape) RuntimeError: shape '[256, 4, 16, 192]' is invalid for input of size 4194304     query, key, value = self.get_query_key_value_tensors(hidden_states, key_value_states)   File ""/vepfs/home/lijinliang/official_proj/MegatronLM/megatron/core/transformer/attention.py"", line 611, in get_query_key_value_tensors     mixed_qkv = mixed_qkv.view(*new_tensor_shape) RuntimeError: shape '[256, 4, 16, 192]' is invalid for input of size 4194304 [20241106 15:47:22,476] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2633092 closing signal SIGTERM [20241106 15:47:24,194] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 1 (pid: 2633093) of binary: /vepfs/home/lijinliang/miniconda3/envs/megatron/bin/python Traceback (most recent call last):   File ""/home/lijinliang/.local/bin/torchrun"", line 33, in      sys.exit(load_entry_point('torch', 'console_scripts', 'torchrun')())   File ""/vepfs/home/lijinliang/miniconda3/envs/megatron/lib/python3.10/sitepackages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 347, in wrapper     return f(*args, **kwargs)   File ""/vepfs/home/lijinliang/miniconda3/envs/megatron/lib/python3.10/sitepackages/torch/distributed/run.py"", line 812, in main     run(args)   File ""/vepfs/home/lijinliang/miniconda3/envs/megatron/lib/python3.10/sitepackages/torch/distributed/run.py"", line 803, in run     elastic_launch(   File ""/vepfs/home/lijinliang/miniconda3/envs/megatron/lib/python3.10/sitepackages/torch/distributed/launcher/api.py"", line 135, in __call__     return launch_agent(self._config, self._entrypoint, list(args))   File ""/vepfs/home/lijinliang/miniconda3/envs/megatron/lib/python3.10/sitepackages/torch/distributed/launcher/api.py"", line 268, in launch_agent     raise ChildFailedError( torch.distributed.elastic.multiprocessing.errors.ChildFailedError:  ============================================================ pretrain_gpt.py FAILED  Failures: [1]:   time      : 20241106_15:47:22   host      : localhost   rank      : 2 (local_rank: 2)   exitcode  : 1 (pid: 2633094)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html [2]:   time      : 20241106_15:47:22   host      : localhost   rank      : 3 (local_rank: 3)   exitcode  : 1 (pid: 2633095)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html  Root Cause (first observed failure): [0]:   time      : 20241106_15:47:22   host      : localhost   rank      : 1 (local_rank: 1)   exitcode  : 1 (pid: 2633093)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html ``` **Environment (please complete the following information):**   MegatronLM commit ID   PyTorch 2.2.1+cu121   CUDA 12.1   NCCL 2.19.3 **Proposed fix** If you have a proposal for how to fix the issue state it here or link to a PR. **Additional context** Add any other context about the problem here.",2024-11-06T08:02:43Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1275,Marking as stale. No activity in 60 days.
ltm920716,[BUG] training crash when set --tp-comm-overlap,"**Describe the bug** training crash when set tpcommoverlap **To Reproduce** MODEL_PARALLEL_ARGS=(         tensormodelparallelsize 8         pipelinemodelparallelsize 1         useflashattn         sequenceparallel         tpcommoverlap ) docker run rm gpus=all shmsize=10g ulimit memlock=1 ulimit stack=67108864 ipc=host v /mnt/data01/fake_data:/home nvcr.io/nvidia/pytorch:24.04py3 bash c ""cd /home/MegatronLM && bash examples/gpt3/single.sh"" **Expected behavior** run successfully **Stack trace/logs** ``` /home/MegatronLM/megatron/training/initialize.py:227: UserWarning: Transformer Engine v1.5.0+6a9edc3 supports only MPI bootstrap backend.   warnings.warn( /home/MegatronLM/megatron/training/initialize.py:227: UserWarning: Transformer Engine v1.5.0+6a9edc3 supports only MPI bootstrap backend.   warnings.warn( /home/MegatronLM/megatron/training/initialize.py:227: UserWarning: Transformer Engine v1.5.0+6a9edc3 supports only MPI bootstrap backend.   warnings.warn( /home/MegatronLM/megatron/training/initialize.py:227: UserWarning: Transformer Engine v1.5.0+6a9edc3 supports only MPI bootstrap backend.   warnings.warn( /home/MegatronLM/megatron/training/initialize.py:227: UserWarning: Transformer Engine v1.5.0+6a9edc3 supports only MPI bootstrap backend.   warnings.warn( /home/MegatronLM/megatron/training/initialize.py:227: UserWarning: Transformer Engine v1.5.0+6a9edc3 supports only MPI bootstrap backend.   warnings.warn( /home/MegatronLM/megatron/training/initialize.py:227: UserWarning: Transformer Engine v1.5.0+6a9edc3 supports only MPI bootstrap backend.   warnings.warn( /home/MegatronLM/megatron/training/initialize.py:227: UserWarning: Transformer Engine v1.5.0+6a9edc3 supports only MPI bootstrap backend.   warnings.warn( [fb2a7d718a49:10272:0:10272] Caught signal 11 (Segmentation fault: invalid permissions for mapped object at address 0x7f01bad4bea8) [fb2a7d718a49:10277:0:10277] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x28) [fb2a7d718a49:10275:0:10275] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x28) [fb2a7d718a49:10276:0:10276] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x28) [fb2a7d718a49:10271:0:10271] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x28) [fb2a7d718a49:10274:0:10274] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x28) [fb2a7d718a49:10273:0:10273] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x28) [fb2a7d718a49:10278:0:10278] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x28) ==== backtrace (tid:  10272) ====  0 0x0000000000042520 __sigaction()  ???:0  1 0x000000000004d128 ompi_group_increment_proc_count()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/group/group_init.c:229  2 0x000000000004d128 opal_atomic_add_fetch_32()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/../opal/include/opal/sys/atomic_impl.h:384  3 0x000000000004d128 opal_thread_add_fetch_32()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/../opal/threads/thread_usage.h:152  4 0x000000000004d128 opal_obj_update()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/../opal/class/opal_object.h:534  5 0x000000000004d128 ompi_group_increment_proc_count()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/group/group_init.c:226  6 0x000000000004d9e9 ompi_group_incl_plist()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/group/group_plist.c:128  7 0x000000000007421b PMPI_Group_incl()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/mpi/c/profile/pgroup_incl.c:87  8 0x0000000004f1ea5d c10d::ProcessGroupMPI::createProcessGroupMPI()  ???:0  9 0x0000000000c35470 pybind11::cpp_function::initialize >) CC(GLUE tasks for BERT)}, c10::intrusive_ptr >, std::vector >, pybind11::name, pybind11::scope, pybind11::sibling, pybind11::call_guard >(torch::distributed::c10d::(anonymous namespace)::c10d_init(_object*, _object*)::{lambda(std::vector >) CC(GLUE tasks for BERT)}&&, c10::intrusive_ptr > (*)(std::vector >), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&, pybind11::call_guard const&)::{lambda(pybind11::detail::function_call&) CC(Compatibility with pytorchtransformers for finetuning )}::_FUN()  init.cpp:0 10 0x000000000042efb7 pybind11::cpp_function::dispatcher()  :0 11 0x000000000015a10e PyObject_CallFunctionObjArgs()  ???:0 12 0x0000000000150a7b _PyObject_MakeTpCall()  ???:0 13 0x0000000000149629 _PyEval_EvalFrameDefault()  ???:0 14 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 15 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 16 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 17 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 18 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 19 0x0000000000169492 PyObject_Call()  ???:0 20 0x00000000001455d7 _PyEval_EvalFrameDefault()  ???:0 21 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 22 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 23 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 24 0x000000000014326d _PyEval_EvalFrameDefault()  ???:0 25 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 26 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 27 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 28 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 29 0x000000000013f9c6 _PyArg_ParseTuple_SizeT()  ???:0 30 0x0000000000235256 PyEval_EvalCode()  ???:0 31 0x0000000000260108 PyUnicode_Tailmatch()  ???:0 32 0x00000000002599cb PyInit__collections()  ???:0 33 0x000000000025fe55 PyUnicode_Tailmatch()  ???:0 34 0x000000000025f338 _PyRun_SimpleFileObject()  ???:0 35 0x000000000025ef83 _PyRun_AnyFileObject()  ???:0 36 0x0000000000251a5e Py_RunMain()  ???:0 37 0x000000000022802d Py_BytesMain()  ???:0 38 0x0000000000029d90 __libc_init_first()  ???:0 39 0x0000000000029e40 __libc_start_main()  ???:0 40 0x0000000000227f25 _start()  ???:0 ================================= ==== backtrace (tid:  10277) ====  0 0x0000000000042520 __sigaction()  ???:0  1 0x0000000000042b60 ompi_dpm_group_is_dyn()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/dpm/dpm.c:1267  2 0x0000000000042b60 ompi_dpm_group_is_dyn()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/dpm/dpm.c:1268  3 0x0000000000042b60 ompi_dpm_mark_dyncomm()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/dpm/dpm.c:1299  4 0x0000000000034388 ompi_comm_set_nb()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/communicator/comm.c:215  5 0x00000000000346ba ompi_comm_set()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/communicator/comm.c:116  6 0x0000000000034ef3 ompi_comm_create()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/communicator/comm.c:344  7 0x000000000006b24a PMPI_Comm_create()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/mpi/c/profile/pcomm_create.c:66  8 0x0000000004f1ea87 c10d::ProcessGroupMPI::createProcessGroupMPI()  ???:0  9 0x0000000000c35470 pybind11::cpp_function::initialize >) CC(GLUE tasks for BERT)}, c10::intrusive_ptr >, std::vector >, pybind11::name, pybind11::scope, pybind11::sibling, pybind11::call_guard >(torch::distributed::c10d::(anonymous namespace)::c10d_init(_object*, _object*)::{lambda(std::vector >) CC(GLUE tasks for BERT)}&&, c10::intrusive_ptr > (*)(std::vector >), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&, pybind11::call_guard const&)::{lambda(pybind11::detail::function_call&) CC(Compatibility with pytorchtransformers for finetuning )}::_FUN()  init.cpp:0 10 0x000000000042efb7 pybind11::cpp_function::dispatcher()  :0 11 0x000000000015a10e PyObject_CallFunctionObjArgs()  ???:0 12 0x0000000000150a7b _PyObject_MakeTpCall()  ???:0 13 0x0000000000149629 _PyEval_EvalFrameDefault()  ???:0 14 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 15 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 16 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 17 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 18 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 19 0x0000000000169492 PyObject_Call()  ???:0 20 0x00000000001455d7 _PyEval_EvalFrameDefault()  ???:0 21 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 22 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 23 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 24 0x000000000014326d _PyEval_EvalFrameDefault()  ???:0 25 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 26 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 27 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 28 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 29 0x000000000013f9c6 _PyArg_ParseTuple_SizeT()  ???:0 30 0x0000000000235256 PyEval_EvalCode()  ???:0 31 0x0000000000260108 PyUnicode_Tailmatch()  ???:0 32 0x00000000002599cb PyInit__collections()  ???:0 33 0x000000000025fe55 PyUnicode_Tailmatch()  ???:0 34 0x000000000025f338 _PyRun_SimpleFileObject()  ???:0 35 0x000000000025ef83 _PyRun_AnyFileObject()  ???:0 36 0x0000000000251a5e Py_RunMain()  ???:0 37 0x000000000022802d Py_BytesMain()  ???:0 38 0x0000000000029d90 __libc_init_first()  ???:0 39 0x0000000000029e40 __libc_start_main()  ???:0 40 0x0000000000227f25 _start()  ???:0 ================================= ==== backtrace (tid:  10275) ====  0 0x0000000000042520 __sigaction()  ???:0  1 0x0000000000042b60 ompi_dpm_group_is_dyn()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/dpm/dpm.c:1267  2 0x0000000000042b60 ompi_dpm_group_is_dyn()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/dpm/dpm.c:1268  3 0x0000000000042b60 ompi_dpm_mark_dyncomm()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/dpm/dpm.c:1299  4 0x0000000000034388 ompi_comm_set_nb()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/communicator/comm.c:215  5 0x00000000000346ba ompi_comm_set()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/communicator/comm.c:116  6 0x0000000000034ef3 ompi_comm_create()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/communicator/comm.c:344  7 0x000000000006b24a PMPI_Comm_create()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/mpi/c/profile/pcomm_create.c:66  8 0x0000000004f1ea87 c10d::ProcessGroupMPI::createProcessGroupMPI()  ???:0  9 0x0000000000c35470 pybind11::cpp_function::initialize >) CC(GLUE tasks for BERT)}, c10::intrusive_ptr >, std::vector >, pybind11::name, pybind11::scope, pybind11::sibling, pybind11::call_guard >(torch::distributed::c10d::(anonymous namespace)::c10d_init(_object*, _object*)::{lambda(std::vector >) CC(GLUE tasks for BERT)}&&, c10::intrusive_ptr > (*)(std::vector >), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&, pybind11::call_guard const&)::{lambda(pybind11::detail::function_call&) CC(Compatibility with pytorchtransformers for finetuning )}::_FUN()  init.cpp:0 10 0x000000000042efb7 pybind11::cpp_function::dispatcher()  :0 11 0x000000000015a10e PyObject_CallFunctionObjArgs()  ???:0 12 0x0000000000150a7b _PyObject_MakeTpCall()  ???:0 13 0x0000000000149629 _PyEval_EvalFrameDefault()  ???:0 14 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 15 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 16 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 17 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 18 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 19 0x0000000000169492 PyObject_Call()  ???:0 20 0x00000000001455d7 _PyEval_EvalFrameDefault()  ???:0 21 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 22 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 23 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 24 0x000000000014326d _PyEval_EvalFrameDefault()  ???:0 25 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 26 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 27 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 28 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 29 0x000000000013f9c6 _PyArg_ParseTuple_SizeT()  ???:0 30 0x0000000000235256 PyEval_EvalCode()  ???:0 31 0x0000000000260108 PyUnicode_Tailmatch()  ???:0 32 0x00000000002599cb PyInit__collections()  ???:0 33 0x000000000025fe55 PyUnicode_Tailmatch()  ???:0 34 0x000000000025f338 _PyRun_SimpleFileObject()  ???:0 35 0x000000000025ef83 _PyRun_AnyFileObject()  ???:0 36 0x0000000000251a5e Py_RunMain()  ???:0 37 0x000000000022802d Py_BytesMain()  ???:0 38 0x0000000000029d90 __libc_init_first()  ???:0 39 0x0000000000029e40 __libc_start_main()  ???:0 40 0x0000000000227f25 _start()  ???:0 ================================= ==== backtrace (tid:  10276) ====  0 0x0000000000042520 __sigaction()  ???:0  1 0x0000000000042b60 ompi_dpm_group_is_dyn()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/dpm/dpm.c:1267  2 0x0000000000042b60 ompi_dpm_group_is_dyn()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/dpm/dpm.c:1268  3 0x0000000000042b60 ompi_dpm_mark_dyncomm()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/dpm/dpm.c:1299  4 0x0000000000034388 ompi_comm_set_nb()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/communicator/comm.c:215  5 0x00000000000346ba ompi_comm_set()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/communicator/comm.c:116  6 0x0000000000034ef3 ompi_comm_create()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/communicator/comm.c:344  7 0x000000000006b24a PMPI_Comm_create()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/mpi/c/profile/pcomm_create.c:66  8 0x0000000004f1ea87 c10d::ProcessGroupMPI::createProcessGroupMPI()  ???:0  9 0x0000000000c35470 pybind11::cpp_function::initialize >) CC(GLUE tasks for BERT)}, c10::intrusive_ptr >, std::vector >, pybind11::name, pybind11::scope, pybind11::sibling, pybind11::call_guard >(torch::distributed::c10d::(anonymous namespace)::c10d_init(_object*, _object*)::{lambda(std::vector >) CC(GLUE tasks for BERT)}&&, c10::intrusive_ptr > (*)(std::vector >), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&, pybind11::call_guard const&)::{lambda(pybind11::detail::function_call&) CC(Compatibility with pytorchtransformers for finetuning )}::_FUN()  init.cpp:0 10 0x000000000042efb7 pybind11::cpp_function::dispatcher()  :0 11 0x000000000015a10e PyObject_CallFunctionObjArgs()  ???:0 12 0x0000000000150a7b _PyObject_MakeTpCall()  ???:0 13 0x0000000000149629 _PyEval_EvalFrameDefault()  ???:0 14 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 15 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 16 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 17 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 18 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 19 0x0000000000169492 PyObject_Call()  ???:0 20 0x00000000001455d7 _PyEval_EvalFrameDefault()  ???:0 21 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 22 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 23 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 24 0x000000000014326d _PyEval_EvalFrameDefault()  ???:0 25 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 26 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 27 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 28 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 29 0x000000000013f9c6 _PyArg_ParseTuple_SizeT()  ???:0 30 0x0000000000235256 PyEval_EvalCode()  ???:0 31 0x0000000000260108 PyUnicode_Tailmatch()  ???:0 32 0x00000000002599cb PyInit__collections()  ???:0 33 0x000000000025fe55 PyUnicode_Tailmatch()  ???:0 34 0x000000000025f338 _PyRun_SimpleFileObject()  ???:0 35 0x000000000025ef83 _PyRun_AnyFileObject()  ???:0 36 0x0000000000251a5e Py_RunMain()  ???:0 37 0x000000000022802d Py_BytesMain()  ???:0 38 0x0000000000029d90 __libc_init_first()  ???:0 39 0x0000000000029e40 __libc_start_main()  ???:0 40 0x0000000000227f25 _start()  ???:0 ================================= ==== backtrace (tid:  10271) ====  0 0x0000000000042520 __sigaction()  ???:0  1 0x0000000000042b60 ompi_dpm_group_is_dyn()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/dpm/dpm.c:1267  2 0x0000000000042b60 ompi_dpm_group_is_dyn()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/dpm/dpm.c:1268  3 0x0000000000042b60 ompi_dpm_mark_dyncomm()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/dpm/dpm.c:1299  4 0x0000000000034388 ompi_comm_set_nb()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/communicator/comm.c:215  5 0x00000000000346ba ompi_comm_set()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/communicator/comm.c:116  6 0x0000000000034ef3 ompi_comm_create()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/communicator/comm.c:344  7 0x000000000006b24a PMPI_Comm_create()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/mpi/c/profile/pcomm_create.c:66  8 0x0000000004f1ea87 c10d::ProcessGroupMPI::createProcessGroupMPI()  ???:0  9 0x0000000000c35470 pybind11::cpp_function::initialize >) CC(GLUE tasks for BERT)}, c10::intrusive_ptr >, std::vector >, pybind11::name, pybind11::scope, pybind11::sibling, pybind11::call_guard >(torch::distributed::c10d::(anonymous namespace)::c10d_init(_object*, _object*)::{lambda(std::vector >) CC(GLUE tasks for BERT)}&&, c10::intrusive_ptr > (*)(std::vector >), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&, pybind11::call_guard const&)::{lambda(pybind11::detail::function_call&) CC(Compatibility with pytorchtransformers for finetuning )}::_FUN()  init.cpp:0 10 0x000000000042efb7 pybind11::cpp_function::dispatcher()  :0 11 0x000000000015a10e PyObject_CallFunctionObjArgs()  ???:0 12 0x0000000000150a7b _PyObject_MakeTpCall()  ???:0 13 0x0000000000149629 _PyEval_EvalFrameDefault()  ???:0 14 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 15 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 16 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 17 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 18 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 19 0x0000000000169492 PyObject_Call()  ???:0 20 0x00000000001455d7 _PyEval_EvalFrameDefault()  ???:0 21 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 22 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 23 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 24 0x000000000014326d _PyEval_EvalFrameDefault()  ???:0 25 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 26 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 27 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 28 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 29 0x000000000013f9c6 _PyArg_ParseTuple_SizeT()  ???:0 30 0x0000000000235256 PyEval_EvalCode()  ???:0 31 0x0000000000260108 PyUnicode_Tailmatch()  ???:0 32 0x00000000002599cb PyInit__collections()  ???:0 33 0x000000000025fe55 PyUnicode_Tailmatch()  ???:0 34 0x000000000025f338 _PyRun_SimpleFileObject()  ???:0 35 0x000000000025ef83 _PyRun_AnyFileObject()  ???:0 36 0x0000000000251a5e Py_RunMain()  ???:0 37 0x000000000022802d Py_BytesMain()  ???:0 38 0x0000000000029d90 __libc_init_first()  ???:0 39 0x0000000000029e40 __libc_start_main()  ???:0 40 0x0000000000227f25 _start()  ???:0 ================================= ==== backtrace (tid:  10273) ====  0 0x0000000000042520 __sigaction()  ???:0  1 0x0000000000042b60 ompi_dpm_group_is_dyn()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/dpm/dpm.c:1267  2 0x0000000000042b60 ompi_dpm_group_is_dyn()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/dpm/dpm.c:1268  3 0x0000000000042b60 ompi_dpm_mark_dyncomm()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/dpm/dpm.c:1299  4 0x0000000000034388 ompi_comm_set_nb()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/communicator/comm.c:215  5 0x00000000000346ba ompi_comm_set()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/communicator/comm.c:116  6 0x0000000000034ef3 ompi_comm_create()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/communicator/comm.c:344  7 0x000000000006b24a PMPI_Comm_create()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/mpi/c/profile/pcomm_create.c:66  8 0x0000000004f1ea87 c10d::ProcessGroupMPI::createProcessGroupMPI()  ???:0  9 0x0000000000c35470 pybind11::cpp_function::initialize >) CC(GLUE tasks for BERT)}, c10::intrusive_ptr >, std::vector >, pybind11::name, pybind11::scope, pybind11::sibling, pybind11::call_guard >(torch::distributed::c10d::(anonymous namespace)::c10d_init(_object*, _object*)::{lambda(std::vector >) CC(GLUE tasks for BERT)}&&, c10::intrusive_ptr > (*)(std::vector >), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&, pybind11::call_guard const&)::{lambda(pybind11::detail::function_call&) CC(Compatibility with pytorchtransformers for finetuning )}::_FUN()  init.cpp:0 10 0x000000000042efb7 pybind11::cpp_function::dispatcher()  :0 11 0x000000000015a10e PyObject_CallFunctionObjArgs()  ???:0 12 0x0000000000150a7b _PyObject_MakeTpCall()  ???:0 13 0x0000000000149629 _PyEval_EvalFrameDefault()  ???:0 14 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 15 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 16 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 17 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 18 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 19 0x0000000000169492 PyObject_Call()  ???:0 20 0x00000000001455d7 _PyEval_EvalFrameDefault()  ???:0 21 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 22 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 23 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 24 0x000000000014326d _PyEval_EvalFrameDefault()  ???:0 25 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 26 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 27 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 28 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 29 0x000000000013f9c6 _PyArg_ParseTuple_SizeT()  ???:0 30 0x0000000000235256 PyEval_EvalCode()  ???:0 31 0x0000000000260108 PyUnicode_Tailmatch()  ???:0 32 0x00000000002599cb PyInit__collections()  ???:0 33 0x000000000025fe55 PyUnicode_Tailmatch()  ???:0 34 0x000000000025f338 _PyRun_SimpleFileObject()  ???:0 35 0x000000000025ef83 _PyRun_AnyFileObject()  ???:0 36 0x0000000000251a5e Py_RunMain()  ???:0 37 0x000000000022802d Py_BytesMain()  ???:0 38 0x0000000000029d90 __libc_init_first()  ???:0 39 0x0000000000029e40 __libc_start_main()  ???:0 40 0x0000000000227f25 _start()  ???:0 ================================= ==== backtrace (tid:  10274) ====  0 0x0000000000042520 __sigaction()  ???:0  1 0x0000000000042b60 ompi_dpm_group_is_dyn()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/dpm/dpm.c:1267  2 0x0000000000042b60 ompi_dpm_group_is_dyn()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/dpm/dpm.c:1268  3 0x0000000000042b60 ompi_dpm_mark_dyncomm()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/dpm/dpm.c:1299  4 0x0000000000034388 ompi_comm_set_nb()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/communicator/comm.c:215  5 0x00000000000346ba ompi_comm_set()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/communicator/comm.c:116  6 0x0000000000034ef3 ompi_comm_create()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/communicator/comm.c:344  7 0x000000000006b24a PMPI_Comm_create()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/mpi/c/profile/pcomm_create.c:66  8 0x0000000004f1ea87 c10d::ProcessGroupMPI::createProcessGroupMPI()  ???:0  9 0x0000000000c35470 pybind11::cpp_function::initialize >) CC(GLUE tasks for BERT)}, c10::intrusive_ptr >, std::vector >, pybind11::name, pybind11::scope, pybind11::sibling, pybind11::call_guard >(torch::distributed::c10d::(anonymous namespace)::c10d_init(_object*, _object*)::{lambda(std::vector >) CC(GLUE tasks for BERT)}&&, c10::intrusive_ptr > (*)(std::vector >), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&, pybind11::call_guard const&)::{lambda(pybind11::detail::function_call&) CC(Compatibility with pytorchtransformers for finetuning )}::_FUN()  init.cpp:0 10 0x000000000042efb7 pybind11::cpp_function::dispatcher()  :0 11 0x000000000015a10e PyObject_CallFunctionObjArgs()  ???:0 12 0x0000000000150a7b _PyObject_MakeTpCall()  ???:0 13 0x0000000000149629 _PyEval_EvalFrameDefault()  ???:0 14 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 15 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 16 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 17 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 18 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 19 0x0000000000169492 PyObject_Call()  ???:0 20 0x00000000001455d7 _PyEval_EvalFrameDefault()  ???:0 21 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 22 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 23 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 24 0x000000000014326d _PyEval_EvalFrameDefault()  ???:0 25 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 26 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 27 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 28 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 29 0x000000000013f9c6 _PyArg_ParseTuple_SizeT()  ???:0 30 0x0000000000235256 PyEval_EvalCode()  ???:0 31 0x0000000000260108 PyUnicode_Tailmatch()  ???:0 32 0x00000000002599cb PyInit__collections()  ???:0 33 0x000000000025fe55 PyUnicode_Tailmatch()  ???:0 34 0x000000000025f338 _PyRun_SimpleFileObject()  ???:0 35 0x000000000025ef83 _PyRun_AnyFileObject()  ???:0 36 0x0000000000251a5e Py_RunMain()  ???:0 37 0x000000000022802d Py_BytesMain()  ???:0 38 0x0000000000029d90 __libc_init_first()  ???:0 39 0x0000000000029e40 __libc_start_main()  ???:0 40 0x0000000000227f25 _start()  ???:0 ================================= ==== backtrace (tid:  10278) ====  0 0x0000000000042520 __sigaction()  ???:0  1 0x0000000000042b60 ompi_dpm_group_is_dyn()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/dpm/dpm.c:1267  2 0x0000000000042b60 ompi_dpm_group_is_dyn()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/dpm/dpm.c:1268  3 0x0000000000042b60 ompi_dpm_mark_dyncomm()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/dpm/dpm.c:1299  4 0x0000000000034388 ompi_comm_set_nb()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/communicator/comm.c:215  5 0x00000000000346ba ompi_comm_set()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/communicator/comm.c:116  6 0x0000000000034ef3 ompi_comm_create()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/communicator/comm.c:344  7 0x000000000006b24a PMPI_Comm_create()  /buildresult/src/hpcxv2.18gccinboxubuntu22.04cuda12x86_64/ompiefbeca7056b93dd17c67b66d1d514d39712e28d6/ompi/mpi/c/profile/pcomm_create.c:66  8 0x0000000004f1ea87 c10d::ProcessGroupMPI::createProcessGroupMPI()  ???:0  9 0x0000000000c35470 pybind11::cpp_function::initialize >) CC(GLUE tasks for BERT)}, c10::intrusive_ptr >, std::vector >, pybind11::name, pybind11::scope, pybind11::sibling, pybind11::call_guard >(torch::distributed::c10d::(anonymous namespace)::c10d_init(_object*, _object*)::{lambda(std::vector >) CC(GLUE tasks for BERT)}&&, c10::intrusive_ptr > (*)(std::vector >), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&, pybind11::call_guard const&)::{lambda(pybind11::detail::function_call&) CC(Compatibility with pytorchtransformers for finetuning )}::_FUN()  init.cpp:0 10 0x000000000042efb7 pybind11::cpp_function::dispatcher()  :0 11 0x000000000015a10e PyObject_CallFunctionObjArgs()  ???:0 12 0x0000000000150a7b _PyObject_MakeTpCall()  ???:0 13 0x0000000000149629 _PyEval_EvalFrameDefault()  ???:0 14 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 15 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 16 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 17 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 18 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 19 0x0000000000169492 PyObject_Call()  ???:0 20 0x00000000001455d7 _PyEval_EvalFrameDefault()  ???:0 21 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 22 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 23 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 24 0x000000000014326d _PyEval_EvalFrameDefault()  ???:0 25 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 26 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 27 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0 28 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0 29 0x000000000013f9c6 _PyArg_ParseTuple_SizeT()  ???:0 30 0x0000000000235256 PyEval_EvalCode()  ???:0 31 0x0000000000260108 PyUnicode_Tailmatch()  ???:0 32 0x00000000002599cb PyInit__collections()  ???:0 33 0x000000000025fe55 PyUnicode_Tailmatch()  ???:0 34 0x000000000025f338 _PyRun_SimpleFileObject()  ???:0 35 0x000000000025ef83 _PyRun_AnyFileObject()  ???:0 36 0x0000000000251a5e Py_RunMain()  ???:0 37 0x000000000022802d Py_BytesMain()  ???:0 38 0x0000000000029d90 __libc_init_first()  ???:0 39 0x0000000000029e40 __libc_start_main()  ???:0 40 0x0000000000227f25 _start()  ???:0 ================================= E1105 06:18:11.067000 140457179309888 torch/distributed/elastic/multiprocessing/api.py:881] failed (exitcode: 11) local_rank: 0 (pid: 10271) of binary: /usr/bin/python Traceback (most recent call last):   File ""/usr/local/bin/torchrun"", line 8, in      sys.exit(main())   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 347, in wrapper     return f(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/run.py"", line 879, in main     run(args)   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/run.py"", line 870, in run     elastic_launch(   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/launcher/api.py"", line 132, in __call__     return launch_agent(self._config, self._entrypoint, list(args))   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/launcher/api.py"", line 263, in launch_agent     raise ChildFailedError( torch.distributed.elastic.multiprocessing.errors.ChildFailedError: ======================================================= pretrain_gpt.py FAILED  Failures: [1]:   time      : 20241105_06:18:10   host      : fb2a7d718a49   rank      : 1 (local_rank: 1)   exitcode  : 11 (pid: 10272)   error_file:    traceback : Signal 11 (SIGSEGV) received by PID 10272 [2]:   time      : 20241105_06:18:10   host      : fb2a7d718a49   rank      : 2 (local_rank: 2)   exitcode  : 11 (pid: 10273)   error_file:    traceback : Signal 11 (SIGSEGV) received by PID 10273 [3]:   time      : 20241105_06:18:10   host      : fb2a7d718a49   rank      : 3 (local_rank: 3)   exitcode  : 11 (pid: 10274)   error_file:    traceback : Signal 11 (SIGSEGV) received by PID 10274 [4]:   time      : 20241105_06:18:10   host      : fb2a7d718a49   rank      : 4 (local_rank: 4)   exitcode  : 11 (pid: 10275)   error_file:    traceback : Signal 11 (SIGSEGV) received by PID 10275 [5]:   time      : 20241105_06:18:10   host      : fb2a7d718a49   rank      : 5 (local_rank: 5)   exitcode  : 11 (pid: 10276)   error_file:    traceback : Signal 11 (SIGSEGV) received by PID 10276 [6]:   time      : 20241105_06:18:10   host      : fb2a7d718a49   rank      : 6 (local_rank: 6)   exitcode  : 11 (pid: 10277)   error_file:    traceback : Signal 11 (SIGSEGV) received by PID 10277 [7]:   time      : 20241105_06:18:10   host      : fb2a7d718a49   rank      : 7 (local_rank: 7)   exitcode  : 11 (pid: 10278)   error_file:    traceback : Signal 11 (SIGSEGV) received by PID 10278  Root Cause (first observed failure): [0]:   time      : 20241105_06:18:10   host      : fb2a7d718a49   rank      : 0 (local_rank: 0)   exitcode  : 11 (pid: 10271)   error_file:    traceback : Signal 11 (SIGSEGV) received by PID 10271 ``` **Environment (please complete the following information):**   megatronï¼š 3d27a9de61534a0af248b7cf5af6013d93bd52db  imageï¼šnvcr.io/nvidia/pytorch:24.04py3  **Proposed fix** If you have a proposal for how to fix the issue state it here or link to a PR. **Additional context** Add any other context about the problem here.",2024-11-05T06:27:28Z,,closed,0,12,https://github.com/NVIDIA/Megatron-LM/issues/1274,helloï¼Œyou can pull up the latest code and use `tpcommbootstrapbackend nccl` to specific the tp backend. This might help you.,> helloï¼Œyou can pull up the latest code and use `tpcommbootstrapbackend nccl` to specific the tp backend. This might help you. hiï¼Œ  I set bellowï¼š ``` MODEL_PARALLEL_ARGS=(         tensormodelparallelsize 2         pipelinemodelparallelsize 2         useflashattn         sequenceparallel         overlapgradreduce         recomputeactivations         recomputegranularity selective         tpcommbootstrapbackend nccl         tpcommoverlap ) ``` and I pull  the latest repoï¼Œthe same error,How about trying the newest TE?  You log shows `Transformer Engine v1.5.0+6a9edc3 supports only MPI bootstrap backend.`,> How about trying the newest TE? You log shows `Transformer Engine v1.5.0+6a9edc3 supports only MPI bootstrap backend.` sorryï¼ŒI have set `transformerimpl local`ï¼Œerror still,Maybe the local implementation does not support tpoverlap. I strongly suggest you to use TE.,"> Maybe the local implementation does not support tpoverlap. I strongly suggest you to use TE. hi ï¼Œ  I am sorry for the late replyï¼ŒI update TE as followï¼š ``` pip install U transformerengine ``` then coming the new error when I start to trainï¼š ``` Traceback (most recent call last):   File ""/usr/local/lib/python3.10/distpackages/transformer_engine/pytorch/__init__.py"", line 52, in _load_library     so_path = next(so_dir.glob(f""{module_name}.*.{extension}"")) StopIteration During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""/home/data/MegatronLM/pretrain_gpt.py"", line 11, in      from megatron.training import get_args   File ""/home/data/MegatronLM/megatron/training/__init__.py"", line 5, in      from .global_vars import get_args   File ""/home/data/MegatronLM/megatron/training/global_vars.py"", line 9, in      from megatron.core import Timers   File ""/home/data/MegatronLM/megatron/core/__init__.py"", line 2, in      import megatron.core.tensor_parallel   File ""/home/data/MegatronLM/megatron/core/tensor_parallel/__init__.py"", line 2, in      from .cross_entropy import vocab_parallel_cross_entropy   File ""/home/data/MegatronLM/megatron/core/tensor_parallel/cross_entropy.py"", line 7, in      from megatron.core.parallel_state import (   File ""/home/data/MegatronLM/megatron/core/parallel_state.py"", line 14, in      from .utils import GlobalMemoryBuffer   File ""/home/data/MegatronLM/megatron/core/utils.py"", line 26, in      from megatron.core.dist_checkpointing.mapping import ShardedTensor   File ""/home/data/MegatronLM/megatron/core/dist_checkpointing/__init__.py"", line 5, in      from .serialization import (   File ""/home/data/MegatronLM/megatron/core/dist_checkpointing/serialization.py"", line 27, in      from .state_dict_transformation import load_preprocess, save_preprocess   File ""/home/data/MegatronLM/megatron/core/dist_checkpointing/state_dict_transformation.py"", line 12, in      from .exchange_utils import determine_main_replica_uniform_distribution, exchange_by_distribution   File ""/home/data/MegatronLM/megatron/core/dist_checkpointing/exchange_utils.py"", line 24, in      from transformer_engine.pytorch.float8_tensor import Float8Tensor   File ""/usr/local/lib/python3.10/distpackages/transformer_engine/pytorch/__init__.py"", line 63, in      _load_library()   File ""/usr/local/lib/python3.10/distpackages/transformer_engine/pytorch/__init__.py"", line 55, in _load_library     so_path = next(so_dir.glob(f""{module_name}.*.{extension}"")) StopIteration Traceback (most recent call last):   File ""/usr/local/lib/python3.10/distpackages/transformer_engine/pytorch/__init__.py"", line 52, in _load_library     so_path = next(so_dir.glob(f""{module_name}.*.{extension}"")) StopIteration ``` that is to say there is not a outofthebox image for megatronï¼Ÿ Thanksï¼","transformer engine is another repo. you can find it here, https://github.com/NVIDIA/TransformerEngine","> transformer engine is another repo. you can find it here, https://github.com/NVIDIA/TransformerEngine yesï¼Œyou are rightï¼ I update the TE by commandï¼š ` pip install U transformer_engine[pytorch]` if I use fp16, it will meet error: ``` megatron.core.utils:Expected dtype=torch.float16, but found A.dtype=torch.float16 and B.dtype=torch.bfloat16 ``` then I  use bf16, and bf16 is ok finally! and moreï¼Œfor the same configï¼Œuse tpcomm will cause oom","TPcomm will allocate more buffer, which is constant times hidden_states. This may cause your oom.","> TPcomm will allocate more buffer, which is constant times hidden_states. thanksï¼I will go deeper to learn this part I have 6 H100 smx nowï¼Œand I want to test the training throughout as the table shows in the readmeï¼š !Image Could you share the parameters config and env setting that can reproduce the resultï¼ŸI can only get half of the MFU~","Sorry, I can not help you with that. The script in examples may help you? I'm not sure about this table. ","> Sorry, I can not help you with that. The script in examples may help you? I'm not sure about this table. Thank you very muchï¼ very kind of youï¼"
huvunvidia,Huvu/update t5 attentionmasktype,,2024-11-04T15:08:20Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1273,Marking as stale. No activity in 60 days.
zixianwang2022,[QUESTION] How to Visualize Computational Graph,"**Your question** Hi,  Is there a way I can visualize the computational graph during backward propagation in Megatron? ",2024-11-02T18:39:15Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1272,Marking as stale. No activity in 60 days.
huvunvidia,Update t5_model.py,,2024-11-02T17:18:49Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1271,Marking as stale. No activity in 60 days.
wdevazelhes,[ENHANCEMENT] Add z-loss,"zloss regularization was shown to stabilize training by preventing logits in the last layer to explode: https://arxiv.org/abs/2309.14322 This PR:   allows one to add zloss regularization  since the total loss is not cross entropy anymore but cross entropy + zloss, this PR also allows one to additionally log the original crossentropy and the zloss",2024-11-01T11:01:07Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1270,"After more careful consideration, seems it could be better to implement zloss similarly to the zloss for MOE, be it for the loss itself, but also for the logging :+1: "
shmily326,[BUG] The `cached_loss_mask` maybe modified unexpectedly in GPTDataset?,"I encountered a problem in pretrain that the `lm loss` become 0.0 after hundreds iterations and hold then, but there was no nan/inf/skip iteration according to the train log. https://github.com/NVIDIA/MegatronLM/blob/2e2bdf62382f3a77f5bd020cde51ed833ee62ead/megatron/core/datasets/gpt_dataset.pyL183L207 I'm wondering whether `loss_mask` may modify `self.cached_loss_mask` unexpectedly in Line206 (since we want to cache loss mask, but `loss_mask` is just a reference to the original tensor), which finally results in zeros accumulated in `self.cached_loss_mask` and an allzero `loss_mask`ï¼Ÿ",2024-11-01T03:42:27Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1269,Marking as stale. No activity in 60 days.
msiddaiah,Enable huggingface tokenizer,,2024-10-30T15:38:33Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1268,Marking as stale. No activity in 60 days.
FortuneBush,[BUG] build multimodal dockerfile problem,!Image I have met problem when I built the multimodal image by the dockerfile provided. the command have problems is below. can you help me solve this problem? I will appreciate it. RUN pip install verbose git+https://github.com/fanshiqing/grouped_gemm !Image !Image !Image !Image !Image !Image,2024-10-30T06:38:33Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1267,Marking as stale. No activity in 60 days.
KookHoiKim,[QUESTION] How to use loader_mcore and why it requires torch distributed,"I trained model while setting 'args.ckpt_format = torch_dist', and the checkpoint files saved like '__0_.distcp, ..., common.pt, metadata.json'.  When i resume training, load_checkpoint works well.  However, i try to convert my checkpoint using loader_mcore, the checkpoint is not loaded occuring errors below: ``` Traceback (most recent call last):   File ""/workspace/code/MegatronLM/tools/checkpoint/convert.py"", line 160, in      main()   File ""/workspace/code/MegatronLM/tools/checkpoint/convert.py"", line 153, in main     loader.load_checkpoint(queue, args)   File ""/workspace/code/MegatronLM/tools/checkpoint/loader_mcore.py"", line 384, in load_checkpoint     _load_checkpoint(queue, args)   File ""/workspace/code/MegatronLM/tools/checkpoint/loader_mcore.py"", line 246, in _load_checkpoint     all_models = [get_models(tp_size, md.params_dtype)]   File ""/workspace/code/MegatronLM/tools/checkpoint/loader_mcore.py"", line 164, in get_models     load_checkpoint(model_, None, None)   File ""/workspace/code/MegatronLM/megatron/training/checkpointing.py"", line 1094, in load_checkpoint     state_dict, checkpoint_name, release, ckpt_type = _load_base_checkpoint(   File ""/workspace/code/MegatronLM/megatron/training/checkpointing.py"", line 850, in _load_base_checkpoint     return _load_global_dist_base_checkpoint(   File ""/workspace/code/MegatronLM/megatron/training/checkpointing.py"", line 778, in _load_global_dist_base_checkpoint     state_dict = dist_checkpointing.load(sharded_state_dict, checkpoint_name, load_strategy, strict=args.dist_ckpt_strictness)   File ""/workspace/code/MegatronLM/megatron/core/dist_checkpointing/serialization.py"", line 126, in load     local_metadata, global_metadata = determine_global_metadata(sharded_state_dict)   File ""/workspace/code/MegatronLM/megatron/core/dist_checkpointing/validation.py"", line 496, in determine_global_metadata     global_metadata = [None] * torch.distributed.get_world_size()   File ""/miniforge3/envs/megatron/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1832, in get_world_size     return _get_group_size(group)   File ""/miniforge3/envs/megatron/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 864, in _get_group_size     default_pg = _get_default_group()   File ""/miniforge3/envs/megatron/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1025, in _get_default_group     raise ValueError( ValueError: Default process group has not been initialized, please make sure to call init_process_group. ``` It requires torch.distributed.is_initialized, but convert.py , or loader_mcore does not include initialization of distribution.  Does it really need distributed.initialization or i do something wrong? ",2024-10-29T11:44:02Z,stale,open,5,2,https://github.com/NVIDIA/Megatron-LM/issues/1266," , you are correct that `tools/checkpoint` does not support converting distributed checkpoints. If you'd like to convert a distributed checkpoint via `tools/checkpoint`, there's a two step process:  Convert from `torch_dist` to `torch` format. This conversion is done by launching a slightly modified version of your normal training script, but with two arguments added (and leaving all other args the same). The new checkpoint is saved and the system exits before doing any training iterations:    `ckptconvertformat torch`: This sets the format for saving the new checkpoint, and we want it to be `torch` format.    `ckptconvertsave ${PATH_TO_TORCH_CKPT}`: This path should be different than your existing `load/save` path, to avoid overwriting your existing checkpoint. This path will also be used for loading in the next step.  Convert using `loader_mcore.py`. For this step we use the path to the newly saved checkpoint by doing `loaddir ${PATH_TO_TORCH_CKPT}`. Please let me know if you have any questions about this.",Marking as stale. No activity in 60 days.
singleheart,fix: remove unnecessary trailing comma in statement," Changes Removed an unnecessary trailing comma at the end of a statement, which was causing a runtime error.  Reason for Change The extra comma resulted in a syntax error, interrupting code execution. This fix corrects the error and ensures the code runs as expected.  Impact  Resolves a runtime error directly affecting code functionality. Please review the changes. Thank you!",2024-10-29T05:58:17Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1265,Marking as stale. No activity in 60 days.
jindajia,Jinda/legal review,,2024-10-28T19:52:08Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1264
dhia680,[ENHANCEMENT] Enabling LR scaling for a specific layer (ex. down-projection...) during pretraining,"**Is your feature request related to a problem? Please describe.** It's currently not possible to scale the learning rate for a specific layer (except `head`) without adding a new argument such as `headlrmult`. **Describe the solution you'd like** This PR enables scaling the learning rate of a layer during pretraining by giving its **name** in `scalelrlayer` and the **multiplier** in `lrmultiplier` by using the existing internal logic of `scale_l_cond` and `lr_mult`. **Describe alternatives you've considered** This implementation generalizes/makes more flexible the existing use of this feature for **lm `head` during finetuning** by making it possible to specify the **name of the target layer** as well as the **LR multiplier**.  Extends its use for pretraining as well. When no layer is specified, the `scale_lr_cond` argument is `None` and no lrscaling is applied. **Proposed implementation** Here is the PR. **Additional context** MuP and several interesting papers that followed (ex. DepthMuP) suggest, among other technics s.a layers' output scaling and initializations, to use **different LRs depending on width** in order to enhance feature learning and avoid that output layers dominate the learning process. When combined with proper initializations and layers' output scaling, it consists of a stable setting especially for sweeping and scaling hyperparameters for pretraining. !MuPsetting A GPT like model typically has an ffnfactor > 1. It's 3.5 for Llama3.1 70B. Which suggests that **downprojection** (`linear_fc2` in Megatron) requires a lower LR. Theoretically `LR x 1/ffn_factor`.  This way, we don't have to add a new argument (ex. `downprojlrmult`) each time we want to test scaling of a certain layer (ex. `linear_fc2`). **P.S**:  Layers' output scaling (before residualconnections) as introduced in DepthMuP to account for depthscaling will be suggested in a separate PR. Same for init.",2024-10-28T12:05:52Z,stale,open,2,1,https://github.com/NVIDIA/Megatron-LM/issues/1263,Marking as stale. No activity in 60 days.
dhia680,Enabling LR scaling for a specific layer (ex. down-projection...) during pretraining,"This PR enables scaling the learning rate of a layer by giving its **name** in `scalelrlayer` and the **multiplier** in `lrmultiplier` by using the existing internal logic of `scale_l_cond` and `lr_mult`.   **Motivation**:  MuP and several interesting papers that followed (ex. DepthMuP) suggest, among other technics s.a layers' output scaling and initializations, to use **different LRs depending on width** in order to enhance feature learning and avoid that output layers dominate the learning process. When combined with proper initializations and layers' output scaling, it consists of a stable setting especially for sweeping and scaling hyperparameters for pretraining. !MuPsetting   **Implementation**:   Generalizes/makes more flexible the existing use of this feature for **lm `head` during finetuning** by making it possible to specify the **name of the target layer** as well as the **LR multiplier**.  Extends its use for pretraining as well. When no layer is specified, the `scale_lr_cond` argument is `None` and no lrscaling is applied.   **Why?**:  A GPT like model typically has an ffnfactor > 1. It's 3.5 for Llama3.1 70B. Which suggests that **downprojection** (`linear_fc2` in Megatron) requires a lower LR. Theoretically `LR x 1/ffn_factor`.  This way, we don't have to add a new argument (ex. `downprojlrmult`) each time we want to test scaling of a certain layer (ex. `linear_fc2`). **P.S**:  Layers' output scaling (before residualconnections) as introduced in DepthMuP to account for depthscaling will be suggested in a separate PR. Same for init.",2024-10-28T11:53:46Z,,open,1,7,https://github.com/NVIDIA/Megatron-LM/issues/1262,Any updates ? Here is the related issue .,"I think this is a really nice and important addition to the code base, which is also emphasized by further research on other parametrizations (https://arxiv.org/abs/2407.05872). However, I think the implementation would ideally be even more flexible and allow specification of multiple layers and learning rates to support all current and future use cases (see, e.g., pageÂ 3, TableÂ 1 in the linked paper). Aside from that, I think it would be helpful to make the change backwardcompatible. I.e., keep the old `headlrmult` argument, but mark it as deprecated. Then in the argument parsing function, do something like: ```python if args.head_lr_mult != 1.0:     warnings.warn(         'headlrmult is deprecated; please use the'         'scalelrlayer and lrmultiplier arguments instead.'     )     assert args.scale_lr_layer is None and args.lr_multiplier == 1.0, \         'cannot set scalelrlayer or lrmultiplier when headlrmult is given.'     args.scale_lr_layer = 'head'     args.lr_multiplier = args.head_lr_mult ```","Thanks . I have 3 points to discuss before suggesting a better version. 1. I thought of enabling multilayer lr scaling and had a version that does so in a simple way.  But without a deeper change in the codebase (of how lrmult is used in megatron/core/optimizer/__init__.py to create `param_groups`), this version would be limited to the case of scaling all those mentionned layers with the same `lrmult`. 2. Another reason why this multilayer lr scaling suggestion could be questionned, is that there is an equivalent `decoupled_lr` logic specific for [`head` and `embeddings`] in `Megatron`. So, one could choose different lr for these layers with `decoupledlr` and scale `linear_fc2` lr with `lrmultiplier`, while being able to have 3 different LRs in total. Not just 2. 3. I agree. It's better to keep the old `headlrmult` argument and handle its use. This makes me also think of the neccessity to handle with an assertion, the case where both `decoupledlr` and `scalelrlayer 'head'` or `'embedding'` are used.  What do you think?    Enabling scaling of many layers' LRs by same factor.   Or scaling many layers with many factors. And in this case, mark any use of `decoupledlr` as deprecated.  Let me tag  (a maintainer).",Marking as stale. No activity in 60 days.,Hope it gets another look!,Marking as stale. No activity in 60 days.,Hope it gets another look!
wdevazelhes,[ENHANCEMENT] Add support for Apex RMSNorm for use in qk-norm,"This PR allows to use `qklayernorm` even when one has set normalization: `""RMSNorm""` (which threw an error before cf. msg from  here from original PR here: indeed, only `LayerNorm` was allowed when using qklayernorm in gpt, not `RMSNorm`, since this commit) **What this PR does is that it:**  **adds `FusedRMSNorm` in `megatroncore/fusions/fused_layer_norm.py`**, which will serve as a wrapper to apex's `FusedRMSNormAffineFunction` (the same way as the existing `megatroncore/fusions/fused_layer_norm.py:FusedLayerNorm` is a wrapper to apex's `FusedLayerNormAffineFunction`) (note that while `FusedLayerNorm` can also support for the persist version of fused layer norm (apex's `contrib.layer_norm.FastLayerNorm`), there's no such persist version in Apex for RMSNorm so we just use a nonpersist version)  **adds a wrapper similar to `TENorm` but for Apex, which we call `ApexNorm`** (`TENorm` is a wrapper that gets transformed into either `te.pytorch.LayerNorm` or `te.pytorch.RMSNorm` depending on whether normalization: 'RMSNorm' or normalization: 'LayerNorm' is used in the config). For that we add an `ApexFusedNorm` which gets transformed into either `megatroncore/fusions/fused_layer_norm.py:FusedLayerNorm`, or the `fused_layer_norm.py:RMSNorm` just added above **Advantage:** this way if we specify `LayerNorm` or `RMSNorm` for `normalization`, we'll use that for qknormalization, and it'll try using first the Apex one, and fallback to the python one if Apex is not installed (we don't use the TE one as it seems to be unstable as was put in comments in the original code and as was forced by this commit) _Note_: for the implementation of `FusedRMSNorm` I just copy/pasted the code from `FusedLayerNorm` and changed it to be doing an RMSNorm Tagging  and  as you could be interested in this PR given that PR of yours. Tagging  and barker, also the authors of this commit could be interested (Mike Chrzanowski and Shanmugam Ramasamy). ",2024-10-28T08:06:37Z,stale,open,1,1,https://github.com/NVIDIA/Megatron-LM/issues/1261,Marking as stale. No activity in 60 days.
puneeshkhanna,Add support to process gzip files,,2024-10-28T08:03:21Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1260,Marking as stale. No activity in 60 days.
efsotr,[BUG] Flash attention cannot be applied by passing the --use-flash-attn flag when the --use-mcore-models flag is also passed,"Passing the useflashattn flag is intended to enable flash attention; however, when the usemcoremodels flag (to use the transformer engine) is also specified, flash attention will not be applied.",2024-10-26T12:38:06Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1259,"TransformerEngine has its own logic to select attention backends. Generally speaking, if all the conditions match, such as q/k/v shape and layout, attention bias, sliding window, and so on, TE prefers FlashAttention v2 on preHopper GPUs (SM = 90). If you want to know why a certain attention backend is selected or disabled, you can set the following env vars to enable TE's logging. ``` NVTE_DEBUG=1 NVTE_DEBUG_LEVEL=2 ```",Marking as stale. No activity in 60 days.
hwang595,[BUG] MoE pre-training does not scale beyond DP dim>8,"**Describe the bug** I used {2, 4, 8, 16, 32} 8xH100 DGX nodes to run Mixtral style MoE pretraining following the Megatron MoE guidance. The model size is roughly 8x3B, and I only used EP and DP for scaling it up. However, when fixing `EP=8` and scaling `DP={2, 4, 8, 16, 32}` (please see the detailed script below). I observed the speed does not improve after `DP=8`. I'm reporting the runtime results below: `DP=2, EP=8` ```  [20241025 04:08:43] iteration        4/  365000  ``` I can of course start introducing `PP>1` as described in the MoE Doc, but not being able to scale the DP dimension beyond `8` still seems to be an issue for me. Any help would be appreciated! **To Reproduce** Below is my script ``` export CUDA_DEVICE_MAX_CONNECTIONS=1 GPUS_PER_NODE=8 NNODES=16 nodes=( $( scontrol show hostnames $SLURM_JOB_NODELIST ) ) nodes_array=($nodes) head_node=${nodes_array[0]} head_node_ip=$(srun nodes=1 ntasks=1 w ""$head_node"" hostname ipaddress) echo Node IP: $head_node_ip echo $SLURM_JOB_NODELIST export LOGLEVEL=INFO DISTRIBUTED_ARGS=(     nproc_per_node $GPUS_PER_NODE     nnodes $NNODES     rdzv_id $RANDOM     rdzv_backend c10d     rdzv_endpoint $head_node_ip:29500 ) MODEL_ARGS=(     disablebiaslinear     seqlength 2048     maxpositionembeddings 32768     numlayers 36     hiddensize 2304     ffnhiddensize 7680     numattentionheads 36     initmethodstd 0.01   need to check source code     attentiondropout 0.0     hiddendropout 0.0     normalization RMSNorm     positionembeddingtype rope     rotarybase 200000     swiglu     untieembeddingsandoutputweights     nopositionembedding ) MOE_ARGS=(     numexperts 8     expertmodelparallelsize 8     moeroutertopk 2     moegroupedgemm  no big influence     moetokendispatchertype alltoall     moerouterloadbalancingtype aux_loss  options: aux_loss, sinkhorn, None. Default is aux_loss.     moeauxlosscoeff 1e2     moezlosscoeff 1e3     moeexpertcapacityfactor 1.0     moetokendroppolicy probs ) DATA_ARGS=(     tokenizertype ${TOKENIZER_TYPE}     tokenizermodel ${TOKENIZER_MODEL}     datapath $DATA_PATH     split 99990,8,2 ) TRAINING_ARGS=(     microbatchsize 2     globalbatchsize 2048     seed 42     lr 3e6  3e4     adambeta1 0.9     adambeta2 0.95     trainiters 365000      lrdecayiters 330000     lrdecaystyle cosine     minlr 3e7     lrwarmupiters 2000     weightdecay 0.1     clipgrad 1.0     bf16     overlapparamgather     overlapgradreduce ) MODEL_PARALLEL_ARGS=(     tensormodelparallelsize 1     pipelinemodelparallelsize 1     sequenceparallel     usedistributedoptimizer     distributedtimeoutminutes 60 ) LOGGING_ARGS=(     loginterval 2     saveinterval 50000     tensorboarddir ""${CHECKPOINT_PATH}/tensorboard""     logmemorytotensorboard      logparamsnorm     logtimerstotensorboard ) CHECKPOINT_ARGS=(     evalinterval 1000     evaliters 5     save $CHECKPOINT_PATH     load $CHECKPOINT_PATH ) srun containermounts=MOUNTDIRS containername=CONTAINERNAME \     containerimage=IMAGENAME \     torchrun ${DISTRIBUTED_ARGS[@]} $WORK_DIR/pretrain_gpt.py \     ${MODEL_ARGS[@]} \     ${MOE_ARGS[@]} \     ${DATA_ARGS[@]} \     ${TRAINING_ARGS[@]} \     ${MODEL_PARALLEL_ARGS[@]} \     ${CHECKPOINT_ARGS[@]} \     ${LOGGING_ARGS[@]} ``` **Expected behavior** Training throughput scale along the `DP` dim when `DP>8`. **Stack trace/logs** N/A **Environment (please complete the following information):** NGC official PyTorch Container `24.09` `nvcr.io/nvidia/pytorch:24.09py3` **Proposed fix** N/A",2024-10-25T15:20:33Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1258,Marking as stale. No activity in 60 days.
clarence-lee-sheng,[QUESTION] NVIDIA Megatron Core 0.9.0 does not have shared_experts.py,"Dear MegatronLM maintainers,  in your 0.9.0 release, you mentioned that shared experts support was released, however, there is currently no shared_experts.py file in mcore r0.9.0 version, may I ask if it is currently missing from your 0.9.0 release? ",2024-10-25T08:37:45Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/1257,Shared expert is in the current main branch but not included in 0.9.0 release branch. The release note results in confusing.,"In megatron/core/transformer/moe/README.md in main branch it was also mentioned that ""MCore v0.9 introduced the shared expert feature. We can enable this feature by setting suitable moesharedexpertintermediatesize."". May I ask if this will be eventually added into mcore0.9.0 or another release? ",No. MCore v0.9 has been finalized and released. It's going to be included in the next release.
sbmaruf,[QUESTION] Effect of sequence parallel with dropout rng context,"Looking back at the recent release of mcore 0.9.0: ``` Known Issue: When using sequence parallel, during the transformer block forward pass, dropout is not using the appropriate rng context. ``` Do you know from which version this breaking change occurred? What are the effects of this issue during training?  ",2024-10-24T14:47:44Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1256,"Hey , this issue is due to this line using ` and ` instead of `, ` to separate the rng and fp8 contexts. It's been like this since the introduction of the fp8 context in June 2023. Since we don't use dropout in many models that we train internally we haven't studied the impact this has on training.",Thanks for reply. 
ZihaoZheng98,[QUESTION]Transformer Engine is totally a shit.,**Your question** Ask a clear and concise question about MegatronLM. I have never seen a pip package that is so hard to install. Must you use Transformer Engine?,2024-10-23T09:11:02Z,,closed,3,1,https://github.com/NVIDIA/Megatron-LM/issues/1239,sameï¼ï¼ï¼ I hate transformer engine.
wplf,"[QUESTION]  Do tp overlap support thd, whose sequence length is flexible?","Hi, thank you for great works. I have a question about tpoverlap.  The function below make a buffer for `args.seq_length * args.micro_batch_size`. Do this support thd format? ```python def _initialize_tp_communicators():     """""" initializing the communicators with user buffers for highperformance tensormodelparallel          communication overlap """"""     try:        import yaml        import transformer_engine        from transformer_engine.pytorch import module as te_module     except ImportError:        raise RuntimeError(""Tensor Parallel Communication/GEMM Overlap optimization needs 'yaml' and ""              ""'transformer_engine' packages"")      args = get_args()     if args.tp_comm_overlap_cfg is not None:        with open(args.tp_comm_overlap_cfg,""r"") as stream:               ub_cfgs = yaml.safe_load(stream)     else:        ub_cfgs = {}     input_shape = [(args.seq_length * args.micro_batch_size) // args.context_parallel_size , args.hidden_size]     We create a MPI process group, which is needed to bootstrap the pipelined      tensormodelparallel communication overlap     torch.distributed.new_group(backend='nccl')     te_module.base.initialize_ub(shape = input_shape, tp_size = args.tensor_model_parallel_size,                                   use_fp8 = (args.fp8 is not None) , ub_cfgs = ub_cfgs,) ``` Follow this question, I have  found that after TP/SP mlp layer, the output shape is exactly `seqlen, args.hidden_size`. So how does that works for qkv_proj `hidden_dim * 3/ tp_size` and mlp `hidden_dim * 2 / tp_size`? ",2024-10-23T07:44:49Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1238
yanchenmochen,"[QUESTION]Using FP8 OOM, otherwise --bf16 works well","When I train a 7B model on H100 GPU using FP8, it turns out OOM, while the same parameters using  bf16 can be trained fine, what is the possible problem? ```        bf16\        fp8format hybrid \        transformerimpl transformer_engine \        fp8amaxhistorylen 1\        fp8amaxcomputealgo max \ ``` I tried to reduce memory by recomputegranularity selective, but it failed. the error info is  ``` [default3]:[rank3]: Traceback (most recent call last): [default3]:[rank3]:   File ""/mnt/nfs131/zhongziban/zhangyi/MegatronLM/pretrain_gpt.py"", line 245, in  [default3]:[rank3]:     pretrain( [default3]:[rank3]:   File ""/mnt/nfs131/zhongziban/zhangyi/MegatronLM/megatron/training/training.py"", line 301, in pretrain [default3]:[rank3]:     iteration, num_floating_point_operations_so_far = train( [default3]:[rank3]:   File ""/mnt/nfs131/zhongziban/zhangyi/MegatronLM/megatron/training/training.py"", line 1115, in train [default3]:[rank3]:     train_step(forward_step_func, [default3]:[rank3]:   File ""/mnt/nfs131/zhongziban/zhangyi/MegatronLM/megatron/training/training.py"", line 612, in train_step [default3]:[rank3]:     losses_reduced = forward_backward_func( [default3]:[rank3]:   File ""/mnt/nfs131/zhongziban/zhangyi/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 392, in forward_backward_no_pipelining [default3]:[rank3]:     output_tensor, num_tokens = forward_step( [default3]:[rank3]:   File ""/mnt/nfs131/zhongziban/zhangyi/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 217, in forward_step [default3]:[rank3]:     output_tensor, loss_func = forward_step_func(data_iterator, model) [default3]:[rank3]:   File ""/mnt/nfs131/zhongziban/zhangyi/MegatronLM/pretrain_gpt.py"", line 174, in forward_step [default3]:[rank3]:     output_tensor = model(tokens, position_ids, attention_mask, [default3]:[rank3]:   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl [default3]:[rank3]:     return self._call_impl(*args, **kwargs) [default3]:[rank3]:   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1561, in _call_impl [default3]:[rank3]:     result = forward_call(*args, **kwargs) [default3]:[rank3]:   File ""/mnt/nfs131/zhongziban/zhangyi/MegatronLM/megatron/core/distributed/distributed_data_parallel.py"", line 204, in forward [default3]:[rank3]:     return self.module(*inputs, **kwargs) [default3]:[rank3]:   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl [default3]:[rank3]:     return self._call_impl(*args, **kwargs) [default3]:[rank3]:   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1561, in _call_impl [default3]:[rank3]:     result = forward_call(*args, **kwargs) [default3]:[rank3]:   File ""/mnt/nfs131/zhongziban/zhangyi/MegatronLM/megatron/legacy/model/module.py"", line 190, in forward [default3]:[rank3]:     outputs = self.module(*inputs, **kwargs) [default3]:[rank3]:   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl [default3]:[rank3]:     return self._call_impl(*args, **kwargs) [default3]:[rank3]:   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1561, in _call_impl [default3]:[rank3]:     result = forward_call(*args, **kwargs) [default3]:[rank3]:   File ""/mnt/nfs131/zhongziban/zhangyi/MegatronLM/megatron/core/models/gpt/gpt_model.py"", line 212, in forward [default3]:[rank3]:     loss = self.compute_language_model_loss(labels, logits) [default3]:[rank3]:   File ""/mnt/nfs131/zhongziban/zhangyi/MegatronLM/megatron/core/models/common/language_module/language_module.py"", line 40, in compute_language_model_loss [default3]:[rank3]:     loss = tensor_parallel.vocab_parallel_cross_entropy(logits, labels) [default3]:[rank3]:   File ""/mnt/nfs131/zhongziban/zhangyi/MegatronLM/megatron/core/tensor_parallel/cross_entropy.py"", line 232, in vocab_parallel_cross_entropy [default3]:[rank3]:     return _VocabParallelCrossEntropy.apply(vocab_parallel_logits, target, label_smoothing) [default3]:[rank3]:   File ""/usr/local/lib/python3.10/distpackages/torch/autograd/function.py"", line 572, in apply [default3]:[rank3]:     return super().apply(*args, **kwargs)   type: ignore[misc] [default3]:[rank3]:   File ""/mnt/nfs131/zhongziban/zhangyi/MegatronLM/megatron/core/tensor_parallel/cross_entropy.py"", line 122, in forward [default3]:[rank3]:     vocab_parallel_logits, logits_max = VocabParallelCrossEntropy.calculate_logits_max( [default3]:[rank3]:   File ""/mnt/nfs131/zhongziban/zhangyi/MegatronLM/megatron/core/tensor_parallel/cross_entropy.py"", line 27, in calculate_logits_max [default3]:[rank3]:     vocab_parallel_logits = vocab_parallel_logits.float() [default3]:[rank3]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 3 has a total capacity of 79.11 GiB of which 1.96 GiB is free. Including nonPyTorch memory, this process has 77.13 GiB memory in use. Of the allocated memory 72.50 GiB is allocated by PyTorch, and 889.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.htmlenvironmentvariables) ```",2024-10-23T07:11:21Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1237
yanchenmochen,Megatron FP8 training is compatible with recompute?,"**Your question** Ask a clear and concise question about MegatronLM. when I tried run the training using fp8 parameters, OOM occurs. the model is 7B model. same parameter is valid for bf16. ```     bf16 \     fp8format hybrid \     fp8amaxcomputealgo max \     fp8amaxhistorylen 16 \     transformerimpl transformer_engine ``` How to set he correct fp8 parameter? I tried to reduce memory usage to avoid OOM by usingrecalculategranularity selective, but failed, still OOM",2024-10-23T02:53:30Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1236
marcromeyn,Add fallbacks for c++ extension + jit_fuser,"In NeMoRun we offer to configure an experiment locally but execute it remotely through a wide variety of different executors. For the best UX we require mcore to be able to be installed locally (eventhough we don't intend to train models locally). Currently there are some issues to make a pipinstall fail. 1. The C++ extension, I propose to log a warning when this fails but don't fail the entire installation. 2. The `jit_fuser` can fail on newer python installs. I propose to make it a noop when it fails. Current error is: ```   from megatron.core.transformer.utils import (   File ""/Users/mromeijn/base/code/.venv/lib/python3.12/sitepackages/megatron/core/transformer/utils.py"", line 40, in            ^^^^^^^^^   File ""/Users/mromeijn/base/code/.venv/lib/python3.12/sitepackages/lightning_fabric/wrappers.py"", line 411, in _capture     return compile_fn(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Users/mromeijn/base/code/.venv/lib/python3.12/sitepackages/torch/__init__.py"", line 1868, in compile     raise RuntimeError(""Dynamo is not supported on Python 3.12+"") RuntimeError: Dynamo is not supported on Python 3.12+ ```",2024-10-22T12:53:52Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1235,Resolved it offline
siriusctrl,[BUG] Cannot Save mamba model in distributed training,"**Describe the bug** While saving mamba based model, distributed optimizer report an error in validation about `dt_bias` **To Reproduce** Start the training of Mamba, and run it for a few step **Expected behavior** The checkpoint should be saved without a problem **Stack trace/logs** ```text   from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor [ERROR    | megatron.core.dist_checkpointing.validation]: Invalid access pattern for ShardedTensor(key='decoder.layers.channel_mixing.mixer.dt_bias', dtype=torch.bfloat16, local_shape=(128,), global_shape=(2, 128), global_offset=(0, 0), axis_fragmentations=(2, 1), replica_id=(0, 0, 0), prepend_axis_num=1, allow_shape_mismatch=False, flattened_range=None): tensor([[1], ``` **Environment (please complete the following information):**   MegatronLM commit ID: 6e4e9df20bdf8fadc4ecb79a51944adfde38ab99   PyTorch version 12.6   CUDA version 12.6   NCCL version 2.22.3 **Proposed fix** No proposed fix **Additional context** No additional context",2024-10-22T10:57:38Z,stale,open,1,13,https://github.com/NVIDIA/Megatron-LM/issues/1234,Please will you provide a minimal script to repro this?,"Also, please paste the rest of the error (I think there will be one more continuation line for `tensor([[1], ...`)","Have you solved the issue? I met a similar problem when training: megatron.core.dist_checkpointing.validation:Invalid access pattern for ShardedTensor(key='decoder.layers.15.mlp.linear_fc1.layer_norm_weight', dtype=torch.bfloat16, local_shape=(1024,), global_shape=(1024,), global_offset=(0,), axis_fragmentations=(1,), replica_id=(0, 0, 0), prepend_axis_num=0, allow_shape_mismatch=False, flattened_range=None): tensor([2], dtype=torch.int32)","If the model layers are heterogeneous, one possible reason is missing the configuration of `non_homogeneous_layers`",You mean in the mamba_block function?,"The `.15.` in the `key` suggests that the heterogeneous setup is used correctly. The `tensor([2], dtype=torch.int32)` access pattern means that for some reason there are 2 main replicas (`replica_id=(0, 0, 0)`) for the `decoder.layers.15.mlp.linear_fc1.layer_norm_weight` tensor across the training processes, not sure why could that be though, it would need to be checked (e.g. by adding a print `if sharding.key == 'decoder.layers.15.mlp.linear_fc1.layer_norm_weight': print(rank)` here to check which ranks hold them)","> If the model layers are heterogeneous, one possible reason is missing the configuration of `non_homogeneous_layers` Can you provide some detail expalnation about this? I think this might be the problem","> > If the model layers are heterogeneous, one possible reason is missing the configuration of `non_homogeneous_layers` >  > Can you provide some detail expalnation about this? I think this might be the problem You can set `non_homogeneous_layers = True` here first to make sure whether your problem is caused by this. If so, just refer to it to config `non_homogeneous_layers`. ",`transformer_block.py` is not used for the hybrid models. The equivalent of this would be `core/ssm/mamba_block.py`: https://github.com/NVIDIA/MegatronLM/blob/14ca285dcc8b6862fbb992ef8de57402479c0e9f/megatron/core/ssm/mamba_block.py,"Same issue. [ERROR    | megatron.core.dist_checkpointing.validation]: Invalid access pattern for ShardedTensor(key='decoder.layers.0.time_mixing_beta', dtype=torch.bfloat16, local_shape=(1024,), global_shape=(1024,), global_offset=(0,), axis_fragmentations=(1,), replica_id=(0, 0, 0), prepend_axis_num=0, allow_shape_mismatch=False, flattened_range=None): tensor([2], dtype=torch.int32)","Referencing 's comment above, `key=decoder.layers.0.time_mixing_beta`. Note that a key in our released code would look something like `decoder.layers.0.mixer.dt_bias`. The final name in the key being the name of the stored trainable variable. In your case, it looks like you've added a new trainable variable to the model at the `mamba_layer.py` level. My understanding is that for distributed checkpointing to work the `sharded_state_dict` method must be overridden correctly at that level of the model hierarchy. See `mamba_mixer.py` for reference. Please confirm or correct, ","> Referencing [](https://github.com/LDH007)'s comment above, `key=decoder.layers.0.time_mixing_beta`. Note that a key in our released code would look something like `decoder.layers.0.mixer.dt_bias`. The final name in the key being the name of the stored trainable variable. In your case, it looks like you've added a new trainable variable to the model at the `mamba_layer.py` level. My understanding is that for distributed checkpointing to work the `sharded_state_dict` method must be overridden correctly at that level of the model hierarchy. See `mamba_mixer.py` for reference. Please confirm or correct, [](https://github.com/mikolajblaz) Correct, don't know the sharding details of `time_mixing_beta` but if it's simply sharded across TP you just have to add it to this dict: https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/ssm/mamba_mixer.pyL592",Marking as stale. No activity in 60 days.
guyueh1,Make it an option to use TransformerEngine activation function  in FFN block,[draft] Add a new config parameter `use_te_activation_func` to control if we want to use TE custom kernels for activation function in MLP. ,2024-10-21T18:58:31Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1233," this PR is ready, please review or assign someone"
polisettyvarma,[QUESTION] How to load checkpoint saved in one parallel configuration (tensor/pipeline/data parallelism) can be loaded in a different parallel configuration ?,"**How to load checkpoint saved in one parallel configuration (tensor/pipeline/data parallelism) can be loaded in a different parallel configuration ?** !Image Based on this doc  https://github.com/NVIDIA/MegatronLM/blob/main/docs/source/apiguide/dist_checkpointing.rst There are some conflicting statements. 1. zarr is not default format as mentioned. 2. i tried 8 card config, src  tp2pp2dp2, dst  tp2pp1dp4 didn't work Can you provide a working example end to end to showcase this feature, Thanks.",2024-10-21T13:00:22Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1232,"You can checkout convert in tools folder,`tools/checkpoint/convert.py`. This may help you."
sunying2018,[QUESTION]how to incorporate MOE into hybrid mamba efficiently,"**how to incorporate MOE into hybrid mamba efficiently** Hi,  You provided code pretrain_mamba.py contains the mamba spec to contain transformer, mamba and mlp layers. I'm wondering if there is an easier way to incorporate MOE to replace some of MLP layers? Since I found args.spec conflicts with simply specified moe args setting. What considerations require these two to be set as mutually exclusive? Thank you so much!",2024-10-21T09:48:59Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1231
wangxiaoyang-dev,[QUESTION] When will multimodal evaluation support pipeline parallelism ?,"In currently, multimodal evaluation does not support pipeline parallelism, refer to https://github.com/NVIDIA/MegatronLM/blob/db7d37b54ef96e35f7afc56e29fffb60f5c957b9/examples/multimodal/run_text_generation.pyL944 In some multimodals such as InternVL2Llama376B, its vision model has 25 attention heads which does not support tensor parallelism, so it needs pipeline parallelism to evaluation. so i want to ask when will multimodal evaluation support pipeline parallelism ? thank.",2024-10-21T07:01:47Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1230
zyqhnu,[Wrong spelling] Update training.py,a **wrong spelling** (maybe).,2024-10-21T06:53:53Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1229,Duplication of https://github.com/NVIDIA/MegatronLM/pull/765. And the typo has been fixed in https://github.com/NVIDIA/MegatronLM/commit/2488e20d488b35259f6eb53267e8a0525a373599.,Marking as stale. No activity in 60 days.
kaiyama12345679,"[QUESTION]ã€€To convert a Llama 3.1 70B checkpoint in torch dcp format to the HuggingFace format,","I've pretrained Llama3.1 70B using MegatronLM (8 tensor parallel, 2 pipeline parallel, using distributed optimizer), and I have confirmed that the checkpoint is saved in the torch dcp format.   However, I am not sure how to convert this checkpoint into a format that can be uploaded to HuggingFace. If anyone knows how to do this, I would greatly appreciate your help.",2024-10-19T05:16:45Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1228
ltm920716,[QUESTION] why all rank0 of tp group make datasetsï¼Ÿ,"**Your question** hi, I am very confused about the logic bellow. Here, datasets are built for each rank 0 of the TP group. If it is a rank 0 in the TP group of the middle layer, is it necessary to build datasets?  If the tp group is [[0,1,2,3], [4,5,6,7]], ppgroup is [[0,4], [1,5], [2,6], [3,7]], worldsize=8, that is global rank0 and rank4 make datasets bothï¼Œthat is rightï¼Ÿ help pleaseï¼Œ thanksï¼ (https://github.com/NVIDIA/MegatronLM/blob/55622ff6ab69d13678dfe595e70d88f1d3368795/megatron/training/training.pyL1690)",2024-10-18T12:31:37Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1227
bugm,[QUESTION] How will recompute-num-layers influence the gpu memory usage with uniform recompute-method.,"MegatronLM gtihub document says: **The uniform method uniformly divides the transformer layers into groups of layers (each group of size recomputenumlayers) and stores the input activations of each group in memory. The baseline group size is 1 and, in this case, the input activation of each transformer layer is stored. When the GPU memory is insufficient, increasing the number of layers per group reduces the memory usage, enabling a bigger model to be trained. For example, when recomputenumlayers is set to 4, only the input activation of each group of 4 transformer layers is stored.** I agree that with a bigger group size, it stores less input activations which take less gpu memory. But with a bigger group size, during the recompute process, which means it will also take more gpu memory for the temporary activations.   So the activations peak memory during the training process should be stored_input_activations_memory + M / num_groups (M is the activations memory with no recomputing for the model).  But the official doc says "" When the GPU memory is insufficient, increasing the number of layers per group reduces the memory usage, enabling a bigger model to be trained"", Is there any other technical applied to support it? But the way, I have tested to set the  recomputenumlayers equal to the number of layers for the model with a uniform recompute method and found it actually saved some activations memory. But I am not clearly about how this is implemented since with  whole layers in one group, which means it needs to store all the activations during the recomputed process, which should be the same as the activations in a norecompute process. ",2024-10-18T07:05:33Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1226
bugm,[BUG] selective activation recompute only decrease little of GPU memory usage during training,"**Describe the bug** According to the paper https://arxiv.org/abs/2205.05198. The normal activation memory for a transformed based model in each layer can be calculated as  !Image and with the selective activation recompute, it can be decreased to  !Image with my training set,  !Image with tp =1 and pp =1, I expected when i use recomputeactivations, the GPU memory usage for storing activation should only be about 34 / (34+80)  = 30% of that with no activation recompute applied. Here are some info about the GPU memory usage  with recomputeactivations !Image without   recomputeactivations !Image I notice the max_memory allocated during training only decreased from 25.52GB to 24.94 GB.  **Expected behavior** The max_memory allocated  during training should decrease more. **Environment (please complete the following information):**   MegatronLM commit ID   PyTorch 2.4.1    CUDA version  12.5   NCCL version 2.20.5 **Additional context** According to the formula above, with b = 12 s =1024 h =1024  L= 20 a=16 t=1, the original activation memory should be around 32GB,  plus the memory for model states  , which is about 7.3 GB for a  0.43B parameters model, which should be around 40GB even not take the temporary buffers and unusable fragment memory into account. That is much bigger than the max_memory allocated without activation recomputing, So I wonder the MegatronLM has done some optimize here? And why the max_memory allocated only changes little with/without recomputeactivations ï¼ˆuse selective activation as default according to the docï¼‰",2024-10-18T03:52:17Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1225
paulaserna16,[BUG] Circular Dependency in transformer_engine and core.utils,"I'm trying to run the Dreambooth tutorial, but I'm encountering some issues in the modules.  First, the megatronlm version being installed when launching a container with nemo framework 24.07, doesn't have the **extensions** module with the transformer engine.  Then, I tried to make something manually to solve it and added the extensions folder to the megatron/core path. However, 7if I try to execute the dreambooth.py example: ! python /opt/NeMo/examples/multimodal/text_to_image/dreambooth/dreambooth.py \     model.unet_config.from_pretrained=/ckpts/unet.bin \     model.unet_config.from_NeMo=False \     model.first_stage_config.from_pretrained=/ckpts/vae.bin \     model.data.instance_dir=/datasets/instance_dir \     model.data.instance_prompt='a photo of a sks dog'  I get an error of:  ImportError                               Traceback (most recent call last) Cell In[13], line 4       2 from megatron.core.distributed import DistributedDataParallel as McoreDDP       3 from megatron.core.distributed import DistributedDataParallelConfig > 4 from megatron.core.extensions.transformer_engine import (       5     TEColumnParallelLinear,       6     TEDotProductAttention,       7     TELayerNormColumnParallelLinear,       8     TENorm,       9     TERowParallelLinear,      10  )      11 from megatron.core.fusions.fused_bias_dropout import get_bias_dropout_add      12 from megatron.core.models.gpt import GPTModel as MCoreGPTModel File /opt/megatronlm/megatron/core/extensions/transformer_engine.py:34      32 from megatron.core.transformer.transformer_config import TransformerConfig      33 from megatron.core.transformer.utils import make_sharded_tensors_for_checkpoint > 34 from megatron.core.utils import get_te_version, is_te_min_version      37 def _get_extra_te_kwargs(config: TransformerConfig):      38     extra_transformer_engine_kwargs = {""params_dtype"": config.params_dtype} ImportError: cannot import name 'get_te_version' from 'megatron.core.utils' (/opt/megatronlm/megatron/core/utils.py) After checking the files, I see that the one in line 4, tranformer engine is trying to import the function from utils:                              from megatron.core.utils import get_te_version, is_te_min_version And in fact, I checked the megatron.core utils.py file, and this one is calling the tranformer engine within that function: def get_te_version():     """"""Get TE version from __version__; if not available use pip's. Use caching.""""""     def get_te_version_str():         import **transformer_engine** as te         if hasattr(te, '__version__'):             return str(te.__version__)         else:             return version(""transformerengine"")     global _te_version     if _te_version is None:         _te_version = PkgVersion(get_te_version_str())     return _te_version I would appreciate your help.  Thanks!",2024-10-17T14:39:08Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1224,"Hi , sorry could you pull in the megatron core branch (maybe core_r0.9.0 or the latest one) and mount it to /opt/megatornlm so that you replace the entire megatroncore rather than manually doing something by adding extensions alone. ","Worked, thank you so much!"
alexchen4ai,Typo fix in readme,,2024-10-17T06:06:53Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/1223,I'll include your commit in my internal fix PR.,Your commit is squash merged in https://github.com/NVIDIA/MegatronLM/commit/2488e20d488b35259f6eb53267e8a0525a373599 and your author information is preserved.  Thanks for your contribution. You can close this PR now. ,Marking as stale. No activity in 60 days.
eljandoubi,torch.cuda.amp.custom_fwd(args...) is deprecated,"torch.cuda.amp.custom_fwd(args...) is deprecated. So use torch.amp.custom_fwd(args...,device_type='cuda') instead.",2024-10-16T16:10:22Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/1222,"We need to keep backward compatibility, so I'm working on a fix like ```python if torch.__version__ >= ""2.4.0"":     custom_fwd = partial(torch.amp.custom_fwd, device_type=""cuda"")     custom_bwd = partial(torch.amp.custom_bwd, device_type=""cuda"") else:     custom_fwd = torch.cuda.amp.custom_fwd     custom_bwd = torch.cuda.amp.custom_bwd ```", I have made the necessary changes.,I fixed most deprecation/future warnings from PyTorch in https://github.com/NVIDIA/MegatronLM/commit/2488e20d488b35259f6eb53267e8a0525a373599.
tao-githup,support qwen2 and siglip weight conversion script to enable training â€¦,â€¦with llavanext and llavaonevision,2024-10-16T08:36:58Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1221,Marking as stale. No activity in 60 days.
CFC87,[QUESTION] Error in pipeline parallelism training for multimodal model,"I am trying to run the example at https://github.com/NVIDIA/MegatronLM/tree/main/examples/multimodal. But I found that once I set the pipeline `pipelinemodelparallelsize` to be larger than 1, I will get the error message: `File ""/workspace/megatronlm/megatron/core/models/gpt/gpt_model.py"", line 219, in forward     rotary_seq_len = self.rotary_pos_emb.get_rotary_seq_len(   File ""/workspace/megatronlm/megatron/core/models/common/embeddings/rotary_pos_embedding.py"", line 173, in get_rotary_seq_len     rotary_seq_len = transformer_input.size(0) AttributeError: 'NoneType' object has no attribute 'size'`",2024-10-15T18:22:35Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1220
lawchingman,[ENHANCEMENT],**Is your feature request related to a problem? Please describe.** A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] **Describe the solution you'd like** A clear and concise description of what you want to happen. **Describe alternatives you've considered** A clear and concise description of any alternative solutions or features you've considered. **Proposed implementation** If you have a proposed implementation for the feature state it here or link to a PR. **Additional context** Add any other context or screenshots about the feature request here.,2024-10-14T02:00:15Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1219
lawchingman,[BUG],"**Describe the bug** A clear and concise description of what the bug is. **To Reproduce** Steps to reproduce the behavior. The easier it is to reproduce the faster it will get maintainer attention. **Expected behavior** A clear and concise description of what you expected to happen. **Stack trace/logs** If applicable, add the stack trace or logs from the time of the error. **Environment (please complete the following information):**   MegatronLM commit ID   PyTorch version   CUDA version   NCCL version **Proposed fix** If you have a proposal for how to fix the issue state it here or link to a PR. **Additional context** Add any other context about the problem here.",2024-10-13T22:48:26Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1218
lawchingman,[ENHANCEMENT],**Is your feature request related to a problem? Please describe.** A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] **Describe the solution you'd like** A clear and concise description of what you want to happen. **Describe alternatives you've considered** A clear and concise description of any alternative solutions or features you've considered. **Proposed implementation** If you have a proposed implementation for the feature state it here or link to a PR. **Additional context** Add any other context or screenshots about the feature request here.,2024-10-13T22:48:17Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1217
jonassteinberg1,readme spelling correction,"""ressearchoriented"" > ""researchoriented""",2024-10-13T22:48:06Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/1216,I'll include your commit in my internal fix PR.,Your commit is squash merged in https://github.com/NVIDIA/MegatronLM/commit/2488e20d488b35259f6eb53267e8a0525a373599 and your author information is preserved.  Thanks for your contribution. You can close this PR now. ,Marking as stale. No activity in 60 days.
Baibaifan,[Functions] Support Packed_seq_params in Megatron-LM,"Support Packed_seq_params in MegatronLM, just for testing.",2024-10-12T08:14:00Z,stale,open,1,1,https://github.com/NVIDIA/Megatron-LM/issues/1215,Marking as stale. No activity in 60 days.
bugm,How is micro-batch-size influencing the throughput per GPU ?,"Hi,    I am testing how is microbatchsize influencing the throughput per GPU with a constant globalbatchsize.    The result shows that as the microbatchsize increases, the throughput per GPU(TFLOP/s/GPU) also increases. I have done some test with a 400M transformer based model on 2 A40 GPUS,  and only use data parallelism. Here are some training Arguments !Image With different test I only change the microbatchsize , trained on 100 iterations with seq_len =1024 and globalbatchsize =24 . Here are some result with different microbatchsize !Image I print the log every 5 iterations and compute the averaged throughput per GPU. For each Iteration , the total computational complexity is the same , but throughput per GPU increases as the microbatchsize increases. I know that may related to the GPU cache load or arithmetic intensity but not quite clear.  Can anyone provide some indepth explanations?",2024-10-12T08:09:03Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1214
bugm,how to use TikTokenizer during Training?,"Hello,     I noticed that ML now support TikTokenizer by setting the **tokenizertype** argument. But I do not know what i should set with **tokenizermodel**. I have checked the source code and find that we should pass a json file, and the function below will convert the json file to Tiktoken format. https://github.com/NVIDIA/MegatronLM/blob/772faca1f8d5030621b738cbd8e8bb2d8d28f6e6/megatron/training/tokenizer/tokenizer.pyL581 The comment says "" Reload our tokenizer JSON file and convert it to Tiktoken format."" What does ""our tokenizer JSON"" means? which format should the json file be?",2024-10-12T06:39:55Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1213
ZacWang,[QUESTION] How to convert a checkpoint to virtual pipeline format checkpoint,"**Your question** I'm using `tools/checkpoint/convert.py` to convert a llama model to mcore model format for training. The `tools/checkpoint/loader_mcore.py` support virtual pipeline model loading, but `tools/checkpoint/saver_mcore.py` doesn't support to save a virtual pipeline model. Do I have any other way to do this convert? Or do I need to modify `saver_mcore.py` to support this? Maybe with support for args like `target_num_layers_per_virtual_pipeline_stage` and `target_virtual_pipeline_model_parallel_size`?",2024-10-11T10:33:02Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1212
JavaZeroo,Fix UnboundLocalError in initialize.py due to uninitialized 'seed' variable,Fixes an `UnboundLocalError` in `initialize.py` where the `seed` variable was referenced before being initialized.,2024-10-11T02:23:47Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1211,I'll include your commit in my internal fix PR.,Your commit is squash merged in https://github.com/NVIDIA/MegatronLM/commit/2488e20d488b35259f6eb53267e8a0525a373599 and your author information is preserved.  Thanks for your contribution. You can close this PR now. 
yanchenmochen,[QUESTION]How to setup fp8 trainging using Megatron-LM,**Your question** Ask a clear and concise question about MegatronLM.,2024-10-11T02:23:05Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1210,Is FP8 training compatible with recompute?
rachitgarg91,Embedding,,2024-10-10T18:49:43Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1209,Marking as stale. No activity in 60 days.
michal2409,Add fused swiglu,,2024-10-10T11:49:50Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1208
takuya576,"[BUG] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=5, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600013 milliseconds before timing out.","**Describe the bug** I encountered a `[rank0]:[E1010 11:50:05.315197062 ProcessGroupNCCL.cpp:565] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=5, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600013 milliseconds before timing out.`, when training a LLaVA architecture LMM model with the pipeline parallel size set to 2. First of all, I'd like to run the train process with LLM model's parameters frozen. When I train the LMM, I split it into two nodes: (vision encoder, projector, and a half of LLM) and the other half of LLM. However, the LLM's parameters are frozen in the training process, so the latter node has an empty training parameter list.  In order to avoid a `ValueError: optimizer got an empty parameter list`, I skip the training process in the latter node. But I got the following error: ```shell /usr/bin/rm: cannot remove '/usr/local/cuda/compat/lib': Readonly file system rm: cannot remove '/usr/local/cuda/compat/lib': Readonly file system W1010 11:39:41.458000 140602427192448 torch/distributed/run.py:757]  W1010 11:39:41.458000 140602427192448 torch/distributed/run.py:757] ***************************************** W1010 11:39:41.458000 140602427192448 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.  W1010 11:39:41.458000 140602427192448 torch/distributed/run.py:757] ***************************************** [rank1]:[W1010 11:39:59.888362044 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank0]:[W1010 11:39:59.888371352 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank0]:[E1010 11:50:05.315197062 ProcessGroupNCCL.cpp:565] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=5, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600013 milliseconds before timing out. [rank0]:[E1010 11:50:06.635888164 ProcessGroupNCCL.cpp:1606] [PG 0 Rank 0] Timeout at NCCL work: 5, last enqueued NCCL work: 5, last completed NCCL work: 4. [rank0]:[E1010 11:50:06.635906689 ProcessGroupNCCL.cpp:579] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data. [rank0]:[E1010 11:50:06.635913192 ProcessGroupNCCL.cpp:585] [Rank 0] To avoid data inconsistency, we are taking the entire process down. [rank0]:[E1010 11:50:06.637844201 ProcessGroupNCCL.cpp:1446] [PG 0 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=5, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600013 milliseconds before timing out. Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:567 (most recent call first): frame CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string, std::allocator >) + 0x99 (0x7f50c0b98969 in /usr/local/lib/python3.10/distpackages/torch/lib/libc10.so) frame CC(Faster dataloader merge): c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional > >) + 0x1e1 (0x7f505bea04e1 in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(rename confusing arg): c10d::ProcessGroupNCCL::watchdogHandler() + 0x222 (0x7f505bea81a2 in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(Compatibility with pytorchtransformers for finetuning ): c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7f505bea9a0f in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(Note that as of now you need to have PySOL cloned to the directory here before building the container.):  + 0xdc253 (0x7f50c06b0253 in /usr/lib/x86_64linuxgnu/libstdc++.so.6) frame CC(I do not know how to solve it.):  + 0x94ac3 (0x7f50cc7acac3 in /usr/lib/x86_64linuxgnu/libc.so.6) frame CC(perplexity too big for gpt2 wikitext evaluation):  + 0x126850 (0x7f50cc83e850 in /usr/lib/x86_64linuxgnu/libc.so.6) terminate called after throwing an instance of 'c10::DistBackendError'   what():  [PG 0 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=5, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600013 milliseconds before timing out. Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:567 (most recent call first): frame CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string, std::allocator >) + 0x99 (0x7f50c0b98969 in /usr/local/lib/python3.10/distpackages/torch/lib/libc10.so) frame CC(Faster dataloader merge): c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional > >) + 0x1e1 (0x7f505bea04e1 in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(rename confusing arg): c10d::ProcessGroupNCCL::watchdogHandler() + 0x222 (0x7f505bea81a2 in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(Compatibility with pytorchtransformers for finetuning ): c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7f505bea9a0f in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(Note that as of now you need to have PySOL cloned to the directory here before building the container.):  + 0xdc253 (0x7f50c06b0253 in /usr/lib/x86_64linuxgnu/libstdc++.so.6) frame CC(I do not know how to solve it.):  + 0x94ac3 (0x7f50cc7acac3 in /usr/lib/x86_64linuxgnu/libc.so.6) frame CC(perplexity too big for gpt2 wikitext evaluation):  + 0x126850 (0x7f50cc83e850 in /usr/lib/x86_64linuxgnu/libc.so.6) Exception raised from ncclCommWatchdog at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1450 (most recent call first): frame CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string, std::allocator >) + 0x99 (0x7f50c0b98969 in /usr/local/lib/python3.10/distpackages/torch/lib/libc10.so) frame CC(Faster dataloader merge):  + 0x10485fe (0x7f505bed05fe in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(rename confusing arg):  + 0xcbc925 (0x7f505bb44925 in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(Compatibility with pytorchtransformers for finetuning ):  + 0xdc253 (0x7f50c06b0253 in /usr/lib/x86_64linuxgnu/libstdc++.so.6) frame CC(Note that as of now you need to have PySOL cloned to the directory here before building the container.):  + 0x94ac3 (0x7f50cc7acac3 in /usr/lib/x86_64linuxgnu/libc.so.6) frame CC(I do not know how to solve it.):  + 0x126850 (0x7f50cc83e850 in /usr/lib/x86_64linuxgnu/libc.so.6) W1010 11:50:57.098000 140602427192448 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 2952340 closing signal SIGTERM E1010 11:50:57.468000 140602427192448 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 6) local_rank: 0 (pid: 2952339) of binary: /usr/bin/python Traceback (most recent call last):   File ""/usr/local/bin/torchrun"", line 33, in      sys.exit(load_entry_point('torch==2.4.0a0+07cecf4168.nv24.5', 'console_scripts', 'torchrun')())   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 347, in wrapper     return f(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/run.py"", line 879, in main     run(args)   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/run.py"", line 870, in run     elastic_launch(   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/launcher/api.py"", line 132, in __call__     return launch_agent(self._config, self._entrypoint, list(args))   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/launcher/api.py"", line 263, in launch_agent     raise ChildFailedError( torch.distributed.elastic.multiprocessing.errors.ChildFailedError: ```  **Expected behavior** I'd like to train the LMM with the pipeline parallel size set to 2. Avoiding`Watchdog caught collective operation timeout:`, I'd like to skip the training process in the latter node. If you have another solution rather than skiping training process, I'll appreciate it.",2024-10-10T05:51:09Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1207,Marking as stale. No activity in 60 days.
abgoswam,[QUESTION] Converting a Megatron-LM ckpt to Nemo,"Following the GPT Pretraining section in the  `MegatronLM` repo,  we are able to successfully train a model using  `MegatronLM`  I saw pointers on how to convert from HF to `nemo`.  For example, this conversion script   convert_llama_hf_to_nemo.py However I did not see any examples of converting a ckpt saved using  the  `MegatronLM` to `nemo` format .  Are there any examples for this? p.s.  I am thinking of doing this conversion to `nemo`, so I can use tools like `NemoAligner` for posttraining",2024-10-10T03:28:15Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1206
lostkevin,Dev/optimizer offloading,,2024-10-10T02:16:32Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1205,Marking as stale. No activity in 60 days.
rachitgarg91,Rachitg/emb,,2024-10-09T15:40:48Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1204
xqiangx1991,fix bugs for multi_latent_attention,"1. The initialization parameters for `DotProductAttention` and `TEDotProductAttention` are different. Using DotProductAttention to construct MultiLatentAttention will result in an error. 2. In the `MLASelfAttention` module, the dimensions of `k_pos_emb` and `k_no_pe` differ in terms of num_attention_heads.`torch.cat` will result in an error.",2024-10-09T06:34:58Z,stale,open,2,1,https://github.com/NVIDIA/Megatron-LM/issues/1203,Marking as stale. No activity in 60 days.
zmtttt,[QUESTION]Fail to build communication between muti machines,"I use two machines to run megatrongpt the .sh parameters as follows: export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 NCCL_DEBUG=WARN GPUS_PER_NODE=8  Change for multinode config MASTER_ADDR=192.168.2.111 MASTER_PORT=45678 NUM_NODES=2 NODE_RANK=$1 WORLD_SIZE=16 BASE_DIR=""/data/train_nfs/offload_megatron/megatron_0.8/zmt/MegatronLM"" /megatron_lm_345m_v0.0/release/mp_rank_00/model_optim_rng.pt CHECKPOINT_PATH=""${BASE_DIR}/release/mp_rank_00/"" tensorboard_logs TENSORBOARD_LOGS_PATH=""${BASE_DIR}/tensorboard_logs"" gpt2vocab.json VOCAB_FILE=""${BASE_DIR}/gpt2vocab.json"" /gpt2vocab.json gpt2vocab.json MERGE_FILE=""${BASE_DIR}/gpt2merges.txt"" /gpt2vocab.json testcorpus.json DATA_PATH=""${BASE_DIR}/oscaren10kmegllama_text_document"" _text_documen I give the instruction as follows:  machine1: bash gpt3.sh 0 machine2: bash gpt3.sh 1 but get the error:  WARNING:torch.distributed.run: ***************************************** Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, ple     ase further tune the variable for optimal performance in your application as needed. ***************************************** Traceback (most recent call last):   File ""/opt/conda/bin/torchrun"", line 33, in      sys.exit(load_entry_point('torch==2.0.0+mc2.23.0.23', 'console_scripts', 'torchrun')())   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper     return f(*args, **kwargs)   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/run.py"", line 794, in main     run(args)   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/run.py"", line 785, in run     elastic_launch(   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/launcher/api.py"", line 134, in __call__     return launch_agent(self._config, self._entrypoint, list(args))   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/launcher/api.py"", line 241, in launch_agent     result = agent.run()   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/elastic/metrics/api.py"", line 129, in wrapper     result = f(*args, **kwargs)   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/elastic/agent/server/api.py"", line 723, in run     result = self._invoke_run(role)   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/elastic/agent/server/api.py"", line 858, in _invoke_run     self._initialize_workers(self._worker_group)   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/elastic/metrics/api.py"", line 129, in wrapper     result = f(*args, **kwargs)   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/elastic/agent/server/api.py"", line 692, in _initialize_workers     self._rendezvous(worker_group)   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/elastic/metrics/api.py"", line 129, in wrapper     result = f(*args, **kwargs)   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/elastic/agent/server/api.py"", line 549, in _rendezvous     workers = self._assign_worker_ranks(store, group_rank, group_world_size, spec)   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/elastic/metrics/api.py"", line 129, in wrapper     result = f(*args, **kwargs)   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/elastic/agent/server/api.py"", line 624, in _assign_worker_ranks     role_infos = self._share_and_gather(store, group_rank, group_world_size, spec)   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/elastic/agent/server/api.py"", line 661, in _share_and_gather     role_infos_bytes = store_util.synchronize(   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/elastic/utils/store.py"", line 64, in synchronize     agent_data = get_all(store, rank, key_prefix, world_size)   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/elastic/utils/store.py"", line 34, in get_all     data = store.get(f""{prefix}{idx}"") RuntimeError: Socket Timeout have you met the same problems?? thanks!!!",2024-10-08T06:44:02Z,stale,open,2,1,https://github.com/NVIDIA/Megatron-LM/issues/1202,Marking as stale. No activity in 60 days.
lawchingman,[ENHANCEMENT],**Is your feature request related to a problem? Please describe.** A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] **Describe the solution you'd like** A clear and concise description of what you want to happen. **Describe alternatives you've considered** A clear and concise description of any alternative solutions or features you've considered. **Proposed implementation** If you have a proposed implementation for the feature state it here or link to a PR. **Additional context** Add any other context or screenshots about the feature request here.,2024-10-06T05:50:40Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1201
MlWoo,[QUESTION] Encoder with more TP than the decoder,"A new model with a heavy module with a light module could be viewd as a t5 model. so the  tp of encoder is more than that of the decoder. The tp partition is not allowed in code https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/parallel_state.pyL519. if it is modified, and change the line https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/parallel_state.pyL616 to `for x, y in zip(e_ranks, cycle(d_ranks)`, is it OK for the model, what else should I considerate?",2024-10-06T01:28:25Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1200,Marking as stale. No activity in 60 days.
lawchingman,[ENHANCEMENT],**Is your feature request related to a problem? Please describe.** A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] **Describe the solution you'd like** A clear and concise description of what you want to happen. **Describe alternatives you've considered** A clear and concise description of any alternative solutions or features you've considered. **Proposed implementation** If you have a proposed implementation for the feature state it here or link to a PR. **Additional context** Add any other context or screenshots about the feature request here.,2024-10-05T00:27:54Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1199
rybakov,[ENHANCEMENT] Add layer name in a layer to improve code debugging,"**Is your feature request related to a problem? Please describe.** I am adding new features in TranformerEngine(TE) and observe issues with model quality (gap in the loss with loss spikes). I am debugging Megatron with TE, by storing tensor statistics in impacted layers. But I do not have information about layer name and layer order(index) in the model topology. **Describe the solution you'd like** It would be great to add proper layer name with its order in the model, so that customers can use it for model debugging. **Describe alternatives you've considered** There are multiple frameworks which support this simple feature, e.g: Lingvo based on TF Praxis based on JAX **Proposed implementation** I propose to add a layer_name filed which will be a unique name with layer hierarchy and its index/order (if there are multiple layers with the same name) Here is an example: ''' class TransformerBlock(MegatronModule):     """"""Transformer class.""""""     def __init__(         self,        ...         layer_name: str = ""TransformerBlock"",     ):          offset is implicit in TransformerLayer         self.layers = torch.nn.ModuleList(             [                 build_layer(layer_spec, i + 1, f""{self.layer_name}.blocks"" if self.layer_name else None)                 for i, layer_spec in enumerate(self.submodules.layer_specs)             ]         ) ''' **Additional context** In our local branch, this feature is already used by multiple people. ",2024-10-04T23:48:43Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1198,Marking as stale. No activity in 60 days.
rayleizhu,"[QUESTION] Why do we need both "" train_valid_test_datasets_provider.is_distributed = True"" and batched data broadcasting ?","I noticed that when `train_valid_test_datasets_provider.is_distributed = True` data loader is created in all processes, ignoring their tensor parallel rank. https://github.com/NVIDIA/MegatronLM/blob/c02b335b6318ada8c6a38c95ce3c754da2a579f9/pretrain_vlm.pyL333 https://github.com/NVIDIA/MegatronLM/blob/c02b335b6318ada8c6a38c95ce3c754da2a579f9/megatron/training/training.pyL1685 However, in get_batch(), the batched data is still broadcasted: https://github.com/NVIDIA/MegatronLM/blob/c02b335b6318ada8c6a38c95ce3c754da2a579f9/pretrain_vlm.pyL242 I got confused why do we need both of them? My understanding is that we need either distributed access or broadcasting from tp rank 0, not both of them.",2024-10-04T02:14:45Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1196,Marking as stale. No activity in 60 days.
youzagou,Use consistent assert message,use consistent log message,2024-10-03T05:46:44Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1195,I'll include your commit in my internal fix PR.,Your commit is squash merged in https://github.com/NVIDIA/MegatronLM/commit/2488e20d488b35259f6eb53267e8a0525a373599 and your author information is preserved.  Thanks for your contribution. You can close this PR now. 
takuya576,"[BUG] ""ValueError: optimizer got an empty parameter list"" under pipeline parallel"," Describe the bug I encountered a `ValueError: optimizer got an empty parameter list`, when training the projector of a LLaVA archtectire LMM model, with the pipeline parallel size set to **2 or higher**. I was attempting to run the train process with **LLM model's parameters frozen**.  To Reproduce In order to reproduce this error, I think it is good for you to train a model with some parts of it frozen and set pipeline parallel size to 2 or greater.  Expected behavior I'd like to resolve this error and successfully run the training process with pipeline parallel size of 2 or higher.",2024-10-02T05:58:50Z,,closed,0,5,https://github.com/NVIDIA/Megatron-LM/issues/1166,Really hope how to solve this. ,same problem,Marking as stale. No activity in 60 days.,Hi  did you solve this?,"Hi , the following commit could solve the problem. https://github.com/NVIDIA/MegatronLM/commit/1468ab01c079d5e14888dda97d1c99d2cb62afb2 I integrated these changes to my repository (and additional changes because my repository is an older version), and it eventually worked. But lmloss and gradnorm graphs didn't converge, so I am investigating whether this is due to incorrect calculations in how these metrics are being tracked."
takuya576,[BUG],"Describe the bug I encountered a `ValueError: optimizer got an empty parameter list`, when training the projector of a LLaVA archtectire LMM model, with the pipeline parallel size set to **2 or higher**. I was attempting to run the train process with **LLM model's parameters frozen**. To Reproduce In order to reproduce this error, I think it is good for you to train a model with some parts of it frozen and set pipeline parallel size to 2 or greater. Expected behavior I'd like to resolve this error and successfully run the training process with pipeline parallel size of 2 or higher.",2024-10-02T05:56:42Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1165
takuya576,[BUG],"**Describe the bug** I encountered a `ValueError: optimizer got an empty parameter list`, when training the projector of a LLaVA archtectire LMM model, with the **pipeline parallel size set to 2 or higher**. I was attempting to run the train process with **LLM model's parameters frozen**. **To Reproduce** In order to reproduce this error, I think it is good for you to train a model with some parts of it frozen and set pipeline parallel size to 2 or greater. **Expected behavior** I'd like to resolve this error and successfully run the training process with pipeline parallel size of 2 or higher.",2024-10-02T05:53:44Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1164
tomlifu,adding cu_seqlens_padded to packed_seq_params.py,This PR supports this NeMo PR: https://github.com/NVIDIA/NeMo/pull/10688,2024-10-01T17:08:39Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1163,Closing this PR
junjzhang,[QUESTION] About all_reduce order while using CP,"I noticed that CP grad is also reduced in DDP, indicating that grads are first reduced among micro batches then the CP group, thereby minimizing communication costs. However, the reduction order differs: in this case, it's splited seq > micro batch > CP group, whereas on a single card, itâ€™s split seq > CP group > micro batch. I believe this difference may lead to errors due to floatingpoint addition. Is my assessment correct, or is this error acceptable in practice?",2024-09-27T09:40:30Z,stale,open,1,2,https://github.com/NVIDIA/Megatron-LM/issues/1162,  Could you help me on this one?,Marking as stale. No activity in 60 days.
yu-depend,Why are not all SMs active when NCCL kernel and compute kernel overlap?[QUESTION],"When I run a single NCCL kernel ,the active SMs is 15%ï¼Œand When I run a single compute kernel ,the active SMs is 100%  ï¼Œbut when I run the compute kernel and the NCCL kernel in parallel, so that they overlapï¼Œthe active SMs is 85%, how to explain thisï¼Ÿ  !Image",2024-09-27T02:39:30Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1161,Marking as stale. No activity in 60 days.
zochaoq,Expose cp_comm_type in ModelParallelConfig,The default cp_comm_type 'p2p' does not work with sliding window attention in TransformerEngine. Exposing this parameter will allow us to test context parallel with sliding window attention. More details about this change can be seen in the previous PRs https://github.com/NVIDIA/TransformerEngine/pull/1060 and https://github.com/NVIDIA/TransformerEngine/pull/1160 ,2024-09-27T01:11:03Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1160,Marking as stale. No activity in 60 days.,This is merged into part of https://github.com/NVIDIA/MegatronLM/commit/91a8a4c8dc3cb6e8b4d6147c855226e7fe6abff9
zyksir,[QUESTION] Do we really need to call np.arange every time we restart the task?,"When we first launch the task, we will build the index for the dataset. Every time we restart the task, we will just load the idx file and npy file I notice in function `_build_megatron_dataset_splits`, MegatronLM will call `numpy.arange` every time. This piece of code can be cpu bound and lead to a slow initialize time. I don't see why we need to call `numpy.arange` every time. It seems that the indices will be used only when we build the index in the first run.",2024-09-26T04:46:11Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1159,Marking as stale. No activity in 60 days.
wangchaottss,[BUG]TypeError: 'type' object is not subscriptable,"megatron/core/dist_checkpointing/strategies/base.py line 24 `default_strategies: DefaultDict[str, dict[tuple, Any]] = defaultdict(dict)` raises a type error",2024-09-25T03:02:16Z,stale,open,1,4,https://github.com/NVIDIA/Megatron-LM/issues/1158,Marking as stale. No activity in 60 days.,I also encountered this error when using megatron core 0.9.0.,"I tried to upgrade python >= 3.9, it has resolved. https://stackoverflow.com/questions/75202610/typeerrortypeobjectisnotsubscriptablepython",Marking as stale. No activity in 60 days.
youngeunkwon0405,Enabling UCC backend for PP communication,"This MR provides an interface to enable the UCC backend for PP communication. To enable the UCC backend, set the following argument: `training.model.pipeline_model_parallel_comm_backend=ucc` Requires a related NeMo PR (link).",2024-09-24T21:14:46Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1157,Marking as stale. No activity in 60 days.
polisettyvarma,[QUESTION] How to enable ZeRO 2/3 stages ?,**How to enable ZeRO 2/3 stages ?** similar to CC([ENHANCEMENT] support zero 2 distributed optimize) ,2024-09-24T12:01:47Z,,closed,0,10,https://github.com/NVIDIA/Megatron-LM/issues/1156,I responded to this on https://github.com/NVIDIA/MegatronLM/issues/589.,please convert this issue to feature request for ZeRO 2/3 Thank you.,"i think article,https://www.deepspeed.ai/tutorials/megatron/, is useful. deepspeed ZeRO 1/2 works with Megatronlm latest code."," Thanks for the inputs, i am familiar with deepspeed framework to enable all ZeRO stages. here query is regarding enabling ZeRO in this repo natively. can you please share commits which added ZeRO 2 support in latest code of this repo. Thank you.",I also look for such example~.,"megatronlm now has its own zero1 (it is called distributed optimizer in this project), but if u are more familiar with deepspeed, then how bout using deepspeedmegatron,  ? And to my best knowledge, zero3 is not compatible with model parallelism (TP or PP) of megatronlm. zero3 reduce vram memory and improve throughput by partitioning and broadcasting model parameters but TP or PP partition its own way and rather communicate activations (allreduce activations for backward and forward). So TP or PP has no room for communicating model parameters.",Thank you  for your inputs. Yes MegatronDeepSpeed repo can be used but it's not up to date with MegatronLM. I agree on Zero > 1 is not compatible with PP. My request here is some similar feature of ZeRO on MegatronLM.,We should have PyTorch FSDP support compatible with TP in the next couple of weeks.,Thank you  for sharing this information.,   https://github.com/NVIDIA/MegatronLM/commit/e1993fa6f70763523a84432ab1f5eb42e77ccf2adiffa7ca552e38c01a3a0cacbe37cec383c05743aeaf8143e57fd0901f4139d4a1a9R119 merged into main 2 hours ago
Baibaifan,opt:opt ltor masks," Problem: In MegatronLM, there is a memory bottleneck when using the reset attention mask to construct long sequences. The following code: (_get_ltor_masks_and_position_ids) !image When a seq_len consists of multiple short documents, there will be multiple values â€‹â€‹in eod_index. Each value means that attention_mask needs to be accessed and loaded once, and the corresponding position is assigned 0. For example, in the 32k scenario, there are multiple assignments in extreme scenarios, which makes the data loading time very slow. As the sequence length increases, the number of positions that need to be assigned increases, and the time consumption will be longer. As shown in the figure below. !image  Solution: Perform a tensor access using the  `block_diag` value. !image",2024-09-24T06:51:41Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1155,Marking as stale. No activity in 60 days.
chotzen,"[BUG] Some checkpoint shards don't save / hang on multi-node setups, since v0.7","**Describe the bug** We're in the process of upgrading MegatronCore from 0.6 to 0.8 and have noticed some problematic behavior with the new distributed async checkpoint saving introduced in mcore 0.7.  When trying to save a checkpoint for a small (say, llama 3 8b sized) model on a decently sized training setup (8 nodes, 2 way MP/32 way DDP) we notice that the checkpoint save hangs. Upon further inspection, we see that only 123 of the expected 128 checkpoint shards were saved to the directory. Which shards are missing varies with each attempt. Notably, this does not happen on smaller setups  **on 1 or 2 nodes, this does not occur, but on 8 nodes, it occurs every time I've tested it.** **To Reproduce** Here is how we call the checkpointing code: ``` self._model: megatron.core.distributed.DistributedDataParallel.DDP model_state_dict: Dict[str, Any] = {} model_state_dict[""model""] = self._model.sharded_state_dict() save_strategy = get_default_save_sharded_strategy('torch_dist') save_strategy = FullyParallelSaveStrategyWrapper(save_strategy, parallel_state.get_data_parallel_group(with_context_parallel=False), False) torch.distributed.barrier() _ = dist_checkpointing.save(     model_state_dict,     os.path.join(checkpoint_path, ""model""),     save_strategy,     async_sharded_save=False, ) ``` We observe the same behavior with and without the `FullyParallelSaveStrategyWrapper`. **Expected behavior** The checkpoint finishes saving **Stack trace/logs** On larger configurations (16 nodes, 2way MP x 64way DDP) we sometimes observe this error. I don't know if this is related to this issue or is a different one. For what it's worth, we don't observe this on the 8node configuration. ``` ""/app/exa/cluster_scheduler/megatron_job_image.binary.runfiles/exafunction/third_party/megatron_lm/MegatronLM/megatron/core/dist_checkpointing/serialization.py"", line 394, in save     sharded_strategy.save(sharded_state_dict, checkpoint_dir)   File ""/app/exa/cluster_scheduler/megatron_job_image.binary.runfiles/exafunction/third_party/megatron_lm/MegatronLM/megatron/core/dist_checkpointing/strategies/base.py"", line 185, in save     async_calls.maybe_finalize_async_calls(blocking=True)   File ""/app/exa/cluster_scheduler/megatron_job_image.binary.runfiles/exafunction/third_party/megatron_lm/MegatronLM/megatron/core/dist_checkpointing/strategies/async_utils.py"", line 217, in maybe_finalize_async_calls     finalize_fn()   File ""/app/exa/cluster_scheduler/megatron_job_image.binary.runfiles/exafunction/third_party/megatron_lm/MegatronLM/megatron/core/dist_checkpointing/strategies/torch.py"", line 661, in finalize_fn     save_state_dict_async_finalize(*save_state_dict_ret)   File ""/app/exa/cluster_scheduler/megatron_job_image.binary.runfiles/exafunction/third_party/megatron_lm/MegatronLM/megatron/core/dist_checkpointing/strategies/state_dict_saver.py"", line 144, in save_state_dict_async_finalize     write_results = storage_writer.retrieve_write_results()   File ""/app/exa/cluster_scheduler/megatron_job_image.binary.runfiles/exafunction/third_party/megatron_lm/MegatronLM/megatron/core/dist_checkpointing/strategies/filesystem_async.py"", line 309, in retrieve_write_results     raise RuntimeError(f'results_queue should not be empty') RuntimeError: results_queue should not be empty ``` **Environment (please complete the following information):**   MegatronLM commit ID: baf94af3c667248865f23df73b9fb8e2395e6fd0 + internal patches unrelated to checkpointing   PyTorch version 2.2.2   CUDA version 12.2.2   NCCL version 2.18 **Additional context** The volume we've saving to is a `cephfs` hosted on another node in our cluster. Checkpointing works with more or less the same setup on megatron v0.6.0 (before the new interface was implemented, we passed in the `('torch_dist', 1)` strategy). Please let me know what additional info I can provide.",2024-09-23T22:23:53Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1154,"Hi, it might be the case the worker processes executing this function are failing, and the 16n log suggests this error might be silently ignored. One thing that might help figure out what's going on is enabling debug logs by setting `MEGATRON_LOGGING_LEVEL=10`, can you try that? Please make sure the output from different ranks is distinguishable, e.g. by setting `l` flag in the slurm run. ____ > we see that only 123 of the expected 128 checkpoint shards were saved to the directory just double checking, if you **don't** use FullyParallelSaveStrategyWrapper, then the directory is expected to have 4 shards (= 2 * 2 = MP * mutiprocessing_factor)  is it the case?",Marking as stale. No activity in 60 days.
zixianwang2022,[BUG] Unable to Convert Mamba PP=1 TP=1 to PP>1 TP>1 Using convert.py,"**Describe the bug** Hi, I am unable to use `tools/checkpoint/convert.py` to convert my pretrained mamba 8b checkpoint to PP>1 and/or TP>1. So that I can finetune from the pretrained checkpoint in a distributed manner.  **To Reproduce** Launch a docker and save the model under desired directory.  ``` python tools/checkpoint/convert.py \         modeltype GPT \         loaddir /workspace/data/ssmretrieval/mamba28b/mamba28b3t4k/ \         savedir /workspace/data/ssmretrieval/mamba28b/pp1_tp2/ \         targettensorparallelsize 2 \         targetpipelineparallelsize 1 \         megatronpath /workspace/megatron ```  **Expected behavior** Well, ideally it should work fine out of box.  **Stack trace/logs** ``` micro_batch_times_data_parallel:1 setting number of microbatches to constant 1 building GPT model ...  loading release checkpoint from /workspace/data/ssmretrieval/mamba28b/mamba28b3t4k/ Traceback (most recent call last):   File ""/workspace/megatron/tools/checkpoint/convert.py"", line 154, in  Loader exited, exiting saver     main()   File ""/workspace/megatron/tools/checkpoint/convert.py"", line 147, in main     loader.load_checkpoint(queue, args)   File ""/workspace/megatron/tools/checkpoint/loader_megatron.py"", line 375, in load_checkpoint     _load_checkpoint(queue, args)   File ""/workspace/megatron/tools/checkpoint/loader_megatron.py"", line 237, in _load_checkpoint     all_models = [get_models(tp_size, md.params_dtype)]   File ""/workspace/megatron/tools/checkpoint/loader_megatron.py"", line 165, in get_models     load_checkpoint(model_, None, None)   File ""/workspace/megatron/megatron/training/checkpointing.py"", line 767, in load_checkpoint     model[0].load_state_dict(state_dict['model'], strict=strict)   File ""/workspace/megatron/megatron/legacy/model/gpt_model.py"", line 122, in load_state_dict     self.language_model.load_state_dict(state_dict, strict=strict)   File ""/workspace/megatron/megatron/legacy/model/language_model.py"", line 608, in load_state_dict     self.encoder.load_state_dict(state_dict_, strict=strict)   File ""/workspace/megatron/megatron/legacy/model/transformer.py"", line 1814, in load_state_dict     super().load_state_dict(state_dict_, strict)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 2152, in load_state_dict     raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format( RuntimeError: Error(s) in loading state_dict for ParallelTransformer:         Missing key(s) in state_dict: ""layers.0.input_norm.weight"", ""layers.0.self_attention.query_key_value.weight"", ""layers.0.self_attention.dense.weight"", ""layers.0.post_attention_norm.weight"", ""layers.0.mlp.dense_h_to_4h.weight"", ""layers.0.mlp.dense_4h_to_h.weight"", ""layers.1.input_norm.weight"", ""layers.1.self_attention.query_key_value.weight"", ""layers.1.self_attention.dense.weight"", ""layers.1.post_attention_norm.weight"", ""layers.1.mlp.dense_h_to_4h.weight"", ""layers.1.mlp.dense_4h_to_h.weight"", ""layers.2.input_norm.weight"", ""layers.2.self_attention.query_key_value.weight"", ""layers.2.self_attention.dense.weight"", ""layers.2.post_attention_norm.weight"", ""layers.2.mlp.dense_h_to_4h.weight"", ""layers.2.mlp.dense_4h_to_h.weight"", ""layers.3.input_norm.weight"", ""layers.3.self_attention.query_key_value.weight"", ""layers.3.self_attention.dense.weight"", ""layers.3.post_attention_norm.weight"", ""layers.3.mlp.dense_h_to_4h.weight"", ""layers.3.mlp.dense_4h_to_h.weight"", ""layers.4.input_norm.weight"", ""layers.4.self_attention.query_key_value.weight"", ""layers.4.self_attention.dense.weight"", ""layers.4.post_attention_norm.weight"", ""layers.4.mlp.dense_h_to_4h.weight"", ""layers.4.mlp.dense_4h_to_h.weight"", ""layers.5.input_norm.weight"", ""layers.5.self_attention.query_key_value.weight"", ""layers.5.self_attention.dense.weight"", ""layers.5.post_attention_norm.weight"", ""layers.5.mlp.dense_h_to_4h.weight"", ""layers.5.mlp.dense_4h_to_h.weight"", ""layers.6.input_norm.weight"", ""layers.6.self_attention.query_key_value.weight"", ""layers.6.self_attention.dense.weight"", ""layers.6.post_attention_norm.weight"", ""layers.6.mlp.dense_h_to_4h.weight"", ""layers.6.mlp.dense_4h_to_h.weight"", ""layers.7.input_norm.weight"", ""layers.7.self_attention.query_key_value.weight"", ""layers.7.self_attention.dense.weight"", ""layers.7.post_attention_norm.weight"", ""layers.7.mlp.dense_h_to_4h.weight"", ""layers.7.mlp.dense_4h_to_h.weight"", ""layers.8.input_norm.weight"", ""layers.8.self_attention.query_key_value.weight"", ""layers.8.self_attention.dense.weight"", ""layers.8.post_attention_norm.weight"", ""layers.8.mlp.dense_h_to_4h.weight"", ""layers.8.mlp.dense_4h_to_h.weight"", ""layers.9.input_norm.weight"", ""layers.9.self_attention.query_key_value.weight"", ""layers.9.self_attention.dense.weight"", ""layers.9.post_attention_norm.weight"", ""layers.9.mlp.dense_h_to_4h.weight"", ""layers.9.mlp.dense_4h_to_h.weight"", ""layers.10.input_norm.weight"", ""layers.10.self_attention.query_key_value.weight"", ""layers.10.self_attention.dense.weight"", ""layers.10.post_attention_norm.weight"", ""layers.10.mlp.dense_h_to_4h.weight"", ""layers.10.mlp.dense_4h_to_h.weight"", ""layers.11.input_norm.weight"", ""layers.11.self_attention.query_key_value.weight"", ""layers.11.self_attention.dense.weight"", ""layers.11.post_attention_norm.weight"", ""layers.11.mlp.dense_h_to_4h.weight"", ""layers.11.mlp.dense_4h_to_h.weight"", ""layers.12.input_norm.weight"", ""layers.12.self_attention.query_key_value.weight"", ""layers.12.self_attention.dense.weight"", ""layers.12.post_attention_norm.weight"", ""layers.12.mlp.dense_h_to_4h.weight"", ""layers.12.mlp.dense_4h_to_h.weight"", ""layers.13.input_norm.weight"", ""layers.13.self_attention.query_key_value.weight"", ""layers.13.self_attention.dense.weight"", ""layers.13.post_attention_norm.weight"", ""layers.13.mlp.dense_h_to_4h.weight"", ""layers.13.mlp.dense_4h_to_h.weight"", ""layers.14.input_norm.weight"", ""layers.14.self_attention.query_key_value.weight"", ""layers.14.self_attention.dense.weight"", ""layers.14.post_attention_norm.weight"", ""layers.14.mlp.dense_h_to_4h.weight"", ""layers.14.mlp.dense_4h_to_h.weight"", ""layers.15.input_norm.weight"", ""layers.15.self_attention.query_key_value.weight"", ""layers.15.self_attention.dense.weight"", ""layers.15.post_attention_norm.weight"", ""layers.15.mlp.dense_h_to_4h.weight"", ""layers.15.mlp.dense_4h_to_h.weight"", ""layers.16.input_norm.weight"", ""layers.16.self_attention.query_key_value.weight"", ""layers.16.self_attention.dense.weight"", ""layers.16.post_attention_norm.weight"", ""layers.16.mlp.dense_h_to_4h.weight"", ""layers.16.mlp.dense_4h_to_h.weight"", ""layers.17.input_norm.weight"", ""layers.17.self_attention.query_key_value.weight"", ""layers.17.self_attention.dense.weight"", ""layers.17.post_attention_norm.weight"", ""layers.17.mlp.dense_h_to_4h.weight"", ""layers.17.mlp.dense_4h_to_h.weight"", ""layers.18.input_norm.weight"", ""layers.18.self_attention.query_key_value.weight"", ""layers.18.self_attention.dense.weight"", ""layers.18.post_attention_norm.weight"", ""layers.18.mlp.dense_h_to_4h.weight"", ""layers.18.mlp.dense_4h_to_h.weight"", ""layers.19.input_norm.weight"", ""layers.19.self_attention.query_key_value.weight"", ""layers.19.self_attention.dense.weight"", ""layers.19.post_attention_norm.weight"", ""layers.19.mlp.dense_h_to_4h.weight"", ""layers.19.mlp.dense_4h_to_h.weight"", ""layers.20.input_norm.weight"", ""layers.20.self_attention.query_key_value.weight"", ""layers.20.self_attention.dense.weight"", ""layers.20.post_attention_norm.weight"", ""layers.20.mlp.dense_h_to_4h.weight"", ""layers.20.mlp.dense_4h_to_h.weight"", ""layers.21.input_norm.weight"", ""layers.21.self_attention.query_key_value.weight"", ""layers.21.self_attention.dense.weight"", ""layers.21.post_attention_norm.weight"", ""layers.21.mlp.dense_h_to_4h.weight"", ""layers.21.mlp.dense_4h_to_h.weight"", ""layers.22.input_norm.weight"", ""layers.22.self_attention.query_key_value.weight"", ""layers.22.self_attention.dense.weight"", ""layers.22.post_attention_norm.weight"", ""layers.22.mlp.dense_h_to_4h.weight"", ""layers.22.mlp.dense_4h_to_h.weight"", ""layers.23.input_norm.weight"", ""layers.23.self_attention.query_key_value.weight"", ""layers.23.self_attention.dense.weight"", ""layers.23.post_attention_norm.weight"", ""layers.23.mlp.dense_h_to_4h.weight"", ""layers.23.mlp.dense_4h_to_h.weight"", ""layers.24.input_norm.weight"", ""layers.24.self_attention.query_key_value.weight"", ""layers.24.self_attention.dense.weight"", ""layers.24.post_attention_norm.weight"", ""layers.24.mlp.dense_h_to_4h.weight"", ""layers.24.mlp.dense_4h_to_h.weight"", ""layers.25.input_norm.weight"", ""layers.25.self_attention.query_key_value.weight"", ""layers.25.self_attention.dense.weight"", ""layers.25.post_attention_norm.weight"", ""layers.25.mlp.dense_h_to_4h.weight"", ""layers.25.mlp.dense_4h_to_h.weight"", ""layers.26.input_norm.weight"", ""layers.26.self_attention.query_key_value.weight"", ""layers.26.self_attention.dense.weight"", ""layers.26.post_attention_norm.weight"", ""layers.26.mlp.dense_h_to_4h.weight"", ""layers.26.mlp.dense_4h_to_h.weight"", ""layers.27.input_norm.weight"", ""layers.27.self_attention.query_key_value.weight"", ""layers.27.self_attention.dense.weight"", ""layers.27.post_attention_norm.weight"", ""layers.27.mlp.dense_h_to_4h.weight"", ""layers.27.mlp.dense_4h_to_h.weight"", ""layers.28.input_norm.weight"", ""layers.28.self_attention.query_key_value.weight"", ""layers.28.self_attention.dense.weight"", ""layers.28.post_attention_norm.weight"", ""layers.28.mlp.dense_h_to_4h.weight"", ""layers.28.mlp.dense_4h_to_h.weight"", ""layers.29.input_norm.weight"", ""layers.29.self_attention.query_key_value.weight"", ""layers.29.self_attention.dense.weight"", ""layers.29.post_attention_norm.weight"", ""layers.29.mlp.dense_h_to_4h.weight"", ""layers.29.mlp.dense_4h_to_h.weight"", ""layers.30.input_norm.weight"", ""layers.30.self_attention.query_key_value.weight"", ""layers.30.self_attention.dense.weight"", ""layers.30.post_attention_norm.weight"", ""layers.30.mlp.dense_h_to_4h.weight"", ""layers.30.mlp.dense_4h_to_h.weight"", ""layers.31.input_norm.weight"", ""layers.31.self_attention.query_key_value.weight"", ""layers.31.self_attention.dense.weight"", ""layers.31.post_attention_norm.weight"", ""layers.31.mlp.dense_h_to_4h.weight"", ""layers.31.mlp.dense_4h_to_h.weight"", ""layers.32.input_norm.weight"", ""layers.32.self_attention.query_key_value.weight"", ""layers.32.self_attention.dense.weight"", ""layers.32.post_attention_norm.weight"", ""layers.32.mlp.dense_h_to_4h.weight"", ""layers.32.mlp.dense_4h_to_h.weight"", ""layers.33.input_norm.weight"", ""layers.33.self_attention.query_key_value.weight"", ""layers.33.self_attention.dense.weight"", ""layers.33.post_attention_norm.weight"", ""layers.33.mlp.dense_h_to_4h.weight"", ""layers.33.mlp.dense_4h_to_h.weight"", ""layers.34.input_norm.weight"", ""layers.34.self_attention.query_key_value.weight"", ""layers.34.self_attention.dense.weight"", ""layers.34.post_attention_norm.weight"", ""layers.34.mlp.dense_h_to_4h.weight"", ""layers.34.mlp.dense_4h_to_h.weight"", ""layers.35.input_norm.weight"", ""layers.35.self_attention.query_key_value.weight"", ""layers.35.self_attention.dense.weight"", ""layers.35.post_attention_norm.weight"", ""layers.35.mlp.dense_h_to_4h.weight"", ""layers.35.mlp.dense_4h_to_h.weight"", ""layers.36.input_norm.weight"", ""layers.36.self_attention.query_key_value.weight"", ""layers.36.self_attention.dense.weight"", ""layers.36.post_attention_norm.weight"", ""layers.36.mlp.dense_h_to_4h.weight"", ""layers.36.mlp.dense_4h_to_h.weight"", ""layers.37.input_norm.weight"", ""layers.37.self_attention.query_key_value.weight"", ""layers.37.self_attention.dense.weight"", ""layers.37.post_attention_norm.weight"", ""layers.37.mlp.dense_h_to_4h.weight"", ""layers.37.mlp.dense_4h_to_h.weight"", ""layers.38.input_norm.weight"", ""layers.38.self_attention.query_key_value.weight"", ""layers.38.self_attention.dense.weight"", ""layers.38.post_attention_norm.weight"", ""layers.38.mlp.dense_h_to_4h.weight"", ""layers.38.mlp.dense_4h_to_h.weight"", ""layers.39.input_norm.weight"", ""layers.39.self_attention.query_key_value.weight"", ""layers.39.self_attention.dense.weight"", ""layers.39.post_attention_norm.weight"", ""layers.39.mlp.dense_h_to_4h.weight"", ""layers.39.mlp.dense_4h_to_h.weight"", ""layers.40.input_norm.weight"", ""layers.40.self_attention.query_key_value.weight"", ""layers.40.self_attention.dense.weight"", ""layers.40.post_attention_norm.weight"", ""layers.40.mlp.dense_h_to_4h.weight"", ""layers.40.mlp.dense_4h_to_h.weight"", ""layers.41.input_norm.weight"", ""layers.41.self_attention.query_key_value.weight"", ""layers.41.self_attention.dense.weight"", ""layers.41.post_attention_norm.weight"", ""layers.41.mlp.dense_h_to_4h.weight"", ""layers.41.mlp.dense_4h_to_h.weight"", ""layers.42.input_norm.weight"", ""layers.42.self_attention.query_key_value.weight"", ""layers.42.self_attention.dense.weight"", ""layers.42.post_attention_norm.weight"", ""layers.42.mlp.dense_h_to_4h.weight"", ""layers.42.mlp.dense_4h_to_h.weight"", ""layers.43.input_norm.weight"", ""layers.43.self_attention.query_key_value.weight"", ""layers.43.self_attention.dense.weight"", ""layers.43.post_attention_norm.weight"", ""layers.43.mlp.dense_h_to_4h.weight"", ""layers.43.mlp.dense_4h_to_h.weight"", ""layers.44.input_norm.weight"", ""layers.44.self_attention.query_key_value.weight"", ""layers.44.self_attention.dense.weight"", ""layers.44.post_attention_norm.weight"", ""layers.44.mlp.dense_h_to_4h.weight"", ""layers.44.mlp.dense_4h_to_h.weight"", ""layers.45.input_norm.weight"", ""layers.45.self_attention.query_key_value.weight"", ""layers.45.self_attention.dense.weight"", ""layers.45.post_attention_norm.weight"", ""layers.45.mlp.dense_h_to_4h.weight"", ""layers.45.mlp.dense_4h_to_h.weight"", ""layers.46.input_norm.weight"", ""layers.46.self_attention.query_key_value.weight"", ""layers.46.self_attention.dense.weight"", ""layers.46.post_attention_norm.weight"", ""layers.46.mlp.dense_h_to_4h.weight"", ""layers.46.mlp.dense_4h_to_h.weight"", ""layers.47.input_norm.weight"", ""layers.47.self_attention.query_key_value.weight"", ""layers.47.self_attention.dense.weight"", ""layers.47.post_attention_norm.weight"", ""layers.47.mlp.dense_h_to_4h.weight"", ""layers.47.mlp.dense_4h_to_h.weight"", ""layers.48.input_norm.weight"", ""layers.48.self_attention.query_key_value.weight"", ""layers.48.self_attention.dense.weight"", ""layers.48.post_attention_norm.weight"", ""layers.48.mlp.dense_h_to_4h.weight"", ""layers.48.mlp.dense_4h_to_h.weight"", ""layers.49.input_norm.weight"", ""layers.49.self_attention.query_key_value.weight"", ""layers.49.self_attention.dense.weight"", ""layers.49.post_attention_norm.weight"", ""layers.49.mlp.dense_h_to_4h.weight"", ""layers.49.mlp.dense_4h_to_h.weight"", ""layers.50.input_norm.weight"", ""layers.50.self_attention.query_key_value.weight"", ""layers.50.self_attention.dense.weight"", ""layers.50.post_attention_norm.weight"", ""layers.50.mlp.dense_h_to_4h.weight"", ""layers.50.mlp.dense_4h_to_h.weight"", ""layers.51.input_norm.weight"", ""layers.51.self_attention.query_key_value.weight"", ""layers.51.self_attention.dense.weight"", ""layers.51.post_attention_norm.weight"", ""layers.51.mlp.dense_h_to_4h.weight"", ""layers.51.mlp.dense_4h_to_h.weight"", ""layers.52.input_norm.weight"", ""layers.52.self_attention.query_key_value.weight"", ""layers.52.self_attention.dense.weight"", ""layers.52.post_attention_norm.weight"", ""layers.52.mlp.dense_h_to_4h.weight"", ""layers.52.mlp.dense_4h_to_h.weight"", ""layers.53.input_norm.weight"", ""layers.53.self_attention.query_key_value.weight"", ""layers.53.self_attention.dense.weight"", ""layers.53.post_attention_norm.weight"", ""layers.53.mlp.dense_h_to_4h.weight"", ""layers.53.mlp.dense_4h_to_h.weight"", ""layers.54.input_norm.weight"", ""layers.54.self_attention.query_key_value.weight"", ""layers.54.self_attention.dense.weight"", ""layers.54.post_attention_norm.weight"", ""layers.54.mlp.dense_h_to_4h.weight"", ""layers.54.mlp.dense_4h_to_h.weight"", ""layers.55.input_norm.weight"", ""layers.55.self_attention.query_key_value.weight"", ""layers.55.self_attention.dense.weight"", ""layers.55.post_attention_norm.weight"", ""layers.55.mlp.dense_h_to_4h.weight"", ""layers.55.mlp.dense_4h_to_h.weight"", ""final_norm.weight"".  root:/workspace/megatron  ``` **Environment (please complete the following information):**   MegatronLM commit ID: 703cc88   PyTorch version: `>>> torch.__version__.  '2.2.0a0+81ea7a4'`    CUDA version ``` root:/workspace/megatron/examples/mamba nvcc version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 20052023 NVIDIA Corporation Built on Wed_Nov_22_10:17:15_PST_2023 Cuda compilation tools, release 12.3, V12.3.107 Build cuda_12.3.r12.3/compiler.33567101_0 ```     NCCL version `2.8.10` **Proposed fix** I have no clues.  **Additional context** The pretrained model should come from distributed training across multigpus and multinodes. At the end the pretrained model is converted into TP=1 and PP=1. So there should be a way to do this. ",2024-09-23T19:07:37Z,,closed,0,7,https://github.com/NVIDIA/Megatron-LM/issues/1153,"Never mind, the regular `convert.py` doesn't work for Mamba, but the `hybrid_conversion.py` does. ","I am using the file 'hybrid_conversion. py', but I am encountering a new error message","File ""/workspace/mnt/xxx/MegatronLM/tools/checkpoint/hybrid_conversion.py"", line 66, in get_split_dim     raise Exception(""Unknown tensor name {}"".format(tensor_name)) Exception: Unknown tensor name decoder.layers.0.mixer.in_proj.layer_norm_weight","> File ""/workspace/mnt/xxx/MegatronLM/tools/checkpoint/hybrid_conversion.py"", line 66, in get_split_dim raise Exception(""Unknown tensor name {}"".format(tensor_name)) Exception: Unknown tensor name decoder.layers.0.mixer.in_proj.layer_norm_weight It seems like the file is not reading the correct format of the checkpoint. Are you sure the model checkpoint is in the right format? ",https://github.com/NVIDIA/MegatronLM/issues/1336,"!/bin/bash  Use: ./train.sh   MODEL_SCALE=""2.7B""  or ""8B"" case ""${MODEL_SCALE}"" in     ""800M"")         TENSOR_MODEL_PARALLEL_SIZE=1         NUM_LAYERS=48         HIDDEN_SIZE=1024         NUM_ATTENTION_HEADS=16         GLOBAL_BATCH_SIZE=32         ;;     ""8B"")         TENSOR_MODEL_PARALLEL_SIZE=4         NUM_LAYERS=56         HIDDEN_SIZE=4096         NUM_ATTENTION_HEADS=32         GLOBAL_BATCH_SIZE=8         ;;     ""2.7B"")         TENSOR_MODEL_PARALLEL_SIZE=2         NUM_LAYERS=64         HIDDEN_SIZE=2560         NUM_ATTENTION_HEADS=40         GLOBAL_BATCH_SIZE=256         ;;     *)         echo ""Invalid version specified""         exit 1         ;; esac DATA_PATH=""/workspace/mnt/xxx/process_datasets/magpie_llama3/magpie_llama3_text_document"" TOKENIZER_PATH=""/workspace/mnt/xxx/model/mambahf/tokenizer.json"" export NCCL_IB_SL=1 export CUDA_DEVICE_MAX_CONNECTIONS=1 export NCCL_IB_TIMEOUT=19 export NCCL_IB_QPS_PER_CONNECTION=4 export CUDA_PROFILER_OUTPUT_FILE=""/workspace/mnt/xxx/MegatronLM/profile""  mamba2 CHECKPOINT_DIR=""/workspace/mnt/xxx/models/mamba22.7b"" save_path=""/workspace/mnt/xxx/ckpt/mamba2test"" DATACACHE_DIR=""/workspace/mnt/xxx/process_datasets/magpie_llama3/magpie_llama3_text_document"" TENSORBOARD_DIR=""/workspace/mnt/xxx/MegatronLM/tensorboard/mamba2"" vocab_file=""/workspace/mnt/xxx/models/mamba22.7b/vocab.json"" merge_file=""/workspace/mnt/xxx/models/mamba22.7b/merges.txt"" mkdir p ${save_path} mkdir p ${CHECKPOINT_DIR} mkdir p ${DATACACHE_DIR} mkdir p ${TENSORBOARD_DIR} export TRITON_CACHE_DIR=""./tritoncache/"" export TRITON_CACHE_MANAGER=""megatron.core.ssm.triton_cache_manager:ParallelFileCacheManager"" SEQ_LEN=1024  TRAIN_SAMPLES=73242188   300B tokens / 4096  LR_WARMUP_SAMPLES=50000  LR_DECAY_SAMPLES=73192188  TRAIN_SAMPLES  LR_WARMUP_SAMPLES options="" \        tensormodelparallelsize ${TENSOR_MODEL_PARALLEL_SIZE} \        sequenceparallel \        pipelinemodelparallelsize 1 \        usedistributedoptimizer \        overlapparamgather \        overlapgradreduce \        vocabfile ${vocab_file} \        mergefile ${merge_file} \        untieembeddingsandoutputweights \        initmethodstd 0.02 \        positionembeddingtype none \        numlayers ${NUM_LAYERS} \        hiddensize ${HIDDEN_SIZE} \        numattentionheads ${NUM_ATTENTION_HEADS} \        groupqueryattention \        numquerygroups 8 \        hybridattentionratio 0.08 \        hybridmlpratio 0.5 \        seqlength ${SEQ_LEN} \        maxpositionembeddings ${SEQ_LEN} \        save ${save_path} \        load ${CHECKPOINT_DIR} \        datapath ${DATA_PATH} \        datacachepath ${DATACACHE_DIR} \        split 100,0,0 \        tokenizertype GPT2BPETokenizer \        tokenizermodel ${TOKENIZER_PATH} \        distributedbackend nccl \        microbatchsize 8 \        globalbatchsize ${GLOBAL_BATCH_SIZE} \        trainiters 1000 \        lrwarmupfraction 0.05 \        lr 2e4 \        minlr 1e5 \        lrdecaystyle cosine \        weightdecay 0.1 \        clipgrad 1.0 \        attentiondropout 0.0 \        hiddendropout 0.0 \        disablebiaslinear \        normalization RMSNorm \        adambeta1 0.9 \        adambeta2 0.95 \        loginterval 5 \        saveinterval 200 \        evalinterval 200 \        evaliters 256 \        fp32residualconnection \        bf16 \        usemcoremodels \        useflashattn \        noloadoptim \        noloadrng \        spec megatron.core.models.mamba.mamba_layer_specs mamba_stack_spec \        nocreateattentionmaskindataloader \        tensorboarddir ${TENSORBOARD_DIR}"" torchrun nproc_per_node 4 /workspace/mnt/xxx/MegatronLM/pretrain_mamba.py ${options} My training script is shown above and the checkpoint has also been successfully saved. You can see the issue I mentioned earlierã€‚","The above problem has been solved, but after conversion, it is still in pt format. How to convert it to safetensors format? https://github.com/NVIDIA/MegatronLM/issues/1339"
viclzhu,[BUG] Loss difference when training with FP8 vs. BF16 MoE,"**Describe the bug** When enabling FP8 mixed precision during training of a Mixtral model (`SequentialMLP` expert layer), we are observing that training and validation loss differs more than expected. **To Reproduce** Start with `examples/mixtral/train_mixtral_8x7b_distributed.sh`.   Disable `moegroupedgemm`.   Pass `fp8format hybrid fp8amaxcomputealgo max fp8amaxhistorylen 1024`. Using `tokenizer.model` from https://huggingface.co/mistralai/Mixtral8x7Bv0.1. **Expected behavior** Training and validation loss across BF16 and FP8 MoE should be approximately the same. **Stack trace/logs** ```  BF16 3:  [20240920 19:40:51] iteration        1/     100  ``` moe_megatron_bf16_22455.log moe_megatron_fp8_22454.log **Environment (please complete the following information):**   MegatronLM commit ID: 835af44a3   Megatroncore version: 0.9.0rc0   PyTorch version: 2.3.0a0+40ec155e58.nv24.3   CUDA version: 12.4   NCCL version: 2.19.4   TransformerEngine version: 1.8.0.dev0+7d576ed   Base Container: nvcr.io/nvidia/nemo:24.07 **Additional context** We also experimented with enabling FP8 with the `TEGroupedMLP` module (padding inputs for FP8), and see some loss differences there as well.",2024-09-20T22:16:29Z,,closed,0,9,https://github.com/NVIDIA/Megatron-LM/issues/1152,"I also encountered this problem, but I was on a 1b dense model and the 200 step difference reached 0.7",Try disabling the recomputation.,"yes, I find this. It is effective for me at TE=1.10, like this issue > Try disabling the recomputation.","Thanks for the responses! I reran with recomputation disabled, and also reduced the `num_layers` from 32 > 16 (due to memory constraints) and still observe a loss difference (though the difference is much smaller!). Is this level of loss difference expected for BF16 vs. FP08? It appears to be around 2e2 for the steps I've run. Changes: ``` recompute_granularity: None  set by not passing recompute_granularity numlayers 16 ``` ```  BF16 3:  [20240924 21:17:13] iteration        1/     100   ``` moe_megatron_bf16_no_recompute_l16_22863.log moe_megatron_fp8_no_recompute_l16_22836.log","I am not sure if disabling recomputation does work in your case, maybe you can doublecheck it by enabling recomputation and using 16 layers. I think the loss diff is acceptable at an early training stage, we expect the loss diff to decrease to <1e2 after several billion tokens.","I see, sounds good will try it! I also ran the bf16/fp08 no recompute jobs for a bit longer and observe the following: ```  FP08 iteration     1000/   20000  ``` My understanding is that this difference in train and validation loss is then acceptable due to the low number of tokens processed, and that with longer training time, the curves will be expected to converge on each other?","Using the same training parameter, bf16 run successfully, but fp8 OOMï¼Œ do you know how to set the initiating paramter","I think it depends on your config and hardware. For context, I was using 4 nodes each with 8xH100 for my experiments (you can check my logs above for arguments). If you're using less than that, you'll probably need to fiddle with the model parallelism parameters such as tensor parallel size and pipeline parallel size to fit with your configuration.",Does FP8 grouped gemm supported?
YJHMITWEB,[BUG] Context parallel gives NCCL error,"**Describe the bug** I am using the `train_gpt3_175b_distributed.sh` script to launch training on a single node with 4 A100 80GB GPUs. Training goes well if I use tensor parallel or pipeline parallel, but fails if I enable context parallel. The following is my script: ``` !/bin/bash  Runs the ""175B"" parameter model export CUDA_DEVICE_MAX_CONNECTIONS=1 GPUS_PER_NODE=4  Change for multinode config MASTER_ADDR=localhost MASTER_PORT=6000 NUM_NODES=1 NODE_RANK=0 WORLD_SIZE=$(($GPUS_PER_NODE*$NUM_NODES)) CHECKPOINT_PATH=$1  TENSORBOARD_LOGS_PATH=$2  VOCAB_FILE=$3 /gpt2vocab.json MERGE_FILE=$4 /gpt2merges.txt DATA_PATH=$5 _text_document DISTRIBUTED_ARGS=(     nproc_per_node $GPUS_PER_NODE      nnodes $NUM_NODES      master_addr $MASTER_ADDR      master_port $MASTER_PORT ) GPT_MODEL_ARGS=(     numlayers 32     hiddensize 2048     ffnhiddensize 8192      numattentionheads 16      seqlength 2048     maxpositionembeddings 2048 ) TRAINING_ARGS=(     microbatchsize 1      globalbatchsize 1      trainiters 500000      weightdecay 0.1      adambeta1 0.9      adambeta2 0.95      initmethodstd 0.006      clipgrad 1.0      fp16     lr 6.0e5      lrdecaystyle cosine      minlr 6.0e6     lrwarmupfraction .001      lrdecayiters 430000      recomputeactivations ) MODEL_PARALLEL_ARGS=( 	tensormodelparallelsize 1  	pipelinemodelparallelsize 1     contextparallelsize 4  ) DATA_ARGS=(     datapath ""/temp_document""     vocabfile ""/gpt2vocab.json""     mergefile ""/gpt2merges.txt""     split 949,50,1 ) EVAL_AND_LOGGING_ARGS=(     loginterval 100     saveinterval 10000      evalinterval 1000      save ""/temp/""     load ""/temp/""     evaliters 10     tensorboarddir ""/MegatronLM/examples/gpt3"" ) torchrun ${DISTRIBUTED_ARGS[@]} pretrain_gpt.py \     ${GPT_MODEL_ARGS[@]} \     ${TRAINING_ARGS[@]} \     ${MODEL_PARALLEL_ARGS[@]} \     ${DATA_ARGS[@]} \     ${EVAL_AND_LOGGING_ARGS[@]} ``` The output log is: ``` building GPT model ...  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1718685696  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1718685696  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1718685696  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1718685696 WARNING: could not find the metadata file /temp/latest_checkpointed_iteration.txt     will not load any checkpoints and will start from random /pytorch/torch/distributed/c10d_logger.py:83: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.   return func(*args, **kwargs) /pytorch/torch/distributed/c10d_logger.py:83: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.   return func(*args, **kwargs) /pytorch/torch/distributed/c10d_logger.py:83: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.   return func(*args, **kwargs) /pytorch/torch/distributed/c10d_logger.py:83: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.   return func(*args, **kwargs) (min, max) time across ranks (ms):     loadcheckpoint ................................: (0.73, 0.78) [after model, optimizer, and learning rate scheduler are built] datetime: 20240919 15:56:43  > building train, validation, and test datasets ...  > datasets target sizes (minimum size):     train:      500000     validation: 5010     test:       10 > building train, validation, and test datasets for GPT ... > finished creating GPT datasets ... [after dataloaders are built] datetime: 20240919 15:56:43  done with setup ... training ... (min, max) time across ranks (ms):     modelandoptimizersetup ......................: (300.04, 309.25)     train/valid/testdataiteratorssetup ..........: (288.56, 334.41) [before the start of training step] datetime: 20240919 15:56:43  WARNING:megatron.core.utils:NCCL Error 5: invalid usage (run with NCCL_DEBUG=WARN for details) ['Traceback (most recent call last):\n', '  File ""/MegatronLM/pretrain_gpt.py"", line 192, in forward_step\n    output_tensor = model(tokens, position_ids, attention_mask,\n', '  File ""/pytorch/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n', '  File ""/pytorch/torch/nn/modules/module.py"", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n', '  File ""/MegatronLM/megatron/core/distributed/distributed_data_parallel.py"", line 305, in forward\n    return self.module(*inputs, **kwargs)\n', '  File ""/pytorch/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n', '  File ""/pytorch/torch/nn/modules/module.py"", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n', '  File ""/MegatronLM/megatron/legacy/model/module.py"", line 189, in forward\n    outputs = self.module(*inputs, **kwargs)\n', '  File ""/pytorch/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n', '  File ""/pytorch/torch/nn/modules/module.py"", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n', '  File ""/MegatronLM/megatron/core/models/gpt/gpt_model.py"", line 217, in forward\n    hidden_states = self.decoder(\n', '  File ""/pytorch/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n', '  File ""/pytorch/torch/nn/modules/module.py"", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n', '  File ""/MegatronLM/megatron/core/transformer/transformer_block.py"", line 496, in forward\n    hidden_states, context = layer(\n', '  File ""/MegatronLM/megatron/core/transformer/transformer_layer.py"", line 377, in __call__\n    return super(MegatronModule, self).__call__(*args, **kwargs)\n', '  File ""/pytorch/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n', '  File ""/pytorch/torch/nn/modules/module.py"", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n', '  File ""/MegatronLM/megatron/core/transformer/transformer_layer.py"", line 281, in forward\n    attention_output_with_bias = self.self_attention(\n', '  File ""/pytorch/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n', '  File ""/pytorch/torch/nn/modules/module.py"", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n', '  File ""/MegatronLM/megatron/core/transformer/attention.py"", line 291, in forward\n    core_attn_out = self._checkpointed_attention_forward(\n', '  File ""/MegatronLM/megatron/core/transformer/attention.py"", line 143, in _checkpointed_attention_forward\n    hidden_states = tensor_parallel.checkpoint(\n', '  File ""/MegatronLM/megatron/core/tensor_parallel/random.py"", line 308, in checkpoint\n    return CheckpointFunction.apply(function, distribute_saved_activations, *args)\n', '  File ""/pytorch/torch/autograd/function.py"", line 575, in apply\n    return super().apply(*args, **kwargs)   type: ignore[misc]\n', '  File ""/MegatronLM/megatron/core/tensor_parallel/random.py"", line 247, in forward\n    outputs = run_function(*args)\n', '  File ""/MegatronLM/megatron/core/transformer/attention.py"", line 130, in custom_forward\n    output_ = self.core_attention(\n', '  File ""/pytorch/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n', '  File ""/pytorch/torch/nn/modules/module.py"", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n', '  File ""/MegatronLM/megatron/core/extensions/transformer_engine.py"", line 589, in forward\n    core_attn_out = super().forward(\n', '  File ""/miniconda3/envs/megatron_lm/lib/python3.10/sitepackages/transformer_engine/pytorch/attention.py"", line 6854, in forward\n    return self.flash_attention(\n', '  File ""/pytorch/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n', '  File ""/pytorch/torch/nn/modules/module.py"", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n', '  File ""/miniconda3/envs/megatron_lm/lib/python3.10/sitepackages/transformer_engine/pytorch/attention.py"", line 4328, in forward\n    output = attn_forward_func_with_cp(\n', '  File ""/miniconda3/envs/megatron_lm/lib/python3.10/sitepackages/transformer_engine/pytorch/attention.py"", line 3379, in attn_forward_func_with_cp\n    out = AttnFuncWithCPAndKVP2P.apply(\n', '  File ""/pytorch/torch/autograd/function.py"", line 575, in apply\n    return super().apply(*args, **kwargs)   type: ignore[misc]\n', '  File ""/miniconda3/envs/megatron_lm/lib/python3.10/sitepackages/transformer_engine/pytorch/attention.py"", line 1473, in forward\n    send_recv_reqs[i % 2] = flash_attn_p2p_communicate(\n', '  File ""/miniconda3/envs/megatron_lm/lib/python3.10/sitepackages/transformer_engine/pytorch/attention.py"", line 1249, in flash_attn_p2p_communicate\n    send_op = torch.distributed.isend(send_tensor, send_dst, cp_group)\n', '  File ""/pytorch/torch/distributed/distributed_c10d.py"", line 2071, in isend\n    return pg.send([tensor], dst, tag)\n', 'RuntimeError: NCCL Error 5: invalid usage (run with NCCL_DEBUG=WARN for details)\n'] [rank2]: Traceback (most recent call last): [rank2]:   File ""/MegatronLM/pretrain_gpt.py"", line 264, in  [rank2]:     pretrain( [rank2]:   File ""/MegatronLM/megatron/training/training.py"", line 355, in pretrain [rank2]:     iteration, num_floating_point_operations_so_far = train( [rank2]:   File ""/MegatronLM/megatron/training/training.py"", line 1234, in train [rank2]:     train_step(forward_step_func, [rank2]:   File ""/MegatronLM/megatron/training/training.py"", line 718, in train_step [rank2]:     losses_reduced = forward_backward_func( [rank2]:   File ""/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 468, in forward_backward_no_pipelining [rank2]:     output_tensor, num_tokens = forward_step( [rank2]:   File ""/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 273, in forward_step [rank2]:     output_tensor, loss_func = forward_step_func(data_iterator, model) [rank2]:   File ""/MegatronLM/pretrain_gpt.py"", line 192, in forward_step [rank2]:     output_tensor = model(tokens, position_ids, attention_mask, [rank2]:   File ""/pytorch/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl [rank2]:     return self._call_impl(*args, **kwargs) [rank2]:   File ""/pytorch/torch/nn/modules/module.py"", line 1747, in _call_impl [rank2]:     return forward_call(*args, **kwargs) [rank2]:   File ""/MegatronLM/megatron/core/distributed/distributed_data_parallel.py"", line 305, in forward [rank2]:     return self.module(*inputs, **kwargs) [rank2]:   File ""/pytorch/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl [rank2]:     return self._call_impl(*args, **kwargs) [rank2]:   File ""/pytorch/torch/nn/modules/module.py"", line 1747, in _call_impl [rank2]:     return forward_call(*args, **kwargs) [rank2]:   File ""/MegatronLM/megatron/legacy/model/module.py"", line 189, in forward [rank2]:     outputs = self.module(*inputs, **kwargs) [rank2]:   File ""/pytorch/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl [rank2]:     return self._call_impl(*args, **kwargs) [rank2]:   File ""/pytorch/torch/nn/modules/module.py"", line 1747, in _call_impl [rank2]:     return forward_call(*args, **kwargs) [rank2]:   File ""/MegatronLM/megatron/core/models/gpt/gpt_model.py"", line 217, in forward [rank2]:     hidden_states = self.decoder( [rank2]:   File ""/pytorch/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl [rank2]:     return self._call_impl(*args, **kwargs) [rank2]:   File ""/pytorch/torch/nn/modules/module.py"", line 1747, in _call_impl [rank2]:     return forward_call(*args, **kwargs) [rank2]:   File ""/MegatronLM/megatron/core/transformer/transformer_block.py"", line 496, in forward [rank2]:     hidden_states, context = layer( [rank2]:   File ""/MegatronLM/megatron/core/transformer/transformer_layer.py"", line 377, in __call__ [rank2]:     return super(MegatronModule, self).__call__(*args, **kwargs) [rank2]:   File ""/pytorch/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl [rank2]:     return self._call_impl(*args, **kwargs) [rank2]:   File ""/pytorch/torch/nn/modules/module.py"", line 1747, in _call_impl [rank2]:     return forward_call(*args, **kwargs) [rank2]:   File ""/MegatronLM/megatron/core/transformer/transformer_layer.py"", line 281, in forward [rank2]:     attention_output_with_bias = self.self_attention( [rank2]:   File ""/pytorch/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl [rank2]:     return self._call_impl(*args, **kwargs) [rank2]:   File ""/pytorch/torch/nn/modules/module.py"", line 1747, in _call_impl [rank2]:     return forward_call(*args, **kwargs) [rank2]:   File ""/MegatronLM/megatron/core/transformer/attention.py"", line 291, in forward [rank2]:     core_attn_out = self._checkpointed_attention_forward( [rank2]:   File ""/MegatronLM/megatron/core/transformer/attention.py"", line 143, in _checkpointed_attention_forward [rank2]:     hidden_states = tensor_parallel.checkpoint( [rank2]:   File ""/MegatronLM/megatron/core/tensor_parallel/random.py"", line 308, in checkpoint [rank2]:     return CheckpointFunction.apply(function, distribute_saved_activations, *args) [rank2]:   File ""/pytorch/torch/autograd/function.py"", line 575, in apply [rank2]:     return super().apply(*args, **kwargs)   type: ignore[misc] [rank2]:   File ""/MegatronLM/megatron/core/tensor_parallel/random.py"", line 247, in forward [rank2]:     outputs = run_function(*args) [rank2]:   File ""/MegatronLM/megatron/core/transformer/attention.py"", line 130, in custom_forward [rank2]:     output_ = self.core_attention( [rank2]:   File ""/pytorch/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl [rank2]:     return self._call_impl(*args, **kwargs) [rank2]:   File ""/pytorch/torch/nn/modules/module.py"", line 1747, in _call_impl [rank2]:     return forward_call(*args, **kwargs) [rank2]:   File ""/MegatronLM/megatron/core/extensions/transformer_engine.py"", line 589, in forward [rank2]:     core_attn_out = super().forward( [rank2]:   File ""/miniconda3/envs/megatron_lm/lib/python3.10/sitepackages/transformer_engine/pytorch/attention.py"", line 6854, in forward [rank2]:     return self.flash_attention( [rank2]:   File ""/pytorch/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl [rank2]:     return self._call_impl(*args, **kwargs) [rank2]:   File ""/pytorch/torch/nn/modules/module.py"", line 1747, in _call_impl [rank2]:     return forward_call(*args, **kwargs) [rank2]:   File ""/miniconda3/envs/megatron_lm/lib/python3.10/sitepackages/transformer_engine/pytorch/attention.py"", line 4328, in forward [rank2]:     output = attn_forward_func_with_cp( [rank2]:   File ""/miniconda3/envs/megatron_lm/lib/python3.10/sitepackages/transformer_engine/pytorch/attention.py"", line 3379, in attn_forward_func_with_cp [rank2]:     out = AttnFuncWithCPAndKVP2P.apply( [rank2]:   File ""/pytorch/torch/autograd/function.py"", line 575, in apply [rank2]:     return super().apply(*args, **kwargs)   type: ignore[misc] [rank2]:   File ""/miniconda3/envs/megatron_lm/lib/python3.10/sitepackages/transformer_engine/pytorch/attention.py"", line 1473, in forward [rank2]:     send_recv_reqs[i % 2] = flash_attn_p2p_communicate( [rank2]:   File ""/miniconda3/envs/megatron_lm/lib/python3.10/sitepackages/transformer_engine/pytorch/attention.py"", line 1249, in flash_attn_p2p_communicate [rank2]:     send_op = torch.distributed.isend(send_tensor, send_dst, cp_group) [rank2]:   File ""/pytorch/torch/distributed/distributed_c10d.py"", line 2071, in isend [rank2]:     return pg.send([tensor], dst, tag) [rank2]: RuntimeError: NCCL Error 5: invalid usage (run with NCCL_DEBUG=WARN for details) ``` **To Reproduce** Steps to reproduce the behavior. The easier it is to reproduce the faster it will get maintainer attention. **Expected behavior** A clear and concise description of what you expected to happen. **Stack trace/logs** If applicable, add the stack trace or logs from the time of the error. **Environment (please complete the following information):**   MegatronLM commit ID: commit 6b35ca80e8baca6f357b97304979e6b1c9a31899    PyTorch version: 2.6.0a0+git803ce50   CUDA version: 12.3.0   NCCL version: 2.21.5   TransformerEngine: 1.10.0   nvidiasmi topo m:  !Image **Proposed fix** If you have a proposal for how to fix the issue state it here or link to a PR. **Additional context** Add any other context about the problem here.",2024-09-19T20:05:34Z,,open,2,3,https://github.com/NVIDIA/Megatron-LM/issues/1151,Marking as stale. No activity in 60 days.,Same problem encountered.,"The environment variable `NVTE_BATCH_MHA_P2P_COMM` needs to be set as 1, then this error will not occur.  See the transformer_engine code here: https://github.com/NVIDIA/TransformerEngine/blob/303c6d16203b3cb01675f7adb7c21956f140e0ee/transformer_engine/pytorch/attention.pyL1869"
haolibai,[QUESTION] Adding new parameters in ColumnParallelLinear/RowParallelLinear raises AssertionError (Communication call has not been issued for this bucket) when using overlap-grad-reduce,"Hi, I am trying to add some new learnable parameters inside ColumnParallelLinear/RowParallelLinear, and the following is an example code snippet: ```python class ColumnParallelLinear(torch.nn.Module):     """"""Linear layer with column parallelism.     The linear layer is defined as Y = XA + b. A is parallelized along     its second dimension as A = [A_1, ..., A_p].     Args:         input_size: first dimension of matrix A.         output_size: second dimension of matrix A.         bias: If true, add bias         gather_output: If true, call allgather on output and make Y available to all GPUs, otherwise, every GPU will have its output which is Y_i = XA_i         init_method: method to initialize weights. Note that bias is always set to zero.         stride: For the strided linear layers.         keep_master_weight_for_test: This was added for testing and should be set to False. It returns the master weights used for initialization.         skip_bias_add: If True, do not add the bias term, instead return it to be added by the caller. This enables performance optimations where bias can be fused with other elementwise operations.         skip_weight_param_allocation: If True, weight parameter is not allocated and must be passed as a keyword argument `weight` during the forward pass. Note that this does not affect bias, which will be allocated if bias is True. Defaults to False.         embedding_activation_buffer: This buffer holds the input activations of the final embedding linear layer on the last pipeline stage when defer_embedding_wgrad_compute is enabled.         grad_output_buffer: This buffer holds the gradient outputs of the final embedding linear layer on the last pipeline stage when defer_embedding_wgrad_compute is enabled.         is_expert: If True, the layer is treated as an MoE expert layer.         config: ModelParallelConfig object         tp_comm_buffer_name: Communication buffer name is not used in nonTransformerEngine modules.     """"""     def __init__(         self,         input_size,         output_size,         *,         config: ModelParallelConfig,         init_method: Callable,         bias=True,         gather_output=False,         stride=1,         keep_master_weight_for_test=False,         skip_bias_add=False,         skip_weight_param_allocation: bool = False,         embedding_activation_buffer: Optional[List[torch.Tensor]] = None,         grad_output_buffer: Optional[List[torch.Tensor]] = None,         is_expert: bool = False,         tp_comm_buffer_name: str = None,   Not used     ):         super(ColumnParallelLinear, self).__init__()         ...          NOTE: a new parameter defined here (for example)         self.new_param = Parameter(torch.randn(config.hidden_size, dtype=config.params_dtype, device=torch.cuda.current_device()))     ))     def forward(self, input_: torch.Tensor, weight: Optional[torch.Tensor] = None):         ...         output = output * self.new_param         return output ``` However, this gives me the following error during training.  ```log Traceback (most recent call last):   File ""/home/mauser/work/haoli/code/PanGu/pretrain_gpt.py"", line 343, in main     pretrain(train_valid_test_datasets_provider,   File ""/home/mauser/work/haoli/code/PanGu/pangu/training/training.py"", line 271, in pretrain     iteration, num_floating_point_operations_so_far = train(   File ""/home/mauser/work/haoli/code/PanGu/pangu/training/training.py"", line 441, in train     train_step(forward_step_func,   File ""/home/mauser/work/haoli/code/third_party/MegatronLM/megatron/training/training.py"", line 553, in train_step     losses_reduced = forward_backward_func(   File ""/home/mauser/work/haoli/code/third_party/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 395, in forward_backward_no_pipelining     config.finalize_model_grads_func([model])   File ""/home/mauser/work/haoli/code/third_party/MegatronLM/megatron/core/distributed/finalize_model_grads.py"", line 135, in finalize_model_grads     model_chunk.finish_grad_sync()   File ""/home/mauser/work/haoli/code/third_party/MegatronLM/megatron/core/distributed/distributed_data_parallel.py"", line 242, in finish_grad_sync     buffer.finish_grad_sync()   File ""/home/mauser/work/haoli/code/third_party/MegatronLM/megatron/core/distributed/param_and_grad_buffer.py"", line 512, in finish_grad_sync     bucket.finish_grad_sync() Traceback (most recent call last):   File ""/home/mauser/work/haoli/code/pangu_sophon_pytorch/third_party/MegatronLM/megatron/core/distributed/param_and_grad_buffer.py"", line 157, in finish_grad_sync     assert self.communication_handle is not None and self.communication_issued, ( AssertionError: Communication call has not been issued for this bucket (1/2 params have grad available) ``` This happens when the following arguments are passed for training: ```log overlapparamgather overlapgradreduce ``` It seems the newly added parameter is not counted into `self.params_with_grad`. However, the training goes normal when I do the same procedure in other places, e.g., the __init__ fucntion of ParallelAttention, or ParallelMLP, with no such errors. ",2024-09-19T13:56:33Z,,open,0,9,https://github.com/NVIDIA/Megatron-LM/issues/1150,Did you solve this problem? I meet the same problem.,Not yet. But I found more colleagues around me also meet this problem.,same issue when using    overlapparamgather \  overlapgradreduce \,Also getting this error with ``` overlapparamgather overlapgradreduce ```,"I am having exactly the same issue. Using the distributed optimizer with EP, and enabling  overlapparamgather overlapgradreduce I get the same bucket problem","whatup guys, maybe my issue is related and if I'm right, you can fix this by locating custom layers in forward order",Marking as stale. No activity in 60 days.,+1+1+1+1,> same issue when using overlapparamgather overlapgradreduce \ Some quick update. I can avoid this issue and start training by removing these configs. Hope this can help more people.  But it seems it is still not clear to me what's the mechanism that causes this issue.
bugm,TikTokenizer tiktoken-pattern v1 and v2,"I noticed that when using TikTokenizer there are two  tiktoken pattern where ``` PATTERN_TIKTOKEN = r""[^\r\n\p{L}\p{N}]?+\p{L}+\\s+"" ``` can you offer some explanation to this two different pattern? Thanks",2024-09-18T11:02:04Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1147
go5paopao,Add/dataset docs,For open data document upload only,2024-09-16T14:11:39Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1146
zixianwang2022,[BUG] offset mismatched in gpt_dataset.py _query_document_sample_shuffle_indices,"**Describe the bug** Hi, line 276 of megatron/core/datasets/gpt_dataset.py has an offset mismatched bug, found in the most recent commit 703cc88.  The line is:   ``` offset = 0 if i > doc_index_beg else doc_index_beg_offset ``` This is to provide the beginning offset of the next input data of the dataset. The intention should be when index `i` is at 0, the starting offset should be whatever the sample's given doc_index_beg_offset.  However, by using `offset = doc_index_beg_offset` for the index 0 example causes the input_ids of that example is loaded **only a portion** of the next data's input_ids.  **How I found the bug**:  I created a dummy dataset where each text begins with `Q: __question_for_the_data__. \n A: ` Every data begins with ""Q: "", this will give me input_ids starting with `877, 251541` when using GPTSentencePieceTokenizer. However, when I print out the input_ids for each data in forward_step function, I observed that 1 data's input_ids does not start with `877, 251541`. When I looked into the code, I found that it was the `offset` that is causing the problem.  **To Reproduce** Follow what I described above by creating a dummy dataset where each text begins with ""Q: "". Include print statements are the two files mentioned above. I am running `MegatronLM/examples/mamba/train.sh` with 800M model, pure ssm structure and no attention, on 1 nproc.  **Expected behavior** The first data will not begin with `877, 251541`.  **Stack trace/logs** Here is the print statement results when including print in line 290.  ```         length = sum(map(len, sample_parts))         from megatron.training import get_tokenizer         tokenizer = get_tokenizer ()         a = tokenizer.detokenize (sample_parts[0].tolist())         print (f'\n input_ids : a: {sample_parts[0]} \n')         print (f'\n detokenized : a: {a} \n')         b = tokenizer.detokenize (sample_parts[1].tolist())         print (f'\n input_ids : b: {sample_parts[1]} \n')         print (f'\n detokenized : b: {b} \n')         c = tokenizer.detokenize (sample_parts[2].tolist())         print (f'\n input_ids : c: {sample_parts[2]} \n')         print (f'\n detokenized : c: {c} \n') ``` print output:  Theoretically, they all should print out ""Q: ..."". However, the first data did not begin with offset 0, which should be the start of the sentence, but rather starting from the middle.  ```  input_ids : a: [   625   2732  43607 251514    305   2245  14389 251597     14 251519  251541]   detokenized : a: which young performer, and American actress? A:   input_ids : b: [   877 251541    624    431   2867   5126    276   1194   8180  69342     291    276   1349 251514   3466    406    514    684   4325    291     261   1632    300  90683  22886    296    580    375   4940   4600    1012   4398  79772 251526    291    276   4750    305  14150    300    7236   3879    300    276   8440  66333     14 251519 251541]   detokenized : b: Q: who was once considered the best kick boxer in the world, however he has been involved in a number of controversies relating to his ""unsportsmanlike conducts"" in the sport and crimes of violence outside of the ring.? A:   input_ids : c: [   877 251541    796   2245   5090 251514   2204 251514    305   7179   10956 251490    624    676   6949    291    261   2619   1632    300  105584 251514   3872 110748    291    276 251490 251525 251561 251564  251561   2204    375  46038   1705    305   4303  33716   1371 251597      14 251519 251541]   detokenized : c: Q: what American stage, film, and television actor  who also appeared in a large number of musicals, played Samson in the 1949 film ""Samson and Delilah"".? A:  ``` **Environment (please complete the following information):**   MegatronLM commit ID: 703cc88   PyTorch version: `>>> torch.__version__.  '2.2.0a0+81ea7a4'`    CUDA version ``` root:/workspace/megatron/examples/mamba nvcc version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 20052023 NVIDIA Corporation Built on Wed_Nov_22_10:17:15_PST_2023 Cuda compilation tools, release 12.3, V12.3.107 Build cuda_12.3.r12.3/compiler.33567101_0 ```     NCCL version `2.8.10` **Proposed fix** Let `offset = 0` fixes the issue above.  **Additional context** Add any other context about the problem here.",2024-09-15T18:30:25Z,,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/1145,"This is not a bug. It is expected. For GPT pretraining, we pack samples using as many documents as is required. So when a document doesn't have enough tokens to fill a sample, we pull from other documents. This consequently means that certain documents will be part of more than one sample. In this case, the first few tokens which you're seeing ""lost"" likely belong to another sample in the dataset, i.e. the last few tokens at the last sample part in a different sample at a different index. Note also that you are referring to index 0 as the first sample, which is incorrect. Index 0 is remapped to another index using the shuffle index, so we're actually talking about a sample != 0 in this example.","> This is not a bug. It is expected. For GPT pretraining, we pack samples using as many documents as is required. So when a document doesn't have enough tokens to fill a sample, we pull from other documents. This consequently means that certain documents will be part of more than one sample. In this case, the first few tokens which you're seeing ""lost"" likely belong to another sample in the dataset, i.e. the last few tokens at the last sample part in a different sample at a different index. Note also that you are referring to index 0 as the first sample, which is incorrect. Index 0 is remapped to another index using the shuffle index, so we're actually talking about a sample != 0 in this example. Hi John, thanks for the clarification! Adding on to this, I have been wanting to finetune the Mamba2 8B model using MegatronLM. Is there a recommended to to do this besides modifying the pretrain_mamba.py?  I don't think there's a finetune version for Mamba specifically. As a result, I have to modify the pretrain_mamba.py and even upto the dataloader to give me 1 data at a time instead many data compacted together. ","The pretrain scripts are meant to serve as good examples of what's possible with megatron, but they don't cover all use cases. If the only difference between what is possible with the pretrain script and what you desire is the dataloader changing such that each dataset sample corresponds to one raw sample, then you should be able to edit the dataset class to do this fairly easily.","Closing this issue with the explanation above, please reopen or create a new one if there are further issues."
CCCCarpediem,[QUESTION] The Reason for calling torch.cuda.synchronize() in func recv_from_prev_pipeline_rank_/send_to_next_pipeline_rank,"Why do we need to Call ""torch.cuda.synchronize()"" to synchronize all streams in /megatron/core/inference/communication_utils.py? It describes as ""To protect against race condition when using batch_isend_irecv()"". But Event Record/ Event wait already inserted in communication/compute streams to ensure order. Thanks.",2024-09-14T06:37:44Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1144
zmtttt,[QUESTION]NCCL timeout error when running the second iteration,"I use one machine and 4GPUs to run gpt3ï¼› the first iteration is runnning without any errors, but the second iteration makes errors , strucked by the second iteration and  the second step, the erros as followsï¼š [iteration] datetime: 20240913 07:04:42  [E ProcessGroupNCCL.cpp:828] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=33, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 607565 milliseconds before timing out. [E ProcessGroupNCCL.cpp:828] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=257, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 608700 milliseconds before timing out. [E ProcessGroupNCCL.cpp:828] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1032, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 608832 milliseconds before timing out. [E ProcessGroupNCCL.cpp:828] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1796, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 608843 milliseconds before timing out. [E ProcessGroupNCCL.cpp:455] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data. [E ProcessGroupNCCL.cpp:460] To avoid data inconsistency, we are taking the entire process down. terminate called after throwing an instance of 'std::runtime_error'   what():  [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1032, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 608832 milliseconds before timing out. have anyone met the same problemï¼Ÿ thanks a lot",2024-09-13T08:28:41Z,stale,open,0,4,https://github.com/NVIDIA/Megatron-LM/issues/1142,"Yep same issue here,  We were able to make the pipeline parallelism work for the value 2 on the same node but beyond 2 and in multi nodes settings, it doesn't work.","> Yep same issue here, We were able to make the pipeline parallelism work for the value 2 on the same node but beyond 2 and in multi nodes settings, it doesn't work. I restarted the server and redownloaded the model and dataset, and the issue was resolved.",I pulled the master and it solved the issue,Marking as stale. No activity in 60 days.
zmtttt,[QUESTION]NCCL timeout error when the second iteration,"I use one machine and 4GPUs to run gpt3ï¼› the first iteration is runnning without any errors, but the second iteration makes errors , strucked by the second iteration and  the second step, the erros as followsï¼š [iteration] datetime: 20240913 07:04:42  [E ProcessGroupNCCL.cpp:828] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=33, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 607565 milliseconds before timing out. [E ProcessGroupNCCL.cpp:828] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=257, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 608700 milliseconds before timing out. [E ProcessGroupNCCL.cpp:828] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1032, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 608832 milliseconds before timing out. [E ProcessGroupNCCL.cpp:828] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1796, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 608843 milliseconds before timing out. [E ProcessGroupNCCL.cpp:455] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data. [E ProcessGroupNCCL.cpp:460] To avoid data inconsistency, we are taking the entire process down. terminate called after throwing an instance of 'std::runtime_error'   what():  [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1032, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 608832 milliseconds before timing out. have anyone met the same problemï¼Ÿ thanks a lot",2024-09-13T07:49:09Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1141,"i have met same problem,may I ask if this problem has been resolved and how it was resolved",Marking as stale. No activity in 60 days.
zmtttt,[QUESTION] NCCL timeout error when the second interation,"I use one machine and 4GPUs to run gpt3ï¼› the first iteration is runnning without any errors, but the second iteration makes errors , strucked by the second iteration and  the second step , the erros as followsï¼š [iteration] datetime: 20240913 07:04:42  [E ProcessGroupNCCL.cpp:828] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=33, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 607565 milliseconds before timing out. [E ProcessGroupNCCL.cpp:828] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=257, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 608700 milliseconds before timing out. [E ProcessGroupNCCL.cpp:828] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1032, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 608832 milliseconds before timing out. [E ProcessGroupNCCL.cpp:828] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1796, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 608843 milliseconds before timing out. [E ProcessGroupNCCL.cpp:455] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data. [E ProcessGroupNCCL.cpp:460] To avoid data inconsistency, we are taking the entire process down. terminate called after throwing an instance of 'std::runtime_error'   what():  [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1032, OpType=ALLREDUCE, Timeout(ms)=600000) ran for 608832 milliseconds before timing out. have anyone met the same problemï¼Ÿ thanks a lot",2024-09-13T07:47:58Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1140
eviler007,"[QUESTION] Why does GPTDataset not directly cache all samples document_index and sample_index, and then construct different shuffle_index for different parameters?","**Your question** Why does GPTDataset not directly cache all samples document_index and sample_index, and then construct different shuffle_index for different parameters?",2024-09-13T07:43:26Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1139,good ideaï¼,Marking as stale. No activity in 60 days.
TissueC,[BUG] Learning rate not overrided when set `--override-opt_param-scheduler`,"**Describe the bug** When setting `overrideopt_paramscheduler` (but still load optimizer and load rng) and setting new learning rate scheduler (including max lr, min lr, decay style, etc.), the learning rate still persists its original scheduler. A related issue could be https://github.com/NVIDIA/MegatronLM/issues/963 **To Reproduce** 1. Set max_lr as 6e4 and constant learning rate. 2. Training some steps and save checkpoint. 3. Load the checkpoint (including optimizer params) and override the scheduler (e.g. cosine 6e4 to 6e5). 4. Then the bug shows: the learning rate is still constantly 6e4. **Expected behavior** The learning rate scheduler should be overrided. **Environment (please complete the following information):**   MegatronLM commit ID [9bcd4175bec] **Proposed fix** The following code (https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/optimizer_param_scheduler.pyL121) leads to reloading the optimizer's learning rate when setting `overrideopt_paramscheduler` ```python def get_lr(self, param_group: dict) > float:         """"""Learning rate decay functions from:         https://openreview.net/pdf?id=BJYwwY9ll pg. 4         Args:             param_group (dict): parameter group from the optimizer.         """"""         max_lr = param_group.get('max_lr', self.max_lr)         min_lr = param_group.get('min_lr', self.min_lr) ``` A possible solution could be ```python def get_lr(self, param_group: dict) > float:         """"""Learning rate decay functions from:         https://openreview.net/pdf?id=BJYwwY9ll pg. 4         Args:             param_group (dict): parameter group from the optimizer.         """"""         max_lr = self.max_lr         min_lr = self.min_lr ```",2024-09-13T07:23:26Z,stale,open,0,4,https://github.com/NVIDIA/Megatron-LM/issues/1138,  Even I hit this issue. Proposed solution may not work when there are decoupled lrs. I have a fix. Will test and raise PR soon for Nvidia folks to review and merge.," Great! I'm looking forward to it. Actually the proposed solution was just reverting the commit of decouple lr, thus it indeed could not work when there are decoupled lrs.","  In training.py , you can do below till I get time to raise a proper PR.  Please import _update_min_and_max_lr_in_param_groups ```     if args.load is not None or args.pretrained_checkpoint is not None:         one_logger and one_logger.log_metrics({             'load_checkpoint_start_time': one_logger_utils.get_timestamp_in_ms()         })         timers('loadcheckpoint', log_level=0).start(barrier=True)         args.iteration, args.num_floating_point_operations_so_far = load_checkpoint(             model, optimizer, opt_param_scheduler)         timers('loadcheckpoint').stop(barrier=True)         timers.log(['loadcheckpoint'])         one_logger and one_logger.log_metrics({             'load_checkpoint_finish_time': one_logger_utils.get_timestamp_in_ms(),             'load_checkpoint_time': timers('loadcheckpoint').active_time()         })     else:         args.iteration = 0         args.num_floating_point_operations_so_far = 0     if args.override_opt_param_scheduler:          Call below again so that the load checkpoint values are overriden again by the input values         optimizer.param_groups = _update_min_and_max_lr_in_param_groups(             optimizer.param_groups,             lr=config.lr,             min_lr=config.min_lr,             decoupled_lr=config.decoupled_lr,             decoupled_min_lr=config.decoupled_min_lr,         ) ```",Marking as stale. No activity in 60 days.
lifeiteng,Fix typo lobal_smoothing -> label_smoothing,,2024-09-13T07:05:40Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1137,I'll include your commit in my internal fix PR.,Your commit is squash merged in https://github.com/NVIDIA/MegatronLM/commit/2488e20d488b35259f6eb53267e8a0525a373599 and your author information is preserved.  Thanks for your contribution. You can close this PR now. 
taozhiwei,[ENHANCEMENT]Is Megatron planning to use flux technologyï¼ŸIntegrating communication and gemm into one operator to improve overlap rate,https://arxiv.org/abs/2406.06858v1  https://github.com/bytedance/flux Is Megatron planning to use flux technologyï¼ŸIntegrating communication and gemm into one operator to improve overlap rate.,2024-09-13T06:59:16Z,stale,open,0,4,https://github.com/NVIDIA/Megatron-LM/issues/1136,We are looking into an approach to fuse the GEMM and its dependent communication into a single kernel. The support for such an optimization will take some time to ensure reliability.,Marking as stale. No activity in 60 days.," Hi, is there any progress on this? Flux is really hard to train and we are really looking forward to the support of Megatron...",Marking as stale. No activity in 60 days.
ianporada,[ENHANCEMENT] Preprocessing data that is already partitioned and gzipped,"**Is your feature request related to a problem? Please describe.** Using tools/preprocess_data.py, I would like to preprocess the Dolma dataset, which is already split into ~2000 `.json.gz` files, without rewriting the entire dataset to intermediate partition `.json` files. To achieve this, I would like to set `args.workers` << `args.partitions` because it is infeasible to have 2000+ workers. **Describe the solution you'd like** Removing the following constraint to support preprocessing data that is already partitioned: https://github.com/NVIDIA/MegatronLM/blob/76f9f48939ba5ecff0fed7bfbd4204df05d3e4da/tools/preprocess_data.pyL343 **Describe alternatives you've considered** Possible alternative: preprocessing the files in separate batches where for each batch `args.workers % args.partitions == 0`. This can be achieved by creating symlinks to each file that follow the appropriate partition file naming convention (`_{id}.json`) and running preprocess_data.py independently for each batch. **Proposed implementation** A possible solution is to modify the preprocessing logic to remove the constraint `args.workers % args.partitions == 0` and directly process each of the input files. **Additional context** Something to note is that when the input is of the form`*.json.gz` then extension is currently calculated as "".gz"" and therefore the partition file pattern is `*.json_{id}.gz` https://github.com/NVIDIA/MegatronLM/blob/76f9f48939ba5ecff0fed7bfbd4204df05d3e4da/tools/preprocess_data.pyL252",2024-09-13T04:42:48Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1135,Marking as stale. No activity in 60 days.
hwang2006,[BUG] 'NoneType' object has no attribute 'shape' error raised when saving model state with the pretrain_gpt.py,"Hi, It seems that the same code is **working fine with when the MegatronLM that I gitcloned in April. With the latest MegatronLM, I've got the following error raised with the pretrain_gpt.py code. It seems that the Megatron cores codes have been upgraded since April, Note that for some testing purpose,  I set saveinterval argument is set to be 50 below.  ``` $  CUDA_DEVICE_MAX_CONNECTIONS=1 torchrun nproc_per_node 2  pretrain_gpt.py     tensormodelparallelsize 2     pipelinemodelparallelsize 1         numlayers 24     hiddensize 1024     numattentionheads 16     seqlength 1024     maxpositionembeddings 1024     microbatchsize 4     globalbatchsize 16     lr 0.00015     trainiters 200     lrdecayiters 320000     lrdecaystyle cosine     minlr 1.0e5     weightdecay 1e2     lrwarmupfraction .01     clipgrad 1.0     fp16 datapath  mygpt2_text_document     vocabfile vocab.json     mergefile merges.txt     split 949,50,1 loginterval 10     saveinterval 50    evalinterval 100     evaliters 10 distributedbackend nccl  save checkpoints/gpt2_345m_dist_mp     load  checkpoints/gpt2_345m_dist_mp attentionsoftmaxinfp32 sequenceparallel W0913 13:50:04.400000 47065146630080 torch/distributed/run.py:779] W0913 13:50:04.400000 47065146630080 torch/distributed/run.py:779] ***************************************** W0913 13:50:04.400000 47065146630080 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. W0913 13:50:04.400000 47065146630080 torch/distributed/run.py:779] ***************************************** /scratch/qualis/test/MegatronLM/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.   def forward(ctx, input, weight, bias, allreduce_dgrad): /scratch/qualis/test/MegatronLM/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.   def backward(ctx, grad_output): /scratch/qualis/test/MegatronLM/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.   def forward(ctx, input, weight, bias, allreduce_dgrad): /scratch/qualis/test/MegatronLM/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.   def backward(ctx, grad_output): /scratch/qualis/test/MegatronLM/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.   def forward( /scratch/qualis/test/MegatronLM/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.   def backward(ctx, grad_output): /scratch/qualis/test/MegatronLM/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.   def forward( /scratch/qualis/test/MegatronLM/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.   def backward(ctx, grad_output): using world size: 2, dataparallel size: 1, contextparallel size: 1, tensormodelparallel size: 2, encodertensormodelparallel size: 0, pipelinemodelparallel size: 1, encoderpipelinemodelparallel size: 0 WARNING: Setting args.overlap_p2p_comm and args.align_param_gather to False since noninterleaved schedule does not support overlapping p2p communication and aligned param AG WARNING: Setting args.check_for_nan_in_loss_and_grad to False since dynamic loss scaling is being used using torch.float16 for parameters ...  arguments    accumulate_allreduce_grads_in_fp32 .............. False   adam_beta1 ...................................... 0.9   adam_beta2 ...................................... 0.999   adam_eps ........................................ 1e08   add_bias_linear ................................. True   add_position_embedding .......................... True   add_qkv_bias .................................... False   adlr_autoresume ................................. False   adlr_autoresume_interval ........................ 1000   align_grad_reduce ............................... True   align_param_gather .............................. False   app_tag_run_name ................................ None   app_tag_run_version ............................. 0.0.0   apply_layernorm_1p .............................. False   apply_query_key_layer_scaling ................... False   apply_residual_connection_post_layernorm ........ False   apply_rope_fusion ............................... True   async_save ...................................... None   async_tensor_model_parallel_allreduce ........... False   attention_dropout ............................... 0.1   attention_softmax_in_fp32 ....................... True   auto_detect_ckpt_format ......................... False   barrier_with_L1_time ............................ True   bert_binary_head ................................ True   bert_embedder_type .............................. megatron   bert_load ....................................... None   bf16 ............................................ False   bias_dropout_fusion ............................. True   bias_gelu_fusion ................................ True   bias_swiglu_fusion .............................. True   biencoder_projection_dim ........................ 0   biencoder_shared_query_context_model ............ False   block_data_path ................................. None   calculate_per_token_loss ........................ False   check_for_nan_in_loss_and_grad .................. False   check_weight_hash_across_dp_replicas_interval ... None   ckpt_assume_constant_structure .................. False   ckpt_convert_format ............................. None   ckpt_convert_save ............................... None   ckpt_convert_update_legacy_dist_opt_format ...... False   ckpt_format ..................................... torch_dist   ckpt_fully_parallel_load ........................ False   ckpt_fully_parallel_save ........................ True   ckpt_fully_parallel_save_deprecated ............. False   ckpt_step ....................................... None   classes_fraction ................................ 1.0   clip_grad ....................................... 1.0   clone_scatter_output_in_embedding ............... True   config_logger_dir ...............................   consumed_train_samples .......................... 0   consumed_valid_samples .......................... 0   context_parallel_size ........................... 1   create_attention_mask_in_dataloader ............. True   cross_entropy_loss_fusion ....................... False   data_cache_path ................................. None   data_parallel_random_init ....................... False   data_parallel_size .............................. 1   data_path ....................................... ['mygpt2_text_document']   data_per_class_fraction ......................... 1.0   data_sharding ................................... True   dataloader_type ................................. single   ddp_average_in_collective ....................... False   ddp_bucket_size ................................. None   decoder_first_pipeline_num_layers ............... None   decoder_last_pipeline_num_layers ................ None   decoder_num_layers .............................. None   decoder_seq_length .............................. None   decoupled_lr .................................... None   decoupled_min_lr ................................ None   decrease_batch_size_if_needed ................... False   defer_embedding_wgrad_compute ................... False   deprecated_use_mcore_models ..................... False   deterministic_mode .............................. False   dino_bottleneck_size ............................ 256   dino_freeze_last_layer .......................... 1   dino_head_hidden_size ........................... 2048   dino_local_crops_number ......................... 10   dino_local_img_size ............................. 96   dino_norm_last_layer ............................ False   dino_teacher_temp ............................... 0.07   dino_warmup_teacher_temp ........................ 0.04   dino_warmup_teacher_temp_epochs ................. 30   disable_straggler_on_startup .................... False   dist_ckpt_format_deprecated ..................... None   dist_ckpt_strictness ............................ assume_ok_unexpected   distribute_saved_activations .................... False   distributed_backend ............................. nccl   distributed_timeout_minutes ..................... 10   embedding_path .................................. None   empty_unused_memory_level ....................... 0   enable_ft_package ............................... False   enable_one_logger ............................... True   encoder_num_layers .............................. 24   encoder_pipeline_model_parallel_size ............ 0   encoder_seq_length .............................. 1024   encoder_tensor_model_parallel_size .............. 0   end_weight_decay ................................ 0.01   eod_mask_loss ................................... False   eval_interval ................................... 100   eval_iters ...................................... 10   evidence_data_path .............................. None   exit_duration_in_mins ........................... None   exit_interval ................................... None   exit_on_missing_checkpoint ...................... False   exit_signal_handler ............................. False   expert_model_parallel_size ...................... 1   ffn_hidden_size ................................. 4096   finetune ........................................ False   fp16 ............................................ True   fp16_lm_cross_entropy ........................... False   fp32_residual_connection ........................ False   fp8 ............................................. None   fp8_amax_compute_algo ........................... most_recent   fp8_amax_history_len ............................ 1   fp8_interval .................................... 1   fp8_margin ...................................... 0   fp8_param_gather ................................ False   fp8_wgrad ....................................... True   global_batch_size ............................... 16   gradient_accumulation_fusion .................... True   group_query_attention ........................... False   head_lr_mult .................................... 1.0   hidden_dropout .................................. 0.1   hidden_size ..................................... 1024   hybrid_attention_ratio .......................... 0.0   hybrid_mlp_ratio ................................ 0.0   hybrid_override_pattern ......................... None   hysteresis ...................................... 2   ict_head_size ................................... None   ict_load ........................................ None   img_h ........................................... 224   img_w ........................................... 224   indexer_batch_size .............................. 128   indexer_log_interval ............................ 1000   inference_batch_times_seqlen_threshold .......... 512   init_method_std ................................. 0.02   init_method_xavier_uniform ...................... False   initial_loss_scale .............................. 4294967296   iter_per_epoch .................................. 1250   kv_channels ..................................... 64   lazy_mpu_init ................................... None   load ............................................ checkpoints/gpt2_345m_dist_mp   local_rank ...................................... 0   log_interval .................................... 10   log_loss_scale_to_tensorboard ................... True   log_memory_to_tensorboard ....................... False   log_num_zeros_in_grad ........................... False   log_params_norm ................................. False   log_progress .................................... False   log_straggler ................................... False   log_throughput .................................. False   log_timers_to_tensorboard ....................... False   log_validation_ppl_to_tensorboard ............... False   log_world_size_to_tensorboard ................... False   logging_level ................................... None   loss_scale ...................................... None   loss_scale_window ............................... 1000   lr .............................................. 0.00015   lr_decay_iters .................................. 320000   lr_decay_samples ................................ None   lr_decay_style .................................. cosine   lr_warmup_fraction .............................. 0.01   lr_warmup_init .................................. 0.0   lr_warmup_iters ................................. 0   lr_warmup_samples ............................... 0   lr_wsd_decay_iters .............................. None   lr_wsd_decay_samples ............................ None   lr_wsd_decay_style .............................. exponential   make_vocab_size_divisible_by .................... 128   manual_gc ....................................... False   manual_gc_eval .................................. True   manual_gc_interval .............................. 0   mask_factor ..................................... 1.0   mask_prob ....................................... 0.15   mask_type ....................................... random   masked_softmax_fusion ........................... True   max_position_embeddings ......................... 1024   max_tokens_to_oom ............................... 12000   merge_file ...................................... merges.txt   micro_batch_size ................................ 4   min_loss_scale .................................. 1.0   min_lr .......................................... 1e05   mmap_bin_files .................................. True   mock_data ....................................... False   moe_aux_loss_coeff .............................. 0.0   moe_expert_capacity_factor ...................... None   moe_extended_tp ................................. False   moe_grouped_gemm ................................ False   moe_input_jitter_eps ............................ None   moe_layer_recompute ............................. False   moe_pad_expert_input_to_capacity ................ False   moe_per_layer_logging ........................... False   moe_router_load_balancing_type .................. aux_loss   moe_router_pre_softmax .......................... False   moe_router_topk ................................. 2   moe_token_dispatcher_type ....................... allgather   moe_token_drop_policy ........................... probs   moe_use_upcycling ............................... False   moe_z_loss_coeff ................................ None   nccl_communicator_config_path ................... None   no_load_optim ................................... None   no_load_rng ..................................... None   no_persist_layer_norm ........................... False   no_save_optim ................................... None   no_save_rng ..................................... None   non_persistent_ckpt_type ........................ None   non_persistent_global_ckpt_dir .................. None   non_persistent_local_ckpt_algo .................. fully_parallel   non_persistent_local_ckpt_dir ................... None   non_persistent_save_interval .................... None   norm_epsilon .................................... 1e05   normalization ................................... LayerNorm   num_attention_heads ............................. 16   num_channels .................................... 3   num_classes ..................................... 1000   num_dataset_builder_threads ..................... 1   num_experts ..................................... None   num_layers ...................................... 24   num_layers_per_virtual_pipeline_stage ........... None   num_query_groups ................................ 1   num_workers ..................................... 2   one_logger_async ................................ False   one_logger_project .............................. megatronlm   one_logger_run_name ............................. None   onnx_safe ....................................... None   openai_gelu ..................................... False   optimizer ....................................... adam   output_bert_embeddings .......................... False   overlap_grad_reduce ............................. False   overlap_p2p_comm ................................ False   overlap_param_gather ............................ False   overlap_param_gather_with_optimizer_step ........ False   override_opt_param_scheduler .................... False   params_dtype .................................... torch.float16   patch_dim ....................................... 16   perform_initialization .......................... True   pipeline_model_parallel_size .................... 1   pipeline_model_parallel_split_rank .............. None   position_embedding_type ......................... learned_absolute   pretrained_checkpoint ........................... None   profile ......................................... False   profile_ranks ................................... [0]   profile_step_end ................................ 12   profile_step_start .............................. 10   qk_layernorm .................................... False   query_in_block_prob ............................. 0.1   rampup_batch_size ............................... None   rank ............................................ 0   recompute_granularity ........................... None   recompute_method ................................ None   recompute_num_layers ............................ None   renormalize_blend_weights ....................... False   reset_attention_mask ............................ False   reset_position_ids .............................. False   retriever_report_topk_accuracies ................ []   retriever_score_scaling ......................... False   retriever_seq_length ............................ 256   retro_add_retriever ............................. False   retro_attention_gate ............................ 1   retro_cyclic_train_iters ........................ None   retro_encoder_attention_dropout ................. 0.1   retro_encoder_hidden_dropout .................... 0.1   retro_encoder_layers ............................ 2   retro_num_neighbors ............................. 2   retro_num_retrieved_chunks ...................... 2   retro_project_dir ............................... None   retro_verify_neighbor_count ..................... True   rotary_base ..................................... 10000   rotary_interleaved .............................. False   rotary_percent .................................. 1.0   rotary_seq_len_interpolation_factor ............. None   s3_cache_path ................................... None   sample_rate ..................................... 1.0   save ............................................ checkpoints/gpt2_345m_dist_mp   save_interval ................................... 50   scatter_gather_tensors_in_pipeline .............. True   seed ............................................ 1234   seq_length ...................................... 1024   sequence_parallel ............................... True   sgd_momentum .................................... 0.9   short_seq_prob .................................. 0.1   skip_train ...................................... False   skipped_train_samples ........................... 0   spec ............................................ None   split ........................................... 949,50,1   squared_relu .................................... False   standalone_embedding_stage ...................... False   start_weight_decay .............................. 0.01   straggler_ctrlr_port ............................ 65535   straggler_minmax_count .......................... 1   swiglu .......................................... False   swin_backbone_type .............................. tiny   tensor_model_parallel_size ...................... 2   tensorboard_dir ................................. None   tensorboard_log_interval ........................ 1   tensorboard_queue_size .......................... 1000   test_data_path .................................. None   test_mode ....................................... False   tiktoken_num_special_tokens ..................... 1000   tiktoken_pattern ................................ None   tiktoken_special_tokens ......................... None   timing_log_level ................................ 0   timing_log_option ............................... minmax   titles_data_path ................................ None   tokenizer_model ................................. None   tokenizer_type .................................. GPT2BPETokenizer   tp_comm_bulk_dgrad .............................. True   tp_comm_bulk_wgrad .............................. True   tp_comm_overlap ................................. False   tp_comm_overlap_ag .............................. True   tp_comm_overlap_cfg ............................. None   tp_comm_overlap_rs .............................. True   tp_comm_overlap_rs_dgrad ........................ False   tp_comm_split_ag ................................ True   tp_comm_split_rs ................................ True   train_data_path ................................. None   train_iters ..................................... 200   train_samples ................................... None   train_sync_interval ............................. None   transformer_impl ................................ transformer_engine   transformer_pipeline_model_parallel_size ........ 1   untie_embeddings_and_output_weights ............. False   use_checkpoint_args ............................. False   use_checkpoint_opt_param_scheduler .............. False   use_cpu_initialization .......................... None   use_dist_ckpt ................................... True   use_dist_ckpt_deprecated ........................ False   use_distributed_optimizer ....................... False   use_flash_attn .................................. False   use_legacy_models ............................... False   use_one_sent_docs ............................... False   use_pytorch_profiler ............................ False   use_ring_exchange_p2p ........................... False   use_rotary_position_embeddings .................. False   use_tp_pp_dp_mapping ............................ False   valid_data_path ................................. None   variable_seq_lengths ............................ False   virtual_pipeline_model_parallel_size ............ None   vision_backbone_type ............................ vit   vision_pretraining .............................. False   vision_pretraining_type ......................... classify   vocab_extra_ids ................................. 0   vocab_file ...................................... vocab.json   vocab_size ...................................... None   wandb_exp_name ..................................   wandb_project ...................................   wandb_save_dir ..................................   weight_decay .................................... 0.01   weight_decay_incr_style ......................... constant   wgrad_deferral_limit ............................ 0   world_size ...................................... 2   yaml_cfg ........................................ None end of arguments  INFO:megatron.core.num_microbatches_calculator:setting number of microbatches to constant 4 > building GPT2BPETokenizer tokenizer ...  > padded vocab (size: 50257) with 175 dummy tokens (new size: 50432) WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it > initializing torch distributed ... > initialized tensor model parallel with size 2 > initialized pipeline model parallel with size 1 > setting random seeds to 1234 ... > compiling dataset index builder ... make: Entering directory `/scratch/qualis/test/MegatronLM/megatron/core/datasets' make: Nothing to be done for `default'. make: Leaving directory `/scratch/qualis/test/MegatronLM/megatron/core/datasets' >>> done with dataset index builder. Compilation time: 0.079 seconds > compiling and loading fused kernels ... >>> done with compiling and loading fused kernels. Compilation time: 0.187 seconds [rank1]:[W913 13:50:25.428850695 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank0]:[W913 13:50:25.428869340 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) time to initialize megatron (seconds): 13.838 [after megatron is initialized] datetime: 20240913 13:50:34 building GPT model ...  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 178100224  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 178100224 INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, overlap_param_gather=False, align_param_gather=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None, average_in_collective=False, fp8_param_gather=False) INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient allreduce / reducescatter: 1 Params for bucket 1 (178100224 elements):         module.decoder.layers.5.self_attention.linear_proj.weight         module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.2.mlp.linear_fc1.layer_norm_bias         module.decoder.layers.0.mlp.linear_fc1.bias         module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.16.mlp.linear_fc2.weight         module.decoder.layers.5.mlp.linear_fc2.bias         module.decoder.layers.4.mlp.linear_fc2.bias         module.decoder.layers.3.mlp.linear_fc1.weight         module.decoder.layers.2.mlp.linear_fc2.bias         module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.0.self_attention.linear_qkv.bias         module.decoder.layers.7.self_attention.linear_qkv.layer_norm_bias         module.decoder.layers.7.mlp.linear_fc1.weight         module.decoder.layers.23.self_attention.linear_qkv.layer_norm_bias         module.decoder.layers.10.mlp.linear_fc2.weight         module.decoder.layers.9.mlp.linear_fc2.weight         module.decoder.layers.4.self_attention.linear_proj.weight         module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.23.mlp.linear_fc1.bias         module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.5.self_attention.linear_qkv.weight         module.decoder.layers.2.mlp.linear_fc1.weight         module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.6.mlp.linear_fc1.bias         module.decoder.layers.3.mlp.linear_fc1.bias         module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.23.self_attention.linear_proj.bias         module.decoder.layers.21.mlp.linear_fc1.layer_norm_bias         module.decoder.layers.20.mlp.linear_fc1.layer_norm_bias         module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.15.mlp.linear_fc2.bias         module.decoder.layers.10.self_attention.linear_qkv.layer_norm_bias         module.decoder.layers.2.mlp.linear_fc2.weight         module.decoder.layers.0.mlp.linear_fc2.weight         module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.1.self_attention.linear_qkv.bias         module.embedding.word_embeddings.weight         module.decoder.layers.6.mlp.linear_fc2.bias         module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.8.self_attention.linear_qkv.layer_norm_bias         module.decoder.layers.9.mlp.linear_fc1.bias         module.decoder.layers.8.mlp.linear_fc1.weight         module.decoder.layers.4.mlp.linear_fc2.weight         module.decoder.layers.23.self_attention.linear_qkv.bias         module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.3.self_attention.linear_proj.bias         module.decoder.layers.1.mlp.linear_fc1.bias         module.embedding.position_embeddings.weight         module.decoder.layers.23.mlp.linear_fc1.weight         module.decoder.layers.22.mlp.linear_fc1.layer_norm_bias         module.decoder.layers.21.self_attention.linear_qkv.layer_norm_bias         module.decoder.layers.20.self_attention.linear_qkv.layer_norm_bias         module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.5.mlp.linear_fc2.weight         module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.0.self_attention.linear_proj.weight         module.decoder.layers.23.self_attention.linear_proj.weight         module.decoder.layers.21.mlp.linear_fc1.bias         module.decoder.layers.20.mlp.linear_fc1.bias         module.decoder.layers.19.mlp.linear_fc1.layer_norm_bias         module.decoder.layers.18.mlp.linear_fc1.layer_norm_bias         module.decoder.layers.17.mlp.linear_fc1.layer_norm_bias         module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.11.self_attention.linear_qkv.bias         module.decoder.layers.2.self_attention.linear_proj.weight         module.decoder.layers.23.mlp.linear_fc2.bias         module.decoder.layers.21.self_attention.linear_proj.bias         module.decoder.layers.20.self_attention.linear_proj.bias         module.decoder.layers.14.mlp.linear_fc1.layer_norm_bias         module.decoder.layers.13.mlp.linear_fc1.layer_norm_bias         module.decoder.layers.12.mlp.linear_fc1.layer_norm_bias         module.decoder.layers.11.mlp.linear_fc1.layer_norm_bias         module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.8.mlp.linear_fc1.layer_norm_bias         module.decoder.layers.7.mlp.linear_fc1.layer_norm_bias         module.decoder.layers.6.mlp.linear_fc1.layer_norm_bias         module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.0.self_attention.linear_qkv.weight         module.decoder.layers.2.self_attention.linear_qkv.weight         module.decoder.layers.23.self_attention.linear_qkv.weight         module.decoder.layers.22.self_attention.linear_qkv.layer_norm_bias         module.decoder.layers.3.self_attention.linear_qkv.weight         module.decoder.final_layernorm.weight         module.decoder.layers.22.mlp.linear_fc1.bias         module.decoder.layers.21.self_attention.linear_qkv.bias         module.decoder.layers.20.self_attention.linear_qkv.bias         module.decoder.layers.19.self_attention.linear_qkv.layer_norm_bias         module.decoder.layers.18.self_attention.linear_qkv.layer_norm_bias         module.decoder.layers.17.self_attention.linear_qkv.layer_norm_bias         module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.9.mlp.linear_fc1.layer_norm_bias         module.decoder.layers.4.self_attention.linear_proj.bias         module.decoder.layers.3.mlp.linear_fc2.weight         module.decoder.layers.3.mlp.linear_fc2.bias         module.decoder.layers.21.mlp.linear_fc1.weight         module.decoder.layers.20.mlp.linear_fc1.weight         module.decoder.layers.19.mlp.linear_fc1.bias         module.decoder.layers.18.mlp.linear_fc1.bias         module.decoder.layers.17.mlp.linear_fc1.bias         module.decoder.layers.16.mlp.linear_fc1.layer_norm_bias         module.decoder.layers.15.mlp.linear_fc1.layer_norm_bias         module.decoder.layers.15.self_attention.linear_qkv.layer_norm_bias         module.decoder.layers.14.self_attention.linear_qkv.layer_norm_bias         module.decoder.layers.13.self_attention.linear_qkv.layer_norm_bias         module.decoder.layers.19.self_attention.linear_proj.bias         module.decoder.layers.18.self_attention.linear_proj.bias         module.decoder.layers.21.self_attention.linear_proj.weight         module.decoder.layers.20.self_attention.linear_proj.weight         module.decoder.layers.17.self_attention.linear_proj.bias         module.decoder.layers.14.mlp.linear_fc1.bias         module.decoder.layers.13.mlp.linear_fc1.bias         module.decoder.layers.12.mlp.linear_fc1.bias         module.decoder.layers.11.mlp.linear_fc1.bias         module.decoder.layers.10.mlp.linear_fc1.layer_norm_bias         module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.1.self_attention.linear_proj.bias         module.decoder.layers.1.mlp.linear_fc2.weight         module.decoder.layers.21.mlp.linear_fc2.bias         module.decoder.layers.20.mlp.linear_fc2.bias         module.decoder.layers.15.self_attention.linear_proj.bias         module.decoder.layers.14.self_attention.linear_proj.bias         module.decoder.layers.13.self_attention.linear_proj.bias         module.decoder.layers.12.self_attention.linear_proj.bias         module.decoder.layers.11.self_attention.linear_proj.bias         module.decoder.layers.8.mlp.linear_fc1.bias         module.decoder.layers.11.self_attention.linear_qkv.layer_norm_bias         module.decoder.layers.7.mlp.linear_fc1.bias         module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.23.mlp.linear_fc2.weight         module.decoder.layers.22.self_attention.linear_qkv.bias         module.decoder.layers.9.self_attention.linear_proj.bias         module.decoder.layers.8.self_attention.linear_proj.bias         module.decoder.layers.7.self_attention.linear_proj.bias         module.decoder.layers.6.self_attention.linear_proj.bias         module.decoder.layers.4.self_attention.linear_qkv.bias         module.decoder.layers.22.mlp.linear_fc1.weight         module.decoder.layers.21.self_attention.linear_qkv.weight         module.decoder.layers.20.self_attention.linear_qkv.weight         module.decoder.layers.19.self_attention.linear_qkv.bias         module.decoder.layers.18.self_attention.linear_qkv.bias         module.decoder.layers.17.self_attention.linear_qkv.bias         module.decoder.layers.16.self_attention.linear_qkv.layer_norm_bias         module.decoder.layers.4.self_attention.linear_qkv.layer_norm_bias         module.decoder.layers.2.self_attention.linear_qkv.bias         module.decoder.layers.4.mlp.linear_fc1.layer_norm_bias         module.decoder.layers.22.self_attention.linear_proj.bias         module.decoder.layers.19.mlp.linear_fc1.weight         module.decoder.layers.18.mlp.linear_fc1.weight         module.decoder.layers.17.mlp.linear_fc1.weight         module.decoder.layers.16.mlp.linear_fc1.bias         module.decoder.layers.15.mlp.linear_fc2.weight         module.decoder.layers.15.self_attention.linear_qkv.bias         module.decoder.layers.14.self_attention.linear_qkv.bias         module.decoder.layers.13.self_attention.linear_qkv.bias         module.decoder.layers.12.self_attention.linear_qkv.bias         module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.18.self_attention.linear_proj.weight         module.decoder.layers.22.mlp.linear_fc2.bias         module.decoder.layers.19.self_attention.linear_proj.weight         module.decoder.layers.17.self_attention.linear_proj.weight         module.decoder.layers.16.self_attention.linear_proj.bias         module.decoder.layers.14.mlp.linear_fc1.weight         module.decoder.layers.13.mlp.linear_fc1.weight         module.decoder.layers.12.mlp.linear_fc1.weight         module.decoder.layers.11.mlp.linear_fc1.weight         module.decoder.layers.10.mlp.linear_fc1.bias         module.decoder.layers.0.self_attention.linear_qkv.layer_norm_bias         module.decoder.layers.19.mlp.linear_fc2.bias         module.decoder.layers.18.mlp.linear_fc2.bias         module.decoder.layers.17.mlp.linear_fc2.bias         module.decoder.layers.15.self_attention.linear_proj.weight         module.decoder.layers.14.self_attention.linear_proj.weight         module.decoder.layers.13.self_attention.linear_proj.weight         module.decoder.layers.12.self_attention.linear_proj.weight         module.decoder.layers.11.self_attention.linear_proj.weight         module.decoder.layers.12.self_attention.linear_qkv.layer_norm_bias         module.decoder.layers.10.self_attention.linear_proj.bias         module.decoder.layers.6.self_attention.linear_qkv.bias         module.decoder.layers.22.self_attention.linear_qkv.weight         module.decoder.layers.14.mlp.linear_fc2.bias         module.decoder.layers.13.mlp.linear_fc2.bias         module.decoder.layers.12.mlp.linear_fc2.bias         module.decoder.layers.11.mlp.linear_fc2.bias         module.decoder.layers.9.self_attention.linear_proj.weight         module.decoder.layers.8.self_attention.linear_proj.weight         module.decoder.layers.7.self_attention.linear_proj.weight         module.decoder.layers.6.self_attention.linear_proj.weight         module.decoder.layers.3.self_attention.linear_qkv.bias         module.decoder.layers.1.mlp.linear_fc2.bias         module.decoder.layers.18.self_attention.linear_qkv.weight         module.decoder.final_layernorm.bias         module.decoder.layers.21.mlp.linear_fc2.weight         module.decoder.layers.20.mlp.linear_fc2.weight         module.decoder.layers.19.self_attention.linear_qkv.weight         module.decoder.layers.17.self_attention.linear_qkv.weight         module.decoder.layers.16.self_attention.linear_qkv.bias         module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.8.mlp.linear_fc2.bias         module.decoder.layers.7.mlp.linear_fc2.bias         module.decoder.layers.5.mlp.linear_fc1.layer_norm_bias         module.decoder.layers.22.self_attention.linear_proj.weight         module.decoder.layers.16.mlp.linear_fc1.weight         module.decoder.layers.15.mlp.linear_fc1.bias         module.decoder.layers.15.self_attention.linear_qkv.weight         module.decoder.layers.14.self_attention.linear_qkv.weight         module.decoder.layers.13.self_attention.linear_qkv.weight         module.decoder.layers.12.self_attention.linear_qkv.weight         module.decoder.layers.11.self_attention.linear_qkv.weight         module.decoder.layers.10.self_attention.linear_qkv.bias         module.decoder.layers.1.mlp.linear_fc1.layer_norm_bias         module.decoder.layers.0.mlp.linear_fc2.bias         module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.16.self_attention.linear_proj.weight         module.decoder.layers.10.mlp.linear_fc1.weight         module.decoder.layers.9.mlp.linear_fc1.weight         module.decoder.layers.8.self_attention.linear_qkv.weight         module.decoder.layers.7.self_attention.linear_qkv.weight         module.decoder.layers.6.self_attention.linear_qkv.weight         module.decoder.layers.5.self_attention.linear_qkv.layer_norm_bias         module.decoder.layers.4.self_attention.linear_qkv.weight         module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.1.self_attention.linear_qkv.layer_norm_bias         module.decoder.layers.1.self_attention.linear_proj.weight         module.decoder.layers.16.mlp.linear_fc2.bias         module.decoder.layers.10.self_attention.linear_proj.weight         module.decoder.layers.5.mlp.linear_fc1.bias         module.decoder.layers.4.mlp.linear_fc1.bias         module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.7.self_attention.linear_qkv.bias         module.decoder.layers.2.mlp.linear_fc1.bias         module.decoder.layers.22.mlp.linear_fc2.weight         module.decoder.layers.10.mlp.linear_fc2.bias         module.decoder.layers.9.mlp.linear_fc2.bias         module.decoder.layers.9.self_attention.linear_qkv.weight         module.decoder.layers.5.self_attention.linear_proj.bias         module.decoder.layers.3.self_attention.linear_qkv.layer_norm_bias         module.decoder.layers.1.mlp.linear_fc1.weight         module.decoder.layers.0.self_attention.linear_proj.bias         module.decoder.layers.18.mlp.linear_fc2.weight         module.decoder.layers.19.mlp.linear_fc2.weight         module.decoder.layers.17.mlp.linear_fc2.weight         module.decoder.layers.16.self_attention.linear_qkv.weight         module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.6.self_attention.linear_qkv.layer_norm_bias         module.decoder.layers.6.mlp.linear_fc1.weight         module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.15.mlp.linear_fc1.weight         module.decoder.layers.14.mlp.linear_fc2.weight         module.decoder.layers.13.mlp.linear_fc2.weight         module.decoder.layers.12.mlp.linear_fc2.weight         module.decoder.layers.11.mlp.linear_fc2.weight         module.decoder.layers.10.self_attention.linear_qkv.weight         module.decoder.layers.9.self_attention.linear_qkv.bias         module.decoder.layers.3.self_attention.linear_proj.weight         module.decoder.layers.2.self_attention.linear_proj.bias         module.decoder.layers.1.self_attention.linear_qkv.weight         module.decoder.layers.23.mlp.linear_fc1.layer_norm_bias         module.decoder.layers.8.mlp.linear_fc2.weight         module.decoder.layers.7.mlp.linear_fc2.weight         module.decoder.layers.6.mlp.linear_fc2.weight         module.decoder.layers.5.self_attention.linear_qkv.bias         module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.5.mlp.linear_fc1.weight         module.decoder.layers.4.mlp.linear_fc1.weight         module.decoder.layers.3.mlp.linear_fc1.layer_norm_bias         module.decoder.layers.0.mlp.linear_fc1.weight         module.decoder.layers.2.self_attention.linear_qkv.layer_norm_bias         module.decoder.layers.9.self_attention.linear_qkv.layer_norm_bias         module.decoder.layers.8.self_attention.linear_qkv.bias         module.decoder.layers.0.mlp.linear_fc1.layer_norm_bias INFO:megatron.core.optimizer:Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_param_gather_with_optimizer_step=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=, config_logger_dir='') INFO:megatron.core.optimizer_param_scheduler:> learning rate decay style: cosine WARNING: could not find the metadata file checkpoints/gpt2_345m_dist_mp/latest_checkpointed_iteration.txt     will not load any checkpoints and will start from random /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/torch/distributed/c10d_logger.py:79: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.   return func(*args, **kwargs) /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/torch/distributed/c10d_logger.py:79: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.   return func(*args, **kwargs) (min, max) time across ranks (ms):     loadcheckpoint ................................: (0.80, 0.81) [after model, optimizer, and learning rate scheduler are built] datetime: 20240913 13:50:36 > building train, validation, and test datasets ...  > datasets target sizes (minimum size):     train:      3200     validation: 480     test:       160 INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)] > building train, validation, and test datasets for GPT ... INFO:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(3200, 480, 160), and config=GPTDatasetConfig(random_seed=1234, sequence_length=1024, blend=(['mygpt2_text_document'], None), blend_per_split=[None, None, None], renormalize_blend_weights=False, split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], num_dataset_builder_threads=1, path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True, drop_last_partial_validation_sequence=True, add_extra_token_to_sequence=True, s3_cache_path=None) INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from mygpt2_text_document.idx INFO:megatron.core.datasets.indexed_dataset:    Extract the sequence lengths INFO:megatron.core.datasets.indexed_dataset:    Extract the sequence pointers INFO:megatron.core.datasets.indexed_dataset:    Extract the document indices INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 10000 INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 10000 INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices INFO:megatron.core.datasets.gpt_dataset:        Load the document index from a375193ee7bd0ffbfa0d131aff630f11GPTDatasettraindocument_index.npy INFO:megatron.core.datasets.gpt_dataset:        Load the sample index from a375193ee7bd0ffbfa0d131aff630f11GPTDatasettrainsample_index.npy INFO:megatron.core.datasets.gpt_dataset:        Load the shuffle index from a375193ee7bd0ffbfa0d131aff630f11GPTDatasettrainshuffle_index.npy INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 3570 INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices INFO:megatron.core.datasets.gpt_dataset:        Load the document index from e9d835bb86bcaca4cd08b4977794f406GPTDatasetvaliddocument_index.npy INFO:megatron.core.datasets.gpt_dataset:        Load the sample index from e9d835bb86bcaca4cd08b4977794f406GPTDatasetvalidsample_index.npy INFO:megatron.core.datasets.gpt_dataset:        Load the shuffle index from e9d835bb86bcaca4cd08b4977794f406GPTDatasetvalidshuffle_index.npy INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 530 INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices INFO:megatron.core.datasets.gpt_dataset:        Load the document index from fceb23beb87bf7aa15e27415203c9636GPTDatasettestdocument_index.npy INFO:megatron.core.datasets.gpt_dataset:        Load the sample index from fceb23beb87bf7aa15e27415203c9636GPTDatasettestsample_index.npy INFO:megatron.core.datasets.gpt_dataset:        Load the shuffle index from fceb23beb87bf7aa15e27415203c9636GPTDatasettestshuffle_index.npy INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 161 > finished creating GPT datasets ... [after dataloaders are built] datetime: 20240913 13:50:37 done with setup ... training ... (min, max) time across ranks (ms):     modelandoptimizersetup ......................: (1990.13, 1990.64)     train/valid/testdataiteratorssetup ..........: (54.45, 431.36) [before the start of training step] datetime: 20240913 13:50:37 /scratch/qualis/test/MegatronLM/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.   warnings.warn( /scratch/qualis/test/MegatronLM/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.   warnings.warn( /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/torch/distributed/c10d_logger.py:79: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.   return func(*args, **kwargs) /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/torch/distributed/c10d_logger.py:79: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.   return func(*args, **kwargs) /scratch/qualis/test/MegatronLM/megatron/core/tensor_parallel/layers.py:475: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.   handle = torch.distributed._reduce_scatter_base( /scratch/qualis/test/MegatronLM/megatron/core/tensor_parallel/layers.py:475: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.   handle = torch.distributed._reduce_scatter_base( /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/torch/distributed/c10d_logger.py:79: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.   return func(*args, **kwargs) /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/torch/distributed/c10d_logger.py:79: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.   return func(*args, **kwargs) /scratch/qualis/test/MegatronLM/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.   warnings.warn( /scratch/qualis/test/MegatronLM/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.   warnings.warn( /scratch/qualis/test/MegatronLM/megatron/core/tensor_parallel/layers.py:475: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.   handle = torch.distributed._reduce_scatter_base( /scratch/qualis/test/MegatronLM/megatron/core/tensor_parallel/layers.py:475: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.   handle = torch.distributed._reduce_scatter_base(  [20240913 13:50:58] iteration       10/     200  saving checkpoint at iteration      50 to checkpoints/gpt2_345m_dist_mp in torch_dist format [rank0]: Traceback (most recent call last): [rank0]:   File ""/scratch/qualis/test/MegatronLM/pretrain_gpt.py"", line 264, in  [rank0]:     pretrain( [rank0]:   File ""/scratch/qualis/test/MegatronLM/megatron/training/training.py"", line 348, in pretrain [rank0]:     iteration, num_floating_point_operations_so_far = train( [rank0]:   File ""/scratch/qualis/test/MegatronLM/megatron/training/training.py"", line 1361, in train [rank0]:     save_checkpoint_and_time(iteration, model, optimizer, [rank0]:   File ""/scratch/qualis/test/MegatronLM/megatron/training/training.py"", line 1065, in save_checkpoint_and_time [rank0]:     save_checkpoint(iteration, model, optimizer, opt_param_scheduler, [rank0]:   File ""/scratch/qualis/test/MegatronLM/megatron/training/checkpointing.py"", line 401, in save_checkpoint [rank0]:     state_dict = generate_state_dict( [rank0]:   File ""/scratch/qualis/test/MegatronLM/megatron/training/checkpointing.py"", line 613, in generate_state_dict [rank0]:     state_dict['optimizer'] = (optimizer.sharded_state_dict(state_dict, **(optim_sd_kwargs or {})) [rank0]:   File ""/scratch/qualis/test/MegatronLM/megatron/core/optimizer/optimizer.py"", line 654, in sharded_state_dict [rank0]:     optim_state_to_sharding_state( [rank0]:   File ""/scratch/qualis/test/MegatronLM/megatron/core/dist_checkpointing/optimizer.py"", line 123, in optim_state_to_sharding_state [rank0]:     sharded_state[param_id][state_key] = make_sharded_optimizer_tensor( [rank0]:   File ""/scratch/qualis/test/MegatronLM/megatron/core/dist_checkpointing/optimizer.py"", line 86, in make_sharded_optimizer_tensor [rank0]:     tuple(optim_param.shape) == model_param.local_shape [rank0]: AttributeError: 'NoneType' object has no attribute 'shape' [rank1]: Traceback (most recent call last): [rank1]:   File ""/scratch/qualis/test/MegatronLM/pretrain_gpt.py"", line 264, in  [rank1]:     pretrain( [rank1]:   File ""/scratch/qualis/test/MegatronLM/megatron/training/training.py"", line 348, in pretrain [rank1]:     iteration, num_floating_point_operations_so_far = train( [rank1]:   File ""/scratch/qualis/test/MegatronLM/megatron/training/training.py"", line 1361, in train [rank1]:     save_checkpoint_and_time(iteration, model, optimizer, [rank1]:   File ""/scratch/qualis/test/MegatronLM/megatron/training/training.py"", line 1065, in save_checkpoint_and_time [rank1]:     save_checkpoint(iteration, model, optimizer, opt_param_scheduler, [rank1]:   File ""/scratch/qualis/test/MegatronLM/megatron/training/checkpointing.py"", line 401, in save_checkpoint [rank1]:     state_dict = generate_state_dict( [rank1]:   File ""/scratch/qualis/test/MegatronLM/megatron/training/checkpointing.py"", line 613, in generate_state_dict [rank1]:     state_dict['optimizer'] = (optimizer.sharded_state_dict(state_dict, **(optim_sd_kwargs or {})) [rank1]:   File ""/scratch/qualis/test/MegatronLM/megatron/core/optimizer/optimizer.py"", line 654, in sharded_state_dict [rank1]:     optim_state_to_sharding_state( [rank1]:   File ""/scratch/qualis/test/MegatronLM/megatron/core/dist_checkpointing/optimizer.py"", line 123, in optim_state_to_sharding_state [rank1]:     sharded_state[param_id][state_key] = make_sharded_optimizer_tensor( [rank1]:   File ""/scratch/qualis/test/MegatronLM/megatron/core/dist_checkpointing/optimizer.py"", line 86, in make_sharded_optimizer_tensor [rank1]:     tuple(optim_param.shape) == model_param.local_shape [rank1]: AttributeError: 'NoneType' object has no attribute 'shape' [rank1]:[W913 13:51:18.226165487 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator()) W0913 13:51:27.911000 47065146630080 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 52065 closing signal SIGTERM E0913 13:51:28.128000 47065146630080 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 1 (pid: 52066) of binary: /scratch/qualis/miniconda3/envs/megatron/bin/python Traceback (most recent call last):   File ""/scratch/qualis/miniconda3/envs/megatron/bin/torchrun"", line 33, in      sys.exit(load_entry_point('torch==2.4.0', 'console_scripts', 'torchrun')())   File ""/scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 348, in wrapper     return f(*args, **kwargs)   File ""/scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/torch/distributed/run.py"", line 901, in main     run(args)   File ""/scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/torch/distributed/run.py"", line 892, in run     elastic_launch(   File ""/scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/torch/distributed/launcher/api.py"", line 133, in __call__     return launch_agent(self._config, self._entrypoint, list(args))   File ""/scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/torch/distributed/launcher/api.py"", line 264, in launch_agent     raise ChildFailedError( torch.distributed.elastic.multiprocessing.errors.ChildFailedError: ============================================================ pretrain_gpt.py FAILED  Failures:     Root Cause (first observed failure): [0]:   time      : 20240913_13:51:27   host      : gpu36.eth   rank      : 1 (local_rank: 1)   exitcode  : 1 (pid: 52066)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html ============================================================ ``` Any comment or suggestion would be appreciated.",2024-09-13T02:21:38Z,,open,0,12,https://github.com/NVIDIA/Megatron-LM/issues/1134,"Hi, here is the same code working fine against the previous MegatronLM. ``` $  CUDA_DEVICE_MAX_CONNECTIONS=1 torchrun nproc_per_node 2  pretrain_gpt.py     tensormodelparallelsize 2     pipelinemodelparallelsize 1         numlayers 24     hiddensize 1024     numattentionheads 16     seqlength 1024     maxpositionembeddings 1024     microbatchsize 4     globalbatchsize 16     lr 0.00015     trainiters 200     lrdecayiters 320000     lrdecaystyle cosine     minlr 1.0e5     weightdecay 1e2     lrwarmupfraction .01     clipgrad 1.0     fp16 datapath  mygpt2_text_document     vocabfile vocab.json     mergefile merges.txt     split 949,50,1 loginterval 10     saveinterval 50    evalinterval 100     evaliters 10 distributedbackend nccl  save checkpoints/gpt2_345m_dist_mp     load  checkpoints/gpt2_345m_dist_mp attentionsoftmaxinfp32 sequenceparallel W0913 14:09:55.827000 47463259365312 torch/distributed/run.py:779] W0913 14:09:55.827000 47463259365312 torch/distributed/run.py:779] ***************************************** W0913 14:09:55.827000 47463259365312 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. W0913 14:09:55.827000 47463259365312 torch/distributed/run.py:779] ***************************************** /scratch/qualis/test/MegatronLM.bak/megatron/core/tensor_parallel/layers.py:260: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.   def forward( /scratch/qualis/test/MegatronLM.bak/megatron/core/tensor_parallel/layers.py:271: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.   def backward(ctx, grad_output): /scratch/qualis/test/MegatronLM.bak/megatron/core/tensor_parallel/layers.py:341: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.   def forward( /scratch/qualis/test/MegatronLM.bak/megatron/core/tensor_parallel/layers.py:378: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.   def backward(ctx, grad_output): /scratch/qualis/test/MegatronLM.bak/megatron/core/tensor_parallel/layers.py:260: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.   def forward( /scratch/qualis/test/MegatronLM.bak/megatron/core/tensor_parallel/layers.py:271: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.   def backward(ctx, grad_output): /scratch/qualis/test/MegatronLM.bak/megatron/core/tensor_parallel/layers.py:341: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.   def forward( /scratch/qualis/test/MegatronLM.bak/megatron/core/tensor_parallel/layers.py:378: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.   def backward(ctx, grad_output): using world size: 2, dataparallel size: 1, contextparallel size: 1 tensormodelparallel size: 2, pipelinemodelparallel size: 1 WARNING: Setting args.overlap_p2p_comm to False since noninterleaved schedule does not support overlapping p2p communication WARNING: Setting args.check_for_nan_in_loss_and_grad to False since dynamic loss scaling is being used using torch.float16 for parameters ...  arguments    accumulate_allreduce_grads_in_fp32 .............. False   adam_beta1 ...................................... 0.9   adam_beta2 ...................................... 0.999   adam_eps ........................................ 1e08   add_bias_linear ................................. True   add_position_embedding .......................... True   add_qkv_bias .................................... False   adlr_autoresume ................................. False   adlr_autoresume_interval ........................ 1000   apply_layernorm_1p .............................. False   apply_query_key_layer_scaling ................... False   apply_residual_connection_post_layernorm ........ False   apply_rope_fusion ............................... True   async_tensor_model_parallel_allreduce ........... False   attention_dropout ............................... 0.1   attention_softmax_in_fp32 ....................... True   auto_detect_ckpt_format ......................... False   barrier_with_L1_time ............................ True   bert_binary_head ................................ True   bert_embedder_type .............................. megatron   bert_load ....................................... None   bf16 ............................................ False   bias_dropout_fusion ............................. True   bias_gelu_fusion ................................ True   bias_swiglu_fusion .............................. True   biencoder_projection_dim ........................ 0   biencoder_shared_query_context_model ............ False   block_data_path ................................. None   check_for_nan_in_loss_and_grad .................. False   check_weight_hash_across_dp_replicas_interval ... None   ckpt_fully_parallel_save ........................ False   ckpt_step ....................................... None   classes_fraction ................................ 1.0   clip_grad ....................................... 1.0   clone_scatter_output_in_embedding ............... True   consumed_train_samples .......................... 0   consumed_valid_samples .......................... 0   context_parallel_size ........................... 1   create_attention_mask_in_dataloader ............. True   data_cache_path ................................. None   data_parallel_random_init ....................... False   data_parallel_size .............................. 1   data_path ....................................... ['mygpt2_text_document']   data_per_class_fraction ......................... 1.0   data_sharding ................................... True   dataloader_type ................................. single   ddp_bucket_size ................................. None   decoder_num_layers .............................. None   decoder_seq_length .............................. None   decoupled_lr .................................... None   decoupled_min_lr ................................ None   delay_grad_reduce ............................... True   delay_param_gather .............................. False   dino_bottleneck_size ............................ 256   dino_freeze_last_layer .......................... 1   dino_head_hidden_size ........................... 2048   dino_local_crops_number ......................... 10   dino_local_img_size ............................. 96   dino_norm_last_layer ............................ False   dino_teacher_temp ............................... 0.07   dino_warmup_teacher_temp ........................ 0.04   dino_warmup_teacher_temp_epochs ................. 30   disable_straggler_on_startup .................... False   dist_ckpt_format ................................ torch_dist   distribute_saved_activations .................... False   distributed_backend ............................. nccl   distributed_timeout_minutes ..................... 10   embedding_path .................................. None   empty_unused_memory_level ....................... 0   enable_one_logger ............................... False   encoder_num_layers .............................. 24   encoder_seq_length .............................. 1024   end_weight_decay ................................ 0.01   eod_mask_loss ................................... False   eval_interval ................................... 100   eval_iters ...................................... 10   evidence_data_path .............................. None   exit_duration_in_mins ........................... None   exit_interval ................................... None   exit_on_missing_checkpoint ...................... False   exit_signal_handler ............................. False   expert_model_parallel_size ...................... 1   ffn_hidden_size ................................. 4096   finetune ........................................ False   fp16 ............................................ True   fp16_lm_cross_entropy ........................... False   fp32_residual_connection ........................ False   fp8 ............................................. None   fp8_amax_compute_algo ........................... most_recent   fp8_amax_history_len ............................ 1   fp8_interval .................................... 1   fp8_margin ...................................... 0   fp8_wgrad ....................................... True   global_batch_size ............................... 16   gradient_accumulation_fusion .................... True   group_query_attention ........................... False   head_lr_mult .................................... 1.0   hidden_dropout .................................. 0.1   hidden_size ..................................... 1024   hysteresis ...................................... 2   ict_head_size ................................... None   ict_load ........................................ None   img_h ........................................... 224   img_w ........................................... 224   indexer_batch_size .............................. 128   indexer_log_interval ............................ 1000   inference_batch_times_seqlen_threshold .......... 512   init_method_std ................................. 0.02   init_method_xavier_uniform ...................... False   initial_loss_scale .............................. 4294967296   iter_per_epoch .................................. 1250   kv_channels ..................................... 64   lazy_mpu_init ................................... None   load ............................................ checkpoints/gpt2_345m_dist_mp   local_rank ...................................... None   log_batch_size_to_tensorboard ................... False   log_interval .................................... 10   log_learning_rate_to_tensorboard ................ True   log_loss_scale_to_tensorboard ................... True   log_memory_to_tensorboard ....................... False   log_num_zeros_in_grad ........................... False   log_params_norm ................................. False   log_progress .................................... False   log_straggler ................................... False   log_throughput .................................. False   log_timers_to_tensorboard ....................... False   log_validation_ppl_to_tensorboard ............... False   log_world_size_to_tensorboard ................... False   loss_scale ...................................... None   loss_scale_window ............................... 1000   lr .............................................. 0.00015   lr_decay_iters .................................. 320000   lr_decay_samples ................................ None   lr_decay_style .................................. cosine   lr_warmup_fraction .............................. 0.01   lr_warmup_init .................................. 0.0   lr_warmup_iters ................................. 0   lr_warmup_samples ............................... 0   make_vocab_size_divisible_by .................... 128   manual_gc ....................................... False   manual_gc_eval .................................. True   manual_gc_interval .............................. 0   mask_factor ..................................... 1.0   mask_prob ....................................... 0.15   mask_type ....................................... random   masked_softmax_fusion ........................... True   max_position_embeddings ......................... 1024   max_tokens_to_oom ............................... 12000   merge_file ...................................... merges.txt   micro_batch_size ................................ 4   min_loss_scale .................................. 1.0   min_lr .......................................... 1e05   mmap_bin_files .................................. True   mock_data ....................................... False   moe_aux_loss_coeff .............................. 0.0   moe_grouped_gemm ................................ False   moe_input_jitter_eps ............................ None   moe_per_layer_logging ........................... False   moe_router_load_balancing_type .................. aux_loss   moe_router_topk ................................. 2   moe_token_dispatcher_type ....................... allgather   moe_token_dropping .............................. False   moe_z_loss_coeff ................................ None   nccl_communicator_config_path ................... None   no_load_optim ................................... None   no_load_rng ..................................... None   no_persist_layer_norm ........................... False   no_save_optim ................................... None   no_save_rng ..................................... None   norm_epsilon .................................... 1e05   normalization ................................... LayerNorm   num_attention_heads ............................. 16   num_channels .................................... 3   num_classes ..................................... 1000   num_experts ..................................... None   num_layers ...................................... 24   num_layers_per_virtual_pipeline_stage ........... None   num_query_groups ................................ 1   num_workers ..................................... 2   one_logger_entity ............................... hwinf_dcm   one_logger_project .............................. e2etracking   one_logger_run_name ............................. None   onnx_safe ....................................... None   openai_gelu ..................................... False   optimizer ....................................... adam   output_bert_embeddings .......................... False   overlap_grad_reduce ............................. False   overlap_p2p_comm ................................ False   overlap_param_gather ............................ False   override_opt_param_scheduler .................... False   params_dtype .................................... torch.float16   patch_dim ....................................... 16   perform_initialization .......................... True   pipeline_model_parallel_size .................... 1   pipeline_model_parallel_split_rank .............. None   position_embedding_type ......................... learned_absolute   pretrained_checkpoint ........................... None   profile ......................................... False   profile_ranks ................................... [0]   profile_step_end ................................ 12   profile_step_start .............................. 10   qk_layernorm .................................... False   query_in_block_prob ............................. 0.1   rampup_batch_size ............................... None   rank ............................................ 0   recompute_granularity ........................... None   recompute_method ................................ None   recompute_num_layers ............................ None   reset_attention_mask ............................ False   reset_position_ids .............................. False   retriever_report_topk_accuracies ................ []   retriever_score_scaling ......................... False   retriever_seq_length ............................ 256   retro_add_retriever ............................. False   retro_attention_gate ............................ 1   retro_cyclic_train_iters ........................ None   retro_encoder_attention_dropout ................. 0.1   retro_encoder_hidden_dropout .................... 0.1   retro_encoder_layers ............................ 2   retro_num_neighbors ............................. 2   retro_num_retrieved_chunks ...................... 2   retro_project_dir ............................... None   retro_verify_neighbor_count ..................... True   rotary_interleaved .............................. False   rotary_percent .................................. 1.0   rotary_seq_len_interpolation_factor ............. None   sample_rate ..................................... 1.0   save ............................................ checkpoints/gpt2_345m_dist_mp   save_interval ................................... 50   scatter_gather_tensors_in_pipeline .............. True   seed ............................................ 1234   seq_length ...................................... 1024   sequence_parallel ............................... True   sgd_momentum .................................... 0.9   short_seq_prob .................................. 0.1   skip_train ...................................... False   spec ............................................ None   split ........................................... 949,50,1   squared_relu .................................... False   standalone_embedding_stage ...................... False   start_weight_decay .............................. 0.01   straggler_ctrlr_port ............................ 65535   straggler_minmax_count .......................... 1   swiglu .......................................... False   swin_backbone_type .............................. tiny   tensor_model_parallel_size ...................... 2   tensorboard_dir ................................. None   tensorboard_log_interval ........................ 1   tensorboard_queue_size .......................... 1000   test_data_path .................................. None   test_mode ....................................... False   timing_log_level ................................ 0   timing_log_option ............................... minmax   titles_data_path ................................ None   tokenizer_model ................................. None   tokenizer_type .................................. GPT2BPETokenizer   tp_comm_bulk_dgrad .............................. True   tp_comm_bulk_wgrad .............................. True   tp_comm_overlap ................................. False   tp_comm_overlap_ag .............................. True   tp_comm_overlap_cfg ............................. None   tp_comm_overlap_rs .............................. True   tp_comm_overlap_rs_dgrad ........................ False   tp_comm_split_ag ................................ True   tp_comm_split_rs ................................ True   train_data_path ................................. None   train_iters ..................................... 200   train_samples ................................... None   transformer_impl ................................ transformer_engine   transformer_pipeline_model_parallel_size ........ 1   untie_embeddings_and_output_weights ............. False   use_checkpoint_args ............................. False   use_checkpoint_opt_param_scheduler .............. False   use_cpu_initialization .......................... None   use_dist_ckpt ................................... False   use_distributed_optimizer ....................... False   use_flash_attn .................................. False   use_mcore_models ................................ False   use_one_sent_docs ............................... False   use_ring_exchange_p2p ........................... False   use_rotary_position_embeddings .................. False   use_tp_pp_dp_mapping ............................ False   valid_data_path ................................. None   variable_seq_lengths ............................ False   virtual_pipeline_model_parallel_size ............ None   vision_backbone_type ............................ vit   vision_pretraining .............................. False   vision_pretraining_type ......................... classify   vocab_extra_ids ................................. 0   vocab_file ...................................... vocab.json   vocab_size ...................................... None   wandb_exp_name ..................................   wandb_project ...................................   wandb_save_dir ..................................   weight_decay .................................... 0.01   weight_decay_incr_style ......................... constant   world_size ...................................... 2   yaml_cfg ........................................ None  end of arguments  setting number of microbatches to constant 4 > building GPT2BPETokenizer tokenizer ...  > padded vocab (size: 50257) with 175 dummy tokens (new size: 50432) > initializing torch distributed ... > initialized tensor model parallel with size 2 > initialized pipeline model parallel with size 1 > setting random seeds to 1234 ... > compiling dataset index builder ... make: Entering directory `/scratch/qualis/test/MegatronLM.bak/megatron/core/datasets' make: Nothing to be done for `default'. make: Leaving directory `/scratch/qualis/test/MegatronLM.bak/megatron/core/datasets' >>> done with dataset index builder. Compilation time: 0.063 seconds > compiling and loading fused kernels ... >>> done with compiling and loading fused kernels. Compilation time: 3.407 seconds [rank1]:[W913 14:10:04.146531972 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank0]:[W913 14:10:04.146585101 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) time to initialize megatron (seconds): 5.393 [after megatron is initialized] datetime: 20240913 14:10:06 building GPT model ... /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/jit.py:162: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.   with torch.cuda.amp.autocast(enabled=False): /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/jit.py:162: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.   with torch.cuda.amp.autocast(enabled=False): /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/jit.py:162: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.   with torch.cuda.amp.autocast(enabled=False): /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/jit.py:162: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.   with torch.cuda.amp.autocast(enabled=False): /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/jit.py:162: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.   with torch.cuda.amp.autocast(enabled=False): /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/jit.py:162: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.   with torch.cuda.amp.autocast(enabled=False): /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/jit.py:162: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.   with torch.cuda.amp.autocast(enabled=False): /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/jit.py:162: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.   with torch.cuda.amp.autocast(enabled=False): /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/jit.py:162: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.   with torch.cuda.amp.autocast(enabled=False): /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/jit.py:162: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.   with torch.cuda.amp.autocast(enabled=False): /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/jit.py:162: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.   with torch.cuda.amp.autocast(enabled=False): /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/jit.py:162: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.   with torch.cuda.amp.autocast(enabled=False): /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/jit.py:162: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.   with torch.cuda.amp.autocast(enabled=False): /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/jit.py:162: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.   with torch.cuda.amp.autocast(enabled=False):  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 178100224 INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None) INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient allreduce / reducescatter: 1 INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (178100224 elements): INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.20.layernorm_mlp.fc2_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.17.layernorm_mlp.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.17.self_attention.layernorm_qkv.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.14.self_attention.proj.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.13.layernorm_mlp.fc2_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.13.self_attention.layernorm_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.8.self_attention.layernorm_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.6.self_attention.layernorm_qkv.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.5.layernorm_mlp.fc2_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.6.self_attention.layernorm_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.4.self_attention.proj.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.3.layernorm_mlp.fc2_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.0.self_attention.proj.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.22.self_attention.proj.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.13.self_attention.layernorm_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.5.self_attention.proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.16.layernorm_mlp.fc1_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.6.layernorm_mlp.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.23.layernorm_mlp.fc2_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.14.layernorm_mlp.fc2_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.12.self_attention.layernorm_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.8.layernorm_mlp.fc2_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.21.self_attention.layernorm_qkv.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.17.layernorm_mlp.fc2_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.7.self_attention.layernorm_qkv.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.6.self_attention.layernorm_qkv.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.23.self_attention.layernorm_qkv.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.14.layernorm_mlp.fc1_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.10.layernorm_mlp.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.9.layernorm_mlp.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.5.self_attention.layernorm_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.16.self_attention.layernorm_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.8.self_attention.layernorm_qkv.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.2.layernorm_mlp.fc1_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.0.self_attention.layernorm_qkv.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.22.layernorm_mlp.fc1_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.20.self_attention.layernorm_qkv.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.16.self_attention.proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.13.layernorm_mlp.fc1_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.8.layernorm_mlp.fc1_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.7.layernorm_mlp.fc1_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.5.self_attention.layernorm_qkv.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.3.layernorm_mlp.fc1_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.23.layernorm_mlp.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.22.layernorm_mlp.fc2_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.12.layernorm_mlp.fc2_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.12.self_attention.proj.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.5.layernorm_mlp.fc1_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.4.layernorm_mlp.fc1_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.4.self_attention.proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.3.self_attention.proj.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.2.self_attention.layernorm_qkv.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.0.self_attention.layernorm_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.21.layernorm_mlp.fc1_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.18.self_attention.layernorm_qkv.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.15.layernorm_mlp.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.14.layernorm_mlp.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.9.self_attention.layernorm_qkv.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.7.layernorm_mlp.fc1_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.19.self_attention.layernorm_qkv.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.18.self_attention.layernorm_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.0.layernorm_mlp.fc2_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.1.layernorm_mlp.fc2_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.18.layernorm_mlp.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.23.layernorm_mlp.fc1_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.19.layernorm_mlp.fc2_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.14.self_attention.layernorm_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.3.self_attention.layernorm_qkv.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.21.layernorm_mlp.fc1_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.16.layernorm_mlp.fc2_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.15.self_attention.proj.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.11.self_attention.layernorm_qkv.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.10.layernorm_mlp.fc1_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.1.self_attention.layernorm_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.0.self_attention.proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.23.layernorm_mlp.fc1_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.23.self_attention.layernorm_qkv.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.15.layernorm_mlp.fc1_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.15.self_attention.layernorm_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.2.self_attention.layernorm_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.19.layernorm_mlp.fc1_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.16.layernorm_mlp.fc1_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.0.layernorm_mlp.fc2_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.19.layernorm_mlp.fc1_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.19.self_attention.layernorm_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.7.layernorm_mlp.fc2_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.6.layernorm_mlp.fc1_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.5.layernorm_mlp.fc2_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.4.layernorm_mlp.fc2_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.19.layernorm_mlp.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.16.layernorm_mlp.fc2_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.14.layernorm_mlp.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.12.layernorm_mlp.fc1_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.11.layernorm_mlp.fc1_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.5.layernorm_mlp.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.19.self_attention.layernorm_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.7.layernorm_mlp.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.3.layernorm_mlp.fc1_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.17.self_attention.layernorm_qkv.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.10.self_attention.proj.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.9.layernorm_mlp.fc1_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.4.layernorm_mlp.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.5.self_attention.layernorm_qkv.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.21.self_attention.layernorm_qkv.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.16.self_attention.layernorm_qkv.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.11.self_attention.layernorm_qkv.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.11.self_attention.layernorm_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.9.self_attention.layernorm_qkv.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.7.self_attention.layernorm_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.13.layernorm_mlp.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.1.layernorm_mlp.fc1_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.17.self_attention.layernorm_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.13.layernorm_mlp.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.12.layernorm_mlp.fc2_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.6.layernorm_mlp.fc1_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.4.self_attention.layernorm_qkv.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.2.self_attention.proj.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.8.self_attention.layernorm_qkv.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.1.self_attention.proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.23.layernorm_mlp.fc2_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.20.self_attention.proj.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.16.self_attention.layernorm_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.15.self_attention.layernorm_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.14.self_attention.layernorm_qkv.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.14.self_attention.layernorm_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.12.self_attention.layernorm_qkv.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.7.self_attention.proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.0.self_attention.layernorm_qkv.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.21.layernorm_mlp.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.18.layernorm_mlp.fc2_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.5.layernorm_mlp.fc1_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.2.layernorm_mlp.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.1.layernorm_mlp.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.1.self_attention.layernorm_qkv.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.17.self_attention.proj.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.15.self_attention.layernorm_qkv.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.10.layernorm_mlp.fc1_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.6.self_attention.layernorm_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.0.layernorm_mlp.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.22.layernorm_mlp.fc1_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.17.layernorm_mlp.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.13.self_attention.proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.8.layernorm_mlp.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.6.self_attention.proj.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.1.layernorm_mlp.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.6.self_attention.proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.7.self_attention.layernorm_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.23.layernorm_mlp.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.21.self_attention.proj.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.13.layernorm_mlp.fc2_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.11.self_attention.layernorm_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.9.self_attention.layernorm_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.8.self_attention.proj.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.15.self_attention.proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.10.self_attention.layernorm_qkv.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.9.self_attention.layernorm_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.21.layernorm_mlp.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.18.layernorm_mlp.fc2_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.6.layernorm_mlp.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.2.layernorm_mlp.fc2_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.9.layernorm_mlp.fc1_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.7.self_attention.layernorm_qkv.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.20.self_attention.layernorm_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.14.layernorm_mlp.fc2_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.0.layernorm_mlp.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.20.self_attention.proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.17.layernorm_mlp.fc1_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.14.self_attention.proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.13.layernorm_mlp.fc1_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.13.self_attention.layernorm_qkv.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.8.layernorm_mlp.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.5.layernorm_mlp.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.0.layernorm_mlp.fc1_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.3.self_attention.layernorm_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.2.self_attention.layernorm_qkv.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.18.self_attention.layernorm_qkv.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.12.layernorm_mlp.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.2.layernorm_mlp.fc2_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.18.layernorm_mlp.fc1_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.22.self_attention.layernorm_qkv.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.11.self_attention.proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.10.self_attention.proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.9.layernorm_mlp.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.9.self_attention.proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.19.layernorm_mlp.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.8.self_attention.proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.3.layernorm_mlp.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.23.self_attention.layernorm_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.22.layernorm_mlp.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.20.layernorm_mlp.fc1_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.12.layernorm_mlp.fc1_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.8.layernorm_mlp.fc2_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.6.layernorm_mlp.fc2_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.6.layernorm_mlp.fc2_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.20.self_attention.layernorm_qkv.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.9.self_attention.proj.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.2.layernorm_mlp.fc1_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.22.self_attention.layernorm_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.19.self_attention.layernorm_qkv.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.7.layernorm_mlp.fc2_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.0.self_attention.layernorm_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.final_norm.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.19.layernorm_mlp.fc2_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.12.layernorm_mlp.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.9.layernorm_mlp.fc2_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.22.self_attention.proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.5.self_attention.proj.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.1.layernorm_mlp.fc2_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.20.layernorm_mlp.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.16.layernorm_mlp.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.15.layernorm_mlp.fc1_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.15.layernorm_mlp.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.15.self_attention.layernorm_qkv.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.12.self_attention.layernorm_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.1.layernorm_mlp.fc1_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.10.layernorm_mlp.fc2_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.10.self_attention.layernorm_qkv.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.2.self_attention.layernorm_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.21.self_attention.layernorm_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.18.layernorm_mlp.fc1_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.7.layernorm_mlp.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.4.self_attention.layernorm_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.3.self_attention.layernorm_qkv.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.11.layernorm_mlp.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.10.self_attention.layernorm_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.3.layernorm_mlp.fc2_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.1.self_attention.layernorm_qkv.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.23.self_attention.proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.18.self_attention.proj.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.11.self_attention.proj.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.3.layernorm_mlp.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.embedding.word_embeddings.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.20.layernorm_mlp.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.17.layernorm_mlp.fc1_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.16.self_attention.proj.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.18.self_attention.layernorm_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.14.layernorm_mlp.fc1_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.13.self_attention.layernorm_qkv.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.2.self_attention.proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.21.layernorm_mlp.fc2_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.18.layernorm_mlp.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.11.layernorm_mlp.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.8.layernorm_mlp.fc1_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.4.layernorm_mlp.fc1_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.0.layernorm_mlp.fc1_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.2.layernorm_mlp.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.22.self_attention.layernorm_qkv.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.21.self_attention.proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.14.self_attention.layernorm_qkv.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.11.layernorm_mlp.fc1_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.9.layernorm_mlp.fc2_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.1.self_attention.proj.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.7.self_attention.proj.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.22.layernorm_mlp.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.17.self_attention.layernorm_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.4.self_attention.layernorm_qkv.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.4.self_attention.layernorm_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.3.self_attention.layernorm_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.1.self_attention.layernorm_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.17.layernorm_mlp.fc2_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.final_norm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.20.layernorm_mlp.fc1_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.15.layernorm_mlp.fc2_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.12.self_attention.proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.10.layernorm_mlp.layer_norm_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.8.self_attention.layernorm_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.23.self_attention.proj.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.21.layernorm_mlp.fc2_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.12.self_attention.layernorm_qkv.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.19.self_attention.proj.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.16.self_attention.layernorm_qkv.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.5.self_attention.layernorm_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.22.layernorm_mlp.fc2_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.3.self_attention.proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.23.self_attention.layernorm_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.20.layernorm_mlp.fc2_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.17.self_attention.proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.embedding.position_embeddings.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.18.self_attention.proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.22.self_attention.layernorm_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.21.self_attention.layernorm_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.10.layernorm_mlp.fc2_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.19.self_attention.proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.16.layernorm_mlp.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.15.layernorm_mlp.fc2_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.13.self_attention.proj.bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.4.layernorm_mlp.fc2_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.20.self_attention.layernorm_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.11.layernorm_mlp.fc2_bias INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.11.layernorm_mlp.fc2_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.10.self_attention.layernorm_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.encoder.layers.4.layernorm_mlp.layer_norm_bias INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1e05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=) > learning rate decay style: cosine  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 178100224 WARNING: could not find the metadata file checkpoints/gpt2_345m_dist_mp/latest_checkpointed_iteration.txt     will not load any checkpoints and will start from random /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/torch/distributed/c10d_logger.py:79: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.   return func(*args, **kwargs) /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/torch/distributed/c10d_logger.py:79: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.   return func(*args, **kwargs) (min, max) time across ranks (ms):     loadcheckpoint ................................: (0.49, 0.52) [after model, optimizer, and learning rate scheduler are built] datetime: 20240913 14:10:10 > building train, validation, and test datasets ...  > datasets target sizes (minimum size):     train:      3200     validation: 480     test:       160 INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)] > building train, validation, and test datasets for GPT ... WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(3200, 480, 160), and config=GPTDatasetConfig(random_seed=1234, sequence_length=1024, blend=(['mygpt2_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True) INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from mygpt2_text_document.idx INFO:megatron.core.datasets.indexed_dataset:    Extract the sequence lengths INFO:megatron.core.datasets.indexed_dataset:    Extract the sequence pointers INFO:megatron.core.datasets.indexed_dataset:    Extract the document indices INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 10000 INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 10000 INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices INFO:megatron.core.datasets.gpt_dataset:        Load the document index from a375193ee7bd0ffbfa0d131aff630f11GPTDatasettraindocument_index.npy INFO:megatron.core.datasets.gpt_dataset:        Load the sample index from a375193ee7bd0ffbfa0d131aff630f11GPTDatasettrainsample_index.npy INFO:megatron.core.datasets.gpt_dataset:        Load the shuffle index from a375193ee7bd0ffbfa0d131aff630f11GPTDatasettrainshuffle_index.npy INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 3570 INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices INFO:megatron.core.datasets.gpt_dataset:        Load the document index from e9d835bb86bcaca4cd08b4977794f406GPTDatasetvaliddocument_index.npy INFO:megatron.core.datasets.gpt_dataset:        Load the sample index from e9d835bb86bcaca4cd08b4977794f406GPTDatasetvalidsample_index.npy INFO:megatron.core.datasets.gpt_dataset:        Load the shuffle index from e9d835bb86bcaca4cd08b4977794f406GPTDatasetvalidshuffle_index.npy INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 530 INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices INFO:megatron.core.datasets.gpt_dataset:        Load the document index from fceb23beb87bf7aa15e27415203c9636GPTDatasettestdocument_index.npy INFO:megatron.core.datasets.gpt_dataset:        Load the sample index from fceb23beb87bf7aa15e27415203c9636GPTDatasettestsample_index.npy INFO:megatron.core.datasets.gpt_dataset:        Load the shuffle index from fceb23beb87bf7aa15e27415203c9636GPTDatasettestshuffle_index.npy INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 161 > finished creating GPT datasets ... [after dataloaders are built] datetime: 20240913 14:10:10 done with setup ... (min, max) time across ranks (ms):     modelandoptimizersetup ......................: (4529.59, 4538.06)     train/valid/testdataiteratorssetup ..........: (34.88, 351.35) training ... [before the start of training step] datetime: 20240913 14:10:10 /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/jit.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.   with torch.cuda.amp.autocast(enabled=False): /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/jit.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.   with torch.cuda.amp.autocast(enabled=False): /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/jit.py:162: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.   with torch.cuda.amp.autocast(enabled=False): /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/jit.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.   with torch.cuda.amp.autocast(enabled=False): /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/jit.py:162: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.   with torch.cuda.amp.autocast(enabled=False): /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/jit.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.   with torch.cuda.amp.autocast(enabled=False): /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/torch/distributed/c10d_logger.py:79: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.   return func(*args, **kwargs) /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/torch/distributed/c10d_logger.py:79: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.   return func(*args, **kwargs) /scratch/qualis/test/MegatronLM.bak/megatron/core/tensor_parallel/layers.py:431: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.   handle = torch.distributed._reduce_scatter_base( /scratch/qualis/test/MegatronLM.bak/megatron/core/tensor_parallel/layers.py:431: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.   handle = torch.distributed._reduce_scatter_base( /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/jit.py:121: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.   with torch.cuda.amp.autocast(enabled=False): /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/jit.py:121: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.   with torch.cuda.amp.autocast(enabled=False): /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/jit.py:121: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.   with torch.cuda.amp.autocast(enabled=False): /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/jit.py:121: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.   with torch.cuda.amp.autocast(enabled=False): /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/torch/distributed/c10d_logger.py:79: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.   return func(*args, **kwargs) /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/torch/distributed/c10d_logger.py:79: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.   return func(*args, **kwargs) /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/jit.py:162: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.   with torch.cuda.amp.autocast(enabled=False): /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/jit.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.   with torch.cuda.amp.autocast(enabled=False): /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/jit.py:162: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.   with torch.cuda.amp.autocast(enabled=False): /scratch/qualis/miniconda3/envs/megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/jit.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.   with torch.cuda.amp.autocast(enabled=False): /scratch/qualis/test/MegatronLM.bak/megatron/core/tensor_parallel/layers.py:431: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.   handle = torch.distributed._reduce_scatter_base( /scratch/qualis/test/MegatronLM.bak/megatron/core/tensor_parallel/layers.py:431: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.   handle = torch.distributed._reduce_scatter_base(  [20240913 14:10:23] iteration       10/     200   [rank1]:[W913 14:12:02.636544709 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator()) [rank0]:[W913 14:12:02.670127115 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator()) ```","My simple workaround was checking out an old branch and ran it again. It worked! I don't know how it is workding. Any comment would be appreciated.  ``` (megatron) $ git checkout core_r0.5.0 Branch core_r0.5.0 set up to track remote branch core_r0.5.0 from origin. Switched to a new branch 'core_r0.5.0' (megatron) $ git branch  * core_r0.5.0   main (megatron) $  CUDA_DEVICE_MAX_CONNECTIONS=1 torchrun nproc_per_node 2  pretrain_gpt.py     tensormodelparallelsize 2     pipelinemodelparallelsize 1         numlayers 24     hiddensize 1024     numattentionheads 16     seqlength 1024     maxpositionembeddings 1024     microbatchsize 4     globalbatchsize 16     lr 0.00015     trainiters 200     lrdecayiters 320000     lrdecaystyle cosine     minlr 1.0e5     weightdecay 1e2     lrwarmupfraction .01     clipgrad 1.0     fp16 datapath  mygpt2_text_document     vocabfile vocab.json     mergefile merges.txt     split 949,50,1 loginterval 10     saveinterval 50    evalinterval 100     evaliters 10 distributedbackend nccl  save checkpoints/gpt2_345m_dist_mp     load  checkpoints/gpt2_345m_dist_mp attentionsoftmaxinfp32 sequenceparallel ```","Same issue here,  Failed with :  ``` [rank21]: Traceback (most recent call last): [rank21]:   File ""MegatronLM/pretrain_gpt.py"", line 264, in  [rank21]:     pretrain( [rank21]:   File ""MegatronLM/megatron/training/training.py"", line 355, in pretrain [rank21]:     iteration, num_floating_point_operations_so_far = train( [rank21]:                                                       ^^^^^^ [rank21]:   File ""MegatronLM/megatron/training/training.py"", line 1368, in train [rank21]:     save_checkpoint_and_time(iteration, model, optimizer, [rank21]:   File ""MegatronLM/megatron/training/training.py"", line 1072, in save_checkpoint_and_time [rank21]:     save_checkpoint(iteration, model, optimizer, opt_param_scheduler, [rank21]:   File ""MegatronLM/megatron/training/checkpointing.py"", line 401, in save_checkpoint [rank21]:     state_dict = generate_state_dict( [rank21]:                  ^^^^^^^^^^^^^^^^^^^^ [rank21]:   File ""MegatronLM/megatron/training/checkpointing.py"", line 613, in generate_state_dict [rank21]:     state_dict['optimizer'] = (optimizer.sharded_state_dict(state_dict, **(optim_sd_kwargs or {})) [rank21]:                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank21]:   File ""MegatronLM/megatron/core/optimizer/optimizer.py"", line 654, in sharded_state_dict [rank21]:     optim_state_to_sharding_state( [rank21]:   File ""MegatronLM/megatron/core/dist_checkpointing/optimizer.py"", line 120, in optim_state_to_sharding_state [rank21]:     sharded_state[param_id][state_key] = make_sharded_optimizer_tensor( [rank21]:                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank21]:   File ""MegatronLM/megatron/core/dist_checkpointing/optimizer.py"", line 83, in make_sharded_optimizer_tensor [rank21]:     tuple(optim_param.shape) == model_param.local_shape [rank21]:           ^^^^^^^^^^^^^^^^^ [rank21]: AttributeError: 'NoneType' object has no attribute 'shape' ``` Any help ?  I can try to fix it but would like some insight to get started I don't want to downgrade as I want to benchmark against Mamba....","I tried using `usedistributedoptimizer` but failed also on a error : ``` [rank21]:   File ""MegatronLM/megatron/core/optimizer/distrib_optimizer.py"", line 1159, in sharded_param_state_fs_model_space [rank21]:     dtype=state_ten.dtype, [rank21]:           ^^^^^^^^^^^^^^^ [rank21]: AttributeError: 'NoneType' object has no attribute 'dtype' ``` Looks like the two error are link ! ","I changed the checkpoint format from `torch_dist` to `torch` and seems to do the work, I haven't tried to restart the training from a backup but no error throw during model saving","Yes, it seemed to work for me as well by setting the command argument ` ckptformat` to `torch` explicitly. BTW, the default checkpoint format is `torch_dist`.  (megatron) $   CUDA_DEVICE_MAX_CONNECTIONS=1 torchrun nproc_per_node 2 master_port 12345 pretrain_gpt.py     tensormodelparallelsize 2     pipelinemodelparallelsize 1         numlayers 24     hiddensize 1024     numattentionheads 16     seqlength 1024     maxpositionembeddings 1024     microbatchsize 4     globalbatchsize 16     lr 0.00015     trainiters 200     lrdecayiters 320000     lrdecaystyle cosine     minlr 1.0e5     weightdecay 1e2     lrwarmupfraction .01     clipgrad 1.0     fp16 datapath  mygpt2_text_document     vocabfile vocab.json     mergefile merges.txt     split 949,50,1 loginterval 10     saveinterval 50    evalinterval 100     evaliters 10 distributedbackend nccl  save checkpoints/gpt2_345m_dist_mp     load  checkpoints/gpt2_345m_dist_mp attentionsoftmaxinfp32 sequenceparallel ckptformat torch . . . [20240923 08:51:58] iteration       50/     200  saving checkpoint at iteration      50 to checkpoints/gpt2_345m_dist_mp in torch format   successfully saved checkpoint from iteration      50 to checkpoints/gpt2_345m_dist_mp (min, max) time across ranks (ms):     savecheckpoint ................................: (3445.21, 3445.43) . . .","> I tried using `usedistributedoptimizer` but failed also on a error : >  > ``` > [rank21]:   File ""MegatronLM/megatron/core/optimizer/distrib_optimizer.py"", line 1159, in sharded_param_state_fs_model_space > [rank21]:     dtype=state_ten.dtype, > [rank21]:           ^^^^^^^^^^^^^^^ > [rank21]: AttributeError: 'NoneType' object has no attribute 'dtype' > ``` >  > Looks like the two error are link ! install newer TE https://github.com/NVIDIA/TransformerEngine/pull/1130","Hi, I tryed by installing the lastest version with :  ``` pip install git+https://github.com/NVIDIA/TransformerEngine.git ``` It might work but now the training just don't start and I have a new error :  ``` TypeError: flash_attn_func() got an unexpected keyword argument 'block_table' ``` I will wait for a new stable release to try again","> Hi, I tryed by installing the lastest version with : >  > ``` > pip install git+https://github.com/NVIDIA/TransformerEngine.git > ``` >  > It might work but now the training just don't start and I have a new error : >  > ``` > TypeError: flash_attn_func() got an unexpected keyword argument 'block_table' > ``` >  > I will wait for a new stable release to try again Same issue. I think it is originated from the Transformer Engine (TE). I am able to reproduce the bug with MegatronCore r0.9.0 and TE v1.10 in training a mixtral model. I found a PR to resolve the issue in TE CC(Fix shape of qk_layernorm.). This PR is included in TE v1.11. Perhaps you could try to upgrade the TE version to 1.11.","Indeed, it works with transformer engine v1.11 thanks",Marking as stale. No activity in 60 days.,"I met the same problem on core_r0.8.0 branch, commit baf94af3 Fail with: ``` [rank3]: Traceback (most recent call last): [rank3]:   File ""MegatronLM/pretrain_gpt.py"", line 279, in  [rank3]:     pretrain( [rank3]:   File ""MegatronLM/megatron/training/training.py"", line 300, in pretrain [rank3]:     iteration, num_floating_point_operations_so_far = train( [rank3]:   File ""MegatronLM/megatron/training/training.py"", line 1235, in train [rank3]:     save_checkpoint_and_time(iteration, model, optimizer, [rank3]:   File ""MegatronLM/megatron/training/training.py"", line 947, in save_checkpoint_and_time [rank3]:     save_checkpoint(iteration, model, optimizer, opt_param_scheduler, [rank3]:   File ""MegatronLM/megatron/training/checkpointing.py"", line 340, in save_checkpoint [rank3]:     state_dict = generate_state_dict(args, model, optimizer, opt_param_scheduler, rng_state, [rank3]:   File ""MegatronLM/megatron/training/checkpointing.py"", line 452, in generate_state_dict [rank3]:     state_dict['optimizer'] = (optimizer.sharded_state_dict(state_dict, **(optim_sd_kwargs or {})) [rank3]:   File ""MegatronLM/megatron/core/optimizer/optimizer.py"", line 656, in sharded_state_dict [rank3]:     optim_state_to_sharding_state( [rank3]:   File ""MegatronLM/megatron/core/dist_checkpointing/optimizer.py"", line 120, in optim_state_to_sharding_state [rank3]:     sharded_state[param_id][state_key] = make_sharded_optimizer_tensor( [rank3]:   File ""MegatronLM/megatron/core/dist_checkpointing/optimizer.py"", line 83, in make_sharded_optimizer_tensor [rank3]:     tuple(optim_param.shape) == model_param.local_shape [rank3]: AttributeError: 'NoneType' object has no attribute 'shape' ``` Training with argument `usedistckpt`. However, the arguments of `distckptformat` only have two choices: `['zarr', 'torch_dist']`, but no `torch`, how can I solve this problem?"
orrzohar,"[QUESTION] Possible to install ""from userlib.auto_resume import AutoResume""","Hi, Is it possible to install the autoresume SDK somehow?  from userlib.auto_resume import AutoResume I could not find a python library `userlib`. https://github.com/NVIDIA/MegatronLM/blob/9ec2337648b31eaac81678e0b8c333decbfcbdf6/megatron/training/global_vars.pyL201 Best, Orr",2024-09-11T23:01:07Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1133,Marking as stale. No activity in 60 days.
mxjmtxrm,"[BUG]""Unexpected key(s) in state_dict"" while loading Llama-megatron checkpoint.","Hi, I tried to finetune Llama27bchat model using megatron. I downloaded the hf checkpoint and convert it to GPT megatron checkpoint referring [https://github.com/NVIDIA/MegatronLM/blob/fe1640a3cc4866e015bfdb6449f0d1943d2243cb/docs/llama_mistral.md?plain=1L73](). The command I used is: ``` python tools/checkpoint/convert.py \     modeltype GPT \     loader llama_mistral \     saver megatron \     targettensorparallelsize 1 \     checkpointtype hf \     modelsize llama27Bf \     loaddir Llama27bchathf \     savedir ./Llama27bchatpp1 \     tokenizermodel Llama27bchathf/tokenizer.model ``` Then I tried to train the llama: ``` !/bin/bash DISTRIBUTED_ARGS=(     nproc_per_node $GPUS_PER_NODE      nnodes $NUM_NODES      master_addr $MASTER_ADDR      master_port $MASTER_PORT ) GPT_MODEL_ARGS=(     numlayers ${NUM_LAYERS}      hiddensize ${HIDDEN_SIZE}      numattentionheads ${NUM_HEAD}      ffnhiddensize ${FFN_HIDDEN_SIZE}      positionembeddingtype rope      maxpositionembeddings ${MAX_POSITION_EMBEDDINGS}      seqlength 4096      maxpositionembeddings 4096  ) TRAINING_ARGS=(     microbatchsize 1      globalbatchsize 32      trainiters 50      weightdecay 0.1      adambeta1 0.9      adambeta2 0.95      initmethodstd 0.006      clipgrad 1.0      bf16     lr 6.0e5      lrdecaystyle cosine      minlr 6.0e6     lrwarmupfraction .001      lrdecayiters 30      noloadrng      noloadoptim     exitonmissingcheckpoint     usecheckpointargs      untieembeddingsandoutputweights      userotarypositionembeddings     useflashattn      nopositionembedding     nomaskedsoftmaxfusion     attentionsoftmaxinfp32 ) MODEL_PARALLEL_ARGS=( 	tensormodelparallelsize 1 	pipelinemodelparallelsize 1 ) DATA_ARGS=(     datapath $DATA_PATH      split 949,50,1     tokenizermodel ${TOKENIZER_PATH}     datacachepath ./data_cache      tokenizertype Llama2Tokenizer ) EVAL_AND_LOGGING_ARGS=(     loginterval 1     saveinterval 5      evalinterval 5      save $CHECKPOINT_PATH      load $CHECKPOINT_PATH      evaliters 10     tensorboarddir $TENSORBOARD_LOGS_PATH  ) torchrun ${DISTRIBUTED_ARGS[@]} pretrain_gpt.py \     ${GPT_MODEL_ARGS[@]} \     ${TRAINING_ARGS[@]} \     ${MODEL_PARALLEL_ARGS[@]} \     ${DATA_ARGS[@]} \     ${EVAL_AND_LOGGING_ARGS[@]} ``` I met the following error: ``` RuntimeError: Error(s) in loading state_dict for GPTModel: 	Missing key(s) in state_dict: ""embedding.word_embeddings.weight"", ""decoder.layers.0.self_attention.linear_proj.weight"",... Unexpected key(s) in state_dict: ""language_model"". ``` How to solve this problem?",2024-09-11T09:48:12Z,stale,open,0,21,https://github.com/NVIDIA/Megatron-LM/issues/1132," , our instructions could be clearer in these docs regarding the compatibility between the converter's `saver` arg and the training model format. There are two model formats, `legacy` (a.k.a., 'megatron') and `mcore`. In the docs and in your command above, `saver megatron` saves to the legacy format, but during training, the default format is mcore, unless otherwise specified. There are two options for your issue:  If possible for your use, save to the newer mcore format by specifying `saver mcore` during conversion.  If you are tied to using the legacy/megatron format, then in your training script, you must add the arg `uselegacymodels` to train using the legacy format (rather than the default mcore format). Let me know if you have any questions.","When  i repeat this issue, and use  ""uselegacymodels"" in script. An error  ""File ""/workspace/MegatronLM/megatron/training/arguments.py"", line 576, in validate_args     raise RuntimeError('usedistckpt is not supported in legacy models.') RuntimeError: usedistckpt is not supported in legacy models. "" was occurd.","This error occurred because the default value for `ckptformat` is `torch_dist`, which means that the distributed checkpoint format is used, which is incompatible with the legacy model format. Again, there are two options to fix this:  During conversion, does the use of `saver mcore` work for your use case? If so, convert your model again with that arg, and then training should run fine.  Again, if you are indeed tied to using the legacy model format (i.e., must convert using `saver megatron`), then try using both of these args when launching training:    `uselegacymodels`    `ckptformat torch`","OK,thank you for reply,I will test it at the next Monday","nvidia  I try to use saver mcore , but it will hit another error. `Traceback (most recent call last):   File ""/usr/lib/python3.10/multiprocessing/process.py"", line 314, in _bootstrap     self.run()   File ""/usr/lib/python3.10/multiprocessing/process.py"", line 108, in run     self._target(*self._args, **self._kwargs)   File ""/workspace/megatron/tools/checkpoint/saver_mcore.py"", line 555, in save_checkpoint     model = get_local_model(0, ep_rank, tp_rank)   File ""/workspace/megatron/tools/checkpoint/saver_mcore.py"", line 548, in get_local_model     models[pp_rank][ep_rank][tp_rank] = model_provider(pre_process, post_process).to(md.params_dtype)   File ""/workspace/megatron/pretrain_gpt.py"", line 96, in model_provider     model = GPTModel(   File ""/workspace/megatron/megatron/core/models/gpt/gpt_model.py"", line 120, in __init__     self.decoder = TransformerBlock(   File ""/workspace/megatron/megatron/core/transformer/transformer_block.py"", line 204, in __init__     get_cpu_offload_context(   File ""/workspace/megatron/megatron/core/extensions/transformer_engine.py"", line 1079, in get_cpu_offload_context     context, sync_func = _get_cpu_offload_context(   File ""/usr/local/lib/python3.10/distpackages/transformer_engine/pytorch/cpu_offload.py"", line 508, in get_cpu_offload_context     cpu_offload_handler = AsyncDoubleBufferGroupOffloadHandler(   File ""/usr/local/lib/python3.10/distpackages/transformer_engine/pytorch/cpu_offload.py"", line 274, in __init__     self.d2h_stream = torch.cuda.Stream()   File ""/usr/local/lib/python3.10/distpackages/torch/cuda/streams.py"", line 34, in __new__     return super().__new__(cls, priority=priority, **kwargs) RuntimeError: CUDA error: initialization error Compile with `TORCH_USE_CUDA_DSA` to enable deviceside assertions. sending transformer layer 18 sending transformer layer 19 `",I did used saver megatron. And the conversion could be done without problem,But I hit the same problem when train legacy model ,"Just to be clear, did your error above happen during conversion or during training? The extra lines at the bottom showing `sending transformer layer ...` indicate that this is a conversion error?",nvidia  During conversion. ,"nvidia  Megatron conversion works. But during training, we hit the exact error as this post. So we change the conversion type to saver mcore. But the conversion couldn't finish. We are kinda stuck in there","nvidia  Just another update, We also tried this two flag. None of the solution you provide works for us uselegacymodels ckptformat torch It still hit the state_dict error: ``` [rank4]: Traceback (most recent call last):                                                                                                                                                [rank4]:   File ""/workspace/megatron/./pretrain_gpt.py"", line 265, in                                                                                                              [rank4]:     pretrain(                                                                                                                                                                     [rank4]:   File ""/workspace/megatron/megatron/training/training.py"", line 301, in pretrain                                                                                                 [rank4]:     model, optimizer, opt_param_scheduler = setup_model_and_optimizer(                                                                                                            [rank4]:   File ""/workspace/megatron/megatron/training/training.py"", line 680, in setup_model_and_optimizer                                                                                [rank4]:     args.iteration, args.num_floating_point_operations_so_far = load_checkpoint(                                                                                                  [rank4]:   File ""/workspace/megatron/megatron/training/checkpointing.py"", line 1152, in load_checkpoint                                                                                    [rank4]:     model[0].load_state_dict(state_dict['model'], strict=strict)                                                                                                                  [rank4]:   File ""/workspace/megatron/megatron/legacy/model/gpt_model.py"", line 122, in load_state_dict                                                                                     [rank4]:     self.language_model.load_state_dict(state_dict, strict=strict)                                                                                                                [rank4]:   File ""/workspace/megatron/megatron/legacy/model/language_model.py"", line 633, in load_state_dict                                                                                [rank4]:     self.encoder.load_state_dict(state_dict_, strict=strict)                                                                                                                      [rank4]:   File ""/workspace/megatron/megatron/legacy/model/transformer.py"", line 1803, in load_state_dict                                                                                  [rank4]:     super().load_state_dict(state_dict_, strict)                                                                                                                                  [rank4]:   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 2584, in load_state_dict                                                                        [rank4]:     raise RuntimeError(                                                                                                                                                           [rank4]: RuntimeError: Error(s) in loading state_dict for ParallelTransformer:                                                                                                             [rank4]:        Missing key(s) in state_dict: ""layers.0.self_attention.layernorm_qkv.layer_norm_weight"", ```"," You should use spawn as the start method for torch multiprocessing, otherwise CUDA context cannot be properly set up. A simple way to fix it is just add `torch.multiprocessing.set_start_method('spawn')` in https://github.com/NVIDIA/MegatronLM/blob/main/tools/checkpoint/convert.py before invoking subprocesses.","nvidia I believe the missing line `torch.multiprocessing.set_start_method('spawn')` in https://github.com/NVIDIA/MegatronLM/blob/main/tools/checkpoint/convert.py is a bug. To be more specific, when initializing an mcore model, the `model_provider` in mcore saver and loader will use TransformerEngine as the backend implementation. During this process, some CUDAstreams are allocated via `torch.cuda.Stream()` in https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/extensions/transformer_engine.py and some internal code in the TransformerEngine package. This requires CUDA context to be properly set up. As mentioned in https://github.com/pytorch/pytorch/issues/2517, adding `mp.set_start_method('spawn')` is a workaround to ensure that the CUDA context is initialized correctly for each subprocess.."," , thanks for your suggestion regarding `spawn`, though in internal testing we have not encountered any issues related to this. Perhaps something is different in our environment setups, but I'm not sure.   , would it be possible for you to setup a reproducible example for each of your issues, using publicly available checkpoints such as the ones listed here: https://github.com/NVIDIA/MegatronLM?tab=readmeovfiledownloadingcheckpoints. If you could do that and provide your launch command, that would help me look into this. Thanks.","> [](https://github.com/TeddLi) You should use spawn as the start method for torch multiprocessing, otherwise CUDA context cannot be properly set up. A simple way to fix it is just add `torch.multiprocessing.set_start_method('spawn')` in https://github.com/NVIDIA/MegatronLM/blob/main/tools/checkpoint/convert.py before invoking subprocesses.   Thanks for the insightful response. Just wondering if you mind if I send you a DM? I send a msg to you email end with .edu.cn, ","Let's keep the discussion of github for now. Did you consider making a reproducible example? If you setup a script based on a public checkpoint, I can try to debug your issue.","> Let's keep the discussion of github for now. Did you consider making a reproducible example? If you setup a script based on a public checkpoint, I can try to debug your issue. Yes, I will make a reproducible example. I just used a random lamda2 checkpoint, will update this page later","nvidia Sure! Below is a reproducible example. I tested on the uptodate main branch of MegatronLM, H100 80G on Azure cloud, `torch==2.5.1+cu124`, `transformerengine=1.13.0.dev0`. I used Llama38B checkpoint downloaded from huggingface and ran the script below: ``` python tools/checkpoint/convert.py \     modeltype GPT \     modelsize llama38B \     loader llama_mistral \     saver mcore \     checkpointtype hf \     loaddir ${HF_FORMAT_DIR} \     savedir ${MG_FORMAT_DIR} \     tokenizermodel ${HF_FORMAT_DIR} \     targettensorparallelsize ${tp} \     targetpipelineparallelsize ${pp} ``` I encountered the error below: ``` Traceback (most recent call last):   File ""/usr/lib/python3.11/multiprocessing/process.py"", line 314, in _bootstrap     self.run()   File ""/usr/lib/python3.11/multiprocessing/process.py"", line 108, in run     self._target(*self._args, **self._kwargs)   File ""PATH/TO/MegatronLM/tools/checkpoint/saver_mcore.py"", line 555, in save_checkpoint     model = get_local_model(0, ep_rank, tp_rank)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""PATH/TO/MegatronLM/tools/checkpoint/saver_mcore.py"", line 548, in get_local_model     models[pp_rank][ep_rank][tp_rank] = model_provider(pre_process, post_process).to(md.params_dtype)                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""PATH/TO/MegatronLM/pretrain_gpt.py"", line 96, in model_provider     model = GPTModel(             ^^^^^^^^^   File ""PATH/TO/MegatronLM/megatron/core/models/gpt/gpt_model.py"", line 120, in __init__     self.decoder = TransformerBlock(                    ^^^^^^^^^^^^^^^^^   File ""PATH/TO/MegatronLM/megatron/core/transformer/transformer_block.py"", line 204, in __init__     get_cpu_offload_context(   File ""PATH/TO/MegatronLM/megatron/core/extensions/transformer_engine.py"", line 1075, in get_cpu_offload_context     context, sync_func = _get_cpu_offload_context(                          ^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/tiger/.local/lib/python3.11/sitepackages/transformer_engine/pytorch/cpu_offload.py"", line 504, in get_cpu_offload_context     cpu_offload_handler = AsyncDoubleBufferGroupOffloadHandler(                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/tiger/.local/lib/python3.11/sitepackages/transformer_engine/pytorch/cpu_offload.py"", line 314, in __init__     self.d2h_stream = torch.cuda.Stream()                       ^^^^^^^^^^^^^^^^^^^   File ""/usr/local/lib/python3.11/distpackages/torch/cuda/streams.py"", line 35, in __new__     return super().__new__(cls, priority=priority, **kwargs)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ RuntimeError: CUDA error: initialization error CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect. For debugging consider passing CUDA_LAUNCH_BLOCKING=1 Compile with `TORCH_USE_CUDA_DSA` to enable deviceside assertions. ```",thanks  for your script. I'll run it and get back to you," , I didn't see any errors when I ran your conversion command above on the Llama 3 8B model. I tested your conversion script for a few different NGC containers from https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch. The containers with the closest `torch`/`transformer_engine` versions you mentioned are:  24.10py3 : `torch==2.5.0a0+e000cf0ad9.nv24.10`, `transformer_engine==1.11.0+4df8488`  24.11py3 : `torch==2.6.0a0+df5bbc09d1.nv24.11`, `transformer_engine==1.12.0+7f2afaa` I wasn't able to test your exact versions because I couldn't either 1) pip install them, or 2) find those exact tags on their Github repos. Nevertheless, everything is working smoothly on my end. Maybe you could try an NGC container?",Marking as stale. No activity in 60 days.
Lzhang-hub,Dose Context Parallel support Packing Inputs Without Cross-Contamination Attention?,"For long seq model train,I want ues both `Context Parallel` and `packing Inputs without crosscontamination attention` link , Dose is support? crosscontamination attention like: !Image",2024-09-11T09:00:29Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1131,Marking as stale. No activity in 60 days.
ftgreat,Fix shape of qk_layernorm.,"With current qk_layernorm implement training did not converge. One shared qk_layernorm acts on every head, however qk_layernorm should affect all heads. So just change the shape of qk_layernorm weights, training converges as expected. Training logs are as follows. List some models using qk_layernorm for references: 1. https://github.com/mlfoundations/open_lm/blob/main/open_lm/model.pyL131 used in DCLM7B 2. https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmoe/modeling_olmoe.pyL394L395 used in OLMoE !img_v3_02ej_7e3a7fd9a3f3432fb66644925371236g",2024-09-10T12:24:55Z,,closed,0,10,https://github.com/NVIDIA/Megatron-LM/issues/1130,what layernorm do you use??? did you use fused norm or TE norm???  the reason why im asking is apex's fused LN does not allow RMSnorm for qk norm. actually i saw this commit first but cant find any analysis for convergence issue but i guess it could be just implementation issue that you mentioned not kernel issue. wdyt?,"i think it make senses normalize 4d tensor (B, T, nhead, dhead), but in this case, it should be like this. cmdr's qk layernorm has (nhead, dhead) shape weights.   but i found old phi model follow megatron's implementation and i think it's wrong like u said","for RMSnorm with TE, i still have convergence problem despite patch your commits  various runs == wide range of LRs, and orange one below is reference",> old phi model Thanks for reply. I used TE and all same settings except the qk_norm patch for both runs.,"did you use TP btw?  i think because TP shard q, k tensor from [B, T, 8, 128] to [B, T, 1 128], it should be head dimension not hidden size as it is now of course, in this case, it is not headwise normalization and they share stats","> did you use TP btw? i think because TP shard q, k tensor from [B, T, 8, 128] to [B, T, 1 128], it should be head dimension not hidden size as it is now >  > of course, in this case, it is not headwise normalization and they share stats No tp. With tp > 1, I think using qk norm of hidden size / tp is more flexible than just along head dimension.","> > did you use TP btw? i think because TP shard q, k tensor from [B, T, 8, 128] to [B, T, 1 128], it should be head dimension not hidden size as it is now > > of course, in this case, it is not headwise normalization and they share stats >  > No tp. With tp > 1, I think using qk norm of hidden size / tp is more flexible than just along head dimension. hmmm that makes sens. can you share what layernorm do you use? rmsnorm or layernorm?","> > > did you use TP btw? i think because TP shard q, k tensor from [B, T, 8, 128] to [B, T, 1 128], it should be head dimension not hidden size as it is now > > > of course, in this case, it is not headwise normalization and they share stats > >  > >  > > No tp. With tp > 1, I think using qk norm of hidden size / tp is more flexible than just along head dimension. >  > hmmm that makes sens. can you share what layernorm do you use? rmsnorm or layernorm? all rmsnorm.","> > > > did you use TP btw? i think because TP shard q, k tensor from [B, T, 8, 128] to [B, T, 1 128], it should be head dimension not hidden size as it is now > > > > of course, in this case, it is not headwise normalization and they share stats > > >  > > >  > > > No tp. With tp > 1, I think using qk norm of hidden size / tp is more flexible than just along head dimension. > >  > >  > > hmmm that makes sens. can you share what layernorm do you use? rmsnorm or layernorm? >  > all rmsnorm.  hmm in my setup, TEnorm + headwise qknorm doesn't converge  here's my runs  purple: 0.0625 lr, w/o qknorm red: 0.0625 lr, w/ qknorm, TE rms (hedawise) (this is your proposed) orange: 0.0625 lr, w/ qknorm, apex rms (hedawise) green: 0.0625 lr, w/ qknorm, apex rms (shared)","> > > > > did you use TP btw? i think because TP shard q, k tensor from [B, T, 8, 128] to [B, T, 1 128], it should be head dimension not hidden size as it is now > > > > > of course, in this case, it is not headwise normalization and they share stats > > > >  > > > >  > > > > No tp. With tp > 1, I think using qk norm of hidden size / tp is more flexible than just along head dimension. > > >  > > >  > > > hmmm that makes sens. can you share what layernorm do you use? rmsnorm or layernorm? > >  > >  > > all rmsnorm. >  >  > hmm in my setup, TEnorm + headwise qknorm doesn't converge >  > here's my runs >  > purple: 0.0625 lr, w/o qknorm red: 0.0625 lr, w/ qknorm, TE rms (hedawise) (this is your proposed) orange: 0.0625 lr, w/ qknorm, apex rms (hedawise) green: 0.0625 lr, w/ qknorm, apex rms (shared) thanks for sharing. "
CaoWGG,[QUESTION] Why is it unsafe for param.grad to be None when overlap_grad_reduce=True?,"**Your question** code link ```python3  if self.ddp_config.overlap_grad_reduce:         assert (             param.grad is not None         ), 'param.grad being None is not safe when overlap_grad_reduce is True' ``` Hi, when I remove the assert statement and set grad to None before param_hook , the training works fine.  I would like to know in what specific cases it is considered unsafe for `param.grad=None` when `overlap_grad_reduce=True`?",2024-09-10T07:36:42Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/1129,Hi! Do you know the answer? It would be quite helpful.,"It is unsafe (or at least was unsafe in past PyTorch releases) because PyTorch can launch backward posthooks in background threads if param.grad is None, leading to potential race conditions.",how to fix this issue? just skip it may cause problem ? 
zixianwang2022,[QUESTION] How to Identify Which Document Processing in Forward,"**Your question** Hi, is there a way to know which document or document id is the forward function processing?  I found that `datacachepath` flag will store `trainsample_index.npy`, `trainshuffle_index.npy`, and `traindocument_index.npy` in https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/datasets/gpt_dataset.pyL303 But is there a way to know exactly which document id my forward function is processing at each loop during training? ",2024-09-09T23:17:33Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1128,Marking as stale. No activity in 60 days.
zixianwang2022,[QUESTION] Epochs Larger Than 1 When Specified with Trained Samples,"**Your question** Ask a clear and concise question about MegatronLM. Hi, I am doing a toy experiment in training the model.  I specified the `TRAIN_SAMPLES=100` in my `train.sh`. And there's only 100 data points in my training dataset.  ``` TRAIN_SAMPLES=100   300B tokens / 4096 LR_WARMUP_SAMPLES=0 LR_DECAY_SAMPLES=100  TRAIN_SAMPLES  LR_WARMUP_SAMPLES options="" \     ...     trainsamples ${TRAIN_SAMPLES} \     lrwarmupsamples ${LR_WARMUP_SAMPLES} \     lrdecaysamples ${LR_DECAY_SAMPLES} \     ...     split 99,1,0 \ torchrun nproc_per_node 1 pretrain_model.py ${options} ``` But the log appears that it shows  `total number of epochs: 165`  despite I set `TRAIN_SAMPLES=100`  Why will this happen when I am using `trainsamples` flag instead of `trainitr`?  !Image",2024-09-09T22:50:50Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1127,Marking as stale. No activity in 60 days.
mikolajblaz,Fix parallel load convergence issue by ensuring BF16 exchange for FP8 tensors,WAR for TE bug,2024-09-09T14:19:24Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1126
KookHoiKim,[QUESTION] tensor_parallel.broadcast_data and train_valid_test_datasets_provider.is_distributed = True,"In my understanding, in pretrain code, it broadcasts the data from tp rank 0 to the rest tp rank gpus.  However, if i activate the option `train_valid_test_datasets_provider.is_distributed = True` while building dataloader, dataloader would be initialized on every gpus.  And it seems they return same data on every iteration. Then what does `tensor_parallel.broadcast_data` do for in this case? I am not sure that i understood the procedure of broadcasting data , so i would be very grateful if give me any information about this. Thanks.",2024-09-09T11:03:48Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1125,Marking as stale. No activity in 60 days.
junjzhang,[QUESTION] CP loss reduction behavior in loss_func of pretrain_gpt.py,"Hi, I wonder why the loss reduction of CP and DP is handled differently in `loss_func`ï¼Œwhere CP is reduced on `loss`ï¼Œand dp is reduced on detached `loss`. In my view, the loss reduction behavior for cross entropy is identical between CP and DP. Also, why should the output loss be multiplied by context_parallel_size? ```python def loss_func(loss_mask: torch.Tensor, output_tensor: torch.Tensor):     """"""Loss function.     Args:         loss_mask (torch.Tensor): Used to mask out some portions of the loss         output_tensor (torch.Tensor): The tensor with the losses     Returns:         the loss scalar for this microbatch         the number of nonpadded tokens in this microbatch         a dict containing reporting metrics on the loss and number of tokens across             the data parallel ranks     """"""     args = get_args()     losses = output_tensor.float()     loss_mask = loss_mask.view(1).float()     total_tokens = loss_mask.sum()     loss = torch.cat([torch.sum(losses.view(1) * loss_mask).view(1), total_tokens.view(1)])     if args.context_parallel_size > 1:         torch.distributed.all_reduce(loss, group=mpu.get_context_parallel_group())      Check individual rank losses are not NaN prior to DP allreduce.     if args.check_for_nan_in_loss_and_grad:         global_rank = torch.distributed.get_rank()         assert not loss[0].isnan(), (             f'Rank {global_rank}: found NaN in local forward loss calculation. '             f'Device: {torch.cuda.current_device()}, node: {os.uname()[1]}'         )      Reduce loss for logging.     reporting_loss = loss.clone().detach()     torch.distributed.all_reduce(reporting_loss, group=mpu.get_data_parallel_group())     local_num_tokens = loss[1].clone().detach().to(torch.int)     return (         loss[0] * args.context_parallel_size,         local_num_tokens,         {'lm loss': (reporting_loss[0], reporting_loss[1])},     ) ```",2024-09-09T07:20:03Z,,closed,0,5,https://github.com/NVIDIA/Megatron-LM/issues/1124,"I also have this question. The previous issue https://github.com/NVIDIA/MegatronLM/issues/673 explains the grads of `dp1cp2` and `dp2cp1` should be the same with the same global batch size. But I think the grads of `dp1cp2` and `dp1cp1` should also be the same with the same global batch size, but that's not the case.","> I also have this question. The previous issue [ CC([BUG] Incorrect loss scaling in context parallel code logic)](https://github.com/NVIDIA/MegatronLM/issues/673) explains the grads of `dp1cp2` and `dp2cp1` should be the same with the same global batch size. But I think the grads of `dp1cp2` and `dp1cp1` should also be the same with the same global batch size, but that's not the case. Thanks for replying! I've checked that issue and that solved my problems. Also, megatron just fixed a bug about grad scaling of CP in DDP, I think you should cherrypick this one to your `FlagScale` repo.",Same question as CC([BUG] Incorrect loss scaling in context parallel code logic) ,"> > I also have this question. The previous issue [ CC([BUG] Incorrect loss scaling in context parallel code logic)](https://github.com/NVIDIA/MegatronLM/issues/673) explains the grads of `dp1cp2` and `dp2cp1` should be the same with the same global batch size. But I think the grads of `dp1cp2` and `dp1cp1` should also be the same with the same global batch size, but that's not the case. >  > Thanks for replying! I've checked that issue and that solved my problems. Also, megatron just fixed a bug about grad scaling of CP in DDP, I think you should cherrypick this one to your `FlagScale` repo. Maybe you should consider the `dp1cp1` and `dp1cp2` cases where the `num_microbachs` is the same.","> > > I also have this question. The previous issue [ CC([BUG] Incorrect loss scaling in context parallel code logic)](https://github.com/NVIDIA/MegatronLM/issues/673) explains the grads of `dp1cp2` and `dp2cp1` should be the same with the same global batch size. But I think the grads of `dp1cp2` and `dp1cp1` should also be the same with the same global batch size, but that's not the case. > >  > >  > > Thanks for replying! I've checked that issue and that solved my problems. Also, megatron just fixed a bug about grad scaling of CP in DDP, I think you should cherrypick this one to your `FlagScale` repo. >  > Maybe you should consider the `dp1cp1` and `dp1cp2` cases where the `num_microbachs` is the same. Yes, I've got the point that CP does not slice batch and the averaging is done by dividing total num tokens. I mean that Megatron just fixes the bug I mentioned above which indicates the grad should be divided by DP x CP size, and I noticed that your repo also has this bug. Maybe you should cherry pick this bugfix or sync with Megatron. "
zixianwang2022,[QUESTION] Setting num-attention-heads=0 for Mamba,"**Your question** Hi, it seems like I have triggered many assertion errors when trying to train pure Mamba2 without any attention by setting ```NUM_ATTENTION_HEADS=0```.  Can I just give  ``` hybridattentionratio 0 \ hybridmlpratio 0 \ ``` and give `NUM_ATTENTION_HEADS` a random num to avoid triggering assertions?  I don't see all the errors by doing so. ",2024-09-08T23:38:31Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1123
ltm920716,[QUESTION] which ngc version is okï¼Ÿ,"**Your question** Ask a clear and concise question about MegatronLM. I tried two ngc pytorch image as base envï¼Œneither is ok. test script: ``` PYTHONPATH=$PYTHONPATH:./megatron torchrun nprocpernode 2 examples/run_simple_mcore_train_loop.py ``` for  nvcr.io/nvidia/pytorch:23.10py3 ``` Traceback (most recent call last):   File ""/home/MegatronLM/examples/run_simple_mcore_train_loop.py"", line 13, in      from megatron.core.models.gpt.gpt_model import GPTModel   File ""/home/MegatronLM/megatron/core/models/gpt/__init__.py"", line 2, in      from .gpt_model import GPTModel   File ""/home/MegatronLM/megatron/core/models/gpt/gpt_model.py"", line 17, in      from megatron.core.transformer.transformer_block import TransformerBlock   File ""/home/MegatronLM/megatron/core/transformer/transformer_block.py"", line 23, in      from megatron.core.extensions.transformer_engine import (   File ""/home/MegatronLM/megatron/core/extensions/transformer_engine.py"", line 853, in  Traceback (most recent call last):   File ""/home/MegatronLM/examples/run_simple_mcore_train_loop.py"", line 13, in      from megatron.core.models.gpt.gpt_model import GPTModel   File ""/home/MegatronLM/megatron/core/models/gpt/__init__.py"", line 2, in      class TECudaRNGStatesTracker(te.pytorch.distributed.CudaRNGStatesTracker): AttributeError: module 'transformer_engine.pytorch.distributed' has no attribute 'CudaRNGStatesTracker'    from .gpt_model import GPTModel   File ""/home/MegatronLM/megatron/core/models/gpt/gpt_model.py"", line 17, in      from megatron.core.transformer.transformer_block import TransformerBlock   File ""/home/MegatronLM/megatron/core/transformer/transformer_block.py"", line 23, in      from megatron.core.extensions.transformer_engine import (   File ""/home/MegatronLM/megatron/core/extensions/transformer_engine.py"", line 853, in      class TECudaRNGStatesTracker(te.pytorch.distributed.CudaRNGStatesTracker): AttributeError: module 'transformer_engine.pytorch.distributed' has no attribute 'CudaRNGStatesTracker' [20240906 14:12:46,521] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 498) of binary: /usr/bin/python Traceback (most recent call last):   File ""/usr/local/bin/torchrun"", line 8, in      sys.exit(main())   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper     return f(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/run.py"", line 806, in main     run(args)   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/run.py"", line 797, in run     elastic_launch(   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/launcher/api.py"", line 134, in __call__     return launch_agent(self._config, self._entrypoint, list(args))   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/launcher/api.py"", line 264, in launch_agent     raise ChildFailedError( torch.distributed.elastic.multiprocessing.errors.ChildFailedError: ``` for nvcr.io/nvidia/pytorch:23.05py3 ``` Traceback (most recent call last):   File ""/home/MegatronLM/examples/run_simple_mcore_train_loop.py"", line 8, in      from megatron.core import parallel_state   File ""/home/MegatronLM/megatron/core/__init__.py"", line 5, in      from megatron.core.distributed import DistributedDataParallel   File ""/home/MegatronLM/megatron/core/distributed/__init__.py"", line 3, in      from .distributed_data_parallel import DistributedDataParallel   File ""/home/MegatronLM/megatron/core/distributed/distributed_data_parallel.py"", line 15, in      from .param_and_grad_buffer import BucketGroup, ParamAndGradBuffer, partition_buckets   File ""/home/MegatronLM/megatron/core/distributed/param_and_grad_buffer.py"", line 10, in      from torch.distributed import _coalescing_manager ImportError: cannot import name '_coalescing_manager' from 'torch.distributed' (/usr/local/lib/python3.10/distpackages/torch/distributed/__init__.py) Traceback (most recent call last):   File ""/home/MegatronLM/examples/run_simple_mcore_train_loop.py"", line 8, in      from megatron.core import parallel_state   File ""/home/MegatronLM/megatron/core/__init__.py"", line 5, in  ``` so sad I  install pytorch locally and do not install transformer_engine, and I  have to change some source code to run",2024-09-06T14:32:11Z,,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/1122,24.02,"> 24.02 thanks,  it is ok! I recompile TransformerEngine from source is ok too by pytorch:23.10py3","If i want to use conda to install pytorch, Which version of ngc24.02 corresponds to in condaï¼Ÿ thankï¼",thxï¼Œi have found it in https://docs.nvidia.com/deeplearning/frameworks/pytorchreleasenotes/rel2402.html. PyTorch quantization wheel 2.1.2
heavyrain-lzy,Encoder and Decoder has different TP_SIZE,"**Your question** 1. I have a question about creating the `pp` groups when enabling `context_parallel_size > 1` and `encoder_tensor_parallel_size != tensor_parallel_size`. When enabling `context_parallel`, the input will be split symmetrically to balance the calculation. Using `zip(cycle(e_ranks), d_ranks)` is wrong. https://github.com/NVIDIA/MegatronLM/blob/4ff9e66ec870e6e45d765531318d50bd47c85d0f/megatron/core/parallel_state.pyL602 2. Why do we use `stack` operator to calculate the sum of received tensor. https://github.com/NVIDIA/MegatronLM/blob/46ca0688e9597697d34340dfcf32f6fc92120272/megatron/core/pipeline_parallel/p2p_communication.pyL402",2024-09-05T01:39:56Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1121,Marking as stale. No activity in 60 days.
pstjohn,Add project section to pyproject.toml,"In bionemo we're exploring using `uv` to handle pinning pypi dependencies in our docker builds. For this to work, we'll need to modernize some of the packaging for NeMo and mcore by adding `project` sections to their pyproject.toml files.",2024-09-04T21:12:39Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1120,Completes: https://jirasw.nvidia.com/browse/CDISCOVERY4418,"Closing since this repo is readonly, MR opened against internal gitlab"
new-TonyWang,[QUESTION] why non-interleaved pipeline does not support overlap_p2p_comm?,"**Your question** Ask a clear and concise question about MegatronLM.  why  noninterleaved does not support overlap_p2p_comm?  ```python     if args.num_layers_per_virtual_pipeline_stage is not None:         if args.overlap_p2p_comm:             assert args.pipeline_model_parallel_size > 1, \                 'when interleaved schedule is used, pipelinemodelparallel size '\                 'should be greater than 1'         else:             assert args.pipeline_model_parallel_size > 2, \                 'when interleaved schedule is used and p2p communication overlap is disabled, '\                 'pipelinemodelparallel size should be greater than 2 to avoid having multiple '\                 'p2p sends and recvs between same 2 ranks per communication batch'         assert args.num_layers % args.transformer_pipeline_model_parallel_size == 0, \             'number of layers should be divisible by the pipeline parallel size'         num_layers_per_pipeline_stage = args.num_layers // args.transformer_pipeline_model_parallel_size         assert num_layers_per_pipeline_stage % args.num_layers_per_virtual_pipeline_stage == 0, \             'number of layers per pipeline stage must be divisible number of layers per virtual pipeline stage'         args.virtual_pipeline_model_parallel_size = num_layers_per_pipeline_stage // \             args.num_layers_per_virtual_pipeline_stage     else:         args.virtual_pipeline_model_parallel_size = None          Overlap P2P communication is disabled if not using the interleaved schedule.         args.overlap_p2p_comm = False         if args.rank == 0:             print('WARNING: Setting args.overlap_p2p_comm to False since noninterleaved '                   'schedule does not support overlapping p2p communication') ```",2024-09-04T15:25:49Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1069
ZSL98,[QUESTION] all gather for MoE permutation seems redundant?,"I have a node with 8 GPUs. The model has 8 experts and I use TP=1, EP=8, with sequence parallel on. Then I expect each GPU has one expert. I use the `MoEAllGatherTokenDispatcher`. The size of the hidden_states is [S/TP, B, H] for `token_permutation`, which is actually [S, B, H] because TP=1. Then why we still need `tensor_parallel.gather_from_sequence_parallel_region_to_moe` to gather and form a `global_hidden_states`, whose size will become [S * EP, B, H]? In my view, each rank has a copy of the [S, B, H] hidden_states, why there is still a need for all gather?  There are only B * S tokens to compute, but now each rank has B * S * EP tokens, because we use get_tensor_and_expert_parallel_group() and its size is TP * EP=8.",2024-09-04T12:23:19Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1068
weipingtao,Update fully_parallel.py,the np.product function is wrong. it should be np.prod,2024-09-04T08:25:37Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1067,Marking as stale. No activity in 60 days.
KookHoiKim,[QUESTION] variable tensor shape when pipeline parallelism (pp),"I am working with LLaVA code and i have a question about sequence length when using pipeline parallelism. In my understanding, tensor shape for recv, send is fixed using args.seq_length in pipeline_parallel/schedules.py. And if padding tokens make up most of the input, it becomes very inefficient in terms of memory or execution speed.  Is there any way to use variable input length when using pipeline parallel?  Thanks. ",2024-09-04T07:37:04Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1066
Aurelius84,Fix duplicate init for self.module in DistributedDataParallel,"When I learning source code in MegatronLLM, I found it seems no need to assign `self.module` twice.",2024-09-04T06:40:10Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1065,Marking as stale. No activity in 60 days.
panpan0000,Doc: fix bad path for original examples scripts in README,fix CC([BUG] Invalid Link for examples script in README) ,2024-09-04T02:49:30Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1064,"Thanks . We have incorporated your changes. MR will be merged soon. It flows from a diff path, so not using this mr. "
panpan0000,[BUG] Invalid Link for examples script in README,**Describe the bug** those pretrain_xxx.sh link and path in README.md is invalid (they are moved out of examples folder) !Image,2024-09-04T02:49:13Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1063,Fixed and closing.
clarence-lee-sheng,[QUESTION] Document indices extended in add_document function in IndexedDatasetBuilder but not in add_item,"Dear MegatronLM team  In the IndexedDatasetBuilder class add_item function (https://github.com/NVIDIA/MegatronLM/blob/912be7889298da7b3dbf39dea5b4441c1e412488/megatron/core/datasets/indexed_dataset.pyL767L779), the document indices is not extended, however for the add_document function, the document indices are extended (https://github.com/NVIDIA/MegatronLM/blob/912be7889298da7b3dbf39dea5b4441c1e412488/megatron/core/datasets/indexed_dataset.pyL781L798). May I ask why is that the case and when do we use add_item?  Thank you! ",2024-09-03T02:59:33Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1062
lawchingman,[ENHANCEMENT],**Is your feature request related to a problem? Please describe.** A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] **Describe the solution you'd like** A clear and concise description of what you want to happen. **Describe alternatives you've considered** A clear and concise description of any alternative solutions or features you've considered. **Proposed implementation** If you have a proposed implementation for the feature state it here or link to a PR. **Additional context** Add any other context or screenshots about the feature request here.,2024-09-02T19:49:39Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1061
qazwsx74269,[BUG] init_method not declared,https://github.com/NVIDIA/MegatronLM/blob/54f1f78529cbc2b9cddad313e7f9d96ac0420a27/megatron/legacy/model/multiple_choice.pyL42,2024-09-02T14:42:40Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1060,"Hi, this is in legacy so it has a low priority. Can you please provide a fix and we will try to merge it?",Marking as stale. No activity in 60 days.
wangdaw2023,[QUESTION] how to control GPU memory layout for 70B LLM model?,"I am training 70B Megatron LLM on A800 with 32 nodes cluster. The cluster is composed of 32 nodes with 8 * A800 and 4 * RoCE 200Gb/s. I find 70B MFU 20% is quite lower than 32B model MFU 47%.  Besides, I find some node GPU memory usage is 70GB, while other node memory usage is 50GB. I would like to tune memory usage to the same level to use bigger micro batch size to improve MFU.  It involves to place/layout which LLM layer to which rank. Any document for this topic? 32B LLM, TP=8, PP=1, MFU=47% 70B LLM, TP=8, PP=2, MFU=20%",2024-09-02T11:12:22Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1059,"Maybe try to set smaller TP larger PP (e.g. TP=4, PP=4 or TP=4, PP=8) for 70B case."
lumosity4tpj,[BUG] No submodule mapping found in .gitmodules for path 'tests/functional_tests/local_recipes',"**Describe the bug** Because of this submodule: tests/functional_tests/local_recipes, we need to add a .gitmodules, otherwise we will receive an error message 'No submodule mapping found'.",2024-09-02T03:13:29Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1058,Fixed by https://github.com/NVIDIA/MegatronLM/commit/c14d9874d2c00274d7df2d958c387cebfc1c76f7 
felipeliliti,A story from Felipe Marcos de Abreu Aquino on Medium,Read â€œ **O Supercomputador Que TornarÃ¡ o Brasil Soberano em Tecnologia: Minha Proposta para a NaÃ§Ã£o**â€œ by Felipe Marcos de Abreu Aquino on Medium: https://medium.com//osupercomputadorquetornar%C3%A1obrasilsoberanoemtecnologiaminhapropostaparaana%C3%A7%C3%A3od0029dd502eb,2024-08-31T16:57:40Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1057
harishankar-gopalan,[BUG] Precision issue while having more than `2.6T` documents in a bin and idx file due to `int32_t` usage and assertion in `gpt_dataset` line 427.,"**Describe the bug** Int32 limit for number of documents and samples within one indexed dataset bin and idx. We created one large bin and idx file and used the same for a training run. The dataset, was loaded through blended_megatron_dataset_builder  backed by the mid level gpt_dataset  which in turn is backed by the low level indexed_dataset. The exact issue starts from these lines and propagates through all the below mentioned code path: 1. blended_magatron_dataset_builder._build_megatron_dataset_splits 2. gpt_dataset._build_document_index 3. gpt_dataset._build_document_sample_shuffle_indices  line:427 4. gpt_dataset._build_document_sample_shuffle_indices  line:440 5. helpers.cppbuild_sample_idx 6. Also another observation about [helpers.cpp]() is that in the version `core_v0.7.0` there are few other parts of the method mentioned in the previous point that use int32_t, for eg:     helpers.cpp CC(Does scaled softmax of qkv perform scaling operations duplicately?)     helpers.cpp CC(How to pretrain model with multiple nodes and multiple gpus?) 7. Although the above occurrences of `int32_t` in `helpers.cpp` seem to be changed to `int64_t` in `core_r0.8.0`, is not sufficient to fix the whole precision problem while using one large `.bin` and `.idx` files. 8. I can surely contribute a fix for the `blended_dataset`, `gpt_dataset` and `indexed_dataset` part of it and even started working on it, but I can see that there are quite few more usages of `blended_dataset` and `indexed_dataset` in other midlevel datasets like Bert based datasets. I am not sure how well I would be able to take those into consideration as I have not used those datasets within NeMo and do not know the full working of those datasets and might need some insider help on those. **To Reproduce** Create one large bin/idx file which can create number of documents larger than int32 range. **Expected behavior** Data loading should work fine even if one bin/idx file contains documents count more than int32 range. **Stack trace/logs** N/A, we will see weird behavior where we get negative indices and that leads to very large documents being tried to be loaded. It becomes very complicated to even understand that the issue is int32 range issue. **Environment (please complete the following information):**   MegatronLM `core_v0.7.0` and `core_v0.8.0`   PyTorch 2.x (but immaterial to reproduce this behavior)   CUDA 12.x (but immaterial to reproduce this behavior)   NCCL 2.x (but immaterial to reproduce this behavior) **Proposed fix** Based on the number of samples in the bin/idx either in32_t or in64_t should be automatically chosen. I have created a proposed patch here.",2024-08-31T06:42:19Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/1056,Thanks for opening this bug. Have you been able to test your proposed fix gopalan ?,"Hi John,   Yes we have run a training using the proposed fix. I can run a re test if required and keep you posted here. But my larger point is the fix does not address a possible similar issue in BertDataset and other datasets that are used as a mid level dataset inside BlendedDataset.",Marking as stale. No activity in 60 days.
vasunvidia,Enable optional kwargs with CUDA graph,,2024-08-30T17:50:15Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1055
crcrpar,"use `torch.exp_` not `torch.exp(..., out=)`",To make it a bit friendlier to torch.compile,2024-08-30T09:33:52Z,stale,open,1,2,https://github.com/NVIDIA/Megatron-LM/issues/1054,", could you please review and help merge this PR? ",Marking as stale. No activity in 60 days.
LouChao98,[BUG] Zarr checkpoint loses distributed optimizer states due to lack of synchronizers on ranks that create arrays,"**Describe the bug** When using a Zarr distributed checkpoint and a distributed optimizer, each rank writes optimizer states according to ShardedTensor's flattened_range. The Zarr strategy uses synchronizers to ensure the correctness of parallel writing. However, synchronizers are not set for ranks that create Zarr arrays. The current implementation only adds synchronizers on ranks that open existing Zarr arrays. Consequently, the writing on the creating ranks may be lost, resulting in all zeros at the corresponding slices in the file. **To Reproduce** run `pretrain_gpt.py`  with DP>1, TP>1 and arguments ``` save {SOME_WHERE} \ saveinterval 10 \ usedistckpt \ distckptformat zarr \ usedistributedoptimizer \ weightdecay 0.1  ensure nonzero grads everywhere and make test easy ``` Then, a toy test inserted after `dist_checkpointing.save` in the following block may not pass https://github.com/NVIDIA/MegatronLM/blob/86e2927edaa977f3e859d6f4b6d38a236114fd38/megatron/training/checkpointing.pyL405L407  ```python  load saved ckpt and compare state_dict_for_load = generate_state_dict(args, model, optimizer, opt_param_scheduler, rng_state,                         use_dist_ckpt, iteration, optim_sd_kwargs=optim_sd_kwargs) load_strategy = get_default_load_sharded_strategy(checkpoint_name) state_dict_loaded = dist_checkpointing.load(state_dict_for_load, checkpoint_name, load_strategy) opt_state_correct = {key: deepcopy(value) for key, value in optimizer.optimizer.state.items()} optimizer.load_state_dict(state_dict_loaded['optimizer']) opt_state_loaded = optimizer.optimizer.state for oi, opt in state_dict_loaded['optimizer']['param_state'].items():     for key in ['exp_avg', 'exp_avg_sq']:         if isinstance(state_dict['optimizer']['param_state'][oi][key], list):             param_key = ';'.join([item.key for item in state_dict['optimizer']['param_state'][oi][key]])         else:             param_key = state_dict['optimizer']['param_state'][oi][key].key          after a few steps of training, it is unlikely to have zerovalued tensors in models and optimizers. So a all zero tensor indicates an error.         assert opt[key].abs().sum() != 0, param_key   ``` Add a barrier under this line and using larger DP size may increase the probability of reproducing the failure: https://github.com/NVIDIA/MegatronLM/blob/86e2927edaa977f3e859d6f4b6d38a236114fd38/megatron/core/dist_checkpointing/strategies/zarr.pyL64 **Expected behavior** All tensors should be written to the disk. **Stack trace/logs** If applicable, add the stack trace or logs from the time of the error. **Environment (please complete the following information):**   MegatronLM commit ID 9bcd4175becc515331537f0c78eb70079de0eaa8   PyTorch version 2.3.0a0+ebedce2   CUDA version 12.3   NCCL version 2.20.3 **Proposed fix** Set synchronizers when creating Zarr arrays, mirroring the logic used when opening existing Zarr arrays: ```         arr = zarr.create(             sharded_tensor.global_shape,             dtype=np_dtype,             store=checkpoint_dir / sharded_tensor.key,             chunks=sharded_tensor.max_allowed_chunks(),             compressor=None,             fill_value=None,             write_empty_chunks=True,             synchronizer=(   add this                 zarr.ProcessSynchronizer(str(checkpoint_dir / f'{sharded_tensor.key}.sync'))    add this                 if sharded_tensor.flattened_range is not None    add this                 else None    add this             ),         ) ``` **Additional context** Add any other context about the problem here.",2024-08-30T07:45:20Z,stale,open,1,12,https://github.com/NVIDIA/Megatron-LM/issues/1053,"Same here. This issue can be pretty serious, and needs to be fixed very soon.","I confirm this is a bug and your fix looks relevant, thanks :+1: Please note the zarr format is being deprecated and in particular does not play well with the DistributedOptimizer, so I suggest updating the ckpt format to `distckptformat torch_dist`.","Does this problem occur only when Tensor Parallelism (TP) > 1 and Data Parallelism (DP) > 1? Currently, I am using DistributedOptimizer with TP = 1 and DP > 1. Will storing checkpoints in Zarr format cause an issue?","> Does this problem occur only when Tensor Parallelism (TP) > 1 and Data Parallelism (DP) > 1? Currently, I am using DistributedOptimizer with TP = 1 and DP > 1. Will storing checkpoints in Zarr format cause an issue? With TP=1 it might be an issue as well, please use `distckptformat torch_dist`.","> Does this problem occur only when Tensor Parallelism (TP) > 1 and Data Parallelism (DP) > 1? Currently, I am using DistributedOptimizer with TP = 1 and DP > 1. Will storing checkpoints in Zarr format cause an issue? It is okay with TP = 1 and DP > 1, in my envs.",Marking as stale. No activity in 60 days.,"same issue with ""usedistributedoptimizer ckptformat torch fp16"".  It seems only fp16 has the problem. The other dtype fp32 or bf16 works well. Megatron: core_r0.9.0, 1afee592e85ac7994887eb5f4ef3998f76384333","> same issue with ""usedistributedoptimizer ckptformat torch"". Megatron: core_r0.9.0, 1afee59  this one is not expected, can you elaborate on the symptoms? With `ckptformat torch` we don't do any filebased synchronization.","When using `usedistributedoptimizer` with  ckptformat torch and using dtype fp16, save_checkpoint has only dp rank=0 gathers all the others optimizer status and save to file. So, it should not have any sync problem? The error info: ```bash ================== tensor keys: dict_keys(['param']), dp rank: 0, optim_state:{}, main_param:tensor([0., 0., 0.,  ..., 0., 0., 0.], ) ================ Traceback (most recent call last):   File ""/workspace/MegatronLM/pretrain_gpt.py"", line 264, in      pretrain(   File ""/workspace/MegatronLM/megatron/training/training.py"", line 349, in pretrain     iteration, num_floating_point_operations_so_far = train(   File ""/workspace/MegatronLM/megatron/training/training.py"", line 1366, in train     save_checkpoint_and_time(iteration, model, optimizer,   File ""/workspace/MegatronLM/megatron/training/training.py"", line 1070, in save_checkpoint_and_time     save_checkpoint(iteration, model, optimizer, opt_param_scheduler,   File ""/workspace/MegatronLM/megatron/training/checkpointing.py"", line 380, in save_checkpoint     optimizer.save_parameter_state(optim_checkpoint_name)   File ""/workspace/MegatronLM/megatron/core/optimizer/distrib_optimizer.py"", line 902, in save_parameter_state     state_dict = self.get_parameter_state_dp_zero()   File ""/workspace/MegatronLM/megatron/core/optimizer/distrib_optimizer.py"", line 852, in get_parameter_state_dp_zero     tensors[key].detach().cpu() KeyError: 'exp_avg' ``` The above tensors is optimizer parameter, and it seems that optimizer state is None.","> When using `usedistributedoptimizer` with ckptformat torch and using dtype fp16, save_checkpoint has only dp rank=0 gathers all the others optimizer status and save to file. So, it should not have any sync problem? >  > The error info: >  > ================== tensor keys: dict_keys(['param']), dp rank: 0, optim_state:{}, main_param:tensor([0., 0., 0.,  ..., 0., 0., 0.], ) ================ > Traceback (most recent call last): >   File ""/workspace/MegatronLM/pretrain_gpt.py"", line 264, in  >     pretrain( >   File ""/workspace/MegatronLM/megatron/training/training.py"", line 349, in pretrain >     iteration, num_floating_point_operations_so_far = train( >   File ""/workspace/MegatronLM/megatron/training/training.py"", line 1366, in train >     save_checkpoint_and_time(iteration, model, optimizer, >   File ""/workspace/MegatronLM/megatron/training/training.py"", line 1070, in save_checkpoint_and_time >     save_checkpoint(iteration, model, optimizer, opt_param_scheduler, >   File ""/workspace/MegatronLM/megatron/training/checkpointing.py"", line 380, in save_checkpoint >     optimizer.save_parameter_state(optim_checkpoint_name) >   File ""/workspace/MegatronLM/megatron/core/optimizer/distrib_optimizer.py"", line 902, in save_parameter_state >     state_dict = self.get_parameter_state_dp_zero() >   File ""/workspace/MegatronLM/megatron/core/optimizer/distrib_optimizer.py"", line 852, in get_parameter_state_dp_zero >     tensors[key].detach().cpu() > KeyError: 'exp_avg' > The above tensors is optimizer parameter, and it seems that optimizer state is None. Please open a different issue for that, it's totally separate","> > When using `usedistributedoptimizer` with ckptformat torch and using dtype fp16, save_checkpoint has only dp rank=0 gathers all the others optimizer status and save to file. So, it should not have any sync problem? > > The error info: > > ================== tensor keys: dict_keys(['param']), dp rank: 0, optim_state:{}, main_param:tensor([0., 0., 0.,  ..., 0., 0., 0.], ) ================ > > Traceback (most recent call last): > > File ""/workspace/MegatronLM/pretrain_gpt.py"", line 264, in  > > pretrain( > > File ""/workspace/MegatronLM/megatron/training/training.py"", line 349, in pretrain > > iteration, num_floating_point_operations_so_far = train( > > File ""/workspace/MegatronLM/megatron/training/training.py"", line 1366, in train > > save_checkpoint_and_time(iteration, model, optimizer, > > File ""/workspace/MegatronLM/megatron/training/training.py"", line 1070, in save_checkpoint_and_time > > save_checkpoint(iteration, model, optimizer, opt_param_scheduler, > > File ""/workspace/MegatronLM/megatron/training/checkpointing.py"", line 380, in save_checkpoint > > optimizer.save_parameter_state(optim_checkpoint_name) > > File ""/workspace/MegatronLM/megatron/core/optimizer/distrib_optimizer.py"", line 902, in save_parameter_state > > state_dict = self.get_parameter_state_dp_zero() > > File ""/workspace/MegatronLM/megatron/core/optimizer/distrib_optimizer.py"", line 852, in get_parameter_state_dp_zero > > tensors[key].detach().cpu() > > KeyError: 'exp_avg' > > The above tensors is optimizer parameter, and it seems that optimizer state is None. >  The issue has been resolved; it was my misunderstanding of FP16 training.  When using FP16 training, the training is unstable during the first few steps, and the optimizer state will be empty. However, I tried saving the optimizer during these unstable steps, which could have caused the error. ",Marking as stale. No activity in 60 days.
qazwsx74269,[QUESTION] Confusion about return if args.partitions==1,"When I want to preprocess data for pretraining Bert model, I find this code snippet, ``` python tools/preprocess_data.py \        input mycorpus.json \        outputprefix mybert \        vocabfile bertvocab.txt \        tokenizertype BertWordPieceLowerCase \        splitsentences ``` does not work. It won't generate .bin and .idx files because `return` after `if args.partitions==1` ends the program. https://github.com/NVIDIA/MegatronLM/blob/86e2927edaa977f3e859d6f4b6d38a236114fd38/tools/preprocess_data.pyL362",2024-08-30T06:22:39Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1052
zmtttt,[QUESTION]NCCL Timeout Error during train_val_test_data_provider,"NCCL Timeout Error during train_val_test_data_provider I encountered an NCCL timeout error during the train_val_test_data_provider step while using 8 GPUs (2, 2, 2).  Why am I experiencing this issue?  Perhaps the NCCL timeout is related to communication problems?  It seems that both too large and too small data sets can cause NCCL timeouts. The code was struck when :         train_data_iterator, valid_data_iterator, test_data_iterator \             = build_train_valid_test_data_iterators(                 train_valid_test_dataset_provider) errors: [E ProcessGroupNCCL.cpp:737] [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=749, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1800258 milliseconds before timing out. [E ProcessGroupNCCL.cpp:414] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data. To avoid this inconsistency, we are taking the entire process down. [E ProcessGroupNCCL.cpp:737] [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=749, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1801313 milliseconds before timing out. terminate called after throwing an instance of 'std::runtime_error'   what():  [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=749, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1800258 milliseconds before timing out. Fatal Python error: Aborted thanks  !!!!",2024-08-30T06:16:00Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1051,    
qq1243196045,The program got stuck in â€compiling and loading fused kernels ...â€œ and timed out after ten minutes,"when I try to train gpt3, program got stuck in ""â€compiling and loading fused kernels "",I Don't know what happend ``` > initialized tensor model parallel with size 2 > initialized pipeline model parallel with size 4 > setting random seeds to 1234 ... > compiling dataset index builder ... make: Entering directory '/home/m01119/projects/MegatronLM/megatron/core/datasets' make: Nothing to be done for 'default'. make: Leaving directory '/home/m01119/projects/MegatronLM/megatron/core/datasets' >>> done with dataset index builder. Compilation time: 0.016 seconds > compiling and loading fused kernels ... ``` and timed out after ten minutes: ``` > compiling and loading fused kernels ... compiling and loading fused kernels[rank1]:[E829 16:32:30.656674606 ProcessGroupNCCL.cpp:607] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600062 milliseconds before timing out. [rank1]:[E829 16:32:30.657006650 ProcessGroupNCCL.cpp:1664] [PG 0 (default_pg) Rank 1] Exception (either an error or timeout) detected by watchdog at work: 1, last enqueued NCCL work: 1, last completed NCCL work: 1. [rank0]:[E829 16:32:31.155765764 ProcessGroupNCCL.cpp:607] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600079 milliseconds before timing out. [rank0]:[E829 16:32:31.156044057 ProcessGroupNCCL.cpp:1664] [PG 0 (default_pg) Rank 0] Exception (either an error or timeout) detected by watchdog at work: 1, last enqueued NCCL work: 1, last completed NCCL work: 1. swdevopsnetrb01:81609:83955 [0] NCCL INFO [Service thread] Connection closed by localRank 0 swdevopsnetrb01:81609:83309 [0] NCCL INFO comm 0xa8e9da0 rank 0 nranks 8 cudaDev 0 busId 4f000  Abort COMPLETE [rank0]:[E829 16:32:32.273721273 ProcessGroupNCCL.cpp:1709] [PG 0 (default_pg) Rank 0] Timeout at NCCL work: 1, last enqueued NCCL work: 1, last completed NCCL work: 1. [rank0]:[E829 16:32:32.273735274 ProcessGroupNCCL.cpp:621] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data. [rank0]:[E829 16:32:32.273743857 ProcessGroupNCCL.cpp:627] [Rank 0] To avoid data inconsistency, we are taking the entire process down. [rank0]:[E829 16:32:32.274971279 ProcessGroupNCCL.cpp:1515] [PG 0 (default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600079 milliseconds before timing out. Exception raised from checkTimeout at /opt/conda/condabld/pytorch_1720538439675/work/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:609 (most recent call first): frame CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fafdfae1f86 in /home/m01119/miniconda3/envs/megatron/lib/python3.12/sitepackages/torch/lib/libc10.so) frame CC(Faster dataloader merge): c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional > >) + 0x1d2 (0x7fafe0dc50b2 in /home/m01119/miniconda3/envs/megatron/lib/python3.12/sitepackages/torch/lib/libtorch_cuda.so) frame CC(rename confusing arg): c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7fafe0dcbaf3 in /home/m01119/miniconda3/envs/megatron/lib/python3.12/sitepackages/torch/lib/libtorch_cuda.so) frame CC(Compatibility with pytorchtransformers for finetuning ): c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10c (0x7fafe0dcdedc in /home/m01119/miniconda3/envs/megatron/lib/python3.12/sitepackages/torch/lib/libtorch_cuda.so) frame CC(Note that as of now you need to have PySOL cloned to the directory here before building the container.):  + 0xdbbf4 (0x7fb0308d3bf4 in /home/m01119/miniconda3/envs/megatron/lib/python3.12/sitepackages/torch/lib/../../../.././libstdc++.so.6) frame CC(I do not know how to solve it.):  + 0x76db (0x7fb03a0f86db in /lib/x86_64linuxgnu/libpthread.so.0) frame CC(perplexity too big for gpt2 wikitext evaluation): clone + 0x3f (0x7fb03967c61f in /lib/x86_64linuxgnu/libc.so.6) W0829 16:32:33.985000 140611253834240 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 81610 closing signal SIGTERM W0829 16:32:33.989000 140611253834240 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 81611 closing signal SIGTERM W0829 16:32:33.993000 140611253834240 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 81612 closing signal SIGTERM W0829 16:32:33.996000 140611253834240 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 81613 closing signal SIGTERM W0829 16:32:33.998000 140611253834240 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 81615 closing signal SIGTERM W0829 16:32:34.015000 140611253834240 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 81616 closing signal SIGTERM W0829 16:32:34.022000 140611253834240 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 81617 closing signal SIGTERM E0829 16:32:39.298000 140611253834240 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 6) local_rank: 0 (pid: 81609) of binary: /home/m01119/miniconda3/envs/megatron/bin/python Traceback (most recent call last):   File ""/home/m01119/miniconda3/envs/megatron/bin/torchrun"", line 33, in      sys.exit(load_entry_point('torch==2.4.0', 'console_scripts', 'torchrun')())              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/m01119/miniconda3/envs/megatron/lib/python3.12/sitepackages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 348, in wrapper     return f(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^   File ""/home/m01119/miniconda3/envs/megatron/lib/python3.12/sitepackages/torch/distributed/run.py"", line 901, in main     run(args)   File ""/home/m01119/miniconda3/envs/megatron/lib/python3.12/sitepackages/torch/distributed/run.py"", line 892, in run     elastic_launch(   File ""/home/m01119/miniconda3/envs/megatron/lib/python3.12/sitepackages/torch/distributed/launcher/api.py"", line 133, in __call__     return launch_agent(self._config, self._entrypoint, list(args))            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/m01119/miniconda3/envs/megatron/lib/python3.12/sitepackages/torch/distributed/launcher/api.py"", line 264, in launch_agent     raise ChildFailedError( torch.distributed.elastic.multiprocessing.errors.ChildFailedError: ====================================================== pretrain_gpt.py FAILED  Failures:     Root Cause (first observed failure): [0]:   time      : 20240829_16:32:33   host      : swdevopsnetrb01   rank      : 0 (local_rank: 0)   exitcode  : 6 (pid: 81609)   error_file:    traceback : Signal 6 (SIGABRT) received by PID 81609 ====================================================== ``` what should I do?",2024-08-29T08:42:09Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1050,https://github.com/NVIDIA/MegatronLM/issues/998 Hope this help you. ,> [ CC([BUG] llava pipeline parallel initialization problem)](https://github.com/NVIDIA/MegatronLM/issues/998) >  > Hope this help you. IT WORKS! thank you :)
Guodanding,[QUESTION]I cannot figure out TE(transformer_engine),"I have not installed nvidia transformer_engine. I try running some example scripts (e.g train_mixtral_8x7b_distributed.sh) but fail with the TE error: ``` [rank0]: Traceback (most recent call last): [rank0]:   File ""/share/home/wenjunyi/guokb/workspace/implemoe/MegatronLM/pretrain_gpt.py"", line 246, in  [rank0]:     pretrain( [rank0]:   File ""/share/home/wenjunyi/guokb/workspace/implemoe/MegatronLM/megatron/training/training.py"", line 270, in pretrain [rank0]:     model, optimizer, opt_param_scheduler = setup_model_and_optimizer( [rank0]:   File ""/share/home/wenjunyi/guokb/workspace/implemoe/MegatronLM/megatron/training/training.py"", line 581, in setup_model_and_optimizer [rank0]:     model = get_model(model_provider_func, model_type) [rank0]:   File ""/share/home/wenjunyi/guokb/workspace/implemoe/MegatronLM/megatron/training/training.py"", line 455, in get_model [rank0]:     model = model_provider_func( [rank0]:   File ""/share/home/wenjunyi/guokb/workspace/implemoe/MegatronLM/pretrain_gpt.py"", line 74, in model_provider [rank0]:     transformer_layer_spec = get_gpt_layer_with_transformer_engine_spec(args.num_experts, args.moe_grouped_gemm, args.qk_layernorm) [rank0]:   File ""/share/home/wenjunyi/guokb/workspace/implemoe/MegatronLM/megatron/core/models/gpt/gpt_layer_specs.py"", line 63, in get_gpt_layer_with_transformer_engine_spec [rank0]:     mlp = _get_mlp_module_spec( [rank0]:   File ""/share/home/wenjunyi/guokb/workspace/implemoe/MegatronLM/megatron/core/models/gpt/gpt_layer_specs.py"", line 154, in _get_mlp_module_spec [rank0]:     linear_fc1 = TEColumnParallelGroupedLinear [rank0]: NameError: name 'TEColumnParallelGroupedLinear' is not defined ``` I have read **MegatronLM/megatron/core/models/gpt/gpt_layer_specs.py** and find that _**HAVE_TE**_ has been set to False: ``` try:     from megatron.core.transformer.custom_layers.transformer_engine import (         TEColumnParallelGroupedLinear,         TEDotProductAttention,         TELayerNormColumnParallelLinear,         TENorm,         TERowParallelGroupedLinear,         TERowParallelLinear,     )     HAVE_TE = True except ImportError:     HAVE_TE = False ``` But it is useless because the code is judged by: `use_te = args.transformer_impl == ""transformer_engine""` or `if args.transformer_impl == ""transformer_engine"" then` I don't know how to set **_args.transformer_impl_** and what choices can be set. Or MegatronLM must be run with nvidia transformer_engine?",2024-08-29T07:03:56Z,,closed,0,12,https://github.com/NVIDIA/Megatron-LM/issues/1049,You can set a transformerimpl local flag There is a transformerimpl argument in megatron/training/arguments.py L707 (https://github.com/NVIDIA/MegatronLM/blob/b76a7d32f7af8cd6441f957bb4ba80afb0db61ba/megatron/training/arguments.pyL707),"> You can set a transformerimpl local flag >  > There is a transformerimpl argument in megatron/training/arguments.py L707 ( >  > MegatronLM/megatron/training/arguments.py >  > Line 707 in b76a7d3 >  >  group.add_argument('transformerimpl', default='transformer_engine',  > ) Thanks!  I know i can set default to '**local**' to achieve my goal. But is there 'standard' way to set the args? I have try this in **train_mixtral_8x7b_distributed.sh**: ``` ENGINE_ARGS=(     transformer_impl local \ ) torchrun ${DISTRIBUTED_ARGS[@]} /share/home/wenjunyi/guokb/workspace/implemoe/MegatronLM/pretrain_gpt.py \     ${MODEL_ARGS[@]} \     ${MOE_ARGS[@]} \     ${DATA_ARGS[@]} \     ${TRAINING_ARGS[@]} \     ${MODEL_PARALLEL_ARGS[@]} \     ${LOGGING_ARGS[@]} \     ${ENGINE_ARGS[@]} ``` but fail: `pretrain_gpt.py: error: unrecognized arguments: transformer_impl local`","For your arguments.py, is there the transformer_impl argument? ","> For your arguments.py, is there the transformer_impl argument? yes.","I have installed `transformer_engine`, but the same error.",Is Apex needed?,I think it's not necessary,"ok. In fact, after i set _**transformer_impl argument**_ to **_local_**, i meet another error: `[rank1]: AssertionError: (RMSNorm) is not supported in by torch Layernorm when instantiating WrappedTorchLayerNorm` So i am wondeing if i miss some needed packages.","You'd better use PyTorch NGC container as your environment (which contains te and apex and so on),   but I confirm that there's some bugs with megatron here.",ok. Thanks.,>  I confirm that there's some bugs with megatron here. agree.,"After i install apex, the error changes to: `[rank0]: AssertionError: (RMSNorm) is not supported in FusedLayerNorm when instantiating FusedLayerNorm`"
yzygitzh,Fix dataset helper compilation,"When using mock data in multinode run, current logic will only build dataset helpers in node 0, causing import errors in other nodes. This commit makes it compiled on all nodes.",2024-08-28T16:35:37Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1048,Marking as stale. No activity in 60 days.
YuvalAtir,[ENHANCEMENT] Training with variable sequence length,"I'd like to train llama3.1 model with variable sequence length as mentioned in Meta's paper. That is, to start the training with one sequence length and after a number of consumed tokens switch to a larger sequence length and continue training. Several approaches in my opinion: 1. Train on dataset A with sequence length A for some iterations and save checkpoint. Continue training from checkpoint with a dataset B which has sequence length B. At runtime seq_length will be fixed and can be taken from args. This will require minimal changes. 2. Use BlendedMegatronDataset which can be used to build a dataset from several datasets. Currently, according to BlendedMegatronDatasetConfig all datasets share same seq_len. This could be changed to make seq_len a list instead of an int. The high level dataset will have variable sized samples. Some samples will come from dataset A with seq_len A and some will have seq_len B from dataset B. In this approach seq_length will not be fixed. It could be taken from a global function that returns the seq_length based on the index of the sample or the length of the sample.  3. A low level dataset could be comprised of variable sequence lengths. It will include a vector with the changes in sequence length. In runtime the sequence length would be taken from sample. At runtime sequence length is needed in several functions. In runtime seq_length will not be fixed, as in previous approach. Any thoughts?",2024-08-28T15:36:13Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1047
1195343015,fix packaging import bug when using setuptools v70,"From `v70.0.0` to `v70.3.0` of `setuptools`, `packaging` can't be imported by  ```python from pkg_resources import packaging ```  It should be modified to: ```python from pkg_resources.extern import packaging ```  And this writing is also applicable in versions before `v70.0.0` The history of the commit associated with it in `setuptools`",2024-08-28T08:04:04Z,,closed,3,1,https://github.com/NVIDIA/Megatron-LM/issues/1045,We have added `packaging` as a dependency of MegatronLM.
githwd2016,[ENHANCEMENT],"In the method `_get_param_groups ` from `megatron/core/optimizer/__init__.py `, when set `wd_mult `and `lr_mult`, the code is: ``` if not no_wd and not scale_lr:     wd_mult, lr_mult = 1.0, 1.0 elif not no_wd and scale_lr:     wd_mult, lr_mult = 1.0, lr_mult elif no_wd and not scale_lr:     wd_mult, lr_mult = 0.0, 1.0 else:     wd_mult, lr_mult = 0.0, lr_mult ``` I think the following code is more concise and clearer. Could I make a PR? ``` wd_mult = 1.0 if not no_wd else 0.0 lr_mult = 1.0 if not scale_lr else lr_mult ```",2024-08-28T02:18:47Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1044
KookHoiKim,"[QUESTION] LLaVA model_type, pipeline parallel training","I'm trying to apply pipeline parallelism in training LLaVA. (TP1, PP2) Although I followed the instruction , the code is not working. (TP2 PP1 working) And i found there is some weird points in the code.  * LLaVA is basically decoderonly model.  In my understanding, vision encoder / vision projector is additional embedding part, which is only used in pre_process part.  However, LLaVA model is initialized as `encoder_and_decoder` model_type. Why not `encoder_or_decoder`? Furthermore, while pp communication, the recv, send tensor shape is set as `(num_image_token, B, hidden_size)`.  It seems shards gives/takes vision embedding, not the intermediate states from middle of language model.  P.S. Currently, i do not use encoder_pipeline_model_parallel_size / tensor parallel size because it occurs errors while initializing megatron that is not divisible `world_size % total_model_size` .  So i forced `vision_config.pipeline_model_parallel_size` to be **1**. I am not familiar with megatron code, and really hope that get some help with llava training.  Thank you. ",2024-08-28T00:58:52Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1043
benoriol,[QUESTION] Llava VQA performance,"Hello, What is the expected VQAv2 accuracy of the instructionfinetuned model?  Should it reach ~80 as reported by LLaVa1.5? After training for a few thousand steps I get under 70. Not sure if it is just a matter of training for longer or there is any other issue with the training recipe. Also it seems the LR is 1e6 vs the 2e5 reported in LlaVa hyperparameters. Is this intentional? What scores did you get with this recipe? Thanks, Benet Probably for barker or  ?",2024-08-28T00:03:23Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1042
clarence-lee-sheng,[BUG] TypeError: get_cpu_offload_context() takes from 0 to 4 positional arguments but 5 were given,"**Describe the bug** in megatron/core/transformer/custom_layers/transformer_engine.py L897, there is a conditional checking if the te version is greater than 1.8.0. If it is greater, then pass in 5 arguments to get_cpu_offload_context, however, on inspection of both transformer_engine 1.8 and 1.9, they both only take in 4 arguments, furthermore, the te_version in the latext nvidia image, nvcr.io/nvidia/pytorch:24.07py3, has transformer_engine version '1.8.0+37280ec', which is evaluated as true when _te_version is evaluated as greater than packaging.version.Version('1.8.0')), this causes the error TypeError: get_cpu_offload_context() takes from 0 to 4 positional arguments but 5 were given",2024-08-27T10:04:28Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/1041,PR CC(fix _te_version issue in transformer_engine.py get_cpu_offload_context()) ,Thank you for bringing it to my attention. My apologies for not noticing that it had been posted earlier.,You're welcome!
wplf,[QUESTION] THD format with CP,**Your question** Ask a clear and concise question about MegatronLM. Thank you for great works!  I'd like to know if you have a Unit test about thd format with cp?,2024-08-27T08:39:39Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1040,"Yes, we have the test in TE https://github.com/NVIDIA/TransformerEngine/blob/4ddb0a7bea787294282d0fe0715adf5ea4a39779/tests/pytorch/fused_attn/test_fused_attn_with_cp.pyL47.","Yesï¼Œthank you very much. I've seen that before. However,  I don't see CP and thd support in MegatronLM, is that right?"
laze44,[QUESTION] Possible communication reduction in MoE training,"Hi Megatron team: During MoE training, there might be opportunities to combine communication operators when simultaneously using sequence and expert parallelism, though I'm uncertain if this hypothesis is accurate. In the original sequence parallelism + expert parallelism process, assuming sp=ep=4, the activations undergo an allgather phase after dropout, followed by gating operations for all sequences, which are then selectively routed to different GPUs' experts. However, if I move the gating function before the allgather phase, each GPU would perform the gating operations on its sequences, followed by an alltoall communication based on the gating results. The rationale behind this approach is that, unlike FFN tensor parallelism, during the MoE forward process, each GPU only needs to handle a subset of sequences. Theoretically, this could eliminate the need for the allgather phase. ï¼ˆAs shown in Figure below) I want to understand whether this approach is correct, as the MoE training process typically incorporates some loadbalancingrelated loss functions. Does altering the order affect the backward process? Moreover, if this approach is correct, does MegatronLM support this communication concept? !Image",2024-08-27T08:04:20Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1039
1195343015,[BUG] `run_simple_mcore_train_loop.py` bugs when moditied `tensor_model_parallel_size` from `2` to `1`,"**Describe the bug** https://github.com/NVIDIA/MegatronLM/blob/01ca03f11e89f4f85682dcac647c2b913b25fcee/examples/run_simple_mcore_train_loop.pyL118 When I moditied `tensor_model_parallel_size` in `run_simple_mcore_train_loop.py` from `2` to `1`, some bugs happened. **Stack trace/logs** ```python /opt/pytorch/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1284: indexSelectLargeIndex: block: [37,0,0], thread: [92,0,0] Assertion `srcIndex  [rank0]:     losses_reduced = forward_backward_func( [rank0]:   File ""/workspace/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1379, in forward_backward_pipelining_without_interleaving [rank0]:     output_tensor, num_tokens = forward_step( [rank0]:   File ""/workspace/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 259, in forward_step [rank0]:     output_tensor, loss_func = forward_step_func(data_iterator, model) [rank0]:   File ""/workspace/MegatronLM/examples/run_simple_mcore_train_loop.py"", line 107, in forward_step_func [rank0]:     output_tensor = model(tokens, position_ids, attention_mask, labels=labels) [rank0]:   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1552, in _wrapped_call_impl [rank0]:     return self._call_impl(*args, **kwargs) [rank0]:   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1561, in _call_impl [rank0]:     return forward_call(*args, **kwargs) [rank0]:   File ""/workspace/MegatronLM/megatron/core/models/gpt/gpt_model.py"", line 186, in forward [rank0]:     decoder_input = self.embedding(input_ids=input_ids, position_ids=position_ids) [rank0]:   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1552, in _wrapped_call_impl [rank0]:     return self._call_impl(*args, **kwargs) [rank0]:   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1561, in _call_impl [rank0]:     return forward_call(*args, **kwargs) [rank0]:   File ""/workspace/MegatronLM/megatron/core/models/common/embeddings/language_model_embedding.py"", line 103, in forward [rank0]:     embeddings = word_embeddings + position_embeddings [rank0]: RuntimeError: CUDA error: deviceside assert triggered [rank0]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect. [rank0]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1 [rank0]: Compile with `TORCH_USE_CUDA_DSA` to enable deviceside assertions. ```  **Environment (please complete the following information):**   MegatronLM commit ID ef85bc94fc744aa5d398d12140f808023afbf78d   PyTorch nvcr.io/nvidia/pytorch:24.07py3",2024-08-27T01:04:10Z,stale,open,0,4,https://github.com/NVIDIA/Megatron-LM/issues/1038,Do you have any solution? I got the same error.,"> Do you have any solution? I got the same error. I think this bug is due to inappropriate default `max_sequence_length` in `MockGPTLowLevelDataset`, which is used to generate mockdataset. https://github.com/NVIDIA/MegatronLM/blob/732a689606810c02d0dc260a163c9ebac099c044/megatron/core/datasets/gpt_dataset.pyL693L697 The default `max_sequence_length` is `4096`. You can modify it to `64`, which makes it the same as `run_simple_mcore_train_loop.py` https://github.com/NVIDIA/MegatronLM/blob/732a689606810c02d0dc260a163c9ebac099c044/examples/run_simple_mcore_train_loop.pyL21 Hope it could be helpful for you.", Thanks a lot. It works for me. I hope this can be fixed more robustly.,Marking as stale. No activity in 60 days.
MekkCyber,Fixing doc link,,2024-08-26T19:58:34Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1037,I'll include your commit in my internal fix PR.,Your commit is squash merged in https://github.com/NVIDIA/MegatronLM/commit/2488e20d488b35259f6eb53267e8a0525a373599 and your author information is preserved.  Thanks for your contribution. You can close this PR now. 
abgoswam,WIP: llama3 trials,,2024-08-26T17:45:20Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1036
felipeliliti,DOCUMENTAÃ‡ÃƒO AURORAX ," DocumentaÃ§Ã£o de Aurorax  1. IntroduÃ§Ã£o **Aurorax** Ã© uma linguagem de programaÃ§Ã£o projetada para criar soluÃ§Ãµes avanÃ§adas em inteligÃªncia artificial (IA), oferecendo suporte para multimodalidade e integraÃ§Ã£o de diversos tipos de dados. Ela Ã© ideal para desenvolver sistemas complexos e escalÃ¡veis.  2. InstalaÃ§Ã£o  2.1. Requisitos  Sistema operacional: Windows, macOS, ou Linux  DependÃªncias: [Liste aqui quaisquer dependÃªncias necessÃ¡rias]  2.2. Passos de InstalaÃ§Ã£o 1. **Download**:     Acesse o [site oficial]() ou o [repositÃ³rio GitHub]() para baixar o pacote de instalaÃ§Ã£o. 2. **ConfiguraÃ§Ã£o**:     Descompacte o arquivo baixado.     Navegue atÃ© o diretÃ³rio descompactado e execute o script de instalaÃ§Ã£o:       Windows: `setup.exe`       macOS/Linux: `./setup.sh` 3. **VerificaÃ§Ã£o**:     Abra um terminal e digite o comando:      ```      aurorax version      ```     Verifique se a versÃ£o da Aurorax Ã© exibida corretamente.  3. Estrutura da Linguagem  3.1. Sintaxe BÃ¡sica Aurorax utiliza uma sintaxe intuitiva para facilitar a programaÃ§Ã£o. Aqui estÃ¡ um exemplo bÃ¡sico: ```aurorax // ComentÃ¡rio de uma linha /*    ComentÃ¡rio de mÃºltiplas linhas */ funÃ§Ã£o exemplo() {     imprimir(""OlÃ¡, Mundo!""); } ```  3.2. Tipos de Dados Aurorax suporta os seguintes tipos de dados:  **Inteiros**: `int`  **Ponto Flutuante**: `float`  **Texto**: `string`  **Booleano**: `bool`  **Lista**: `list`  **DicionÃ¡rio**: `dict`  3.3. Estruturas de Controle  **Condicional**:   ```aurorax   se (condiÃ§Ã£o) {       // CÃ³digo a ser executado se a condiÃ§Ã£o for verdadeira   } senÃ£o {       // CÃ³digo a ser executado se a condiÃ§Ã£o for falsa   }   ```  **Loop**:   ```aurorax   para (variÃ¡vel em lista) {       // CÃ³digo a ser executado em cada iteraÃ§Ã£o   }   ```  4. Multimodalidade  4.1. Processamento de Imagens ```aurorax imagem = carregar_imagem(""caminho/para/imagem.jpg"") resultado = processar_imagem(imagem, ""algoritmo"") ```  4.2. Processamento de Ãudio ```aurorax audio = carregar_audio(""caminho/para/audio.mp3"") resultado = analisar_audio(audio, ""algoritmo"") ```  4.3. Processamento de VÃ­deo ```aurorax video = carregar_video(""caminho/para/video.mp4"") resultado = extrair_dados_video(video, ""algoritmo"") ```  5. Funcionalidades AvanÃ§adas  5.1. IntegraÃ§Ã£o com Outras Linguagens Aurorax permite a integraÃ§Ã£o com outras linguagens de programaÃ§Ã£o usando APIs. Veja um exemplo de integraÃ§Ã£o com Python: ```aurorax importar_python(""caminho/para/script.py"") resultado = chamar_funcao_python(""nome_da_funcao"", argumento) ```  5.2. SimulaÃ§Ã£o de Entes Queridos A funcionalidade de ""modo clone"" permite criar simulaÃ§Ãµes baseadas em dados. Exemplo: ```aurorax clone = criar_clone(""nome_da_pessoa"", ""caminho/para/dados"") clone.exibir_relembranÃ§as() ```  6. Exemplos de Projetos  6.1. Projeto de AnÃ¡lise de Dados ```aurorax dados = carregar_dados(""caminho/para/dados.csv"") anÃ¡lise = realizar_anÃ¡lise(dados, ""algoritmo"") imprimir(anÃ¡lise) ```  6.2. Projeto de Reconhecimento de Imagens ```aurorax imagem = carregar_imagem(""caminho/para/imagem.jpg"") resultados = reconhecer_imagens(imagem, ""modelo"") imprimir(resultados) ```  7. Recursos e Suporte  7.1. DocumentaÃ§Ã£o Adicional Para mais informaÃ§Ãµes, consulte a [documentaÃ§Ã£o completa]() e os tutoriais disponÃ­veis no [site oficial]().  7.2. Comunidade Participe da comunidade de desenvolvedores Aurorax no [fÃ³rum]() ou no [grupo de discussÃ£o]().  7.3. Suporte TÃ©cnico Para suporte tÃ©cnico, entre em contato com nossa equipe atravÃ©s do [email de suporte](). ",2024-08-26T17:38:10Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1035
felipeliliti,MANUAL DE USO AURORAX ," Manual de Uso  Aurorax  IntroduÃ§Ã£o Aurorax Ã© uma linguagem de programaÃ§Ã£o avanÃ§ada projetada para o desenvolvimento de sistemas complexos de inteligÃªncia artificial. Este manual de uso fornece orientaÃ§Ãµes sobre como comeÃ§ar a trabalhar com Aurorax, incluindo exemplos bÃ¡sicos e avanÃ§ados.  1. ConfiguraÃ§Ã£o Inicial  1.1. InstalaÃ§Ã£o 1. **Baixe o Pacote**:     Acesse o [site oficial]() ou o [repositÃ³rio GitHub]() e baixe o pacote de instalaÃ§Ã£o. 2. **Instale a Linguagem**:     Descompacte o arquivo baixado e execute o script de instalaÃ§Ã£o:       **Windows**: `setup.exe`       **macOS/Linux**: `./setup.sh` 3. **Verifique a InstalaÃ§Ã£o**:     Abra um terminal e digite:      ```      aurorax version      ```     Verifique se a versÃ£o da Aurorax Ã© exibida corretamente.  1.2. ConfiguraÃ§Ã£o do Ambiente Certifiquese de que o ambiente de desenvolvimento estÃ¡ configurado corretamente:  **Editor de CÃ³digo**: Utilize um editor de cÃ³digo compatÃ­vel, como VSCode ou Atom.  **VariÃ¡veis de Ambiente**: Adicione o diretÃ³rio de instalaÃ§Ã£o ao PATH do sistema, se necessÃ¡rio.  2. Estrutura BÃ¡sica  2.1. Criando um Arquivo Crie um novo arquivo com a extensÃ£o `.aur` para escrever seu cÃ³digo Aurorax.  2.2. Escrevendo o Primeiro Programa ```aurorax // Este Ã© um programa simples em Aurorax funÃ§Ã£o saudacao(nome) {     imprimir(""OlÃ¡, "" + nome + ""!""); } saudacao(""Mundo""); ```  2.3. Executando o Programa No terminal, navegue atÃ© o diretÃ³rio onde o arquivo `.aur` estÃ¡ localizado e execute: ``` aurorax nome_do_arquivo.aur ```  3. Recursos e Funcionalidades  3.1. Tipos de Dados Aurorax suporta os seguintes tipos de dados:  **Inteiros**: `int`  **Ponto Flutuante**: `float`  **Texto**: `string`  **Booleano**: `bool`  **Lista**: `list`  **DicionÃ¡rio**: `dict` **Exemplo**: ```aurorax idade = 30 nome = ""Felipe"" lista = [1, 2, 3, 4] dicionario = {""chave"": ""valor""} ```  3.2. Estruturas de Controle  **Condicional**:   ```aurorax   se (idade > 18) {       imprimir(""Maior de idade"");   } senÃ£o {       imprimir(""Menor de idade"");   }   ```  **Loop**:   ```aurorax   para (i em lista) {       imprimir(i);   }   ```  3.3. FunÃ§Ãµes Defina e utilize funÃ§Ãµes para organizar o cÃ³digo. **Exemplo**: ```aurorax funÃ§Ã£o soma(a, b) {     retornar a + b; } resultado = soma(5, 3); imprimir(resultado); ```  4. Multimodalidade  4.1. Processamento de Imagens ```aurorax imagem = carregar_imagem(""caminho/para/imagem.jpg"") resultado = processar_imagem(imagem, ""algoritmo"") ```  4.2. Processamento de Ãudio ```aurorax audio = carregar_audio(""caminho/para/audio.mp3"") resultado = analisar_audio(audio, ""algoritmo"") ```  4.3. Processamento de VÃ­deo ```aurorax video = carregar_video(""caminho/para/video.mp4"") resultado = extrair_dados_video(video, ""algoritmo"") ```  5. Funcionalidades AvanÃ§adas  5.1. IntegraÃ§Ã£o com Outras Linguagens Para integrar com Python: ```aurorax importar_python(""caminho/para/script.py"") resultado = chamar_funcao_python(""nome_da_funcao"", argumento) ```  5.2. SimulaÃ§Ã£o de Entes Queridos Para criar um clone: ```aurorax clone = criar_clone(""nome_da_pessoa"", ""caminho/para/dados"") clone.exibir_relembranÃ§as() ```  6. Exemplos de Projetos  6.1. AnÃ¡lise de Dados ```aurorax dados = carregar_dados(""caminho/para/dados.csv"") anÃ¡lise = realizar_anÃ¡lise(dados, ""algoritmo"") imprimir(anÃ¡lise) ```  6.2. Reconhecimento de Imagens ```aurorax imagem = carregar_imagem(""caminho/para/imagem.jpg"") resultados = reconhecer_imagens(imagem, ""modelo"") imprimir(resultados) ```  7. Recursos e Suporte  7.1. DocumentaÃ§Ã£o Adicional Para mais detalhes, consulte a [documentaÃ§Ã£o completa]() e os tutoriais no [site oficial]().  7.2. Comunidade Participe da comunidade Aurorax no [fÃ³rum]() ou no [grupo de discussÃ£o]().  7.3. Suporte TÃ©cnico Para assistÃªncia tÃ©cnica, entre em contato com nossa equipe pelo [email de suporte]().",2024-08-26T17:37:07Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1034
felipeliliti,AURORA I. A HUMANOIDE ðŸ‡§ðŸ‡·,"```aurorax  Aurora 1 Teste  Sistema de IA Multimodal com Modo Clone module Aurora1Test {     import Aurorax.NLP.Core;     import Aurorax.ML.Engine;     import Aurorax.Emotion.Recognition;     import Aurorax.Cognitive.Sim;     import Aurorax.Multimodal.Process;     import Aurorax.Clone.Mode;     class Aurora {         method initialize() {             NLP = new NLP_Core();             ML = new ML_Engine();             EmotionRecog = new Emotion_Recognition();             Cognitive = new Cognitive_Sim();             Multimodal = new Multimodal_Process();             CloneMode = new Clone_Mode();         }          Processamento Multimodal: Responde com base em texto, imagens, sons e vÃ­deos         method multimodal_response(input Text, image Image, audio Audio, video Video) > Text {             emotion = EmotionRecog.analyze(input, audio);             context = NLP.process(input);             visual_data = Multimodal.analyze_image(image);             audio_data = Multimodal.analyze_audio(audio);             video_data = Multimodal.analyze_video(video);             combined_context = Multimodal.fuse_data(context, visual_data, audio_data, video_data);             response = Cognitive.generate_response(combined_context, emotion);             return response;         }          Modo Clone: Cria uma simulaÃ§Ã£o de uma pessoa com base em dados multimodais         method create_clone(name Text, image Image, audio Audio, video Video) > CloneProfile {             clone_profile = CloneMode.generate_clone(name, image, audio, video);             return clone_profile;         }          InteraÃ§Ã£o com o Clone         method interact_with_clone(clone CloneProfile, input Text) > Text {             clone_response = CloneMode.simulate_interaction(clone, input);             return clone_response;         }     } }  Aurora Multimodal Processing module Multimodal_Process {     class Multimodal {         method analyze_image(image Image) > VisualData {              Processa a imagem e extrai informaÃ§Ãµes relevantes             visual_data = ML.extract_visual_features(image);             return visual_data;         }         method analyze_audio(audio Audio) > AudioData {              Processa o Ã¡udio e extrai informaÃ§Ãµes emocionais e contextuais             audio_data = ML.extract_audio_features(audio);             return audio_data;         }         method analyze_video(video Video) > VideoData {              Processa o vÃ­deo e extrai informaÃ§Ãµes tanto visuais quanto auditivas             video_data = ML.extract_video_features(video);             return video_data;         }         method fuse_data(context TextData, visual VisualData, audio AudioData, video VideoData) > CombinedData {              Combina os dados multimodais em um Ãºnico contexto para anÃ¡lise             combined_data = ML.fuse(context, visual, audio, video);             return combined_data;         }     } }  Aurora Clone Mode module Clone_Mode {     class CloneProfile {         property name Text;         property memory DataSet;         property personality ML_Model;         method initialize(name Text, memory DataSet, personality ML_Model) {             self.name = name;             self.memory = memory;             self.personality = personality;         }     }     class CloneMode {         method generate_clone(name Text, image Image, audio Audio, video Video) > CloneProfile {              Gera um clone baseado em dados multimodais coletados             memory_data = Multimodal_Process.Multimodal.fuse_data(image, audio, video);             personality_model = ML.train_personality(memory_data);             clone_profile = new CloneProfile(name, memory_data, personality_model);             return clone_profile;         }         method simulate_interaction(clone CloneProfile, input Text) > Text {              Simula uma interaÃ§Ã£o com o clone baseado nas memÃ³rias e personalidade treinada             response = Cognitive.simulate_thought_process(clone.personality, input);             return response;         }     } }  Initialization AuroraApp = new Aurora1Test.Aurora(); AuroraApp.initialize(); ```  **ExplicaÃ§Ã£o das Funcionalidades:**  **Multimodalidade:** O sistema de IA Aurora 1 Teste Ã© capaz de processar dados multimodais (texto, imagem, Ã¡udio, vÃ­deo) e responder de forma integrada, considerando todas essas fontes de informaÃ§Ã£o.  **Modo Clone:** Essa funcionalidade permite criar uma simulaÃ§Ã£o de uma pessoa, baseada em dados coletados (imagens, sons, vÃ­deos). O clone pode interagir com o usuÃ¡rio, simulando uma personalidade e memÃ³rias que refletem a pessoa em questÃ£o.",2024-08-26T05:26:11Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1033
haolin-nju,"[BUG] When the model has extra layers, initializing the model from dist-ckpt results in an error","**Describe the bug** I'm trying to use the Llama2 model saved with `usedistckpt` after SFT (Supervised FineTuning) to train a reward model. The reward model does not require the original checkpoint's `output_layer`, but instead needs an additional linearpooler layer. If I use `usedistckpt` to load the original SFT checkpoint, it throws the following error: ```shell [rank0]: Traceback (most recent call last): ...[Deleted for privacy concerns] [rank0]:   File ""/mnt/data/someone/MegatronLMbenchmark/megatron/training/training.py"", line 234, in pretrain [rank0]:     model, optimizer, opt_param_scheduler = setup_model_and_optimizer( [rank0]:   File ""/mnt/data/someone/MegatronLMbenchmark/megatron/training/training.py"", line 553, in setup_model_and_optimizer [rank0]:     args.iteration, args.num_floating_point_operations_so_far = load_checkpoint( [rank0]:   File ""/mnt/data/someone/MegatronLMbenchmark/megatron/training/checkpointing.py"", line 790, in load_checkpoint [rank0]:     state_dict, checkpoint_name, release = _load_base_checkpoint(load_dir, rank0=False, **load_kwargs) [rank0]:   File ""/mnt/data/someone/MegatronLMbenchmark/megatron/training/checkpointing.py"", line 601, in _load_base_checkpoint [rank0]:     state_dict = dist_checkpointing.load(sharded_state_dict, checkpoint_name, load_strategy, strict=args.dist_ckpt_strictness) [rank0]:   File ""/mnt/data/someone/MegatronLMbenchmark/megatron/core/dist_checkpointing/serialization.py"", line 160, in load [rank0]:     loaded_state_dict = sharded_strategy.load(sharded_state_dict, checkpoint_dir) [rank0]:   File ""/mnt/data/someone/MegatronLMbenchmark/megatron/core/dist_checkpointing/strategies/torch.py"", line 726, in load [rank0]:     checkpoint.load_state_dict( [rank0]:   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/checkpoint/state_dict_loader.py"", line 36, in load_state_dict [rank0]:     return _load_state_dict( [rank0]:   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/checkpoint/state_dict_loader.py"", line 216, in _load_state_dict [rank0]:     central_plan: LoadPlan = distW.reduce_scatter(""plan"", local_step, global_step) [rank0]:   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/checkpoint/utils.py"", line 190, in reduce_scatter [rank0]:     raise result [rank0]: torch.distributed.checkpoint.api.CheckpointException: CheckpointException ranks:dict_keys([0, 1, 2, 3, 4, 5, 6, 7]) [rank0]: Traceback (most recent call last): (RANK 0) [rank0]:   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/checkpoint/utils.py"", line 163, in reduce_scatter [rank0]:     local_data = map_fun() [rank0]:   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/checkpoint/logger.py"", line 64, in wrapper [rank0]:     result = func(*args, **kwargs) [rank0]:   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/checkpoint/state_dict_loader.py"", line 205, in local_step [rank0]:     local_plan = planner.create_local_plan() [rank0]:   File ""/mnt/data/someoneMegatronLMbenchmark/megatron/core/dist_checkpointing/strategies/torch.py"", line 506, in create_local_plan [rank0]:     self._validate_global_shapes(self.metadata, self.shapes_validation_sharded_tensors) [rank0]:   File ""/mnt/data/someone/MegatronLMbenchmark/megatron/core/dist_checkpointing/strategies/torch.py"", line 492, in _validate_global_shapes [rank0]:     loaded_shape = metadata.state_dict_metadata[sh_ten.key].size [rank0]: KeyError: 'pooler_head.dense1.weight' ... [rank7]: Traceback (most recent call last): (RANK 6) [rank7]:   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/checkpoint/utils.py"", line 163, in reduce_scatter [rank7]:     local_data = map_fun() [rank7]:   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/checkpoint/logger.py"", line 64, in wrapper [rank7]:     result = func(*args, **kwargs) [rank7]:   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/checkpoint/state_dict_loader.py"", line 205, in local_step [rank7]:     local_plan = planner.create_local_plan() [rank7]:   File ""/mnt/data/someone/MegatronLMbenchmark/megatron/core/dist_checkpointing/strategies/torch.py"", line 506, in create_local_plan [rank7]:     self._validate_global_shapes(self.metadata, self.shapes_validation_sharded_tensors) [rank7]:   File ""/mnt/data/someone/MegatronLMbenchmark/megatron/core/dist_checkpointing/strategies/torch.py"", line 492, in _validate_global_shapes [rank7]:     loaded_shape = metadata.state_dict_metadata[sh_ten.key].size [rank7]: KeyError: 'pooler_head.dense1.weight' [rank7]: Traceback (most recent call last): (RANK 7) [rank7]:   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/checkpoint/utils.py"", line 163, in reduce_scatter [rank7]:     local_data = map_fun() [rank7]:   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/checkpoint/logger.py"", line 64, in wrapper [rank7]:     result = func(*args, **kwargs) [rank7]:   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/checkpoint/state_dict_loader.py"", line 205, in local_step [rank7]:     local_plan = planner.create_local_plan() [rank7]:   File ""/mnt/data/someone/MegatronLMbenchmark/megatron/core/dist_checkpointing/strategies/torch.py"", line 506, in create_local_plan [rank7]:     self._validate_global_shapes(self.metadata, self.shapes_validation_sharded_tensors) [rank7]:   File ""/mnt/data/someone/MegatronLMbenchmark/megatron/core/dist_checkpointing/strategies/torch.py"", line 492, in _validate_global_shapes [rank7]:     loaded_shape = metadata.state_dict_metadata[sh_ten.key].size [rank7]: KeyError: 'pooler_head.dense1.weight' ``` It seems that the loader cannot find `sh_ten.key`, which is `pooler_head.dense1.weight`. Disabling `usedistckpt` and selfdefine a `load_state_dict` function (to suppress the error) works for me. However, the error cannot be suppressed in the userside when enabling `usedistckpt`. Such inconsistency might result in confusion. **To Reproduce** 1. Train a Llama2 7B SFT checkpoint (e.g., for 10 iterations) and save with `usedistckpt` in `Llama27btp8pp1sftcore080distckpt` 2. Define a Reward model       Set Llama2 7B as its backbone network.       Add a LinearPooler layer as follows and forward the LinearPooler layer but not original `output_layer` in `forward` function 3. Load the checkpoint from `Llama27btp8pp1sftcore080distckpt`, train the Llama2 7B Reward model, and you will reproduce the error. ```python from megatron.core.transformer.module import MegatronModule from megatron.core.transformer.utils import get_linear_layer class LinearPooler(MegatronModule):     def __init__(self, config, score_dimensions):         super().__init__(config=config)         args = get_args()         hidden_size = config.hidden_size         init_method = config.init_method         self.dense1 = get_linear_layer(hidden_size, hidden_size, init_method, args.perform_initialization)         self.dense2 = get_linear_layer(hidden_size, score_dimensions, init_method, args.perform_initialization)         self.sequence_parallel = args.sequence_parallel     def forward(self, hidden_states, sequence_indices=None):         if self.sequence_parallel:             hidden_states = tensor_parallel.gather_from_sequence_parallel_region(                 hidden_states,   [s, b, h]                 tensor_parallel_output_grad=False)         if sequence_indices is not None:             selected_hidden = torch.index_select(hidden_states, 0, sequence_indices)             selected_hidden = selected_hidden.diagonal(dim1=0, dim2=1).T             pooled = self.dense2(torch.nn.functional.relu(self.dense1(selected_hidden)))         else:             selected_hidden = hidden_states   [s, b, h]             pooled = self.dense2(torch.nn.functional.relu(self.dense1(selected_hidden))).squeeze(2)   [s, b, scoredim]         return pooled ``` > Note: You could mock the data. **Expected behavior** Successfully load dist checkpoint **Stack trace/logs** ```shell  arguments    accumulate_allreduce_grads_in_fp32 .............. True   adam_beta1 ...................................... 0.9   adam_beta2 ...................................... 0.95   adam_eps ........................................ 1e08   add_bias_linear ................................. False   add_position_embedding .......................... False   add_qkv_bias .................................... False   adlr_autoresume ................................. False   adlr_autoresume_interval ........................ 1000   app_tag_run_name ................................ None   app_tag_run_version ............................. 0.0.0   apply_layernorm_1p .............................. False   apply_query_key_layer_scaling ................... False   apply_residual_connection_post_layernorm ........ False   apply_rope_fusion ............................... True   async_save ...................................... None   async_tensor_model_parallel_allreduce ........... False   attention_dropout ............................... 0.1   attention_softmax_in_fp32 ....................... True   auto_detect_ckpt_format ......................... True   barrier_with_L1_time ............................ True   bert_binary_head ................................ True   bert_embedder_type .............................. megatron   bert_load ....................................... None   bf16 ............................................ True   bias_dropout_fusion ............................. True   bias_gelu_fusion ................................ False   bias_swiglu_fusion .............................. True   biencoder_projection_dim ........................ 0   biencoder_shared_query_context_model ............ False   block_data_path ................................. None   calculate_per_token_loss ........................ False   check_for_nan_in_loss_and_grad .................. True   check_weight_hash_across_dp_replicas_interval ... None   ckpt_assume_constant_structure .................. False   ckpt_fully_parallel_load ........................ False   ckpt_fully_parallel_save ........................ True   ckpt_fully_parallel_save_deprecated ............. False   ckpt_step ....................................... None   classes_fraction ................................ 1.0   clip_grad ....................................... 1.0   clone_scatter_output_in_embedding ............... True   consumed_train_samples .......................... 0   consumed_valid_samples .......................... 0   context_parallel_size ........................... 1   create_attention_mask_in_dataloader ............. True   cross_entropy_loss_fusion ....................... False   data_cache_path ................................. None   data_parallel_random_init ....................... False   data_parallel_size .............................. 1   data_path ....................................... ['/mnt/data/someone/data/reward/train.jsonl', '/mnt/data/someone/data/reward/dev.jsonl', '/mnt/data/someone/data/reward/dev.jsonl']   data_per_class_fraction ......................... 1.0   data_sharding ................................... True   dataloader_type ................................. cyclic   ddp_average_in_collective ....................... False   ddp_bucket_size ................................. None   decoder_num_layers .............................. None   decoder_seq_length .............................. None   decoupled_lr .................................... None   decoupled_min_lr ................................ None   defer_embedding_wgrad_compute ................... False   delay_grad_reduce ............................... True   delay_param_gather .............................. False   deprecated_use_mcore_models ..................... False   deterministic_mode .............................. False   dino_bottleneck_size ............................ 256   dino_freeze_last_layer .......................... 1   dino_head_hidden_size ........................... 2048   dino_local_crops_number ......................... 10   dino_local_img_size ............................. 96   dino_norm_last_layer ............................ False   dino_teacher_temp ............................... 0.07   dino_warmup_teacher_temp ........................ 0.04   dino_warmup_teacher_temp_epochs ................. 30   disable_straggler_on_startup .................... False   dist_ckpt_format ................................ torch_dist   dist_ckpt_strictness ............................ assume_ok_unexpected   distribute_saved_activations .................... False   distributed_backend ............................. nccl   distributed_timeout_minutes ..................... 10   embedding_path .................................. None   empty_unused_memory_level ....................... 0   enable_one_logger ............................... True   encoder_num_layers .............................. 32   encoder_seq_length .............................. 2048   end_weight_decay ................................ 0.1   eod_mask_loss ................................... False   eval_interval ................................... 1000   eval_iters ...................................... 20   evidence_data_path .............................. None   exit_duration_in_mins ........................... None   exit_interval ................................... None   exit_on_missing_checkpoint ...................... True   exit_signal_handler ............................. False   expert_model_parallel_size ...................... 1   ffn_hidden_size ................................. 11008   finetune ........................................ True   fp16 ............................................ False   fp16_lm_cross_entropy ........................... False   fp32_residual_connection ........................ False   fp8 ............................................. None   fp8_amax_compute_algo ........................... most_recent   fp8_amax_history_len ............................ 1   fp8_interval .................................... 1   fp8_margin ...................................... 0   fp8_wgrad ....................................... True   global_batch_size ............................... 64   gradient_accumulation_fusion .................... True   group_query_attention ........................... True   head_lr_mult .................................... 1.0   hidden_dropout .................................. 0.1   hidden_size ..................................... 4096   hybrid_attention_ratio .......................... 0.0   hybrid_mlp_ratio ................................ 0.0   hybrid_override_pattern ......................... None   hysteresis ...................................... 2   ict_head_size ................................... None   ict_load ........................................ None   img_h ........................................... 224   img_w ........................................... 224   indexer_batch_size .............................. 128   indexer_log_interval ............................ 1000   inference_batch_times_seqlen_threshold .......... 512   init_method_std ................................. 0.006   init_method_xavier_uniform ...................... False   initial_loss_scale .............................. 4294967296   iter_per_epoch .................................. 1250   iteration ....................................... 10   kv_channels ..................................... 128   lazy_mpu_init ................................... None   load ............................................ /mnt/data/someone/checkpoint/mcore/Llama27btp8pp1sftcore080distckpt   local_rank ...................................... None   log_batch_size_to_tensorboard ................... True   log_interval .................................... 1   log_learning_rate_to_tensorboard ................ True   log_loss_scale_to_tensorboard ................... True   log_memory_to_tensorboard ....................... False   log_num_zeros_in_grad ........................... False   log_params_norm ................................. False   log_progress .................................... False   log_straggler ................................... False   log_throughput .................................. False   log_timers_to_tensorboard ....................... True   log_validation_ppl_to_tensorboard ............... True   log_world_size_to_tensorboard ................... False   logging_level ................................... None   loss_scale ...................................... None   loss_scale_window ............................... 1000   lr .............................................. 1e05   lr_decay_iters .................................. 1000   lr_decay_samples ................................ None   lr_decay_style .................................. cosine   lr_warmup_fraction .............................. None   lr_warmup_init .................................. 0.0   lr_warmup_iters ................................. 300   lr_warmup_samples ............................... 0   lr_wsd_decay_iters .............................. None   lr_wsd_decay_samples ............................ None   lr_wsd_decay_style .............................. exponential   make_vocab_size_divisible_by .................... 128   manual_gc ....................................... False   manual_gc_eval .................................. True   manual_gc_interval .............................. 0   mask_factor ..................................... 1.0   mask_prob ....................................... 0.15   mask_type ....................................... random   masked_softmax_fusion ........................... False   max_position_embeddings ......................... 4096   max_response .................................... 2   max_tokens_to_oom ............................... 12000   merge_file ...................................... None   micro_batch_size ................................ 4   min_loss_scale .................................. 1.0   min_lr .......................................... 6e12   mmap_bin_files .................................. True   mock_data ....................................... False   moe_aux_loss_coeff .............................. 0.0   moe_expert_capacity_factor ...................... None   moe_extended_tp ................................. False   moe_grouped_gemm ................................ False   moe_input_jitter_eps ............................ None   moe_layer_recompute ............................. False   moe_pad_expert_input_to_capacity ................ False   moe_per_layer_logging ........................... False   moe_router_load_balancing_type .................. aux_loss   moe_router_pre_softmax .......................... False   moe_router_topk ................................. 2   moe_token_dispatcher_type ....................... allgather   moe_token_drop_policy ........................... probs   moe_z_loss_coeff ................................ None   nccl_communicator_config_path ................... None   no_load_optim ................................... True   no_load_rng ..................................... True   no_persist_layer_norm ........................... False   no_save_optim ................................... None   no_save_rng ..................................... None   norm_epsilon .................................... 1e05   normalization ................................... RMSNorm   num_attention_heads ............................. 32   num_channels .................................... 3   num_classes ..................................... 1000   num_dataset_builder_threads ..................... 1   num_experts ..................................... None   num_layers ...................................... 32   num_layers_per_virtual_pipeline_stage ........... None   num_query_groups ................................ 32   num_workers ..................................... 8   one_logger_async ................................ False   one_logger_project .............................. megatronlm   one_logger_run_name ............................. None   onnx_safe ....................................... None   openai_gelu ..................................... False   optimizer ....................................... adam   output_bert_embeddings .......................... False   overlap_grad_reduce ............................. False   overlap_p2p_comm ................................ False   overlap_param_gather ............................ False   override_opt_param_scheduler .................... False   padded_vocab_size ............................... 32000   params_dtype .................................... torch.bfloat16   patch_dim ....................................... 16   perform_initialization .......................... True   pipeline_model_parallel_size .................... 1   pipeline_model_parallel_split_rank .............. None   position_embedding_type ......................... rope   pretrained_checkpoint ........................... None   profile ......................................... False   profile_ranks ................................... [0]   profile_step_end ................................ 12   profile_step_start .............................. 10   qk_layernorm .................................... False   query_in_block_prob ............................. 0.1   rampup_batch_size ............................... None   rank ............................................ 0   recompute_granularity ........................... None   recompute_method ................................ None   recompute_num_layers ............................ None   reset_attention_mask ............................ False   reset_position_ids .............................. False   retriever_report_topk_accuracies ................ []   retriever_score_scaling ......................... False   retriever_seq_length ............................ 256   retro_add_retriever ............................. False   retro_attention_gate ............................ 1   retro_cyclic_train_iters ........................ None   retro_encoder_attention_dropout ................. 0.1   retro_encoder_hidden_dropout .................... 0.1   retro_encoder_layers ............................ 2   retro_num_neighbors ............................. 2   retro_num_retrieved_chunks ...................... 2   retro_project_dir ............................... None   retro_verify_neighbor_count ..................... True   rotary_base ..................................... 10000   rotary_interleaved .............................. False   rotary_percent .................................. 1.0   rotary_seq_len_interpolation_factor ............. None   s3_cache_path ................................... None   sample_rate ..................................... 1.0   save ............................................ /mnt/data/someone/checkpoint/mcore/Llama27btp8pp1rewardcore080distckpt   save_interval ................................... 1000   scatter_gather_tensors_in_pipeline .............. True   seed ............................................ 1234   select_max_response ............................. firstk   seq_length ...................................... 2048   sequence_parallel ............................... True   sgd_momentum .................................... 0.9   short_seq_prob .................................. 0.1   skip_train ...................................... False   spec ............................................ None   split ........................................... 98,2,0   squared_relu .................................... False   standalone_embedding_stage ...................... False   start_weight_decay .............................. 0.1   straggler_ctrlr_port ............................ 65535   straggler_minmax_count .......................... 1   swiglu .......................................... True   swin_backbone_type .............................. tiny   tensor_model_parallel_size ...................... 8   tensorboard_dir ................................. /mnt/data/someone/checkpoint/mcore/Llama27btp8pp1rewardcore080distckpt   tensorboard_log_interval ........................ 100   tensorboard_queue_size .......................... 1000   test_data_path .................................. None   test_mode ....................................... False   tiktoken_num_special_tokens ..................... 1000   tiktoken_pattern ................................ None   tiktoken_special_tokens ......................... None   timing_log_level ................................ 0   timing_log_option ............................... minmax   titles_data_path ................................ None   tokenizer_model ................................. /mnt/data/someone/model/llama2_tokenizer.model   tokenizer_type .................................. Llama2Tokenizer   tp_comm_bulk_dgrad .............................. True   tp_comm_bulk_wgrad .............................. True   tp_comm_overlap ................................. False   tp_comm_overlap_ag .............................. True   tp_comm_overlap_cfg ............................. None   tp_comm_overlap_rs .............................. True   tp_comm_overlap_rs_dgrad ........................ False   tp_comm_split_ag ................................ True   tp_comm_split_rs ................................ True   train_data_path ................................. None   train_iters ..................................... 1000   train_samples ................................... None   transformer_impl ................................ transformer_engine   transformer_pipeline_model_parallel_size ........ 1   untie_embeddings_and_output_weights ............. True   use_checkpoint_args ............................. True   use_checkpoint_opt_param_scheduler .............. False   use_cpu_initialization .......................... None   use_dist_ckpt ................................... True   use_distributed_optimizer ....................... False   use_flash_attn .................................. True   use_legacy_models ............................... False   use_one_sent_docs ............................... False   use_ring_exchange_p2p ........................... False   use_rotary_position_embeddings .................. True   use_tp_pp_dp_mapping ............................ False   valid_data_path ................................. None   variable_seq_lengths ............................ False   virtual_pipeline_model_parallel_size ............ None   vision_backbone_type ............................ vit   vision_pretraining .............................. False   vision_pretraining_type ......................... classify   vocab_extra_ids ................................. 0   vocab_file ...................................... None   vocab_size ...................................... None   wandb_exp_name ..................................    wandb_project ...................................    wandb_save_dir ..................................    weight_decay .................................... 0.1   weight_decay_incr_style ......................... constant   wgrad_deferral_limit ............................ 0   world_size ...................................... 8   yaml_cfg ........................................ None  end of arguments  ``` **Environment (please complete the following information):**   MegatronLM: core 0.8   NGC Container: 24.06 **Proposed fix** Allow userdefined shard strategy, or fix `distckptstrictness assume_ok_unexpected` **Additional context** I tried commenting out `self._validate_global_shapes(self.metadata, self.shapes_validation_sharded_tensors)`. Such solution didn't work because megatron threw another exception: ```shell [rank0]: Traceback (most recent call last): (RANK 0) [rank0]:   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/checkpoint/utils.py"", line 163, in reduce_scatter [rank0]:     local_data = map_fun() [rank0]:   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/checkpoint/logger.py"", line 64, in wrapper [rank0]:     result = func(*args, **kwargs) [rank0]:   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/checkpoint/state_dict_loader.py"", line 205, in local_step [rank0]:     local_plan = planner.create_local_plan() [rank0]:   File ""/mnt/data/someone/MegatronLMbenchmark/megatron/core/dist_checkpointing/strategies/torch.py"", line 507, in create_local_plan [rank0]:     return super().create_local_plan() [rank0]:   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/checkpoint/default_planner.py"", line 188, in create_local_plan [rank0]:     return create_default_local_load_plan( [rank0]:   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/checkpoint/default_planner.py"", line 306, in create_default_local_load_plan [rank0]:     raise RuntimeError(f""Missing key in checkpoint state_dict: {fqn}."") [rank0]: RuntimeError: Missing key in checkpoint state_dict: pooler_head.dense1.weight. ```",2024-08-26T02:41:57Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1032,Have you tried setting `distckptstrictness log_all`?,Marking as stale. No activity in 60 days.
clarence-lee-sheng,[QUESTION] Throughput low for 70B training,"Hi Megatron team,  Could I check if the following is expected:  I saw that in your README, the throughput for 70B using TP8 PP2 and DP 48 on 768GPUs was 420.5 TFLOPs/s/GPU However, running on 70B on TP8 PP8 and DP2 on 128 GPUs (H100s)  with activation checkpointing was only 296.5 TFLOPs/s/GPU for me.   May I check what are the optimizations that you all have used to maximise your throughput as reported in the table in the README? I have also used overlapgradreduce and overlapparamgather Thank you! ",2024-08-26T01:52:57Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1031,"Can you try PP2 and DP8? I presume you are also using `usedistributedoptimizer`? `tpcommoverlap` should also help, but you will need to set have a container built with TE + UserBuffer support.","I get OOM when using PP2 and TP8, to prevent oom with PP8 and TP8, I used recomputegranularity full, recomputemethod uniform, recomputenumlayers 5 and distributesavedactivations. May I check if this is similar to how you all implement it? Also, when using tpcommoverlap and sequenceparallel, I get error [dgx16:4064993:0] cl_basic_team.c:132 CL_BASIC_ERROR no tl teams were created [dgx16:4064993:0:4064993] Caught signal 11 (Segmentation fault: address not mapped to object at address (nil))"
felipeliliti,*#aurorax_project##*,"``` aurorax_project/ â”‚ â”œâ”€â”€ aurorax/ â”‚   â”œâ”€â”€ __init__.py â”‚   â”œâ”€â”€ interpreter.py â”‚   â”œâ”€â”€ syntax.py â”‚   â”œâ”€â”€ runtime.py â”‚ â”œâ”€â”€ examples/ â”‚   â””â”€â”€ example_code.aur â”‚ â”œâ”€â”€ tests/ â”‚   â”œâ”€â”€ __init__.py â”‚   â””â”€â”€ test_interpreter.py â”‚ â”œâ”€â”€ README.md â””â”€â”€ setup.py ```  Arquivos e ConteÃºdos **`aurorax/__init__.py`** ```python  Package initializer ``` **`aurorax/interpreter.py`** ```python class AuroraxInterpreter:     def __init__(self):         self.variables = {}     def execute(self, code):         lines = code.split('\n')         for line in lines:             self.parse(line.strip())     def parse(self, line):         if line.startswith('print '):             self.print_statement(line[6:])         elif '=' in line:             self.assignment(line)         else:             print(f""Unknown statement: {line}"")     def print_statement(self, value):         if value in self.variables:             print(self.variables[value])         else:             print(value)     def assignment(self, line):         var, value = line.split('=')         self.variables[var.strip()] = value.strip() ``` **`aurorax/syntax.py`** ```python  Define syntax rules and language constructs ``` **`aurorax/runtime.py`** ```python  Define runtime environment and execution logic ``` **`examples/example_code.aur`** ```aurorax // Example Aurorax code def predict(data):     model = create_model()     model.train(data)     return model.predict(data) data = load_data(""data.csv"") result = predict(data) print result ``` **`tests/test_interpreter.py`** ```python import unittest from aurorax.interpreter import AuroraxInterpreter class TestAuroraxInterpreter(unittest.TestCase):     def setUp(self):         self.interpreter = AuroraxInterpreter()     def test_print_statement(self):         self.interpreter.execute('x = 10\nprint x')          Expected output: 10     def test_assignment(self):         self.interpreter.execute('y = 20')         self.assertEqual(self.interpreter.variables['y'], '20') if __name__ == '__main__':     unittest.main() ``` **`README.md`** ```markdown  Aurorax Aurorax is a programming language designed to create innovative solutions for improving the planet in all areas. This project includes an interpreter, syntax definitions, and example code.  Getting Started 1. Clone the repository 2. Install dependencies 3. Run the interpreter with example code  Usage See `examples/example_code.aur` for sample Aurorax code.  Contributing Contributions are welcome. Please open an issue or pull request.  License This project is licensed under the MIT License. ``` **`setup.py`** ```python from setuptools import setup, find_packages setup(     name='aurorax',     version='0.1',     packages=find_packages(),     install_requires=[],     tests_require=['unittest'],     python_requires='>=3.6', ) ```",2024-08-25T17:57:19Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1030
felipeliliti,AURORAX,0 0   | 369369 _____ nova linguagem stdk ..... (  ________________&&bummmmmm Erro 7 stk realidade espelhada 2070 ano do recomeÃ§o voltando aos paramentos ide com intervalos aramicos tenunciantes em cÃ³dificacao RS aurorax MÃ¡quina nuvens com seguimentos realÃ­stico multimidal em aÃ§Ã£o conjunta no exato edx em forma de 3.=%@,2024-08-24T19:44:42Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1029
xinyu-yang,[BUG] Computation wrapped by jit_fuser get stuck after using a standalone thread to schedule the collective comm operations of pipeline parallel.,"**Describe the bug** I would like to implement a scheduler to schedule the orders of collective communication operations of the pipeline parallel. I implemented a scheduling class with a priority queue that provides two main methods, enqueue and dequeue. Whenever the ""group.comm"" like ""group.isend"", ""group.irecv"" functions are called. I just put them into the queue, including the operator, args, and kwargs. At the same time, there is a separate thread that always schedules and dequeues the collective communication operations. Currently, I just schedule the collective communication ops of the pipeline parallel to the original order (enqueue order) they are. However, the process gets stuck at the first computation function decorated by ""jit_fuser"". In our case, it is ""bias_dropout_add_fused_train"" function. Note that the process gets stuck when I use NCCL backend **but** works well when I use GLOO backend. I totally do not have any idea of this problem. Thanks to anyone who can help or provide any hints! ðŸ˜Š",2024-08-23T07:33:41Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1028,Marking as stale. No activity in 60 days.
sandyhouse,[BUG] Training got stuck when pipeline is used.,"**Describe the bug** When I used pipeline parallelism to train a 7B GPT model, the program hung. **To Reproduce** I used the following script for training: ```shell !/bin/bash  Runs the ""7B"" parameter model export CUDA_DEVICE_MAX_CONNECTIONS=1 export OMP_NUM_THREADS=1 GPUS_PER_NODE=8  Change for multinode config MASTER_ADDR=localhost MASTER_PORT=6000 NUM_NODES=1 NODE_RANK=0 WORLD_SIZE=$(($GPUS_PER_NODE*$NUM_NODES)) CHECKPOINT_PATH=$1  TENSORBOARD_LOGS_PATH=$2  VOCAB_FILE=$3 /gpt2vocab.json MERGE_FILE=$4 /gpt2merges.txt DATA_PATH=$3 _text_document TOKENIZER_MODEL_PATH=$4 DISTRIBUTED_ARGS=(     nproc_per_node $GPUS_PER_NODE     nnodes $NUM_NODES     master_addr $MASTER_ADDR     master_port $MASTER_PORT ) GPT_MODEL_ARGS=(     usemcoremodels     transformerimpl ""transformer_engine""     numlayers 32     hiddensize 4096     numattentionheads 32     seqlength 4096     maxpositionembeddings 4096 ) TRAINING_ARGS=(     microbatchsize 1     globalbatchsize 128     trainiters 275     weightdecay 0.1     adambeta1 0.9     adambeta2 0.95     initmethodstd 0.006     clipgrad 1.0     fp16     lr 3.0e4     lrdecaystyle cosine     minlr 3.0e5     lrwarmupfraction .001     lrdecayiters 275 ) MODEL_PARALLEL_ARGS=(         tensormodelparallelsize 1         pipelinemodelparallelsize 4         usedistributedoptimizer         sequenceparallel ) DATA_ARGS=(     datapath $DATA_PATH     split 100,0,0     tokenizertype Llama2Tokenizer     tokenizermodel ${TOKENIZER_MODEL_PATH}     datacachepath $DATA_CACHE_PATH ) EVAL_AND_LOGGING_ARGS=(     loginterval 1     saveinterval 10000     evalinterval 1000     save $CHECKPOINT_PATH     load $CHECKPOINT_PATH     evaliters 0     tensorboarddir $TENSORBOARD_LOGS_PATH     logthroughput ) torchrun ${DISTRIBUTED_ARGS[@]} pretrain_gpt.py \     ${GPT_MODEL_ARGS[@]} \     ${TRAINING_ARGS[@]} \     ${MODEL_PARALLEL_ARGS[@]} \     ${DATA_ARGS[@]} \     ${EVAL_AND_LOGGING_ARGS[@]} ``` I found that this issue occurred because the device_id parameter was set when calling the torch.distributed.init_process_group function. This caused processes that were not members of the new_group to use ncclCommSplit to create a child communicator. However, for processes that were members of the new_group, the communicator was not created until the first communication operation was called, causing the nonnew_group member processes to keep waiting on the parent communicator for other processes to create the communicator. In other words, this violates the convention that all ranks must call ncclCommSplit on the original communicator.  ```python  in megatron/training/initialize.py: 431L (_initialize_distributed function) if packaging.version.Version(torch.__version__) >= packaging.version.Version(""2.3.0""):     init_process_group_kwargs['device_id'] = device_id ``` A detailed description can be found at https://github.com/pytorch/pytorch/issues/134314.  **Environment (please complete the following information):**   MegatronLM commit ID: 85bd1f9af2b6a85d5f348509cca5a6251e0cbef1   PyTorch version: 2.4.0a0+f70bd71a48.nv24.6 (NGC   CUDA version: 12.5   NCCL version: 2.21.5+cuda12.5 **Proposed fix** Before this issue is fixed in PyTorch, I think we should avoid passing the device_id parameter to torch.distributed.init_process_group, thereby falling back to the original behavior.",2024-08-23T07:00:18Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1027,Marking as stale. No activity in 60 days.
heavyrain-lzy,[QUESTION]Create the `pp` groups when enable `context_parallel_size > 1` and `encoder_tensor_parallel_size != tensor_parallel_size`,"**Your question** I have a quesstion about create the `pp` groups when enable `context_parallel_size > 1` and `encoder_tensor_parallel_size != tensor_parallel_size`. When enabling `context_parallel`, the input will be splited symmetrically in order to balance calculation. Using `zip(cycle(e_ranks), d_ranks)` is not right. https://github.com/NVIDIA/MegatronLM/blob/4ff9e66ec870e6e45d765531318d50bd47c85d0f/megatron/core/parallel_state.pyL602",2024-08-23T02:57:59Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1026
abgoswam,[BUG] Long context training using context-parallel hangs/crashes,"**Describe the bug**   `contextparallelsize` > 1    `maxpositionembeddings` >= 131072  (i.e. 128k tokens) This causes megatronlm to crash in the `get_batch` call.  **Observations:** 1. The same config  works  for `maxpositionembeddings`  [rank0]:     pretrain( [rank0]:   File ""/tmp/amltcodedownload/MegatronLM/megatron/training/training.py"", line 312, in pretrain [rank0]:     iteration, num_floating_point_operations_so_far = train( [rank0]:   File ""/tmp/amltcodedownload/MegatronLM/megatron/training/training.py"", line 1115, in train [rank0]:     train_step(forward_step_func, [rank0]:   File ""/tmp/amltcodedownload/MegatronLM/megatron/training/training.py"", line 611, in train_step [rank0]:     losses_reduced = forward_backward_func( [rank0]:   File ""/tmp/amltcodedownload/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 460, in forward_backward_no_pipelining [rank0]:     output_tensor, num_tokens = forward_step( [rank0]:   File ""/tmp/amltcodedownload/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 266, in forward_step [rank0]:     output_tensor, loss_func = forward_step_func(data_iterator, model) [rank0]:   File ""/tmp/amltcodedownload/MegatronLM/pretrain_gpt.py"", line 169, in forward_step [rank0]:     tokens, labels, loss_mask, attention_mask, position_ids = get_batch( [rank0]:   File ""/tmp/amltcodedownload/MegatronLM/pretrain_gpt.py"", line 104, in get_batch [rank0]:     batch = get_batch_on_this_tp_rank(data_iterator) [rank0]:   File ""/tmp/amltcodedownload/MegatronLM/megatron/training/utils.py"", line 313, in get_batch_on_this_tp_rank [rank0]:     data = next(data_iterator) [rank0]:   File ""/usr/local/lib/python3.10/distpackages/torch/utils/data/dataloader.py"", line 629, in __next__ [rank0]:     data = self._next_data() [rank0]:   File ""/usr/local/lib/python3.10/distpackages/torch/utils/data/dataloader.py"", line 1326, in _next_data [rank0]:     idx, data = self._get_data() [rank0]:   File ""/usr/local/lib/python3.10/distpackages/torch/utils/data/dataloader.py"", line 1282, in _get_data [rank0]:     success, data = self._try_get_data() [rank0]:   File ""/usr/local/lib/python3.10/distpackages/torch/utils/data/dataloader.py"", line 1130, in _try_get_data [rank0]:     data = self._data_queue.get(timeout=timeout) [rank0]:   File ""/usr/lib/python3.10/queue.py"", line 180, in get [rank0]:     self.not_empty.wait(remaining) [rank0]:   File ""/usr/lib/python3.10/threading.py"", line 324, in wait [rank0]:     gotit = waiter.acquire(True, timeout) [rank0]:   File ""/usr/local/lib/python3.10/distpackages/torch/utils/data/_utils/signal_handling.py"", line 66, in handler [rank0]:     _error_if_any_worker_fails() ``` **Environment (please complete the following information):**   MegatronLM commit ID: c873429cbaa43257d4d4fc01df2a7a50453b7984   base image: nvcr.io/nvidia/pytorch:24.06py3 **Proposed fix**  NA **Additional context**  NA",2024-08-21T23:30:47Z,,closed,1,2,https://github.com/NVIDIA/Megatron-LM/issues/1025,can you try `nocreateattentionmaskindataloader`?,Thanks  for the tip.  The `nocreateattentionmaskindataloader`  indeed helps out here.  Closing the issue. 
ben-8878,TypeError: get_cpu_offload_context() takes from 0 to 4 positional arguments but 5 were given,"te_version: 1.9.0+e79d915 ``` Traceback (most recent call last):   File ""/home/ybZhang/miniconda3/envs/glmm/lib/python3.8/sitepackages/swift/cli/export.py"", line 5, in      export_main()   File ""/home/ybZhang/miniconda3/envs/glmm/lib/python3.8/sitepackages/swift/utils/run_utils.py"", line 32, in x_main     result = llm_x(args, **kwargs)   File ""/home/ybZhang/miniconda3/envs/glmm/lib/python3.8/sitepackages/swift/llm/export.py"", line 302, in llm_export     convert_hf_to_megatron(model, extra_args, args.torch_dtype)   File ""/home/ybZhang/miniconda3/envs/glmm/lib/python3.8/sitepackages/swift/llm/megatron/convert.py"", line 22, in convert_hf_to_megatron     mg_model = model_provider()   File ""/home/ybZhang/miniconda3/envs/glmm/lib/python3.8/sitepackages/swift/llm/megatron/model.py"", line 61, in model_provider     model = gpt_model_cls(   File ""./PaiMegatronPatch/megatron_patch/model/qwen2/model.py"", line 103, in __init__     self.decoder = TransformerBlock(   File ""./PaiMegatronPatch/megatron_patch/model/qwen2/transformer_block.py"", line 137, in __init__     ) = get_cpu_offload_context(   File ""./MegatronLM/megatron/core/transformer/custom_layers/transformer_engine.py"", line 899, in get_cpu_offload_context     context, sync_func = _get_cpu_offload_context( TypeError: get_cpu_offload_context() takes from 0 to 4 positional arguments but 5 were given ```",2024-08-21T06:29:27Z,stale,open,1,2,https://github.com/NVIDIA/Megatron-LM/issues/1018,PR CC(fix _te_version issue in transformer_engine.py get_cpu_offload_context()) ,Marking as stale. No activity in 60 days.
bingnandu,Fix: timers('interval-time') bug and abnormal termination,"The `save_checkpoint_and_time()` function now internally manages the `'intervaltime'` timer. As a result, the external calls to start and stop this timer should be removed to prevent conflicts and potential errors due to double timing. !save  ",2024-08-20T15:22:07Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/1017,"During the pretraining of a GPT model, if the profile flag is enabled and the exitinterval is greater than the profilestepstart, the application would abnormally terminate with a 6 exit status.  !image",Marking as stale. No activity in 60 days.,Marking as stale. No activity in 60 days.
felipeliliti,TitanForge AI fase liliti stk homem de ferro," **Directory Structure:** ``` TitanForgeAI/ â”‚ â”œâ”€â”€ include/ â”‚   â”œâ”€â”€ nlp_module.hpp â”‚   â”œâ”€â”€ vision_module.hpp â”‚   â”œâ”€â”€ simulation_module.hpp â”‚   â””â”€â”€ sustainability_module.hpp â”‚ â”œâ”€â”€ src/ â”‚   â”œâ”€â”€ nlp_module.cpp â”‚   â”œâ”€â”€ vision_module.cpp â”‚   â”œâ”€â”€ simulation_module.cpp â”‚   â””â”€â”€ sustainability_module.cpp â”‚ â””â”€â”€ main.cpp ```  **main.cpp:** ```cpp include  include ""include/nlp_module.hpp"" include ""include/vision_module.hpp"" include ""include/simulation_module.hpp"" include ""include/sustainability_module.hpp"" class TitanForgeAIFramework { private:     NLPModule nlpModule;     VisionModule visionModule;     SimulationModule simulationModule;     SustainabilityModule sustainabilityModule; public:     void run() {         std::cout  include  class NLPModule { public:     void processText(const std::string& text); }; endif ```  **nlp_module.cpp:** ```cpp include ""nlp_module.hpp"" void NLPModule::processText(const std::string& text) {     std::cout  class VisionModule { public:     void processImage(const std::string& imagePath); }; endif ```  **vision_module.cpp:** ```cpp include ""vision_module.hpp"" include  void VisionModule::processImage(const std::string& imagePath) {     cv::Mat image = cv::imread(imagePath);     if (!image.empty()) {         cv::imshow(""TitanForge AI Vision"", image);         cv::waitKey(0);     } else {         std::cerr  void SimulationModule::runSimulation() {     std::cout  void SustainabilityModule::monitorEnvironment() {     std::cout << ""Monitoring Environmental Data in TitanForge AI..."" << std::endl;     // Add environment monitoring logic here } ```  **Instructions for Compilation:** To compile the project: 1. **Install OpenCV** (if you plan to use the vision module). 2. **Compile** the project using a C++ compiler: ```bash g++ std=c++11 main.cpp src/nlp_module.cpp src/vision_module.cpp src/simulation_module.cpp src/sustainability_module.cpp o titanforge lopencv_core lopencv_highgui lopencv_imgcodecs ```  **Description:** **TitanForge AI** is a comprehensive AI framework combining advanced modules for **Natural Language Processing (NLP)**, **Computer Vision**, **Simulation**, and **Sustainability Monitoring**. Each module is designed to provide a specific function within the broader scope of AI and technology integration, inspired by innovations from leading tech companies. This project is intended for developers, researchers, and companies looking to integrate stateoftheart AI into their systems, with a focus on innovation and sustainability.",2024-08-20T13:00:53Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1016
yanboliang,[ENHANCEMENT] Integrating torch.compile with Megatron/TransformerEngine,"**Is your feature request related to a problem? Please describe.** * PyTorch 2.0 added a new feature ```torch.compile``` which captures computation/communication ops into FX graph and generates optimized execution plan by fusing ops and leveraging computation/communication overlap.  * Meanwhile, many cool features built on top of ```torch.compile```, like ```FlexAttention``` which provided a flexible API that can automatic generates high performance kernels for many attention variants. * We would like to explore if ```torch.compile``` + Megatron can unleash even great power at both LLM training and inference space. **Describe the solution you'd like** * Enable ```torch.compile``` on top of Megatron modules, tensor parallel, context parallel and the underlying TransformerEngine, capture computation/communication graphs, investigate better fusion and computation/communication overlap optimization, etc. * Explore local compilation, e.g, integrating ```FlexAttention``` into the Megatron attention module. **Additional context** * I looked code here and TransformerEngine repro, find many places have been decorated by ```no_torch_dynamo```, which just skipped Dynamo tracing. However, it seems Megatron supports ```torch.compile``` by ```jit_fuser = torch.compile```. I'd like to know more context on the discrepancy of if allowing ```torch.compile``` between these two repros. * I understand TransformerEngine has a lot of hand written CUDA kernels and fused modules, which already leverage fusion to squeeze model's performance. I think ```torch.compile``` can provide even more benefit than fusion only, like better fusion and leveraging computation/communication in the distributed setup, etc. * We would like to work with experts at here to figure out other opportunities of providing a better user experience and performance by integrating ```torch.compile``` with Megatron/TransformerEngine. ",2024-08-20T05:31:04Z,,closed,1,2,https://github.com/NVIDIA/Megatron-LM/issues/1015,"Hi, thanks for the issue. Is there anything more specific you'd like to contribute? "," Thanks for your reply! We have a list of enhancements that could contribute to Megatron/TransformerEngine, mainly focus on integrating ```torch.compile``` + Megatron/TransformerEngine. Some examples include: * We already did many fixes to enhance ```torch.compile``` for capturing full graph for Megatron, now we almost done for TP and we are working on SP/CP/PP use cases. We think most of the issues should be fixed at PyTroch side, but we do find some very tricky cases need to work with Megatron community to figure out a better solution. So what we would directly contribute to this bucket includes: 1/ Adding torch compiled mode unit tests to ensure it works for major use cases; 2/ We would like to open a few issues about the cases that block us from capturing full graph (e.,g proper support for ```torch.cuda.current_device```), work with Megatron community to get feedback about our solution and finally fix these gaps. * We have some other features, like ```FlexAttention```, which provided a flexible API that can automatic generates high performance kernels for many attention variants. We believe this would help LLM researchers and engineers to quickly try their new attention score modification and masking ideas in an easy way. We'd like to check if this is aligned with Megatron community's vision, and we could add this to Megatron to have more users easily to access it. * In general, we are happy to work with Megatron community to enable any features around ```torch.compile```, to have users in LLM space can take advantage the cuttingedge technology from both Megatron and PyTorch. Let me know if you have any question! Thank you!"
yanmenxue,[BUG]--target-tensor-parallel-size vs arg.target_tensor_parallel_size,"**Describe the bug** In saver_megatron.py, the input to args is targettensorparallelsize and targetpipelineparallelsize, but in further use, they are cited as args.target_tensor_parallel_size and args.target_pipeline_parallel_size.",2024-08-18T18:56:13Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1014,Closing. Please use the template and reopen if you encounter any issues with it.
Annmarie56,[BUG] hacked by developer ,"**Describe the bug** A clear and concise description of what the bug is. **To Reproduce** Steps to reproduce the behavior. The easier it is to reproduce the faster it will get maintainer attention. **Expected behavior** A clear and concise description of what you expected to happen. **Stack trace/logs** If applicable, add the stack trace or logs from the time of the error. **Environment (please complete the following information):**   MegatronLM commit ID   PyTorch version   CUDA version   NCCL version **Proposed fix** If you have a proposal for how to fix the issue state it here or link to a PR. **Additional context** Add any other context about the problem here.",2024-08-16T22:23:15Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1013
QPHutu,[BUG] `finish_embedding_wgrad_compute` appears after grad all-reduce,"**Describe the bug** In  `megatron/core/pipeline_parallel/schedules.py`, `finish_embedding_wgrad_compute` should appear before `enable_grad_sync` and `grad_sync_func`?  **Expected behavior** Gradient allreduce should happen after gradient computations.",2024-08-16T06:42:04Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1012, Can you share some comments? Thank you!,Marking as stale. No activity in 60 days.
Fragile-azalea,[QUESTION] I found that the Huggingface Tokenizer cannot be selected in Date Preprocessing. Is this intentional or a bug?,I found that the Huggingface Tokenizer cannot be selected in Date Preprocessing. Is this intentional or a bug?,2024-08-16T04:36:48Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1011
imhmhm,"[BUG] ""builders[key].finalize(...)"" only finalize the last key after the loop in the ""preprocess_data.py""","https://github.com/NVIDIA/MegatronLM/blob/e8f8e63f13a074f7e35d72c8bfb3e1168cd84e8e/tools/preprocess_data.pyL179L186 If multiple keys exist, the current line 186 will only finalize the last key.  The following modification might work correctly. ``` for key in self.args.json_keys:     builders[key].finalize(output_idx_files[key]) ```",2024-08-15T11:41:56Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/1010,Marking as stale. No activity in 60 days.,I am also confused with this code statement.,Marking as stale. No activity in 60 days.
wplf,[QUESTION] How to save model trained by TP=8 and load model in TP=4 or other?,"Hi, there. Thank you for great project. I have a question about checkpoint save and load.   My model is saved as normal state dict, but split into 8 parts.  How can I load them in 4 or 2 GPUs, with sufficient GPU memory? ```bash iter_0150000/mp_rank_00: distrib_optim.pt  model_optim_rng.pt iter_0150000/mp_rank_01: distrib_optim.pt  model_optim_rng.pt iter_0150000/mp_rank_02: distrib_optim.pt  model_optim_rng.pt iter_0150000/mp_rank_03: distrib_optim.pt  model_optim_rng.pt iter_0150000/mp_rank_04: distrib_optim.pt  model_optim_rng.pt iter_0150000/mp_rank_05: distrib_optim.pt  model_optim_rng.pt iter_0150000/mp_rank_06: distrib_optim.pt  model_optim_rng.pt iter_0150000/mp_rank_07: distrib_optim.pt  model_optim_rng.pt ```",2024-08-15T07:31:56Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1009,"Hiï¼Œ Iâ€˜ve found the solution. `tools/checkpoint/saver_megatron.py` might do this job. Thank you for great repo, again!"
jstjohn,Add a nicer exception for missing model keys in the checkpoint,"When debugging a failing checkpoint load in test, I was just confronted with a missing Key exception with no indication of what the target options were. This improved exception message allowed me to identify/fix the problem on my end.",2024-08-14T20:46:48Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/1008,Moved PR over to MR against internal clone of the repo.,Duplicated by: https://gitlabmaster.nvidia.com/ADLR/megatronlm//merge_requests/1927 
janEbert,Fix BlendableDataset for low sampling probs,"When supplying low enough sampling probabilities for some datasets so that they won't be included in the blend, this part could incorrectly raise an error because `sizes` was potentially compared with an incorrect dataset which was never part of the blend to begin with. We now make sure that the dataset is actually part of the blend by only iterating over those indices that are present at all.",2024-08-14T12:24:19Z,,open,0,6,https://github.com/NVIDIA/Megatron-LM/issues/1007,Marking as stale. No activity in 60 days.,What are you missing to merge this?,Marking as stale. No activity in 60 days.,Anything missing to merge this?,Marking as stale. No activity in 60 days.,Anything missing to merge this?
matthew-frank,optional use of mpi instead of gloo for distributed checkpoint load/save,"We've been transiently seeing the error `[E ProcessGroupGloo.cpp:144] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144]` when running at scales of 10k ranks or more. (The error seems to happen with increasing rate as the number of ranks is increased.  Fewer than one failure in 20 runs at 10240 gpus, about one failure in 3 runs at 10992 gpus.) This PR enables enables an envvar (CPU_COMMS_BACKEND_OVERRIDE) to override Megatron Core's backend for cpucomms from the default `gloo` to the option `mpi`. I've tested the new option extensively on the MLPerf benchmark at several scales. I've tested that when CPU_COMMS_BACKEND_OVERRIDE is unset (or set to 'gloo') that the code behaves as it did before, both functionally and from a perf perspective. I've tested that setting CPU_COMMS_BACKEND_OVERRIDE=BREAKME (a nonsense value) leads to a failure.  (So the envvar is actually changing the backend). I've tested that when CPU_COMMS_BACKEND_OVERRIDE=mpi, the Gloo connectFullMesh error ""goes away"" at very large scale.  (In my very first and only run today at 1380 nodes with CPU_COMMS_BACKEND_OVERRIDE unset I got the Gloo connectFullMesh error, then I set CPU_COMMS_BACKEND_OVERRIDE=mpi and did 5 runs in a row that succeeded in loading the checkpoint without any error.",2024-08-13T21:55:39Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1006,"I hope this message finds you well. your outstanding contributions to the MPI community,  it's truly commendable. I am particularly interested in understanding more about the integration of MPI with PyTorch, as it opens up exciting possibilities for scalable machine learning applications. Could you kindly share which versions of MPI and PyTorch you recommend for optimal compatibility and performance?"
andrewvli,[BUG] T5 extended attention mask shape mismatch with transformer engine,"**Describe the bug** Running the most recent version of the T5 pretraining script out of the box raises a Value Error, particularly in the following line: ``` [rank0]:   File ""/home/miniconda3/lib/python3.12/sitepackages/transformer_engine/pytorch/attention.py"", line 243, in get_cu_seqlens_and_indices [rank0]:     bs, seqlen = mask.shape [rank0]:     ^^^^^^^^^^ [rank0]: ValueError: too many values to unpack (expected 2) ``` When taking a look at the T5 model, the `t5_extended_attention_mask()` function in `/megatron/core/models/T5/t5_model.py` returns a List of Tensors with shape `[b, 1, s, s]`, as shown below. ``` def t5_extended_attention_mask(attention_mask_list: List[Tensor]) > List[Tensor]:     def attn_mask_postprocess(attn_mask):          [b, 1, s, s]         extended_attention_mask = attn_mask.unsqueeze(1)         return extended_attention_mask     return [attn_mask_postprocess(attn_mask) for attn_mask in attention_mask_list] ``` However, when looking at the Transformer Engine repository where the error originates, it appears that the shape of the attention mask expected should be `[b, 1, 1, s]`. The comment in `NVIDIA/TransformerEngine/transformer_engine/pytorch/attention.py` where that's indicated (line 5517) is listed as follows: ```         attention_mask: Optional[Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]],              default = `None`. Boolean tensor(s) used to mask out attention softmax input.              It should be `None` for causal masks and ""`no_mask`"". For padding masks, it should be              a single tensor of [batch_size, 1, 1, seqlen_q] for selfattention, and a tuple of              two tensors in shapes [batch_size, 1, 1, seqlen_q] and [batch_size, 1, 1, seqlen_kv]              for crossattention. For ""`arbitrary`"" mask, it should be in a shape broadcastable              to [batch_size, num_heads, max_seqlen_q, max_seqlen_kv]. A `True` value means              the corresponding position is masked out and a `False` means that position              is allowed to participate in attention. ``` Given that the attention masks produced for T5 have different dimensions (e.g. when printing out the dimensionality of the masks, I saw three tensors with dimensions `[batch_size, encoder_seqlen, encoder_seqlen], [batch_size, decoder_seqlen, decoder_seqlen], [batch_size, encoder_seqlen, decoder_seqlen]`), I was wondering if there was a fix for this issue.  **To Reproduce** Running the pretraining script in a base conda environment with the environment described below.  **Expected behavior** I'd expect the pretraining script to reach the training iterations without throwing an error. **Stack trace/logs** ``` [rank0]: Traceback (most recent call last): [rank0]:   File ""/home/MegatronLM/pretrain_t5.py"", line 257, in  [rank0]:     pretrain( [rank0]:   File ""/home/MegatronLM/megatron/training/training.py"", line 278, in pretrain [rank0]:     iteration, num_floating_point_operations_so_far = train( [rank0]:                                                       ^^^^^^ [rank0]:   File ""/home/MegatronLM/megatron/training/training.py"", line 1057, in train [rank0]:     train_step(forward_step_func, [rank0]:   File ""/home/MegatronLM/megatron/training/training.py"", line 558, in train_step [rank0]:     losses_reduced = forward_backward_func( [rank0]:                      ^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/home/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 399, in forward_backward_no_pipelining [rank0]:     output_tensor, num_tokens = forward_step( [rank0]:                                 ^^^^^^^^^^^^^ [rank0]:   File ""/home/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 206, in forward_step [rank0]:     output_tensor, loss_func = forward_step_func(data_iterator, model) [rank0]:                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/home/MegatronLM/pretrain_t5.py"", line 199, in forward_step [rank0]:     output_tensor = model( [rank0]:                     ^^^^^^ [rank0]:   File ""/home/pytorch/torch/nn/modules/module.py"", line 1532, in _wrapped_call_impl [rank0]:     return self._call_impl(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/home/pytorch/torch/nn/modules/module.py"", line 1541, in _call_impl [rank0]:     return forward_call(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/home/MegatronLM/megatron/core/distributed/distributed_data_parallel.py"", line 202, in forward [rank0]:     return self.module(*inputs, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/home/pytorch/torch/nn/modules/module.py"", line 1532, in _wrapped_call_impl [rank0]:     return self._call_impl(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/home/pytorch/torch/nn/modules/module.py"", line 1541, in _call_impl [rank0]:     return forward_call(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/home/MegatronLM/megatron/legacy/model/module.py"", line 190, in forward [rank0]:     outputs = self.module(*inputs, **kwargs) [rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/home/pytorch/torch/nn/modules/module.py"", line 1532, in _wrapped_call_impl [rank0]:     return self._call_impl(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/home/pytorch/torch/nn/modules/module.py"", line 1541, in _call_impl [rank0]:     return forward_call(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/home/MegatronLM/megatron/core/models/T5/t5_model.py"", line 273, in forward [rank0]:     decoder_hidden_states = self.decoder( [rank0]:                             ^^^^^^^^^^^^^ [rank0]:   File ""/home/pytorch/torch/nn/modules/module.py"", line 1532, in _wrapped_call_impl [rank0]:     return self._call_impl(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/home/pytorch/torch/nn/modules/module.py"", line 1541, in _call_impl [rank0]:     return forward_call(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/home/MegatronLM/megatron/core/transformer/transformer_block.py"", line 383, in forward [rank0]:     hidden_states, context = layer( [rank0]:                              ^^^^^^ [rank0]:   File ""/home/pytorch/torch/nn/modules/module.py"", line 1532, in _wrapped_call_impl [rank0]:     return self._call_impl(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/home/pytorch/torch/nn/modules/module.py"", line 1541, in _call_impl [rank0]:     return forward_call(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/home/MegatronLM/megatron/core/transformer/transformer_layer.py"", line 200, in forward [rank0]:     attention_output_with_bias = self.cross_attention( [rank0]:                                  ^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/home/pytorch/torch/nn/modules/module.py"", line 1532, in _wrapped_call_impl [rank0]:     return self._call_impl(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/home/pytorch/torch/nn/modules/module.py"", line 1541, in _call_impl [rank0]:     return forward_call(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/home/MegatronLM/megatron/core/transformer/attention.py"", line 315, in forward [rank0]:     core_attn_out = self.core_attention( [rank0]:                     ^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/home/pytorch/torch/nn/modules/module.py"", line 1532, in _wrapped_call_impl [rank0]:     return self._call_impl(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/home/pytorch/torch/nn/modules/module.py"", line 1541, in _call_impl [rank0]:     return forward_call(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/home/MegatronLM/megatron/core/transformer/custom_layers/transformer_engine.py"", line 534, in forward [rank0]:     core_attn_out = super().forward( [rank0]:                     ^^^^^^^^^^^^^^^^ [rank0]:   File ""/home/miniconda3/lib/python3.12/sitepackages/transformer_engine/pytorch/attention.py"", line 3862, in forward [rank0]:     return self.flash_attention(query_layer, [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/home/pytorch/torch/nn/modules/module.py"", line 1532, in _wrapped_call_impl [rank0]:     return self._call_impl(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/home/pytorch/torch/nn/modules/module.py"", line 1541, in _call_impl [rank0]:     return forward_call(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/home/miniconda3/lib/python3.12/sitepackages/transformer_engine/pytorch/attention.py"", line 2106, in forward [rank0]:     cu_seqlens_q, indices_q = get_cu_seqlens_and_indices( [rank0]:                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/home/miniconda3/lib/python3.12/sitepackages/transformer_engine/pytorch/attention.py"", line 243, in get_cu_seqlens_and_indices [rank0]:     bs, seqlen = mask.shape [rank0]:     ^^^^^^^^^^ [rank0]: ValueError: too many values to unpack (expected 2) ``` **Environment (please complete the following information):**   MegatronLM commit ID: 44cc262   PyTorch version: 2.4.0a0+git4f14282   CUDA version: 12.2   NCCL version: 2.21.5",2024-08-13T21:10:38Z,stale,open,0,6,https://github.com/NVIDIA/Megatron-LM/issues/1005,"Hi  , this is about the transformer engine version you are using and the type of attention you are using. There are a couple of ways to fix this :  You could set NVTE_FLASH_ATTN=0 and NVTE_FUSED_ATTN=0 , that way it goes through the unfused path, and so it accepts [b ,1,s ,s ] mask.  The other way to do it will be to just change the mask to be [b,1,1,s] if possible, if its just a padding mask .    Can you look if we can send [b,1,1,s] as the attention mask ? You might have to do something like what bert is doing. ",  Can you share transformer engine verison  pip list | grep transformer,"  You just have to switch the encoder mask alone to [b,1,1,s] for any transformer engine version greater than 1.5 I think (Maybe 1.7) ", Transformer engine has version 1.7.0+4e7caa1. Setting NVTE_FLASH_ATTN=0 and NVTE_FUSED_ATTN=0 resolves the original issue. Thanks!,  One more thing that you can try send attn_mask_type as 'arbitrary'. Anyway   is looking into this now and he will raise a fix soon,Marking as stale. No activity in 60 days.
1195343015,fix torch.py when saving checkpoints using pytorch2.4,"issue CC([BUG] bugs when using pytorch2.4.0 to run run_simple_mcore_train_loop.py )  this bug is due to the calling of function _add_non_coordinator_iobytes_request() in create_local_plan() https://github.com/NVIDIA/MegatronLM/blob/6bf8448ba065a0a37b2b874f49fd65ca9547b5c0/megatron/core/dist_checkpointing/strategies/torch.pyL445L447 https://github.com/NVIDIA/MegatronLM/blob/6bf8448ba065a0a37b2b874f49fd65ca9547b5c0/megatron/core/dist_checkpointing/strategies/torch.pyL469L474 function create_default_local_save_plan() is different between Pytorch 2.3 and 2.4  PyTorch 2.3 https://github.com/pytorch/pytorch/blob/63d5e9221bedd1546b7d364b5ce4171547db12a9/torch/distributed/checkpoint/default_planner.pyL259 ```python def create_default_local_save_plan(     state_dict: Dict[str, Any], is_coordinator: bool ) > SavePlan:     """"""     Create the ``SavePlan`` used by DefaultSavePlanner.     On noncoordinator ranks, this function ignores tensors and nontensor objects,     only producing writes for ShardedTensor objects.     On the coordinator rank, produce writes for all values.     """"""     requests = []     for fqn, obj in state_dict.items():          Since DTensor supports submesh, adding extra check to ensure _create_write_items()          gets called only when the current rank is part of the mesh for the corresponding DTensor.         if isinstance(obj, DTensor):             if obj.device_mesh.get_coordinate() is not None:                 requests += _create_write_items(fqn, obj)         elif isinstance(obj, (torch.Tensor)) or is_coordinator:             requests += _create_write_items(fqn, obj)     return SavePlan(requests) ```  Pytorch 2.4 https://github.com/pytorch/pytorch/blob/f6fb80b0f906ebc89e4bfcfe4e322585cbbcecb8/torch/distributed/checkpoint/default_planner.pyL344 ```python def create_default_local_save_plan(     state_dict: Dict[str, Any], is_coordinator: bool ) > SavePlan:     """"""     Create the ``SavePlan`` used by DefaultSavePlanner.     On noncoordinator ranks, this function ignores tensors and nontensor objects,     only producing writes for ShardedTensor objects.     On the coordinator rank, produce writes for all values.     """"""     requests = []     for fqn, obj in state_dict.items():          Since DTensor supports submesh, adding extra check to ensure _create_write_items()          gets called only when the current rank is part of the mesh for the corresponding DTensor.         if isinstance(obj, DTensor):             if obj.device_mesh.get_coordinate() is not None:                 requests += _create_write_items(fqn, obj)         else:              For the plain tensor and nontensor values, add the request for all              the ranks. Coordinator will decides whether to deduplicate the              values based on the keys.             requests += _create_write_items(fqn, obj)     return SavePlan(requests) ```  This bug happens when using pytorch2.4. Because in pytorch 2.4, `is_coordinator` condition check is removed. `non_coordinator` requests haved been added in ""plan"" by PyTorch. So it's no need to call `_add_non_coordinator_iobytes_request` in Megatron core.",2024-08-13T13:44:28Z,,closed,2,3,https://github.com/NVIDIA/Megatron-LM/issues/1004,CC  ,Thanks  for providing pointers to this silent change on the PyT side! Let me incorporate your fix,Merged and available in MCore v0.9 (in a slightly different formulation: https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/dist_checkpointing/strategies/torch.pyL460)
1195343015,[BUG] bugs when using pytorch2.4.0 to run run_simple_mcore_train_loop.py ,"**Describe the bug** bugs when using pytorch2.4.0 to run run_simple_mcore_train_loop.py  **To Reproduce** ```python  PYTHONPATH=$PYTHON_PATH:./megatron torchrun nprocpernode 2 examples/run_simple_mcore_train_loop.py ```  **Stack trace/logs** ```python [rank1]: Traceback (most recent call last): [rank1]:   File ""/workspace/megatron/examples/run_simple_mcore_train_loop.py"", line 152, in  [rank1]:     save_distributed_checkpoint(gpt_model=gpt_model, checkpoint_path=ckpt_path) [rank1]:   File ""/workspace/megatron/examples/run_simple_mcore_train_loop.py"", line 109, in save_distributed_checkpoint [rank1]:     dist_checkpointing.save(sharded_state_dict=sharded_state_dict, checkpoint_dir=checkpoint_path) [rank1]:   File ""/workspace/megatron/megatron/core/dist_checkpointing/serialization.py"", line 393, in save [rank1]:     sharded_strategy.save(sharded_state_dict, checkpoint_dir) [rank1]:   File ""/workspace/megatron/megatron/core/dist_checkpointing/strategies/base.py"", line 180, in save [rank1]:     async_request = self.async_save(sharded_state_dict, checkpoint_dir) [rank1]:   File ""/workspace/megatron/megatron/core/dist_checkpointing/strategies/torch.py"", line 632, in async_save [rank1]:     ) = save_state_dict_async_plan( [rank1]:   File ""/workspace/megatron/megatron/core/dist_checkpointing/strategies/state_dict_saver.py"", line 108, in save_state_dict_async_plan [rank1]:     central_plan = dist_wrapper.reduce_scatter(""plan"", local_step, global_step) [rank1]:   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/checkpoint/utils.py"", line 191, in reduce_scatter [rank1]:     raise result [rank1]: torch.distributed.checkpoint.api.CheckpointException: CheckpointException ranks:dict_keys([0]) [rank1]: Traceback (most recent call last): (RANK 0) [rank1]:   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/checkpoint/utils.py"", line 179, in reduce_scatter [rank1]:     reduce_fun(cast(List[T], all_data)), [rank1]:   File ""/workspace/megatron/megatron/core/dist_checkpointing/strategies/state_dict_saver.py"", line 97, in global_step [rank1]:     all_local_plans, global_metadata = planner.create_global_plan(all_local_plans) [rank1]:   File ""/workspace/megatron/megatron/core/dist_checkpointing/strategies/torch.py"", line 465, in create_global_plan [rank1]:     global_plan, metadata = super().create_global_plan(all_plans) [rank1]:   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/checkpoint/default_planner.py"", line 109, in create_global_plan [rank1]:     global_plan, metadata = create_default_global_save_plan(all_plans) [rank1]:   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/checkpoint/default_planner.py"", line 388, in create_default_global_save_plan [rank1]:     assert item.index.fqn not in md [rank1]: AssertionError ```  **Environment (please complete the following information):**   MegatronLM commit ID 6bf8448   nvcr.io/nvidia/pytorch:24.07py3 **Proposed fix** PR https://github.com/NVIDIA/MegatronLM/pull/1004",2024-08-13T12:59:17Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1003,    Is adding this fix to the library. Thanks
SefaZeng,[QUESTION] Will the data get re-shuffled if the sequence length is modified during training?,"**Your question** If I change the `seqlength` during training, like 4096 > 8192, or 4096 > 1024, will the data loader reset the order of the dataset or continue with the old order? I want to change the `seqlength` after training maybe 100k steps, and I need the model to continue train with the left data, will MegatronLM do this by default?",2024-08-13T09:52:40Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1002
yu-depend,[QUESTION]Why is the operator of computation slower when computation overlaps with communication,Why is the operator of computation slower when computation overlaps with communication,2024-08-13T09:42:37Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/1001
ZeroAGI,[QUESTION] Training Llama3 70B on 16 x A100 only achieves low throughput of 20 TFLOPS,"**Your question** Machine: 2 nodes * 8 A100 TP=8 PP=2 DP=1 CP=1 seq_length=4096 micro_batch_size=1 global_batch_size=1 enable recompute activation, flash attention, distribute optimizer Megatron version: core_v0.7.0 Thanks for you help!",2024-08-13T08:47:09Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1000,"arguments: ``` using world size: 16, dataparallel size: 1, contextparallel size: 1 tensormodelparallel size: 8, pipelinemodelparallel size: 2  WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:Llama3Tokenizer WARNING: Setting args.overlap_p2p_comm to False since noninterleaved schedule does not support overlapping p2p communication using torch.float32 for parameters ...  arguments    accumulate_allreduce_grads_in_fp32 .............. False   adam_beta1 ...................................... 0.9   adam_beta2 ...................................... 0.95   adam_eps ........................................ 1e08   add_bias_linear ................................. False   add_position_embedding .......................... False   add_qkv_bias .................................... False   adlr_autoresume ................................. False   adlr_autoresume_interval ........................ 1000   apply_layernorm_1p .............................. False   apply_query_key_layer_scaling ................... False   apply_residual_connection_post_layernorm ........ False   apply_rope_fusion ............................... True   async_save ...................................... None   async_tensor_model_parallel_allreduce ........... False   attention_dropout ............................... 0.1   attention_softmax_in_fp32 ....................... True   auto_detect_ckpt_format ......................... False   barrier_with_L1_time ............................ True   bert_binary_head ................................ True   bert_embedder_type .............................. megatron   bert_load ....................................... None   bf16 ............................................ False   bias_dropout_fusion ............................. True   bias_gelu_fusion ................................ False   bias_swiglu_fusion .............................. True   biencoder_projection_dim ........................ 0   biencoder_shared_query_context_model ............ False   block_data_path ................................. None   calculate_per_token_loss ........................ False   check_for_nan_in_loss_and_grad .................. True   check_weight_hash_across_dp_replicas_interval ... None   ckpt_assume_constant_structure .................. False   ckpt_fully_parallel_load ........................ False   ckpt_fully_parallel_save ........................ False   ckpt_step ....................................... None   classes_fraction ................................ 1.0   clip_grad ....................................... 1.0   clone_scatter_output_in_embedding ............... True   consumed_train_samples .......................... 0   consumed_valid_samples .......................... 0   context_parallel_size ........................... 1   create_attention_mask_in_dataloader ............. True   data_cache_path ................................. None   data_parallel_random_init ....................... False   data_parallel_size .............................. 1   data_path ....................................... ['/mnt/project/new/MegatronLM/myllama_text_document']   data_per_class_fraction ......................... 1.0   data_sharding ................................... True   dataloader_type ................................. single   ddp_bucket_size ................................. None   decoder_num_layers .............................. None   decoder_seq_length .............................. None   decoupled_lr .................................... None   decoupled_min_lr ................................ None   delay_grad_reduce ............................... True   delay_param_gather .............................. False   dino_bottleneck_size ............................ 256   dino_freeze_last_layer .......................... 1   dino_head_hidden_size ........................... 2048   dino_local_crops_number ......................... 10   dino_local_img_size ............................. 96   dino_norm_last_layer ............................ False   dino_teacher_temp ............................... 0.07   dino_warmup_teacher_temp ........................ 0.04   dino_warmup_teacher_temp_epochs ................. 30   disable_straggler_on_startup .................... False   dist_ckpt_format ................................ torch_dist   distribute_saved_activations .................... False   distributed_backend ............................. nccl   distributed_timeout_minutes ..................... 60   embedding_path .................................. None   empty_unused_memory_level ....................... 0   enable_one_logger ............................... False   encoder_num_layers .............................. 80   encoder_seq_length .............................. 4096   end_weight_decay ................................ 0.1   eod_mask_loss ................................... False   eval_interval ................................... 1000   eval_iters ...................................... 0   evidence_data_path .............................. None   exit_duration_in_mins ........................... None   exit_interval ................................... None   exit_on_missing_checkpoint ...................... True   exit_signal_handler ............................. False   expert_model_parallel_size ...................... 1   ffn_hidden_size ................................. 28672   finetune ........................................ False   fp16 ............................................ False   fp16_lm_cross_entropy ........................... False   fp32_residual_connection ........................ False   fp8 ............................................. None   fp8_amax_compute_algo ........................... most_recent   fp8_amax_history_len ............................ 1   fp8_interval .................................... 1   fp8_margin ...................................... 0   fp8_wgrad ....................................... True   global_batch_size ............................... 1   gradient_accumulation_fusion .................... True   group_query_attention ........................... True   head_lr_mult .................................... 1.0   hidden_dropout .................................. 0.1   hidden_size ..................................... 8192   hysteresis ...................................... 2   ict_head_size ................................... None   ict_load ........................................ None   img_h ........................................... 224   img_w ........................................... 224   indexer_batch_size .............................. 128   indexer_log_interval ............................ 1000   inference_batch_times_seqlen_threshold .......... 512   init_method_std ................................. 0.006   init_method_xavier_uniform ...................... False   initial_loss_scale .............................. 4294967296   iter_per_epoch .................................. 1250   iteration ....................................... 1   kv_channels ..................................... 128   lazy_mpu_init ................................... None   load ............................................ /mnt/model/megatron/pp2/MetaLlama3.170B   local_rank ...................................... None   log_batch_size_to_tensorboard ................... False   log_interval .................................... 1   log_learning_rate_to_tensorboard ................ True   log_loss_scale_to_tensorboard ................... True   log_memory_to_tensorboard ....................... False   log_num_zeros_in_grad ........................... False   log_params_norm ................................. False   log_progress .................................... False   log_straggler ................................... False   log_throughput .................................. True   log_timers_to_tensorboard ....................... False   log_validation_ppl_to_tensorboard ............... False   log_world_size_to_tensorboard ................... False   loss_scale ...................................... None   loss_scale_window ............................... 1000   lr .............................................. 6e05   lr_decay_iters .................................. 430000   lr_decay_samples ................................ None   lr_decay_style .................................. cosine   lr_warmup_fraction .............................. 0.001   lr_warmup_init .................................. 0.0   lr_warmup_iters ................................. 0   lr_warmup_samples ............................... 0   make_vocab_size_divisible_by .................... 128   manual_gc ....................................... False   manual_gc_eval .................................. True   manual_gc_interval .............................. 0   mask_factor ..................................... 1.0   mask_prob ....................................... 0.15   mask_type ....................................... random   masked_softmax_fusion ........................... False   max_position_embeddings ......................... 4096   max_tokens_to_oom ............................... 12000   merge_file ...................................... None   micro_batch_size ................................ 1   min_loss_scale .................................. 1.0   min_lr .......................................... 6e06   mmap_bin_files .................................. True   mock_data ....................................... False   moe_aux_loss_coeff .............................. 0.0   moe_expert_capacity_factor ...................... None   moe_extended_tp ................................. False   moe_grouped_gemm ................................ False   moe_input_jitter_eps ............................ None   moe_layer_recompute ............................. False   moe_pad_expert_input_to_capacity ................ False   moe_per_layer_logging ........................... False   moe_router_load_balancing_type .................. aux_loss   moe_router_topk ................................. 2   moe_token_dispatcher_type ....................... allgather   moe_token_drop_policy ........................... probs   moe_z_loss_coeff ................................ None   nccl_communicator_config_path ................... None   no_load_optim ................................... True   no_load_rng ..................................... True   no_persist_layer_norm ........................... False   no_save_optim ................................... None   no_save_rng ..................................... None   norm_epsilon .................................... 1e05   normalization ................................... RMSNorm   num_attention_heads ............................. 64   num_channels .................................... 3   num_classes ..................................... 1000   num_dataset_builder_threads ..................... 1   num_experts ..................................... None   num_layers ...................................... 80   num_layers_per_virtual_pipeline_stage ........... None   num_query_groups ................................ 8   num_workers ..................................... 8   one_logger_entity ............................... hwinf_dcm   one_logger_project .............................. e2etracking   one_logger_run_name ............................. None   onnx_safe ....................................... None   openai_gelu ..................................... False   optimizer ....................................... adam   output_bert_embeddings .......................... False   overlap_grad_reduce ............................. True   overlap_p2p_comm ................................ False   overlap_param_gather ............................ True   override_opt_param_scheduler .................... False   padded_vocab_size ............................... 129024   params_dtype .................................... torch.float32   patch_dim ....................................... 16   perform_initialization .......................... True   pipeline_model_parallel_size .................... 2   pipeline_model_parallel_split_rank .............. None   position_embedding_type ......................... rope   pretrained_checkpoint ........................... None   profile ......................................... False   profile_ranks ................................... [0]   profile_step_end ................................ 12   profile_step_start .............................. 10   qk_layernorm .................................... False   query_in_block_prob ............................. 0.1   rampup_batch_size ............................... None   rank ............................................ 0   recompute_granularity ........................... full   recompute_method ................................ uniform   recompute_num_layers ............................ 1   reset_attention_mask ............................ False   reset_position_ids .............................. False   retriever_report_topk_accuracies ................ []   retriever_score_scaling ......................... False   retriever_seq_length ............................ 256   retro_add_retriever ............................. False   retro_attention_gate ............................ 1   retro_cyclic_train_iters ........................ None   retro_encoder_attention_dropout ................. 0.1   retro_encoder_hidden_dropout .................... 0.1   retro_encoder_layers ............................ 2   retro_num_neighbors ............................. 2   retro_num_retrieved_chunks ...................... 2   retro_project_dir ............................... None   retro_verify_neighbor_count ..................... True   rotary_interleaved .............................. False   rotary_percent .................................. 1.0   rotary_seq_len_interpolation_factor ............. None   sample_rate ..................................... 1.0   save ............................................ /mnt/model/megatron_save/MetaLlama3.170B   save_interval ................................... 500   scatter_gather_tensors_in_pipeline .............. True   seed ............................................ 1234   seq_length ...................................... 4096   sequence_parallel ............................... True   sgd_momentum .................................... 0.9   short_seq_prob .................................. 0.1   skip_train ...................................... False   spec ............................................ None   split ........................................... 969, 30, 1   squared_relu .................................... False   standalone_embedding_stage ...................... False   start_weight_decay .............................. 0.1   straggler_ctrlr_port ............................ 65535   straggler_minmax_count .......................... 1   swiglu .......................................... True   swin_backbone_type .............................. tiny   tensor_model_parallel_size ...................... 8   tensorboard_dir ................................. ./tensorboard   tensorboard_log_interval ........................ 1   tensorboard_queue_size .......................... 1000   test_data_path .................................. None   test_mode ....................................... False   timing_log_level ................................ 0   timing_log_option ............................... minmax   titles_data_path ................................ None   tokenizer_model ................................. /mnt/model/MetaLlama3.170B/original/tokenizer.model   tokenizer_type .................................. Llama3Tokenizer   tp_comm_bulk_dgrad .............................. True   tp_comm_bulk_wgrad .............................. True   tp_comm_overlap ................................. False   tp_comm_overlap_ag .............................. True   tp_comm_overlap_cfg ............................. None   tp_comm_overlap_rs .............................. True   tp_comm_overlap_rs_dgrad ........................ False   tp_comm_split_ag ................................ True   tp_comm_split_rs ................................ True   train_data_path ................................. None   train_iters ..................................... 1000   train_samples ................................... None   transformer_impl ................................ transformer_engine   transformer_pipeline_model_parallel_size ........ 2   untie_embeddings_and_output_weights ............. True   use_checkpoint_args ............................. True   use_checkpoint_opt_param_scheduler .............. False   use_cpu_initialization .......................... None   use_dist_ckpt ................................... False   use_distributed_optimizer ....................... True   use_flash_attn .................................. True   use_mcore_models ................................ True   use_one_sent_docs ............................... False   use_ring_exchange_p2p ........................... False   use_rotary_position_embeddings .................. True   use_tp_pp_dp_mapping ............................ False   valid_data_path ................................. None   variable_seq_lengths ............................ False   virtual_pipeline_model_parallel_size ............ None   vision_backbone_type ............................ vit   vision_pretraining .............................. False   vision_pretraining_type ......................... classify   vocab_extra_ids ................................. 0   vocab_file ...................................... None   vocab_size ...................................... None   wandb_exp_name ..................................    wandb_project ...................................    wandb_save_dir ..................................    weight_decay .................................... 0.1   weight_decay_incr_style ......................... constant   world_size ...................................... 16   yaml_cfg ........................................ None  end of arguments  ``` throughput log: ``` [20240813 08:39:36] iteration        2/    1000  ```"
mushan09,Why is gather_output not supported in ColumnParallelLinear when using sequence parallelism?,https://github.com/NVIDIA/MegatronLM/blob/6bf8448ba065a0a37b2b874f49fd65ca9547b5c0/megatron/core/tensor_parallel/layers.pyL907,2024-08-13T06:11:00Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/999,Marking as stale. No activity in 60 days.
KookHoiKim,[BUG] llava pipeline parallel initialization problem,"**Describe the bug** I am currently working with llava model in megatron. I tested tensor parallel and it works well. However, when i set pipeline parallel, it stucks while initialization.  I found that in initialize_model_parallel , `group_gloo = torch.distributed.new_group(ranks, backend=""gloo""` is not passed for rank1 gpu.  I am using 2 A100 gpus , so I set TP=1 .  If anyone have any idea, please help.  Thanks.  FYI, i add NCCL_DEBUG info .  ``` run297066megatron:76841:76841 [0] NCCL INFO Bootstrap : Using eth0:10.15.140.244 run297066megatron:76841:76841 [0] NCCL INFO cudaDriverVersion 12050 run297066megatron:76841:76841 [0] NCCL INFO NCCL version 2.22.3+cuda12.5 > setting tensorboard ... WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it run297066megatron:76841:76991 [0] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libncclnet.so run297066megatron:76841:76991 [0] NCCL INFO P2P plugin IBext_v8 run297066megatron:76841:76991 [0] NCCL INFO NET/IB : No device found. run297066megatron:76841:76991 [0] NCCL INFO NET/IB : No device found. run297066megatron:76841:76991 [0] NCCL INFO NET/Socket : Using [0]eth0:10.15.140.244 run297066megatron:76841:76991 [0] NCCL INFO Using network Socket run297066megatron:76842:76842 [1] NCCL INFO cudaDriverVersion 12050 run297066megatron:76842:76842 [1] NCCL INFO Bootstrap : Using eth0:10.15.140.244 run297066megatron:76842:76842 [1] NCCL INFO NCCL version 2.22.3+cuda12.5 run297066megatron:76842:76996 [1] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libncclnet.so run297066megatron:76842:76996 [1] NCCL INFO P2P plugin IBext_v8 run297066megatron:76842:76996 [1] NCCL INFO NET/IB : No device found. run297066megatron:76842:76996 [1] NCCL INFO NET/IB : No device found. run297066megatron:76842:76996 [1] NCCL INFO NET/Socket : Using [0]eth0:10.15.140.244 run297066megatron:76842:76996 [1] NCCL INFO Using network Socket run297066megatron:76842:76996 [1] NCCL INFO ncclCommInitRank comm 0x55db1283efc0 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId cb000 commId 0x279e4fc0fff67dc0  Init START run297066megatron:76841:76991 [0] NCCL INFO ncclCommInitRank comm 0x5577888c8650 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 7000 commId 0x279e4fc0fff67dc0  Init START run297066megatron:76841:76991 [0] NCCL INFO Setting affinity for GPU 0 to ffff0000,00000000,00000000,00000000,ffff0000,00000000 run297066megatron:76842:76996 [1] NCCL INFO Setting affinity for GPU 1 to ffff0000,00000000,00000000,00000000,ffff0000,00000000,00000000 run297066megatron:76841:76991 [0] NCCL INFO comm 0x5577888c8650 rank 0 nRanks 2 nNodes 1 localRanks 2 localRank 0 MNNVL 0 run297066megatron:76842:76996 [1] NCCL INFO comm 0x55db1283efc0 rank 1 nRanks 2 nNodes 1 localRanks 2 localRank 1 MNNVL 0 run297066megatron:76841:76991 [0] NCCL INFO Channel 00/24 :    0   1 run297066megatron:76842:76996 [1] NCCL INFO Trees [0] 1/1/1>1>0 [1] 1/1/1>1>0 [2] 1/1/1>1>0 [3] 1/1/1>1>0 [4] 1/1/1>1>0 [5] 1/1/1>1>0 [6] 0/1/1>1>1 [7] 0/1/1>1>1 [8] 0/1/1>1>1 [9] 0/1/1>1>1 [10] 0/1/1>1>1 [11] 0/1/1>1>1 [12] 1/1/1>1>0 [13] 1/1/1>1>0 [14] 1/1/1>1>0 [15] 1/1/1>1>0 [16] 1/1/1>1>0 [17] 1/1/1>1>0 [18] 0/1/1>1>1 [19] 0/1/1>1>1 [20] 0/1/1>1>1 [21] 0/1/1>1>1 [22] 0/1/1>1>1 [23] 0/1/1>1>1 run297066megatron:76841:76991 [0] NCCL INFO Channel 01/24 :    0   1 run297066megatron:76842:76996 [1] NCCL INFO P2P Chunksize set to 524288 run297066megatron:76841:76991 [0] NCCL INFO Channel 02/24 :    0   1 run297066megatron:76841:76991 [0] NCCL INFO Channel 03/24 :    0   1 run297066megatron:76841:76991 [0] NCCL INFO Channel 04/24 :    0   1 run297066megatron:76841:76991 [0] NCCL INFO Channel 05/24 :    0   1 run297066megatron:76841:76991 [0] NCCL INFO Channel 06/24 :    0   1 run297066megatron:76841:76991 [0] NCCL INFO Channel 07/24 :    0   1 run297066megatron:76841:76991 [0] NCCL INFO Channel 08/24 :    0   1 run297066megatron:76841:76991 [0] NCCL INFO Channel 09/24 :    0   1 run297066megatron:76841:76991 [0] NCCL INFO Channel 10/24 :    0   1 run297066megatron:76841:76991 [0] NCCL INFO Channel 11/24 :    0   1 run297066megatron:76841:76991 [0] NCCL INFO Channel 12/24 :    0   1 run297066megatron:76841:76991 [0] NCCL INFO Channel 13/24 :    0   1 run297066megatron:76841:76991 [0] NCCL INFO Channel 14/24 :    0   1 run297066megatron:76841:76991 [0] NCCL INFO Channel 15/24 :    0   1 run297066megatron:76841:76991 [0] NCCL INFO Channel 16/24 :    0   1 run297066megatron:76841:76991 [0] NCCL INFO Channel 17/24 :    0   1 run297066megatron:76841:76991 [0] NCCL INFO Channel 18/24 :    0   1 run297066megatron:76841:76991 [0] NCCL INFO Channel 19/24 :    0   1 run297066megatron:76841:76991 [0] NCCL INFO Channel 20/24 :    0   1 run297066megatron:76841:76991 [0] NCCL INFO Channel 21/24 :    0   1 run297066megatron:76841:76991 [0] NCCL INFO Channel 22/24 :    0   1 run297066megatron:76841:76991 [0] NCCL INFO Channel 23/24 :    0   1 run297066megatron:76841:76991 [0] NCCL INFO Trees [0] 1/1/1>0>1 [1] 1/1/1>0>1 [2] 1/1/1>0>1 [3] 1/1/1>0>1 [4] 1/1/1>0>1 [5] 1/1/1>0>1 [6] 1/1/1>0>1 [7] 1/1/1>0>1 [8] 1/1/1>0>1 [9] 1/1/1>0>1 [10] 1/1/1>0>1 [11] 1/1/1>0>1 [12] 1/1/1>0>1 [13] 1/1/1>0>1 [14] 1/1/1>0>1 [15] 1/1/1>0>1 [16] 1/1/1>0>1 [17] 1/1/1>0>1 [18] 1/1/1>0>1 [19] 1/1/1>0>1 [20] 1/1/1>0>1 [21] 1/1/1>0>1 [22] 1/1/1>0>1 [23] 1/1/1>0>1 run297066megatron:76841:76991 [0] NCCL INFO P2P Chunksize set to 524288 run297066megatron:76842:76996 [1] NCCL INFO threadThresholds 8/8/64  512 run297066megatron:76841:76991 [0] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer run297066megatron:76841:76991 [0] NCCL INFO CC Off, MultiGPU CC Off, workFifoBytes 1048576 run297066megatron:76842:76996 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol. run297066megatron:76842:76996 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead. run297066megatron:76842:76996 [1] NCCL INFO ncclCommInitRank comm 0x55db1283efc0 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId cb000 commId 0x279e4fc0fff67dc0  Init COMPLETE run297066megatron:76841:76991 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol. run297066megatron:76842:76996 [1] NCCL INFO Init timings: rank 1 nranks 2 total 0.21 (kernels 0.10, bootstrap 0.01, allgathers 0.00, topo 0.08, graphs 0.00, connections 0.02, rest 0.01) run297066megatron:76841:76991 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead. run297066megatron:76841:76991 [0] NCCL INFO ncclCommInitRank comm 0x5577888c8650 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 7000 commId 0x279e4fc0fff67dc0  Init COMPLETE run297066megatron:76841:76991 [0] NCCL INFO Init timings: rank 0 nranks 2 total 0.48 (kernels 0.11, bootstrap 0.26, allgathers 0.02, topo 0.07, graphs 0.00, connections 0.02, rest 0.01) run297066megatron:76842:76842 [1] NCCL INFO Rank 1 has color with NCCL_SPLIT_NOCOLOR, not creating a new communicator run297066megatron:76841:76841 [0] NCCL INFO Rank 0 has color with NCCL_SPLIT_NOCOLOR, not creating a new communicator run297066megatron:76842:76842 [1] NCCL INFO Rank 1 has color with NCCL_SPLIT_NOCOLOR, not creating a new communicator run297066megatron:76841:76841 [0] NCCL INFO Rank 0 has color with NCCL_SPLIT_NOCOLOR, not creating a new communicator run297066megatron:76842:76842 [1] NCCL INFO Rank 1 has color with NCCL_SPLIT_NOCOLOR, not creating a new communicator run297066megatron:76841:76841 [0] NCCL INFO Rank 0 has color with NCCL_SPLIT_NOCOLOR, not creating a new communicator run297066megatron:76841:76841 [0] NCCL INFO Rank 0 has color with NCCL_SPLIT_NOCOLOR, not creating a new communicator run297066megatron:76842:76842 [1] NCCL INFO Rank 1 has color with NCCL_SPLIT_NOCOLOR, not creating a new communicator run297066megatron:76841:76841 [0] NCCL INFO Rank 0 has color with NCCL_SPLIT_NOCOLOR, not creating a new communicator run297066megatron:76842:76842 [1] NCCL INFO Rank 1 has color with NCCL_SPLIT_NOCOLOR, not creating a new communicator run297066megatron:76842:76842 [1] NCCL INFO Rank 1 has color with NCCL_SPLIT_NOCOLOR, not creating a new communicator run297066megatron:76841:76841 [0] NCCL INFO Rank 0 has color with NCCL_SPLIT_NOCOLOR, not creating a new communicator run297066megatron:76842:76842 [1] NCCL INFO Rank 1 has color with NCCL_SPLIT_NOCOLOR, not creating a new communicator run297066megatron:76841:76841 [0] NCCL INFO Rank 0 has color with NCCL_SPLIT_NOCOLOR, not creating a new communicator run297066megatron:76842:76842 [1] NCCL INFO Rank 1 has color with NCCL_SPLIT_NOCOLOR, not creating a new communicator run297066megatron:76841:76841 [0] NCCL INFO Rank 0 has color with NCCL_SPLIT_NOCOLOR, not creating a new communicator run297066megatron:76841:76841 [0] NCCL INFO Rank 0 has color with NCCL_SPLIT_NOCOLOR, not creating a new communicator run297066megatron:76842:76842 [1] NCCL INFO Rank 1 has color with NCCL_SPLIT_NOCOLOR, not creating a new communicator run297066megatron:76841:76841 [0] NCCL INFO Rank 0 has color with NCCL_SPLIT_NOCOLOR, not creating a new communicator run297066megatron:76842:76842 [1] NCCL INFO Rank 1 has color with NCCL_SPLIT_NOCOLOR, not creating a new communicator run297066megatron:76842:76842 [1] NCCL INFO Rank 1 has color with NCCL_SPLIT_NOCOLOR, not creating a new communicator run297066megatron:76841:76841 [0] NCCL INFO Rank 0 has color with NCCL_SPLIT_NOCOLOR, not creating a new communicator run297066megatron:76842:76842 [1] NCCL INFO Rank 1 has color with NCCL_SPLIT_NOCOLOR, not creating a new communicator run297066megatron:76841:76841 [0] NCCL INFO Rank 0 has color with NCCL_SPLIT_NOCOLOR, not creating a new communicator run297066megatron:76841:76841 [0] NCCL INFO Rank 0 has color with NCCL_SPLIT_NOCOLOR, not creating a new communicator run297066megatron:76842:76842 [1] NCCL INFO Rank 1 has color with NCCL_SPLIT_NOCOLOR, not creating a new communicator run297066megatron:76842:76842 [1] NCCL INFO Rank 1 has color with NCCL_SPLIT_NOCOLOR, not creating a new communicator > initialized tensor model parallel with size 1 > initialized pipeline model parallel with size 2 > setting random seeds to 1234 ... > compiling dataset index builder ... make: Entering directory '/khoi.kim/workspace/code/MegatronLM/megatron/core/datasets' make: Nothing to be done for 'default'. make: Leaving directory '/khoi.kim/workspace/code/MegatronLM/megatron/core/datasets' >>> done with dataset index builder. Compilation time: 0.118 seconds WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations. > compiling and loading fused kernels ... run297066megatron:76841:77141 [0] NCCL INFO Channel 00/0 : 0[0] > 1[1] via P2P/CUMEM/read run297066megatron:76841:77141 [0] NCCL INFO Channel 01/0 : 0[0] > 1[1] via P2P/CUMEM/read run297066megatron:76841:77141 [0] NCCL INFO Channel 02/0 : 0[0] > 1[1] via P2P/CUMEM/read run297066megatron:76841:77141 [0] NCCL INFO Channel 03/0 : 0[0] > 1[1] via P2P/CUMEM/read run297066megatron:76841:77141 [0] NCCL INFO Channel 04/0 : 0[0] > 1[1] via P2P/CUMEM/read run297066megatron:76841:77141 [0] NCCL INFO Channel 05/0 : 0[0] > 1[1] via P2P/CUMEM/read run297066megatron:76841:77141 [0] NCCL INFO Channel 06/0 : 0[0] > 1[1] via P2P/CUMEM/read run297066megatron:76841:77141 [0] NCCL INFO Channel 07/0 : 0[0] > 1[1] via P2P/CUMEM/read run297066megatron:76841:77141 [0] NCCL INFO Channel 08/0 : 0[0] > 1[1] via P2P/CUMEM/read run297066megatron:76841:77141 [0] NCCL INFO Channel 09/0 : 0[0] > 1[1] via P2P/CUMEM/read run297066megatron:76841:77141 [0] NCCL INFO Channel 10/0 : 0[0] > 1[1] via P2P/CUMEM/read run297066megatron:76841:77141 [0] NCCL INFO Channel 11/0 : 0[0] > 1[1] via P2P/CUMEM/read run297066megatron:76841:77141 [0] NCCL INFO Channel 12/0 : 0[0] > 1[1] via P2P/CUMEM/read run297066megatron:76841:77141 [0] NCCL INFO Channel 13/0 : 0[0] > 1[1] via P2P/CUMEM/read run297066megatron:76841:77141 [0] NCCL INFO Channel 14/0 : 0[0] > 1[1] via P2P/CUMEM/read run297066megatron:76841:77141 [0] NCCL INFO Channel 15/0 : 0[0] > 1[1] via P2P/CUMEM/read run297066megatron:76841:77141 [0] NCCL INFO Channel 16/0 : 0[0] > 1[1] via P2P/CUMEM/read run297066megatron:76841:77141 [0] NCCL INFO Channel 17/0 : 0[0] > 1[1] via P2P/CUMEM/read run297066megatron:76841:77141 [0] NCCL INFO Channel 18/0 : 0[0] > 1[1] via P2P/CUMEM/read run297066megatron:76841:77141 [0] NCCL INFO Channel 19/0 : 0[0] > 1[1] via P2P/CUMEM/read run297066megatron:76841:77141 [0] NCCL INFO Channel 20/0 : 0[0] > 1[1] via P2P/CUMEM/read run297066megatron:76841:77141 [0] NCCL INFO Channel 21/0 : 0[0] > 1[1] via P2P/CUMEM/read run297066megatron:76841:77141 [0] NCCL INFO Channel 22/0 : 0[0] > 1[1] via P2P/CUMEM/read run297066megatron:76841:77141 [0] NCCL INFO Channel 23/0 : 0[0] > 1[1] via P2P/CUMEM/read ```",2024-08-12T13:08:00Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/998,"I am working with nvcr.io/nvidia/pytorch:24.07py3 image. It installed torch==2.4.0 .  When comment out the code L.256~257 in initialize.py, initialization does not stuck anymore.  ```          Call the init process         init_process_group_kwargs = {             'backend' : args.distributed_backend,             'world_size': args.world_size,             'rank': args.rank,             'timeout': timedelta(minutes=args.distributed_timeout_minutes),         }          if packaging.version.Version(torch.__version__) >= packaging.version.Version(""2.3.0""):              init_process_group_kwargs['device_id'] = device_id ```",Marking as stale. No activity in 60 days.
hxdtest,[QUESTION] Why and When dose matmul call different kernelsï¼Ÿ,"I run an Megatron sft training(DP=8, PP=1, TP=1), but the speed is very slow. I tried profiling the training process and found that the matmul calls `cutlass::Kernel<cutlass_75_tensorop_bf16_s1688gemm_bf16_256x128_tn_align1`ï¼ˆbatch_size=4ï¼Œ hidden_size=4096ï¼Œsequence_length=4096ï¼‰However if I make a copy and matmul their copiesï¼Œ the matmul call `ampere_bf16_s16816gemm_bf16_128x256_ldg8_f2f_stages_64x3_tn`ï¼Ÿ",2024-08-12T10:11:10Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/997,!image
1195343015,fix _te_version issue in transformer_engine.py get_cpu_offload_context(),"issue CC([BUG] arguments of get_cpu_offload_context() in transformer_engine.py for different version of te)  When the version of transformerengine is higher than 1.8.0 and lower than 1.10.0, some bugs in transformer_engine.py get_cpu_offload_context() https://github.com/NVIDIA/MegatronLM/blob/094d66b488514beaac2106c3e0f9581d27ea9533/megatron/core/transformer/custom_layers/transformer_engine.pyL890L904 ```python [rank1]: Traceback (most recent call last): [rank1]:   File ""/workspace/megatron/examples/run_simple_mcore_train_loop.py"", line 121, in  [rank1]:     gpt_model = model_provider() [rank1]:   File ""/workspace/megatron/examples/run_simple_mcore_train_loop.py"", line 47, in model_provider [rank1]:     gpt_model = GPTModel( [rank1]:   File ""/workspace/megatron/megatron/core/models/gpt/gpt_model.py"", line 101, in __init__ [rank1]:     self.decoder = TransformerBlock( [rank1]:   File ""/workspace/megatron/megatron/core/transformer/transformer_block.py"", line 148, in __init__ [rank1]:     get_cpu_offload_context( [rank1]:   File ""/workspace/megatron/megatron/core/transformer/custom_layers/transformer_engine.py"", line 898, in get_cpu_offload_context [rank1]:     context, sync_func = _get_cpu_offload_context( [rank1]: **TypeError: get_cpu_offload_context() takes from 0 to 4 positional arguments but 5 were given** ```  I have checked the source code of transformerengine.  Only when the version of transformerengine is higher than 1.9.0, function  get_cpu_offload_context() need 5 args, so **1.8.0 in the code should be modified to 1.10.0.dev** ```python   from transformer_engine.pytorch.cpu_offload import (       get_cpu_offload_context as _get_cpu_offload_context,   )   def get_cpu_offload_context(       enabled, num_layers, model_layers, activation_offloading, weight_offloading   ):       if _te_version >= packaging.version.Version(""1.10.0.dev""):           context, sync_func = _get_cpu_offload_context(               enabled, num_layers, model_layers, activation_offloading, weight_offloading           )       else:           context, sync_func = _get_cpu_offload_context(               enabled, num_layers, activation_offloading, weight_offloading           )  ``` ",2024-08-12T09:49:48Z,,closed,1,4,https://github.com/NVIDIA/Megatron-LM/issues/996,supplement:  branch release_v1.9 of TransformerEngine get_cpu_offload_context() https://github.com/NVIDIA/TransformerEngine/blob/ba36f90d05c203787294b7e490af901d79f07d30/transformer_engine/pytorch/cpu_offload.pyL482 branch main(version 1.10.0.dev0) of TransformerEngine get_cpu_offload_context()  https://github.com/NVIDIA/TransformerEngine/blob/def4d1cbfd24e4bb28608d045634a817f638abb7/transformer_engine/pytorch/cpu_offload.pyL438, ,Thanks  for the contribution we will include your PR soon. Thanks again.,Fixed in https://github.com/NVIDIA/MegatronLM/commit/98b43c91d004dec254f1610d9cffae8aff8550f3. 
janEbert,Fix FLOPs calculation,"The denominator of the embedding/logit FLOP calculation was wrong, becoming 12 instead of 16 in the original formula. Expanding the calculation for reference for a model with simple MHA, no GLU, a factor 4 MLP expansion, and no MoE: ``` 12 * gbs * seq_len * num_layers * hidden_size^2 * (     (1 + 1 + (seq_length / hidden_size)) * 1     + (4 * 1 * 1)     + (vocab_size / (2 * num_layers * hidden_size)) ) = 12 * gbs * seq_len * num_layers * hidden_size^2 * (     (2 + (seq_length / hidden_size))     + 4     + (vocab_size / (2 * num_layers * hidden_size)) ) = 12 * gbs * seq_len * num_layers * hidden_size^2 * (     6     + (seq_length / hidden_size)     + (vocab_size / (2 * num_layers * hidden_size)) ) = 24 * gbs * seq_len * num_layers * hidden_size^2 * (     3     + (seq_length / (2 * hidden_size))     + (vocab_size / (4 * num_layers * hidden_size)) ) = 72 * gbs * seq_len * num_layers * hidden_size^2 * (     1     + (seq_length / (6 * hidden_size))     + (vocab_size / (12 * num_layers * hidden_size)) ) ``` However, the last formula should be ``` 72 * gbs * seq_len * num_layers * hidden_size^2 * (     1     + (seq_length / (6 * hidden_size))     + (vocab_size / (16 * num_layers * hidden_size)) ) ``` (i.e., change 12 to 16 in the final line) to be consistent with the MegatronLM PTDP paper's formula. This is achieved with this commit's addition of 2/3 to the embedding/logit calculation denominator.",2024-08-12T08:38:00Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/995,"I'm wrong, the denominator is correct because of the prefactor 72."
wavy-jung,[QUESTION] How to freeze specific modules while training?,"Hi, I want to continually pretrain llamaseries of models, while freezing selfattention layer. I just found out that simply set `requires_grad = False` does not work for the purpose (raised some errors while debugging). How can I update the model parameters except for specific layer? (including the scenario when tensor parallel is applied)",2024-08-12T06:19:31Z,,closed,0,9,https://github.com/NVIDIA/Megatron-LM/issues/994,I found the solution while I was exploring the training script for llava model.,"Hi jung,  I am also facing a similar situation. Could you direct me on finding how to freeze some part of the model while training? ","> Hi jung, >  > I am also facing a similar situation. Could you direct me on finding how to freeze some part of the model while training? Nvm! I found it! ","> > Hi jung, > > I am also facing a similar situation. Could you direct me on finding how to freeze some part of the model while training? >  > Nvm! I found it! May I ask where can i found it","> > > Hi jung, > > > I am also facing a similar situation. Could you direct me on finding how to freeze some part of the model while training? > >  > >  > > Nvm! I found it! >  > May I ask where can i found it Here is what I did before. This is for Mamba. What model you were using? ```     def freeze(         self,          freeze_mamba_model: bool,          freeze_embedding_model:bool,          freeze_output_layer:bool,          unfreeze_decoder_last_layer: bool = False,     ):         """"""         Zixian: Sept 8 19:11pm VERIFIED functionality          Freeze model modules.          Make specific modules nontrainable by setting requires_grad to False for the module's parameters.         Args:         freeze_mamba_model (bool): Freeze the entire decoder module.         freeze_embedding_model (bool): Freeze the embedding module.         freeze_output_layer (bool): Freeze the output layer.         unfreeze_decoder_last_layer (bool): Unfreeze decoder's last layer.          """"""          for l in range(self.model.decoder.num_layers_per_pipeline_rank):              layer_params = count_parameters_in_layer(model, f'decoder.layers.{l}.')         modules = []         if freeze_mamba_model:             modules.append(self.decoder)         if freeze_embedding_model:              Zixian: Prevent second host triggerring errors for hosting model only              if self.pre_process:                 modules.append(self.embedding)         if freeze_output_layer:              Zixian: Prevent second host triggerring errors for hosting model only              if self.post_process:                 modules.append(self.output_layer)          Update Sept 7 22:12pm Not tested yet.           TODO: if does not work, follow similar method in counting params          for module in modules:             print (f' \n\n freezing {module} \n\n')             for param in module.parameters():                 param.requires_grad = False ```","> > > > Hi jung, > > > > I am also facing a similar situation. Could you direct me on finding how to freeze some part of the model while training? > > >  > > >  > > > Nvm! I found it! > >  > >  > > May I ask where can i found it >  > Here is what I did before. This is for Mamba. What model you were using? >  > ``` >     def freeze( >         self,  >         freeze_mamba_model: bool,  >         freeze_embedding_model:bool,  >         freeze_output_layer:bool,  >         unfreeze_decoder_last_layer: bool = False, >     ): >         """""" >         Zixian: Sept 8 19:11pm VERIFIED functionality  >          >         Freeze model modules.  >  >         Make specific modules nontrainable by setting requires_grad to False for the module's parameters. >  >         Args: >         freeze_mamba_model (bool): Freeze the entire decoder module. >         freeze_embedding_model (bool): Freeze the embedding module. >         freeze_output_layer (bool): Freeze the output layer. >         unfreeze_decoder_last_layer (bool): Unfreeze decoder's last layer.  >          >         """""" >          >          for l in range(self.model.decoder.num_layers_per_pipeline_rank): >              layer_params = count_parameters_in_layer(model, f'decoder.layers.{l}.') >         modules = [] >         if freeze_mamba_model: >             modules.append(self.decoder) >         if freeze_embedding_model: >              Zixian: Prevent second host triggerring errors for hosting model only  >             if self.pre_process: >                 modules.append(self.embedding) >         if freeze_output_layer: >              Zixian: Prevent second host triggerring errors for hosting model only  >             if self.post_process: >                 modules.append(self.output_layer) >          >          Update Sept 7 22:12pm Not tested yet.  >          TODO: if does not work, follow similar method in counting params  >         for module in modules: >             print (f' \n\n freezing {module} \n\n') >             for param in module.parameters(): >                 param.requires_grad = False > ``` I am working on Llama 7B, I don't know if this method will work.","> > > > Hi jung, > > > > I am also facing a similar situation. Could you direct me on finding how to freeze some part of the model while training? > > >  > > >  > > > Nvm! I found it! > >  > >  > > May I ask where can i found it >  > Here is what I did before. This is for Mamba. What model you were using? >  > ``` >     def freeze( >         self,  >         freeze_mamba_model: bool,  >         freeze_embedding_model:bool,  >         freeze_output_layer:bool,  >         unfreeze_decoder_last_layer: bool = False, >     ): >         """""" >         Zixian: Sept 8 19:11pm VERIFIED functionality  >          >         Freeze model modules.  >  >         Make specific modules nontrainable by setting requires_grad to False for the module's parameters. >  >         Args: >         freeze_mamba_model (bool): Freeze the entire decoder module. >         freeze_embedding_model (bool): Freeze the embedding module. >         freeze_output_layer (bool): Freeze the output layer. >         unfreeze_decoder_last_layer (bool): Unfreeze decoder's last layer.  >          >         """""" >          >          for l in range(self.model.decoder.num_layers_per_pipeline_rank): >              layer_params = count_parameters_in_layer(model, f'decoder.layers.{l}.') >         modules = [] >         if freeze_mamba_model: >             modules.append(self.decoder) >         if freeze_embedding_model: >              Zixian: Prevent second host triggerring errors for hosting model only  >             if self.pre_process: >                 modules.append(self.embedding) >         if freeze_output_layer: >              Zixian: Prevent second host triggerring errors for hosting model only  >             if self.post_process: >                 modules.append(self.output_layer) >          >          Update Sept 7 22:12pm Not tested yet.  >          TODO: if does not work, follow similar method in counting params  >         for module in modules: >             print (f' \n\n freezing {module} \n\n') >             for param in module.parameters(): >                 param.requires_grad = False > ``` PS where should i call the freeze function","> > > > > Hi jung, > > > > > I am also facing a similar situation. Could you direct me on finding how to freeze some part of the model while training? > > > >  > > > >  > > > > Nvm! I found it! > > >  > > >  > > > May I ask where can i found it > >  > >  > > Here is what I did before. This is for Mamba. What model you were using? > > ``` > >     def freeze( > >         self,  > >         freeze_mamba_model: bool,  > >         freeze_embedding_model:bool,  > >         freeze_output_layer:bool,  > >         unfreeze_decoder_last_layer: bool = False, > >     ): > >         """""" > >         Zixian: Sept 8 19:11pm VERIFIED functionality  > >          > >         Freeze model modules.  > >  > >         Make specific modules nontrainable by setting requires_grad to False for the module's parameters. > >  > >         Args: > >         freeze_mamba_model (bool): Freeze the entire decoder module. > >         freeze_embedding_model (bool): Freeze the embedding module. > >         freeze_output_layer (bool): Freeze the output layer. > >         unfreeze_decoder_last_layer (bool): Unfreeze decoder's last layer.  > >          > >         """""" > >          > >          for l in range(self.model.decoder.num_layers_per_pipeline_rank): > >              layer_params = count_parameters_in_layer(model, f'decoder.layers.{l}.') > >         modules = [] > >         if freeze_mamba_model: > >             modules.append(self.decoder) > >         if freeze_embedding_model: > >              Zixian: Prevent second host triggerring errors for hosting model only  > >             if self.pre_process: > >                 modules.append(self.embedding) > >         if freeze_output_layer: > >              Zixian: Prevent second host triggerring errors for hosting model only  > >             if self.post_process: > >                 modules.append(self.output_layer) > >          > >          Update Sept 7 22:12pm Not tested yet.  > >          TODO: if does not work, follow similar method in counting params  > >         for module in modules: > >             print (f' \n\n freezing {module} \n\n') > >             for param in module.parameters(): > >                 param.requires_grad = False > > ``` >  > PS where should i call the freeze function There is a `mamba_model.py`, which serves as the main function to run training. I assume there will be a similar file for llama. Just include it before running forward. ","> > > > > > Hi jung, > > > > > > I am also facing a similar situation. Could you direct me on finding how to freeze some part of the model while training? > > > > >  > > > > >  > > > > > Nvm! I found it! > > > >  > > > >  > > > > May I ask where can i found it > > >  > > >  > > > Here is what I did before. This is for Mamba. What model you were using? > > > ``` > > >     def freeze( > > >         self,  > > >         freeze_mamba_model: bool,  > > >         freeze_embedding_model:bool,  > > >         freeze_output_layer:bool,  > > >         unfreeze_decoder_last_layer: bool = False, > > >     ): > > >         """""" > > >         Zixian: Sept 8 19:11pm VERIFIED functionality  > > >          > > >         Freeze model modules.  > > >  > > >         Make specific modules nontrainable by setting requires_grad to False for the module's parameters. > > >  > > >         Args: > > >         freeze_mamba_model (bool): Freeze the entire decoder module. > > >         freeze_embedding_model (bool): Freeze the embedding module. > > >         freeze_output_layer (bool): Freeze the output layer. > > >         unfreeze_decoder_last_layer (bool): Unfreeze decoder's last layer.  > > >          > > >         """""" > > >          > > >          for l in range(self.model.decoder.num_layers_per_pipeline_rank): > > >              layer_params = count_parameters_in_layer(model, f'decoder.layers.{l}.') > > >         modules = [] > > >         if freeze_mamba_model: > > >             modules.append(self.decoder) > > >         if freeze_embedding_model: > > >              Zixian: Prevent second host triggerring errors for hosting model only  > > >             if self.pre_process: > > >                 modules.append(self.embedding) > > >         if freeze_output_layer: > > >              Zixian: Prevent second host triggerring errors for hosting model only  > > >             if self.post_process: > > >                 modules.append(self.output_layer) > > >          > > >          Update Sept 7 22:12pm Not tested yet.  > > >          TODO: if does not work, follow similar method in counting params  > > >         for module in modules: > > >             print (f' \n\n freezing {module} \n\n') > > >             for param in module.parameters(): > > >                 param.requires_grad = False > > > ``` > >  > >  > > PS where should i call the freeze function >  > There is a `mamba_model.py`, which serves as the main function to run training. I assume there will be a similar file for llama. Just include it before running forward. i see, THX"
felipeliliti,FASE 6 LILITI STK 3.6.9 INTELIGÃŠNCIA ARTIFICIAL ANTI CARBONO. ,"```python  Importando bibliotecas necessÃ¡rias import json  Definindo as Ã¡reas que o projeto Liliti STK 3.6.9 irÃ¡ atender com funcionalidades modernas areas = {     ""SaÃºde"": {         ""AnÃ¡lise de DiagnÃ³stico"": ""Implementa algoritmos de aprendizado profundo para anÃ¡lise de imagens mÃ©dicas e diagnÃ³sticos."",         ""Gerenciamento de Dados de Pacientes"": ""Sistema baseado em blockchain para armazenamento seguro e gerenciamento de dados de pacientes."",         ""PrevisÃ£o de DoenÃ§as"": ""Modelos preditivos para identificar riscos de doenÃ§as com base em histÃ³rico mÃ©dico e dados genÃ©ticos."",         ""Assistente Virtual de SaÃºde"": ""Chatbots avanÃ§ados para orientaÃ§Ã£o e triagem mÃ©dica inicial.""     },     ""EducaÃ§Ã£o"": {         ""PersonalizaÃ§Ã£o de ConteÃºdo"": ""RecomendaÃ§Ãµes baseadas em IA para conteÃºdo educacional e planos de estudo personalizados."",         ""AnÃ¡lise de Desempenho"": ""Ferramentas de anÃ¡lise de dados para monitorar e melhorar o desempenho dos alunos usando anÃ¡lise preditiva."",         ""Tutoria Virtual"": ""Sistemas de tutoria baseados em IA que oferecem suporte acadÃªmico personalizado em tempo real."",         ""Realidade Aumentada (AR)"": ""ExperiÃªncias de aprendizagem imersivas usando AR para visualizaÃ§Ã£o de conceitos complexos.""     },     ""FinanÃ§as"": {         ""DetecÃ§Ã£o de Fraude"": ""Algoritmos de machine learning para detectar e prevenir atividades fraudulentas em tempo real."",         ""AnÃ¡lise de Mercado"": ""Ferramentas de anÃ¡lise preditiva para prever tendÃªncias de mercado e otimizaÃ§Ã£o de investimentos."",         ""Assistente Financeiro Virtual"": ""Chatbots que oferecem conselhos financeiros personalizados e gerenciamento de contas."",         ""AutomaÃ§Ã£o de Processos Financeiros"": ""RobÃ´s de software para automatizar tarefas financeiras repetitivas e reduzir erros.""     },     ""SeguranÃ§a"": {         ""Reconhecimento Facial"": ""Tecnologia de reconhecimento facial avanÃ§ada para seguranÃ§a e autenticaÃ§Ã£o."",         ""DetecÃ§Ã£o de Anomalias"": ""Sistemas de IA que identificam comportamentos anÃ´malos em tempo real para prevenÃ§Ã£o de ameaÃ§as."",         ""AnÃ¡lise Forense Digital"": ""Ferramentas para anÃ¡lise e recuperaÃ§Ã£o de dados em investigaÃ§Ãµes digitais."",         ""Sistema de Alerta Inteligente"": ""NotificaÃ§Ãµes e alertas automÃ¡ticos baseados em IA para eventos de seguranÃ§a.""     },     ""Transporte"": {         ""Roteamento Inteligente"": ""Algoritmos de otimizaÃ§Ã£o para melhorar a eficiÃªncia de rotas e reduzir o consumo de combustÃ­vel."",         ""VeÃ­culos AutÃ´nomos"": ""Tecnologia de conduÃ§Ã£o autÃ´noma para transporte seguro e eficiente."",         ""GestÃ£o de TrÃ¡fego"": ""Sistemas baseados em IA para gerenciamento e controle de trÃ¡fego urbano."",         ""PrevisÃ£o de ManutenÃ§Ã£o"": ""Modelos preditivos para prever e agendar manutenÃ§Ã£o de veÃ­culos e equipamentos.""     },     ""Energia"": {         ""GestÃ£o Inteligente de Redes ElÃ©tricas"": ""Sistemas de IA para monitoramento e controle eficiente das redes elÃ©tricas."",         ""OtimizaÃ§Ã£o de Consumo"": ""Ferramentas para otimizaÃ§Ã£o do consumo de energia em residÃªncias e indÃºstrias."",         ""Energia RenovÃ¡vel"": ""Algoritmos para otimizaÃ§Ã£o da produÃ§Ã£o e distribuiÃ§Ã£o de energia renovÃ¡vel."",         ""PrevisÃ£o de Demanda"": ""Modelos preditivos para prever a demanda de energia e ajustar a oferta.""     },     ""Agricultura"": {         ""Monitoramento de Culturas"": ""Sensores e IA para monitorar a saÃºde das culturas e otimizar o uso de recursos."",         ""Agricultura de PrecisÃ£o"": ""Tecnologias para aplicar tÃ©cnicas de cultivo especÃ­ficas para diferentes Ã¡reas de uma fazenda."",         ""PrevisÃ£o de Colheita"": ""Modelos preditivos para prever o rendimento das colheitas e otimizar o planejamento."",         ""Controle de Pragas"": ""Sistemas baseados em IA para detectar e controlar pragas e doenÃ§as das plantas.""     },     ""Entretenimento"": {         ""RecomendaÃ§Ãµes Personalizadas"": ""Algoritmos de recomendaÃ§Ã£o para oferecer conteÃºdo personalizado com base em preferÃªncias do usuÃ¡rio."",         ""CriaÃ§Ã£o de ConteÃºdo Automatizada"": ""IA para gerar conteÃºdo de entretenimento, como mÃºsica e textos criativos."",         ""Realidade Virtual (VR)"": ""ExperiÃªncias imersivas de VR para entretenimento e jogos."",         ""AnÃ¡lise de Sentimento"": ""Ferramentas para analisar o sentimento do pÃºblico em relaÃ§Ã£o a filmes, jogos e outras mÃ­dias.""     },     ""ComunicaÃ§Ã£o"": {         ""TraduÃ§Ã£o AutomÃ¡tica"": ""Sistemas de traduÃ§Ã£o automÃ¡tica baseados em IA para comunicaÃ§Ã£o multilÃ­ngue."",         ""AnÃ¡lise de Texto"": ""Ferramentas para anÃ¡lise e compreensÃ£o de grandes volumes de texto e conversas."",         ""Assistentes Virtuais"": ""Chatbots e assistentes virtuais para suporte e interaÃ§Ã£o com usuÃ¡rios."",         ""DetecÃ§Ã£o de Fake News"": ""Algoritmos para identificar e filtrar notÃ­cias falsas e desinformaÃ§Ã£o.""     },     ""Ambiente"": {         ""Monitoramento de PoluiÃ§Ã£o"": ""Sensores e IA para monitorar e analisar nÃ­veis de poluiÃ§Ã£o do ar e da Ã¡gua."",         ""AnÃ¡lise de Impacto Ambiental"": ""Modelos para avaliar o impacto ambiental de projetos e atividades."",         ""Gerenciamento de ResÃ­duos"": ""Sistemas para otimizar a coleta e o gerenciamento de resÃ­duos urbanos e industriais."",         ""ConservaÃ§Ã£o de Recursos Naturais"": ""Ferramentas para monitoramento e conservaÃ§Ã£o de recursos naturais e biodiversidade.""     },     ""LogÃ­stica"": {         ""OtimizaÃ§Ã£o de Cadeia de Suprimentos"": ""Algoritmos para melhorar a eficiÃªncia da cadeia de suprimentos e reduzir custos."",         ""GestÃ£o de ArmazÃ©ns"": ""Sistemas baseados em IA para gerenciar e otimizar operaÃ§Ãµes de armazÃ©ns."",         ""Roteamento e Entrega"": ""OtimizaÃ§Ã£o de rotas e programaÃ§Ã£o de entregas para melhorar a eficiÃªncia logÃ­stica."",         ""Monitoramento em Tempo Real"": ""Tecnologia para rastreamento e monitoramento em tempo real de cargas e veÃ­culos.""     },     ""ComÃ©rcio"": {         ""RecomendaÃ§Ãµes de Produtos"": ""Sistemas de recomendaÃ§Ã£o para sugerir produtos com base no comportamento de compra dos clientes."",         ""AnÃ¡lise de Mercado"": ""Ferramentas de IA para anÃ¡lise de tendÃªncias de mercado e comportamento do consumidor."",         ""AutomaÃ§Ã£o de Atendimento ao Cliente"": ""Chatbots e assistentes virtuais para suporte e atendimento ao cliente."",         ""GestÃ£o de InventÃ¡rio Inteligente"": ""Sistemas para otimizar o gerenciamento de inventÃ¡rio e prever demanda.""     },     ""Turismo"": {         ""RecomendaÃ§Ãµes Personalizadas"": ""IA para sugerir destinos e atividades com base nas preferÃªncias dos viajantes."",         ""GestÃ£o de Reservas"": ""Sistema para gerenciamento automatizado de reservas de viagens e acomodaÃ§Ãµes."",         ""Assistente de Viagem Virtual"": ""Chatbots que ajudam os viajantes com planejamento e informaÃ§Ãµes durante a viagem."",         ""AnÃ¡lise de ExperiÃªncia do UsuÃ¡rio"": ""Ferramentas para avaliar e melhorar a experiÃªncia do usuÃ¡rio em serviÃ§os de turismo.""     },     ""HabitaÃ§Ã£o"": {         ""Gerenciamento de ImÃ³veis"": ""Sistemas baseados em IA para gestÃ£o e manutenÃ§Ã£o de propriedades residenciais e comerciais."",         ""AnÃ¡lise de Mercado ImobiliÃ¡rio"": ""Ferramentas para avaliar e prever tendÃªncias no mercado imobiliÃ¡rio."",         ""AutomaÃ§Ã£o Residencial"": ""Tecnologia para automaÃ§Ã£o de casas, incluindo controle de iluminaÃ§Ã£o, seguranÃ§a e conforto."",         ""PrevisÃ£o de Valor de Propriedades"": ""Modelos preditivos para estimar o valor futuro de propriedades e investimentos.""     },     ""Governo"": {         ""AnÃ¡lise de PolÃ­ticas PÃºblicas"": ""Ferramentas para avaliar e prever o impacto de polÃ­ticas pÃºblicas e regulamentaÃ§Ãµes."",         ""Gerenciamento de ServiÃ§os PÃºblicos"": ""Sistemas baseados em IA para otimizar a prestaÃ§Ã£o de serviÃ§os pÃºblicos e atendimento ao cidadÃ£o."",         ""PrevisÃ£o de TendÃªncias Sociais"": ""Modelos para prever e analisar tendÃªncias sociais e comportamentais."",         ""SeguranÃ§a e VigilÃ¢ncia Inteligente"": ""Tecnologias de seguranÃ§a para monitoramento e controle em Ã¡reas urbanas e eventos pÃºblicos.""     },     ""Pesquisa e Desenvolvimento"": {         ""InovaÃ§Ã£o em IA"": ""Ferramentas para acelerar a pesquisa e desenvolvimento de novas tecnologias de IA."",         ""AnÃ¡lise de Dados AvanÃ§ada"": ""Sistemas para anÃ¡lise e visualizaÃ§Ã£o de grandes volumes de dados em projetos de P&D."",         ""ColaboraÃ§Ã£o em Pesquisa"": ""Plataformas para colaboraÃ§Ã£o e compartilhamento de informaÃ§Ãµes entre pesquisadores e cientistas."",         ""AutomaÃ§Ã£o de ExperimentaÃ§Ã£o"": ""Tecnologias para automatizar e otimizar processos de experimentaÃ§Ã£o e testes.""     },     ""Tecnologia"": {         ""Desenvolvimento de Software"": ""Ferramentas de IA para automaÃ§Ã£o e otimizaÃ§Ã£o do desenvolvimento de software."",         ""SeguranÃ§a CibernÃ©tica"": ""Sistemas avanÃ§ados para proteÃ§Ã£o contra ameaÃ§as e vulnerabilidades cibernÃ©ticas."",         ""IntegraÃ§Ã£o de Sistemas"": ""Tecnologias para integraÃ§Ã£o e interoperabilidade entre diferentes sistemas e plataformas."",         ""AnÃ¡lise de TendÃªncias TecnolÃ³gicas"": ""Ferramentas para identificar e analisar tendÃªncias emergentes em tecnologia.""     },     ""ServiÃ§os PÃºblicos"": {         ""OtimizaÃ§Ã£o de Infraestrutura"": ""Sistemas para melhorar a eficiÃªncia e a gestÃ£o de infraestrutura pÃºblica."",         ""Monitoramento de Qualidade dos ServiÃ§os"": """,2024-08-11T11:24:25Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/993,Marking as stale. No activity in 60 days.
KookHoiKim,[BUG] error raised while converting llm to megatron,"**Describe the bug** I followed llama_mistral.md using mistral 7b model. (also using llama model too) However, it raises error below.  ```using world size: 1, dataparallel size: 1, contextparallel size: 1 tensormodelparallel size: 1, encodertensormodelparallel size: 0pipelinemodelparallel size: 1 encoderpipelinemodelparallel size: 0 WARNING: Setting args.overlap_p2p_comm to False since noninterleaved schedule does not support overlapping p2p communication using torch.float32 for parameters ... Process Process1: Traceback (most recent call last):   File ""/workspace/code/MegatronLM/tools/checkpoint/saver_mcore.py"", line 449, in save_checkpoint     validate_args(margs)   File ""/workspace/code/MegatronLM/megatron/training/arguments.py"", line 559, in validate_args     raise RuntimeError('usedistckpt is not supported in legacy models.') RuntimeError: usedistckpt is not supported in legacy models. ``` If i modify manually use_legacy_models = False, another error occurs.  ``` Traceback (most recent call last):   File ""/workspace/code/MegatronLM/tools/checkpoint/saver_mcore.py"", line 790, in save_checkpoint     save_checkpoint(md.iteration, [get_local_model(pp_rank, ep_rank, tp_rank)], None, None, num_floating_point_operations_so_far=0,   File ""/workspace/code/MegatronLM/megatron/training/checkpointing.py"", line 396, in save_checkpoint     save_strategy = FullyParallelSaveStrategyWrapper(save_strategy, mpu.get_data_parallel_group(with_context_parallel=True),   File ""/workspace/code/MegatronLM/megatron/core/parallel_state.py"", line 917, in get_data_parallel_group     _DATA_PARALLEL_GROUP_WITH_CP is not None AssertionError: data parallel group with context parallel combined is not initialized ``` FYI I remember that this issue is occured after pulling recent commit of main branch.  On 0b4c4cfced47cffad4cec8c4047986bfa60e7f10 commit, error is not occured. ",2024-08-09T10:50:34Z,stale,open,1,1,https://github.com/NVIDIA/Megatron-LM/issues/992,Marking as stale. No activity in 60 days.
KookHoiKim,[BUG] clip key mismatch,"**Describe the bug** I try to use LLaVA example and faced to key mismatch error. I am on latest commit in main branch. (094d66b) [rank0]: RuntimeError: Error(s) in loading state_dict for LLaVAModel: [rank0]:        Missing key(s) in state_dict: ""vision_model.decoder.layers.0.self_attention.linear_proj._extra_state"", ""vision_model.decoder.layers.0.self_attention.linear_qkv._extra_state"", ""vision_model.decoder.layers.0.mlp.linear_fc1._extra_state"", ""vision_model.decoder.layers.0.mlp.linear_fc2._extra_state"", ""vision_model.decoder.layers.1.self_attention.linear_proj._extra_state"", ""vision_model.decoder.layers.1.self_attention.linear_qkv._extra_state"", ""vision_model.decoder.layers.1.mlp.linear_fc1._extra_state"", ""vision_model.decoder.layers.1.mlp.linear_fc2._extra_state"", ""vision_model.decoder.layers.2.self_attention.linear_proj._extra_state"", ""vision_model.decoder.layers.2.self_attention.linear_qkv._extra_state"", ""vision_model.decoder.layers.2.mlp.linear_fc1._extra_state"", ""vision_model.decoder.layers.2.mlp.linear_fc2._extra_state"", ""vision_model.decoder.layers.3.self_attention.linear_proj._extra_state"", ""vision_model.decoder.layers.3.self_attention.linear_qkv._extra_state"", ""vision_model.decoder.layers.3.mlp.linear_fc1._extra_state"", ""vision_model.decoder.layers.3.mlp.linear_fc2._extra_state"", ""vision_model.decoder.layers.4.self_attention.linear_proj._extra_state"", ""vision_model.decoder.layers.4.self_attention.linear_qkv._extra_state"", ""vision_model.decoder.layers.4.mlp.linear_fc1._extra_state"", ""vision_model.decoder.layers.4.mlp.linear_fc2._extra_state"", ""vision_model.decoder.layers.5.self_attention.linear_proj._extra_state"", ""vision_model.decoder.layers.5.self_attention.linear_qkv._extra_state"", ""vision_model.decoder.layers.5.mlp.linear_fc1._extra_state"", ""vision_model.decoder.layers.5.mlp.linear_fc2._extra_state"", ""vision_model.decoder.layers.6.self_attention.linear_proj._extra_state"", ""vision_model.decoder.layers.6.self_attention.linear_qkv._extra_state"", ""vision_model.decoder.layers.6.mlp.linear_fc1._extra_state"", ""vision_model.decoder.layers.6.mlp.linear_fc2._extra_state"", ""vision_model.decoder.layers.7.self_attention.linear_proj._extra_state"", ""vision_model.decoder.layers.7.self_attention.linear_qkv._extra_state"", ""vision_model.decoder.layers.7.mlp.linear_fc1._extra_state"", ""vision_model.decoder.layers.7.mlp.linear_fc2._extra_state"", ""vision_model.decoder.layers.8.self_attention.linear_proj._extra_state"", ""vision_model.decoder.layers.8.self_attention.linear_qkv._extra_state"", ""vision_model.decoder.layers.8.mlp.linear_fc1._extra_state"", ""vision_model.decoder.layers.8.mlp.linear_fc2._extra_state"", ""vision_model.decoder.layers.9.self_attention.linear_proj._extra_state"", ""vision_model.decoder.layers.9.self_attention.linear_qkv._extra_state"", ""vision_model.decoder.layers.9.mlp.linear_fc1._extra_state"", ""vision_model.decoder.layers.9.mlp.linear_fc2._extra_state"", ""vision_model.decoder.layers.10.self_attention.linear_proj._extra_state"", ""vision_model.decoder.layers.10.self_attention.linear_qkv._extra_state"", ""vision_model.decoder.layers.10.mlp.linear_fc1._extra_state"", ""vision_model.decoder.layers.10.mlp.linear_fc2._extra_state"", ""vision_model.decoder.layers.11.self_attention.linear_proj._extra_state"", ""vision_model.decoder.layers.11.self_attention.linear_qkv._extra_state"", ""vision_model.decoder.layers.11.mlp.linear_fc1._extra_state"", ""vision_model.decoder.layers.11.mlp.linear_fc2._extra_state"", ""vision_model.decoder.layers.12.self_attention.linear_proj._extra_state"", ""vision_model.decoder.layers.12.self_attention.linear_qkv._extra_state"", ""vision_model.decoder.layers.12.mlp.linear_fc1._extra_state"", ""vision_model.decoder.layers.12.mlp.linear_fc2._extra_state"", ""vision_model.decoder.layers.13.self_attention.linear_proj._extra_state"", ""vision_model.decoder.layers.13.self_attention.linear_qkv._extra_state"", ""vision_model.decoder.layers.13.mlp.linear_fc1._extra_state"", ""vision_model.decoder.layers.13.mlp.linear_fc2._extra_state"", ""vision_model.decoder.layers.14.self_attention.linear_proj._extra_state"", ""vision_model.decoder.layers.14.self_attention.linear_qkv._extra_state"", ""vision_model.decoder.layers.14.mlp.linear_fc1._extra_state"", ""vision_model.decoder.layers.14.mlp.linear_fc2._extra_state"", ""vision_model.decoder.layers.15.self_attention.linear_proj._extra_state"", ""vision_model.decoder.layers.15.self_attention.linear_qkv._extra_state"", ""vision_model.decoder.layers.15.mlp.linear_fc1._extra_state"", ""vision_model.decoder.layers.15.mlp.linear_fc2._extra_state"", ""vision_model.decoder.layers.16.self_attention.linear_proj._extra_state"", ""vision_model.decoder.layers.16.self_attention.linear_qkv._extra_state"", ""vision_model.decoder.layers.16.mlp.linear_fc1._extra_state"", ""vision_model.decoder.layers.16.mlp.linear_fc2._extra_state"", ""vision_model.decoder.layers.17.self_attention.linear_proj._extra_state"", ""vision_model.decoder.layers.17.self_attention.linear_qkv._extra_state"", ""vision_model.decoder.layers.17.mlp.linear_fc1._extra_state"", ""vision_model.decoder.layers.17.mlp.linear_fc2._extra_state"", ""vision_model.decoder.layers.18.self_attention.linear_proj._extra_state"", ""vision_model.decoder.layers.18.self_attention.linear_qkv._extra_state"", ""vision_model.decoder.layers.18.mlp.linear_fc1._extra_state"", ""vision_model.decoder.layers.18.mlp.linear_fc2._extra_state"", ""vision_model.decoder.layers.19.self_attention.linear_proj._extra_state"", ""vision_model.decoder.layers.19.self_attention.linear_qkv._extra_state"", ""vision_model.decoder.layers.19.mlp.linear_fc1._extra_state"", ""vision_model.decoder.layers.19.mlp.linear_fc2._extra_state"", ""vision_model.decoder.layers.20.self_attention.linear_proj._extra_state"", ""vision_model.decoder.layers.20.self_attention.linear_qkv._extra_state"", ""vision_model.decoder.layers.20.mlp.linear_fc1._extra_state"", ""vision_model.decoder.layers.20.mlp.linear_fc2._extra_state"", ""vision_model.decoder.layers.21.self_attention.linear_proj._extra_state"", ""vision_model.decoder.layers.21.self_attention.linear_qkv._extra_state"", ""vision_model.decoder.layers.21.mlp.linear_fc1._extra_state"", ""vision_model.decoder.layers.21.mlp.linear_fc2._extra_state"", ""vision_model.decoder.layers.22.self_attention.linear_proj._extra_state"", ""vision_model.decoder.layers.22.self_attention.linear_qkv._extra_state"", ""vision_model.decoder.layers.22.mlp.linear_fc1._extra_state"", ""vision_model.decoder.layers.22.mlp.linear_fc2._extra_state"", ""vision_model.decoder.layers.23.self_attention.linear_proj._extra_state"", ""vision_model.decoder.layers.23.self_attention.linear_qkv._extra_state"", ""vision_model.decoder.layers.23.mlp.linear_fc1._extra_state"", ""vision_model.decoder.layers.23.mlp.linear_fc2._extra_state"". There is another keys but it was resolved when use 'usetelayernormlinear'  while clip converting. (It was from recent commit)",2024-08-09T07:57:50Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/991,"Same issue. Even with 'usetelayernormlinear' for converting CLIP, the error still exists.",Marking as stale. No activity in 60 days.
hityupeng,No pre-norm for non-moe gpt style model when using TE-transformer layer spec???,"I was trying to run the pretrain of a nonmoe gpt style model, like llama. I found it was using the TEtransformer layer spec.  But in the function ""get_gpt_layer_with_transformer_engine_spec"" it looks like there is no ""input_layernorm"" setup for the prenormalization of the transformer layer. Besides, the ""pre_mlp_layernorml"" is set on condition of ""num_experts""??? But they are both set in the ""get_gpt_layer_local_spec"", and they are needed for llama... So I am confused why there is no prenorm setup in the TEtransformer layer spec? Is this a bug or it's set somewhere else? ",2024-08-09T07:52:02Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/990,prenorm is fused with fc1 https://github.com/NVIDIA/MegatronLM/blob/203b463689bd322eb915afb3e4d1076bcc4783ba/megatron/core/models/gpt/gpt_layer_specs.pyL119C28L119C59,"> prenorm is fused with fc1 >  > https://github.com/NVIDIA/MegatronLM/blob/203b463689bd322eb915afb3e4d1076bcc4783ba/megatron/core/models/gpt/gpt_layer_specs.pyL119C28L119C59 Got it, thanks",Marking as stale. No activity in 60 days.
mushan09,[QUESTION] Does it support Knowledge Distillation?,Does it support Knowledge Distillation?,2024-08-09T07:11:28Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/989,"check Nemo: https://github.com/NVIDIA/NeMo/pull/9849  they are already developed but not released yet,  > 4. Experiments and Results > We use the NVIDIA MegatronLM framework [45] to implement our pruning and distillation algorithms for compression and retraining.  https://arxiv.org/pdf/2407.14679"
1195343015,[BUG] arguments of get_cpu_offload_context() in transformer_engine.py for different version of te,"**Describe the bug** When I use nvcr.io/nvidia/pytorch:24.07 to run run_simple_mcore_train_loop.py in commit 094d66b(newest) It seems like some wrong in megatron/core/transformer/custom_layers/**transformer_engine.py** get_cpu_offload_context() for the version of transformerengine  the version of transformerengine in nvcr.io/nvidia/pytorch:24.07 is 1.8.0+37280ec **To Reproduce** ```python PYTHONPATH=$PYTHON_PATH:./megatron torchrun nprocpernode 2 examples/run_simple_mcore_train_loop.py ```  **Stack trace/logs** ```python [rank1]: Traceback (most recent call last): [rank1]:   File ""/workspace/megatron/examples/run_simple_mcore_train_loop.py"", line 121, in  [rank1]:     gpt_model = model_provider() [rank1]:   File ""/workspace/megatron/examples/run_simple_mcore_train_loop.py"", line 47, in model_provider [rank1]:     gpt_model = GPTModel( [rank1]:   File ""/workspace/megatron/megatron/core/models/gpt/gpt_model.py"", line 101, in __init__ [rank1]:     self.decoder = TransformerBlock( [rank1]:   File ""/workspace/megatron/megatron/core/transformer/transformer_block.py"", line 148, in __init__ [rank1]:     get_cpu_offload_context( [rank1]:   File ""/workspace/megatron/megatron/core/transformer/custom_layers/transformer_engine.py"", line 898, in get_cpu_offload_context [rank1]:     context, sync_func = _get_cpu_offload_context( [rank1]: **TypeError: get_cpu_offload_context() takes from 0 to 4 positional arguments but 5 were given** ```  **Environment (please complete the following information):** nvcr.io/nvidia/pytorch:24.07 transformerengine 1.8.0+37280ec **Proposed fix** some wrong in megatron/core/transformer/custom_layers/**transformer_engine.py** get_cpu_offload_context()  https://github.com/NVIDIA/MegatronLM/blob/094d66b488514beaac2106c3e0f9581d27ea9533/megatron/core/transformer/custom_layers/transformer_engine.pyL890L904 PR https://github.com/NVIDIA/MegatronLM/pull/996",2024-08-09T01:53:56Z,,closed,1,4,https://github.com/NVIDIA/Megatron-LM/issues/988,this bug also happened when I run examples/inference/run_text_generation_server_345M.sh,"same issue. It seems ' _te_version > packaging.version.Version(""1.8.0"")' would be **False**, but it results in True. ","That's the reason, thank for your reply. Hope it can be fixed quickly","I have checked the source code of transformerengine, 1.8.0 in the code should be modified to 1.9.0. ```python   from transformer_engine.pytorch.cpu_offload import (       get_cpu_offload_context as _get_cpu_offload_context,   )   def get_cpu_offload_context(       enabled, num_layers, model_layers, activation_offloading, weight_offloading   ):       if _te_version > packaging.version.Version(""1.9.0""):           context, sync_func = _get_cpu_offload_context(               enabled, num_layers, model_layers, activation_offloading, weight_offloading           )       else:           context, sync_func = _get_cpu_offload_context(               enabled, num_layers, activation_offloading, weight_offloading           )  ``` "
yiakwy-xpu-ml-framework-team,add hoper llama golden with mcore calling stack,Add Hoper llama2 7b mcore gold example,2024-08-08T11:01:46Z,stale,open,0,6,https://github.com/NVIDIA/Megatron-LM/issues/987,uselegacymodels  why this option is passed ?,"> uselegacymodels  why this option is passed ? The latest updates use mcore models by default. For llama2 benchmark test, no need to switch to mcore model and new dataset API: check here","when i use convert shell script in your commit,  It shows ""Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/workspace/models/Llama27bhf/tokenizer.model'. Use `repo_type` argument if needed."" error,   Do you know how to use local tokenizer.model file? Thank you.","and  I found, Using ""TOKENIZER_MODEL=metallama/Llama27bhf"" in shell script can convert hf to megatron successfully.","> and I found, Using ""TOKENIZER_MODEL=metallama/Llama27bhf"" in shell script can convert hf to megatron successfully. Hi  **/workspace/models** is the standard location where I have models in the docker, you can either create soft links of the location to point the real model, if it hosted in your distributed file system. ``` HF_MODEL_DIR=/workspace/models/$MODEL OUTPUT=/workspace/models/$MODELtomegatrontp$TPpp$PP TOKENIZER_MODEL=/workspace/models/$MODEL/tokenizer.model ``` For tput test, you don't need to download the dataset and model parameters, otherwise, you should run convert script first to create 3D parallel (classical llama2) checkpoint and load the weights and optimizer states depending on your task type, The usage of Llama2 tokenizer relies on the tokenizer class you choose. For latest megatron (> 2403), I recommend to use metallama2 tokenrizer. For old megatron ( 2403 uses sentencepiece proto API to load Meta 32K BPE vocabulary class _Llama2Tokenizer(_SentencePieceTokenizer):     """"""SentencePieceTokenizerMegatron wrapper""""""     def __init__(self, model_file,):         super().__init__(model_file, vocab_extra_ids=0)     def _initalize(self, vocab_extra_ids):         self._populate_vocab()          BOS / EOS token IDs         self.n_words: int = self.tokenizer.vocab_size()         self.bos_id: int = self.tokenizer.bos_id()         self.eos_id: int = self.tokenizer.eos_id()         self.pad_id: int = self.tokenizer.pad_id()         assert self.tokenizer.vocab_size() == self.tokenizer.get_piece_size()         ... ``` which uses sentencepiece to load model: ```         import sentencepiece         self.tokenizer = sentencepiece.SentencePieceProcessor(model_file=model_file) ``` That means the megatron after 2403 is built for metallama. ",Marking as stale. No activity in 60 days.
billishyahao,[Bugfix] Fix typo in moe doc,This minor patch is to fix typo in moe introduction document.,2024-08-08T06:45:19Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/986,Good catch. Fixed in https://github.com/NVIDIA/MegatronLM/commit/e223b920724c523a098558b2c128b4d6eb7f8ff0.,Marking as stale. No activity in 60 days.
Teng-xu,[QUESTION] glu activation with tensor parallel in GroupedMLP,"**Description:** When training with GroupedMLP and Tensor Parallel (TP) enabled, and `gated_linear_unit` is activated, the activation function is applied to fc1_output. Assuming a TP degree of 2, this intermediate output only contains half of the information as it holds the tensor values on one TP rank. Applying the GLU activation function on this output leads to a loss of information because only half of the tensor values are involved in the activation function. Specifically, in the GLU function (https://github.com/NVIDIA/MegatronLM/blob/core_v0.7.0/megatron/core/transformer/moe/experts.pyL48): `self.config.activation_func(x[0]) * x[1]` Both self.config.activation_func(x[0]) and x[1] contain half of the output tensor due to TP being enabled, resulting in an output that does not match the results from training without TP. **Steps to Reproduce:** 1. Enable gated_linear_unit in the GroupedMLP configuration. 2. Train the model with Tensor Parallel (TP) enabled. 3. Compare the intermediate outputs of the GLU activation function with and without TP enabled. (https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/transformer/moe/experts.pyL176) **Expected Behavior:** The activation function should correctly handle the tensor values across all TP ranks to prevent any loss of information, ensuring consistency with results obtained without TP. **Actual Behavior:** The GLU activation function is applied to tensor values that only represent half of the full tensor due to TP, leading to inconsistent results.",2024-08-07T19:44:54Z,,closed,0,6,https://github.com/NVIDIA/Megatron-LM/issues/985,> Both self.config.activation_func(x[0]) and x[1] contain half of the output tensor due to TP being enabled this is the expected behavior not an error. TP will add the fc2_output from different tp ranks to get the final result https://github.com/NVIDIA/MegatronLM/blob/203b463689bd322eb915afb3e4d1076bcc4783ba/megatron/core/transformer/moe/token_dispatcher.pyL227,"I understand that TP adds the fc2_output from different TP ranks to get the final result. However, my concern is with the correctness of the intermediate output from the activation layer. If this intermediate output is incorrect, then the reduced final result will also be incorrect. The following is the activation computation func (https://github.com/NVIDIA/MegatronLM/blob/core_v0.7.0/megatron/core/transformer/moe/experts.pyL46): ``` x = torch.chunk(x, 2, dim=1) return self.config.activation_func(x[0]) * x[1] ``` which yields different results when TP is applied versus when it's not, even after reduction.  Consider the following example: Without TP (mat1): ``` mat1 = torch.tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],         [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]]) x1 = torch.chunk(mat1, 2, dim=1) r1 = x1[0] * x1[1] Result:  r1 = tensor([[  0,   9,  20,  33,  48,  65,  84, 105],         [384, 425, 468, 513, 560, 609, 660, 713]]) ``` With TP degree == 2 (mat2 and mat3 being inputs on TP rank 0 and 1): ``` mat2 = torch.tensor([[0, 1, 2, 3, 4, 5, 6, 7],         [16, 17, 18, 19, 20, 21, 22, 23]]) mat3 = torch.tensor([[8, 9, 10, 11, 12, 13, 14, 15],         [24, 25, 26, 27, 28, 29, 30, 31]]) x2 = torch.chunk(mat2, 2, dim=1) r2 = x2[0] * x2[1] x3 = torch.chunk(mat3, 2, dim=1) r3 = x3[0] * x3[1] Results: r2 =  tensor([[  0,   5,  12,  21],         [320, 357, 396, 437]])  r3 = tensor([[ 96, 117, 140, 165],         [672, 725, 780, 837]]) ``` The reduced results from r2 and r3 do not match r1 because, when TP degree > 1, each TP rank is multiplying using incorrect tensor values compared to the nonTP case.",The issue with the GLU activation in Tensor Parallel is causing correctness problems that are blocking training. An update on this or any suggestions for moving forward would be greatly appreciated.,"i see your point. in your example, the results are different because of different tensor layout. The order of TP, glu sharding is shuffled. In practice, this shouldn't affect training because the linear layers are learned. this might affect training where the parallelism strategy or model architecture is changed mid training. ","Thanks for your response. My primary concern is with finetuning. If we pretrain using TP 2 and then load the checkpoint to finetune with TP1 or any configs other than TP2, then we would see loss issues. Do you have any suggestions for addressing this, or are there any plans on your end to provide fixes for this?",a workaround is to manually convert tensor layout when you switch to finetuning. 
shafiqabedin,[QUESTION]Splitting large document and bucketing,"I asked this question in the discussion section but did not receive any response so asking here with a little bit more details. I am trying to figure out if bucketization is done (or can be done) for model training. By ""bucketization"" I am referring to batching based on similar sequence length (https://torchtext.readthedocs.io/en/latest/data.htmlbucketiterator).. The motivation is that I have documents with very large text and I want pick a splitting schema (which may create a lot of samples with small number of tokens and bucketize them). That brings me to the second question which is  is splitting supported in Megatron? Any answer would be much appreciated  Thanks.",2024-08-07T18:07:29Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/975
taowangcheng,[bugfix] Fix _warmup_jit_function," The most important thing is adding `// args.context_parallel_size`, otherwise when we scale very long sequence on many GPUs using context parallelism, it will cause OOM here.  Also add warmup for swiglu since swiglu is common in many kinds of large language models.",2024-08-07T15:32:02Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/973,"Hi , thanks for your contribution. We merged this PR in https://github.com/NVIDIA/MegatronLM/commit/813b11869b2ca17c31df1b136d14b423094f5013 and your author info is well preserved.","Hi , thanks very much.",Marking as stale. No activity in 60 days.
aaa123git,[bugfix] Fix the incorrect with-statement,"You may want to enter multiple context managers in one withstatement. However, `with rng_context and fp8_context` is equivalent to `with fp8_context`, because `rng_context` doesn't rewrite `__and__` method. The correct way is `with rng_context, fp8_context:` . Ref: https://docs.python.org/3/reference/compound_stmts.htmlthewithstatement",2024-08-07T11:34:10Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/972,Marking as stale. No activity in 60 days.
TJ-Solergibert,[QUESTION] Megatron-LM `DistributedOptimizer` or NeMo `MegatronDistributedFusedAdam` Optimizer?,"Hi, After going across both MegatronLM & NeMo I've found that NeMo configs set by default the `MegatronDistributedFusedAdam` optimizer from the NeMo framework. But Megatron also contains a `DistributedOptimizer`.  The NeMo one is based on the Apex's `DistributedFusedAdam` which incorporates ZeRO2. I would like to know which one is better, both in terms of throughput and performance or which config do you recommend to use for large scale training. Thanks!",2024-08-06T16:59:52Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/971
syx11237744,[QUESTION] Checkpoint storage format,"**Your question** Ask a clear and concise question about MegatronLM. Could you let me know which version I should revert to if I want to use the previous checkpoint storage format, which is stored as .pt? Or are there any other methods to save it as a .pt file?Thank you! DATASET_PATH=/share/root/out_file/sum.jsonl SAVE_PATH=/share/sunyuanxu/out_file/sum VOCAB_FILE=gpt2_/vocab.json MERGE_FILE=gpt2_/merges.txt export CUDA_DEVICE_MAX_CONNECTIONS=1 GPUS_PER_NODE=1  Change for multinode config MASTER_ADDR=localhost MASTER_PORT=6000 NNODES=1 NODE_RANK=0 WORLD_SIZE=$(($GPUS_PER_NODE*$NNODES)) CHECKPOINT_PATH=/share/root/checkpoint/cp DATA_PATH=/share/root/out_file/sum_text_document DISTRIBUTED_ARGS=""     nproc_per_node $GPUS_PER_NODE \     nnodes $NNODES \     node_rank $NODE_RANK \     master_addr $MASTER_ADDR \     master_port $MASTER_PORT "" GPT_ARGS=""     numlayers 24 \     hiddensize 1024 \     numattentionheads 16 \     seqlength 1024 \     maxpositionembeddings 1024 \     microbatchsize 32 \     globalbatchsize 256 \     lr 0.00015 \     trainiters 1000 \     lrdecayiters 320000 \     lrdecaystyle cosine \     minlr 1.0e5 \     weightdecay 1e2 \     lrwarmupfraction .01 \     clipgrad 1.0 \     fp16 \     attentionsoftmaxinfp32 \ "" DATA_ARGS=""     datapath $DATA_PATH \     vocabfile $VOCAB_FILE \     mergefile $MERGE_FILE \     split 949,50,1 "" OUTPUT_ARGS=""     loginterval 100 \     saveinterval 10000 \     evalinterval 1000 \     evaliters 10 "" torchrun $DISTRIBUTED_ARGS MegatronLM/pretrain_gpt.py \     $GPT_ARGS \     $DATA_ARGS \     $OUTPUT_ARGS \     distributedbackend nccl \     save $CHECKPOINT_PATH \     load $CHECKPOINT_PATH",2024-08-06T14:04:43Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/970
suzewei,[QUESTION] ,"Hi, I am training my Llama27b model with MegatronLM, using four H20s, 32 GPUs in total. The parallel strategy is set to: TP=8/PP=2/DP=2. Now, I want to know the data capacity of different parallel groups communicating, is there some parameter setting to get these values, or if there is no such parameter, how can I get it from the code? Thank you.",2024-08-06T11:55:28Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/969
wangwz6666,nothing,,2024-08-06T07:51:13Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/968
wuhouming,BitPipe_initial_version,,2024-08-06T03:54:57Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/967
sambar1729,"[QUESTION]How to convert a huggingface checkpoint, and also use PP > 1 or TP > 1","**Your question** I want to ingest a checkpoint from HF into Megatron LM and then continue training on that. For the latter part (training) I will need TP > 1 or PP > 1 (Given the model size and the gpu memory i have). For this, when I convert the HF checkpoint to work with Megatron, I need the TP and PP values to match what I need in the training part.  However, right now the conversion scripts from HF to mcore seems to take in PP = 1 and TP = 1 (I am hoping I am mistaken here). How do I use the conversion scripts in `tools/checkpoints/convert.py` so I may be able to use TP > 1 and/or PP > 1?  Thanks. Edit:  I am guessing this is answered (in the negative that no, there is no way currently to do this conversion) by https://github.com/NVIDIA/MegatronLM/issues/296issuecomment1732407585  wondering if we have any updates here.",2024-08-06T00:25:05Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/966
sambar1729,[QUESTION] About memory usage in dot_product_attention.py,"**Your question** Getting OOM errors training Megatron LM on Phi3 architecture. Should that happen? ``` GPU  = A100 80GB (1 node) minibatchsize = 1 sequence length = 4096 hidden size = 3072 hidden ffn size = 8192 ```  Details When running Megatron LM training, I am getting the following memory usages in the forward function of `megatron/core/transformer/dot_product_attention.py/DotProductAttention`: ``` [Start] Allocated: 36659.26 MB, Reserved: 37246.00 MB [After repeat_interleave] Allocated: 36659.26 MB, Reserved: 37246.00 MB [After reshape and view] Allocated: 36659.26 MB, Reserved: 37246.00 MB [After baddbmm] Allocated: 38707.26 MB, Reserved: 39294.00 MB [After scale_mask_softmax] Allocated: 39731.26 MB, Reserved: 40318.00 MB [After attention_dropout] Allocated: 41267.26 MB, Reserved: 41854.00 MB [After context computation] Allocated: 41291.26 MB, Reserved: 41854.00 MB [End] Allocated: 41291.26 MB, Reserved: 41854.00 MB [Start] Allocated: 40699.33 MB, Reserved: 41854.00 MB [After repeat_interleave] Allocated: 40699.33 MB, Reserved: 41854.00 MB [After reshape and view] Allocated: 40699.33 MB, Reserved: 41854.00 MB [After baddbmm] Allocated: 41723.33 MB, Reserved: 42878.00 MB [After scale_mask_softmax] Allocated: 42747.33 MB, Reserved: 43902.00 MB [After attention_dropout] Allocated: 44283.33 MB, Reserved: 44926.00 MB [After context computation] Allocated: 44307.33 MB, Reserved: 44926.00 MB [End] Allocated: 44307.33 MB, Reserved: 44926.00 MB [Start] Allocated: 43715.39 MB, Reserved: 44926.00 MB [After repeat_interleave] Allocated: 43715.39 MB, Reserved: 44926.00 MB [After reshape and view] Allocated: 43715.39 MB, Reserved: 44926.00 MB [After baddbmm] Allocated: 44739.39 MB, Reserved: 45950.00 MB [After scale_mask_softmax] Allocated: 45763.39 MB, Reserved: 46974.00 MB [After attention_dropout] Allocated: 47299.39 MB, Reserved: 47998.00 MB [After context computation] Allocated: 47323.39 MB, Reserved: 47998.00 MB [End] Allocated: 47323.39 MB, Reserved: 47998.00 MB ``` Thus for each call to DotProductAttention the memory usage is increasing by ~4gb, and on the 80gb A100 I am using, I am getting only a few iterations.  Is this expected? Between baddbmm, scale_mask_softmax and attention_dropout we seem to be incurring a lot of memory usage. the other three operations (context computation, repeat interleave, reshape and view) seem to be fine.  Also, sharing the code with the breakpoints for the memory usage here: ```         log_memory_usage(""Start"")          ===================================          Raw attention scores. [b, n/p, s, s]          ===================================          expand the key and value [sk, b, ng, hn] > [sk, b, np, hn]          This is a noop for normal attention where ng == np. When using group query attention this          creates a view that has the keys and values virtually repeated along their dimension to          match the number of queries.          attn_mask_type is not used.         if self.num_attention_heads_per_partition // self.num_query_groups_per_partition > 1:             key = key.repeat_interleave(                 self.num_attention_heads_per_partition // self.num_query_groups_per_partition, dim=2             )             value = value.repeat_interleave(                 self.num_attention_heads_per_partition // self.num_query_groups_per_partition, dim=2             )         log_memory_usage(""After repeat_interleave"")          [b, np, sq, sk]         output_size = (             query.size(1),             query.size(2),             query.size(0),             key.size(0),         )          [sq, b, np, hn] > [sq, b * np, hn]          This will be a simple view when doing normal attention, but in group query attention          the key and value tensors are repeated to match the queries so you can't use simple strides          to extract the queries.         query = query.reshape(output_size[2], output_size[0] * output_size[1], 1)          [sk, b, np, hn] > [sk, b * np, hn]         key = key.view(output_size[3], output_size[0] * output_size[1], 1)         log_memory_usage(""After reshape and view"")          preallocting input tensor: [b * np, sq, sk]         matmul_input_buffer = parallel_state.get_global_memory_buffer().get_tensor(             (output_size[0] * output_size[1], output_size[2], output_size[3]), query.dtype, ""mpu"",         )          Raw attention scores. [b * np, sq, sk]         matmul_result = torch.baddbmm(             matmul_input_buffer,             query.transpose(0, 1),   [b * np, sq, hn]             key.transpose(0, 1).transpose(1, 2),   [b * np, hn, sk]             beta=0.0,             alpha=(1.0 / self.norm_factor),         )         log_memory_usage(""After baddbmm"")          change view to [b, np, sq, sk]         attention_scores = matmul_result.view(*output_size)          ===========================          Attention probs and dropout          ===========================          attention scores and attention mask [b, np, sq, sk]         attention_probs: Tensor = self.scale_mask_softmax(attention_scores, attention_mask)         log_memory_usage(""After scale_mask_softmax"")          This is actually dropping out entire tokens to attend to, which might          seem a bit unusual, but is taken from the original Transformer paper.         if not self.config.sequence_parallel:             with tensor_parallel.get_cuda_rng_tracker().fork():                 attention_probs = self.attention_dropout(attention_probs)         else:             attention_probs = self.attention_dropout(attention_probs)         log_memory_usage(""After attention_dropout"")          =========================          Context layer. [sq, b, hp]          =========================          value > context layer.          [sk, b, np, hn] > [b, np, sq, hn]          context layer shape: [b, np, sq, hn]         output_size = (             value.size(1),             value.size(2),             query.size(0),             value.size(3),         )          change view [sk, b * np, hn]         value = value.view(value.size(0), output_size[0] * output_size[1], 1)          change view [b * np, sq, sk]         attention_probs = attention_probs.view(output_size[0] * output_size[1], output_size[2], 1)          matmul: [b * np, sq, hn]         context = torch.bmm(attention_probs, value.transpose(0, 1))         log_memory_usage(""After context computation"")          change view [b, np, sq, hn]         context = context.view(*output_size)          [b, np, sq, hn] > [sq, b, np, hn]         context = context.permute(2, 0, 1, 3).contiguous()          [sq, b, np, hn] > [sq, b, hp]         new_context_shape = context.size()[:2] + (self.hidden_size_per_partition,)         context = context.view(*new_context_shape)         log_memory_usage(""End"") ```",2024-08-05T18:50:00Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/965,met the same issue
zhaoyang-star,[QUESTION] Asynchronous Checkpoint Saving,I saw MegatronLM has supported asynchronous checkpoint saving since v0.7.0.  I did some test on this feature and saw it benefits a lot. I tried to dive into it and found the ckpt's format has changed a lot compared to the synchronous saving. Just 3 questions:  What is the meaning of `__0_0.distcp` and `__0_1.distcp`? There is no readme or blog about this feature. Could you please explain it?  How to convert this format to the synchronous saving format? Such as `distrib_optim.pt` and `model_optim_rng.pt`.  How to convert this format to HuggingFace .bin format in order to do inference? Thanks for your help ^_^ ``` root21:/home/MegatronLM/fp8_async_save_te1.7_outputs/checkpoint/8Blr1e4tp1pp4 tree lh . â”œâ”€â”€ [ 325]  iter_0000010 â”‚   â”œâ”€â”€ [2.4G]  __0_0.distcp â”‚   â”œâ”€â”€ [ 31G]  __0_1.distcp â”‚   â”œâ”€â”€ [1.8G]  __2_0.distcp â”‚   â”œâ”€â”€ [ 23G]  __2_1.distcp â”‚   â”œâ”€â”€ [1.8G]  __4_0.distcp â”‚   â”œâ”€â”€ [ 23G]  __4_1.distcp â”‚   â”œâ”€â”€ [2.4G]  __6_0.distcp â”‚   â”œâ”€â”€ [ 31G]  __6_1.distcp â”‚   â”œâ”€â”€ [ 15K]  common.pt â”‚   â””â”€â”€ [ 119]  metadata.json â”œâ”€â”€ [ 325]  iter_0000020 â”‚   â”œâ”€â”€ [2.4G]  __0_0.distcp â”‚   â”œâ”€â”€ [ 31G]  __0_1.distcp â”‚   â”œâ”€â”€ [1.8G]  __2_0.distcp â”‚   â”œâ”€â”€ [ 23G]  __2_1.distcp â”‚   â”œâ”€â”€ [1.8G]  __4_0.distcp â”‚   â”œâ”€â”€ [ 23G]  __4_1.distcp â”‚   â”œâ”€â”€ [2.4G]  __6_0.distcp â”‚   â”œâ”€â”€ [ 31G]  __6_1.distcp â”‚   â”œâ”€â”€ [ 15K]  common.pt â”‚   â””â”€â”€ [ 119]  metadata.json â”œâ”€â”€ [ 325]  iter_0000030 â”‚   â”œâ”€â”€ [2.4G]  __0_0.distcp â”‚   â”œâ”€â”€ [ 31G]  __0_1.distcp â”‚   â”œâ”€â”€ [1.8G]  __2_0.distcp â”‚   â”œâ”€â”€ [ 23G]  __2_1.distcp â”‚   â”œâ”€â”€ [1.8G]  __4_0.distcp â”‚   â”œâ”€â”€ [ 23G]  __4_1.distcp â”‚   â”œâ”€â”€ [2.4G]  __6_0.distcp â”‚   â”œâ”€â”€ [ 31G]  __6_1.distcp â”‚   â”œâ”€â”€ [ 15K]  common.pt â”‚   â””â”€â”€ [ 119]  metadata.json â””â”€â”€ [   2]  latest_checkpointed_iteration.txt ```,2024-08-05T09:42:37Z,,closed,0,11,https://github.com/NVIDIA/Megatron-LM/issues/964,">  What is the meaning of __0_0.distcp and __0_1.distcp? There is no readme or blog about this feature. Could you please explain it? It has the format as ____.distcp, _global rank_ is straightforward. It's global rank in the default process group created by Pytorch. We create multiple writer processes to make checkpoint writing with asynchrony. _writer process ID_ simply indicates where the corresponding checkpoint is from. >  How to convert this format to the synchronous saving format? Such as distrib_optim.pt and model_optim_rng.pt. The previous checkpoint format in MegatronLM was converted due to the introduction of `dist_checkpointing` by . So, synchronous checkpointing (usedistckpt only) also generates the same checkpoint.  If your question is how to convert the checkpoint in the new `dist_checkpointing` format to the previous legacy format, we don't have a converter for this. >  How to convert this format to HuggingFace .bin format in order to do inference? MegatronCore doesn't provide a converter. Any production framework based on MegatronCore may have that such as NeMo. ","> It has the format as .distcp, global rank is straightforward. It's global rank in the default process group created by Pytorch. We create multiple writer processes to make checkpoint writing with asynchrony. writer process ID simply indicates where the corresponding checkpoint is from. I noticed the file is named as `__x_y.distcp`. What do the x/y mean if TP=1/PP=4/DP=2? x will be 0/2/4/6 and y will be 0/1. > The previous checkpoint format in MegatronLM was converted due to the introduction of dist_checkpointing by . So, synchronous checkpointing (usedistckpt only) also generates the same checkpoint. If your question is how to convert the checkpoint in the new dist_checkpointing format to the previous legacy format, we don't have a converter for this. It is common we already got a ckpt with legacy format (such as `distrib_optim.pt` and `model_optim_rng.pt`) and want to do continuous training using `usemcoremodels usedistckpt asyncsave`.  Is it possible to support this case? > MegatronCore doesn't provide a converter. Any production framework based on MegatronCore may have that such as NeMo. Could you please provide a demo link for converting `*.distcp` to HF `*.bin`?  ","> I noticed the file is named as __x_y.distcp. What do the x/y mean if TP=1/PP=4/DP=2? x will be 0/2/4/6 and y will be 0/1. Recently, we've introduced fullyparallel saving(FPS, `ckptfullyparallelsave`) as well as asyncparallel saving. without FPS, as you said, rank [0,2,4,6] will save checkpoints as before with 2 writer processes, which lead to [0, 1] for `y`. If you turn on FPS, it leverages duplicate states of models in dataparallelism to parallelize checkpoint saving in internode level as described in the article below. In this case, `x` will be [08] and `y` is [01]. MegatronCore Tech blog article > It is common we already got a ckpt with legacy format (such as distrib_optim.pt and model_optim_rng.pt) and want to do continuous training using usemcoremodels usedistckpt asyncsave. Is it possible to support this case? The `load_checkpoint` basically tries to see if the checkpoint in the passed path is legacy or distckpt format. So, you can load the legacy checkpoint and save in the newer format with the option > Could you please provide a demo link for converting *.distcp to HF *.bin?  You can easily find examples in NeMo","> The `load_checkpoint` basically tries to see if the checkpoint in the passed path is legacy or distckpt format. So, you can load the legacy checkpoint and save in the newer format with the option I tested and the results are as followings. Note that:  dist_ckpt format cannot be saved back as legacy format.  The blog said Most importantly, MegatronCore enables users to resume training from a checkpoint saved with different tensor and pipeline parallelism degrees, providing the flexibility to change training configurations as needed during training.  But I found it failed due to mismatch TP/PP for DistributedOptimizer. From the codebase we can see `TODO: add DistributedOptimizer support for differing TPxPP` is a TODO list. SO could I think this feature has not been implemented yet? Please correct me if I misunderstand anything. ``` 192.169.125.13: (TP, PP) mismatch after resume ((1, 4) vs (2, 2) from checkpoint): RNG state will be ignored 192.169.125.13: [rank3]: Traceback (most recent call last): 192.169.125.13: [rank3]:   File ""/home/MegatronLM/pretrain_gpt.py"", line 271, in  192.169.125.13: [rank3]:     pretrain( 192.169.125.13: [rank3]:   File ""/home/MegatronLM/megatron/training/training.py"", line 227, in pretrain 192.169.125.13: [rank3]:     model, optimizer, opt_param_scheduler = setup_model_and_optimizer( 192.169.125.13: [rank3]:   File ""/home/MegatronLM/megatron/training/training.py"", line 530, in setup_model_and_optimizer 192.169.125.13: [rank3]:     args.iteration, args.num_floating_point_operations_so_far = load_checkpoint( 192.169.125.13: [rank3]:   File ""/home/MegatronLM/megatron/training/checkpointing.py"", line 715, in load_checkpoint 192.169.125.13: [rank3]:     raise RuntimeError(""{}: not supported for DistributedOptimizer"".format(mismatch_msg)) 192.169.125.13: [rank3]: RuntimeError: (TP, PP) mismatch after resume ((1, 4) vs (2, 2) from checkpoint): not supported for DistributedOptimizer ``` ```  ``` > You can easily find examples in NeMo From the NeMo docs, there is a demo script converting ckpt trained by MegatronLM into the NeMo compatible formats. And Using another script to convert NeMo format to HuggingFace format. This link is the support matrix and the way that directly converting from MLM to HF format is not found. So is there any easy way to convert dist_ckpt from MLM into HF format? Thanks for you kind help  ","> > The `load_checkpoint` basically tries to see if the checkpoint in the passed path is legacy or distckpt format. So, you can load the legacy checkpoint and save in the newer format with the option >  > I tested and the results are as followings. Note that: >  > * dist_ckpt format cannot be saved back as legacy format. > * The blog said Most importantly, MegatronCore enables users to resume training from a checkpoint saved with different tensor and pipeline parallelism degrees, providing the flexibility to change training configurations as needed during training.  But I found it failed due to mismatch TP/PP for DistributedOptimizer. From the codebase we can see `TODO: add DistributedOptimizer support for differing TPxPP` is a TODO list. SO could I think this feature has not been implemented yet? >  > Please correct me if I misunderstand anything. >  > ``` > 192.169.125.13: (TP, PP) mismatch after resume ((1, 4) vs (2, 2) from checkpoint): RNG state will be ignored > 192.169.125.13: [rank3]: Traceback (most recent call last): > 192.169.125.13: [rank3]:   File ""/home/MegatronLM/pretrain_gpt.py"", line 271, in  > 192.169.125.13: [rank3]:     pretrain( > 192.169.125.13: [rank3]:   File ""/home/MegatronLM/megatron/training/training.py"", line 227, in pretrain > 192.169.125.13: [rank3]:     model, optimizer, opt_param_scheduler = setup_model_and_optimizer( > 192.169.125.13: [rank3]:   File ""/home/MegatronLM/megatron/training/training.py"", line 530, in setup_model_and_optimizer > 192.169.125.13: [rank3]:     args.iteration, args.num_floating_point_operations_so_far = load_checkpoint( > 192.169.125.13: [rank3]:   File ""/home/MegatronLM/megatron/training/checkpointing.py"", line 715, in load_checkpoint > 192.169.125.13: [rank3]:     raise RuntimeError(""{}: not supported for DistributedOptimizer"".format(mismatch_msg)) > 192.169.125.13: [rank3]: RuntimeError: (TP, PP) mismatch after resume ((1, 4) vs (2, 2) from checkpoint): not supported for DistributedOptimizer > ``` >  > ``` >  > ``` >  > > You can easily find examples in NeMo >  > From the NeMo docs, there is a demo script converting ckpt trained by MegatronLM into the NeMo compatible formats. And Using another script to convert NeMo format to HuggingFace format. This link is the support matrix and the way that directly converting from MLM to HF format is not found. So is there any easy way to convert dis_ckpt from MLM into HF format? Thanks for you kind help  star Could you please provide a reference for your conversion to a Nemo script? Iâ€™m encountering this issueï¼š File ""/usr/local/lib/python3.10/distpackages/nemo/collections/nlp/models/nlp_model.py"", line 380, in load_from_checkpoint     model = ptl_load_state(cls, checkpoint, strict=strict, cfg=cfg, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/pytorch_lightning/core/saving.py"", line 144, in _load_state     obj = cls(**_cls_kwargs)   File ""/usr/local/lib/python3.10/distpackages/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py"", line 243, in __init__     super().__init__(cfg, trainer=trainer, no_lm_init=True)   File ""/usr/local/lib/python3.10/distpackages/nemo/collections/nlp/models/language_modeling/megatron_base_model.py"", line 118, in __init__     with open_dict(cfg):   File ""/usr/lib/python3.10/contextlib.py"", line 135, in __enter__     return next(self.gen)   File ""/usr/local/lib/python3.10/distpackages/omegaconf/omegaconf.py"", line 983, in open_dict     prev_state = config._get_node_flag(""struct"") AttributeError: 'dict' object has no attribute '_get_node_flag' thanks", This link is about converting from MLM to NeMo. But I haven't tested it yet. ,">  This link is about converting from MLM to NeMo. But I haven't tested it yet. Thank you! Iâ€™m using this script, but Iâ€™m encountering the above error. Do you know of any other methods to convert to the HuggingFace format besides the one mentioned above?"," nvidia  It is great to see that there are convert tools: tools/checkpoint/convert.py in MegatronLM repo. Is there any docs for converting from `torch_dist` format into huggingface format? Thanks. `dist_checkpointing` is introduced by  , how could I convert from `torch_dist` format into `torch` format?",">  nvidia It is great to see that there are convert tools: tools/checkpoint/convert.py in MegatronLM repo. Is there any docs for converting from `torch_dist` format into huggingface format? Thanks. `dist_checkpointing` is introduced by  , how could I convert from `torch_dist` format into `torch` format? There is no converters from MLM directly to HF, you have to go through NeMo. Converting  from torch_dist to torch is a step backwards and is not recommended. However if you need it for some reason, it believe recent tools/checkpoint/convert.py should already support such conversion ",Any updates about the conversion ? the Nemo converter does not support distcp format it uses the legacy format apparently Code,Usefull : DCP to Torch
TtCWH,learning rate error when continue training,"**Describe the bug** I have completed the model training of the first stage. The settings of the first stage are lr=3e4 and min_lr=3e5. The settings of the second stage are lr=3e5 and min_lr=2e5. Moreover, I enabled the three parameters of resetdataloader overrideopt_paramscheduler resetiteration. In the output log, lr and min_lr were indeed overridden to the settings of the second stage. However, after loading the checkpoint of the first stage and starting the training, lr changed back to 3e4.  **To Reproduce** 1. Set lr=3e4 and min_lr=3e5, and stop the training after obtaining the first checkpoint; 2. Set the load_checkpoint_path to the path of the first checkpoint, set lr=3e5 and min_lr=2e5, enable resetdataloader overrideopt_paramscheduler resetiteration, and disable warmup, then continue the training; 3. After starting the training, it is shown that the learning rate of the first step is still 3e4.  **Expected behavior** In step 3 above, after starting the training, the learning rate of the first step should be 3e5.  **Stack trace/logs** No **Environment (please complete the following information):**   MegatronLM commit ID: latest   PyTorch version: 2.3.0a0+40ec155e58.nv24.03   CUDA version: 12.4   NCCL version:  **Proposed fix** Set the args after loading the checkpoint **Additional context**",2024-08-02T02:42:07Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/963,"I have solved this problem, the arg ""noloadoptim"" should be set if you don't want your optimizer to load the checkpoint",I think the issue should be reconsidered. There should exist a way to override learning rate scheduler but still load optimizer.
ArtificialZeng,Update README.md,,2024-08-01T02:24:14Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/961,Marking as stale. No activity in 60 days.,> Marking as stale. No activity in 60 days. thank you,Marking as stale. No activity in 60 days.
xinqiu,fix typo in  token_dispatcher.py,hiddden_shape_before_permute > hidden_shape_before_permute,2024-07-31T13:09:36Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/960,Good catch. Fixed in https://github.com/NVIDIA/MegatronLM/commit/e223b920724c523a098558b2c128b4d6eb7f8ff0.,Marking as stale. No activity in 60 days.
yuvraj27khanna02,transformer_engine import error,"Hello, I am using this repo for the first time and am a beginner at MLOps so please excuse any oversight or trivial mistakes from my end. I have the following python file called `temp.py` that contains the following code (this is to create a basic implementation of the ) ``` from megatron.core.transformer.transformer_config import TransformerConfig import torch.nn.functional as F from megatron.core.models.gpt.gpt_model import GPTModel import torch from megatron.core import parallel_state from megatron.core.models.gpt.gpt_layer_specs import get_gpt_layer_local_spec def init_distribution(tensor_model_parallel_size = 1, pipeline_model_parallel_size = 1) > None:     parallel_state.initialize_model_parallel(tensor_model_parallel_size=tensor_model_parallel_size,                                              pipeline_model_parallel_size=pipeline_model_parallel_size) init_distribution() small_config = TransformerConfig(     num_layers=1,     hidden_size=128,     num_attention_heads=3,     ffn_hidden_size=512,     activation_func=F.relu,     tensor_model_parallel_size=1,     pipeline_model_parallel_size=1,     pipeline_dtype=torch.float16 ) gpt_model = GPTModel(     config=small_config,     transformer_layer_spec=get_gpt_layer_local_spec(),     vocab_size=100,     max_sequence_length=64 ) print(gpt_model) ``` When I try to run this I get the following error: ``` python temp.py  Traceback (most recent call last):   File ""/mnt/batch/tasks/shared/LS_root/mounts/clusters/yuvraj1/code/MegatronLM/megatron/core/transformer/transformer_block.py"", line 30, in      from megatron.core.transformer.custom_layers.transformer_engine import (   File ""/mnt/batch/tasks/shared/LS_root/mounts/clusters/yuvraj1/code/MegatronLM/megatron/core/transformer/custom_layers/transformer_engine.py"", line 9, in      import transformer_engine as te ModuleNotFoundError: No module named 'transformer_engine' During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""/mnt/batch/tasks/shared/LS_root/mounts/clusters/yuvraj1/code/MegatronLM/uv_rough.py"", line 3, in      from megatron.core.models.gpt.gpt_model import GPTModel   File ""/mnt/batch/tasks/shared/LS_root/mounts/clusters/yuvraj1/code/MegatronLM/megatron/core/models/gpt/__init__.py"", line 2, in      from .gpt_model import GPTModel   File ""/mnt/batch/tasks/shared/LS_root/mounts/clusters/yuvraj1/code/MegatronLM/megatron/core/models/gpt/gpt_model.py"", line 17, in      from megatron.core.transformer.transformer_block import TransformerBlock   File ""/mnt/batch/tasks/shared/LS_root/mounts/clusters/yuvraj1/code/MegatronLM/megatron/core/transformer/transformer_block.py"", line 43, in      import apex   File ""/anaconda/envs/azureml_py38/lib/python3.9/sitepackages/apex/__init__.py"", line 13, in      from pyramid.session import UnencryptedCookieSessionFactoryConfig ImportError: cannot import name 'UnencryptedCookieSessionFactoryConfig' from 'pyramid.session' (unknown location) ``` I have tried `pip install git+https://github.com/NVIDIA/TransformerEngine.git` and 'docker run gpus all it rm nvcr.io/nvidia/pytorch:23.10py3' but it does not work. I get the following error while trying to download """"""   note: This error originates from a subprocess, and is likely not a problem with pip. error: legacyinstallfailure Ã— Encountered error while trying to install package. â•°â”€> transformerengine note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. """""" I want to understand how to fix this and how to download the transformerengine model.",2024-07-31T03:50:30Z,stale,open,2,3,https://github.com/NVIDIA/Megatron-LM/issues/959,Same problem. Don't know why,"you can build TE from source, cuda 12.1 is ok for every version.",Marking as stale. No activity in 60 days.
SkanderBS2024,(Pre training mamba with train.sh) Error : GPT2BPETokenizer : assert args.vocab_file is not None,"Hello, after preprocessing the dataset with a BPE tokenizer, when i launch the 'train.sh' script for mamba i do get this error.  In the script it's mentionned that i have to specify the tokenizer path, i did put the path for the ""tokenizer.json"" and when inspecting the code it expects the vocab.json and merge.json files does anyone have an idea about the path i should give as an argument ? this is the command that launches the training script :  /workspace/megatron/examples/mamba ./train.sh /workspace/megatron/examples/mamba/dataset/CleanData2/preprocessed2_text_document /workspace/megatron/examples/mamba/dataset/mamba_tokenizer/tokenizer.json Assuming that CleanData2 contains the .bin and .idx processed data Assuming that mamba_tokenizer contains :  ",2024-07-30T23:04:16Z,stale,open,0,7,https://github.com/NVIDIA/Megatron-LM/issues/958,"> Hello, >  > after preprocessing the dataset with a BPE tokenizer, when i launch the 'train.sh' script for mamba i do get this error. >  >  > In the script it's mentionned that i have to specify the tokenizer path, i did put the path for the ""tokenizer.json"" and when inspecting the code it expects the vocab.json and merge.json files does anyone have an idea about the path i should give as an argument ? >  > this is the command that launches the training script : >  > /workspace/megatron/examples/mamba ./train.sh /workspace/megatron/examples/mamba/dataset/CleanData2/preprocessed2_text_document /workspace/megatron/examples/mamba/dataset/mamba_tokenizer/tokenizer.json >  > Assuming that CleanData2 contains the .bin and .idx processed data Assuming that mamba_tokenizer contains : >  >  Have you solved it? I also have this problem.",  I've temporarily hard coded the paths here :  https://github.com/NVIDIA/MegatronLM/blob/c873429cbaa43257d4d4fc01df2a7a50453b7984/megatron/training/tokenizer/tokenizer.pyL38L40,">  >  > I've temporarily hard coded the paths here : >  > https://github.com/NVIDIA/MegatronLM/blob/c873429cbaa43257d4d4fc01df2a7a50453b7984/megatron/training/tokenizer/tokenizer.pyL38L40 How to do it specifically? I changed it to the following but still got an error: ```         assert args.vocab_file is not None         assert args.merge_file is not None         tokenizer = _GPT2BPETokenizer('/home/eva.liu/MegatronLM/vocab.json', '/home/eva.liu/MegatronLM/merges.txt') ```","remove the assertions and declare a variable for each path and give them as parameter to ""_GPT2BPETokenizer""","Thank you, the problem is solved.","Hi, I think Mamba is using GPTSentencePieceTokenizer as described in the `train.sh`.  ```        tokenizertype GPTSentencePieceTokenizer \        tokenizermodel ${TOKENIZER_PATH} \ ```",Marking as stale. No activity in 60 days.
Dune-Z,[BUG] Infinite Loop in `_get_num_epochs` Function of `GPTDataset` Class When `num_tokens_per_epoch` is Zero,"**Describe the bug** In `megatron/core/datasets/gpt_dataset.py`, the `GPTDatase` class has a function `_get_num_epochs `which can result in an infinite loop when `num_tokens_per_epoch` is set to 0. This occurs without any explicit notification. This function is invoked in `_build_document_sample_shuffle_indices` when building a `GPTDataset` instance and saving caches. https://github.com/NVIDIA/MegatronLM/blob/85bd1f9af2b6a85d5f348509cca5a6251e0cbef1/megatron/core/datasets/gpt_dataset.pyL550L552 `num_tokens_per_epoch` is set to zero when the split setting allocates a very small percentage of the valid/test dataset split. For example, using a split option like 9998, 0.002, 0.002 can cause `num_tokens_per_epoch` to be zero if the dataset size is insufficient. https://github.com/NVIDIA/MegatronLM/blob/85bd1f9af2b6a85d5f348509cca5a6251e0cbef1/examples/mixtral/train_mixtral_8x7b_distributed.shL65 Adjusting the configuration to `9990, 8, 2` or `9900, 80, 20` based on the actual data amount resolves this issue. **To Reproduce** This issue was encountered while training MixtralMoE 8x7B. It can be reproduced as long as the dataset configuration matches the problematic split and is unrelated to the model itself. To replicate, follow the instructions on the MixtralMoE 8x7B training page. The following dataset was used: ``` wget https://atpmodelzoowlcbpai.osscnwulanchabu.aliyuncs.com/release/models/paimegatronpatch/mistraldatasets/wudao_mistralbpe_content_document.bin wget https://atpmodelzoowlcbpai.osscnwulanchabu.aliyuncs.com/release/models/paimegatronpatch/mistraldatasets/wudao_mistralbpe_content_document.idx ``` The training was launched on 4 nodes, each containing 8xA100 GPUs. **Expected behavior** A clear warning should be provided indicating the possibility of an infinite loop along with guidance to modify the configuration to resolve the issue. **Proposed fix** Raise an error at an appropriate place to handle this issue. Possible locations include: 1. In function `_get_num_epochs`, detect if an infinite loop will occur directly. 2. In function `_get_num_tokens_per_epoch` which calculates the variable `num_tokens_per_epoch`. 3. During the initialization of the `GPTDataset` class, if `self.indices` is an empty array, it will cause the `_get_num_tokens_per_epoch` function to return 0",2024-07-30T02:34:49Z,stale,open,2,1,https://github.com/NVIDIA/Megatron-LM/issues/957,Marking as stale. No activity in 60 days.
alex-ht,fix llama3 checkpoint converter,vocab_size => n_words,2024-07-29T01:51:15Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/956
Au3C2,[BUG] MoE Router TopK algorithm is differeent from huggingface implement,"**Describe the bug** A clear and concise description of what the bug is. In Megatron:topk_softmax_with_capacity() ```python     scores, top_indices = torch.topk(logits, k=topk, dim=1)      topk first     probs = torch.softmax(scores, dim=1, dtype=torch.float32).type_as(logits) ``` In transformersmixtral:MixtralSparseMoeBlock ```python         routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)         routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=1 ``` This two algorithm result in two different inference anser. Is there any reason why megatron calculate probs like this? **To Reproduce** Steps to reproduce the behavior. The easier it is to reproduce the faster it will get maintainer attention. **Expected behavior** A clear and concise description of what you expected to happen. **Stack trace/logs** If applicable, add the stack trace or logs from the time of the error. **Environment (please complete the following information):**   MegatronLM commit ID   PyTorch version   CUDA version   NCCL version **Proposed fix** If you have a proposal for how to fix the issue state it here or link to a PR. **Additional context** Add any other context about the problem here.",2024-07-27T08:56:54Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/955,See https://github.com/NVIDIA/MegatronLM/issues/894issuecomment2212765002
dtamayo-nlp,[QUESTION] Why is `reset_attention_mask=False` by default?,"**Your question** When we want to make a training in LLMs with a lot of corpora, I understand that the usual approach is to introduce the documents with the following format: [doc 1] \ [doc 2] \ ...  Until the context length is full. However, the intuitive way of optimizing that I see is using something that you call `reset_attention_mask` and you have implemented here. What I did not expect is to find this attribute as False in most `yaml` configurations of open models. Examples:  https://github.com/NVIDIA/MegatronLM/blob/main/examples/gpt3/gpt_config.yaml  https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/conf/megatron_gpt_config.yaml  https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/conf/megatron_llama_config.yaml While I understand that there might be some potential benefits to not using masking, I don't trivially see why it should be the default approach. I haven't found much on the internet on this topic, any information would be welcome! ",2024-07-26T10:40:43Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/954
wplf,[QUESTION] One possible typo in  docs/source/distrib_optimizer.md,**Your question** Ask a clear and concise question about MegatronLM. Is  `backward` below supposed to be `forward` ? !image,2024-07-26T07:34:15Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/953
fabiancpl,[BUG] Error pre-training BERT,"Hi guys, I am following the MegatronLM example to pretrain a BERT model but I'm getting this error: ``` [rank0]: Traceback (most recent call last): [rank0]:   File ""/root/MegatronLM/pretrain_bert.py"", line 193, in  [rank0]:     pretrain(train_valid_test_datasets_provider, model_provider, [rank0]:   File ""/root/MegatronLM/megatron/training/training.py"", line 274, in pretrain [rank0]:     iteration, num_floating_point_operations_so_far = train( [rank0]:                                                       ^^^^^^ [rank0]:   File ""/root/MegatronLM/megatron/training/training.py"", line 1027, in train [rank0]:     train_step(forward_step_func, [rank0]:   File ""/root/MegatronLM/megatron/training/training.py"", line 550, in train_step [rank0]:     losses_reduced = forward_backward_func( [rank0]:                      ^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/root/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 381, in forward_backward_no_pipelining [rank0]:     output_tensor, num_tokens = forward_step( [rank0]:                                 ^^^^^^^^^^^^^ [rank0]:   File ""/root/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 206, in forward_step [rank0]:     output_tensor, loss_func = forward_step_func(data_iterator, model) [rank0]:                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/root/MegatronLM/pretrain_bert.py"", line 139, in forward_step [rank0]:     output_tensor = model(tokens, padding_mask, [rank0]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/opt/conda/lib/python3.12/sitepackages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl [rank0]:     return self._call_impl(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/opt/conda/lib/python3.12/sitepackages/torch/nn/modules/module.py"", line 1562, in _call_impl [rank0]:     return forward_call(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/root/MegatronLM/megatron/core/distributed/distributed_data_parallel.py"", line 180, in forward [rank0]:     return self.module(*inputs, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/opt/conda/lib/python3.12/sitepackages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl [rank0]:     return self._call_impl(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/opt/conda/lib/python3.12/sitepackages/torch/nn/modules/module.py"", line 1562, in _call_impl [rank0]:     return forward_call(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/root/MegatronLM/megatron/legacy/model/module.py"", line 190, in forward [rank0]:     outputs = self.module(*inputs, **kwargs) [rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/opt/conda/lib/python3.12/sitepackages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl [rank0]:     return self._call_impl(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/opt/conda/lib/python3.12/sitepackages/torch/nn/modules/module.py"", line 1562, in _call_impl [rank0]:     return forward_call(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/root/MegatronLM/megatron/core/models/bert/bert_model.py"", line 237, in forward [rank0]:     hidden_states = self.encoder( [rank0]:                     ^^^^^^^^^^^^^ [rank0]:   File ""/opt/conda/lib/python3.12/sitepackages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl [rank0]:     return self._call_impl(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/opt/conda/lib/python3.12/sitepackages/torch/nn/modules/module.py"", line 1562, in _call_impl [rank0]:     return forward_call(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/root/MegatronLM/megatron/core/transformer/transformer_block.py"", line 383, in forward [rank0]:     hidden_states, context = layer( [rank0]:                              ^^^^^^ [rank0]:   File ""/opt/conda/lib/python3.12/sitepackages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl [rank0]:     return self._call_impl(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/opt/conda/lib/python3.12/sitepackages/torch/nn/modules/module.py"", line 1562, in _call_impl [rank0]:     return forward_call(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/root/MegatronLM/megatron/core/transformer/transformer_layer.py"", line 178, in forward [rank0]:     attention_output_with_bias = self.self_attention( [rank0]:                                  ^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/opt/conda/lib/python3.12/sitepackages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl [rank0]:     return self._call_impl(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/opt/conda/lib/python3.12/sitepackages/torch/nn/modules/module.py"", line 1562, in _call_impl [rank0]:     return forward_call(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/root/MegatronLM/megatron/core/transformer/attention.py"", line 315, in forward [rank0]:     core_attn_out = self.core_attention( [rank0]:                     ^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/opt/conda/lib/python3.12/sitepackages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl [rank0]:     return self._call_impl(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/opt/conda/lib/python3.12/sitepackages/torch/nn/modules/module.py"", line 1562, in _call_impl [rank0]:     return forward_call(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/root/MegatronLM/megatron/core/transformer/custom_layers/transformer_engine.py"", line 514, in forward [rank0]:     core_attn_out = super().forward( [rank0]:                     ^^^^^^^^^^^^^^^^ [rank0]:   File ""/opt/conda/lib/python3.12/sitepackages/transformer_engine/pytorch/attention.py"", line 5267, in forward [rank0]:     return self.fused_attention( [rank0]:            ^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/opt/conda/lib/python3.12/sitepackages/torch/nn/modules/module.py"", line 1553, in _wrapped_call_impl [rank0]:     return self._call_impl(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/opt/conda/lib/python3.12/sitepackages/torch/nn/modules/module.py"", line 1562, in _call_impl [rank0]:     return forward_call(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/opt/conda/lib/python3.12/sitepackages/torch/_dynamo/eval_frame.py"", line 600, in _fn [rank0]:     return fn(*args, **kwargs) [rank0]:            ^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/opt/conda/lib/python3.12/sitepackages/transformer_engine/pytorch/attention.py"", line 4157, in forward [rank0]:     cu_seqlens_q = get_cu_seqlens(attention_mask) [rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]:   File ""/opt/conda/lib/python3.12/sitepackages/transformer_engine/pytorch/attention.py"", line 247, in get_cu_seqlens [rank0]:     cu_seqlens = torch.cat((zero, cu_seqlens)) [rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]: RuntimeError: Tensors must have same number of dimensions: got 1 and 2 ``` I'm using transformer_engine==1.8.0+3ec998e with megatron core_v0.7.0. My pretraining script is like this: ``` !/bin/bash export CUDA_DEVICE_MAX_CONNECTIONS=1 export NVTE_FLASH_ATTN=0 CHECKPOINT_PATH=""./checkpoints"" VOCAB_FILE=""./vocabs/bertbasecasedvocab.txt"" DATA_PATH=""mybert_text_sentence"" BERT_ARGS=""     numlayers 24 \     hiddensize 1024 \     numattentionheads 16 \     seqlength 512 \     maxpositionembeddings 512 \     microbatchsize 4 \     globalbatchsize 8 \     lr 0.0001 \     trainiters 2000000 \     lrdecayiters 990000 \     lrdecaystyle linear \     minlr 0.00001 \     weightdecay 1e2 \     lrwarmupfraction .01 \     clipgrad 1.0 \     fp16     usemcoremodels "" DATA_ARGS=""     datapath $DATA_PATH \     vocabfile $VOCAB_FILE \     split 949,50,1 "" OUTPUT_ARGS=""     loginterval 100 \     saveinterval 10000 \     evalinterval 1000 \     evaliters 10 "" torchrun pretrain_bert.py \     $BERT_ARGS \     $DATA_ARGS \     $OUTPUT_ARGS \     save $CHECKPOINT_PATH \     load $CHECKPOINT_PATH ``` I'm also interested in using the BERT cased checkpoint instead of pretraining from scratch. Thanks in advance.",2024-07-25T18:34:16Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/952,same issue here,Marking as stale. No activity in 60 days.
dustinwloring1988,Differnt Tokenizer,"**Your question** Is there a way to start training on a llama2 with a llama3 tokenizer? I plan on doing all the pretraining myself, if so and someone can provide any information please let me know.",2024-07-25T13:52:54Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/951
Kingsleyandher,[BUG] when use --use-mcore-models and --overlap-param-gather bug,"**Describe the bug** When the sequence of calculation parameters (FP16/BF16) in the buffer is different from the forward execution sequence of the model: As a result, when the `overlapparamgather` command is executed in the distributed optimizer scenario, the allgather update sequence of buckets is inconsistent with the forward execution sequence.  As a result, when some parameters are executed in the forward direction, the bucket where the parameters are located has not been updated through allgather. **The weight of the previous step is still used when the parameter is calculated in the forward direction**. It is assumed that in a most extreme case, parameters A, B, and C are respectively in a bucket 1, a bucket 2, and a bucket 3, but an execution sequence of forward computation is A, C, and B. In this case, the value of `all_gather_handle` is `None` during the forward calculation of C. https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/optimizer/distrib_optimizer.pyL1413 **To Reproduce** you can use DP=4, and add or not add `overlapparamgather` to train 100 steps, and check loss. ```bash GPT_ARGS=""     tensormodelparallelsize 1 \     pipelinemodelparallelsize 1 \     overlapgradreduce \     usemcoremodels \     usedistributedoptimizer \     sequenceparallel \     numlayers 1 \     hiddensize 8192   \     numattentionheads 64 \     seqlength 2048 \     maxpositionembeddings 2048 \     microbatchsize 1 \     globalbatchsize 4 \     trainiters 1000 \     lrdecayiters 320000 \     lr 5.0e7 \     lrdecaystyle cosine \     clipgrad 1.0 \     weightdecay 0.1 \     adambeta1 0.9 \     adambeta2 0.95 \     initmethodstd 0.006 \     nogradientaccumulationfusion \     useflashattn \     disablebiaslinear \     positionembeddingtype rope \     attentiondropout 0.0 \     hiddendropout 0.0 \     bf16 "" ```",2024-07-25T06:56:50Z,,open,0,5,https://github.com/NVIDIA/Megatron-LM/issues/950,"Hmm, the above script is nondeterministic (e.g., because of `useflashattn`), so I wouldn't expect two executions of the same script to produce identical losses (if that is what you were expecting). Moreover, even with deterministic execution, it is expected that the logged losses will look different with and without `overlapparamgather`. In particular, if you set `loginterval 1`, you should see the losses are identical with an offset of 1 (please also use the `determisticmode` flag).",Marking as stale. No activity in 60 days.,"  Honestly, the reproduce parameters from  might not be solid enough, but the issue really exists and I am struggling with this bug for a week. Will be perfect if you could double check Kingsleyandher's issue description together with the source code. Without using overlapparamgather, my issue is gone(by using deterministic, the losses are perfactly the same). His logic could convince me, since it maches bunches of experiments I have made.","> [](https://github.com/deepakn94) Honestly, the reproduce parameters from [](https://github.com/Kingsleyandher) might not be solid enough, but the issue really exists and I am struggling with this bug for a week. Will be perfect if you could double check Kingsleyandher's issue description together with the source code. Without using overlapparamgather, my issue is gone(by using deterministic, the losses are perfactly the same). His logic could convince me, since it maches bunches of experiments I have made. I solved the problem. The problem is caused by the sequence of initializing the mcore model.","> [](https://github.com/deepakn94) Honestly, the reproduce parameters from [](https://github.com/Kingsleyandher) might not be solid enough, but the issue really exists and I am struggling with this bug for a week. Will be perfect if you could double check Kingsleyandher's issue description together with the source code. Without using overlapparamgather, my issue is gone(by using deterministic, the losses are perfactly the same). His logic could convince me, since it maches bunches of experiments I have made. You can refer to the handling of config.reset_attention_order and Line 128"
Baibaifan,[BUG]`examples/multimodal/combine_mistral_clip.sh` Vision model file mismatch.,**Describe the bug** The file format output by `python examples/multimodal/clip_converter.py` does not match the file format required by `examples/multimodal/combine_mistral_clip.sh`. `xxx\state_dict_tp_x.pt` is not `xxx/iter_0000001/mp_rank_00/model_optim_rng.pt`? **To Reproduce**  **Expected behavior** File format of the same name. **Stack trace/logs**  **Environment (please complete the following information):**   MegatronLM commit ID: 9bcd4175becc515331537f0c78eb70079de0eaa8   PyTorch version: 2.3.0a0+ebedce2   CUDA version: 12.4   NCCL version: 2.20.3 **Proposed fix** PR948 **Additional context** ,2024-07-25T05:37:08Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/949,Marking as stale. No activity in 60 days.
Baibaifan,[bugfix]: fixed combine_mistral_clip.sh," Problem description The file format output by `python examples/multimodal/clip_converter.py` does not match the file format required by `examples/multimodal/combine_mistral_clip.sh`. bug issue  After fix Under the original configuration, the conversion was successful. ",2024-07-25T05:36:13Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/948,Marking as stale. No activity in 60 days.
shh2000,[QUESTION] About Optimizer & Params Offload,"Hello, I wonder if there's any plan to support offload optimizer states even params like ZeROoffload? DeepSpeed has offload but without mcore's parallel or TE. I hope there's any possibility to use offload with TP/PP/CP and TE, achieving high performance, especially when h2d/d2h bandwidth is higher like those mentioned in https://openreview.net/pdf?id=rqn2v1Ltgn0. Looking forward to your reply and thanks!",2024-07-24T04:53:05Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/946,CPU offloading is not on our roadmap right now.
SkanderBS2024,"ERROR: Could not find a version that satisfies the requirement triton==2.1.0 (from versions: none) ""MAMBA""","**Describe the bug** Hello, i tried to build an mage from the docker file 'examples/mamba' and after pulling the image there was an error while installing packages. I tried to do it manually, so i pulled the image and then when installing the packages manually i got the same error related to triton 2.1.0 :  any updates on this ?",2024-07-23T15:24:46Z,stale,open,0,5,https://github.com/NVIDIA/Megatron-LM/issues/945,I got the same error. Did you manage to fix it? , library available only on linux. If trying to install on windows or macos you'll encouter this error,>  library available only on linux. If trying to install on windows or macos you'll encouter this error   I am running it on linux machine. I have no clues why the image isn't built correctly during pip install phase.  , try running commands manually without the dockerfile after pulling the nvidia image and running the docker container.,Marking as stale. No activity in 60 days.
SkanderBS2024,Distributed Mamba Training,"**How to customise the train.sh for a distributed Mamba Training ?** Hello,  As i've seen in the megatron modules, there isn't a predefined bash script to pretrain a mamba model on multigpu, how can i set it up for model / data parallelism ... ",2024-07-23T13:25:51Z,stale,open,0,8,https://github.com/NVIDIA/Megatron-LM/issues/944,This runs training on 8 GPUs: https://github.com/NVIDIA/MegatronLM/blob/ssm/examples/mamba/train.sh. You can extend to multinode by passing in appropriate arguments to `torchrun` (adapted from https://pytorch.org/docs/stable/elastic/run.htmlusage): ``` torchrun     nnodes=$NUM_NODES     nprocpernode=$NUM_TRAINERS     rdzvid=$JOB_ID     rdzvbackend=c10d     rdzvendpoint=$HOST_NODE_ADDR     YOUR_TRAINING_SCRIPT.py (arg1 ... train script args...) ```,"I meant multinodes*, so multinodes (nnodes) and gpu_count (nprocpernode). Thank you !"," is it possible to set up the gpus dynamically during training ? (for example i have a total of 180 GPU's 90 of them are fixed for the whole training time and the 90 others will not be always available, a sort of switching where the other gpus are available) ?",Maybe the elastic options here are useful: https://pytorch.org/docs/stable/elastic/run.htmlelasticmin1max4toleratesupto3membershipchangesorfailures?,I'll take a look thank you !,"Hi  , do you know how can I finetune from the existing pretrained 8B checkpoint in a distributed manner?  I manually included the following lines in the `train.sh` ``` load  /workspace/data/ssmretrieval/mamba28b/mamba28b3t4k \ finetune \ ``` When I try to set with `TENSOR_MODEL_PARALLEL_SIZE=2` instead of `TENSOR_MODEL_PARALLEL_SIZE=1` to finetune from the existing Megatron Mamba8B checkpoint, by giving `torchrun nproc_per_node 2` instead of `torchrun nproc_per_node 1`, I got the size_mismatch errors: ``` size mismatch for decoder.layers.55.mixer.conv1d.bias: copying a param with shape torch.Size([10240]) from checkpoint, the shape in current model is torch.Size([5120]).         size mismatch for decoder.layers.55.mixer.norm.weight: copying a param with shape torch.Size([8192]) from checkpoint, the shape in current model is torch.Size([4096]).         size mismatch for decoder.layers.55.mixer.out_proj.weight: copying a param with shape torch.Size([4096, 8192]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).         size mismatch for output_layer.weight: copying a param with shape torch.Size([256000, 4096]) from checkpoint, the shape in current model is torch.Size([128000, 4096]). [20240920 19:50:23,454] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 48849) of binary: /usr/bin/python ``` When I tried to give it a `pipelinemodelparallelsize 2 \` instead of `pipelinemodelparallelsize 1 \` with `torchrun nproc_per_node 2`, it says that it cannot find a file  ``` [Errno 2] No such file or directory: 'mamba28b3t4k/release/mp_rank_00_000/model_optim_rng.pt' ```  But I do have  ``` 'mamba28b3t4k/release/mp_rank_00/model_optim_rng.pt' ``` So I assume using pipeline parallel will look for a specific path of the data. Same for the tensor parallel, where the model is saved differently.  Is there a way I can change / modify / do to enable distributed training/finetuning from the existing pretrained mamba 8b model?   Edit Sept 23:  I found it under `tools/checkpoint`","> This runs training on 8 GPUs: https://github.com/NVIDIA/MegatronLM/blob/ssm/examples/mamba/train.sh. You can extend to multinode by passing in appropriate arguments to `torchrun` (adapted from https://pytorch.org/docs/stable/elastic/run.htmlusage): >  > ``` > torchrun >     nnodes=$NUM_NODES >     nprocpernode=$NUM_TRAINERS >     rdzvid=$JOB_ID >     rdzvbackend=c10d >     rdzvendpoint=$HOST_NODE_ADDR >     YOUR_TRAINING_SCRIPT.py (arg1 ... train script args...) > ``` When I try to run it with multiple nodes, the program freezes, can you tell me what pytorch environment you are using?",Marking as stale. No activity in 60 days.
G-keng,[BUG]  Spelling mistake,**Describe the bug** Function name spelling mistake **To Reproduce** https://github.com/NVIDIA/MegatronLM/blob/9bcd4175becc515331537f0c78eb70079de0eaa8/megatron/core/parallel_state.pyL936 https://github.com/NVIDIA/MegatronLM/blob/9bcd4175becc515331537f0c78eb70079de0eaa8/megatron/core/parallel_state.pyL949,2024-07-23T06:39:25Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/943
wccccp,[BUG]RuntimeError: CUDA error: device-side assert triggered,"Traceback (most recent call last): [rank2]:   File ""/workspace/megatron/wcp/test_mega/MegatronLM/./pretrain_gpt.py"", line 263, in  [rank2]:     pretrain(train_valid_test_datasets_provider, [rank2]:   File ""/workspace/megatron/wcp/test_mega/MegatronLM/megatron/training.py"", line 170, in pretrain [rank2]:     iteration = train(forward_step_func, [rank2]:   File ""/workspace/megatron/wcp/test_mega/MegatronLM/megatron/training.py"", line 812, in train [rank2]:     train_step(forward_step_func, [rank2]:   File ""/workspace/megatron/wcp/test_mega/MegatronLM/megatron/training.py"", line 454, in train_step [rank2]:     losses_reduced = forward_backward_func( [rank2]:   File ""/workspace/megatron/wcp/test_mega/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 327, in forward_backward_no_pipelining [rank2]:     output_tensor = forward_step( [rank2]:   File ""/workspace/megatron/wcp/test_mega/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 183, in forward_step [rank2]:     output_tensor, loss_func = forward_step_func(data_iterator, model) [rank2]:   File ""/workspace/megatron/wcp/test_mega/MegatronLM/./pretrain_gpt.py"", line 218, in forward_step [rank2]:     output_tensor = model(tokens, position_ids, attention_mask, [rank2]:   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1532, in _wrapped_call_impl [rank2]:     return self._call_impl(*args, **kwargs) [rank2]:   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1541, in _call_impl [rank2]:     return forward_call(*args, **kwargs) [rank2]:   File ""/workspace/megatron/wcp/test_mega/MegatronLM/megatron/core/distributed.py"", line 287, in forward [rank2]:     return self.module(*inputs, **kwargs) [rank2]:   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1532, in _wrapped_call_impl [rank2]:     return self._call_impl(*args, **kwargs) [rank2]:   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1541, in _call_impl [rank2]:     return forward_call(*args, **kwargs) [rank2]:   File ""/workspace/megatron/wcp/test_mega/MegatronLM/megatron/model/module.py"", line 181, in forward [rank2]:     outputs = self.module(*inputs, **kwargs) [rank2]:   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1532, in _wrapped_call_impl [rank2]:     return self._call_impl(*args, **kwargs) [rank2]:   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1541, in _call_impl [rank2]:     return forward_call(*args, **kwargs) [rank2]:   File ""/workspace/megatron/wcp/test_mega/MegatronLM/megatron/model/gpt_model.py"", line 83, in forward [rank2]:     lm_output = self.language_model( [rank2]:   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1532, in _wrapped_call_impl [rank2]:     return self._call_impl(*args, **kwargs) [rank2]:   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1541, in _call_impl [rank2]:     return forward_call(*args, **kwargs) [rank2]:   File ""/workspace/megatron/wcp/test_mega/MegatronLM/megatron/model/language_model.py"", line 487, in forward [rank2]:     rotary_pos_emb = self.rotary_pos_emb(self.seq_length) [rank2]:   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1532, in _wrapped_call_impl [rank2]:     return self._call_impl(*args, **kwargs) [rank2]:   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1541, in _call_impl [rank2]:     return forward_call(*args, **kwargs) [rank2]:   File ""/workspace/megatron/wcp/test_mega/MegatronLM/megatron/core/models/common/embeddings/rotary_pos_embedding.py"", line 89, in forward [rank2]:     emb = torch.cat((freqs, freqs), dim=1) [rank2]: RuntimeError: CUDA error: deviceside assert triggered [rank2]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect. [rank2]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1. [rank2]: Compile with `TORCH_USE_CUDA_DSA` to enable deviceside assertions.",2024-07-23T02:35:23Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/942,When creating an issue please use the template.
lausannel,[DOC] Fix wrong llama2 pretrain url in README,"The url link in this section is outdated and invalid, I think it should be updated",2024-07-22T09:16:10Z,,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/941,Marking as stale. No activity in 60 days.,Thanks. I'll include your commit in my internal fix PR.,Your commit is squash merged in https://github.com/NVIDIA/MegatronLM/commit/2488e20d488b35259f6eb53267e8a0525a373599 and your author information is preserved.  Thanks for your contribution. You can close this PR now. ,Thank you!
loadams,Fix yml in python tests,,2024-07-19T22:07:00Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/940
yiakwy-xpu-ml-framework-team,add distributed pdb to enable efficient debugging," Description For researchers and LLM practitioners, we need to debug distributed codes to study how to improve the algorithm. So we need a distributed pdb to help us to debug Megatron (if you need for algorithm, performance finetune) with any scale and any number of python processes, with a single action: > from megatorn.core.remote_pdb import set_trace   Test & Usage To use it , it is very simple. First you need to install tcp utilities: ``` sudo aptget install y rlwrap socat ``` In one terminal , you start a multinodes training; in a second terminal, you can just connect to remote Pdb client (TCP L4 protocal endpoint) to hook into the code stack: > rlwrap socat  tcp:127.0.1.1:43723 !Screenshot 20240719 193242",2024-07-19T12:06:21Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/939,Closed. Prefer to use this standalone repository : https://github.com/yiakwyxpumlframeworkteam/Tooklkitremotepdbforpytorchdistributed
echo-valor,When will megatron Flash attention 3 be supportedï¼Ÿ,"**Your question** Flash attention 3 has been updated, link:  Blogpost: https://tridao.me/blog/2024/flash3/  Paper: https://tridao.me/publications/flash3/flash3.pdf Megatron's support for Flash attention 3 to improve training efficiencyã€‚FlashAttention3 is optimized for Hopper GPUs (e.g. H100).",2024-07-19T09:42:30Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/938
nakroy,[BUG]Get an AtrributeError when trying to finetune llama3-8B model with multi nodes,"**Describe the bug** I try to finetune `llama38B` model with multi nodes but get an AtrributeError when finishing loading mcore format checkpoint and starting to build datasets, the error is below: `AttributeError: '_Llama3Tokenizer' object has no attribute 'unique_identifiers'` **To Reproduce** 1. The finetune  dataset I use is downloading from https://huggingface.co/datasets/tatsulab/alpaca/blob/main/data/train00000of00001a09b74b3ef9c3b56.parquet. I convert it into `json` format after downloading it. 2.  The preprocessing script I used is as follow: ```shell INPUT_FILE=/workspace/dataset/finetune_dataset/train00000of00001a09b74b3ef9c3b56.json MODEL_PATH=/workspace/model_weights/llama38b TOKENIZER_MODEL=${MODEL_PATH}/original/tokenizer.model OUTPUT_DIR=/workspace/dataset/finetune_dataset/llama38b OUTPUT_PREFIX=${OUTPUT_DIR}/alpaca TOKENIZER_TYPE=Llama3Tokenizer mkdir p ${OUTPUT_DIR} python ./tools/preprocess_data.py \ input ${INPUT_FILE} \ outputprefix ${OUTPUT_PREFIX} \ tokenizermodel ${TOKENIZER_MODEL} \ workers 4 \ loginterval 1000 \ tokenizertype ${TOKENIZER_TYPE} \ appendeod ```  3. After preprocessing dataset, I start trainning with 2 nodes, and loading checkpoints successfully, but I met with the problem when building megatron dataset . **Stack trace/logs** The error log is shown as follows: ```shell torchrun nproc_per_node 8 nnodes 2 node_rank 0 master_addr 10.0.1.6 master_port 6543 /workspace/megatron/pretrain_gpt.py tensormodelparallelsize 8 pipelinemodelparallelsize 2 distributedbackend nccl usedistributedoptimizer sequenceparallel overlapgradreduce numlayers 32 hiddensize 4096 numattentionheads 32 groupqueryattention numquerygroups 8 ffnhiddensize 14336 positionembeddingtype rope userotarypositionembeddings rotarybase 500000 maxpositionembeddings 8192 makevocabsizedivisibleby 16128 normepsilon 1e5 normalization RMSNorm swiglu untieembeddingsandoutputweights useflashattn attentionsoftmaxinfp32 logtimerstotensorboard logvalidationppltotensorboard logmemorytotensorboard loginterval 1 attentiondropout 0.0 hiddendropout 0.0 weightdecay 1e1 clipgrad 1.0 adambeta1 0.9 adambeta2 0.95 adameps 1e8 nogradientaccumulationfusion microbatchsize 8 globalbatchsize 256 trainiters 200 disablebiaslinear nobiasgelufusion optimizer adam recomputeactivations recomputegranularity selective seed 2024 initmethodstd 0.01 initiallossscale 4096 lr 1.25e6 lrdecaystyle cosine lrwarmupfraction 0.01 minlr 1.25e7 weightdecay 1e1 load /workspace/model_weights/llama38btp8pp2 finetune noloadoptim noloadrng save /workspace/megatron_train_result/ckpt/llama38B_pretrain_WS16_TP8_PP2 saveinterval 200 bf16 evalinterval 100 evaliters 10 datapath /workspace/dataset/finetune_dataset/llama38b/alpaca_text_document split 949,50,1 seqlength 8192 numworkers 0 tokenizertype Llama3Tokenizer tokenizermodel /workspace/model_weights/llama38b/original/tokenizer.model using world size: 16, dataparallel size: 1, contextparallel size: 1 tensormodelparallel size: 8, pipelinemodelparallel size: 2  WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:Llama3Tokenizer WARNING: Setting args.overlap_p2p_comm to False since noninterleaved schedule does not support overlapping p2p communication accumulate and allreduce gradients in fp32 for bfloat16 data type. using torch.bfloat16 for parameters ...  arguments    accumulate_allreduce_grads_in_fp32 .............. True   adam_beta1 ...................................... 0.9   adam_beta2 ...................................... 0.95   adam_eps ........................................ 1e08   add_bias_linear ................................. False   add_position_embedding .......................... True   add_qkv_bias .................................... False   adlr_autoresume ................................. False   adlr_autoresume_interval ........................ 1000   apply_layernorm_1p .............................. False   apply_query_key_layer_scaling ................... False   apply_residual_connection_post_layernorm ........ False   apply_rope_fusion ............................... True   async_save ...................................... None   async_tensor_model_parallel_allreduce ........... False   attention_dropout ............................... 0.0   attention_softmax_in_fp32 ....................... True   auto_detect_ckpt_format ......................... False   barrier_with_L1_time ............................ True   bert_binary_head ................................ True   bert_embedder_type .............................. megatron   bert_load ....................................... None   bf16 ............................................ True   bias_dropout_fusion ............................. True   bias_gelu_fusion ................................ False   bias_swiglu_fusion .............................. True   biencoder_projection_dim ........................ 0   biencoder_shared_query_context_model ............ False   block_data_path ................................. None   calculate_per_token_loss ........................ False   check_for_nan_in_loss_and_grad .................. True   check_weight_hash_across_dp_replicas_interval ... None   ckpt_assume_constant_structure .................. False   ckpt_fully_parallel_load ........................ False   ckpt_fully_parallel_save ........................ False   ckpt_step ....................................... None   classes_fraction ................................ 1.0   clip_grad ....................................... 1.0   clone_scatter_output_in_embedding ............... True   consumed_train_samples .......................... 0   consumed_valid_samples .......................... 0   context_parallel_size ........................... 1   create_attention_mask_in_dataloader ............. True   cross_entropy_loss_fusion ....................... False   data_cache_path ................................. None   data_parallel_random_init ....................... False   data_parallel_size .............................. 1   data_path ....................................... ['/workspace/dataset/finetune_dataset/llama38b/alpaca_text_document']   data_per_class_fraction ......................... 1.0   data_sharding ................................... True   dataloader_type ................................. single   ddp_average_in_collective ....................... False   ddp_bucket_size ................................. None   decoder_num_layers .............................. None   decoder_seq_length .............................. None   decoupled_lr .................................... None   decoupled_min_lr ................................ None   delay_grad_reduce ............................... True   delay_param_gather .............................. False   deprecated_use_mcore_models ..................... False   deterministic_mode .............................. False   dino_bottleneck_size ............................ 256   dino_freeze_last_layer .......................... 1   dino_head_hidden_size ........................... 2048   dino_local_crops_number ......................... 10   dino_local_img_size ............................. 96   dino_norm_last_layer ............................ False   dino_teacher_temp ............................... 0.07   dino_warmup_teacher_temp ........................ 0.04   dino_warmup_teacher_temp_epochs ................. 30   disable_straggler_on_startup .................... False   dist_ckpt_format ................................ torch_dist   distribute_saved_activations .................... False   distributed_backend ............................. nccl   distributed_timeout_minutes ..................... 10   embedding_path .................................. None   empty_unused_memory_level ....................... 0   enable_one_logger ............................... False   encoder_num_layers .............................. 32   encoder_seq_length .............................. 8192   end_weight_decay ................................ 0.1   eod_mask_loss ................................... False   eval_interval ................................... 100   eval_iters ...................................... 10   evidence_data_path .............................. None   exit_duration_in_mins ........................... None   exit_interval ................................... None   exit_on_missing_checkpoint ...................... False   exit_signal_handler ............................. False   expert_model_parallel_size ...................... 1   ffn_hidden_size ................................. 14336   finetune ........................................ True   fp16 ............................................ False   fp16_lm_cross_entropy ........................... False   fp32_residual_connection ........................ False   fp8 ............................................. None   fp8_amax_compute_algo ........................... most_recent   fp8_amax_history_len ............................ 1   fp8_interval .................................... 1   fp8_margin ...................................... 0   fp8_wgrad ....................................... True   global_batch_size ............................... 256   gradient_accumulation_fusion .................... False   group_query_attention ........................... True   head_lr_mult .................................... 1.0   hidden_dropout .................................. 0.0   hidden_size ..................................... 4096   hybrid_attention_ratio .......................... 0.0   hybrid_mlp_ratio ................................ 0.0   hybrid_override_pattern ......................... None   hysteresis ...................................... 2   ict_head_size ................................... None   ict_load ........................................ None   img_h ........................................... 224   img_w ........................................... 224   indexer_batch_size .............................. 128   indexer_log_interval ............................ 1000   inference_batch_times_seqlen_threshold .......... 512   init_method_std ................................. 0.01   init_method_xavier_uniform ...................... False   initial_loss_scale .............................. 4096.0   iter_per_epoch .................................. 1250   kv_channels ..................................... 128   lazy_mpu_init ................................... None   load ............................................ /workspace/model_weights/llama38btp8pp2   local_rank ...................................... None   log_batch_size_to_tensorboard ................... False   log_interval .................................... 1   log_learning_rate_to_tensorboard ................ True   log_loss_scale_to_tensorboard ................... True   log_memory_to_tensorboard ....................... True   log_num_zeros_in_grad ........................... False   log_params_norm ................................. False   log_progress .................................... False   log_straggler ................................... False   log_throughput .................................. False   log_timers_to_tensorboard ....................... True   log_validation_ppl_to_tensorboard ............... True   log_world_size_to_tensorboard ................... False   logging_level ................................... None   loss_scale ...................................... None   loss_scale_window ............................... 1000   lr .............................................. 1.25e06   lr_decay_iters .................................. None   lr_decay_samples ................................ None   lr_decay_style .................................. cosine   lr_warmup_fraction .............................. 0.01   lr_warmup_init .................................. 0.0   lr_warmup_iters ................................. 0   lr_warmup_samples ............................... 0   lr_wsd_decay_iters .............................. None   lr_wsd_decay_samples ............................ None   lr_wsd_decay_style .............................. exponential   make_vocab_size_divisible_by .................... 16128   manual_gc ....................................... False   manual_gc_eval .................................. True   manual_gc_interval .............................. 0   mask_factor ..................................... 1.0   mask_prob ....................................... 0.15   mask_type ....................................... random   masked_softmax_fusion ........................... True   max_position_embeddings ......................... 8192   max_tokens_to_oom ............................... 12000   merge_file ...................................... None   micro_batch_size ................................ 8   min_loss_scale .................................. 1.0   min_lr .......................................... 1.25e07   mmap_bin_files .................................. True   mock_data ....................................... False   moe_aux_loss_coeff .............................. 0.0   moe_expert_capacity_factor ...................... None   moe_extended_tp ................................. False   moe_grouped_gemm ................................ False   moe_input_jitter_eps ............................ None   moe_layer_recompute ............................. False   moe_pad_expert_input_to_capacity ................ False   moe_per_layer_logging ........................... False   moe_router_load_balancing_type .................. aux_loss   moe_router_topk ................................. 2   moe_token_dispatcher_type ....................... allgather   moe_token_drop_policy ........................... probs   moe_z_loss_coeff ................................ None   nccl_communicator_config_path ................... None   no_load_optim ................................... True   no_load_rng ..................................... True   no_persist_layer_norm ........................... False   no_save_optim ................................... None   no_save_rng ..................................... None   norm_epsilon .................................... 1e05   normalization ................................... RMSNorm   num_attention_heads ............................. 32   num_channels .................................... 3   num_classes ..................................... 1000   num_dataset_builder_threads ..................... 1   num_experts ..................................... None   num_layers ...................................... 32   num_layers_per_virtual_pipeline_stage ........... None   num_query_groups ................................ 8   num_workers ..................................... 0   one_logger_entity ............................... hwinf_dcm   one_logger_project .............................. e2etracking   one_logger_run_name ............................. None   onnx_safe ....................................... None   openai_gelu ..................................... False   optimizer ....................................... adam   output_bert_embeddings .......................... False   overlap_grad_reduce ............................. True   overlap_p2p_comm ................................ False   overlap_param_gather ............................ False   override_opt_param_scheduler .................... False   params_dtype .................................... torch.bfloat16   patch_dim ....................................... 16   perform_initialization .......................... True   pipeline_model_parallel_size .................... 2   pipeline_model_parallel_split_rank .............. None   position_embedding_type ......................... rope   pretrained_checkpoint ........................... None   profile ......................................... False   profile_ranks ................................... [0]   profile_step_end ................................ 12   profile_step_start .............................. 10   qk_layernorm .................................... False   query_in_block_prob ............................. 0.1   rampup_batch_size ............................... None   rank ............................................ 0   recompute_granularity ........................... selective   recompute_method ................................ None   recompute_num_layers ............................ None   reset_attention_mask ............................ False   reset_position_ids .............................. False   retriever_report_topk_accuracies ................ []   retriever_score_scaling ......................... False   retriever_seq_length ............................ 256   retro_add_retriever ............................. False   retro_attention_gate ............................ 1   retro_cyclic_train_iters ........................ None   retro_encoder_attention_dropout ................. 0.1   retro_encoder_hidden_dropout .................... 0.1   retro_encoder_layers ............................ 2   retro_num_neighbors ............................. 2   retro_num_retrieved_chunks ...................... 2   retro_project_dir ............................... None   retro_verify_neighbor_count ..................... True   rotary_base ..................................... 500000   rotary_interleaved .............................. False   rotary_percent .................................. 1.0   rotary_seq_len_interpolation_factor ............. None   sample_rate ..................................... 1.0   save ............................................ /workspace/megatron_train_result/ckpt/llama38B_pretrain_WS16_TP8_PP2   save_interval ................................... 200   scatter_gather_tensors_in_pipeline .............. True   seed ............................................ 2024   seq_length ...................................... 8192   sequence_parallel ............................... True   sgd_momentum .................................... 0.9   short_seq_prob .................................. 0.1   skip_train ...................................... False   spec ............................................ None   split ........................................... 949,50,1   squared_relu .................................... False   standalone_embedding_stage ...................... False   start_weight_decay .............................. 0.1   straggler_ctrlr_port ............................ 65535   straggler_minmax_count .......................... 1   swiglu .......................................... True   swin_backbone_type .............................. tiny   tensor_model_parallel_size ...................... 8   tensorboard_dir ................................. None   tensorboard_log_interval ........................ 1   tensorboard_queue_size .......................... 1000   test_data_path .................................. None   test_mode ....................................... False   timing_log_level ................................ 0   timing_log_option ............................... minmax   titles_data_path ................................ None   tokenizer_model ................................. /workspace/model_weights/llama38b/original/tokenizer.model   tokenizer_type .................................. Llama3Tokenizer   tp_comm_bulk_dgrad .............................. True   tp_comm_bulk_wgrad .............................. True   tp_comm_overlap ................................. False   tp_comm_overlap_ag .............................. True   tp_comm_overlap_cfg ............................. None   tp_comm_overlap_rs .............................. True   tp_comm_overlap_rs_dgrad ........................ False   tp_comm_split_ag ................................ True   tp_comm_split_rs ................................ True   train_data_path ................................. None   train_iters ..................................... 200   train_samples ................................... None   transformer_impl ................................ transformer_engine   transformer_pipeline_model_parallel_size ........ 2   untie_embeddings_and_output_weights ............. True   use_checkpoint_args ............................. False   use_checkpoint_opt_param_scheduler .............. False   use_cpu_initialization .......................... None   use_dist_ckpt ................................... False   use_distributed_optimizer ....................... True   use_flash_attn .................................. True   use_legacy_models ............................... False   use_one_sent_docs ............................... False   use_ring_exchange_p2p ........................... False   use_rotary_position_embeddings .................. True   use_tp_pp_dp_mapping ............................ False   valid_data_path ................................. None   variable_seq_lengths ............................ False   virtual_pipeline_model_parallel_size ............ None   vision_backbone_type ............................ vit   vision_pretraining .............................. False   vision_pretraining_type ......................... classify   vocab_extra_ids ................................. 0   vocab_file ...................................... None   vocab_size ...................................... None   wandb_exp_name ..................................    wandb_project ...................................    wandb_save_dir ..................................    weight_decay .................................... 0.1   weight_decay_incr_style ......................... constant   world_size ...................................... 16   yaml_cfg ........................................ None  end of arguments  setting number of microbatches to constant 32 > building Llama3Tokenizer tokenizer ... INFO:llama.tokenizer:Reloaded tiktoken model from /workspace/model_weights/llama38b/original/tokenizer.model INFO:llama.tokenizer:words: 128256  BOS ID: 128000  EOS ID: 128001 INFO:llama.tokenizer:Reloaded tiktoken model from /workspace/model_weights/llama38b/original/tokenizer.model INFO:llama.tokenizer:words: 128256  BOS ID: 128000  EOS ID: 128001 INFO:llama.tokenizer:Reloaded tiktoken model from /workspace/model_weights/llama38b/original/tokenizer.model INFO:llama.tokenizer:words: 128256  BOS ID: 128000  EOS ID: 128001 INFO:llama.tokenizer:Reloaded tiktoken model from /workspace/model_weights/llama38b/original/tokenizer.model INFO:llama.tokenizer:words: 128256  BOS ID: 128000  EOS ID: 128001 INFO:llama.tokenizer:Reloaded tiktoken model from /workspace/model_weights/llama38b/original/tokenizer.model INFO:llama.tokenizer:words: 128256  BOS ID: 128000  EOS ID: 128001  > padded vocab (size: 128256) with 768 dummy tokens (new size: 129024) > initializing torch distributed ... INFO:llama.tokenizer:Reloaded tiktoken model from /workspace/model_weights/llama38b/original/tokenizer.model INFO:llama.tokenizer:words: 128256  BOS ID: 128000  EOS ID: 128001 > initialized tensor model parallel with size 8 > initialized pipeline model parallel with size 2 > setting random seeds to 2024 ... > compiling dataset index builder ... make: Entering directory '/workspace/megatron/megatron/core/datasets' INFO:llama.tokenizer:Reloaded tiktoken model from /workspace/model_weights/llama38b/original/tokenizer.model INFO:llama.tokenizer:words: 128256  BOS ID: 128000  EOS ID: 128001 INFO:llama.tokenizer:Reloaded tiktoken model from /workspace/model_weights/llama38b/original/tokenizer.model INFO:llama.tokenizer:words: 128256  BOS ID: 128000  EOS ID: 128001 make: Nothing to be done for 'default'. make: Leaving directory '/workspace/megatron/megatron/core/datasets' >>> done with dataset index builder. Compilation time: 0.935 seconds > compiling and loading fused kernels ... NCCL version 2.21.5+cuda12.5 >>> done with compiling and loading fused kernels. Compilation time: 9.434 seconds [rank1]:[W719 08:56:38.529363367 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank0]:[W719 08:56:38.529412190 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank2]:[W719 08:56:38.529412186 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank3]:[W719 08:56:38.529511793 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank5]:[W719 08:56:38.529562878 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank4]:[W719 08:56:38.529565198 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank7]:[W719 08:56:38.529627099 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank6]:[W719 08:56:38.529662246 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) time to initialize megatron (seconds): 14.767 [after megatron is initialized] datetime: 20240719 08:56:42  building GPT model ...  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 502398976  > number of parameters on (tensor, pipeline) model parallel rank (4, 0): 502398976  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 502398976  > number of parameters on (tensor, pipeline) model parallel rank (5, 0): 502398976  > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 502398976  > number of parameters on (tensor, pipeline) model parallel rank (6, 0): 502398976  > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 502398976  > number of parameters on (tensor, pipeline) model parallel rank (7, 0): 502398976 INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=True, overlap_grad_reduce=True, use_distributed_optimizer=True, check_for_nan_in_grad=True, bucket_size=40000000, average_in_collective=False) INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient allreduce / reducescatter: 9 Params for bucket 1 (49291264 elements):         module.decoder.layers.15.mlp.linear_fc2.weight         module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.15.mlp.linear_fc1.weight         module.decoder.layers.15.self_attention.linear_qkv.weight         module.decoder.layers.14.mlp.linear_fc2.weight         module.decoder.layers.14.mlp.linear_fc1.weight         module.decoder.layers.15.self_attention.linear_proj.weight Params for bucket 2 (54542336 elements):         module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.14.self_attention.linear_qkv.weight         module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.13.mlp.linear_fc2.weight         module.decoder.layers.13.mlp.linear_fc1.weight         module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.13.self_attention.linear_qkv.weight         module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.12.mlp.linear_fc1.weight         module.decoder.layers.14.self_attention.linear_proj.weight         module.decoder.layers.13.self_attention.linear_proj.weight         module.decoder.layers.12.mlp.linear_fc2.weight Params for bucket 3 (54542336 elements):         module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.11.mlp.linear_fc2.weight         module.decoder.layers.12.self_attention.linear_qkv.weight         module.decoder.layers.11.mlp.linear_fc1.weight         module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.11.self_attention.linear_qkv.weight         module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.10.mlp.linear_fc2.weight         module.decoder.layers.10.mlp.linear_fc1.weight         module.decoder.layers.12.self_attention.linear_proj.weight         module.decoder.layers.11.self_attention.linear_proj.weight Params for bucket 4 (54542336 elements):         module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.10.self_attention.linear_qkv.weight         module.decoder.layers.10.self_attention.linear_proj.weight         module.decoder.layers.9.self_attention.linear_proj.weight         module.decoder.layers.9.mlp.linear_fc2.weight         module.decoder.layers.9.mlp.linear_fc1.weight         module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.9.self_attention.linear_qkv.weight         module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.8.mlp.linear_fc2.weight         module.decoder.layers.8.mlp.linear_fc1.weight Params for bucket 5 (54542336 elements):         module.decoder.layers.8.self_attention.linear_proj.weight         module.decoder.layers.7.self_attention.linear_proj.weight         module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.8.self_attention.linear_qkv.weight         module.decoder.layers.7.mlp.linear_fc2.weight         module.decoder.layers.7.mlp.linear_fc1.weight         module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.7.self_attention.linear_qkv.weight         module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.6.mlp.linear_fc2.weight         module.decoder.layers.6.mlp.linear_fc1.weight Params for bucket 6 (54542336 elements):         module.decoder.layers.6.self_attention.linear_proj.weight         module.decoder.layers.5.self_attention.linear_proj.weight         module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.5.mlp.linear_fc2.weight         module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.6.self_attention.linear_qkv.weight         module.decoder.layers.5.mlp.linear_fc1.weight         module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.5.self_attention.linear_qkv.weight         module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.4.mlp.linear_fc2.weight         module.decoder.layers.4.mlp.linear_fc1.weight Params for bucket 7 (54542336 elements):         module.decoder.layers.4.self_attention.linear_qkv.weight         module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.2.mlp.linear_fc2.weight         module.decoder.layers.2.mlp.linear_fc1.weight         module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.4.self_attention.linear_proj.weight         module.decoder.layers.3.mlp.linear_fc2.weight         module.decoder.layers.3.mlp.linear_fc1.weight         module.decoder.layers.3.self_attention.linear_qkv.weight         module.decoder.layers.3.self_attention.linear_proj.weight Params for bucket 8 (54542336 elements):         module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.2.self_attention.linear_proj.weight         module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.2.self_attention.linear_qkv.weight         module.decoder.layers.1.mlp.linear_fc2.weight         module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.1.self_attention.linear_proj.weight         module.decoder.layers.1.mlp.linear_fc1.weight         module.decoder.layers.1.self_attention.linear_qkv.weight         module.decoder.layers.0.mlp.linear_fc2.weight         module.decoder.layers.0.mlp.linear_fc1.weight Params for bucket 9 (71311360 elements):         module.decoder.layers.0.self_attention.linear_proj.weight         module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight         module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight         module.decoder.layers.0.self_attention.linear_qkv.weight         module.embedding.word_embeddings.weight INFO:megatron.core.optimizer:Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=1.25e06, min_lr=1.25e07, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=False, bf16=True, params_dtype=torch.bfloat16, loss_scale=None, initial_loss_scale=4096.0, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e08, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_grad_reduce=True, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=) > learning rate decay style: cosine  loading checkpoint from /workspace/model_weights/llama38btp8pp2 at iteration 1 could not find arguments in the checkpoint ...  checkpoint version 3.0   successfully loaded checkpoint from /workspace/model_weights/llama38btp8pp2 [ t 0, p 0 ] at iteration 0 [after model, optimizer, and learning rate scheduler are built] datetime: 20240719 08:56:43  > building train, validation, and test datasets ...  > datasets target sizes (minimum size):     train:      51200     validation: 7680     test:       2560 INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)] > building train, validation, and test datasets for GPT ... INFO:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(51200, 7680, 2560), and config=GPTDatasetConfig(random_seed=2024, sequence_length=8192, blend=(['/workspace/dataset/finetune_dataset/llama38b/alpaca_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], num_dataset_builder_threads=1, path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=._Llama3Tokenizer object at 0x7fca952132b0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True, drop_last_partial_validation_sequence=True, add_extra_token_to_sequence=True) INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /workspace/dataset/finetune_dataset/llama38b/alpaca_text_document.idx INFO:megatron.core.datasets.indexed_dataset:    Extract the sequence lengths INFO:megatron.core.datasets.indexed_dataset:    Extract the sequence pointers INFO:megatron.core.datasets.indexed_dataset:    Extract the document indices INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 52002 INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 52002 [rank0]: Traceback (most recent call last): [rank0]:   File ""/workspace/megatron/pretrain_gpt.py"", line 243, in  [rank0]:     pretrain( [rank0]:   File ""/workspace/megatron/megatron/training/training.py"", line 251, in pretrain [rank0]:     = build_train_valid_test_data_iterators( [rank0]:   File ""/workspace/megatron/megatron/training/training.py"", line 1467, in build_train_valid_test_data_iterators [rank0]:     build_train_valid_test_data_loaders( [rank0]:   File ""/workspace/megatron/megatron/training/training.py"", line 1428, in build_train_valid_test_data_loaders [rank0]:     train_ds, valid_ds, test_ds = build_train_valid_test_datasets( [rank0]:   File ""/workspace/megatron/megatron/training/training.py"", line 1398, in build_train_valid_test_datasets [rank0]:     return build_train_valid_test_datasets_provider(train_valid_test_num_samples) [rank0]:   File ""/workspace/megatron/pretrain_gpt.py"", line 231, in train_valid_test_datasets_provider [rank0]:     ).build() [rank0]:   File ""/workspace/megatron/megatron/core/datasets/blended_megatron_dataset_builder.py"", line 126, in build [rank0]:     datasets = self._build_blended_dataset_splits() [rank0]:   File ""/workspace/megatron/megatron/core/datasets/blended_megatron_dataset_builder.py"", line 175, in _build_blended_dataset_splits [rank0]:     return self._build_megatron_dataset_splits(prefixes[0], split, self.sizes) [rank0]:   File ""/workspace/megatron/megatron/core/datasets/blended_megatron_dataset_builder.py"", line 408, in _build_megatron_dataset_splits [rank0]:     self.build_generic_dataset( [rank0]:   File ""/workspace/megatron/megatron/core/datasets/blended_megatron_dataset_builder.py"", line 456, in build_generic_dataset [rank0]:     dataset = cls(*args) [rank0]:   File ""/workspace/megatron/megatron/core/datasets/gpt_dataset.py"", line 88, in __init__ [rank0]:     super().__init__( [rank0]:   File ""/workspace/megatron/megatron/core/datasets/megatron_dataset.py"", line 61, in __init__ [rank0]:     self.unique_description = json.dumps( [rank0]:   File ""/usr/lib/python3.10/json/__init__.py"", line 238, in dumps [rank0]:     **kw).encode(obj) [rank0]:   File ""/usr/lib/python3.10/json/encoder.py"", line 201, in encode [rank0]:     chunks = list(chunks) [rank0]:   File ""/usr/lib/python3.10/json/encoder.py"", line 431, in _iterencode [rank0]:     yield from _iterencode_dict(o, _current_indent_level) [rank0]:   File ""/usr/lib/python3.10/json/encoder.py"", line 405, in _iterencode_dict [rank0]:     yield from chunks [rank0]:   File ""/usr/lib/python3.10/json/encoder.py"", line 438, in _iterencode [rank0]:     o = _default(o) [rank0]:   File ""/workspace/megatron/megatron/core/datasets/megatron_dataset.py"", line 62, in  [rank0]:     self.unique_identifiers, indent=4, default=lambda obj: obj.unique_identifiers [rank0]: AttributeError: '_Llama3Tokenizer' object has no attribute 'unique_identifiers' ```  **Environment (please complete the following information):**   MegatronLM commit ID: 86850db   Nvidia docker image: nvcr.io/nvidia/pytorch:24.06py3 ",2024-07-19T09:38:33Z,stale,open,0,5,https://github.com/NVIDIA/Megatron-LM/issues/937,"I switch to use 'HuggingFaceTokenizer' as the arg 'tokenizertype', but there will be some other bugs.","The problem is that `unique_identifiers` is not implemented in Llama3Tokenizer which is not inherited from MegatronTokenizer. Changing the lines https://github.com/NVIDIA/MegatronLM/blob/9bcd4175becc515331537f0c78eb70079de0eaa8/megatron/training/tokenizer/tokenizer.pyL567L569  to following should solve the problem: ```     class _Llama3Tokenizer(Llama3Tokenizer):         def __init__(self, *args, **kwargs):             super().__init__(*args, **kwargs)             self.unique_identifiers = OrderedDict()             self.unique_identifiers[""class""] = type(self).__name__             self.unique_identifiers[""tokenizer_path""] = args if len(args) > 0 else [""n/a""]             for option in kwargs:                 self.unique_identifiers[option] = str(kwargs[option])             self.unique_description = json.dumps(self.unique_identifiers, indent=4) ```","> The problem is that `unique_identifiers` is not implemented in Llama3Tokenizer which is not inherited from MegatronTokenizer. Changing the lines >  > https://github.com/NVIDIA/MegatronLM/blob/9bcd4175becc515331537f0c78eb70079de0eaa8/megatron/training/tokenizer/tokenizer.pyL567L569 >  >  > to following should solve the problem: > ``` >     class _Llama3Tokenizer(Llama3Tokenizer): >         def __init__(self, *args, **kwargs): >             super().__init__(*args, **kwargs) >             self.unique_identifiers = OrderedDict() >             self.unique_identifiers[""class""] = type(self).__name__ >             self.unique_identifiers[""tokenizer_path""] = args if len(args) > 0 else [""n/a""] >             for option in kwargs: >                 self.unique_identifiers[option] = str(kwargs[option]) >  >             self.unique_description = json.dumps(self.unique_identifiers, indent=4) > ``` Thanks, it works for me. It seems that Llama3Tokenizer still needs to be fixed with some little problems before it really can be used for finetuning properly...","> I switch to use 'HuggingFaceTokenizer' as the arg 'tokenizertype', but there will be some other bugs. I think Llama3Tokenizer is the suitable one for llama3 model training, but it's unstable while using it as far, or maybe the arguments I set is not proper, because I just changed some arguments from the scripts that I used to finetune llama2",Marking as stale. No activity in 60 days.
clarence-lee-sheng,[QUESTION] Calculations regarding calculate_per_token_loss parameter ,"In line 231233 in megatron/core/pipeline_parallel/schedules.py (megatron/core/pipeline_parallel/schedules.py), I have two questions:  1. Why are we dividing by num_tokens when the conditional is ""if not config.calculate_per_token_loss"" 2. What is the purpose of dividing by num_microbatches if it is a constant, and if it is important, why do we not also divide by num_microbatches outside of the condition for the config.calculate_per_token_loss true case. ",2024-07-19T09:33:27Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/936
wccccp,terminate called after throwing an instance of 'c10::DistBackendError',"frame CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string, std::allocator >) + 0x99 (0x7fe76ab98969 in /usr/local/lib/python3.10/distpackages/torch/lib/libc10.so) frame CC(Faster dataloader merge): c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional > >) + 0x1e1 (0x7fe705ea04e1 in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(rename confusing arg): c10d::ProcessGroupNCCL::watchdogHandler() + 0x222 (0x7fe705ea81a2 in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(Compatibility with pytorchtransformers for finetuning ): c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7fe705ea9a0f in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(Note that as of now you need to have PySOL cloned to the directory here before building the container.):  + 0xdc253 (0x7fe76a6b0253 in /usr/lib/x86_64linuxgnu/libstdc++.so.6) frame CC(I do not know how to solve it.):  + 0x94ac3 (0x7fe7765d9ac3 in /usr/lib/x86_64linuxgnu/libc.so.6) frame CC(perplexity too big for gpt2 wikitext evaluation): clone + 0x44 (0x7fe77666aa04 in /usr/lib/x86_64linuxgnu/libc.so.6) terminate called after throwing an instance of 'c10::DistBackendError'   what():  [PG 1 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=199, OpType=_REDUCE_SCATTER_BASE, NumelIn=7098994688, NumelOut=443687168, Timeout(ms)=600000) ran for 882725 milliseconds before timing out. Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:567 (most recent call first): frame CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string, std::allocator >) + 0x99 (0x7fe76ab98969 in /usr/local/lib/python3.10/distpackages/torch/lib/libc10.so) frame CC(Faster dataloader merge): c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional > >) + 0x1e1 (0x7fe705ea04e1 in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(rename confusing arg): c10d::ProcessGroupNCCL::watchdogHandler() + 0x222 (0x7fe705ea81a2 in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(Compatibility with pytorchtransformers for finetuning ): c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7fe705ea9a0f in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(Note that as of now you need to have PySOL cloned to the directory here before building the container.):  + 0xdc253 (0x7fe76a6b0253 in /usr/lib/x86_64linuxgnu/libstdc++.so.6) frame CC(I do not know how to solve it.):  + 0x94ac3 (0x7fe7765d9ac3 in /usr/lib/x86_64linuxgnu/libc.so.6) frame CC(perplexity too big for gpt2 wikitext evaluation): clone + 0x44 (0x7fe77666aa04 in /usr/lib/x86_64linuxgnu/libc.so.6) Exception raised from ncclCommWatchdog at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1450 (most recent call first): frame CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string, std::allocator >) + 0x99 (0x7fe76ab98969 in /usr/local/lib/python3.10/distpackages/torch/lib/libc10.so) frame CC(Faster dataloader merge):  + 0x10485fe (0x7fe705ed05fe in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(rename confusing arg):  + 0xcbc925 (0x7fe705b44925 in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(Compatibility with pytorchtransformers for finetuning ):  + 0xdc253 (0x7fe76a6b0253 in /usr/lib/x86_64linuxgnu/libstdc++.so.6) frame CC(Note that as of now you need to have PySOL cloned to the directory here before building the container.):  + 0x94ac3 (0x7fe7765d9ac3 in /usr/lib/x86_64linuxgnu/libc.so.6) frame CC(I do not know how to solve it.): clone + 0x44 (0x7fe77666aa04 in /usr/lib/x86_64linuxgnu/libc.so.6) [rank6]:[E719 09:25:40.733826925 ProcessGroupNCCL.cpp:1606] [PG 1 Rank 6] Timeout at NCCL work: 199, last enqueued NCCL work: 199, last completed NCCL work: 198. [rank6]:[E719 09:25:40.733860630 ProcessGroupNCCL.cpp:579] [Rank 6] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data. [rank6]:[E719 09:25:40.733868816 ProcessGroupNCCL.cpp:585] [Rank 6] To avoid data inconsistency, we are taking the entire process down. [rank6]:[E719 09:25:40.734423949 ProcessGroupNCCL.cpp:1446] [PG 1 Rank 6] Process group watchdog thread terminated with exception: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=199, OpType=_REDUCE_SCATTER_BASE, NumelIn=7098994688, NumelOut=443687168, Timeout(ms)=600000) ran for 882722 milliseconds before timing out. Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:567 (most recent call first): frame CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string, std::allocator >) + 0x99 (0x7fb635d98969 in /usr/local/lib/python3.10/distpackages/torch/lib/libc10.so) frame CC(Faster dataloader merge): c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional > >) + 0x1e1 (0x7fb5d10a04e1 in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(rename confusing arg): c10d::ProcessGroupNCCL::watchdogHandler() + 0x222 (0x7fb5d10a81a2 in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(Compatibility with pytorchtransformers for finetuning ): c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7fb5d10a9a0f in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(Note that as of now you need to have PySOL cloned to the directory here before building the container.):  + 0xdc253 (0x7fb6358b0253 in /usr/lib/x86_64linuxgnu/libstdc++.so.6) frame CC(I do not know how to solve it.):  + 0x94ac3 (0x7fb641802ac3 in /usr/lib/x86_64linuxgnu/libc.so.6) frame CC(perplexity too big for gpt2 wikitext evaluation): clone + 0x44 (0x7fb641893a04 in /usr/lib/x86_64linuxgnu/libc.so.6) terminate called after throwing an instance of 'c10::DistBackendError'   what():  [PG 1 Rank 6] Process group watchdog thread terminated with exception: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=199, OpType=_REDUCE_SCATTER_BASE, NumelIn=7098994688, NumelOut=443687168, Timeout(ms)=600000) ran for 882722 milliseconds before timing out. Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:567 (most recent call first): frame CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string, std::allocator >) + 0x99 (0x7fb635d98969 in /usr/local/lib/python3.10/distpackages/torch/lib/libc10.so) frame CC(Faster dataloader merge): c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional > >) + 0x1e1 (0x7fb5d10a04e1 in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(rename confusing arg): c10d::ProcessGroupNCCL::watchdogHandler() + 0x222 (0x7fb5d10a81a2 in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(Compatibility with pytorchtransformers for finetuning ): c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7fb5d10a9a0f in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(Note that as of now you need to have PySOL cloned to the directory here before building the container.):  + 0xdc253 (0x7fb6358b0253 in /usr/lib/x86_64linuxgnu/libstdc++.so.6) frame CC(I do not know how to solve it.):  + 0x94ac3 (0x7fb641802ac3 in /usr/lib/x86_64linuxgnu/libc.so.6) frame CC(perplexity too big for gpt2 wikitext evaluation): clone + 0x44 (0x7fb641893a04 in /usr/lib/x86_64linuxgnu/libc.so.6) Exception raised from ncclCommWatchdog at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1450 (most recent call first): frame CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string, std::allocator >) + 0x99 (0x7fb635d98969 in /usr/local/lib/python3.10/distpackages/torch/lib/libc10.so) frame CC(Faster dataloader merge):  + 0x10485fe (0x7fb5d10d05fe in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(rename confusing arg):  + 0xcbc925 (0x7fb5d0d44925 in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(Compatibility with pytorchtransformers for finetuning ):  + 0xdc253 (0x7fb6358b0253 in /usr/lib/x86_64linuxgnu/libstdc++.so.6) frame CC(Note that as of now you need to have PySOL cloned to the directory here before building the container.):  + 0x94ac3 (0x7fb641802ac3 in /usr/lib/x86_64linuxgnu/libc.so.6) frame CC(I do not know how to solve it.): clone + 0x44 (0x7fb641893a04 in /usr/lib/x86_64linuxgnu/libc.so.6) timaker8b8djlxvfemaster0:156:1120 [7] NCCL INFO comm 0x562232115d50 rank 7 nranks 16 cudaDev 7 busId d0000  Abort COMPLETE [rank7]:[E719 09:25:40.740359294 ProcessGroupNCCL.cpp:1606] [PG 1 Rank 7] Timeout at NCCL work: 199, last enqueued NCCL work: 199, last completed NCCL work: 198. [rank7]:[E719 09:25:40.740413970 ProcessGroupNCCL.cpp:579] [Rank 7] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data. [rank7]:[E719 09:25:40.740434350 ProcessGroupNCCL.cpp:585] [Rank 7] To avoid data inconsistency, we are taking the entire process down. [rank7]:[E719 09:25:40.740575966 ProcessGroupNCCL.cpp:1446] [PG 1 Rank 7] Process group watchdog thread terminated with exception: [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=199, OpType=_REDUCE_SCATTER_BASE, NumelIn=7098994688, NumelOut=443687168, Timeout(ms)=600000) ran for 882725 milliseconds before timing out. Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:567 (most recent call first): frame CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string, std::allocator >) + 0x99 (0x7ff9a0afb969 in /usr/local/lib/python3.10/distpackages/torch/lib/libc10.so) frame CC(Faster dataloader merge): c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional > >) + 0x1e1 (0x7ff93bea04e1 in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(rename confusing arg): c10d::ProcessGroupNCCL::watchdogHandler() + 0x222 (0x7ff93bea81a2 in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(Compatibility with pytorchtransformers for finetuning ): c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7ff93bea9a0f in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(Note that as of now you need to have PySOL cloned to the directory here before building the container.):  + 0xdc253 (0x7ff9a06b0253 in /usr/lib/x86_64linuxgnu/libstdc++.so.6) frame CC(I do not know how to solve it.):  + 0x94ac3 (0x7ff9ac55dac3 in /usr/lib/x86_64linuxgnu/libc.so.6) frame CC(perplexity too big for gpt2 wikitext evaluation): clone + 0x44 (0x7ff9ac5eea04 in /usr/lib/x86_64linuxgnu/libc.so.6) terminate called after throwing an instance of 'c10::DistBackendError'   what():  [PG 1 Rank 7] Process group watchdog thread terminated with exception: [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=199, OpType=_REDUCE_SCATTER_BASE, NumelIn=7098994688, NumelOut=443687168, Timeout(ms)=600000) ran for 882725 milliseconds before timing out. Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:567 (most recent call first): frame CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string, std::allocator >) + 0x99 (0x7ff9a0afb969 in /usr/local/lib/python3.10/distpackages/torch/lib/libc10.so) frame CC(Faster dataloader merge): c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional > >) + 0x1e1 (0x7ff93bea04e1 in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(rename confusing arg): c10d::ProcessGroupNCCL::watchdogHandler() + 0x222 (0x7ff93bea81a2 in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(Compatibility with pytorchtransformers for finetuning ): c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7ff93bea9a0f in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(Note that as of now you need to have PySOL cloned to the directory here before building the container.):  + 0xdc253 (0x7ff9a06b0253 in /usr/lib/x86_64linuxgnu/libstdc++.so.6) frame CC(I do not know how to solve it.):  + 0x94ac3 (0x7ff9ac55dac3 in /usr/lib/x86_64linuxgnu/libc.so.6) frame CC(perplexity too big for gpt2 wikitext evaluation): clone + 0x44 (0x7ff9ac5eea04 in /usr/lib/x86_64linuxgnu/libc.so.6) Exception raised from ncclCommWatchdog at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1450 (most recent call first): frame CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string, std::allocator >) + 0x99 (0x7ff9a0afb969 in /usr/local/lib/python3.10/distpackages/torch/lib/libc10.so) frame CC(Faster dataloader merge):  + 0x10485fe (0x7ff93bed05fe in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(rename confusing arg):  + 0xcbc925 (0x7ff93bb44925 in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(Compatibility with pytorchtransformers for finetuning ):  + 0xdc253 (0x7ff9a06b0253 in /usr/lib/x86_64linuxgnu/libstdc++.so.6) frame CC(Note that as of now you need to have PySOL cloned to the directory here before building the container.):  + 0x94ac3 (0x7ff9ac55dac3 in /usr/lib/x86_64linuxgnu/libc.so.6) frame CC(I do not know how to solve it.): clone + 0x44 (0x7ff9ac5eea04 in /usr/lib/x86_64linuxgnu/libc.so.6) timaker8b8djlxvfemaster0:154:1112 [5] NCCL INFO comm 0x563b1cff6180 rank 5 nranks 16 cudaDev 5 busId 98000  Abort COMPLETE timaker8b8djlxvfemaster0:152:1125 [3] NCCL INFO comm 0x55c0f86c8470 rank 3 nranks 16 cudaDev 3 busId 50000  Abort COMPLETE timaker8b8djlxvfemaster0:151:1143 [2] NCCL INFO comm 0x563f7888ec70 rank 2 nranks 16 cudaDev 2 busId 4a000  Abort COMPLETE [rank5]:[E719 09:25:40.746336670 ProcessGroupNCCL.cpp:1606] [PG 1 Rank 5] Timeout at NCCL work: 199, last enqueued NCCL work: 199, last completed NCCL work: 198. [rank5]:[E719 09:25:40.746363262 ProcessGroupNCCL.cpp:579] [Rank 5] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data. [rank5]:[E719 09:25:40.746373402 ProcessGroupNCCL.cpp:585] [Rank 5] To avoid data inconsistency, we are taking the entire process down. [rank5]:[E719 09:25:40.746461013 ProcessGroupNCCL.cpp:1446] [PG 1 Rank 5] Process group watchdog thread terminated with exception: [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=199, OpType=_REDUCE_SCATTER_BASE, NumelIn=7098994688, NumelOut=443687168, Timeout(ms)=600000) ran for 882737 milliseconds before timing out. Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:567 (most recent call first): frame CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string, std::allocator >) + 0x99 (0x7fb2d6398969 in /usr/local/lib/python3.10/distpackages/torch/lib/libc10.so) frame CC(Faster dataloader merge): c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional > >) + 0x1e1 (0x7fb2716a04e1 in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(rename confusing arg): c10d::ProcessGroupNCCL::watchdogHandler() + 0x222 (0x7fb2716a81a2 in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(Compatibility with pytorchtransformers for finetuning ): c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7fb2716a9a0f in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(Note that as of now you need to have PySOL cloned to the directory here before building the container.):  + 0xdc253 (0x7fb2d5eb0253 in /usr/lib/x86_64linuxgnu/libstdc++.so.6) frame CC(I do not know how to solve it.):  + 0x94ac3 (0x7fb2e1e42ac3 in /usr/lib/x86_64linuxgnu/libc.so.6) frame CC(perplexity too big for gpt2 wikitext evaluation): clone + 0x44 (0x7fb2e1ed3a04 in /usr/lib/x86_64linuxgnu/libc.so.6) terminate called after throwing an instance of 'c10::DistBackendError'   what():  [PG 1 Rank 5] Process group watchdog thread terminated with exception: [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=199, OpType=_REDUCE_SCATTER_BASE, NumelIn=7098994688, NumelOut=443687168, Timeout(ms)=600000) ran for 882737 milliseconds before timing out. Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:567 (most recent call first): frame CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string, std::allocator >) + 0x99 (0x7fb2d6398969 in /usr/local/lib/python3.10/distpackages/torch/lib/libc10.so) frame CC(Faster dataloader merge): c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional > >) + 0x1e1 (0x7fb2716a04e1 in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(rename confusing arg): c10d::ProcessGroupNCCL::watchdogHandler() + 0x222 (0x7fb2716a81a2 in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(Compatibility with pytorchtransformers for finetuning ): c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10f (0x7fb2716a9a0f in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(Note that as of now you need to have PySOL cloned to the directory here before building the container.):  + 0xdc253 (0x7fb2d5eb0253 in /usr/lib/x86_64linuxgnu/libstdc++.so.6) frame CC(I do not know how to solve it.):  + 0x94ac3 (0x7fb2e1e42ac3 in /usr/lib/x86_64linuxgnu/libc.so.6) frame CC(perplexity too big for gpt2 wikitext evaluation): clone + 0x44 (0x7fb2e1ed3a04 in /usr/lib/x86_64linuxgnu/libc.so.6)",2024-07-19T09:31:28Z,stale,open,1,2,https://github.com/NVIDIA/Megatron-LM/issues/935,"Hi, I see the same thing. Did you find how to solve the problem?",Marking as stale. No activity in 60 days.
nakroy,[BUG]Get an AtrributeError when trying to convert llama3-8B model from HF format to mcore format,"Describe the bug Get an AtrributeError when trying to convert llama38B model from HF format to mcore format, the error is below: `AttributeError: 'Tokenizer' object has no attribute 'vocab_size'` To Reproduce 1. I git clone https://github.com/metallama/llama3.git and use `pip install e .` to install llama30.0.1 wheel following to llama_mistral.md; 2. I try to convert llama38B model downloading from https://huggingface.co/metallama/MetaLlama38B/tree/main into mcore format , and the script I use is below: ```shell TP=8 PP=2 MODEL_SIZE=llama38B HF_FORMAT_DIR=/workspace/model_weights/llama38b MEGATRON_FORMAT_DIR=${HF_FORMAT_DIR}tp${TP}pp${PP} TOKENIZER_MODEL=${HF_FORMAT_DIR}/original/tokenizer.model python tools/checkpoint/convert.py \ modeltype GPT \ loader llama_mistral \ saver mcore \ checkpointtype hf \ modelsize ${MODEL_SIZE} \ loaddir ${HF_FORMAT_DIR} \ savedir ${MEGATRON_FORMAT_DIR} \ tokenizermodel ${TOKENIZER_MODEL} \ targettensorparallelsize ${TP} \ targetpipelineparallelsize ${PP} \ bf16 ```  Stack trace/logs ```shell Loaded loader_llama_mistral as the loader. Loaded saver_mcore as the saver. Starting saver... Starting loader... using world size: 1, dataparallel size: 1, contextparallel size: 1 tensormodelparallel size: 1, pipelinemodelparallel size: 1  WARNING: Setting args.overlap_p2p_comm to False since noninterleaved schedule does not support overlapping p2p communication using torch.float32 for parameters ...  arguments    accumulate_allreduce_grads_in_fp32 .............. False   adam_beta1 ...................................... 0.9   adam_beta2 ...................................... 0.999   adam_eps ........................................ 1e08   add_bias_linear ................................. False   add_position_embedding .......................... False   add_qkv_bias .................................... False   adlr_autoresume ................................. False   adlr_autoresume_interval ........................ 1000   apply_layernorm_1p .............................. False   apply_query_key_layer_scaling ................... False   apply_residual_connection_post_layernorm ........ False   apply_rope_fusion ............................... True   async_save ...................................... None   async_tensor_model_parallel_allreduce ........... False   attention_dropout ............................... 0.1   attention_softmax_in_fp32 ....................... False   auto_detect_ckpt_format ......................... False   barrier_with_L1_time ............................ True   bert_binary_head ................................ True   bert_embedder_type .............................. megatron   bert_load ....................................... None   bf16 ............................................ False   bias_dropout_fusion ............................. False   bias_gelu_fusion ................................ False   bias_swiglu_fusion .............................. True   biencoder_projection_dim ........................ 0   biencoder_shared_query_context_model ............ False   block_data_path ................................. None   calculate_per_token_loss ........................ False   check_for_nan_in_loss_and_grad .................. True   check_weight_hash_across_dp_replicas_interval ... None   ckpt_assume_constant_structure .................. False   ckpt_fully_parallel_load ........................ False   ckpt_fully_parallel_save ........................ False   ckpt_step ....................................... None   classes_fraction ................................ 1.0   clip_grad ....................................... 1.0   clone_scatter_output_in_embedding ............... True   consumed_train_samples .......................... 0   consumed_valid_samples .......................... 0   context_parallel_size ........................... 1   create_attention_mask_in_dataloader ............. True   cross_entropy_loss_fusion ....................... False   data_cache_path ................................. None   data_parallel_random_init ....................... False   data_parallel_size .............................. 1   data_path ....................................... None   data_per_class_fraction ......................... 1.0   data_sharding ................................... True   dataloader_type ................................. single   ddp_average_in_collective ....................... False   ddp_bucket_size ................................. None   decoder_num_layers .............................. None   decoder_seq_length .............................. None   decoupled_lr .................................... None   decoupled_min_lr ................................ None   delay_grad_reduce ............................... True   delay_param_gather .............................. False   deprecated_use_mcore_models ..................... False   deterministic_mode .............................. False   dino_bottleneck_size ............................ 256   dino_freeze_last_layer .......................... 1   dino_head_hidden_size ........................... 2048   dino_local_crops_number ......................... 10   dino_local_img_size ............................. 96   dino_norm_last_layer ............................ False   dino_teacher_temp ............................... 0.07   dino_warmup_teacher_temp ........................ 0.04   dino_warmup_teacher_temp_epochs ................. 30   disable_straggler_on_startup .................... False   dist_ckpt_format ................................ torch_dist   distribute_saved_activations .................... False   distributed_backend ............................. nccl   distributed_timeout_minutes ..................... 10   embedding_path .................................. None   empty_unused_memory_level ....................... 0   enable_one_logger ............................... False   encoder_num_layers .............................. 32   encoder_seq_length .............................. 4096   end_weight_decay ................................ 0.01   eod_mask_loss ................................... False   eval_interval ................................... 1000   eval_iters ...................................... 100   evidence_data_path .............................. None   exit_duration_in_mins ........................... None   exit_interval ................................... None   exit_on_missing_checkpoint ...................... False   exit_signal_handler ............................. False   expert_model_parallel_size ...................... 1   ffn_hidden_size ................................. 14336   finetune ........................................ False   fp16 ............................................ False   fp16_lm_cross_entropy ........................... False   fp32_residual_connection ........................ False   fp8 ............................................. None   fp8_amax_compute_algo ........................... most_recent   fp8_amax_history_len ............................ 1   fp8_interval .................................... 1   fp8_margin ...................................... 0   fp8_wgrad ....................................... True   global_batch_size ............................... 1024   gradient_accumulation_fusion .................... True   group_query_attention ........................... True   head_lr_mult .................................... 1.0   hidden_dropout .................................. 0.1   hidden_size ..................................... 4096   hybrid_attention_ratio .......................... 0.0   hybrid_mlp_ratio ................................ 0.0   hybrid_override_pattern ......................... None   hysteresis ...................................... 2   ict_head_size ................................... None   ict_load ........................................ None   img_h ........................................... 224   img_w ........................................... 224   indexer_batch_size .............................. 128   indexer_log_interval ............................ 1000   inference_batch_times_seqlen_threshold .......... 512   init_method_std ................................. 0.02   init_method_xavier_uniform ...................... False   initial_loss_scale .............................. 4294967296   iter_per_epoch .................................. 1250   iteration ....................................... 1   kv_channels ..................................... 128   lazy_mpu_init ................................... None   load ............................................ /workspace/model_weights/llama38b   local_rank ...................................... None   log_batch_size_to_tensorboard ................... False   log_interval .................................... 100   log_learning_rate_to_tensorboard ................ True   log_loss_scale_to_tensorboard ................... True   log_memory_to_tensorboard ....................... False   log_num_zeros_in_grad ........................... False   log_params_norm ................................. False   log_progress .................................... False   log_straggler ................................... False   log_throughput .................................. False   log_timers_to_tensorboard ....................... False   log_validation_ppl_to_tensorboard ............... False   log_world_size_to_tensorboard ................... False   logging_level ................................... None   loss_scale ...................................... None   loss_scale_window ............................... 1000   lr .............................................. None   lr_decay_iters .................................. None   lr_decay_samples ................................ None   lr_decay_style .................................. linear   lr_warmup_fraction .............................. None   lr_warmup_init .................................. 0.0   lr_warmup_iters ................................. 0   lr_warmup_samples ............................... 0   lr_wsd_decay_iters .............................. None   lr_wsd_decay_samples ............................ None   lr_wsd_decay_style .............................. exponential   make_vocab_size_divisible_by .................... 128   manual_gc ....................................... False   manual_gc_eval .................................. True   manual_gc_interval .............................. 0   mask_factor ..................................... 1.0   mask_prob ....................................... 0.15   mask_type ....................................... random   masked_softmax_fusion ........................... False   max_position_embeddings ......................... 8192   max_tokens_to_oom ............................... 12000   merge_file ...................................... None   micro_batch_size ................................ 1   min_loss_scale .................................. 1.0   min_lr .......................................... 0.0   mmap_bin_files .................................. True   mock_data ....................................... False   moe_aux_loss_coeff .............................. 0.0   moe_expert_capacity_factor ...................... None   moe_extended_tp ................................. False   moe_grouped_gemm ................................ False   moe_input_jitter_eps ............................ None   moe_layer_recompute ............................. False   moe_pad_expert_input_to_capacity ................ False   moe_per_layer_logging ........................... False   moe_router_load_balancing_type .................. aux_loss   moe_router_topk ................................. 2   moe_token_dispatcher_type ....................... allgather   moe_token_drop_policy ........................... probs   moe_z_loss_coeff ................................ None   nccl_communicator_config_path ................... None   no_load_optim ................................... True   no_load_rng ..................................... True   no_persist_layer_norm ........................... False   no_save_optim ................................... True   no_save_rng ..................................... True   norm_epsilon .................................... 1e05   normalization ................................... RMSNorm   num_attention_heads ............................. 32   num_channels .................................... 3   num_classes ..................................... 1000   num_dataset_builder_threads ..................... 1   num_experts ..................................... None   num_layers ...................................... 32   num_layers_per_virtual_pipeline_stage ........... None   num_query_groups ................................ 8   num_workers ..................................... 2   one_logger_entity ............................... hwinf_dcm   one_logger_project .............................. e2etracking   one_logger_run_name ............................. None   onnx_safe ....................................... None   openai_gelu ..................................... False   optimizer ....................................... adam   output_bert_embeddings .......................... False   overlap_grad_reduce ............................. False   overlap_p2p_comm ................................ False   overlap_param_gather ............................ False   override_opt_param_scheduler .................... False   padded_vocab_size ............................... 128256   params_dtype .................................... torch.float32   patch_dim ....................................... 16   perform_initialization .......................... False   pipeline_model_parallel_size .................... 1   pipeline_model_parallel_split_rank .............. None   position_embedding_type ......................... rope   pretrained_checkpoint ........................... None   profile ......................................... False   profile_ranks ................................... [0]   profile_step_end ................................ 12   profile_step_start .............................. 10   qk_layernorm .................................... False   query_in_block_prob ............................. 0.1   rampup_batch_size ............................... None   rank ............................................ 0   recompute_granularity ........................... None   recompute_method ................................ None   recompute_num_layers ............................ None   reset_attention_mask ............................ False   reset_position_ids .............................. False   retriever_report_topk_accuracies ................ []   retriever_score_scaling ......................... False   retriever_seq_length ............................ 256   retro_add_retriever ............................. False   retro_attention_gate ............................ 1   retro_cyclic_train_iters ........................ None   retro_encoder_attention_dropout ................. 0.1   retro_encoder_hidden_dropout .................... 0.1   retro_encoder_layers ............................ 2   retro_num_neighbors ............................. 2   retro_num_retrieved_chunks ...................... 2   retro_project_dir ............................... None   retro_verify_neighbor_count ..................... True   rotary_base ..................................... 10000   rotary_interleaved .............................. False   rotary_percent .................................. 1.0   rotary_seq_len_interpolation_factor ............. None   sample_rate ..................................... 1.0   save ............................................ None   save_interval ................................... None   scatter_gather_tensors_in_pipeline .............. True   seed ............................................ 1234   seq_length ...................................... 4096   sequence_parallel ............................... False   sgd_momentum .................................... 0.9   short_seq_prob .................................. 0.1   skip_train ...................................... False   spec ............................................ None   split ........................................... None   squared_relu .................................... False   standalone_embedding_stage ...................... False   start_weight_decay .............................. 0.01   straggler_ctrlr_port ............................ 65535   straggler_minmax_count .......................... 1   swiglu .......................................... True   swin_backbone_type .............................. tiny   tensor_model_parallel_size ...................... 1   tensorboard_dir ................................. None   tensorboard_log_interval ........................ 1   tensorboard_queue_size .......................... 1000   test_data_path .................................. None   test_mode ....................................... False   timing_log_level ................................ 0   timing_log_option ............................... minmax   titles_data_path ................................ None   tokenizer_model ................................. /workspace/model_weights/llama38b/original/tokenizer.model   tokenizer_type .................................. Llama3Tokenizer   tp_comm_bulk_dgrad .............................. True   tp_comm_bulk_wgrad .............................. True   tp_comm_overlap ................................. False   tp_comm_overlap_ag .............................. True   tp_comm_overlap_cfg ............................. None   tp_comm_overlap_rs .............................. True   tp_comm_overlap_rs_dgrad ........................ False   tp_comm_split_ag ................................ True   tp_comm_split_rs ................................ True   train_data_path ................................. None   train_iters ..................................... None   train_samples ................................... None   transformer_impl ................................ transformer_engine   transformer_pipeline_model_parallel_size ........ 1   untie_embeddings_and_output_weights ............. True   use_checkpoint_args ............................. False   use_checkpoint_opt_param_scheduler .............. False   use_cpu_initialization .......................... True   use_dist_ckpt ................................... False   use_distributed_optimizer ....................... False   use_flash_attn .................................. False   use_legacy_models ............................... False   use_one_sent_docs ............................... False   use_ring_exchange_p2p ........................... False   use_rotary_position_embeddings .................. True   use_tp_pp_dp_mapping ............................ False   valid_data_path ................................. None   variable_seq_lengths ............................ False   virtual_pipeline_model_parallel_size ............ None   vision_backbone_type ............................ vit   vision_pretraining .............................. False   vision_pretraining_type ......................... classify   vocab_extra_ids ................................. 0   vocab_file ...................................... None   vocab_size ...................................... 128256   wandb_exp_name ..................................    wandb_project ...................................    wandb_save_dir ..................................    weight_decay .................................... 0.01   weight_decay_incr_style ......................... constant   world_size ...................................... 1   yaml_cfg ........................................ None  end of arguments  setting number of microbatches to constant 1024 INFO:llama.tokenizer:Reloaded tiktoken model from /workspace/model_weights/llama38b/original/tokenizer.model INFO:llama.tokenizer:words: 128256  BOS ID: 128000  EOS ID: 128001 Traceback (most recent call last):   File ""/workspace/megatron/tools/checkpoint/convert.py"", line 154, in  Loader exited, exiting saver     main()   File ""/workspace/megatron/tools/checkpoint/convert.py"", line 147, in main     loader.load_checkpoint(queue, args)   File ""/workspace/megatron/tools/checkpoint/loader_llama_mistral.py"", line 663, in load_checkpoint     _load_checkpoint(queue, args)   File ""/workspace/megatron/tools/checkpoint/loader_llama_mistral.py"", line 563, in _load_checkpoint     md.true_vocab_size = tokenizer.vocab_size AttributeError: 'Tokenizer' object has no attribute 'vocab_size' ```  Environment (please complete the following information):   MegatronLM commit ID: 86850db   Nvidia pytorch docker image: nvcr.io/nvidia/pytorch:24.06py3 Additional Found: It seems that `tools/checkpoint/convert.py` would call the function from `tools/checkpoint/loader_llama_mistral.py` to load the HF format checkpoint, and when the case is `llama3`, it would use `Llama3Tokenizer` to get true vocab size as shown in 557~563 line in `tools/checkpoint/loader_llama_mistral.py`. But I check out the Llama3Tokenizer definition which is from https://github.com/metallama/llama3/blob/main/llama/tokenizer.py and did not find the attribute `vocab_size` , but maybe it means the same attribute as following line show (which is defined in 86 line from https://github.com/metallama/llama3/blob/main/llama/tokenizer.py: ```python self.n_words: int = self.model.n_vocab ```  So I change the code `md.true_vocab_size = tokenizer.vocab_size` into `md.true_vocab_size = tokenizer.n_words`, and it successfully converted to mcore format.  But I'm still not sure whether it's a bug or whether the operation I did is wrong that cause the failure of converting llama38B model.",2024-07-19T08:44:46Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/934,"I occur recently some similar problems to yours, and my conclusion is that the tokenizer class used in Llama and the method in Megatron are not consistent.",Same here!,Marking as stale. No activity in 60 days.
NEU-rzh,[QUESTION] add_position_embedding=False in checkpoint_args during Llama3 8B training,"**Your question** The error message during the execution of the llama3 training .sh is as follows: ``` [rank0]: Traceback (most recent call last): [rank0]:   File ""/workspace/wangws/MegatronLM/pretrain_gpt.py"", line 243, in  [rank0]:     pretrain( [rank0]:   File ""/workspace/wangws/MegatronLM/megatron/training/training.py"", line 227, in pretrain [rank0]:     model, optimizer, opt_param_scheduler = setup_model_and_optimizer( [rank0]:   File ""/workspace/wangws/MegatronLM/megatron/training/training.py"", line 522, in setup_model_and_optimizer [rank0]:     args.iteration, args.num_floating_point_operations_so_far = load_checkpoint( [rank0]:   File ""/workspace/wangws/MegatronLM/megatron/training/checkpointing.py"", line 804, in load_checkpoint [rank0]:     check_checkpoint_args(checkpoint_args) [rank0]:   File ""/workspace/wangws/MegatronLM/megatron/training/checkpointing.py"", line 78, in check_checkpoint_args [rank0]:     _compare('add_position_embedding', default=True) [rank0]:   File ""/workspace/wangws/MegatronLM/megatron/training/checkpointing.py"", line 73, in _compare [rank0]:     assert checkpoint_value == args_value, error_message ``` It seems that there is an issue with the megatron format checkpoint. The checkpoint_args: `checkpoint_args:Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, hidden_size=4096, ffn_hidden_size=14336, num_attention_heads=32, kv_channels=128, group_query_attention=True, num_query_groups=8, max_position_embeddings=8192, position_embedding_type='rope', use_rotary_position_embeddings=True, rotary_base=10000, rotary_percent=1.0, rotary_interleaved=False, rotary_seq_len_interpolation_factor=None, add_position_embedding=False, make_vocab_size_divisible_by=16032, normalization='RMSNorm', norm_epsilon=1e05, apply_layernorm_1p=False, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=True, onnx_safe=None, bert_binary_head=True, untie_embeddings_and_output_weights=True, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.01, start_weight_decay=0.01, end_weight_decay=0.01, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1024, rampup_batch_size=None, recompute_granularity=None, check_for_nan_in_loss_and_grad=True, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=None, clone_scatter_output_in_embedding=True, profile=False, profile_step_start=10, profile_step_end=12, profile_ranks=[0], tp_comm_overlap=False, tp_comm_overlap_cfg=None, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_bulk_dgrad=True, tp_comm_bulk_wgrad=True, use_cpu_initialization=True, empty_unused_memory_level=0, deterministic_mode=False, check_weight_hash_across_dp_replicas_interval=None, calculate_per_token_loss=False, train_iters=None, train_samples=None, log_interval=100, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=False, tensorboard_dir=None, masked_softmax_fusion=False, bias_gelu_fusion=False, bias_swiglu_fusion=True, bias_dropout_fusion=False, apply_rope_fusion=True, cross_entropy_loss_fusion=False, use_flash_attn=False, add_bias_linear=False, add_qkv_bias=False, optimizer='adam', dataloader_type='single', async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, gradient_accumulation_fusion=True, deprecated_use_mcore_models=False, use_legacy_models=False, manual_gc=False, manual_gc_interval=0, manual_gc_eval=True, tp_comm_split_ag=True, tp_comm_split_rs=True, seed=1234, data_parallel_random_init=False, init_method_std=0.02, init_method_xavier_uniform=False, lr=None, lr_decay_style='linear', lr_wsd_decay_style='exponential', lr_decay_iters=None, lr_decay_samples=None, lr_wsd_decay_samples=None, lr_wsd_decay_iters=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_init=0.0, min_lr=0.0, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, decoupled_lr=None, decoupled_min_lr=None, save='/workspace/wangws/MegatronLM/models/llama38bhfv0.3tp8pp1/', save_interval=1, no_save_optim=True, no_save_rng=True, load=None, no_load_optim=True, no_load_rng=True, finetune=False, pretrained_checkpoint=None, ckpt_step=None, perform_initialization=False, use_checkpoint_args=False, exit_on_missing_checkpoint=False, use_dist_ckpt=False, auto_detect_ckpt_format=False, dist_ckpt_format='torch_dist', ckpt_fully_parallel_save_deprecated=False, ckpt_fully_parallel_save=True, async_save=None, ckpt_fully_parallel_load=False, ckpt_assume_constant_structure=False, fp16=False, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, overlap_grad_reduce=False, delay_grad_reduce=True, ddp_bucket_size=None, ddp_average_in_collective=False, overlap_param_gather=False, delay_param_gather=False, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=None, lazy_mpu_init=None, standalone_embedding_stage=False, use_distributed_optimizer=False, context_parallel_size=1, nccl_communicator_config_path=None, use_tp_pp_dp_mapping=False, eval_iters=100, eval_interval=1000, test_mode=False, skip_train=False, data_path=[], split=None, train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, mmap_bin_files=True, mock_data=False, vocab_size=128256, vocab_file=None, merge_file=None, vocab_extra_ids=0, seq_length=4096, encoder_seq_length=4096, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, num_workers=2, tokenizer_type='Llama3Tokenizer', tokenizer_model=None, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask_in_dataloader=True, num_dataset_builder_threads=1, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, qk_layernorm=False, expert_model_parallel_size=1, num_experts=None, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_grouped_gemm=False, moe_aux_loss_coeff=0.0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dispatcher_type='allgather', moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_extended_tp=False, log_params_norm=False, log_num_zeros_in_grad=False, log_throughput=False, log_progress=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, wandb_project='', wandb_exp_name='', wandb_save_dir='', enable_one_logger=False, one_logger_project='e2etracking', one_logger_entity='hwinf_dcm', one_logger_run_name=None, logging_level=None, log_straggler=False, disable_straggler_on_startup=False, straggler_ctrlr_port=65535, straggler_minmax_count=1, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8=None, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, transformer_impl='transformer_engine', retro_project_dir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_attention_gate=1, retro_verify_neighbor_count=True, spec=None, hybrid_attention_ratio=0.0, hybrid_mlp_ratio=0.0, hybrid_override_pattern=None, yaml_cfg=None, rank=0, world_size=8, transformer_pipeline_model_parallel_size=1, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float32, consumed_train_samples=0, consumed_valid_samples=0, variable_seq_lengths=False, blendable_index_path=None, model_type=, padded_vocab_size=128256)` Error code after commenting out in MegatronLM/megatron/training/checkpointing.py, line 78 is to compare the input arg and arg in checkpointas: ``` def check_checkpoint_args(checkpoint_args):     """"""Ensure fixed arguments for a model are the same for the input     arguments and the one retrieved from checkpoint.""""""     args = get_args()     def _compare(arg_name, old_arg_name=None, default=None):         if old_arg_name is not None:             ckpt_arg_name = old_arg_name         else:             ckpt_arg_name = arg_name         if default is not None:             checkpoint_value = getattr(checkpoint_args, ckpt_arg_name, default)         else:             checkpoint_value = getattr(checkpoint_args, ckpt_arg_name)         args_value = getattr(args, arg_name)         error_message = '{} value from checkpoint ({}) is not equal to the ' \                         'input argument value ({}).'.format(                             arg_name, checkpoint_value, args_value)         assert checkpoint_value == args_value, error_message     _compare('num_layers')     _compare('hidden_size')     _compare('num_attention_heads')      _compare('add_position_embedding', default=True) ``` After commenting out, the training script could run, but the train_loss at beginning is too high as follow: `[20240717 07:05:35] iteration        2/    2000 ` The checkpoint converting .sh is as follow: ``` python tools/checkpoint/convert.py \     modeltype GPT \     loader llama_mistral \     saver mcore \     checkpointtype hf \     modelsize llama38B \     loaddir ${LLAMA_META_FORMAT_DIR} \     savedir ${MEGATRON_FORMAT_DIR} \     tokenizermodel ${TOKENIZER_MODEL} \     targettensorparallelsize ${TP} \     targetpipelineparallelsize ${PP} \     bf16 ``` I think the reason is due to the lack of position_embedding. There may have been an issue in my checkpoint converting. I use 8 RTX4090. The docker image is nvcr.io/nvidia/pytorch:24.06py3.",2024-07-17T07:11:11Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/933,I met with the same question,I failed to start training because of the Llama3Tokenizer seems to have some problems conflicting with what Megatron core defined...
G-keng,[BUG] Resource Leak When Profile Parameter is Enabled,"https://github.com/NVIDIA/MegatronLM/blob/c7a1f82d761577e6ca0338d3521eac82f2aa0904/megatron/training/training.pyL1065 In this line, NVTX is enabled but not disabled at the end of training.",2024-07-16T10:04:55Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/932,Marking as stale. No activity in 60 days.
binglinchengxiash,how to install llama package,When I use Llama3tokenizerï¼Œerrorï¼š ImportError: Module 'llama' is required but not installed.,2024-07-16T07:35:40Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/931,https://github.com/NVIDIA/MegatronLM/blob/main/docs/llama_mistral.mdinstallllamapackage
exnx,what's the biggest dataset you've tried?,"Hello, I have a dataset of 7T tokens, which when I run in the gptneox codebase, it creates about 5000 .npy files.  I can get this to train for a 7B model on 32 gpus.  But when I try to use 64 gpus, I get an error that says too many files have been opened, reaching the limit for max files opened.  I believe it's opening a file decorator for each gpu and worker of all 5000 .npy files, so the mode gpus, the more files opened.  Has anyone else ran into a similar limit?  The current limit when typing ` ulimit n` is 1048576. Here's the error I got: ``` GPUCA6E:   File ""/home/cirrascale/miniconda3/envs/flashdevocopy/lib/python3.11/socket.py"", line 546, in fromfd GPUCA6E:   File ""/home/cirrascale/miniconda3/envs/flashdevocopy/lib/python3.11/threading.py"", line 975, in run GPUCA6E:     nfd = dup(fd) GPUCA6E:             self._target(*self._args, **self._kwargs)  GPUCA6E:  ^^^^^^^ GPUCA6E: OSError: [Errno 24] Too many open files GPUCA6E:   File ""/home/cirrascale/miniconda3/envs/flashdevocopy/lib/python3.11/sitepackages/torch/utils/data/_utils/pin_memory.py"", line 54, in _pin_memory_loop GPUCA6E:     do_one_step() GPUCA6E:   File ""/home/cirrascale/miniconda3/envs/flashdevocopy/lib/python3.11/sitepackages/torch/utils/data/_utils/pin_memory.py"", line 31, in do_one_step GPUCA6E:     r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL) GPUCA6E:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ GPUCA6E:   File ""/home/cirrascale/miniconda3/envs/flashdevocopy/lib/python3.11/multiprocessing/queues.py"", line 122, in get GPUCA6E:     return _ForkingPickler.loads(res) GPUCA6E:            ^^^^^^^^^^^^^^^^^^^^^^^^^^ GPUCA6E:   File ""/home/cirrascale/miniconda3/envs/flashdevocopy/lib/python3.11/sitepackages/torch/multiprocessing/reductions.py"", line 355, in rebuild_storage_fd GPUCA6E:     fd = df.detach() GPUCA6E:          ^^^^^^^^^^^ GPUCA6E:   File ""/home/cirrascale/miniconda3/envs/flashdevocopy/lib/python3.11/multiprocessing/resource_sharer.py"", line 58, in detach GPUCA6E:     return reduction.recv_handle(conn) GPUCA6E:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^ GPUCA6E:   File ""/home/cirrascale/miniconda3/envs/flashdevocopy/lib/python3.11/multiprocessing/reduction.py"", line 189, in recv_handle GPUCA6E:     return recvfds(s, 1)[0] GPUCA6E:            ^^^^^^^^^^^^^ GPUCA6E:   File ""/home/cirrascale/miniconda3/envs/flashdevocopy/lib/python3.11/multiprocessing/reduction.py"", line 159, in recvfds GPUCA6E:     raise EOFError GPUCA6E: EOFError ```",2024-07-15T08:17:52Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/930,Marking as stale. No activity in 60 days.
exnx,"too many .bin files for dataloader, crashed","Hello, I am training with a very large dataset, 7T tokens, across 45 .bin files.  When I try to use more than 32 gpus, I get an error that says too many files are open.  I am wondering if anyone else has come across this?  I am using gptneox version of Megatron LM.  Here's the error I receive.  Thanks so much! ``` GPUCA6E:     with socket.fromfd(conn.fileno(), socket.AF_UNIX, socket.SOCK_STREAM) as s: GPUCA6E:          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ GPUCA6E:   File ""/home/cirrascale/miniconda3/envs/flashdevocopy/lib/python3.11/socket.py"", line 546, in fromfd GPUCA6E:     fd, addr = self._accept() GPUCA6E:     return recvfds(s, 1)[0] GPUCA6E:                ^^^^^^^^^^^^^^ GPUCA6E:    OSError  : [Errno 24] Too many open files GPUCA6E:        nfd = dup(fd) GPUCA6E:           ^^^^^^^ GPUCA6E: OSError: [Errno 24] Too many open files GPUCA6E:    ^^^^    return recvfds(s, 1)[0] GPUCA6E:           ^ ^^^^^^^^^^^^^^^^ GPUCA6E:   File ""/home/cirrascale/miniconda3/envs/flashdevocopy/lib/python3.11/multiprocessing/reduction.py"", line 159, in recvfds GPUCA6E: ^^^^^ GPUCA6E:   File ""/home/cirrascale/miniconda3/envs/flashdevocopy/lib/python3.11/multiprocessing/reduction.py"", line 164, in recvfds GPUCA6E:     raise EOFError     GPUCA6E: EOFErrorTraceback (most recent call last): GPUCA6E:   File ""/home/cirrascale/miniconda3/envs/flashdevocopy/lib/python3.11/multiprocessing/resource_sharer.py"", line 145, in _serve GPUCA6E:  GPUCA6E: Exception in thread raise RuntimeError('received %d items of ancdata' % GPUCA6E: Thread4 (_pin_memory_loop): GPUCA6E: Traceback (most recent call last): GPUCA6E: RuntimeError: received 0 items of ancdata GPUCA6E:   File ""/home/cirrascale/miniconda3/envs/flashdevocopy/lib/python3.11/threading.py"", line 1038, in _bootstrap_inner GPUCA6E:     send(conn, destination_pid) GPUCA6E:   File ""/home/cirrascale/miniconda3/envs/flashdevocopy/lib/python3.11/multiprocessing/resource_sharer.py"", line 50, in send GPUCA6E:     reduction.send_handle(conn, new_fd, pid) GPUCA6E:   File ""/home/cirrascale/miniconda3/envs/flashdevocopy/lib/python3.11/multiprocessing/reduction.py"", line 183, in send_handle GPUCA6E:     with socket.fromfd(conn.fileno(), socket.AF_UNIX, socket.SOCK_STREAM) as s: GPUCA6E:          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^    ^self.run()^ GPUCA6E: ^^^^^^^^^^^ GPUCA6E:   File ""/home/cirrascale/miniconda3/envs/flashdevocopy/lib/python3.11/socket.py"", line 546, in fromfd GPUCA6E:   File ""/home/cirrascale/miniconda3/envs/flashdevocopy/lib/python3.11/threading.py"", line 975, in run GPUCA6E:     nfd = dup(fd) GPUCA6E:             self._target(*self._args, **self._kwargs)  GPUCA6E:  ^^^^^^^ GPUCA6E: OSError: [Errno 24] Too many open files ```",2024-07-13T07:36:31Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/929
shanmugamr1992,Update README.md to add some mcore documentation and links,,2024-07-11T18:41:09Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/928,Marking as stale. No activity in 60 days.
Liangyz2019,How to train multiple binariey files at the same time or merge them?,"Suppose I have three datasets and convert to binary files **train1.bin, train1.idx, train2.bin, train2.idx, train3.bin, train3.idx.**  During training, I want these three data sets to be merged into one and trained together. How should I do this? It would be better if there are clear examples and guidance, thank you very much.",2024-07-11T12:30:50Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/927,You can use `blend_per_split` in the `BlendedMegatronDatasetConfig`.,Marking as stale. No activity in 60 days.
panjianfei,VocabParallelEmbedding,"**why subtract self.vocab_start_index** ```     def forward(self, input_):         assert not torch.any(             (input_ = self.num_embeddings)         ), ""An input token is out of bounds of the embedding table""         if self.tensor_model_parallel_size > 1:              Build the mask.             input_mask = (input_ = self.vocab_end_index)              Mask the input.             masked_input = input_.clone()  self.vocab_start_index             masked_input[input_mask] = 0         else:             masked_input = input_          Get the embeddings.         output_parallel = self.weight[masked_input]          Mask the output embedding.         if self.tensor_model_parallel_size > 1:             output_parallel[input_mask, :] = 0.0          Reduce across all the model parallel GPUs.         output = reduce_from_tensor_model_parallel_region(output_parallel)         return output ``` https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/tensor_parallel/layers.pyL225",2024-07-11T12:13:13Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/926,Marking as stale. No activity in 60 days.,"As every rank only embedding the  `vocal_size / world_size`, and embedding the input_ids  from 0 to the local_vocal_size,  assuming we have world_size = 2 , and vocal_size = 4,  the first rank  got a ` emb1 = torch.nn.Embedding(2, emd_dim)`,  and the second rank got a  ` emb2 = torch.nn.Embedding(2, emd_dim)`,  if we have a inputs_ids = [1,3],  for the second rank,  vocab_start_index=2, vocab_start_index=3,  if we don't sub the vocab_start_index and using the emb2 to embedding the [1,3],  3 is out of range.",Marking as stale. No activity in 60 days.
jinzhuer,[QUESTION] How Do NCCL_ALGO and Flash Attention Affect Deterministic Training in Megatron?," Issue Description: I read the information about reproducibility, which mentions using `deterministicmode` by setting `NCCL_ALGO`, `NVTE_ALLOW_NONDETERMINISTIC_ALGO=0`, and not using `useflashattn` to achieve deterministic training. I tested Megatron with dualnode (TP=2, PP=2) setups using eight A800 GPUs each, training for 50 iterations. I used this configuration for multiple runs and checked whether the saved models were identical each time (comparing parameters one by one). I found that setting `NVTE_ALLOW_NONDETERMINISTIC_ALGO=0` alone ensured identical model parameters across runs. It seems only this setting matters for reproducibility in my tests. Conversely, not setting this environment variable resulted in different model parameters being saved after each run. **Questions:** 1. Under what conditions do `NCCL_ALGO` and `useflashattn` cause nondeterministic training results? 2. In my environment, `NCCL_ALGO` defaults to None. In this case, how does NCCL choose the algorithm, and how can I know which algorithm is being selected? **Environment Details:**  Hardware: Eight A800 GPUs per node  Setup: TP=2, PP=2  Training iterations: 50  Deterministic setting used: `NVTE_ALLOW_NONDETERMINISTIC_ALGO=0` Thank you for your assistance.",2024-07-11T10:01:09Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/925,"Flash attention added a deterministic flag since v2.4. For FA version >= 2.4, `NVTE_ALLOW_NONDETERMINISTIC_ALGO=0` will automatically set this flag. For FA version < 2.4, you need to disable it. `NCCL_ALGO=NVLS` is only supported on platforms with NVLink switches. You can set `NCCL_DEBUG=INFO` to check which algorithm is selected."
kumulaor,[BUG] Failed to load the megatron_mixtral checkpoint,"**Describe the bug** After converting the weights using the following code, I get an error when I use examples/mixtral/train_mixtral_8x7b_distributed.sh to load the model weights: `export CUDA_DEVICE_MAX_CONNECTIONS=1 python tools/checkpoint/convert.py \ modeltype GPT \ loader mixtral_hf \ saver mcore \ targettensorparallelsize 4 \ targetpipelineparallelsize 1 \ targetexpertparallelsize 4 \ loaddir /workspace/mixtral/model/ \ savedir /workspace/mixtral/model_4tp1pp4ep/ \ tokenizermodel /workspace/mixtral/model/tokenizer.model` **Expected behavior** use the alpaca dataset to fine tune the moe mixtral **Stack trace/logs** [rank3]: Traceback (most recent call last): [rank3]:   File ""/workspace/MegatronLM/pretrain_gpt.py"", line 245, in  [rank3]:     pretrain( [rank3]:   File ""/workspace/MegatronLM/megatron/training/training.py"", line 234, in pretrain [rank3]:     model, optimizer, opt_param_scheduler = setup_model_and_optimizer( [rank3]:   File ""/workspace/MegatronLM/megatron/training/training.py"", line 553, in setup_model_and_optimizer [rank3]:     args.iteration, args.num_floating_point_operations_so_far = load_checkpoint( [rank3]:   File ""/workspace/MegatronLM/megatron/training/checkpointing.py"", line 840, in load_checkpoint [rank3]:     model[0].load_state_dict(state_dict['model'], strict=strict) [rank3]:   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 2182, in load_state_dict [rank3]:     raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format( [rank3]: RuntimeError: Error(s) in loading state_dict for GPTModel: [rank3]:        Missing key(s) in state_dict: ""decoder.layers.0.mlp.experts.weight1"", ""decoder.layers.0.mlp.experts.weight2"", ""decoder.layers.1.mlp.experts.weight1"", ""decoder.layers.1.mlp.experts.weight2"", ""decoder.layers.2.mlp.experts.weight1"", ""decoder.layers.2.mlp.experts.weight2"", ""decoder.layers.3.mlp.experts.weight1"", ""decoder.layers.3.mlp.experts.weight2"", ""decoder.layers.4.mlp.experts.weight1"", ""decoder.layers.4.mlp.experts.weight2"", ""decoder.layers.5.mlp.experts.weight1"", ""decoder.layers.5.mlp.experts.weight2"", ""decoder.layers.6.mlp.experts.weight1"", ""decoder.layers.6.mlp.experts.weight2"", ""decoder.layers.7.mlp.experts.weight1"", ""decoder.layers.7.mlp.experts.weight2"", ""decoder.layers.8.mlp.experts.weight1"", ""decoder.layers.8.mlp.experts.weight2"", ""decoder.layers.9.mlp.experts.weight1"", ""decoder.layers.9.mlp.experts.weight2"", ""decoder.layers.10.mlp.experts.weight1"", ""decoder.layers.10.mlp.experts.weight2"", ""decoder.layers.11.mlp.experts.weight1"", ""decoder.layers.11.mlp.experts.weight2"", ""decoder.layers.12.mlp.experts.weight1"", ""decoder.layers.12.mlp.experts.weight2"", ""decoder.layers.13.mlp.experts.weight1"", ""decoder.layers.13.mlp.experts.weight2"", ""decoder.layers.14.mlp.experts.weight1"", ""decoder.layers.14.mlp.experts.weight2"", ""decoder.layers.15.mlp.experts.weight1"", ""decoder.layers.15.mlp.experts.weight2"", ""decoder.layers.16.mlp.experts.weight1"", ""decoder.layers.16.mlp.experts.weight2"", ""decoder.layers.17.mlp.experts.weight1"", ""decoder.layers.17.mlp.experts.weight2"", ""decoder.layers.18.mlp.experts.weight1"", ""decoder.layers.18.mlp.experts.weight2"", ""decoder.layers.19.mlp.experts.weight1"", ""decoder.layers.19.mlp.experts.weight2"", ""decoder.layers.20.mlp.experts.weight1"", ""decoder.layers.20.mlp.experts.weight2"", ""decoder.layers.21.mlp.experts.weight1"", ""decoder.layers.21.mlp.experts.weight2"", ""decoder.layers.22.mlp.experts.weight1"", ""decoder.layers.22.mlp.experts.weight2"", ""decoder.layers.23.mlp.experts.weight1"", ""decoder.layers.23.mlp.experts.weight2"", ""decoder.layers.24.mlp.experts.weight1"", ""decoder.layers.24.mlp.experts.weight2"", ""decoder.layers.25.mlp.experts.weight1"", ""decoder.layers.25.mlp.experts.weight2"", ""decoder.layers.26.mlp.experts.weight1"", ""decoder.layers.26.mlp.experts.weight2"", ""decoder.layers.27.mlp.experts.weight1"", ""decoder.layers.27.mlp.experts.weight2"", ""decoder.layers.28.mlp.experts.weight1"", ""decoder.layers.28.mlp.experts.weight2"", ""decoder.layers.29.mlp.experts.weight1"", ""decoder.layers.29.mlp.experts.weight2"", ""decoder.layers.30.mlp.experts.weight1"", ""decoder.layers.30.mlp.experts.weight2"", ""decoder.layers.31.mlp.experts.weight1"", ""decoder.layers.31.mlp.experts.weight2"".  [rank3]:        Unexpected key(s) in state_dict: ""decoder.layers.0.mlp.experts.local_experts.0.linear_fc1.weight"", ""decoder.layers.0.mlp.experts.local_experts.0.linear_fc2.weight"", ""decoder.layers.0.mlp.experts.local_experts.1.linear_fc1.weight"", ""decoder.layers.0.mlp.experts.local_experts.1.linear_fc2.weight"", ""decoder.layers.1.mlp.experts.local_experts.0.linear_fc1.weight"", ""decoder.layers.1.mlp.experts.local_experts.0.linear_fc2.weight"", ""decoder.layers.1.mlp.experts.local_experts.1.linear_fc1.weight"", ""decoder.layers.1.mlp.experts.local_experts.1.linear_fc2.weight"", ""decoder.layers.2.mlp.experts.local_experts.0.linear_fc1.weight"", ""decoder.layers.2.mlp.experts.local_experts.0.linear_fc2.weight"", ""decoder.layers.2.mlp.experts.local_experts.1.linear_fc1.weight"", ""decoder.layers.2.mlp.experts.local_experts.1.linear_fc2.weight"", ""decoder.layers.3.mlp.experts.local_experts.0.linear_fc1.weight"", ""decoder.layers.3.mlp.experts.local_experts.0.linear_fc2.weight"", ""decoder.layers.3.mlp.experts.local_experts.1.linear_fc1.weight"", ""decoder.layers.3.mlp.experts.local_experts.1.linear_fc2.weight"", ""decoder.layers.4.mlp.experts.local_experts.0.linear_fc1.weight"", ""decoder.layers.4.mlp.experts.local_experts.0.linear_fc2.weight"", ""decoder.layers.4.mlp.experts.local_experts.1.linear_fc1.weight"", ""decoder.layers.4.mlp.experts.local_experts.1.linear_fc2.weight"", ""decoder.layers.5.mlp.experts.local_experts.0.linear_fc1.weight"", ""decoder.layers.5.mlp.experts.local_experts.0.linear_fc2.weight"", ""decoder.layers.5.mlp.experts.local_experts.1.linear_fc1.weight"", ""decoder.layers.5.mlp.experts.local_experts.1.linear_fc2.weight"", ""decoder.layers.6.mlp.experts.local_experts.0.linear_fc1.weight"", ""decoder.layers.6.mlp.experts.local_experts.0.linear_fc2.weight"", ""decoder.layers.6.mlp.experts.local_experts.1.linear_fc1.weight"", ""decoder.layers.6.mlp.experts.local_experts.1.linear_fc2.weight"", ""decoder.layers.7.mlp.experts.local_experts.0.linear_fc1.weight"", ""decoder.layers.7.mlp.experts.local_experts.0.linear_fc2.weight"", ""decoder.layers.7.mlp.experts.local_experts.1.linear_fc1.weight"", ""decoder.layers.7.mlp.experts.local_experts.1.linear_fc2.weight"", ""decoder.layers.8.mlp.experts.local_experts.0.linear_fc1.weight"", ""decoder.layers.8.mlp.experts.local_experts.0.linear_fc2.weight"", ""decoder.layers.8.mlp.experts.local_experts.1.linear_fc1.weight"", ""decoder.layers.8.mlp.experts.local_experts.1.linear_fc2.weight"", ""decoder.layers.9.mlp.experts.local_experts.0.linear_fc1.weight"", ""decoder.layers.9.mlp.experts.local_experts.0.linear_fc2.weight"", ""decoder.layers.9.mlp.experts.local_experts.1.linear_fc1.weight"", ""decoder.layers.9.mlp.experts.local_experts.1.linear_fc2.weight"", ""decoder.layers.10.mlp.experts.local_experts.0.linear_fc1.weight"", ""decoder.layers.10.mlp.experts.local_experts.0.linear_fc2.weight"", ""decoder.layers.10.mlp.experts.local_experts.1.linear_fc1.weight"", ""decoder.layers.10.mlp.experts.local_experts.1.linear_fc2.weight"", ""decoder.layers.11.mlp.experts.local_experts.0.linear_fc1.weight"", ""decoder.layers.11.mlp.experts.local_experts.0.linear_fc2.weight"", ""decoder.layers.11.mlp.experts.local_experts.1.linear_fc1.weight"", ""decoder.layers.11.mlp.experts.local_experts.1.linear_fc2.weight"", ""decoder.layers.12.mlp.experts.local_experts.0.linear_fc1.weight"", ""decoder.layers.12.mlp.experts.local_experts.0.linear_fc2.weight"", ""decoder.layers.12.mlp.experts.local_experts.1.linear_fc1.weight"", ""decoder.layers.12.mlp.experts.local_experts.1.linear_fc2.weight"", ""decoder.layers.13.mlp.experts.local_experts.0.linear_fc1.weight"", ""decoder.layers.13.mlp.experts.local_experts.0.linear_fc2.weight"", ""decoder.layers.13.mlp.experts.local_experts.1.linear_fc1.weight"", ""decoder.layers.13.mlp.experts.local_experts.1.linear_fc2.weight"", ""decoder.layers.14.mlp.experts.local_experts.0.linear_fc1.weight"", ""decoder.layers.14.mlp.experts.local_experts.0.linear_fc2.weight"", ""decoder.layers.14.mlp.experts.local_experts.1.linear_fc1.weight"", ""decoder.layers.14.mlp.experts.local_experts.1.linear_fc2.weight"", ""decoder.layers.15.mlp.experts.local_experts.0.linear_fc1.weight"", ""decoder.layers.15.mlp.experts.local_experts.0.linear_fc2.weight"", ""decoder.layers.15.mlp.experts.local_experts.1.linear_fc1.weight"", ""decoder.layers.15.mlp.experts.local_experts.1.linear_fc2.weight"", ""decoder.layers.16.mlp.experts.local_experts.0.linear_fc1.weight"", ""decoder.layers.16.mlp.experts.local_experts.0.linear_fc2.weight"", ""decoder.layers.16.mlp.experts.local_experts.1.linear_fc1.weight"", ""decoder.layers.16.mlp.experts.local_experts.1.linear_fc2.weight"", ""decoder.layers.17.mlp.experts.local_experts.0.linear_fc1.weight"", ""decoder.layers.17.mlp.experts.local_experts.0.linear_fc2.weight"", ""decoder.layers.17.mlp.experts.local_experts.1.linear_fc1.weight"", ""decoder.layers.17.mlp.experts.local_experts.1.linear_fc2.weight"", ""decoder.layers.18.mlp.experts.local_experts.0.linear_fc1.weight"", ""decoder.layers.18.mlp.experts.local_experts.0.linear_fc2.weight"", ""decoder.layers.18.mlp.experts.local_experts.1.linear_fc1.weight"", ""decoder.layers.18.mlp.experts.local_experts.1.linear_fc2.weight"", ""decoder.layers.19.mlp.experts.local_experts.0.linear_fc1.weight"", ""decoder.layers.19.mlp.experts.local_experts.0.linear_fc2.weight"", ""decoder.layers.19.mlp.experts.local_experts.1.linear_fc1.weight"", ""decoder.layers.19.mlp.experts.local_experts.1.linear_fc2.weight"", ""decoder.layers.20.mlp.experts.local_experts.0.linear_fc1.weight"", ""decoder.layers.20.mlp.experts.local_experts.0.linear_fc2.weight"", ""decoder.layers.20.mlp.experts.local_experts.1.linear_fc1.weight"", ""decoder.layers.20.mlp.experts.local_experts.1.linear_fc2.weight"", ""decoder.layers.21.mlp.experts.local_experts.0.linear_fc1.weight"", ""decoder.layers.21.mlp.experts.local_experts.0.linear_fc2.weight"", ""decoder.layers.21.mlp.experts.local_experts.1.linear_fc1.weight"", ""decoder.layers.21.mlp.experts.local_experts.1.linear_fc2.weight"", ""decoder.layers.22.mlp.experts.local_experts.0.linear_fc1.weight"", ""decoder.layers.22.mlp.experts.local_experts.0.linear_fc2.weight"", ""decoder.layers.22.mlp.experts.local_experts.1.linear_fc1.weight"", ""decoder.layers.22.mlp.experts.local_experts.1.linear_fc2.weight"", ""decoder.layers.23.mlp.experts.local_experts.0.linear_fc1.weight"", ""decoder.layers.23.mlp.experts.local_experts.0.linear_fc2.weight"", ""decoder.layers.23.mlp.experts.local_experts.1.linear_fc1.weight"", ""decoder.layers.23.mlp.experts.local_experts.1.linear_fc2.weight"", ""decoder.layers.24.mlp.experts.local_experts.0.linear_fc1.weight"", ""decoder.layers.24.mlp.experts.local_experts.0.linear_fc2.weight"", ""decoder.layers.24.mlp.experts.local_experts.1.linear_fc1.weight"", ""decoder.layers.24.mlp.experts.local_experts.1.linear_fc2.weight"", ""decoder.layers.25.mlp.experts.local_experts.0.linear_fc1.weight"", ""decoder.layers.25.mlp.experts.local_experts.0.linear_fc2.weight"", ""decoder.layers.25.mlp.experts.local_experts.1.linear_fc1.weight"", ""decoder.layers.25.mlp.experts.local_experts.1.linear_fc2.weight"", ""decoder.layers.26.mlp.experts.local_experts.0.linear_fc1.weight"", ""decoder.layers.26.mlp.experts.local_experts.0.linear_fc2.weight"", ""decoder.layers.26.mlp.experts.local_experts.1.linear_fc1.weight"", ""decoder.layers.26.mlp.experts.local_experts.1.linear_fc2.weight"", ""decoder.layers.27.mlp.experts.local_experts.0.linear_fc1.weight"", ""decoder.layers.27.mlp.experts.local_experts.0.linear_fc2.weight"", ""decoder.layers.27.mlp.experts.local_experts.1.linear_fc1.weight"", ""decoder.layers.27.mlp.experts.local_experts.1.linear_fc2.weight"", ""decoder.layers.28.mlp.experts.local_experts.0.linear_fc1.weight"", ""decoder.layers.28.mlp.experts.local_experts.0.linear_fc2.weight"", ""decoder.layers.28.mlp.experts.local_experts.1.linear_fc1.weight"", ""decoder.layers.28.mlp.experts.local_experts.1.linear_fc2.weight"", ""decoder.layers.29.mlp.experts.local_experts.0.linear_fc1.weight"", ""decoder.layers.29.mlp.experts.local_experts.0.linear_fc2.weight"", ""decoder.layers.29.mlp.experts.local_experts.1.linear_fc1.weight"", ""decoder.layers.29.mlp.experts.local_experts.1.linear_fc2.weight"", ""decoder.layers.30.mlp.experts.local_experts.0.linear_fc1.weight"", ""decoder.layers.30.mlp.experts.local_experts.0.linear_fc2.weight"", ""decoder.layers.30.mlp.experts.local_experts.1.linear_fc1.weight"", ""decoder.layers.30.mlp.experts.local_experts.1.linear_fc2.weight"", ""decoder.layers.31.mlp.experts.local_experts.0.linear_fc1.weight"", ""decoder.layers.31.mlp.experts.local_experts.0.linear_fc2.weight"", ""decoder.layers.31.mlp.experts.local_experts.1.linear_fc1.weight"", ""decoder.layers.31.mlp.experts.local_experts.1.linear_fc2.weight"".  **Environment (please complete the following information):**   MegatronLM commit ID nvcr.io/nvidia/pytorch:24.04py3   PyTorch version 2.3.0a0+6ddf5cf85e.nv24.4   CUDA version 12.1   NCCL version 2.21.5",2024-07-11T09:30:59Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/924,May I ask how it was resolved,"> May I ask how it was resolved This issue is due to the setting of the parameter moe grouped gemm in the moe args of the mixtral model in train_mixtral_8x7b_distributed.shã€‚When using  moe grouped gemm, the type of experts in MoELayer is GroupedMLP, and the type of experts in the checkpoint is SequentialMLPã€‚I fixed it by deleting moe grouped gemmã€‚"
Ethan-yt,Refactor: Use safe_get_rank in Logging to Handle Uninitialized Distributed Mode," Refactor: Use `safe_get_rank` in Logging to Handle Uninitialized Distributed Mode  Description This PR introduces changes to improve logging within the `checkpointing.py` module by handling cases where the distributed mode is not initialized. The key change involves replacing direct calls to `torch.distributed.get_rank()` with a new utility function, `safe_get_rank()`, which checks if the distributed mode is initialized before attempting to get the rank.  Changes 1. **Modified `checkpointing.py`**:     Replaced `torch.distributed.get_rank()` with `safe_get_rank()` in logging statements to avoid errors when the distributed mode is not initialized.    ```python    logger.debug(f""rank: {safe_get_rank()}, takes {end_ckpt  start_ckpt} to prepare state dict for ckpt "")    logger.debug(f""rank: {safe_get_rank()}, takes {end_misc  start_misc} to finalize ckpt save "")    ``` 2. **Added `safe_get_rank` in `utils.py`**:     Created a new utility function `safe_get_rank()` which returns the rank if distributed mode is initialized, otherwise returns `""N/A""`.    ```python    def safe_get_rank():        if torch.distributed.is_initialized():            return torch.distributed.get_rank()        else:            return ""N/A""    ```  Motivation These changes ensure that logging does not raise errors in scenarios where the distributed mode has not been initialized, enhancing robustness and providing clearer logging information during model training and checkpointing.  Please review the changes and provide any feedback. Thank you! ",2024-07-10T11:52:15Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/923,This is now fixed in main
MaCasK9,[QUESTION] Why Megatron choose sync style training?,"**Your question** Ask a clear and concise question about MegatronLM. Hello, I've noticed that Megatron is running in a sync style (i.e. SSGD/BSP). To my knowledge, this method works well in homogeneous setting, but suffers from heterogeneity of FLOPS/s, bandwith etc.. So I'm wondering why Megatron choose to only support sync style? Is it rare to see a heterogeneous env so there is no much need for async support? Really appreciate for any answers.",2024-07-10T11:45:37Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/922,"Hi  MaCask, megatron mainly focuses on Homogenous training setting as you rightly pointed since heterogenous is quite rare. Its not on our roadmap either in the short term. "
singleheart,Fix weight name mismatch for checkpoint conversion in legacy Megatron using TransformerEngine,"When using `transformer_engine` for `transformer_impl` in legacy Megatron without applying MCore, an error occurs during checkpoint conversion due to a mismatch in weight variable names compared to `local`. This update corrects the variable names to be appropriate when `transformer_engine` is used.",2024-07-10T07:59:51Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/921,Marking as stale. No activity in 60 days.
haolin-nju,[BUG] Getting distributed rank in save_checkpoint when torch.distributed is not initialized.,"**Describe the bug** Regarding the code in https://github.com/NVIDIA/MegatronLM/blob/c7a1f82d761577e6ca0338d3521eac82f2aa0904/megatron/training/checkpointing.pyL426 In the case of converting a checkpoint, torch.distributed() might not have been initialized, and calling torch.distributed.get_rank() within the logger.debug will result in an exception: `Default process group has not been initialized, please make sure to call init_process_group.` **To Reproduce** Try converting a checkpoint as described in https://github.com/NVIDIA/MegatronLM/blob/c7a1f82d761577e6ca0338d3521eac82f2aa0904/examples/mixtral/README.md?plain=1L14 **Stack trace/logs** ```shell ...  > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000) saving checkpoint at iteration       1 to /mnt/workspace/someone/checkpoint/mcore/Mixtral8x7Bv0.1tp1pp1ep1 in torch format   successfully saved checkpoint from iteration       1 to /mnt/workspace/someone/checkpoint/mcore/Mixtral8x7Bv0.1tp1pp1ep1 Process Process1: Traceback (most recent call last):   File ""/usr/lib/python3.10/multiprocessing/process.py"", line 314, in _bootstrap     self.run()   File ""/usr/lib/python3.10/multiprocessing/process.py"", line 108, in run     self._target(*self._args, **self._kwargs)   File ""/mnt/workspace/someone/MegatronLM/tools/checkpoint/saver_mcore.py"", line 789, in save_checkpoint     save_checkpoint(md.iteration, [get_local_model(pp_rank, ep_rank, tp_rank)], None, None, num_floating_point_operations_so_far=0,   File ""/mnt/workspace/someone/MegatronLM/megatron/training/checkpointing.py"", line 426, in save_checkpoint     logger.debug(f""rank: {torch.distributed.get_rank()}, takes {end_misc  start_misc} to finalize ckpt save "")   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/distributed_c10d.py"", line 1507, in get_rank     default_pg = _get_default_group()   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/distributed_c10d.py"", line 958, in _get_default_group     raise ValueError( ValueError: Default process group has not been initialized, please make sure to call init_process_group. ``` **Proposed fix** Adding `if torch.distributed.is_initialized()`.",2024-07-10T03:08:34Z,,closed,0,5,https://github.com/NVIDIA/Megatron-LM/issues/920,This seems to be the same as CC([BUG]  Missing init_process_group call when converting model to HF format.) ," it is, I'm working on a fix",We have implemented a fix internally and it will be pushed out to github later this week.,"I have created a PR to fix this issue. For those who wants a quick fix, you can clone my fork. https://github.com/NVIDIA/MegatronLM/pull/923",Now fixed in main
panda66666666,"[QUESTION] When i use --use-dist-ckpt to load ,there is something error and I can't tell if it's my configuration problem or the code problem.","**Your question** As you can see when i use the usedistckpt and args.dist_ckpt_format is 'torch', my folder tree is just like that: !ckptæé—® but i checkout the logic of load files in https://github.com/NVIDIA/MegatronLM/blob/c7a1f82d761577e6ca0338d3521eac82f2aa0904/megatron/training/checkpointing.pyL570 ,it need the file name like""xxxx.pt"" which is the result after disable ""usedistckpt"" ,maybe like this(i output the model ckpt without use usedistckpt): !ckptæé—®2 so does it support loading the checkpoints I use?",2024-07-10T01:25:15Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/919
skothenhill-nv,[ENHANCEMENT] Enable non-gelu activations for BERT LM Head,"**Is your feature request related to a problem? Please describe.** Bert LMHead hardcode's gelu as the activation, this has caused problems in downstream projects (e.g. NeMo). **Describe the solution you'd like** Rather than hardcoding nn.gelu, we accept the field set to `TransformerConfig.activation_func` as done for other models. **Describe alternatives you've considered** did not consider alternatives **Proposed implementation** PR. **Additional context** Spoke with  and used his suggested solution. ",2024-07-09T23:34:58Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/918,Marking as stale. No activity in 60 days.
skothenhill-nv,remove hardcoded gelu from BERT models.,Based on discussion with  ,2024-07-09T22:48:58Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/917,Marking as stale. No activity in 60 days.
pedrohenriqueamartins,Annealing,,2024-07-09T12:41:59Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/916
haolin-nju,[BUG] Unnecessary initialization for router in megatron-core ,"**Describe the bug** In megatron/core/transformer/moe/router.py, the class `Router`, as well as `TopKRouter`, will always perform weight initialization. However, there exists cases that initialization is unnecessary, such as converting checkpoints. **Proposed fix** Please kindly refer to PR914.",2024-07-09T09:46:06Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/915,"Thanks for reporting and fixing it. Unfortunately, we can't directly merge this PR on Github, but we'll include the fix in the next version.",Marking as stale. No activity in 60 days.
haolin-nju,[BUG] Suppress initialization for moe router if not necessary,"In megatron/core/transformer/moe/router.py, the class `Router`, as well as `TopKRouter`, will always perform weight initialization. However, there exists cases that initialization is unnecessary, such as converting checkpoints. I found that most of other modules in megatroncore have included the `if config.perform_initialization` clause.(e.g. `GroupedMLP` in megatron/core/transformer/moe/experts.py). Thus, I add the clause here for suppressing unnecessary initializations. Please find the details in the attached files:)",2024-07-09T09:08:32Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/914,Marking as stale. No activity in 60 days.
blankde,[QUESTION] Why set requires_grad=True for token unpermutation in MoE ,"I am curious that why we set requires_grad=True in https://github.com/NVIDIA/MegatronLM/blob/0bc3547702464501feefeb5523b7a17e591b21fa/megatron/core/transformer/moe/moe_utils.pyL279 ```        empty_tokens = torch.zeros(         restore_shape,         dtype=combined_output.dtype,         device=combined_output.device,         requires_grad=True,     )      Scatter the combined tokens back to their original positions     unpermuted_tokens = torch.scatter_add(empty_tokens, 0, indices, combined_output) ``` in my case, unpermuted_tokens always have grad whether we set empty_tokens as requires_grad=True or False, but set requires_grad=True for empty_tokens  will consume much more memory (almost 20GB). !image",2024-07-09T07:10:45Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/913,lasted code fix this. Thanks.
exnx,[QUESTION] Can fp8 and pipeline parallelism be used together?,"**Your question** Ask a clear and concise question about MegatronLM. Hello, can fp8 and pipeline parallelism be used together?  When I try to use both the training gets hung up, and then timed out by NCCL.  Training code starts up, but no logging update occurs. I can use fp8 and model parallelism ok, though. Curious if anyone else noticed this? Thanks!",2024-07-09T05:34:40Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/912, sorry I don't understand. FP8 has independent groups to keep reduce is accurate. Could it be a problem of your fp8 group and pipeline group setting ?
benoriol,[BUG]  Missing init_process_group call when converting model to HF format.,"**Describe the bug** Getting the following error when trying to convert Mistral 7B model from HF format to mcore using instructions in docs/llama_mistral.md `ValueError: Default process group has not been initialized, please make sure to call init_process_group.` **To Reproduce** I am following step by step the instructions in the page linked below 1. First download the Mistral weight from Huggingface 2. Install `mistralcommons` 3. **Here is where I get the error**. Run the command `python tools/checkpoint/convert.py` with the same arguments described in the tutorial: `python tools/checkpoint/convert.py modeltype GPT  loader llama_mistral saver mcore  targettensorparallelsize 4  checkpointtype hf loaddir /workspace/checkpoints/Mistral7Bv0 .3/ savedir /workspace/checkpoints/Mistral7Bv0.3Megatron tokenizermodel /workspace/checkpoints/Mistral7Bv0.3/tokenizer.model modelsize mistral7B`.  **Expected behavior** The conversion script should continue running and obtain the mcore format of the model.  **Stack trace/logs** ``` root:/workspace/megatron python tools/checkpoint/convert.py modeltype GPT  loader llama_mistral saver mcore  targettensorparallelsize 4  checkpointtype hf loaddir /workspace/checkpoints/Mistral7Bv0 .3/ savedir /workspace/checkpoints/Mistral7Bv0.3Megatron tokenizermodel /workspace/checkpoints/Mistral7Bv0.3/tokenizer.model modelsize mistral7B Loaded loader_llama_mistral as the loader. Loaded saver_mcore as the saver. Starting saver... Starting loader... Loading checkpoint shards: 100% 3/3 [00:26 memory usage: 'saver', rank 0 / 4, mem 0.6/1121.8 gb. building GPT model ... sending transformer layer 12 > memory usage: 'saver', rank 1 / 4, mem 0.6/1121.8 gb. building GPT model ... sending transformer layer 13 sending transformer layer 14 sending transformer layer 15 > memory usage: 'saver', rank 2 / 4, mem 0.6/1121.8 gb. building GPT model ... sending transformer layer 16 > memory usage: 'saver', rank 3 / 4, mem 0.6/1121.8 gb. sending transformer layer 17 sending transformer layer 18 sending transformer layer 19 received transformer layer 0 sending transformer layer 20 received transformer layer 1 sending transformer layer 21 sending transformer layer 22 sending transformer layer 23 sending transformer layer 24 received transformer layer 2 sending transformer layer 25 sending transformer layer 26 sending transformer layer 27 sending transformer layer 28 sending transformer layer 29 sending transformer layer 30 received transformer layer 3 sending transformer layer 31 sending final norm sending output layer received transformer layer 4 received transformer layer 5 received transformer layer 6 Waiting for saver to complete... received transformer layer 7 received transformer layer 8 received transformer layer 9 received transformer layer 10 received transformer layer 11 received transformer layer 12 received transformer layer 13 received transformer layer 14 received transformer layer 15 received transformer layer 16 received transformer layer 17 received transformer layer 18 received transformer layer 19 received transformer layer 20 received transformer layer 21 received transformer layer 22 received transformer layer 23 received transformer layer 24 received transformer layer 25 received transformer layer 26 received transformer layer 27 received transformer layer 28 received transformer layer 29 received transformer layer 30 received transformer layer 31 received final norm received output layer saving checkpoint at iteration       1 to /workspace/checkpoints/Mistral7Bv0.3Megatron in torch format   successfully saved checkpoint from iteration       1 to /workspace/checkpoints/Mistral7Bv0.3Megatron Process Process1: Traceback (most recent call last):   File ""/usr/lib/python3.10/multiprocessing/process.py"", line 314, in _bootstrap     self.run()   File ""/usr/lib/python3.10/multiprocessing/process.py"", line 108, in run     self._target(*self._args, **self._kwargs)   File ""/workspace/megatron/tools/checkpoint/saver_mcore.py"", line 669, in save_checkpoint     save_checkpoint(md.iteration, [models[tp_rank]], None, None,   File ""/workspace/megatron/megatron/training/checkpointing.py"", line 410, in save_checkpoint     logger.debug(f""rank: {torch.distributed.get_rank()}, takes {end_misc  start_misc} to finalize ckpt save "")   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/distributed_c10d.py"", line 1633, in get_rank     default_pg = _get_default_group()   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/distributed_c10d.py"", line 985, in _get_default_group     raise ValueError( ValueError: Default process group has not been initialized, please make sure to call init_process_group. ``` **Environment (please complete the following information):**   MegatronLM commit ID: df61e60bf5670b1196fcae2264311401d3bb82db   PyTorch version: '2.3.0a0+ebedce2'   CUDA version: '12.3'   NCCL version: (2, 20, 3) **Proposed fix** Initialize torch.distributed at some point in the script. **Additional context** I am running on a single node with 8 A100 GPU, I don't plan on using it on more than one node. Using the current `main` branch  Started docker with  `docker run gpus all it rm v /my/path/to/MegatronLM:/workspace/megatron v /my/path/to/data:/workspace/dataset v /my/path/to/checkpoints:/workspace/checkpoints w /workspace/megatron/examples/multimodal shmsize 6G megatron:multimodal` Dockerfile in `MegatronLM/examples/multimodal` since I am planning to use this to train multimodal LlaVa as described in examples/multimodal My WORLD_SIZE, RANK, MASTER_ADDR, MASTER_PORT environment variables are unset. I am not sure if I have any other env variables that can affect. From outside the docker: ``` $ docker network ls NETWORK ID     NAME         DRIVER    SCOPE f1584b1ae327   bridge       bridge    local 1ce4f004699d   host         host      local 06c0830e8909   my_network   bridge    local ee252c0411d4   none         null      local ```",2024-07-08T20:45:51Z,,closed,1,9,https://github.com/NVIDIA/Megatron-LM/issues/911,Pseudosolved by checking out the following commit: 86850db930c85ed925e661574acc7564debf7988,"I have already updated to 0bc3547, but encountered the same problem when converting the llama3 70B model.",I meant I had to **downgrade** to that commit via `git checkout 86850db930c85ed925e661574acc7564debf7988`,> Pseudosolved by checking out the following commit: 86850db temporarily sovled the problem...,barker  Can you take a look into this please ,This is a regression. An update to checkpointing code last week made the incorrect assumption that you'd always be in a distributed setting when saving checkpoints. We'll make a fix internally and push it out asap.,A short term WAR is to comment out the `logger.debug` lines in `megatron/training/checkpointing.py`,We have implemented a fix internally and it will be pushed out to github later this week.,Now fixed in main
woson,[QUESTION] ," my shell scripts:  `PYTORCH_IMAGE=nvcr.io/nvidia/pytorch:23.12py3 CHECKPOINT_PATH=""/workspace/checkpoints""  VOCAB_FILE=""/workspace/dataset/my_vocab.json"" /gpt2vocab.json MERGE_FILE=""/workspace/dataset/merges.txt"" /gpt2merges.txt DATA_PATH=""/workspace/dataset"" _text_document docker run \   gpus=all \   ipc=host \   workdir /workspace/megatron \   v /root/data0/MegatronLM:/workspace/megatron \   v /root/data0/MegatronLM/workspace/dataset:/workspace/dataset \   v /root/data0/MegatronLM/workspace/checkpoints:/workspace/checkpoints \   $PYTORCH_IMAGE \   bash examples/gpt3/train_gpt2_857M_distributed.sh $CHECKPOINT_PATH $VOCAB_FILE $MERGE_FILE $DATA_PATH ` The download URL for my_vocab.json is: https://s3.amazonaws.com/models.huggingface.co/bert/gpt2vocab.json After running this script, it prompts: > building GPT2BPETokenizer tokenizer ... Traceback (most recent call last):   File ""/workspace/megatron/pretrain_gpt.py"", line 243, in      pretrain(   File ""/workspace/megatron/megatron/training/training.py"", line 190, in pretrain     initialize_megatron(extra_args_provider=extra_args_provider,   File ""/workspace/megatron/megatron/training/initialize.py"", line 62, in initialize_megatron     set_global_variables(args)   File ""/workspace/megatron/megatron/training/global_vars.py"", line 100, in set_global_variables     _ = _build_tokenizer(args)   File ""/workspace/megatron/megatron/training/global_vars.py"", line 130, in _build_tokenizer     _GLOBAL_TOKENIZER = build_tokenizer(args)   File ""/workspace/megatron/megatron/training/tokenizer/tokenizer.py"", line 36, in build_tokenizer     tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)   File ""/workspace/megatron/megatron/training/tokenizer/tokenizer.py"", line 262, in __init__     self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',   File ""/workspace/megatron/megatron/training/tokenizer/gpt2_tokenization.py"", line 159, in __init__     self.encoder = json.load(open(vocab_file))   File ""/usr/lib/python3.10/json/__init__.py"", line 293, in load     return loads(fp.read(),   File ""/usr/lib/python3.10/json/__init__.py"", line 346, in loads     return _default_decoder.decode(s)   File ""/usr/lib/python3.10/json/decoder.py"", line 337, in decode     obj, end = self.raw_decode(s, idx=_w(s, 0).end())   File ""/usr/lib/python3.10/json/decoder.py"", line 355, in raw_decode     raise JSONDecodeError(""Expecting value"", s, err.value) from None json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0) Can you help me check where the problem lies?",2024-07-08T14:27:57Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/910
Emperorizzis,"[QUESTION]More experts (more than 8, each EP group > 1), continuing pre-training of MoE with grouped_gemm results in abnormal initial loss (too high), around 6.x.","Hi! Thank you for your opensource work! When continuing the pretraining of Qwen1.5MoEA2.7B with the same configuration, using `SequentialMLP`, the **loss remains around 1.x**, but using `GroupedMLP`, the **loss is around 6.x**. I noticed the structural differences between `GroupedMLP` and `SequentialMLP`. After converting the model weights from Huggingface to a format accepted by Megatron (`SequentialMLP`, and verified with a loss around 1.x), I used the following method to convert the Experts' weights: ```python ... def convert_seq_mlp_to_grouped_gemm(ep_state_dict: dict, num_layers: int, num_local_experts: int):     for l in tqdm(range(num_layers), desc='Transform experts format'):         fc1_params = [f""decoder.layers.{l}.mlp.experts.local_experts.{e}.linear_fc1.weight"" for e in range(num_local_experts)]         w1 = torch.cat([ep_state_dict[p] for p in fc1_params], dim=0).transpose(0,1)         for p in fc1_params:             del ep_state_dict[p]         ep_state_dict[f""decoder.layers.{l}.mlp.experts.weight1""] = w1         fc2_params = [f""decoder.layers.{l}.mlp.experts.local_experts.{e}.linear_fc2.weight"" for e in range(num_local_experts)]         w2 = torch.cat([ep_state_dict[p] for p in fc2_params], dim=1).transpose(0,1)         for p in fc2_params:             del ep_state_dict[p]         ep_state_dict[f""decoder.layers.{l}.mlp.experts.weight2""] = w2 ... ``` The model weights load correctly, but the **initial loss has increased to 6.x**. After comparing, I found that the output values from the forward computations of `SequentialMLP` experts and `GroupedMLP` experts are different, while everything else is the same. Could there be an issue with the model weight conversion logic, or is it something else wrong? The version of `grouped_gemm` being used is the latest version (main branch). **Looking forward to your reply!**",2024-07-08T11:57:07Z,,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/909,"Let's find the shape of each tensor for FC1:  **FC1 Weight of SeqMLP**: (ffn_hidden, hidden)  **After concatenation**: (expert_num * ffn_hidden, hidden)  **After transposition**: (hidden, expert_num * ffn_hidden) This is correct according to the definition of GroupedMLP. However, if you examine the `forward` method of GroupedMLP, you'll find: ```python w1 = self.weight1.view(self.num_local_experts, self.config.hidden_size, 1) w2 = self.weight2.view(self.num_local_experts, 1, self.config.hidden_size) ``` This code actually accepts a tensor with the shape (expert_num, hidden, ffn_hidden). Therefore, you should first create a tensor with this shape and then reshape it to (hidden, expert_num * ffn_hidden).  The discrepancy between the definition and usage is somewhat confusing. Recently, a new module called `TEGroupedMLP` was introduced. Switching to this module might be a better solution.  Does this meet your needs?","> Let's find the shape of each tensor for FC1: >  > * **FC1 Weight of SeqMLP**: (ffn_hidden, hidden) > * **After concatenation**: (expert_num * ffn_hidden, hidden) > * **After transposition**: (hidden, expert_num * ffn_hidden) >  > This is correct according to the definition of GroupedMLP. However, if you examine the `forward` method of GroupedMLP, you'll find: >  > ```python > w1 = self.weight1.view(self.num_local_experts, self.config.hidden_size, 1) > w2 = self.weight2.view(self.num_local_experts, 1, self.config.hidden_size) > ``` >  > This code actually accepts a tensor with the shape (expert_num, hidden, ffn_hidden). Therefore, you should first create a tensor with this shape and then reshape it to (hidden, expert_num * ffn_hidden). >  > The discrepancy between the definition and usage is somewhat confusing. Recently, a new module called `TEGroupedMLP` was introduced. Switching to this module might be a better solution. >  > Does this meet your needs? Thank you very much for your answer, the problem has been resolved now! ðŸ¤—","> Let's find the shape of each tensor for FC1: >  > * **FC1 Weight of SeqMLP**: (ffn_hidden, hidden) > * **After concatenation**: (expert_num * ffn_hidden, hidden) > * **After transposition**: (hidden, expert_num * ffn_hidden) >  > This is correct according to the definition of GroupedMLP. However, if you examine the `forward` method of GroupedMLP, you'll find: >  > w1 = self.weight1.view(self.num_local_experts, self.config.hidden_size, 1) > w2 = self.weight2.view(self.num_local_experts, 1, self.config.hidden_size) > This code actually accepts a tensor with the shape (expert_num, hidden, ffn_hidden). Therefore, you should first create a tensor with this shape and then reshape it to (hidden, expert_num * ffn_hidden). >  > The discrepancy between the definition and usage is somewhat confusing. Recently, a new module called `TEGroupedMLP` was introduced. Switching to this module might be a better solution. >  > Does this meet your needs? Hi yt , can I ask why the weights in GroupedMLP need to be defined and used in different shapes? Can I simply define the weight as (expert_num, hidden, ffn_hidden) ? Besides, if I want to manipulate the weight of a specific expert, what is the correct way to do that ? Thank you !","> > Let's find the shape of each tensor for FC1: > >  > > * **FC1 Weight of SeqMLP**: (ffn_hidden, hidden) > > * **After concatenation**: (expert_num * ffn_hidden, hidden) > > * **After transposition**: (hidden, expert_num * ffn_hidden) > >  > > This is correct according to the definition of GroupedMLP. However, if you examine the `forward` method of GroupedMLP, you'll find: > > w1 = self.weight1.view(self.num_local_experts, self.config.hidden_size, 1) > > w2 = self.weight2.view(self.num_local_experts, 1, self.config.hidden_size) > > This code actually accepts a tensor with the shape (expert_num, hidden, ffn_hidden). Therefore, you should first create a tensor with this shape and then reshape it to (hidden, expert_num * ffn_hidden). > > The discrepancy between the definition and usage is somewhat confusing. Recently, a new module called `TEGroupedMLP` was introduced. Switching to this module might be a better solution. > > Does this meet your needs? >  > Hi yt , can I ask why the weights in GroupedMLP need to be defined and used in different shapes? Can I simply define the weight as (expert_num, hidden, ffn_hidden) ? Besides, if I want to manipulate the weight of a specific expert, what is the correct way to do that ? Thank you ! `GroupedMLP ` is designed to be used in that way. I think you can try `TEGroupedMLP`."
ywb2018,function missing,"i cannot  find the defination of the function get_blend_from_list , anyone help?",2024-07-08T05:10:27Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/908
dementrock,[BUG] GPTDataset._build_document_sample_shuffle_indices does not build the indices on non-root nodes when not using NFS,"**Describe the bug** If the training data does not live on NFS but on nodespecific storage, the current logic in https://github.com/NVIDIA/MegatronLM/blob/0bc3547702464501feefeb5523b7a17e591b21fa/megatron/core/datasets/gpt_dataset.pyL346 skips building the indices and result in an error when loading the document index at https://github.com/NVIDIA/MegatronLM/blob/0bc3547702464501feefeb5523b7a17e591b21fa/megatron/core/datasets/gpt_dataset.pyL484, complaining that the file does not exist. **To Reproduce** Try running multinode training, pointing to training data not living on NFS. **Expected behavior** Ideally there should be a flag indicating whether the data storage is shared file system. If not, the index needs to be built on each node separately. **Stack trace/logs** ``` (worker6, rank=6, pid=8930, ip=10.42.3.242)   File ""/opt/megatronlm/megatron/core/datasets/blended_megatron_dataset_builder.py"", line 470, in build_generic_dataset (worker6, rank=6, pid=8930, ip=10.42.3.242)     dataset = cls(*args) (worker6, rank=6, pid=8930, ip=10.42.3.242)   File ""/opt/megatronlm/megatron/core/datasets/gpt_dataset.py"", line 111, in __init__ (worker6, rank=6, pid=8930, ip=10.42.3.242)     ) = self._build_document_sample_shuffle_indices() (worker6, rank=6, pid=8930, ip=10.42.3.242)   File ""/opt/megatronlm/megatron/core/datasets/gpt_dataset.py"", line 474, in _build_document_sample_shuffle_indices (worker6, rank=6, pid=8930, ip=10.42.3.242)     document_index = numpy.load(path_to_document_index, allow_pickle=True, mmap_mode='r') (worker6, rank=6, pid=8930, ip=10.42.3.242)   File ""/usr/local/lib/python3.10/distpackages/numpy/lib/npyio.py"", line 405, in load (worker6, rank=6, pid=8930, ip=10.42.3.242)     fid = stack.enter_context(open(os_fspath(file), ""rb"")) (worker6, rank=6, pid=8930, ip=10.42.3.242) FileNotFoundError: [Errno 2] No such file or directory: '/wiki/mistral_7b_v0.3_training_data_text_document/cache/GPTDataset_indices/81e3d4d910e734899c56ceb4ba98b98cGPTDatasettraindocument_index.npy' ``` **Environment (please complete the following information):**   MegatronLM commit ID: not sure how to check; using NeMo and `nvcr.io/nvidia/nemo:24.05.01`   PyTorch version: `2.3.0a0+ebedce2`   CUDA version: 12.4   NCCL version: 2.20.3 **Proposed fix** My workaround is the following patch: ```patch  /opt/megatronlm/megatron/core/datasets/gpt_dataset.py      20240707 03:48:09.635073980 +0000 +++ /opt/megatronlm/megatron/core/datasets/gpt_dataset.py.new  20240707 03:48:07.383130640 +0000 @@ 8,6 +8,7 @@  import numpy  import torch +import torch.distributed  from megatron.core.datasets.blended_megatron_dataset_config import BlendedMegatronDatasetConfig  from megatron.core.datasets.indexed_dataset import IndexedDataset @@ 342,7 +343,7 @@          if not path_to_cache or (              not cache_hit             and (not torch.distributed.is_initialized() or torch.distributed.get_rank() == 0) +            and (not torch.distributed.is_initialized() or os.environ.get('LOCAL_RANK', '0') == '0')          ):              log_single_rank( @@ 459,7 +460,9 @@              )              log_single_rank(logger, logging.INFO, f""> total number of epochs: {num_epochs}"")             return document_index, sample_index, shuffle_index +             return document_index, sample_index, shuffle_index + +        torch.distributed.barrier()          log_single_rank(              logger, logging.INFO, f""Load the {type(self).__name__} {self.index_split.name} indices"" ``` But it does not offer the flexibility of a flag.",2024-07-07T04:23:23Z,stale,open,0,10,https://github.com/NVIDIA/Megatron-LM/issues/907,"Met with the same problem when using 2 nodes 16 GPUs to finetune llama2 model. I use NFS to synchronize dataset files, but it will still cause a FileNotFoundError in the second node, although  it  already synchronized  the cache train dataset files after the first node generated them",Met the same problem when using multiple nodes and moving dataset to shared disk solved the problem.,"> Met with the same problem when using 2 nodes 16 GPUs to finetune llama2 model. I use NFS to synchronize dataset files, but it will still cause a FileNotFoundError in the second node, although it already synchronized the cache train dataset files after the first node generated them Set the seed and use NFS to synchronize dataset solve the problem",Marking as stale. No activity in 60 days.,"Hi, you can check this simple tutorial to learn how to use NFS to share files between different nodes. https://bluexp.netapp.com/blog/azureanfblglinuxnfsserverhowtosetupserverandclient I recommend that you use the master node to install nfskernelserver, and other nodes to install nfsclient. Creating a dataset directory (for your MegatronLM training) on the master node, and define access for other nodes in export file, so that you can synchronize training dataset between all your nodes. By the way, it may not work when the first time you launch training scripts ( it seems like a bug that `dist.barrier()` did not work well in new pytorch version, but the master node would successfully generate the train files. So you can launch it again, because the second time it wouldn't generate train files again and start training.) Nakroy ***@***.*** &nbsp; &nbsp;åŽŸå§‹é‚®ä»¶&nbsp; å‘ä»¶äºº:                                                                                                                        ""NVIDIA/MegatronLM""                                                                                    ***@***.***&gt;; å‘é€æ—¶é—´:&nbsp;2024å¹´9æœˆ23æ—¥(æ˜ŸæœŸä¸€) ä¸‹åˆ4:23 ***@***.***&gt;; ***@***.******@***.***&gt;; ä¸»é¢˜:&nbsp;Re: [NVIDIA/MegatronLM] [BUG] GPTDataset._build_document_sample_shuffle_indices does not build the indices on nonroot nodes when not using NFS (Issue CC([BUG] GPTDataset._build_document_sample_shuffle_indices does not build the indices on nonroot nodes when not using NFS)) Met with the same problem when using 2 nodes 16 GPUs to finetune llama2 model. I use NFS to synchronize dataset files, but it will still cause a FileNotFoundError in the second node, although it already synchronized the cache train dataset files after the first node generated them Set the seed and use NFS to synchronize dataset solve the problem Hello, I met the same problem. And I don't understand how to use NFS. Could you please share me this method? â€” Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you commented.Message ID: ***@***.***&gt;",Marking as stale. No activity in 60 days.," Met with the same problem,  > > Met with the same problem when using 2 nodes 16 GPUs to finetune llama2 model. I use NFS to synchronize dataset files, but it will still cause a FileNotFoundError in the second node, although it already synchronized the cache train dataset files after the first node generated them >  > Set the seed and use NFS to synchronize dataset solve the problem I also use the NFS file system, but it seems that I need to start the task twice to solve it, which is not friendly to me. ðŸ™","> Met with the same problem, >  > > > Met with the same problem when using 2 nodes 16 GPUs to finetune llama2 model. I use NFS to synchronize dataset files, but it will still cause a FileNotFoundError in the second node, although it already synchronized the cache train dataset files after the first node generated them > >  > >  > > Set the seed and use NFS to synchronize dataset solve the problem >  > I also use the NFS file system, but it seems that I need to start the task twice to solve it, which is not friendly to me. ðŸ™ Yes, I need to launch twice because the two nodes would automatically generate training data respectively, that's not graceful...","I encountered the same issue.   Here is the data file generated by the main node:   > data/BookCorpusDataset_text_document/cache/GPTDataset_indices: 6d84f3595d97dfece2364c3950a26906GPTDatasetvaliddescription.txt 6d84f3595d97dfece2364c3950a26906GPTDatasetvaliddocument_index.npy 6d84f3595d97dfece2364c3950a26906GPTDatasetvalidsample_index.npy 6d84f3595d97dfece2364c3950a26906GPTDatasetvalidshuffle_index.npy d296b3899150edfd9092c34b30fa03c1GPTDatasettestdescription.txt d296b3899150edfd9092c34b30fa03c1GPTDatasettestdocument_index.npy d296b3899150edfd9092c34b30fa03c1GPTDatasettestsample_index.npy d296b3899150edfd9092c34b30fa03c1GPTDatasettestshuffle_index.npy fa7397310cb8333b787979cb7c45c55fGPTDatasettraindescription.txt fa7397310cb8333b787979cb7c45c55fGPTDatasettraindocument_index.npy fa7397310cb8333b787979cb7c45c55fGPTDatasettrainsample_index.npy fa7397310cb8333b787979cb7c45c55fGPTDatasettrainshuffle_index.npy  When I add the mounted NFS shared dataset path [model/sharedata] on the subnode, the subnode locates a data file that doesn't exist on the main node.   `FileNotFoundError: [Errno 2] No such file or directory: '/workspace/megatronlm/models/gpt3/dataset/BookCorpusDataset_text_document/cache/GPTDataset_indices/2bcc0d01e685e944ad9c0c8ea43d126fGPTDatasettraindocument_index.npy`  However, when I add a nonshared dataset [model/data] on the subnode, the subnode locates a data file that does exist on the main node.   `FileNotFoundError: [Errno 2] No such file or directory: '/workspace/megatronlm/models/gpt3/dataset/BookCorpusDataset_text_document/cache/GPTDataset_indices/fa7397310cb8333b787979cb7c45c55fGPTDatasettraindocument_index.npy` Could it be due to different seeds? Why is this happening?",Marking as stale. No activity in 60 days.
zhaoyinglia,[BUG] wrong loss scaling when context parallel is on,"**Describe the bug** Hi, I think there is a bug when context parallel is on and we can discuss it. https://github.com/NVIDIA/MegatronLM/blob/0bc3547702464501feefeb5523b7a17e591b21fa/pretrain_gpt.pyL148 From this issueï¼Œi know the result is same for dp2cp4 and dp8 with the same global batch_size.  But the code logic is different bewteen above issue and current code logic. In above issue logic, the loss scaling with cp_size and grad_data scaling with the world_size from get_data_parallel_group(with_context_parallel=True) In current code logic, the loss scaling with cp_size, but grad_data scaling with the world_size from get_data_parallel_group() Two logic have different grad_data. (print the grad_data after allreduce it)  **To Reproduce** dp2cp4 and dp8 with same parameter can reproduce the result **Proposed fix** remove the loss scaling with cp_size in loss_func",2024-07-07T02:20:27Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/906, Can you please help to answer this question? Thank you!,"Hi   Thanks for reaching out. Could you please point me to the code of the following? > In current code logic, the loss scaling with cp_size, but grad_data scaling with the world_size from get_data_parallel_group() Weight parameters are replicated across the combined CP+DP group, so we are supposed to do weight gradients allreduce or reducescatter across combined CP+DP group. If we indeed have above grad_data scaling change, we need to fix it.",The issue has been resolved at https://github.com/NVIDIA/MegatronLM/commit/3bdcbbbe5d2a455a75e28969be7250cd4bd27bae. Thank you!
felipeliliti,AURORA STK 3.6.9 ,https://www.notion.so/d301219405294676a2a30650bea24baa?v=6c09be9b3fd64ff9bb1c2bae62086984&pvs=4,2024-07-06T21:11:28Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/905
omahs,Fix typos,Fix typos,2024-07-06T15:57:24Z,,closed,0,5,https://github.com/NVIDIA/Megatron-LM/issues/904,Marking as stale. No activity in 60 days.,up,"Hi , can you fix conflicts and compact your modification into a single commit? Then I can help land it in our internal repo."," Hey, I just squashed them and fixed the conflict",Your commit is squash merged in https://github.com/NVIDIA/MegatronLM/commit/2488e20d488b35259f6eb53267e8a0525a373599 and your author information is preserved.  Thanks for your contribution. You can close this PR now. 
Yuxin-CV,[BUG] context manager syntax bug in transformer_block.py,please see https://github.com/NVIDIA/MegatronLM/pull/902,2024-07-06T04:21:21Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/903, Can you please take a look at the proposed changes? Thank you!,Acked!,Marking as stale. No activity in 60 days.
Yuxin-CV,fix context manager syntax bug in transformer_block.py,,2024-07-06T04:17:47Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/902,Thanks for spotting the bug! Working on fixing for the next release,Marking as stale. No activity in 60 days.
felipeliliti,AURORA STK 3.6.9 I A ,https://www.notion.so/BEMVINDOAOAURORASTK369Id301219405294676a2a30650bea24baa?pvs=4,2024-07-03T15:02:57Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/901
1049451037,[QUESTION] When will model have `_extra_state`?,"After updating to the main branch of MegatronLM recently, I met this error when loading model: ``` Unexpected key(s) in state_dict: ""decoder.layers.0.self_attention.core_attention._extra_state"" ``` The checkpoint is transformed by the `tools/checkpoint/convert.py`, and loaded by `pretrain_gpt.py`.",2024-07-03T06:53:27Z,,closed,4,1,https://github.com/NVIDIA/Megatron-LM/issues/900,"I had faced this error too, recently, and had to write a downstream script to remove this from every layer  looks like the Mcore format is out of sync with what MegatronLM consumes?"
Leo-T-Zang,[QUESTION] Does Megatron-LM supports Flash Attention for BERT and T5 Pretraining?,"**My question** Does MegatronLM supports Flash Attention for BERT and T5 Pretraining? If so, where is the code specifically supports such feature? Thanks!!!",2024-07-02T23:08:46Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/899
willy808,[BUG] WHEN install with nemo,"when i do make in core/datasets  after install, it got below error please help! make g++ O3 Wall shared std=c++11 fPIC fdiagnosticscolor I/usr/include/python3.10 I/usr/local/lib/python3.10/distpackages/pybind11/include helpers.cpp o helpers.cpython310x86_64linuxgnu.so In file included from helpers.cpp:12: /usr/local/lib/python3.10/distpackages/pybind11/include/pybind11/numpy.h: In function 'void build_exhaustive_blending_indices(pybind11::array_t&, pybind11::array_t&, const pybind11::array_t&, int32_t)': /usr/local/lib/python3.10/distpackages/pybind11/include/pybind11/numpy.h:494:14: warning: 'error_argmax' may be used uninitialized in this function [Wmaybeuninitialized]   494              ^~~~~~~~~~~~",2024-07-02T06:01:21Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/898,This is a warning. Closing the issue and please reopen if needed.
Yuhanleeee,Batch_input and elapsed time per iteration slow down during model training," Batch_input and elapsed time per iteration slow down during model training !å¾®ä¿¡å›¾ç‰‡ç¼–è¾‘_20240629150957  Arguments data_impl ....................... mmap........................updated   deepspeed_extra_args ............ {'bf16': {'enabled': True}}.updated   dynamic_loss_scale .............. True........................updated   eval_interval ................... 40000.......................updated   eval_iters ...................... 10..........................updated   fp32_allreduce .................. True........................updated   global_num_gpus ................. 4...........................updated   gpt_j_residual .................. True........................updated   hidden_size ..................... 768.........................updated   init_method ..................... small_init..................updated   is_pipe_parallel ................ True........................updated   launcher ........................ slurm.......................updated   log_interval .................... 10..........................updated   lr .............................. 0.0006......................updated   lr_decay_iters .................. 143000......................updated   lr_decay_style .................. cosine......................updated   max_position_embeddings ......... 2048........................updated   min_lr .......................... 6e05.......................updated   no_weight_tying ................. True........................updated   num_attention_heads ............. 12..........................updated   num_layers ...................... 12..........................updated   num_workers ..................... 32..........................updated   optimizer ....................... {'type': 'Adam', 'params': {'lr': 0.0006, 'betas': [0.9, 0.95], 'eps': 1e08}}updated   optimizer_type .................. Adam........................updated   output_layer_init_method ........ wang_init...................updated   partition_activations ........... True........................updated   pipe_parallel_size .............. 1...........................updated   pos_emb ......................... rotary......................updated   precision ....................... bfloat16....................updated   rotary_pct ...................... 0.25........................updated   save ............................ /pythia/checkpoints/test_1updated   save_iters ...................... [10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000, 100000, 110000, 120000, 130000, 140000]updated   seq_length ...................... 2048........................updated   sparsity_config ................. {}..........................updated   synchronize_each_layer .......... True........................updated   test_data_paths ................. ['/pile_0.87_deduped_text_document/pile_0.87_deduped_text_document']updated   test_data_weights ............... [1.0].......................updated   text_gen_type ................... unconditional...............updated   tokenizer_type .................. HFTokenizer.................updated   train_batch_size ................ 128.........................updated   train_data_paths ................ ['/pile_0.87_deduped_text_document/pile_0.87_deduped_text_document']updated   train_data_weights .............. [1.0].......................updated   train_iters ..................... 143000......................updated   train_micro_batch_size_per_gpu .. 32..........................updated   user_script ..................... train.py....................updated   valid_data_paths ................ ['pile_0.87_deduped_text_document/pile_0.87_deduped_text_document']updated   valid_data_weights .............. [1.0].......................updated   vocab_file ....................../pythia/utils/20B_tokenizer.jsonupdated   wall_clock_breakdown ............ True........................updated   zero_allgather_bucket_size ...... 500000000...................updated   zero_contiguous_gradients ....... True........................updated   zero_optimization ............... {'stage': 0, 'allgather_partitions': True, 'allgather_bucket_size': 500000000, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 500000000, 'contiguous_gradients': True, 'cpu_offload': False, 'load_from_fp32_weights': False}updated   zero_reduce_bucket_size ......... 500000000...................updated   zero_reduce_scatter ............. True........................updated   zero_stage ...................... 0...........................updated   account ......................... None........................default   activation ...................... gelu........................default   activation_checkpointing ........ None........................default   adlr_autoresume ................. False.......................default   adlr_autoresume_interval ........ 1000........................default   amp ............................. None........................default   apply_query_key_layer_scaling ... False.......................default   attention_dropout ............... 0...........................default   attention_softmax_in_fp32 ....... False.......................default   autotuning ...................... None........................default   autotuning_run .................. None........................default   base_shapes_file ................ None........................default   bf16 ............................ None........................default   bias_dropout_fusion ............. False.......................default   bias_gelu_fusion ................ False.......................default   char_level_ppl .................. False.......................default   checkpoint ...................... None........................default   checkpoint_in_cpu ............... False.......................default   checkpoint_num_layers ........... 1...........................default   checkpoint_scale ................ linear......................default   checkpoint_validation_with_forward_pass  False................default   clip_grad ....................... 1.0.........................default   comment ......................... None........................default   comms_logger .................... None........................default   communication_data_type ......... None........................default   compression_training ............ None........................default   contiguous_checkpointing ........ False.......................default   coord_check ..................... False.......................default   create_moe_param_group .......... True........................default   csv_monitor ..................... None........................default   curriculum_learning ............. None........................default   curriculum_seqlen ............... 0...........................default   data_efficiency ................. None........................default   data_path ....................... None........................default   data_types ...................... None........................default   deepscale ....................... False.......................default   deepscale_config ................ None........................default   deepspeed ....................... True........................default   deepspeed_activation_checkpointing  True......................default   deepspeed_mpi ................... False.......................default   deepspeed_slurm ................. False.......................default   detect_nvlink_pairs ............. False.......................default   distributed_backend ............. nccl........................default   do_test ......................... None........................default   do_train ........................ None........................default   do_valid ........................ None........................default   dump_state ...................... False.......................default   elasticity ...................... None........................default   enable_expert_tensor_parallelism  False.......................default   eod_mask_loss ................... False.......................default   eval_results_prefix ............. ............................default   eval_tasks ...................... None........................default   exclude ......................... None........................default   exit_interval ................... None........................default   expert_interval ................. 2...........................default   extra_save_iters ................ None........................default   finetune ........................ False.......................default   flops_profiler .................. None........................default   force_multi ..................... False.......................default   fp16 ............................ None........................default   fp16_lm_cross_entropy ........... False.......................default   git_hash ........................ 4c426da.....................default   gmlp_attn_dim ................... 64..........................default   gpt_j_tied ...................... False.......................default   gradient_accumulation_steps ..... 1...........................default   gradient_clipping ............... 1.0.........................default   gradient_noise_scale_cpu_offload  False.......................default   gradient_noise_scale_n_batches .. 5...........................default   gradient_predivide_factor ....... 1.0.........................default   hidden_dropout .................. 0...........................default   hostfile ........................ None........................default   hysteresis ...................... 2...........................default   include ......................... None........................default   init_method_std ................. 0.02........................default   intermediate_size ............... None........................default   iteration ....................... None........................default   keep_last_n_checkpoints ......... None........................default   label_data_paths ................ None........................default   layernorm_epsilon ............... 1e05.......................default   layernorm_fusion ................ False.......................default   lazy_mpu_init ................... False.......................default   load ............................ None........................default   local_rank ...................... None........................default   log_dir ......................... None........................default   log_grad_norm ................... False.......................default   log_grad_pct_zeros .............. False.......................default   log_gradient_noise_scale ........ False.......................default   log_optimizer_states ............ False.......................default   log_param_norm .................. False.......................default   loss_scale ...................... None........................default   loss_scale_window ............... 1000.0......................default   make_vocab_size_divisible_by .... 128.........................default   mamba_causal_conv_fusion ........ False.......................default   mamba_inner_func_fusion ......... False.......................default   mamba_selective_fp32_params ..... True........................default   mamba_selective_scan_fusion ..... False.......................default   mamba_use_bias_in_conv .......... True........................default   mamba_use_bias_in_linears ....... False.......................default   master_addr ..................... None........................default   master_port ..................... 29500.......................default   maximum_tokens .................. 64..........................default   memory_profiling ................ False.......................default   memory_profiling_path ........... None........................default   merge_file ...................... None........................default   min_scale ....................... 1.0.........................default   mlp_type ........................ regular.....................default   mmap_warmup ..................... False.......................default   model_parallel_size ............. 1...........................default   moe_eval_capacity_factor ........ 1.0.........................default   moe_expert_parallel_size ........ 1...........................default   moe_glu ......................... False.......................default   moe_jitter_eps .................. None........................default   moe_lbl_in_fp32 ................. False.......................default   moe_loss_coeff .................. 0.1.........................default   moe_min_capacity ................ 4...........................default   moe_num_experts ................. 1...........................default   moe_token_dropping .............. False.......................default   moe_top_k ....................... 1...........................default   moe_train_capacity_factor ....... 1.0.........................default   moe_type ........................ megablocks..................default   moe_use_residual ................ True........................default   mup_attn_temp ................... 1.0.........................default   mup_embedding_mult .............. 1.0.........................default   mup_init_scale .................. 1.0.........................default   mup_output_temp ................. 1.0.........................default   mup_rp_embedding_mult ........... 1.0.........................default   mup_width_scale ................. 2...........................default   no_load_optim ................... False.......................default   no_load_rng ..................... False.......................default   no_save_optim ................... False.......................default   no_save_rng ..................... False.......................default   no_ssh_check .................... False.......................default   norm ............................ layernorm...................default   num_gpus ........................ None........................default   num_kv_heads .................... None........................default   num_nodes ....................... 1..........................default   num_samples ..................... 1...........................default   num_unique_layers ............... None........................default   onnx_safe ....................... False.......................default   opt_pos_emb_offset .............. 0...........................default   output_layer_parallelism ........ column......................default   override_lr_scheduler ........... False.......................default   padded_vocab_size ............... None........................default   param_sharing_style ............. grouped.....................default   pipe_partition_method ........... type:transformer|mlp........default   prescale_gradients .............. False.......................default   profile ......................... False.......................default   profile_backward ................ False.......................default   profile_step_start .............. 10..........................default   profile_step_stop ............... 12..........................default   prompt_end ......................  ...........................default   rank ............................ None........................default   recompute ....................... False.......................default   return_logits ................... False.......................default   rms_norm_epsilon ................ 1e08.......................default   rope_fusion ..................... False.......................default   rotary_emb_base ................. 10000.......................default   rotary_save_freqs_buffer ........ False.......................default   rpe_max_distance ................ 128.........................default   rpe_num_buckets ................. 32..........................default   s3_chunk_size ................... 104857600...................default   s3_path ......................... None........................default   sample_input_file ............... None........................default   sample_output_file .............. samples.txt.................default   save_base_shapes ................ False.......................default   scaled_masked_softmax_fusion .... False.......................default   scaled_upper_triang_masked_softmax_fusion  False..............default   scalenorm_epsilon ............... 1e08.......................default   scheduler ....................... None........................default   seed ............................ 1234........................default   short_seq_prob .................. 0.1.........................default   sliding_window_width ............ None........................default   soft_prompt_tuning .............. None........................default   sparse_attention ................ None........................default   sparse_gradients ................ False.......................default   split ........................... 969, 30, 1..................default   steps_per_print ................. 10..........................default   temperature ..................... 0.0.........................default   tensorboard ..................... None........................default   tensorboard_dir ................. None........................default   top_k ........................... 0...........................default   top_p ........................... 0.0.........................default   use_bias_in_attn_linear ......... True........................default   use_bias_in_norms ............... True........................default   use_bnb_optimizer ............... False.......................default   use_checkpoint_lr_scheduler ..... False.......................default   use_cpu_initialization .......... False.......................default   use_mup ......................... False.......................default   use_qk_layernorm ................ False.......................default   use_shared_fs ................... True........................default   use_tutel ....................... False.......................default   use_wandb ....................... None........................default   wandb ........................... None........................default   wandb_group ..................... None........................default   wandb_host ...................... https://api.wandb.ai........default   wandb_init_all_ranks ............ False.......................default   wandb_project ................... neox........................default   wandb_team ...................... None........................default   warmup .......................... 0.01........................default   weight_by_num_documents ......... False.......................default   weight_decay .................... 0.1.........................default   weighted_sampler_alpha .......... 1.0.........................default   world_size ...................... None........................default  Environment:  PyTorch  version: 2.3.1  CUDA version: 12.2  NCCL version: 2.20.5  Hardware:  GPU: A100SXM440GB   CPU: AMD EPYC 7543 32Core Processor  Memory: 263793632 kB (total), 195607748 kB (free)",2024-06-29T12:38:59Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/897,Can you try running without deepspeed? Thanks,Marking as stale. No activity in 60 days.
longzhang418,[BUG]Question about helpers.cpp in version core_v0.7.0,"First question: I found a bug in the helpers.cpp file about the error_argmax variable. It was declared on line 49, but the use of it on line 61 and later may trigger the error of using it without initialized.  Second question: And when I first compiled helpers.cpp, I encountered this phenomenon. Has anyone encountered this before? Is it a problem with the pybind11/numpy.h version?  Looking forward to your reply, thank you~",2024-06-28T06:32:47Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/896
costin-eseanu,Fix missing BookCorpus dataset in the sequence parallelism example.,,2024-06-27T22:43:43Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/895
kiddyboots216,[REGRESSION] MoEs are obtaining higher loss than they should during training,"**Describe the regression** In the forks of MegatronLM used by gptneox and megatrondeepspeed, MoEs are obtaining lower loss than they are in MegatronLM with the same configuration. **To Reproduce** Attached to this issue are config files to reproduce the exact MoE we are running. `megatron_125M_k1_e16_moe_3e4_config.sh` is the MoE config for megatronlm, `megatron_dense_125M_config.sh` is the dense config for megatronlm, `gptneox_e16_k1_config.yaml` is the MoE config for gptneox. All models are gptstyle.  megatron_dense_125M_config.txt megatron_125M_k1_e16_moe_3e4_config.txt gptneox_e16_k1_config.txt **Previous performance** After step `12000` in gptneox the MoE has training loss `2.452`. **New performance** After step `12000` in megatronlm the MoE has training loss `2.649` which is the same as the dense model. **Stack trace/logs** The logs are attached to this issue. **Environment (please complete the following information):**   Previous MegatronLM commit ID: the version of MegatronLM forked by gptneox / megatrondeepspeed   New MegatronLM commit ID: the current commit ID; `e33c8f78a35765d5aa37475a144da60e8a2349d1`   Previous PyTorch version: same PyTorch version   New PyTorch version: same PyTorch version   Previous CUDA version: same CUDA version   New CUDA version: same CUDA version   Previous NCCL version: same NCCL version   New NCCL version: same NCCL version **Proposed fix** No proposed fix. **Additional context** Presumably a bug was introduced in the MoE training. However, I looked into the `gpt` code in mcore.models and was unable to find any potential causes.",2024-06-27T17:46:37Z,,closed,1,10,https://github.com/NVIDIA/Megatron-LM/issues/894,"Here is some additional information: The figure below shows validation loss curves for 125M MoE and dense models trained with megatron_125M_k1_e16_moe_3e4_config.txt (maxLR \in {1e4. 3e4, 6e4, 9e4}) and megatron_dense_125M_config.txt, respectively. We observe that the MoE models underperform the dense models contrary to results form the literature. As mentioned above, this suggests that there is a bug in MegatronLM !e16k1moemaxlr3e4_vs_densemaxlr3e4 Moreover, below is a plot directly comparing the training loss of dense and MoE models in Megatron and GPTNeoX trained using GBS=768, SL=2048, E=16 (total exps), K=1 (active exps). All models are trained using the same dataset and the same linear warmup+consine annealing LRS (maxLR3e4 to minLR3e5). We observe that the GPTNeoX implementation has results in line with the literature (e.g., switch transformer Figure1 right), while the Megatron implementation does not. This suggests there is a bug in MegatronLM   barker  !megatron_and_neox_comparison"," Here is the validation loss plot for more MoE configs, again with varying LRs that all underperform the dense model.","> Here is some additional information: >  > The figure below shows validation loss curves for 125M MoE and dense models trained with megatron_125M_k1_e16_moe_3e4_config.txt (maxLR \in {1e4. 3e4, 6e4, 9e4}) and megatron_dense_125M_config.txt, respectively. We observe that the MoE models underperform the dense models contrary to results form the literature. As mentioned above, this suggests that there is a bug in MegatronLM >  > !e16k1moemaxlr3e4_vs_densemaxlr3e4 Personally, I think we should change the horizontal axis to FLOPs and then compare the loss.","> > Here is some additional information: >  > >  >  > > The figure below shows validation loss curves for 125M MoE and dense models trained with megatron_125M_k1_e16_moe_3e4_config.txt (maxLR \in {1e4. 3e4, 6e4, 9e4}) and megatron_dense_125M_config.txt, respectively. We observe that the MoE models underperform the dense models contrary to results form the literature. As mentioned above, this suggests that there is a bug in MegatronLM >  > >  >  > > !e16k1moemaxlr3e4_vs_densemaxlr3e4 >  >  >  > Personally, I think we should change the horizontal axis to FLOPs and then compare the loss. These MoEs are all K=1, so they are already FLOPSmatched (in other words the plots would be the same if we changed the horizontal axis to FLOPS.)","Running the same config with MegatronDeepSpeed does result in the MoE outperforming the dense model. This was run with 8 experts, topk=1 and a 125M base model.  ",Thank you for reporting the issue! We will investigate it and get back to you soon.,Thank you  !,"Hi   , We have done some investigations and discovered that the issue specifically pertains to the Top1 selection, and the root cause is the ordering of softmax and topk. In short, we should apply softmax before selecting the topk if k equals 1, since performing softmax on [num_tokens, 1] would result in a gradient of 0. Below is our experiments and code changes:  ","Thanks  . I did observe that with topk =2, the results were significantly better than what the literature suggests in terms of being better than topk 1. ",FYI: We have added the argument `moerouterpresoftmax` on commit Thank you all!
ashors1,Make TE and Apex dependencies optional,This PR provides a fallback codepath that defaults to pure Pytorch/jit when TE and Apex are not installed.,2024-06-26T22:25:57Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/893
sambar1729,[QUESTION] Getting tools/preprocess_data.py to work is painful,"**Your question** Can `tools/preprocess_data.py` be simplified?  Using  ``` python tools/preprocess_data.py \        input mycorpus.json \        outputprefix mygpt2 \        vocabfile gpt2vocab.json \        tokenizertype GPT2BPETokenizer \        mergefile gpt2merges.txt \        appendeod ``` Right now, it requires nltk, torch, transformer_engine, as well as apex.  Installing transformer_engine does not work out of the box  had to install out of box (on a A100).  Installing apex has similar problems, when using https://github.com/NVIDIA/apex?tab=readmeovfilelinux   Given that the repo does not have some sample `idx`, `bin` files, one would expect the `preprocess_data` process to be relatively simple. Could this process be simplified?  Installing apex ``` $ pip install v disablepipversioncheck nocachedir nobuildisolation configsettings ""buildoption=cpp_ext"" configsettings ""buildoption=cuda_ext"" ./ ``` gives  ``` .... ....   If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].     warnings.warn(   Emitting ninja build file /home/megauser/apex/build/temp.linuxx86_64cpython310/build.ninja...   Compiling objects...   Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)   [1/2] c++ MMD MF /home/megauser/apex/build/temp.linuxx86_64cpython310/csrc/mlp.o.d pthread B /home/megauser/.conda/envs/pre/compiler_compat Wnounusedresult Wsigncompare DNDEBUG fwrapv O2 Wall fPIC O2 isystem /home/megauser/.conda/envs/pre/include fPIC O2 isystem /home/megauser/.conda/envs/pre/include fPIC I/home/megauser/.conda/envs/pre/lib/python3.10/sitepackages/torch/include I/home/megauser/.conda/envs/pre/lib/python3.10/sitepackages/torch/include/torch/csrc/api/include I/home/megauser/.conda/envs/pre/lib/python3.10/sitepackages/torch/include/TH I/home/megauser/.conda/envs/pre/lib/python3.10/sitepackages/torch/include/THC I/usr/local/cuda/include I/home/megauser/.conda/envs/pre/include/python3.10 c c /home/megauser/apex/csrc/mlp.cpp o /home/megauser/apex/build/temp.linuxx86_64cpython310/csrc/mlp.o O3 DVERSION_GE_1_1 DVERSION_GE_1_3 DVERSION_GE_1_5 DTORCH_API_INCLUDE_EXTENSION_H 'DPYBIND11_COMPILER_TYPE=""_gcc""' 'DPYBIND11_STDLIB=""_libstdcpp""' 'DPYBIND11_BUILD_ABI=""_cxxabi1011""' DTORCH_EXTENSION_NAME=mlp_cuda D_GLIBCXX_USE_CXX11_ABI=0 std=c++17   /home/megauser/apex/csrc/mlp.cpp: In function â€˜std::vector mlp_forward(int, int, std::vector)â€™:   /home/megauser/apex/csrc/mlp.cpp:57:21: warning: comparison of integer expressions of different signedness: â€˜intâ€™ and â€˜long unsigned intâ€™ [Wsigncompare]      57                               ^~~~ ````",2024-06-26T19:34:58Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/892
sambar1729,"[QUESTION] Sample idx, bin files in public domain for trying out pretrain_gpt.py? ","**Your question** Ask a clear and concise question about MegatronLM. Can we have a sample idx + bin files as required by the pretrain_gpt.py ?  Running tools/preprocess_data.py on some sample data like ``` {""src"": ""www.nvidia.com"", ""text"": ""The quick brown fox"", ""type"": ""Eng"", ""id"": ""0"", ""title"": ""First Part""} {""src"": ""The Internet"", ""text"": ""jumps over the lazy dog"", ""type"": ""Eng"", ""id"": ""42"", ""title"": ""Second Part""} ``` needs transformer_engine and on an A100 this takes a long time to build from source (the pip install  ``` pip install git+https://github.com/NVIDIA/TransformerEngine.git ``` also fails).  This is just too much work to get some training data to run `pretrain_gpt.py` with. Can some sample `idx`, `bin` files as required by the pretraining be provided in a public place? Thanks.",2024-06-26T18:35:27Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/891, Can you please help with this question? Thank you!,"Hi, we've made some changes to Megatron recently to remove the required dependency on Transformer Engine. You should no longer need to install Transformer Engine to run this script. The following works for me:  ``` conda create name megatron python==3.10.12 conda activate megatron conda install pytorch==2.2.0 torchvision torchaudio pytorchcuda=12.1 c pytorch c nvidia git clone https://github.com/NVIDIA/MegatronLM.git && cd MegatronLM && pip install . pip install six regex python3 tools/preprocess_data.py input your_input.json tokenizertype GPT2BPETokenizer workers 1 outputprefix test vocabfile your_bpe/vocab.json mergefile your_bpe/merges.txt ```"
JiwenJ,[QUESTION] Has standalone_embedding_stage been supported yet in core?,"I met an issue and want to split the embedding layer out of transformer block to make it alone in single pp stage, but I found that it has not been supported in core. Am I right? https://github.com/NVIDIA/MegatronLM/blob/e33c8f78a35765d5aa37475a144da60e8a2349d1/megatron/core/transformer/transformer_block.pyL164",2024-06-26T06:45:23Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/890,"Same question. Keep tuned. I also have another question: As the language model head has the same shape with embedding layer, does MegatronLM support standalone the language model head as a single pp stage? It will save memory and improve training efficiency."
Mooler0410,[QUESTION] Why does the tokenizer of mamba-2-hybrid have two ids for the token 'Yes'? id 24639 and id 7298,Hi! I found that the id 24639 and id 7298 will be decoded to the same token 'Yes' for mamba2hybrid. ```bash $: tokenizer.detokenize([24639])==tokenizer.detokenize([7298])  $: True ``` Also: ```bash $: tokenizer.detokenize([24639]) $: 'Yes' $: tokenizer.detokenize([7298]) $: 'Yes' ``` I always think different ids correspond to different tokens. Is there anything wrong with my understanding? Thanks!,2024-06-25T21:42:36Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/889,https://github.com/NVIDIA/MegatronLM/tree/main/examples/mamba I use this script to try mamba2. And I insert three lines: ```python     from megatron.training import get_tokenizer     tokenizer = get_tokenizer()     import pdb; pdb.set_trace() ``` before this line: https://github.com/NVIDIA/MegatronLM/blob/e33c8f78a35765d5aa37475a144da60e8a2349d1/tools/run_mamba_text_generation_server.pyL105 to test the tokenizer. 
wplf,modify typo in megatron/core/models/bert/bert_model.py,!image,2024-06-24T08:42:19Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/888
wplf,modify typo in bert_model.py,!image,2024-06-24T08:40:57Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/887
FancyXun,Rename the correct variable of seed,Rename seed to seed_,2024-06-23T12:55:08Z,stale,open,0,4,https://github.com/NVIDIA/Megatron-LM/issues/886,Marking as stale. No activity in 60 days.,Thanks. I'll include your commit in my internal fix PR.,Your commit is squash merged in https://github.com/NVIDIA/MegatronLM/commit/2488e20d488b35259f6eb53267e8a0525a373599 and your author information is preserved.  Thanks for your contribution. You can close this PR now. ,Marking as stale. No activity in 60 days.
jingxu9x,OPTIM get_batch traffic when enable context-parallel,"we can split batch's sequencelength before broadcast in tp_group, which can save time in get_batch",2024-06-22T06:20:50Z,stale,open,0,5,https://github.com/NVIDIA/Megatron-LM/issues/885,Marking as stale. No activity in 60 days.,"Hi Superkeyv, I encountered a similar issue while training with a sequence length of 32k on 8*H800 GPUs. At certain steps, the TP broadcast takes an extremely long time (about 10 seconds). This is quite unusual because the total data size is only 2 * 16k * 4 bytes (batch size * sequence length * 4 bytes, and we do not use an attention mask). In theory, this should only take a few us. Do you have any idea why this might be happening?",  Can you explain in detail what the certain step did? do eval?,I figured it outâ€”the latency is due to the dataset server being under high load. This causes some steps to slow down. Setting a larger prefetch_factor should help fix it.,Marking as stale. No activity in 60 days.
clarence-lee-sheng,[QUESTION] Why is TELayerNormColumnParallelLinear used instead of TEColumnParallelLinear in gpt_layer_specs,"In the file megatron/core/models/gpt/gpt_layer_specs.py line 95, on the line ""linear_fc1=TELayerNormColumnParallelLinear if use_te else ColumnParallelLinear"" why is it TELayerNormColumnParallelLinear used instead of TEColumnParallelLinear since TEColumnParallelLinear should be the direct conversion of ColumnParallelLinear",2024-06-21T10:09:14Z,,closed,1,3,https://github.com/NVIDIA/Megatron-LM/issues/884,"This line https://github.com/NVIDIA/MegatronLM/blob/e33c8f78a35765d5aa37475a144da60e8a2349d1/megatron/core/models/gpt/gpt_layer_specs.pyL45 makes it such that if you are doing a dense model, the `pre_mlp_layernorm` is an `IdentityOp`. Therefore, it has to be fused into the fc1 in the MLP, which is what `TELayerNormColumnParallelLinear` does. The `pre_mlp_layernorm` is only not an identity op if you're doing an MoE, in which case it's a `FusedLayerNorm` and then the MoE's GroupedMLP doesn't have a LayerNorm. TL;DR either way there is just one LayerNorm call. ","`TELayerNormColumnParallelLinear` has additional improvements over `TEColumnParallelLinear`, so we try to use it when possible. For example, in FP8 training, workflows are:  `TELayerNormColumnParallelLinear`: Layernorm (output in FP8) > FP8 GEMM  `TENorm` + `TEColumnParallelLinear`: Layernorm (output in BF16) > cast_to_fp8 (output in FP8) > FP8 GEMM.",Thank you so much for the explanation and also the additional insight of the improvements for FP8 training :)
dong-liuliu,"[QUESTION] What's the internal difference for training when setting only ""fp8-format"" or setting ""fp8-format""+""bf16""","**Your question** I'm trying to train GPT/LLAMA on top of MegatronLM, but confused on fp8 performance. Setting fp8 format parameters together with ""bf16"" is much better than the situation without ""bf16"". So what's difference between them inside MegatronLM? When setting fp8 + bf16, will MegatronLM try to split some computation to bf16 if more efficient, or to fp8 for high throughtput?",2024-06-21T09:25:11Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/883,"Only ""fp8format"": FP32 + FP8 ""fp8format""+""bf16"": BF16 + FP8 You could consider FP8 as an additional feature on top of your current (BF16 or no BF16) training recipe.",Marking as stale. No activity in 60 days.
artyomtugaryov,[BUG] Wrong lr multiplier,"In the `megatron/core/optimizer/__init__.py` file the `_get_param_groups` function overides original `lr_mult` and setup groups of parameters uncorrectly. Thus, I propose the fix to kip the original value and setup parameters groups correctly.",2024-06-21T02:06:09Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/882
schheda1,[BUGS] Pipeline Parallelism fails/hangs with Megatron Core example,"**Describe the bug** When the provided example script is configured to use pipeline parallelism, two different behaviours are observed. 1. When tensor parallelism (tp) = 1 and pipeline parallelism (pp) = {2,4}, the script fails and CUDA device side assertions are triggered.  Tested on 1 node with 2,4 GPUs.   2. When tp={2} and pp={2,4} on 12 nodes, the script hangs and does not return anything.  `NCCL_DEBUG` does not throw any errors.  **To Reproduce** PP and TP are modified manually with arguments to `initialize_distributed()`. Script run as  `srun python u run_simple_mcore_train_loop.py` **Expected behavior** The example runs without throwing any errors. **Stack trace/logs** Truncated stack trace for case 1:  ``` ... WARNING:megatron.core.datasets.gpt_dataset:Unable to save the MockGPTDataset indexes because path_to_cache is None INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 1066985 INFO:megatron.core.datasets.gpt_dataset:> total number of epochs: 1 ../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [22,0,0], thread: [64,0,0] Assertion `sizes[i]  [rank0]:     losses_reduced = forward_backward_func( [rank0]:   File ""/scratch/sd/u/user/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1271, in forward_backward_pipelining_without_interleaving [rank0]:     output_tensor, num_tokens = forward_step( [rank0]:   File ""/scratch/sd/u/user/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 206, in forward_step [rank0]:     output_tensor, loss_func = forward_step_func(data_iterator, model) [rank0]:   File ""/scratch/sd/u/user/MegatronLM/examples/run_simple_mcore_train_loop.py"", line 110, in forward_step_func [rank0]:     output_tensor = model(tokens, position_ids, attention_mask, [rank0]:   File ""/scratch/sd/u/user/torch/env2/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1532, in _wrapped_call_impl [rank0]:     return self._call_impl(*args, **kwargs) [rank0]:   File ""/scratch/sd/u/user/torch/env2/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1541, in _call_impl [rank0]:     return forward_call(*args, **kwargs) [rank0]:   File ""/scratch/sd/u/user/MegatronLM/megatron/core/models/gpt/gpt_model.py"", line 175, in forward [rank0]:     decoder_input = self.embedding(input_ids=input_ids, position_ids=position_ids) [rank0]:   File ""/scratch/sd/u/user/torch/env2/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1532, in _wrapped_call_impl [rank0]:     return self._call_impl(*args, **kwargs) [rank0]:   File ""/scratch/sd/u/user/torch/env2/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1541, in _call_impl [rank0]:     return forward_call(*args, **kwargs) [rank0]:   File ""/scratch/sd/u/user/MegatronLM/megatron/core/models/common/embeddings/language_model_embedding.py"", line 100, in forward [rank0]:     word_embeddings = self.word_embeddings(input_ids) [rank0]:   File ""/scratch/sd/u/user/torch/env2/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1532, in _wrapped_call_impl [rank0]:     return self._call_impl(*args, **kwargs) [rank0]:   File ""/scratch/sd/u/user/torch/env2/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1541, in _call_impl [rank0]:     return forward_call(*args, **kwargs) [rank0]:   File ""/scratch/sd/u/user/MegatronLM/megatron/core/tensor_parallel/layers.py"", line 229, in forward [rank0]:     output_parallel = self.weight[masked_input] [rank0]: RuntimeError: CUDA error: deviceside assert triggered [rank0]: Compile with `TORCH_USE_CUDA_DSA` to enable deviceside assertions. ```  Stderr for Case 2: ``` ... WARNING:megatron.core.datasets.gpt_dataset:Unable to save the MockGPTDataset indexes because path_to_cache is None INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 1066985 INFO:megatron.core.datasets.gpt_dataset:> total number of epochs: 1 srun: Job step aborted: Waiting up to 32 seconds for job step to finish. ``` **Environment (please complete the following information):**   MegatronLM commit ID: e33c8f7   PyTorch version: `2.5.0a0+git153362f`   CUDA version: 12.2   NCCL version: 2.19.4 **Proposed fix** N/A **Additional context** For the case where the hangs were observed, the script seems to run into a problem right around the pipeline warmup phase and before 1F1B schedule in `forward_backward_pipelining_without_interleaving()` in `megatron/core/pipeline_parallel/schedules.py`. Any resolution would be greatly appreciated! ",2024-06-20T16:05:32Z,stale,open,0,4,https://github.com/NVIDIA/Megatron-LM/issues/881,"No, `srun` was used to launch the example.  The amount of resources are controlled by `sbatch` during submission and read accordingly by modifying `initialize_distributed()` in the example script.  An excerpt for reference:  ```     rank = int(os.getenv(""SLURM_PROCID""))     local_rank = int(os.getenv(""SLURM_LOCALID""))     world_size = int(os.getenv(""SLURM_NTASKS""))     address = os.getenv(""SLURM_LAUNCH_NODE_IPADDR"")     port = ""29500""     os.environ[""MASTER_ADDR""] = address     os.environ[""MASTER_PORT""] = port     torch.cuda.set_device(local_rank)     torch.distributed.init_process_group(backend=""nccl"", init_method=""env://"",                                          world_size=world_size, rank=rank) ``` The indexing errors have been fixed locally, however the hangs still remain when TP=1, PP=2 and 1 node with 2 GPUs are allocated to the job.","maybe your tokenizer has wrong config, you should check the vocab size is ok both on tokenizer and megatron config. Usually vocab_size is larger in mcore",I also noticed same behavior when Pipeline parallelism is enabled.,Marking as stale. No activity in 60 days.
Edenzzzz,[BUG] @jit_fuser fails with Unknown type constructor Sequence,"**Describe the bug** Using torch 2.1.1, running bash examples/bert/train_bert_340m_distributed.sh produces JIT error due to the Sequence annotator in `calculate_logits_max` ``` return torch.jit.script(fn, _rcb=rcb)  File ""/root/sharedDisk/home/tanwenxuan/miniconda3/lib/python3.8/sitepackages/torch/jit/_script.py"", line 1381, in script   File ""/root/sharedDisk/home/tanwenxuan/miniconda3/lib/python3.8/sitepackages/torch/jit/_script.py"", line 1381, in script     return torch.jit.script(fn, _rcb=rcb)     return torch.jit.script(fn, _rcb=rcb)  File ""/root/sharedDisk/home/tanwenxuan/miniconda3/lib/python3.8/sitepackages/torch/jit/_script.py"", line 1381, in script return torch.jit.script(fn, _rcb=rcb)  File ""/root/sharedDisk/home/tanwenxuan/miniconda3/lib/python3.8/sitepackages/torch/jit/_script.py"", line 1381, in script   File ""/root/sharedDisk/home/tanwenxuan/miniconda3/lib/python3.8/sitepackages/torch/jit/_script.py"", line 1381, in script     fn = torch._C._jit_script_compile( RuntimeError:  Unknown type constructor Sequence:   File ""/root/sharedDisk/home/tanwenxuan/MegatronLM/megatron/core/tensor_parallel/utils.py"", line 106     def vocab_range_from_per_partition_vocab_size(         per_partition_vocab_size: int, rank, world_size: int     ) > Sequence[int]:          ~~~~~~~~~~~~~ < HERE         index_f = rank * per_partition_vocab_size         index_l = index_f + per_partition_vocab_size 'vocab_range_from_per_partition_vocab_size' is being compiled since it was called from 'calculate_predicted_logits'   File ""/root/sharedDisk/home/tanwenxuan/MegatronLM/megatron/core/tensor_parallel/cross_entropy.py"", line 41          Get the partition's vocab indices         get_vocab_range = VocabUtility.vocab_range_from_per_partition_vocab_size         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ < HERE         partition_vocab_size = vocab_parallel_logits.size()[1]         rank = get_tensor_model_parallel_rank() 'calculate_predicted_logits' is being compiled since it was called from 'calculate_predicted_logits' ``` **To Reproduce** bash examples/bert/train_bert_340m_distributed.sh **Expected behavior** **Stack trace/logs** **Environment (please complete the following information):**   MegatronLM e33c8f78a35765d5aa37475a144da60e8a2349d1   PyTorch 2.1.1   CUDA 12.1 **Proposed fix** Disable jit if this occurs **Additional context** Add any other context about the problem here.",2024-06-20T12:03:57Z,stale,open,3,7,https://github.com/NVIDIA/Megatron-LM/issues/880,This is very weird as TorchScipt explicitly forbids using `Sequence` annotator https://pytorch.org/docs/stable/jit_language_reference.htmlsupportedtype I also ran with NGC docker with torch 2.1.0; didn't work,Can confirm I get the same error using   PyTorch 2.3.1  MegatronLM https://github.com/NVIDIA/MegatronLM/commit/e33c8f78a35765d5aa37475a144da60e8a2349d1  CUDA 12.1,"> Can confirm I get the same error using >  > * PyTorch 2.3.1 > * MegatronLM e33c8f7 > * CUDA 12.1 While they didn't say this, using the newest NVIDIA PyTorch container (torch 2.4) seems to work",Thanks for pointing this out. We will have a fix shortly.,"When I was processing the training dataset for GPT using the `tools/preprocess_data.py` script, I encountered this issue. 'vocab_range_from_per_partition_vocab_size' is being compiled since it was called from 'calculate_predicted_logits'","python tools/preprocess_data.py \        input data/oscar1GB.jsonl \        outputprefix mygpt3 \        vocabfile gpt2vocab.json \        tokenizertype GPT2BPETokenizer \        mergefile gpt2merges.txt \        appendeod Traceback (most recent call last):   File ""/mnt/users/lihai/gpt3/code/MegatronLM/tools/preprocess_data.py"", line 23, in      from megatron.training.tokenizer import build_tokenizer   File ""/mnt/users/lihai/gpt3/code/MegatronLM/megatron/training/__init__.py"", line 16, in      from .initialize  import initialize_megatron   File ""/mnt/users/lihai/gpt3/code/MegatronLM/megatron/training/initialize.py"", line 18, in      from megatron.training.arguments import parse_args, validate_args   File ""/mnt/users/lihai/gpt3/code/MegatronLM/megatron/training/arguments.py"", line 14, in      from megatron.core.models.retro.utils import (   File ""/mnt/users/lihai/gpt3/code/MegatronLM/megatron/core/models/retro/__init__.py"", line 12, in      from .decoder_spec import get_retro_decoder_block_spec   File ""/mnt/users/lihai/gpt3/code/MegatronLM/megatron/core/models/retro/decoder_spec.py"", line 9, in      from megatron.core.models.gpt.gpt_layer_specs import (   File ""/mnt/users/lihai/gpt3/code/MegatronLM/megatron/core/models/gpt/__init__.py"", line 1, in      from .gpt_model import GPTModel   File ""/mnt/users/lihai/gpt3/code/MegatronLM/megatron/core/models/gpt/gpt_model.py"", line 13, in      from megatron.core.models.common.language_module.language_module import LanguageModule   File ""/mnt/users/lihai/gpt3/code/MegatronLM/megatron/core/models/common/language_module/language_module.py"", line 9, in      from megatron.core.fusions.fused_cross_entropy import fused_vocab_parallel_cross_entropy   File ""/mnt/users/lihai/gpt3/code/MegatronLM/megatron/core/fusions/fused_cross_entropy.py"", line 27, in      def calculate_predicted_logits(   File ""/mnt/users/lihai/miniconda3/envs/gpt3/lib/python3.9/sitepackages/torch/jit/_script.py"", line 1381, in script     fn = torch._C._jit_script_compile(   File ""/mnt/users/lihai/miniconda3/envs/gpt3/lib/python3.9/sitepackages/torch/jit/_recursive.py"", line 1010, in try_compile_fn     return torch.jit.script(fn, _rcb=rcb)   File ""/mnt/users/lihai/miniconda3/envs/gpt3/lib/python3.9/sitepackages/torch/jit/_script.py"", line 1381, in script     fn = torch._C._jit_script_compile(   File ""/mnt/users/lihai/miniconda3/envs/gpt3/lib/python3.9/sitepackages/torch/jit/_recursive.py"", line 1010, in try_compile_fn     return torch.jit.script(fn, _rcb=rcb)   File ""/mnt/users/lihai/miniconda3/envs/gpt3/lib/python3.9/sitepackages/torch/jit/_script.py"", line 1381, in script     fn = torch._C._jit_script_compile( RuntimeError: Unknown type constructor Sequence:   File ""/mnt/users/lihai/gpt3/code/MegatronLM/megatron/core/tensor_parallel/utils.py"", line 106     def vocab_range_from_per_partition_vocab_size(         per_partition_vocab_size: int, rank, world_size: int     ) > Sequence[int]:          ~~~~~~~~~~~~~  Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:     (     ~         target_mask,         ~~~~~~~~~~~~         masked_target_1d,         ~~~~~~~~~~~~~~~~~         predicted_logits,         ~~~~~~~~~~~~~~~~~         sum_exp_logits,         ~~~~~~~~~~~~~~~         exp_logits,         ~~~~~~~~~~~     ) = VocabParallelCrossEntropy.calculate_predicted_logits(     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~         vocab_parallel_logits, target, logits_max         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ < HERE     )",Marking as stale. No activity in 60 days.
jambo6,[QUESTION] --overlap-grad-allreduce failing as gradients coming through as None in param hook,"When I set `overlapgradallreduce` my run fails because gradients are None inside the hook. It then fails due to this code  ```                 if self.ddp_config.overlap_grad_reduce:                     assert (                         param.grad is not None                     ), 'param.grad being None is not safe when overlap_grad_reduce is True' ``` Gradients are available in the optimizer step, so its not that I'm just not computing gradients. When I disable overlap I also find that every gradient is None inside the backwards hook. ",2024-06-19T10:40:21Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/879,Issue on my end,What was the issue? Might be useful for other users.,May I follow up if there is any way to solve this issue? I am getting this same error 
zhaoyz1017,[QUESTION] OSError: [Errno 28] No space left on device,"when i was running: * `bash examples/pretrain_gpt_distributed.sh` It reports : `INFO:megatron.core.datasets.gpt_dataset:        Load the sample index from 9e7c6a2dd63142a411059e0b84109cddGPTDatasetsample_index.npy INFO:megatron.core.datasets.gpt_dataset:        Load the shuffle index from 9e7c6a2dd63142a411059e0b84109cddGPTDatasetshuffle_index.npy INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 640  finished creating GPT datasets ... ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm). ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm). ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm). Traceback (most recent call last):   File ""/home/zyz/code/MegatronLMcore_v0.6.0/pretrain_gpt.py"", line 224, in      Traceback (most recent call last): pretrain(train_valid_test_datasets_provider,  File ""/home/zyz/code/MegatronLMcore_v0.6.0/pretrain_gpt.py"", line 224, in    File ""/home/zyz/code/MegatronLMcore_v0.6.0/megatron/training/training.py"", line 250, in pretrain Traceback (most recent call last):   File ""/home/zyz/code/MegatronLMcore_v0.6.0/pretrain_gpt.py"", line 224, in      = build_train_valid_test_data_iterators(   File ""/home/zyz/code/MegatronLMcore_v0.6.0/megatron/training/training.py"", line 1415, in build_train_valid_test_data_iterators     pretrain(train_valid_test_datasets_provider,   File ""/home/zyz/code/MegatronLMcore_v0.6.0/megatron/training/training.py"", line 250, in pretrain     = build_train_valid_test_data_iterators(   File ""/home/zyz/code/MegatronLMcore_v0.6.0/megatron/training/training.py"", line 1415, in build_train_valid_test_data_iterators         valid_data_iterator = _get_iterator(dl_type, valid_dataloader)pretrain(train_valid_test_datasets_provider,   File ""/home/zyz/code/MegatronLMcore_v0.6.0/megatron/training/training.py"", line 1400, in _get_iterator   File ""/home/zyz/code/MegatronLMcore_v0.6.0/megatron/training/training.py"", line 250, in pretrain     valid_data_iterator = _get_iterator(dl_type, valid_dataloader)   File ""/home/zyz/code/MegatronLMcore_v0.6.0/megatron/training/training.py"", line 1400, in _get_iterator     return iter(dataloader)   File ""/usr/local/lib/python3.10/distpackages/torch/utils/data/dataloader.py"", line 436, in __iter__     = build_train_valid_test_data_iterators(   File ""/home/zyz/code/MegatronLMcore_v0.6.0/megatron/training/training.py"", line 1415, in build_train_valid_test_data_iterators     return iter(dataloader)   File ""/usr/local/lib/python3.10/distpackages/torch/utils/data/dataloader.py"", line 436, in __iter__         self._iterator = self._get_iterator()self._iterator = self._get_iterator()   File ""/usr/local/lib/python3.10/distpackages/torch/utils/data/dataloader.py"", line 388, in _get_iterator   File ""/usr/local/lib/python3.10/distpackages/torch/utils/data/dataloader.py"", line 388, in _get_iterator     return _MultiProcessingDataLoaderIter(self)     return _MultiProcessingDataLoaderIter(self)   File ""/usr/local/lib/python3.10/distpackages/torch/utils/data/dataloader.py"", line 1024, in __init__   File ""/usr/local/lib/python3.10/distpackages/torch/utils/data/dataloader.py"", line 1024, in __init__     valid_data_iterator = _get_iterator(dl_type, valid_dataloader)   File ""/home/zyz/code/MegatronLMcore_v0.6.0/megatron/training/training.py"", line 1400, in _get_iterator         index_queue = multiprocessing_context.Queue()   type: ignore[varannotated]index_queue = multiprocessing_context.Queue()   type: ignore[varannotated]   File ""/usr/lib/python3.10/multiprocessing/context.py"", line 103, in Queue   File ""/usr/lib/python3.10/multiprocessing/context.py"", line 103, in Queue         return Queue(maxsize, ctx=self.get_context())return Queue(maxsize, ctx=self.get_context())   File ""/usr/lib/python3.10/multiprocessing/queues.py"", line 43, in __init__   File ""/usr/lib/python3.10/multiprocessing/queues.py"", line 43, in __init__         self._rlock = ctx.Lock()self._rlock = ctx.Lock()   File ""/usr/lib/python3.10/multiprocessing/context.py"", line 68, in Lock   File ""/usr/lib/python3.10/multiprocessing/context.py"", line 68, in Lock     return Lock(ctx=self.get_context())       File ""/usr/lib/python3.10/multiprocessing/synchronize.py"", line 162, in __init__ return Lock(ctx=self.get_context())   File ""/usr/lib/python3.10/multiprocessing/synchronize.py"", line 162, in __init__     SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)   File ""/usr/lib/python3.10/multiprocessing/synchronize.py"", line 57, in __init__     SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)   File ""/usr/lib/python3.10/multiprocessing/synchronize.py"", line 57, in __init__     sl = self._semlock = _multiprocessing.SemLock(     sl = self._semlock = _multiprocessing.SemLock( OSError: [Errno 28] No space left on device     return iter(dataloader)OSError [Errno 28] No space left on device   File ""/usr/local/lib/python3.10/distpackages/torch/utils/data/dataloader.py"", line 436, in __iter__     self._iterator = self._get_iterator()   File ""/usr/local/lib/python3.10/distpackages/torch/utils/data/dataloader.py"", line 388, in _get_iterator     return _MultiProcessingDataLoaderIter(self)   File ""/usr/local/lib/python3.10/distpackages/torch/utils/data/dataloader.py"", line 1024, in __init__     index_queue = multiprocessing_context.Queue()   type: ignore[varannotated]   File ""/usr/lib/python3.10/multiprocessing/context.py"", line 103, in Queue     return Queue(maxsize, ctx=self.get_context())   File ""/usr/lib/python3.10/multiprocessing/queues.py"", line 43, in __init__     self._rlock = ctx.Lock()   File ""/usr/lib/python3.10/multiprocessing/context.py"", line 68, in Lock     return Lock(ctx=self.get_context())   File ""/usr/lib/python3.10/multiprocessing/synchronize.py"", line 162, in __init__     SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)   File ""/usr/lib/python3.10/multiprocessing/synchronize.py"", line 57, in __init__     sl = self._semlock = _multiprocessing.SemLock( OSError: [Errno 28] No space left on device Traceback (most recent call last):   File ""/home/zyz/code/MegatronLMcore_v0.6.0/pretrain_gpt.py"", line 224, in      pretrain(train_valid_test_datasets_provider,   File ""/home/zyz/code/MegatronLMcore_v0.6.0/megatron/training/training.py"", line 250, in pretrain     = build_train_valid_test_data_iterators(   File ""/home/zyz/code/MegatronLMcore_v0.6.0/megatron/training/training.py"", line 1420, in build_train_valid_test_data_iterators     test_data_iterator = _get_iterator(dl_type, test_dataloader)   File ""/home/zyz/code/MegatronLMcore_v0.6.0/megatron/training/training.py"", line 1400, in _get_iterator     return iter(dataloader)   File ""/usr/local/lib/python3.10/distpackages/torch/utils/data/dataloader.py"", line 436, in __iter__     self._iterator = self._get_iterator()   File ""/usr/local/lib/python3.10/distpackages/torch/utils/data/dataloader.py"", line 388, in _get_iterator     return _MultiProcessingDataLoaderIter(self)   File ""/usr/local/lib/python3.10/distpackages/torch/utils/data/dataloader.py"", line 1015, in __init__     self._worker_result_queue = multiprocessing_context.Queue()   type: ignore[varannotated]   File ""/usr/lib/python3.10/multiprocessing/context.py"", line 103, in Queue     return Queue(maxsize, ctx=self.get_context())   File ""/usr/lib/python3.10/multiprocessing/queues.py"", line 43, in __init__     self._rlock = ctx.Lock()   File ""/usr/lib/python3.10/multiprocessing/context.py"", line 68, in Lock     return Lock(ctx=self.get_context())   File ""/usr/lib/python3.10/multiprocessing/synchronize.py"", line 162, in __init__     SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)   File ""/usr/lib/python3.10/multiprocessing/synchronize.py"", line 57, in __init__     sl = self._semlock = _multiprocessing.SemLock( OSError: [Errno 28] No space left on device ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm). ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm). Exception ignored in atexit callback:  Traceback (most recent call last):   File ""/usr/local/lib/python3.10/distpackages/torch/utils/data/dataloader.py"", line 1475, in _clean_up_worker     w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)   File ""/usr/lib/python3.10/multiprocessing/process.py"", line 149, in join     res = self._popen.wait(timeout)   File ""/usr/lib/python3.10/multiprocessing/popen_fork.py"", line 40, in wait     if not wait([self.sentinel], timeout):   File ""/usr/lib/python3.10/multiprocessing/connection.py"", line 931, in wait     ready = selector.select(timeout)   File ""/usr/lib/python3.10/selectors.py"", line 416, in select     fd_event_list = self._selector.poll(timeout)   File ""/usr/local/lib/python3.10/distpackages/torch/utils/data/_utils/signal_handling.py"", line 66, in handler     _error_if_any_worker_fails() RuntimeError: DataLoader worker (pid 56490) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit. Exception ignored in atexit callback:  Traceback (most recent call last):   File ""/usr/local/lib/python3.10/distpackages/torch/utils/data/dataloader.py"", line 1475, in _clean_up_worker     w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)   File ""/usr/lib/python3.10/multiprocessing/process.py"", line 149, in join     res = self._popen.wait(timeout)   File ""/usr/lib/python3.10/multiprocessing/popen_fork.py"", line 40, in wait     if not wait([self.sentinel], timeout):   File ""/usr/lib/python3.10/multiprocessing/connection.py"", line 931, in wait     ready = selector.select(timeout)   File ""/usr/lib/python3.10/selectors.py"", line 416, in select     fd_event_list = self._selector.poll(timeout)   File ""/usr/local/lib/python3.10/distpackages/torch/utils/data/_utils/signal_handling.py"", line 66, in handler     _error_if_any_worker_fails() RuntimeError: DataLoader worker (pid 56811) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit. Exception ignored in atexit callback:  Traceback (most recent call last):   File ""/usr/local/lib/python3.10/distpackages/torch/utils/data/dataloader.py"", line 1475, in _clean_up_worker     w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)   File ""/usr/lib/python3.10/multiprocessing/process.py"", line 149, in join     res = self._popen.wait(timeout)   File ""/usr/lib/python3.10/multiprocessing/popen_fork.py"", line 40, in wait     if not wait([self.sentinel], timeout):   File ""/usr/lib/python3.10/multiprocessing/connection.py"", line 931, in wait     ready = selector.select(timeout)   File ""/usr/lib/python3.10/selectors.py"", line 416, in select     fd_event_list = self._selector.poll(timeout)   File ""/usr/local/lib/python3.10/distpackages/torch/utils/data/_utils/signal_handling.py"", line 66, in handler     _error_if_any_worker_fails() RuntimeError: DataLoader worker (pid 57242) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit. Exception in thread Thread3 (_pin_memory_loop): Traceback (most recent call last):   File ""/usr/lib/python3.10/threading.py"", line 1016, in _bootstrap_inner Exception ignored in atexit callback:  Traceback (most recent call last):   File ""/usr/local/lib/python3.10/distpackages/torch/utils/data/dataloader.py"", line 1475, in _clean_up_worker         self.run()w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)   File ""/usr/lib/python3.10/threading.py"", line 953, in run   File ""/usr/lib/python3.10/multiprocessing/process.py"", line 149, in join     res = self._popen.wait(timeout)   File ""/usr/lib/python3.10/multiprocessing/popen_fork.py"", line 40, in wait         if not wait([self.sentinel], timeout):   File ""/usr/lib/python3.10/multiprocessing/connection.py"", line 931, in wait self._target(*self._args, **self._kwargs)     ready = selector.select(timeout)  File ""/usr/local/lib/python3.10/distpackages/torch/utils/data/_utils/pin_memory.py"", line 54, in _pin_memory_loop   File ""/usr/lib/python3.10/selectors.py"", line 416, in select     fd_event_list = self._selector.poll(timeout)   File ""/usr/local/lib/python3.10/distpackages/torch/utils/data/_utils/signal_handling.py"", line 66, in handler     do_one_step()       File ""/usr/local/lib/python3.10/distpackages/torch/utils/data/_utils/pin_memory.py"", line 31, in do_one_step _error_if_any_worker_fails() RuntimeError: DataLoader worker (pid 56434) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.     r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)   File ""/usr/lib/python3.10/multiprocessing/queues.py"", line 122, in get     return _ForkingPickler.loads(res)   File ""/usr/local/lib/python3.10/distpackages/torch/multiprocessing/reductions.py"", line 316, in rebuild_storage_fd     fd = df.detach()   File ""/usr/lib/python3.10/multiprocessing/resource_sharer.py"", line 57, in detach     with _resource_sharer.get_connection(self._id) as conn:   File ""/usr/lib/python3.10/multiprocessing/resource_sharer.py"", line 86, in get_connection     c = Client(address, authkey=process.current_process().authkey)   File ""/usr/lib/python3.10/multiprocessing/connection.py"", line 508, in Client     answer_challenge(c, authkey)   File ""/usr/lib/python3.10/multiprocessing/connection.py"", line 752, in answer_challenge     message = connection.recv_bytes(256)          reject large message   File ""/usr/lib/python3.10/multiprocessing/connection.py"", line 216, in recv_bytes     buf = self._recv_bytes(maxlength)   File ""/usr/lib/python3.10/multiprocessing/connection.py"", line 414, in _recv_bytes     buf = self._recv(4)   File ""/usr/lib/python3.10/multiprocessing/connection.py"", line 379, in _recv     chunk = read(handle, remaining) ConnectionResetError: [Errno 104] Connection reset by peer [20240619 08:07:08,726] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 54798 closing signal SIGTERM [20240619 08:07:08,726] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 54800 closing signal SIGTERM [20240619 08:07:08,791] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 1 (pid: 54799) of binary: /usr/bin/python Traceback (most recent call last):   File ""/usr/local/bin/torchrun"", line 33, in      sys.exit(load_entry_point('torch==2.1.0a0+29c30b1', 'console_scripts', 'torchrun')())   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper     return f(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/run.py"", line 797, in main     run(args)   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/run.py"", line 788, in run     elastic_launch(   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/launcher/api.py"", line 134, in __call__     return launch_agent(self._config, self._entrypoint, list(args))   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/launcher/api.py"", line 264, in launch_agent     raise ChildFailedError( torch.distributed.elastic.multiprocessing.errors.ChildFailedError:   I am not sure about what cause this Error and how to fix this problem",2024-06-19T08:47:06Z,,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/878,Are you running this in a docker container? What command if so?,"> Are you running this in a docker container? What command if so? yes, I run it in docker.  `docker run it name zhaomegatron v /jfs/yuzhe.zhao:/home/zyz gpus all nvcr.io/nvidia/pytorch:23.09py3`","> Are you running this in a docker container? What command if so? Thanks,  I think this error is about docker container's shared memory. I fixed that with `docker run shmsize=""64g"" `",I believe `ipc=host` should also work.
Genlovy-Hoo,"[QUESTION] Gloo connectFullMesh failed when the number of nodes setting ""export GLOO_SOCKET_IFNAME=bond4"" exceeds 60","When we train models on a multinode cluster, it will raise ""RuntimeError: Gloo connectFullMesh failed ..."" if the number of nodes setting ""export GLOO_SOCKET_IFNAME=bond4"" exceeds 60, such as 64. And it works when the ""bond4nodes"" is less than or equal to 60.  Are there any restrictions for using the Gloo backend with the bond4 network configuration during training?",2024-06-19T08:17:09Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/877
shanyuaa,[QUESTION]when pretraining bertï¼Œmeet bugï¼šcuBLAS Error: the requested functionality is not supported,**Your question** Ask a clear and concise question about MegatronLM. my train_bert_340m_distributed.sh(specialize part) is as follows: CHECKPOINT_PATH=/proj/bert/checkpoints/null/ TENSORBOARD_LOGS_PATH=/proj/bert/logs VOCAB_FILE=/proj/bert/checkpoints/bertlargeuncasedvocab.txt DATA_PATH=/proj/bert/dataset/ag_news_text_sentence the error is: ,2024-06-18T14:33:03Z,,closed,0,5,https://github.com/NVIDIA/Megatron-LM/issues/876,"also encountered this issue, were you able to resolve the cuBLAS error?",ä½ å¥½ï¼Œæˆ‘å·²æ”¶åˆ°ä¿¡ä»¶ï¼Œåœ¨é˜…è¯»åŽä¼šå°½å¿«ç»™å›žå¤ã€‚ç¥å¥½ã€‚,This https://github.com/NVIDIA/TransformerEngine/pull/845 may help. Try to install TE after this commit.,Marking as stale. No activity in 60 days.,ä½ å¥½ï¼Œæˆ‘å·²æ”¶åˆ°ä¿¡ä»¶ï¼Œåœ¨é˜…è¯»åŽä¼šå°½å¿«ç»™å›žå¤ã€‚ç¥å¥½ã€‚
janelu9,[BUG] RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.,"```  File ""/mnt/e/nlp/MegatronLM/megatron/legacy/model/transformer.py"", line 697, in forward [rank0]:     query_layer = query_layer.view(query_layer.size(0), query_layer.size(1), 1, self.hidden_size_per_attention_head) [rank0]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [rank0]: RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead. ```",2024-06-18T08:54:52Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/875,https://github.com/NVIDIA/MegatronLM/blob/e33c8f78a35765d5aa37475a144da60e8a2349d1/megatron/legacy/model/transformer.pyL697,Marking as stale. No activity in 60 days.
JimmyZhang12,draft: Bert context parallelism support,,2024-06-18T01:16:34Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/874
qby10,[BUG] pipeline_paralle is not available when pp_size > 2,"This func is wrong, the program will hang because of the ""group"" variable. def _batched_p2p_ops(     *,     tensor_send_prev: Optional[torch.Tensor],     tensor_recv_prev: Optional[torch.Tensor],     tensor_send_next: Optional[torch.Tensor],     tensor_recv_next: Optional[torch.Tensor],     group: torch.distributed.ProcessGroup ) after modified:  def _batched_p2p_ops(     *,     tensor_send_prev: Optional[torch.Tensor],     tensor_recv_prev: Optional[torch.Tensor],     tensor_send_next: Optional[torch.Tensor],     tensor_recv_next: Optional[torch.Tensor] )",2024-06-17T13:48:46Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/873,Please add instructions how to reproduce. ,Marking as stale. No activity in 60 days.
zainsarwar865,[BUG] Rank worldsize mismatch prevents tensorboard from being set,"In the ```global_vars.py``` file, the ```_set_tensorboard_writer``` function only gets set if the ```rank == world size  1```. However, this can be an issue if ranks are local and world size > number of gpus on one node. Thus, I propose setting it to the master node.",2024-06-17T09:25:41Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/872
Weifan1226,[QUESTION] How to time the code,"Hi Megatron team! I wanted to time some of Megatron's code blocks, but didn't find a good way to do it.  But I was  surprised to find code like this:               if config.timers is not None:                       config.timers('layernormgradsallreduce', log_level=1).start(                           barrier=config.barrier_with_L1_time                       )                   _allreduce_layernorm_grads(model, config)                   if config.timers is not None:                       config.timers('layernormgradsallreduce').stop() Can I us config.timers() to time the code?  If yes,  how can i see the output from config.timers. Thanks!",2024-06-16T16:15:11Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/871
windprak,[BUG] megatron.training not found,"**Describe the bug** Module megatron.training not found in latest version of megatron_core 0.8.0rc0   ``` MegatronLM/examples/run_simple_mcore_train_loop.py"", line 18, in      from megatron.training.tokenizer.tokenizer import _NullTokenizer ModuleNotFoundError: No module named 'megatron.training' ``` **To Reproduce** ``` annotatedtypes          0.7.0 apex                     0.1 click                    8.1.7 einops                   0.8.0 filelock                 3.13.1 flash_attn               2.4.2 fsspec                   2024.2.0 Jinja2                   3.1.3 joblib                   1.4.2 MarkupSafe               2.1.5 megatron_core            0.8.0rc0               mpmath                   1.3.0 networkx                 3.2.1 ninja                    1.11.1.1 nltk                     3.8.1 numpy                    1.26.4 nvidiacublascu12       12.4.2.65 nvidiacudacupticu12   12.4.99 nvidiacudanvrtccu12   12.4.99 nvidiacudaruntimecu12 12.4.99 nvidiacudnncu12        9.1.0.70 nvidiacufftcu12        11.2.0.44 nvidiacurandcu12       10.3.5.119 nvidiacusolvercu12     11.6.0.99 nvidiacusparsecu12     12.3.0.142 nvidiancclcu12         2.20.5 nvidianvjitlinkcu12    12.4.99 nvidianvtxcu12         12.4.99 packaging                24.1 pip                      24.0 pydantic                 2.7.4 pydantic_core            2.18.4 pytorchtriton           3.0.0+45fff310c8 regex                    2024.5.15 sentencepiece            0.2.0 setuptools               69.5.1 sympy                    1.12 torch                    2.4.0.dev20240611+cu124 tqdm                     4.66.4 transformer_engine       1.6.0+c81733f typing_extensions        4.8.0 wheel                    0.43.0 ``` ``` git clone https://github.com/NVIDIA/MegatronLM.git cd MegatronLM pip install e . cd examples NUM_GPUS=2 torchrun nprocpernode $NUM_GPUS run_simple_mcore_train_loop.py ``` **Expected behavior** Import without errors",2024-06-14T14:49:55Z,,closed,0,8,https://github.com/NVIDIA/Megatron-LM/issues/870,same problem,"I fixed it adding packages=setuptools.find_namespace_packages(include=[""megatron.core"", ""megatron.core.*"",""megatron.training""]) it to the setup.py But guess what the ""simple"" script still crashes: `[rank1]:   File ""/home/atuin/b216dc/b216dc10/software/private/conda/envs/megatron/lib/python3.10/sitepackages/torch/distributed/checkpoint/default_planner.py"", line 389, in create_default_global_save_plan [rank1]:     assert item.index.fqn not in md [rank1]: AssertionError `",I think if you run it as  ``` PYTHONPATH=$PYTHONPATH:./megatron torchrun nprocpernode 2 examples/run_simple_mcore_train_loop.py ``` it should work. (mentioned in QuickStard.md),"> I fixed it adding packages=setuptools.find_namespace_packages(include=[""megatron.core"", ""megatron.core.*"",""megatron.training""]) it to the setup.py >  > But guess what the ""simple"" script still crashes: `[rank1]: File ""/home/atuin/b216dc/b216dc10/software/private/conda/envs/megatron/lib/python3.10/sitepackages/torch/distributed/checkpoint/default_planner.py"", line 389, in create_default_global_save_plan [rank1]: assert item.index.fqn not in md [rank1]: AssertionError ` I had the same problem, did you solve it?","> > I fixed it adding packages=setuptools.find_namespace_packages(include=[""megatron.core"", ""megatron.core.*"",""megatron.training""]) it to the setup.py > > But guess what the ""simple"" script still crashes: `[rank1]: File ""/home/atuin/b216dc/b216dc10/software/private/conda/envs/megatron/lib/python3.10/sitepackages/torch/distributed/checkpoint/default_planner.py"", line 389, in create_default_global_save_plan [rank1]: assert item.index.fqn not in md [rank1]: AssertionError ` >  > I had the same problem, did you solve it? Use this shared strategy works for me. Env is `nvcr.io/nvidia/pytorch:24.07py3`. ```python sharded_strategy = dist_checkpointing.serialization.get_default_save_sharded_strategy(""zarr"") sharded_state_dict = model.sharded_state_dict(prefix="""") dist_checkpointing.save(     sharded_state_dict=sharded_state_dict,     checkpoint_dir=checkpoint_path,     sharded_strategy=sharded_strategy,     async_sharded_save=False,     ) ```","> > > I fixed it adding packages=setuptools.find_namespace_packages(include=[""megatron.core"", ""megatron.core.*"",""megatron.training""]) it to the setup.py > > > But guess what the ""simple"" script still crashes: `[rank1]: File ""/home/atuin/b216dc/b216dc10/software/private/conda/envs/megatron/lib/python3.10/sitepackages/torch/distributed/checkpoint/default_planner.py"", line 389, in create_default_global_save_plan [rank1]: assert item.index.fqn not in md [rank1]: AssertionError ` > >  > >  > > I had the same problem, did you solve it? >  > Use this shared strategy works for me. Env is `nvcr.io/nvidia/pytorch:24.07py3`. >  > ```python > sharded_strategy = dist_checkpointing.serialization.get_default_save_sharded_strategy(""zarr"") > sharded_state_dict = model.sharded_state_dict(prefix="""") > dist_checkpointing.save( >     sharded_state_dict=sharded_state_dict, >     checkpoint_dir=checkpoint_path, >     sharded_strategy=sharded_strategy, >     async_sharded_save=False, >     ) > ``` Thank you for your help!","I > I fixed it adding packages=setuptools.find_namespace_packages(include=[""megatron.core"", ""megatron.core.*"",""megatron.training""]) it to the setup.py >  > But guess what the ""simple"" script still crashes: `[rank1]: File ""/home/atuin/b216dc/b216dc10/software/private/conda/envs/megatron/lib/python3.10/sitepackages/torch/distributed/checkpoint/default_planner.py"", line 389, in create_default_global_save_plan [rank1]: assert item.index.fqn not in md [rank1]: AssertionError ` I fix this bug in PR CC(fix torch.py when saving checkpoints using pytorch2.4) ",export PYTHONPATH=$PYTHONPATH:/path/to/MegatronLM
wavy-jung,[QUESTION] Question about Mixtral compatibility with Megatron-LM core0.7.0,"If I understand correctly, in the Mixtral model, layer normalization is applied after selfattention, and then these hidden states are forwarded to each expert. In the implementation of the MoE model in MegatronLM, layer normalization is applied before each expert MLP layer. Is this correct?",2024-06-14T14:15:35Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/869
cporrasn,[QUESTION] Using segformer segmentation models,"**Your question** It is possible to use the segmentation models that exist in megatron, I have found a main with the refinement, but I am not clear how a simple inference can be proven. Do these segmentation models use tensor parallelism?",2024-06-14T11:11:00Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/868
janelu9,[ENHANCEMENT]Can we pass a tuple that includs all the tensors I'd like to pass between pipeline's different stages?,It seems I can only pass one output hidden tesnor now,2024-06-14T08:29:22Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/867,We currently don't have this feature in our near term roadmap. Please reopen if needed.
adoda,[BUG] the argument of parser.add_argument is wrong in tools/checkpoint/convert.py,"**Describe the bug** https://github.com/NVIDIA/MegatronLM/blob/main/tools/checkpoint/convert.pyL115 It must be 'choices=['GPT', 'BERT'],'  not 'choice=['GPT', 'BERT'],'",2024-06-14T03:31:49Z,,closed,1,1,https://github.com/NVIDIA/Megatron-LM/issues/866,This should be fixed. Please reopen if needed.
lichenlu,[QUESTION] why the _p2p_ops functions has the condition branches for get_pipeline_model_parallel_rank() ,"**Your question** Ask a clear and concise question about MegatronLM. ** !image ** why the _p2p_ops func has the condition branches to distinguish between get_pipeline_model_parallel_rank() % 2 == 0 and get_pipeline_model_parallel_rank() % 2 != 0 ï¼Ÿ just for different send recv launch order? but send next and recv prev will use different stream,   no dependency",2024-06-14T03:03:51Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/865
Mooler0410,[QUESTION]Mamba-2-hybrid Weights,"**Your question** An Empirical Study of Mambabased Language Models](https://github.com/NVIDIA/MegatronLM/tree/ssm/examples/mamba) Hi! I'm impressed by this work and cannot wait to try the new mamba2hybrid.  This paper mentioned that the weights are released on Huggingface. But I cannot find any. Wondering have they been released? If yes, where can I download them?  Thanks a lot for your folks' contribution to the community!",2024-06-13T19:24:30Z,,closed,6,5,https://github.com/NVIDIA/Megatron-LM/issues/864,I think the model weights are released here: https://huggingface.co/collections/nvidia/ssms666a362c5c3bb7e4a6bcfb9c,"> I think the model weights are released here: https://huggingface.co/collections/nvidia/ssms666a362c5c3bb7e4a6bcfb9c Thanks! I've already found it. While when this question is posted, the weights haven't been set as public. Now, I'm looking for the tokenizerðŸ¤£. To run the example, a tokenizer is required. But I cannot find any. Any idea about this? ","I think the tokenizer path should point to the .model file in the huggingface repos. For example, I downloaded the `mamba2hybrid8b3t4k` repo from huggingface, and `mamba2hybrid8b3t4k/mt_nlg_plus_multilingual_ja_zh_the_stack_frac_015_256k.model` is the tokenizer. I'm running inference using `run_text_gen_server_8b.sh` and the checkpoint/tokenizer paths are ``` CHECKPOINT_PATH=""/workspace/checkpoints/mamba2hybrid8b3t4k/"" TOKENIZER_PATH=""/workspace/checkpoints/mamba2hybrid8b3t4k/mt_nlg_plus_multilingual_ja_zh_the_stack_frac_015_256k.model"" ``` respectively.","Wow, thank you so much for your guidance! It took me hours to find something like a tokenizer.  Never used megatron beforeðŸ™ƒ. You did save my life!!","!/bin/bash CHECKPOINT_PATH=""/workspace/mnt/xxx/models/mamba2hybrid8b3t4k"" TOKENIZER_PATH=""/workspace/mnt/xxx/models/mamba2hybrid8b3t4k/mt_nlg_plus_multilingual_ja_zh_the_stack_frac_015_256k.model"" DISTRIBUTED_ARGS=""nproc_per_node 1 \                   nnodes 1 \                   node_rank 0 \                   master_addr localhost \                   master_port 6608"" export NCCL_IB_SL=1 export CUDA_DEVICE_MAX_CONNECTIONS=1 export NCCL_IB_TIMEOUT=19 export NCCL_IB_QPS_PER_CONNECTION=4 export TRITON_CACHE_DIR=""./tritoncache/"" export TRITON_CACHE_MANAGER=""megatron.core.ssm.triton_cache_manager:ParallelFileCacheManager"" torchrun $DISTRIBUTED_ARGS /workspace/mnt/xxx/MegatronLM/tools/run_mamba_text_generation_server.py \        tensormodelparallelsize 1  \        pipelinemodelparallelsize 1  \        untieembeddingsandoutputweights \        numlayers 56  \        hiddensize 4096  \        load ${CHECKPOINT_PATH}  \        numattentionheads 32  \        groupqueryattention \        numquerygroups 8 \        hybridattentionratio 0.08 \        hybridmlpratio 0.5 \        attentiondropout 0.0 \        hiddendropout 0.0 \        disablebiaslinear \        normalization RMSNorm \        seqlength 4096  \        maxpositionembeddings 4096  \        positionembeddingtype none \        tokenizertype GPTSentencePieceTokenizer  \        tokenizermodel ${TOKENIZER_PATH} \        distributedbackend nccl \        distributedtimeoutminutes 1440 \        bf16  \        microbatchsize 1  \        usemcoremodels \        spec megatron.core.models.mamba.mamba_layer_specs mamba_stack_spec \        seed 42 errorï¼š state = torch.load(state, map_location='cuda') [rank0]: Traceback (most recent call last): [rank0]:   File ""/workspace/mnt/xxx/MegatronLM/tools/run_mamba_text_generation_server.py"", line 101, in  [rank0]:     _ = load_checkpoint(model, None, None) [rank0]:   File ""/workspace/mnt/xxx/MegatronLM/megatron/training/checkpointing.py"", line 839, in load_checkpoint [rank0]:     model[0].load_state_dict(state_dict['model'], strict=strict) [rank0]:   File ""/opt/conda/envs/mamba2/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 2215, in load_state_dict [rank0]:     raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format( [rank0]: RuntimeError: Error(s) in loading state_dict for MambaModel: [rank0]:        Missing key(s) in state_dict: ""decoder.layers.0.mixer.in_proj.layer_norm_weight"", ""decoder.layers.0.mixer.in_proj._extra_state"", ""decoder.layers.0.mixer.out_proj._extra_state"", ""decoder.layers.2.mixer.in_proj.layer_norm_weight"", ""decoder.layers.2.mixer.in_proj._extra_state"", ""decoder.layers.2.mixer.out_proj._extra_state"", ""decoder.layers.4.mixer.in_proj.layer_norm_weight"", ""decoder.layers.4.mixer.in_proj._extra_state"", ""decoder.layers.4.mixer.out_proj._extra_state"", ""decoder.layers.7.mixer.in_proj.layer_norm_weight"", ""decoder.layers.7.mixer.in_proj._extra_state"", ""decoder.layers.7.mixer.out_proj._extra_state"", ""decoder.layers.9.mixer.in_proj.layer_norm_weight"", ""decoder.layers.9.mixer.in_proj._extra_state"", ""decoder.layers.9.mixer.out_proj._extra_state"", ""decoder.layers.12.mixer.in_proj.layer_norm_weight"", ""decoder.layers.12.mixer.in_proj._extra_state"", ""decoder.layers.12.mixer.out_proj._extra_state"", ""decoder.layers.14.mixer.in_proj.layer_norm_weight"", ""decoder.layers.14.mixer.in_proj._extra_state"", ""decoder.layers.14.mixer.out_proj._extra_state"", ""decoder.layers.16.mixer.in_proj.layer_norm_weight"", ""decoder.layers.16.mixer.in_proj._extra_state"", ""decoder.layers.16.mixer.out_proj._extra_state"", ""decoder.layers.18.mixer.in_proj.layer_norm_weight"", ""decoder.layers.18.mixer.in_proj._extra_state"", ""decoder.layers.18.mixer.out_proj._extra_state"", ""decoder.layers.21.mixer.in_proj.layer_norm_weight"", ""decoder.layers.21.mixer.in_proj._extra_state"", ""decoder.layers.21.mixer.out_proj._extra_state"", ""decoder.layers.24.mixer.in_proj.layer_norm_weight"", ""decoder.layers.24.mixer.in_proj._extra_state"", ""decoder.layers.24.mixer.out_proj._extra_state"", ""decoder.layers.26.mixer.in_proj.layer_norm_weight"", ""decoder.layers.26.mixer.in_proj._extra_state"", ""decoder.layers.26.mixer.out_proj._extra_state"", ""decoder.layers.28.mixer.in_proj.layer_norm_weight"", ""decoder.layers.28.mixer.in_proj._extra_state"", ""decoder.layers.28.mixer.out_proj._extra_state"", ""decoder.layers.30.mixer.in_proj.layer_norm_weight"", ""decoder.layers.30.mixer.in_proj._extra_state"", ""decoder.layers.30.mixer.out_proj._extra_state"", ""decoder.layers.32.mixer.in_proj.layer_norm_weight"", ""decoder.layers.32.mixer.in_proj._extra_state"", ""decoder.layers.32.mixer.out_proj._extra_state"", ""decoder.layers.36.mixer.in_proj.layer_norm_weight"", ""decoder.layers.36.mixer.in_proj._extra_state"", ""decoder.layers.36.mixer.out_proj._extra_state"", ""decoder.layers.38.mixer.in_proj.layer_norm_weight"", ""decoder.layers.38.mixer.in_proj._extra_state"", ""decoder.layers.38.mixer.out_proj._extra_state"", ""decoder.layers.40.mixer.in_proj.layer_norm_weight"", ""decoder.layers.40.mixer.in_proj._extra_state"", ""decoder.layers.40.mixer.out_proj._extra_state"", ""decoder.layers.42.mixer.in_proj.layer_norm_weight"", ""decoder.layers.42.mixer.in_proj._extra_state"", ""decoder.layers.42.mixer.out_proj._extra_state"", ""decoder.layers.44.mixer.in_proj.layer_norm_weight"", ""decoder.layers.44.mixer.in_proj._extra_state"", ""decoder.layers.44.mixer.out_proj._extra_state"", ""decoder.layers.47.mixer.in_proj.layer_norm_weight"", ""decoder.layers.47.mixer.in_proj._extra_state"", ""decoder.layers.47.mixer.out_proj._extra_state"", ""decoder.layers.50.mixer.in_proj.layer_norm_weight"", ""decoder.layers.50.mixer.in_proj._extra_state"", ""decoder.layers.50.mixer.out_proj._extra_state"", ""decoder.layers.52.mixer.in_proj.layer_norm_weight"", ""decoder.layers.52.mixer.in_proj._extra_state"", ""decoder.layers.52.mixer.out_proj._extra_state"", ""decoder.layers.54.mixer.in_proj.layer_norm_weight"", ""decoder.layers.54.mixer.in_proj._extra_state"", ""decoder.layers.54.mixer.out_proj._extra_state"".  [rank0]:        Unexpected key(s) in state_dict: ""decoder.layers.0.norm.weight"", ""decoder.layers.2.norm.weight"", ""decoder.layers.4.norm.weight"", ""decoder.layers.7.norm.weight"", ""decoder.layers.9.norm.weight"", ""decoder.layers.12.norm.weight"", ""decoder.layers.14.norm.weight"", ""decoder.layers.16.norm.weight"", ""decoder.layers.18.norm.weight"", ""decoder.layers.21.norm.weight"", ""decoder.layers.24.norm.weight"", ""decoder.layers.26.norm.weight"", ""decoder.layers.28.norm.weight"", ""decoder.layers.30.norm.weight"", ""decoder.layers.32.norm.weight"", ""decoder.layers.36.norm.weight"", ""decoder.layers.38.norm.weight"", ""decoder.layers.40.norm.weight"", ""decoder.layers.42.norm.weight"", ""decoder.layers.44.norm.weight"", ""decoder.layers.47.norm.weight"", ""decoder.layers.50.norm.weight"", ""decoder.layers.52.norm.weight"", ""decoder.layers.54.norm.weight"".  E0110 10:25:42.590000 140450964977472 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 201340) of binary: /opt/conda/envs/mamba2/bin/python3.10 why errorï¼Ÿ"
Andy666G,Fix(memory optimization): inplace subtract vocab_parallel_logits,"vocab_parallel_logits's shape is [seq_len, batch_size, vocab_size / tp]. if vocab_size is very large like Llama3, use inplace subtract to reduce memory usage.",2024-06-13T08:28:12Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/863,Marking as stale. No activity in 60 days.
janelu9,[QUESTION] Loss increased by 10 times at second step (after one step of backward). ,why? !image,2024-06-13T06:47:32Z,,closed,0,17,https://github.com/NVIDIA/Megatron-LM/issues/862,So magical. I guess there may be something wrong with the optimizer,Hello~ï¼ŒI might have encountered the same issue. What is your problem background? Pretrain or further pretrain? Llama Family? What is the loss in the following step?,"> Hello~ï¼ŒI might have encountered the same issue. What is your problem background? Pretrain or further pretrain? Llama Family? What is the loss in the following step? loss decreased slowly in following steps. I load the llama38b's weights by my custom method , it seemed the model loaded weights sucessful , but model's weights were all  reseted randomlly at second step I think.","Did you ever train llama2 or 3 for enough time? What is your loss now? Would the loss decrease to near zero, which is happening to me.","> Did you ever train llama2 or 3 for enough time? What is your loss now? Would the loss decrease to near zero, which is happening to me. No.  When I trained llama3 by other engine, the loss never increased so much on the same dataset. Maybe some bugs in my custom method",Probably we all need someone more professional to tackle it.,> Probably we all need someone more professional to tackle it. I really  believe that the params are reseted randomly at the second iteration.,"I further pretrain llama2 for some steps in NvidiaGPU, which is fine, but encoutered the similar issue in other type GPU.","> I further pretrain llama2 for some steps in NvidiaGPU, which is fine, but encoutered the similar issue in other type GPU. I set learning rate very small as 1e9, the loss still increased exponentially at scend iteration that confirmed my guess","> I further pretrain llama2 for some steps in NvidiaGPU, which is fine, but encoutered the similar issue in other type GPU. Well, I have solved my problem by loading the pretrained weights before the optimizer is built",Can you tell me the details such as the line of code you change. Very appreciate that !!,"> Can you tell me the details such as the line of code you change. Very appreciate that !! ``` def setup_model_and_optimizer(model_provider_func,                               model_type,                               no_wd_decay_cond=None,                               scale_lr_cond=None,                               lr_mult=1.0):     args = get_args()     timers = get_timers()     model = get_model(model_provider_func, model_type)     if args.load_hf_model:          load_hf_model(model,args.model,args.cache_model)         if args.only_cache_model:             sys.exit()     unwrapped_model = unwrap_model(model)     kwargs = {}     for f in dataclasses.fields(OptimizerConfig):         if hasattr(args, f.name):             kwargs[f.name] = getattr(args, f.name)     config = OptimizerConfig(**kwargs)     config.timers = timers     optimizer = get_megatron_optimizer(config, model, no_wd_decay_cond,                                        scale_lr_cond, lr_mult)     opt_param_scheduler = get_optimizer_param_scheduler(optimizer)     ... ``` I modified  `setup_model_and_optimizer` in `megatron.training.training` like the codes commented out , `load_hf_model` is my custom weights loading method who can load weights from huggingface directelly.","```     if args.load is not None or args.pretrained_checkpoint is not None:         timers('loadcheckpoint', log_level=0).start(barrier=True)         args.iteration, args.num_floating_point_operations_so_far = load_checkpoint(             model, optimizer, opt_param_scheduler)         timers('loadcheckpoint').stop(barrier=True)         timers.log(['loadcheckpoint'])     else:         args.iteration = 0         args.num_floating_point_operations_so_far = 0 ``` thx, but how to deal with these following codes","> ``` >     if args.load is not None or args.pretrained_checkpoint is not None: >         timers('loadcheckpoint', log_level=0).start(barrier=True) >         args.iteration, args.num_floating_point_operations_so_far = load_checkpoint( >             model, optimizer, opt_param_scheduler) >         timers('loadcheckpoint').stop(barrier=True) >         timers.log(['loadcheckpoint']) >     else: >         args.iteration = 0 >         args.num_floating_point_operations_so_far = 0 > ``` >  > thx, but how to deal with these following codes These codes doesn't conflict, they load weights from checkpoint. I don't have any checkpoint, I just want to train model from huggingface's weights.","OK, Got you. Thanks for your help~"," Sorry to bother, I have just the last one question, if you turn off flash attention, will the first step of loss during the further pretrain be changed?",">  Sorry to bother, I have just the last one question, if you turn off flash attention, will the first step of loss during the further pretrain be changed? Maybe, according to the micro batch data"
janelu9,[QUESTION]Where does the attention_mask come from when the gpt_model is not the first or last pipeline stage?,"I know the hidden_states are the output of previous stage, but I don't understand the how the attention_mask is passed to the next transformer block.",2024-06-08T14:39:25Z,,closed,1,0,https://github.com/NVIDIA/Megatron-LM/issues/861
yangzhipeng1108,"When H800 is trained with FP8, the performance is not significantly improved compared to FP16, and is even worse than FP16.",**Your question** Ask a clear and concise question about MegatronLM.,2024-06-06T06:59:55Z,,closed,0,7,https://github.com/NVIDIA/Megatron-LM/issues/860,"helloï¼ŒPlease tell me, do you know how to start fp8 training? thanks",        bf16 \         fp8format hybrid \         fp8amaxcomputealgo max \         fp8amaxhistorylen 1024 \         transformerimpl transformer_engine,"> ``` >     bf16 \ >     fp8format hybrid \ >     fp8amaxcomputealgo max \ >     fp8amaxhistorylen 1024 \ >     transformerimpl transformer_engine > ``` thanksï¼ŒI occur the same performance degradation problem as you on H20 GPU, please check your transformer engine version, upgrading to the new version may solve your problem"," thanks the parameter setting about fp8, if I want to make experiments which firstly trains llama2 using fp8 (25%)ï¼Œ then trains using bf16 (75%) how to initiate the trainging process","> [](https://github.com/LiaoYuanF) thanks the parameter setting about fp8, if I want to make experiments which firstly trains llama2 using fp8 (25%)ï¼Œ then trains using bf16 (75%) how to initiate the trainging process you need to save checkpoint, stop training, turn off fp8, then start again","> [](https://github.com/LiaoYuanF) thanks the parameter setting about fp8, if I want to make experiments which firstly trains llama2 using fp8 (25%)ï¼Œ then trains using bf16 (75%) how to initiate the trainging process You can separate the epochs for different accuracy ï¼ˆfp 8 or bf 16ï¼‰training, although I don't understand what your intention is"," I tried initiate my fp8 experiment using the parameters, But unfortunately, The error OOM occur. Whe bf16 succeed, but adding the fp8 parameter oom error occurs. ``` vocab_parallel_logits = vocab_parallel_logits.float() torch.OutOfMemoryError: ```"
felipeliliti,Projeto liliti stk 3.6.9 acabou,https://github.com/googlecodelabs/tools/issues/903,2024-06-05T18:55:49Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/859,Marking as stale. No activity in 60 days.
cong-bai,[BUG] Mismatch Between Docstring and Behavior in core.tensor_parallel.random.model_parallel_cuda_manual_seed,"**Describe the bug** The following function (in `megatron.core.tensor_parallel.random`) is called when we initialize the random seeds. Now I am suspecting the behavior of this function doesn't match the docstring, even if we consider different ways of passing seeds to the function:  One unified input seed for all ranks: If this is true, then all ranks will have the same `data_parallel_seed` or ""default state"", while the docstring says: ""default state: This is for data parallelism and is (...) different across different model paralle groups.""  One unified input seed for one DP group: If this is true, then different DP and MP rank will both lead to different `tensor_model_parallel_seed` or ""tensormodelparallel state"", while the docstring says: ""tensormodelparallel state: This state is (...) the same across data parallel groups."" ```python def model_parallel_cuda_manual_seed(seed):     """"""Initialize model parallel cuda seed.     This function should be called after the model parallel is     initialized. Also, no torch.cuda.manual_seed should be called     after this function. Basically, this is replacement for that     function.     Two set of RNG states are tracked:     default state: This is for data parallelism and is the same among a set of model parallel GPUs but different across different model paralle groups. This is used for example for dropout in the nontensormodelparallel regions.     tensormodelparallel state: This state is different among a set of model parallel GPUs, but the same across data parallel groups. This is used for example for dropout in model parallel regions.     """"""      2718 is just for fun and any POSITIVE value will work.     offset = seed + 2718     tensor_model_parallel_seed = offset + get_tensor_model_parallel_rank()      Data parallel gets the original seed.     data_parallel_seed = seed     initialize_rng_tracker()     _CUDA_RNG_STATE_TRACKER.reset()      Set the default state.     torch.cuda.manual_seed(data_parallel_seed)     _CUDA_RNG_STATE_TRACKER.add(_DATA_PARALLEL_RNG_TRACKER_NAME, data_parallel_seed)      and model parallel state.     _CUDA_RNG_STATE_TRACKER.add(_MODEL_PARALLEL_RNG_TRACKER_NAME, tensor_model_parallel_seed)     expert_parallel_seed = (         seed + 1024 + 100 * get_expert_model_parallel_rank() + get_tensor_model_parallel_rank()     )     _CUDA_RNG_STATE_TRACKER.add(_EXPERT_PARALLEL_RNG_TRACKER_NAME, expert_parallel_seed) ``` **To Reproduce** N/A **Expected behavior** I guess the docstring states the intention (and is the expected behavior) of this function **Stack trace/logs** N/A **Environment (please complete the following information):**   MegatronLM commit ID c4d12e26b2dc25a2eab7da92e2ac30338c0ed3de   PyTorch version N/A   CUDA version N/A   NCCL version N/A **Proposed fix** N/A **Additional context** I suspect `model_parallel_cuda_manual_seed` are meant to take care of all the details about seeding in different parallelisms. If this is true, then the following usage (in `megatron/training/initialize.py`) seems problematic: ```python def _set_random_seed(seed_, data_parallel_random_init=False):     """"""Set random seed for reproducability.""""""     if seed_ is not None and seed_ > 0:          Ensure that different pipeline MP stages get different seeds.         seed = seed_ + (100 * mpu.get_pipeline_model_parallel_rank())          Ensure different data parallel ranks get different seeds         if data_parallel_random_init:             seed = seed + (10 * mpu.get_data_parallel_rank())         random.seed(seed)         np.random.seed(seed)         torch.manual_seed(seed)         if torch.cuda.device_count() > 0:             tensor_parallel.model_parallel_cuda_manual_seed(seed)     else:         raise ValueError(""Seed ({}) should be a positive integer."".format(seed)) ```",2024-06-05T10:30:05Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/858,Marking as stale. No activity in 60 days.
janelu9,[ENHANCEMENT]How to specify the number of layers in each pipeline stage in my mind?,Does megatron put same number of transformer decoder layers in each pipeline stage by the following codesï¼Ÿ https://github.com/NVIDIA/MegatronLM/blob/c4d12e26b2dc25a2eab7da92e2ac30338c0ed3de/megatron/core/transformer/transformer_block.pyL31,2024-06-05T09:53:05Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/857
liangshaopeng,"[ENHANCEMENT]How, or rather, is there any support provided for MOE models of Qwen2MoeForCausalLM in the transformers library?","**Is your feature request related to a problem? Please describe.** I have seen support for training MOE models in Megatron, including scripts for the Mixtral 8x7B model, at: https://docs.nvidia.com/megatroncore/developerguide/latest/apiguide/moe.html.  However, at Alibaba, we are interested in training MOE models for Qwen2MoeForCausalLM, supported by Megatron, where the model is based on the architecture code from the transformers library: https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2_moe/modeling_qwen2_moe.py.  I wonder if such support is available?",2024-06-05T02:42:24Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/856,Marking as stale. No activity in 60 days.,Do you still have this feature request? maybe  we can help.,Marking as stale. No activity in 60 days.
schheda1,[BUG] Megatron Core example not working,"**Describe the bug** The provided example script `run_simple_mcore_train_loop.py` throws errors in Step 3: GPT Mock dataset setup utility.   **To Reproduce** For simplicity, the example is run with a single GPU with `tensor_model_parallel_size=1` and `pipeline_model_parallel_size=1`.  `srun python u run_simple_mcore_train_loop.py` **Stack trace/logs** ``` [rank0]: Traceback (most recent call last): [rank0]:   File ""/scratch/sd/u/user/MegatronLM/examples/run_simple_mcore_train_loop.py"", line 115, in  [rank0]:     train_iterator = get_train_data_iterator() [rank0]:   File ""/scratch/sd/u/user/MegatronLM/examples/run_simple_mcore_train_loop.py"", line 55, in get_train_data_iterator [rank0]:     config = GPTDatasetConfig( [rank0]:   File """", line 18, in __init__ [rank0]:   File ""/scratch/sd/u/user/MegatronLM/megatron/core/datasets/gpt_dataset.py"", line 52, in __post_init__ [rank0]:     super().__post_init__() [rank0]:   File ""/scratch/sd/u/user/MegatronLM/megatron/core/datasets/blended_megatron_dataset_config.py"", line 87, in __post_init__ [rank0]:     assert self.split is not None, ""split must be provided in absence of blend_per_split"" [rank0]: AssertionError: split must be provided in absence of blend_per_split ``` **Environment (please complete the following information):**   MegatronLM commit ID: `a5534c8`   PyTorch version: `2.4.0a0+gitd957c2d`   CUDA version: 12.2   NCCL version: 2.19.4 **Proposed fix** N/A **Additional context** Upon applying a temporary fix in `get_train_data_iterator` for the GPT config with `split='1'`,  other errors are thrown when creating an object of class MockGPTDataset.  Additionally, this GPT config refers to a `dummy` tokenizer which is missing. Any assistance with resolving this issue would be appreciated, thank you!",2024-06-03T20:06:03Z,stale,open,4,4,https://github.com/NVIDIA/Megatron-LM/issues/855,I also meet this problem,same,6c7bec6 fixes this issue partially.  A `split` argument is required to be passed to GPTDatasetConfig (from BlendedMegatronDatasetConfig) if `blend` is None to make the example work. ,Marking as stale. No activity in 60 days.
srivassid,[QUESTION] Problems performing inference,"I have trained a model for 800 iters, just for testing purposes, and i am trying to perform inference on it, but the server crashes.  I have hosted a server, but when i try `tools/text_generation_cli.py localhost:5000` it asks me for a prompt but then the server crashes saying `AttributeError: 'InferenceParams' object has no attribute 'max_sequence_len'` Can anyone help me out?  Thanks",2024-06-03T12:19:32Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/854,"Updating transformer engine within docker image and creating a new docker image with the changes solved the issue.  Docker image pytorch24.01 contains transformer engine 1.2.0, whereas the latest stable TE is 1.6.0 "
hwdef,[ENHANCEMENT] update black version,**Is your feature request related to a problem? Please describe.** I noticed that the current black version of megatronlm is very old and already behind NeMo https://github.com/NVIDIA/NeMo/blob/b0f3138a6be7fab3175deb8935f8492aeb1445bd/pyproject.tomlL33 **Describe the solution you'd like** update black version,2024-06-03T10:07:57Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/853,"  Hi, Cloud you please take a look? If you don't have time, I can update it.",Marking as stale. No activity in 60 days.
zainsarwar865,[BUG] Preprocess_data.py does not finalize all keys,"In the `tools/preprocess_data.py` there is a function `process_json_files` which will read json files, encode them and save them in .bin/.idx format. While this reads every json key and creats a `MMapIndexedDatasetBuilder` for every key, it does not call the `finalize` function for every key (since it is missing a for loop). This prevents the index from being created.",2024-06-02T16:23:03Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/852," Hi, why closed this pr?"
WailordHe,[QUESTION] Question about resume with distributed optimizer,"**Your question** Is it possible to load an optimizer that was previously saved using a distributed optimizer configuration, and then continue the training without employing a distributed optimizer?",2024-06-01T07:44:11Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/851,Not without a checkpoint converter.,Marking as stale. No activity in 60 days.
Btlmd,[QUESTION] Why TE is not used for an MoE layer?,I noticed that TransformerEngine implementation is not used when building an MoE layer even `use_te` is specified. https://github.com/NVIDIA/MegatronLM/blob/a5534c8f3e2c49ad8ce486f5cba3408e14f5fcc2/megatron/core/models/gpt/gpt_layer_specs.pyL101L106 I wonder the reason for not using TE implementation.,2024-05-30T08:11:28Z,,closed,0,10,https://github.com/NVIDIA/Megatron-LM/issues/850,"MoE requires some small changes to TE (see this PR), which is only available since TE v1.7+. We have a plan to adopt TE's linear layer when enabling FP8 for MoE training. Stay tuned. For BF16, we need more comprehensive convergence tests before switching to TE's linear layer.",Thanks a lot ðŸ˜€,"> MoE requires some small changes to TE (see this PR), which is only available since TE v1.7+. We have a plan to adopt TE's linear layer when enabling FP8 for MoE training. Stay tuned. For BF16, we need more comprehensive convergence tests before switching to TE's linear layer. Hi, Is there any recent development on TEbased FP8 MoE layers?","> > MoE requires some small changes to TE (see this PR), which is only available since TE v1.7+. We have a plan to adopt TE's linear layer when enabling FP8 for MoE training. Stay tuned. For BF16, we need more comprehensive convergence tests before switching to TE's linear layer. >  > Hi, Is there any recent development on TEbased FP8 MoE layers? Now we default to TE's linear for both `SequentialMLP` and `TEGroupedMLP`. And they both support FP8 training. Please use TE 1.x for now because there're some bugs related to TE 2.0.","> > > MoE requires some small changes to TE (see this PR), which is only available since TE v1.7+. We have a plan to adopt TE's linear layer when enabling FP8 for MoE training. Stay tuned. For BF16, we need more comprehensive convergence tests before switching to TE's linear layer. > >  > >  > > Hi, Is there any recent development on TEbased FP8 MoE layers? >  > Now we default to TE's linear for both `SequentialMLP` and `TEGroupedMLP`. And they both support FP8 training. Please use TE 1.x for now because there're some bugs related to TE 2.0. thanks for your reply. I notice that there is a pull request about fp8 alltoall in transformerengine pr. Why wasn't the PR merged into Megatron or TE? Is there an issue with the training accuracy of FP8 alltoall?""","> > > MoE requires some small changes to TE (see this PR), which is only available since TE v1.7+. We have a plan to adopt TE's linear layer when enabling FP8 for MoE training. Stay tuned. For BF16, we need more comprehensive convergence tests before switching to TE's linear layer. > >  > >  > > Hi, Is there any recent development on TEbased FP8 MoE layers? >  > Now we default to TE's linear for both `SequentialMLP` and `TEGroupedMLP`. And they both support FP8 training. Please use TE 1.x for now because there're some bugs related to TE 2.0. Could you provide the te2.0 code link related the fp8 bug. Because we use te2.0 fp8 to train the moe model. Thanks a lot","> > > > MoE requires some small changes to TE (see this PR), which is only available since TE v1.7+. We have a plan to adopt TE's linear layer when enabling FP8 for MoE training. Stay tuned. For BF16, we need more comprehensive convergence tests before switching to TE's linear layer. > > >  > > >  > > > Hi, Is there any recent development on TEbased FP8 MoE layers? > >  > >  > > Now we default to TE's linear for both `SequentialMLP` and `TEGroupedMLP`. And they both support FP8 training. Please use TE 1.x for now because there're some bugs related to TE 2.0. >  > Could you provide the te2.0 code link related the fp8 bug. Because we use te2.0 fp8 to train the moe model. Thanks a lot  https://github.com/NVIDIA/TransformerEngine/pull/1488  https://github.com/NVIDIA/TransformerEngine/pull/1475  https://github.com/NVIDIA/TransformerEngine/pull/1474","> > > > MoE requires some small changes to TE (see this PR), which is only available since TE v1.7+. We have a plan to adopt TE's linear layer when enabling FP8 for MoE training. Stay tuned. For BF16, we need more comprehensive convergence tests before switching to TE's linear layer. > > >  > > >  > > > Hi, Is there any recent development on TEbased FP8 MoE layers? > >  > >  > > Now we default to TE's linear for both `SequentialMLP` and `TEGroupedMLP`. And they both support FP8 training. Please use TE 1.x for now because there're some bugs related to TE 2.0. >  > thanks for your reply. I notice that there is a pull request about fp8 alltoall in transformerengine pr. Why wasn't the PR merged into Megatron or TE? Is there an issue with the training accuracy of FP8 alltoall?"" That PR was for delayed scaling only. At that time, we have already found convergence issues with delayed scaling, and started to develop new fp8 recipes such as pertensor current scaling (TE PR 1471) and subchannel scaling (deepseekv3like, TE PR 1513), so the fp8 alltoall PR was closed. We have the plan to enable FP8 alltoall via DeepEP in the future. And we find for fp8 alltoall, you could get performance gain only when EP is internode. While at that time, our primary goal was to optimize the performance of Mixtral 8x7B and 8x22B, and their EP is intranode (EP size = 8). So we didn't move forward.","> > > > > MoE requires some small changes to TE (see this PR), which is only available since TE v1.7+. We have a plan to adopt TE's linear layer when enabling FP8 for MoE training. Stay tuned. For BF16, we need more comprehensive convergence tests before switching to TE's linear layer. > > > >  > > > >  > > > > Hi, Is there any recent development on TEbased FP8 MoE layers? > > >  > > >  > > > Now we default to TE's linear for both `SequentialMLP` and `TEGroupedMLP`. And they both support FP8 training. Please use TE 1.x for now because there're some bugs related to TE 2.0. > >  > >  > > Could you provide the te2.0 code link related the fp8 bug. Because we use te2.0 fp8 to train the moe model. Thanks a lot >  > * [[PyTorch] Fix fuse_wgrad_accumulation for GroupedLinearÂ TransformerEngine CC([QUESTION] Forward and Backward WallClock Time Comparison)](https://github.com/NVIDIA/TransformerEngine/pull/1488) > * [Fix a bug for D being nullptr in grouped gemmÂ TransformerEngine CC([ENHANCEMENT] update theoretical memory usage and tflops per iteration for MLA)](https://github.com/NVIDIA/TransformerEngine/pull/1475) > * [Fix issues for MCore DDP.Â TransformerEngine CC([QUESTION] How does megatron manage clusters?)](https://github.com/NVIDIA/TransformerEngine/pull/1474) thx for your reply. I will check the code in te2.0","> > > > > MoE requires some small changes to TE (see this PR), which is only available since TE v1.7+. We have a plan to adopt TE's linear layer when enabling FP8 for MoE training. Stay tuned. For BF16, we need more comprehensive convergence tests before switching to TE's linear layer. > > > >  > > > >  > > > > Hi, Is there any recent development on TEbased FP8 MoE layers? > > >  > > >  > > > Now we default to TE's linear for both `SequentialMLP` and `TEGroupedMLP`. And they both support FP8 training. Please use TE 1.x for now because there're some bugs related to TE 2.0. > >  > >  > > thanks for your reply. I notice that there is a pull request about fp8 alltoall in transformerengine pr. Why wasn't the PR merged into Megatron or TE? Is there an issue with the training accuracy of FP8 alltoall?"" >  > That PR was for delayed scaling only. At that time, we have already found convergence issues with delayed scaling, and started to develop new fp8 recipes such as pertensor current scaling (TE PR 1471) and subchannel scaling (deepseekv3like, TE PR 1513), so the fp8 alltoall PR was closed. We have the plan to enable FP8 alltoall via DeepEP in the future. >  > And we find for fp8 alltoall, you could get performance gain only when EP is internode. While at that time, our primary goal was to optimize the performance of Mixtral 8x7B and 8x22B, and their EP is intranode (EP size = 8). So we didn't move forward. okay, got it. thanks again. I take a look at the PR 1471 and PR 1513"
gaokaiz2,[QUESTION] Does Megatron-LM supports P100?,"Does MegatronLM supports P100? I heard that due to different hardware architecture it may not, but I haven't found direct evidence.",2024-05-29T22:12:04Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/849,"It should, though we haven't tested it on P100s in years, and a few bugfixes might be required,",Would be useful for a museum demo,Marking as stale. No activity in 60 days.
jingxu9x,[BUG] wrong scale softmax for local transformer implement,DotProductAttention implementation multiplies the wrong scaling factor This PR provider a simple fix https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/transformer/dot_product_attention.pyL67L81,2024-05-29T17:40:15Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/848,Marking as stale. No activity in 60 days.
Gstdioh,Fix the bug where the optimizer doesn't actually call multi_tensor_applier under float16.,"Fix the bug where the optimizer doesn't actually use multi_tensor_applier under float16, because overflow_buf is always False. Specifically, `overflow_buf = self._dummy_overflow_buf`, and `self._dummy_overflow_buf` is initialized as `torch.tensor([0], dtype=torch.int, device='cuda')` under float16. However, `bool(torch.tensor([0], dtype=torch.int, device='cuda'))` is False, meaning `overflow_buf` is always False. The original code is similar to the following: ```python if self.config.bf16:     self._dummy_overflow_buf = None else:     self._dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device='cuda') ... _multi_tensor_copy_this_to_that(     this=main_data, that=model_data, overflow_buf=self._dummy_overflow_buf ) ... def _multi_tensor_copy_this_to_that(     this: List[torch.Tensor], that: List[torch.Tensor], overflow_buf: Optional[torch.Tensor] = None ):     """"""     Use multitensorapplier to copy values from one list to another.     We don't have a bfloat16 implementation so for now if the overflow_buf     is not provided, we default back to simple loop copy to be compatible     with bfloat16.     """"""     if overflow_buf:         overflow_buf.fill_(0)          Scaling with factor `1.0` is equivalent to copy.         multi_tensor_applier(amp_C.multi_tensor_scale, overflow_buf, [this, that], 1.0)     else:         for this_, that_ in zip(this, that):             that_.copy_(this_) ``` Therefore, the optimizer doesn't actually use multi_tensor_applier under float16. Here, it can be changed to directly determine whether overflow_buf is None.",2024-05-29T17:18:37Z,,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/847,Marking as stale. No activity in 60 days.,Thanks. I'll include your commit in my internal fix PR.,Your commit is squash merged in https://github.com/NVIDIA/MegatronLM/commit/029025c4c44a9e5fb5488fbb31bbc596ee6aaeca and your author information is preserved. Thanks for your contribution. You can close this PR now.,okey!
Gstdioh,Fix the bug where the optimizer doesn't actually use multi_tensor_applier under float16.,"Fix the bug where the optimizer doesn't actually use multi_tensor_applier under float16, because overflow_buf is always False.  Specifically, `overflow_buf = self._dummy_overflow_buf`, and `self._dummy_overflow_buf` is initialized as `torch.tensor([0], dtype=torch.int, device='cuda')` under float16.  However, `bool(torch.tensor([0], dtype=torch.int, device='cuda'))` is False, meaning overflow_buf is always False.  The original code is similar to the following: ```python if self.config.bf16:     self._dummy_overflow_buf = None else:     self._dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device='cuda') ... _multi_tensor_copy_this_to_that(     this=main_data, that=model_data, overflow_buf=self._dummy_overflow_buf ) ... def _multi_tensor_copy_this_to_that(     this: List[torch.Tensor], that: List[torch.Tensor], overflow_buf: Optional[torch.Tensor] = None ):     """"""     Use multitensorapplier to copy values from one list to another.     We don't have a bfloat16 implementation so for now if the overflow_buf     is not provided, we default back to simple loop copy to be compatible     with bfloat16.     """"""     if overflow_buf:         overflow_buf.fill_(0)          Scaling with factor `1.0` is equivalent to copy.         multi_tensor_applier(amp_C.multi_tensor_scale, overflow_buf, [this, that], 1.0)     else:         for this_, that_ in zip(this, that):             that_.copy_(this_) ``` Therefore, the optimizer doesn't actually use multi_tensor_applier under float16. Here, it can be changed to directly determine whether overflow_buf is None.",2024-05-29T17:00:16Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/846
ltm920716,[QUESTION] how to configure llama3 model,"Hi,     I have run the gpt2 demo successfully by `sh examples/pretrain_gpt.sh`, and I want to build the llama38b model through MegatronLM. So I change the params in `examples/pretrain_gpt.sh` like bellow: ``` GPT_ARGS=""     numlayers 2 \     hiddensize 4096 \     numattentionheads 32 \     seqlength 512 \     maxpositionembeddings 8192 \     ffnhiddensize 14336 \     microbatchsize 1 \     globalbatchsize 2 \     lr 0.00015 \     trainiters 5000 \     lrdecayiters 3200 \     lrdecaystyle cosine \     minlr 1.0e5 \     weightdecay 1e2 \     lrwarmupfraction .01 \     numquerygroups 8 \     groupqueryattention \     fp16 \     userotarypositionembeddings \     normalization RMSNorm \     nopositionembedding \     attentionsoftmaxinfp32 "" ``` I  also add code snippet in 'pretrain_gpt.py' to show the model layers as follow: ``` for name, param in model.named_parameters():         print(f""{name} {param.shape}"") ```  the output result is ``` language_model.embedding.word_embeddings.weight torch.Size([128000, 4096]) language_model.encoder.layers.0.self_attention.layernorm_qkv.layer_norm_weight torch.Size([4096]) language_model.encoder.layers.0.self_attention.layernorm_qkv.weight torch.Size([12288, 4096]) language_model.encoder.layers.0.self_attention.layernorm_qkv.bias torch.Size([12288]) language_model.encoder.layers.0.self_attention.proj.weight torch.Size([4096, 4096]) language_model.encoder.layers.0.self_attention.proj.bias torch.Size([4096]) language_model.encoder.layers.0.layernorm_mlp.layer_norm_weight torch.Size([4096]) language_model.encoder.layers.0.layernorm_mlp.fc1_weight torch.Size([14336, 4096]) language_model.encoder.layers.0.layernorm_mlp.fc1_bias torch.Size([14336]) language_model.encoder.layers.0.layernorm_mlp.fc2_weight torch.Size([4096, 14336]) language_model.encoder.layers.0.layernorm_mlp.fc2_bias torch.Size([4096]) language_model.encoder.layers.1.self_attention.layernorm_qkv.layer_norm_weight torch.Size([4096]) language_model.encoder.layers.1.self_attention.layernorm_qkv.weight torch.Size([12288, 4096]) language_model.encoder.layers.1.self_attention.layernorm_qkv.bias torch.Size([12288]) language_model.encoder.layers.1.self_attention.proj.weight torch.Size([4096, 4096]) language_model.encoder.layers.1.self_attention.proj.bias torch.Size([4096]) language_model.encoder.layers.1.layernorm_mlp.layer_norm_weight torch.Size([4096]) language_model.encoder.layers.1.layernorm_mlp.fc1_weight torch.Size([14336, 4096]) language_model.encoder.layers.1.layernorm_mlp.fc1_bias torch.Size([14336]) language_model.encoder.layers.1.layernorm_mlp.fc2_weight torch.Size([4096, 14336]) language_model.encoder.layers.1.layernorm_mlp.fc2_bias torch.Size([4096]) language_model.encoder.final_norm.weight torch.Size([4096]) ``` I think the qkv part is not correct, right? the params: ``` numquerygroups 8  groupqueryattention  ``` have no effect, please help, thanks! by the way, I have converted the llama38b hf to megatron, the converted model layers are: ``` word_embeddings torch.Size([128256, 4096]) layers.0.input_norm.weight torch.Size([4096]) layers.0.self_attention.query_key_value.weight torch.Size([6144, 4096]) layers.0.self_attention.dense.weight torch.Size([4096, 4096]) layers.0.post_attention_norm.weight torch.Size([4096]) layers.0.mlp.dense_h_to_4h.weight torch.Size([28672, 4096]) layers.0.mlp.dense_4h_to_h.weight torch.Size([4096, 14336]) layers.1.input_norm.weight torch.Size([4096]) layers.1.self_attention.query_key_value.weight torch.Size([6144, 4096]) layers.1.self_attention.dense.weight torch.Size([4096, 4096]) layers.1.post_attention_norm.weight torch.Size([4096]) layers.1.mlp.dense_h_to_4h.weight torch.Size([28672, 4096]) layers.1.mlp.dense_4h_to_h.weight torch.Size([4096, 14336]) layers.2.input_norm.weight torch.Size([4096]) ...... final_norm.weight torch.Size([4096]) weight torch.Size([128256, 4096]) ```",2024-05-29T02:00:12Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/845,"> by the way, I have converted the llama38b hf to megatron, the converted model layers are My understanding is that `megatron` model type is deprecated. Consider using `mcore` model type and `usemcoremodels` when doing training.","> My understanding is that `megatron` model type is deprecated. Consider using `mcore` model type and `usemcoremodels` when doing training. hi thanks, usemcoremodels is usefulï¼Œbut something like ffngate and so on is still matterï¼ŒI will go to nemoframeworklancher and see the difference"
li-plus,[BUG] Wrong embedding gradients with distributed optimizer and shared embedding,"**Describe the bug** When `use_distributed_optimizer` is enabled for models with `share_embeddings_and_output_weights` such as GPT2, all model gradients are reducescattered across DP ranks before the embedding gradients are allreduced across PP[0] & PP[1]. See https://github.com/NVIDIA/MegatronLM/blob/0650d8335d45162845398a97880374b81c4d84b1/megatron/core/distributed/finalize_model_grads.pyL99L150 Note that the wte gradients and lm_head gradients lie in different partitions of the contiguous gradient buffer (wte is the first weight on PP[0], lm_head is the last weight on PP[1]), so they will be reducescattered to different DP ranks. The following embedding gradients allreduce across PP[0] and PP[1] within same DP group will add up partial results, producing wrong embedding gradients. For example, consider only embedding gradients with dp=2 and pp=2 on 4 GPUs: 1. before reducescatter across DP ranks: ",2024-05-28T09:44:04Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/844,"Does the following fix not work: https://github.com/NVIDIA/MegatronLM/commit/daf000673726b7dee40c834f181f76703808b2fc? In particular, these lines: https://github.com/NVIDIA/MegatronLM/commit/daf000673726b7dee40c834f181f76703808b2fcdiff703512d9cce575fe32a776ec738162312b6276de08ac4846a50f07e3903cfdacR239R245.",It works. Problem is I've been using Megatron released on Jan without separate bucket for shared embedding. Just switched to latest master and solved it. Thanks!,"Awesome, great to hear!"
felipeliliti,Fonte facilitada em fractal 2030,**Is your feature request related to a problem? Please describe.** A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] **Describe the solution you'd like** A clear and concise description of what you want to happen. **Describe alternatives you've considered** A clear and concise description of any alternative solutions or features you've considered. **Proposed implementation** If you have a proposed implementation for the feature state it here or link to a PR. **Additional context** Add any other context or screenshots about the feature request here.,2024-05-28T04:12:25Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/843,Marking as stale. No activity in 60 days.
felipeliliti,[BUG],"**Describe the bug** A clear and concise description of what the bug is. **To Reproduce** Steps to reproduce the behavior. The easier it is to reproduce the faster it will get maintainer attention. **Expected behavior** A clear and concise description of what you expected to happen. **Stack trace/logs** If applicable, add the stack trace or logs from the time of the error. **Environment (please complete the following information):**   MegatronLM commit ID   PyTorch version   CUDA version   NCCL version **Proposed fix** If you have a proposal for how to fix the issue state it here or link to a PR. **Additional context** Add any other context about the problem here.",2024-05-28T04:11:32Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/842,Marking as stale. No activity in 60 days.
Eisenhower,"Configuring datasets using train-data-path, valid-data-path, and test-data-path results in training errors","**Describe the bug** When I configure datasets for a training task using traindatapath, validdatapath, and testdatapath, running the training task results in an error. The error message is shown in the screenshot below: File ""/home/kas/kas_workspace/dataset/zrh/paimegatronpatch/PaiMegatronPatch/MegatronLM240405/megatron/core/datasets/blended_megatron_dataset_config.py"", line 72, in __post_init__ assert self.split is None, ""split and blend_per_split are incompatible"" AssertionError assert self.split is None, ""split and blend_per_split are incompatible"": split and blend_per_split are incompatible **To Reproduce** Configure training datasets using traindatapath, validdatapath, and testdatapath. **Expected behavior** Enable configuring datasets using traindatapath, validdatapath, and testdatapath **Stack trace/logs** File ""/home/kas/kas_workspace/dataset/zrh/paimegatronpatch/PaiMegatronPatch/MegatronLM240405/megatron/core/datasets/blended_megatron_dataset_config.py"", line 72, in __post_init__ assert self.split is None, ""split and blend_per_split are incompatible"" AssertionError assert self.split is None, ""split and blend_per_split are incompatible"": split and blend_per_split are incompatible **Environment (please complete the following information):**   MegatronLM commit ID   PyTorch version   CUDA version   NCCL version **Proposed fix** https://github.com/NVIDIA/MegatronLM/pull/840 **Additional context** Add any other context about the problem here.",2024-05-27T12:06:26Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/841,Marking as stale. No activity in 60 days.
Eisenhower,"Fix Bug: Configuring Datasets with train-data-path, valid-data-path, test-data-path","Fixed the bug that prevents configuring datasets using traindatapath, validdatapath, and testdatapath. When the split parameter is not configured, the split parameter will be set to the default value 969, 30, 1. In the blended_megatron_dataset_config.py file, within the __post_init__ function, the following code will raise an error when configuring datasets using traindatapath, validdatapath, and testdatapath because the split parameter is not None: if self.blend_per_split is not None and any(self.blend_per_split):     assert self.blend is None, ""blend and blend_per_split are incompatible""     assert self.split is None, ""split and blend_per_split are incompatible""",2024-05-27T11:42:19Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/840,Marking as stale. No activity in 60 days.
Baibaifan,[BUG] GroupedMLP calculation problem.,"**Describe the bug**  As shown in the figure above, when calculating `w1` in this part, using `view` will cause element confusion. !image As shown in the figure above, it is wrong to use `view`. `Split` or `chunk` should be used for conversion. Because `w2` does not have the dimensionality of `ffn_hidden_size`, there is no problem using `view`. test codes ```python import torch from torch.nn.parameter import Parameter import math torch.manual_seed(123) def scaled_init_method_normal(sigma=0.02, num_layers=10):     """"""Init method based on N(0, sigma/sqrt(2*num_layers).""""""     std = sigma / math.sqrt(2.0 * num_layers)     def init_(tensor):         return torch.nn.init.normal_(tensor, mean=0.0, std=std)     return init_ weight2 = Parameter(     torch.empty(         4,         10,         dtype=torch.bfloat16                 )             ) init_weight2 = scaled_init_method_normal() init_weight2(weight2) print(weight2) weight2_v = weight2.view(5, 4, 1)  weight2 (4,10)>(5, 4, 2) weight2_v = weight2_v[0,:,:]  weight2_v 1st experts (4, 2) print(weight2_v) ```  **To Reproduce**  **Expected behavior** Determine if there is a problem with `GroupedMLP`. **Stack trace/logs**  **Environment (please complete the following information):**   MegatronLM commit ID :0650d8335d45162845398a97880374b81c4d84b1 **Proposed fix**  **Additional context** ",2024-05-27T09:11:47Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/839,"this is a minor issue. it won't affect the correctness. If you want to load weights, just need to make sure the layout is in the view format"," Thank you for your answer. I have made some modifications to the loading scene. I understand that using `view` is to have better continuous memory usage efficiency. If `split` or `chunk` is used, it will return [tensor1, tensor2]. Are there any CPU scheduling issues?",Marking as stale. No activity in 60 days.
arktoswb,[BUG] Can't continue training from GPT-345M checkpoint with TransformerEngine - RuntimeError: Error(s) in loading state_dict for ParallelTransformer,"**Describe the bug** While running `examples/pretrain_gpt.sh` from GPT345M checkpoint I encounter such error: ``` [rank0]: RuntimeError: Error(s) in loading state_dict for ParallelTransformer: [rank0]: 	Missing key(s) in state_dict: ""layers.0.self_attention.layernorm_qkv.layer_norm_weight"", ... [rank0]: 	Unexpected key(s) in state_dict: ""layers.0.input_norm.weight"", ... ``` **To Reproduce** ``` wget contentdisposition https://api.ngc.nvidia.com/v2/models/nvidia/megatron_lm_345m/versions/v0.0/zip O megatron_lm_345m_v0.0.zip unzip megatron_lm_345m_v0.0.zip ``` Run `examples/pretrain_gpt.sh`. `attentionsoftmaxinfp32` arg is added (does not work otherwise). Also tried llama2 checkpoint. The similar error. However, the script successfully runs: 1. From the zero state, and continues running from locally created checkpoint. 2. With `transformerimpl local` from provided GPT345M checkpoint, but that's deprecated, and will not work with llama models per my understanding. **Expected behavior** `examples/pretrain_gpt.sh` should run fine from GPT345M checkpoint on the latest release without any modifications. **Stack trace/logs** https://gist.github.com/arktoswb/7830a87d514fd53cdad17882128d5122 **Environment:**   MegatronLM commit ID https://github.com/nvidia/MegatronLM/commit/c3677e09aa4e2eec37048307bd795928b8f8324a   PyTorch version ```  $ python c ""import torch; print(torch.__version__)""  2.3.0+cu121  ```   CUDA version ``` $ nvcc version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 20052023 NVIDIA Corporation Built on Tue_Jun_13_19:16:58_PDT_2023 Cuda compilation tools, release 12.2, V12.2.91 Build cuda_12.2.r12.2/compiler.32965470_0 ```    NCCL version ``` $ python c ""import torch; print(torch.cuda.nccl.version())"" (2, 20, 5) ```  TransformerEngine I have tried both `stable` and   `release_v1.1` (related: https://github.com/NVIDIA/MegatronLM/issues/577)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ",2024-05-24T17:22:50Z,,closed,0,12,https://github.com/NVIDIA/Megatron-LM/issues/838,The same error inside docker PyTorch Release 23.04: https://gist.github.com/arktoswb/d5835a666e7fcf9bfa3d7ff59173299c,"Apparently, TransformerEngine is supported with model_type = 'mcore'. So, in order to continue training from GPT345M checkpoint: 1. Convert `python3 tools/checkpoint/convert.py modeltype GPT    loader megatron    saver megatron` 2. Run training with `usemcoremodels` I will close this issue, but I suggest to edit example scripts and README to make it more clear.","Hello, thanks for suggesting me to use the convert.py, but i still have some problems. Could you please help me take a look at the issue I encountered when using convert?  Here is my code:  `python3 tools/checkpoint/convert.py modeltype GPT loader megatron saver megatron loaddir models/megatron_lm_345m_v0.0 savedir models/convert/gpt2 megatronpath /home/zyz/code/MegatronLMcore_v0.6.0` Which reports: `  File ""/home/zyz/code/MegatronLMcore_v0.6.0/tools/checkpoint/loader_megatron.py"", line 70, in _load_checkpoint     margs, checkpoint_args = load_args_from_checkpoint(margs, exit_on_missing_checkpoint=True) TypeError: cannot unpack noniterable Namespace object` Can you let me see all the code you're using here? thanks again","> Hello, thanks for suggesting me to use the convert.py, but i still have some problems. Could you please help me take a look at the issue I encountered when using convert? Here is my code: `python3 tools/checkpoint/convert.py modeltype GPT loader megatron saver megatron loaddir models/megatron_lm_345m_v0.0 savedir models/convert/gpt2 megatronpath /home/zyz/code/MegatronLMcore_v0.6.0` >  > Which reports: ` File ""/home/zyz/code/MegatronLMcore_v0.6.0/tools/checkpoint/loader_megatron.py"", line 70, in _load_checkpoint margs, checkpoint_args = load_args_from_checkpoint(margs, exit_on_missing_checkpoint=True) TypeError: cannot unpack noniterable Namespace object` Can you let me see all the code you're using here? thanks again Yeah, there are multiple problems with that: 1. `loader megatron`: you are loading megatron model 2. `saver megatron`: you are saving megatron model Megatron core model is `saver mcore`. As for loading you can make edits to the code and hardcode multiple model parameters to make it work:  I believe, the error in question is because `load_args_from_checkpoint` returns just `args` in some conditions instead of returning `args, checkpoint_args`. You can bypass it by returning `args, args`  Once fixed, you will have more errors because loader does not know all the parameters of the model it needs to know. Add them: ```     check_for_arg('num_layers', 24)     check_for_arg('hidden_size', 1024)     check_for_arg('num_attention_heads', 16)     check_for_arg('max_position_embeddings', 1024)     check_for_arg('seq_length', 1024)     check_for_arg('tokenizer_type', 'GPT2BPETokenizer')      Validate margs.     margs = validate_args(margs)     margs.use_mcore_models = False     margs.transformer_impl = args.loader_transformer_impl     check_for_arg('tensor_model_parallel_size')     check_for_arg('pipeline_model_parallel_size')     check_for_arg('position_embedding_type')     check_for_arg('iteration', 666)     check_for_arg('padded_vocab_size', 50304)     check_for_arg('bert_binary_head')     check_for_arg('disable_bias_linear', False)     check_for_arg('params_dtype')     check_for_arg('swiglu', False) ``` But honestly you will have easier experience loading llama2 7b model. Even better experience on NeMo  it's in a better shape and also supports llama3","Thank you very much for your response, which has been very helpful. ","Hi  , does convert.py support convert a pretrained checkpoint with PP=1, TP=1 to either PP>1 and/or TP>1? I want to finetune Mamba 8B from a pretrained checkpoint, but 1 GPU can't afford the memory constraints. ","> Hi [](https://github.com/arktoswb) , does convert.py support convert a pretrained checkpoint with PP=1, TP=1 to either PP>1 and/or TP>1? I want to finetune Mamba 8B from a pretrained checkpoint, but 1 GPU can't afford the memory constraints. Yes, I believe it does. I stopped working with Megatron months ago, so I am not the best person to ask this question.","> > Hi [](https://github.com/arktoswb) , does convert.py support convert a pretrained checkpoint with PP=1, TP=1 to either PP>1 and/or TP>1? I want to finetune Mamba 8B from a pretrained checkpoint, but 1 GPU can't afford the memory constraints. >  > Yes, I believe it does. >  > I stopped working with Megatron months ago, so I am not the best person to ask this question. Thanks for replying! I did not find a flag I can specify the PP and TP at convert.py. Do you have any clues on this? Or do you know anyone who may have some clues? ","> > > Hi [](https://github.com/arktoswb) , does convert.py support convert a pretrained checkpoint with PP=1, TP=1 to either PP>1 and/or TP>1? I want to finetune Mamba 8B from a pretrained checkpoint, but 1 GPU can't afford the memory constraints. > >  > >  > > Yes, I believe it does. > > I stopped working with Megatron months ago, so I am not the best person to ask this question. >  > Thanks for replying! I did not find a flag I can specify the PP and TP at convert.py. Do you have any clues on this? Or do you know anyone who may have some clues? From https://github.com/NVIDIA/MegatronLM?tab=readmeovfileevaluationandtasks: flags `targettensorparallelsize` and `targetpipelineparallelsize`","> > > > Hi [](https://github.com/arktoswb) , does convert.py support convert a pretrained checkpoint with PP=1, TP=1 to either PP>1 and/or TP>1? I want to finetune Mamba 8B from a pretrained checkpoint, but 1 GPU can't afford the memory constraints. > > >  > > >  > > > Yes, I believe it does. > > > I stopped working with Megatron months ago, so I am not the best person to ask this question. > >  > >  > > Thanks for replying! I did not find a flag I can specify the PP and TP at convert.py. Do you have any clues on this? Or do you know anyone who may have some clues? >  > From https://github.com/NVIDIA/MegatronLM?tab=readmeovfileevaluationandtasks: flags `targettensorparallelsize` and `targetpipelineparallelsize` Thanks! It took some effort to solve the import error, but at the end I encountered another error that says for all layers ``` untimeError: Error(s) in loading state_dict for ParallelTransformer:         Missing key(s) in state_dict:  ""layers.0.input_norm.weight"", ""layers.0.self_attention.query_key_value.weight"", ""layers.0.self_attention.dense.wei ... ``` I think it is the problem with how to enable distributed model for Mamba model specifically. I opened up another github issue on this .  But thanks for directing to the `convert.py`! ","Update: `convert.py` does not support Mamba at the moment, but the `hybrid_conversion.py` does. ",Have you ever tried load HF model and continue train it?
JiwenJ,[QUESTION] Is FP32 supported in MultiNode Training,"We plan to finetune a model in MegatronLM, the model (11B) is sharded with tp=4, pp=16. We want to finetune the model in fp32 rather than fp16 or bf16. The error is as follows: ```bash TypeError: Can't instantiate abstract class FP32Optimizer with abstract methods sharded_state_dict         return FP32Optimizer(optimizer, config, init_state_fn,)return FP32Optimizer(optimizer, config, init_state_fn,)   File ""/work/home/sx_mitee/MegatronLM/megatron/core/optimizer/__init__.py"", line 252, in _get_megatron_optimizer_based_on_param_groups TypeError: Can't instantiate abstract class FP32Optimizer with abstract methods sharded_state_dict     return FP32Optimizer(optimizer, config, init_state_fn,)     return FP32Optimizer(optimizer, config, init_state_fn,)     _get_megatron_optimizer_based_on_param_groups( ```",2024-05-24T07:42:45Z,,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/837,"Stay tuned, we have a bugfix in the works.","I try to use the flag `usedistributedoptimzier`, and it does not report the error. So is `usedistributedoptimzier`  a must for multinode training in fp32?",Should work now: https://github.com/NVIDIA/MegatronLM/commit/020b51796e1302bc91a30154b665a4d0afc59dd6.,"Going to close this, please reopen if you are still seeing issues."
alpha0422,Remove Redundant Host & Device Sync,"This unnecessary sync breaks CUDA graph of Stable Diffusion in NeMo.  Please take a review, thanks!",2024-05-23T15:27:33Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/836,Will merge internally and then sync to Github.
Baibaifan,[BUG] The problems with bucket and shared_embedding.,"**Describe the bug** !image As shown in the figure above, `shared_embedding` and other parameters are distinguished when building the `bucket`. When the `data_end_index` of the parameter before `shared_embedding` is not divisible by `self.data_parallel_world_size`, there will be a problem here. **To Reproduce**  **Expected behavior**  **Stack trace/logs**  **Environment (please complete the following information):**   MegatronLM commit ID :c3677e09aa4e2eec37048307bd795928b8f8324a   PyTorch version: 2.2.0a0+81ea7a4   CUDA version:    NCCL version:  **Proposed fix**  **Additional context** ",2024-05-23T07:53:59Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/835,"I fixed it, but the megatron has not respondedðŸ‘€ https://github.com/NVIDIA/MegatronLM/pull/762","> I fixed it, but the megatron has not respondedðŸ‘€ CC(fix new bucket when param require new bucket) Itâ€™s so awesome, I must give u a Turing Award.",Marking as stale. No activity in 60 days.
chotzen,[BUG] Checkpoint saving is slow for zarr backend + distributed optimizer ,"**Describe the bug** The distributed optimizer state is being saved in an inefficient way when zarr is used as a backend. This causes slowdowns like the following writes to a local SSD (holding everything constant for a 12 layer x 128 head_dim x 12 head llamastyle transformer):  `Float16OptimizerWithFloat16Params`: 10 seconds  `MixedPrecisionOptimizer`: 64 seconds  After profiling the checkpoint saving workload, it looks like this is what happens for each parameter being saved in the optimizer state:  the entire optimizer state is being fetched into memory (the __getitem__ part)  the full optimizer state is modified in a small region corresponding to that parameter  the full optimizer state is saved This full process takes 450 ms, and is repeated many times once per parameter in the distributed optimizer. **To Reproduce** Spawn a `GPTModel` (with e.g. 12 layers, 128 head dim, 12 heads) on 2 x 2 x 2 pipeline x tensor x data partitions, then try to save the distributed optimizer as a checkpoint with the `zarr` `fully_sharded_bucket_space` backend. **Expected behavior** This is nearly as fast as the saving the nondistributed optimizer, and the difference does not grow at a faster rate than the number of model parameters.  **Environment (please complete the following information):**   MegatronLM commit ID 299f96ffe61a4bae9044a2082570b19b94d13335   PyTorch version 2.2.2   CUDA version 12.1.105   NCCL version 2.20.5 **Proposed fix** N/A  **Additional context** N/A",2024-05-22T23:24:44Z,stale,closed,0,5,https://github.com/NVIDIA/Megatron-LM/issues/834,Have you tried the `torch_dist` (https://github.com/NVIDIA/MegatronLM/blob/main/megatron/training/arguments.pyL1252) distributed checkpoint format?,"yes I've tried it, we ran into some other issue regarding saving the optimizer step with dp_partitions >= 2. will file another bug for that when I have a chance to reproduce it.","Hi , which kinds of checkpoint resharding are meant to be supported for the torch_dist backend? I'm unable to load a (D, P, T) = (2, 2, 2) checkpoint into a (2, 1, 2) partitioning scheme with any combination of (torch_dist, 1) and sharding_type=""dp_zero_gather_scatter"" or not.","Hi , please use the recommended `torch_dist` backend, especially for the DistributedOptimzer  `zarr` backend saving is very slow for DitOpt like sharding type > I'm unable to load a (D, P, T) = (2, 2, 2) checkpoint into a (2, 1, 2) partitioning scheme Changing TPxPP is not supported with DistOpt yet (only DP for now), will be supported in the nearest future (target is MCore v0.8)",Marking as stale. No activity in 60 days.
raywan-110,[QUESTION] Why enable `non_blocking=True` when doing synchronous D2H?,"The comment on line 76 of `filesystem_async.py` indicates that Megatron performs synchronous DevicetoHost (D2H) transfers for checkpointing. However, on line 94, the code enables non_blocking=True during these transfers (code link). Unfortunately, I did not find any explicit CUDA Stream or Event synchronization primitives in the subsequent steps of the checkpointing process. Could this omission potentially introduce security risks, such as saving incomplete CPU tensors to the disk?",2024-05-22T03:45:48Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/833,FYI: ,Marking as stale. No activity in 60 days.
fwyc0573,[QUESTION] How to Obtain Computation Model Graphs in Megatron-LM?,"Hi everyone, I'm currently working on a project involving MegatronLM and I'm looking for a way to obtain the graphs (computation graphs) of submodels after partitioning, along with the attributes of the operators. I've tried using tools such as torch.fx and the new compile and dynamo tools in PyTorch 2.0, but I've encountered several issues. It seems that some of these problems are related to the custom operators implemented in MegatronLM. Could anyone provide a feasible solution or guidance on how to achieve this? Any help would be greatly appreciated! Thank you in advance!",2024-05-19T07:35:45Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/832,Marking as stale. No activity in 60 days.,"I encountered a similar problem about tracing MegatronLM transformer model to graph, using torch.fx as well. Some customized module is recognized as wrong type.. May I know your specific problem and how did you solve it? Thanks!  ",Marking as stale. No activity in 60 days.
Yuxin-CV,[BUG] Modify FLOPs in MFU calculation for casual mask when using FlashAttention.,"Hi, I suggest we modify the FLOPs calculation in the MFU according to the FlashAttention benchmark script. Specifically, the current calculation for the casual mask **can exceed 100% MFU** for seq_len = 16k (189 * 2 / 312 = 1.21), which is inaccurate. The FLOPs for the casual mask setting should be divided by 2 when using FlashAttention. ",2024-05-17T06:28:21Z,stale,open,1,1,https://github.com/NVIDIA/Megatron-LM/issues/831,Marking as stale. No activity in 60 days.
Hongjie1Chu,Question with forward_backward_pipelining_without_interleaving in Megatron-LM Pipeline,"I encountered a problem when using the Megatron pipeline. The function I am using is forward_backward_pipelining_without_interleaving. In this pipeline function, each pipeline stage calls forward_step for the forward pass: output_tensor = forward_step(forward_step_func, data_iterator, model, input_tensor, losses_reduced) The input for the forward pass should be the output from the previous stage. However, in the megatron/schedule.py file, the forward_step function is defined as follows: unwrapped_model.set_input_tensor(input_tensor) output_tensor, loss_func = forward_step_func(data_iterator, model) This implies that each stage in the forward pass still gets data from the dataset and processes it, which seems to contradict the concept of pipelining. Could you please explain the rationale behind this design? code in pretrained_gpy.py: !image Here are my results: !image My configuration: GPUS_PER_NODE=4  Change for multinode config MASTER_ADDR=172.20.20.220 MASTER_PORT=6000 NNODES=1 NODE_RANK=0 WORLD_SIZE=$(($GPUS_PER_NODE*$NNODES)) DATA_PATH=data/mygpt2_text_document CHECKPOINT_PATH=model/model_optim_rng.pt MODEL_PATH=model/output/pp DISTRIBUTED_ARGS=""nproc_per_node $GPUS_PER_NODE nnodes $NNODES node_rank $NODE_RANK master_addr $MASTER_ADDR master_port $MASTER_PORT"" python m torch.distributed.launch $DISTRIBUTED_ARGS \        pretrain_gpt.py \        tensormodelparallelsize 1 \        pipelinemodelparallelsize 4 \        numlayers 12 \        hiddensize 1024 \        numattentionheads 16 \        microbatchsize 16 \        globalbatchsize 64 \        seqlength 1024 \        maxpositionembeddings 1024 \        trainiters 1 Feel free to adjust anything as needed before posting!",2024-05-17T06:11:38Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/830,Marking as stale. No activity in 60 days.
GuWei007,[QUESTION] Why not use tensor parallel APIs of pytorch,**Your question** Ask a clear and concise question about MegatronLM. https://pytorch.org/docs/stable/distributed.tensor.parallel.html Why not use tensor parallel APIs of pytorch,2024-05-16T02:14:13Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/829,afaik DTensor is still a prototype release,Marking as stale. No activity in 60 days.
starstream,[QUESTION] how to profile bubble time in pipeline parallelism?,**Your question** Ask a clear and concise question about MegatronLM. How can I profile bubble time and p2p comm time in pipeline parallelism?,2024-05-15T03:36:12Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/828,"O paralelismo de pipeline Ã© uma tÃ©cnica crucial para treinamento distribuÃ­do em larga escala, mas pode sofrer com ""bolhas"" no pipeline. Essas bolhas sÃ£o ineficiÃªncias que surgem devido a atrasos de sincronizaÃ§Ã£o entre diferentes estÃ¡gios do pipeline. Vamos explorar algumas estratÃ©gias para lidar com isso: 1. **DataParallel (DP)**: Essa abordagem replica o modelo inteiro em vÃ¡rias GPUs, com cada GPU processando uma parte dos dados. ApÃ³s cada etapa de treinamento, as configuraÃ§Ãµes sincronizam seus resultados. No entanto, o DP nÃ£o aborda diretamente as bolhas no pipeline. 2. **TensorParallel (TP)**: No TP, os tensores sÃ£o divididos em pedaÃ§os, e cada fragmento reside em uma GPU designada. Durante o processamento, os fragmentos sÃ£o calculados em paralelo, e os resultados sÃ£o sincronizados no final da etapa. O TP reduz o uso de memÃ³ria, mas ainda nÃ£o elimina as bolhas no pipeline. 3. **PipelineParallel (PP)**: O PP divide verticalmente o modelo entre as GPUs, colocando apenas camadas especÃ­ficas em cada GPU. Cada GPU processa diferentes estÃ¡gios do pipeline em paralelo, trabalhando com pequenos lotes de dados. O PP pode mitigar as bolhas no pipeline otimizando o paralelismo em nÃ­vel de camada. 4. **Zero Redundancy Optimizer (ZeRO)**: O ZeRO divide os tensores de maneira semelhante ao TP, mas reconstrÃ³i o tensor inteiro durante os cÃ¡lculos de avanÃ§o ou retrocesso. Isso evita a modificaÃ§Ã£o do modelo e suporta tÃ©cnicas de descarregamento para memÃ³ria limitada da GPU. 5. **Zero Bubble Pipeline Parallelism**: Pesquisas recentes introduzem estratÃ©gias de agendamento para alcanÃ§ar zero bolhas no pipeline sob semÃ¢ntica de treinamento sÃ­ncrono. Esses mÃ©todos visam minimizar os atrasos de sincronizaÃ§Ã£o e melhorar a eficiÃªncia. 6. **Adaptive Blockwise Taskinterleaved Pipeline Parallelism (ZeroPP)**: O ZeroPP equilibra bolhas no pipeline, memÃ³ria e comunicaÃ§Ã£o usando agendamento adaptativo. Ele agenda os cÃ¡lculos de gradiente de entrada cedo e insere cÃ¡lculos de gradiente de peso para reduzir as bolhas.",Marking as stale. No activity in 60 days.
chrisgao7,[BUG],"**Issue Title** MegatronLM: Zero1 with Distributed Optimizer Showing No Overlap in Communication and Computation **Issue Description** We are experiencing an issue with MegatronLM where enabling zero1 (overlapgradreduce overlapparamgather) along with the distributed optimizer (usedistributedoptimizer) does not result in the expected overlap of communication and computation during training. Despite our attempts to increase CUDA_DEVICE_MAX_CONNECTIONS, we still observe serial execution of communication and computation steps. **Steps to Reproduce** Set up MegatronLM training with zero1 enabled (overlapgradreduce overlapparamgather). Enable the distributed optimizer (usedistributedoptimizer). Attempt to increase CUDA_DEVICE_MAX_CONNECTIONS to a higher value. Start the training process and observe the execution flow. Expected Behavior We expect to see overlap in communication and computation during training, as enabled by zero1 and the distributed optimizer. **Actual Behavior** Communication and computation steps are executed serially, without any overlap. **Environment** MegatronLM version: cafda95 PyTorch version: 2.1.0a0+32f93b1 CUDA version: 12.2 GPU models and configuration: H800 **Timeline** ",2024-05-14T12:44:40Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/827,Marking as stale. No activity in 60 days.
Hoonly,[BUG] The argument --no-position-embedding should be fixed,"**Describe the bug** from the file MegatronLM/megatron/training/arguments.py ```     group.add_argument('nopositionembedding',                        action='store_false',                        help='Disable position embedding. Deprecated: use positionembeddingtype',                        dest='add_position_embedding') ``` I can see that this argument is Deprecated, but if we only use positionembeddingtype=rope, the add_position_embedding will accept default value True, conflict to the rope **To Reproduce** **Expected behavior** **Stack trace/logs** **Environment (please complete the following information):**   MegatronLM commit ID 0d983e64afcd84cab83124e0b7ca89a3d8ec9655   PyTorch version 2.3.0a0+ebedce2   CUDA version  V12.3.107   NCCL version 2.19.4 **Proposed fix** If you have a proposal for how to fix the issue state it here or link to a PR. **Additional context** Add any other context about the problem here.",2024-05-14T11:09:27Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/826,Marking as stale. No activity in 60 days.,è¿™æ˜¯æ¥è‡ªQQé‚®ç®±çš„è‡ªåŠ¨å›žå¤é‚®ä»¶ã€‚ æ‚¨å¥½ï¼Œæ‚¨çš„é‚®ä»¶æˆ‘å·²æ”¶åˆ°ï¼Œè°¢è°¢ã€‚,Marking as stale. No activity in 60 days.
starkhu,"[BUG]:there is a small chance  that it will get stuck, If i repeat runing test_serialization.py many times,","**Describe the bug** If i repeat runing test/unit_tests/distcheckpoing/test_serialization.py many times, there is a small chance that it will get stuck **To Reproduce** step1: cd tests/unit_tests step2: bash ut.sh the following file is ut.sh ``` ! /bin/bash set e for((i=1;i<=500;i++));   do            torchrun nproc_per_node 8 m pytest dist_checkpointing/test_serialization.py done  ``` **Expected behavior** no error. **Stack trace/logs** ``` dist_checkpointing/test_serialization.py rootdir: /workspace/volume/huyongan/LLM/github/MegatronLM plugins: flakefinder1.1.0, shard0.1.2, hypothesis5.35.1, xdoctest1.0.2, rerunfailures13.0, xdist3.5.0 collected 6 items Running 6 items in this shard dist_checkpointing/test_serialization.py ................FFFFFFFFF[rank4]:[E ProcessGroupNCCL.cpp:564] [Rank 4] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data. [rank4]:[E ProcessGroupNCCL.cpp:570] [Rank 4] To avoid data inconsistency, we are taking the entire process down. [rank4]:[E ProcessGroupNCCL.cpp:1335] [PG 0 Rank 4] NCCL watchdog thread terminated with exception: [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2, OpType=ALLGATHER, NumelIn=384, NumelOut=3072, Timeout(ms)=600000) ran for 600488 milliseconds before timing out. ``` **Environment (please complete the following information):**   MegatronLM core_r0.5.0   PyTorch torch2.3   CUDA 12.3   NCCL 2.30.3",2024-05-14T02:23:40Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/825,Marking as stale. No activity in 60 days.
wen020,Does Megatron has plan to support llama pre-trainï¼Ÿ,Does Megatron has plan to support llama pretrainï¼Ÿ,2024-05-13T09:54:30Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/824,it's available through nemo which uses megatron https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/conf/megatron_llama_config.yaml,https://github.com/NVIDIA/MegatronLM/issues/818,Marking as stale. No activity in 60 days.
kenkenpa2126,[Fix] Assertion to check if `num_layers` is divisible by the pipeline size,"Move the assertion to check if num_layers is divisible by the pipeline size, as there is currently no assertion when args.num_layers_per_virtual_pipeline_stage is None, even though it should be divisible.  I suspect that if num_layers is not divisible by the pipeline size, the remainder is ignored without generating any error messages, resulting in a different actual num_layers.",2024-05-13T06:37:15Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/823,"In the legacy code, there's an assertion for divisibility when obtaining the num_layers here: https://github.com/NVIDIA/MegatronLM/blob/f5f3be5dc5aa7dc84f190c1fefacb096d6c224f8/megatron/legacy/model/transformer.pyL1341L1343 However, it seems there's no assertion in the mcore code: https://github.com/NVIDIA/MegatronLM/blob/f5f3be5dc5aa7dc84f190c1fefacb096d6c224f8/megatron/core/transformer/transformer_block.pyL31L35",Marking as stale. No activity in 60 days.
felipeliliti,Projeto liliti stk 3.6.9 inteligÃªncia artificial ðŸ¤– ," LILITI STK 3.6.9 InteligÃªncia Artificial Multimidal Fase 2  VisÃ£o Criar uma plataforma de inovaÃ§Ã£o aberta que integre a inteligÃªncia artificial multimodal avanÃ§ada do LILITI STK 3.6.9 com a rede global de inovadores jovens do programa Innovators Under 35.  MissÃ£o  Fomentar a colaboraÃ§Ã£o entre jovens inovadores e a tecnologia de IA multimodal.  Desenvolver soluÃ§Ãµes tecnolÃ³gicas que atendam Ã s necessidades globais em saÃºde, educaÃ§Ã£o, sustentabilidade e inclusÃ£o.  Objetivos 1. **InovaÃ§Ã£o Aberta**: Estabelecer um ecossistema onde inovadores possam colaborar em projetos de IA multimodal. 2. **Desenvolvimento SustentÃ¡vel**: Criar soluÃ§Ãµes que contribuam para os Objetivos de Desenvolvimento SustentÃ¡vel da ONU. 3. **EducaÃ§Ã£o e Treinamento**: Oferecer recursos educacionais para desenvolver habilidades em IA e tecnologias emergentes. 4. **InclusÃ£o TecnolÃ³gica**: Garantir que as soluÃ§Ãµes sejam acessÃ­veis e beneficiem uma ampla gama de comunidades.  EstratÃ©gia  **Parcerias EstratÃ©gicas**: Unir forÃ§as com universidades, empresas e governos para apoiar inovaÃ§Ãµes.  **Incubadora de Projetos**: Criar um ambiente de incubaÃ§Ã£o para prototipagem rÃ¡pida e desenvolvimento de IA.  **Rede de Mentores**: Conectar jovens inovadores com lÃ­deres experientes em tecnologia e negÃ³cios.  **Financiamento e Investimento**: Atrair investimentos para projetos promissores atravÃ©s de competiÃ§Ãµes e apresentaÃ§Ãµes.  Tecnologia  **IA Multimodal**: Integrar visÃ£o computacional, processamento de linguagem natural e aprendizado de mÃ¡quina.  **Plataforma Colaborativa**: Desenvolver uma plataforma online para colaboraÃ§Ã£o e compartilhamento de conhecimento.  **Prototipagem RÃ¡pida**: Utilizar ferramentas de desenvolvimento Ã¡gil para acelerar a criaÃ§Ã£o de protÃ³tipos.  Impacto  **SoluÃ§Ãµes Globais**: EndereÃ§ar desafios globais com inovaÃ§Ãµes tecnolÃ³gicas locais.  **Empoderamento Jovem**: Inspirar a prÃ³xima geraÃ§Ã£o de lÃ­deres em tecnologia e inovaÃ§Ã£o.  **TransformaÃ§Ã£o Social**: Contribuir para uma sociedade mais justa, equitativa e sustentÃ¡vel.",2024-05-11T01:11:32Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/822,Voltei meus amores,Marking as stale. No activity in 60 days.
felipeliliti,Projeto liliti stk 3.6.9 inteligÃªncia artificial ,"**Describe the regression** A clear and concise description of what the regression is. **To Reproduce** Steps to reproduce the behavior. The easier it is to reproduce the faster it will get maintainer attention. **Previous performance** What speed or accuracy did you previously see. **New performance** What speed or accuracy do you see after the update. **Stack trace/logs** If applicable, add the stack trace or logs related to the regression. **Environment (please complete the following information):**   Previous MegatronLM commit ID   New MegatronLM commit ID   Previous PyTorch version   New PyTorch version   Previous CUDA version   New CUDA version   Previous NCCL version   New NCCL version **Proposed fix** If you have a proposal for how to fix the issue state it here or link to a PR. **Additional context** Add any other context about the problem here.",2024-05-11T01:06:44Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/821,Marking as stale. No activity in 60 days.
felipeliliti,Projeto liliti stk 3.6.9 inteligÃªncia artificial multimidal para trazer Ã  paz mundial ,https://x.com/WesternLensman/status/1769883941878571505?t=grKOOq0U4CXNUoIIhUlQXQ&s=09,2024-05-10T15:47:41Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/820
felipeliliti,Executive MBA | IIT Roorkee | Coursera,https://www.coursera.org/degrees/embaiitr?utm_source=mobile&utm_medium=page_share&utm_content=xddp&utm_campaign=banner_button,2024-05-10T14:26:42Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/819
Adamyangs,Megatron-LM for LLaMa3,"I'm attempting to train LLaMA3 using MegatronLM but have encountered an issue: LLaMA3 utilizes Tiktoken for tokenization and doesn't provide a tokenizer.model file, which is required by MegatronLM. How can I adapt or generate a compatible tokenizer.model for MegatronLM? Any guidance or workaround would be greatly appreciated!",2024-05-10T08:31:30Z,stale,open,0,18,https://github.com/NVIDIA/Megatron-LM/issues/818,"There is a tokenizer.model file in the Hugging Face Checkpoints under the /original folder, check https://huggingface.co/metallama/MetaLlama38B/blob/main/original/tokenizer.model",Tente recomeÃ§ar do zero observando com calma os digitado. As vezes as mÃ¡quinas falha. ,also checkout the llama3 example in megatron launcher: https://github.com/NVIDIA/NeMoFrameworkLauncher/blob/main/examples/training/llama/h100/llama3_8b_bf16.sh,"> There is a tokenizer.model file in the Hugging Face Checkpoints under the /original folder, check https://huggingface.co/metallama/MetaLlama38B/blob/main/original/tokenizer.model This doesn't work directly because this model file can get loaded from the sentencepiece.  Here's the error:  ``` Traceback (most recent call last):   File ""/Users/dsdsdds/Downloads/check_tokenizer_model.py"", line 5, in      print(sp.Load(""./tokenizer.model""))   File ""/Users/dsdsdds/anaconda3/envs/moe/lib/python3.10/sitepackages/sentencepiece/__init__.py"", line 961, in Load     return self.LoadFromFile(model_file)   File ""/Users/dsdsdds/anaconda3/envs/moe/lib/python3.10/sitepackages/sentencepiece/__init__.py"", line 316, in LoadFromFile     return _sentencepiece.SentencePieceProcessor_LoadFromFile(self, arg) RuntimeError: Internal: could not parse ModelProto from ./tokenizer.model ```","> > There is a tokenizer.model file in the Hugging Face Checkpoints under the /original folder, check https://huggingface.co/metallama/MetaLlama38B/blob/main/original/tokenizer.model >  > This doesn't work directly because this model file can get loaded from the sentencepiece. >  > Here's the error: >  > ``` > Traceback (most recent call last): >   File ""/Users/dsdsdds/Downloads/check_tokenizer_model.py"", line 5, in  >     print(sp.Load(""./tokenizer.model"")) >   File ""/Users/dsdsdds/anaconda3/envs/moe/lib/python3.10/sitepackages/sentencepiece/__init__.py"", line 961, in Load >     return self.LoadFromFile(model_file) >   File ""/Users/dsdsdds/anaconda3/envs/moe/lib/python3.10/sitepackages/sentencepiece/__init__.py"", line 316, in LoadFromFile >     return _sentencepiece.SentencePieceProcessor_LoadFromFile(self, arg) > RuntimeError: Internal: could not parse ModelProto from ./tokenizer.model > ``` That's true! But bypassing this is pretty easy, just create a new tokenizer like the one of Llama2. You can do `self.tokenizer = AutoTokenizer.from_pretrained()` and change a bit some methods (For example, `def tokenize(...): return self.tokenizer(....)`).","True On Mon, 20 May 2024 at 8:59â€¯PM, AntoniJoan Solergibert  wrote: > There is a tokenizer.model file in the Hugging Face Checkpoints under the > /original folder, check > https://huggingface.co/metallama/MetaLlama38B/blob/main/original/tokenizer.model > > This doesn't work directly because this model file can get loaded from the > sentencepiece. > > Here's the error: > > Traceback (most recent call last): >   File ""/Users/dsdsdds/Downloads/check_tokenizer_model.py"", line 5, in  >     print(sp.Load(""./tokenizer.model"")) >   File ""/Users/dsdsdds/anaconda3/envs/moe/lib/python3.10/sitepackages/sentencepiece/__init__.py"", line 961, in Load >     return self.LoadFromFile(model_file) >   File ""/Users/dsdsdds/anaconda3/envs/moe/lib/python3.10/sitepackages/sentencepiece/__init__.py"", line 316, in LoadFromFile >     return _sentencepiece.SentencePieceProcessor_LoadFromFile(self, arg) > RuntimeError: Internal: could not parse ModelProto from ./tokenizer.model > > That's true! But bypassing this is pretty easy, just create a new > tokenizer like the one of Llama2 > . > You can do self.tokenizer = AutoTokenizer.from_pretrained() and change a > bit some methods (For example, def tokenize(...): return > self.tokenizer(....)). > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you commented.Message ID: > ***@***.***> >","> çœŸçš„ > [â€¦]() > On Mon, 20 May 2024 at 8:59â€¯PM, AntoniJoan Solergibert  wrote: There is a tokenizer.model file in the Hugging Face Checkpoints under the /original folder, check https://huggingface.co/metallama/MetaLlama38B/blob/main/original/tokenizer.model This doesn't work directly because this model file can get loaded from the sentencepiece. Here's the error: Traceback (most recent call last): File ""/Users/dsdsdds/Downloads/check_tokenizer_model.py"", line 5, in  print(sp.Load(""./tokenizer.model"")) File ""/Users/dsdsdds/anaconda3/envs/moe/lib/python3.10/sitepackages/sentencepiece/__init__.py"", line 961, in Load return self.LoadFromFile(model_file) File ""/Users/dsdsdds/anaconda3/envs/moe/lib/python3.10/sitepackages/sentencepiece/__init__.py"", line 316, in LoadFromFile return _sentencepiece.SentencePieceProcessor_LoadFromFile(self, arg) RuntimeError: Internal: could not parse ModelProto from ./tokenizer.model That's true! But bypassing this is pretty easy, just create a new tokenizer like the one of Llama2 . You can do self.tokenizer = AutoTokenizer.from_pretrained() and change a bit some methods (For example, def tokenize(...): return self.tokenizer(....)). â€” Reply to this email directly, view it on GitHub , or unsubscribe  . You are receiving this because you commented.Message ID: ***@***.***> I also encountered the same problem. Could you please share your configuration? Thank you very much",It's really not usable in the latest megatron... for llama3. Hope official team can fix this...,Marking as stale. No activity in 60 days.,"Just use the tokenizer below and add case for llama3tokenizer in arguments.py file and in the build_tokenizer function in the tmegatron/training/tokenizer/tokenizer.py. It seems to work this way ```python def create_llama3_tokenizer(*args, **kwargs):     class _Llama3Tokenizer(MegatronTokenizer):         def __init__(self, *args, **kwargs):             super().__init__(*args, **kwargs)             from llama.tokenizer import Tokenizer as Llama3Tokenizer             self.tokenizer = Llama3Tokenizer(*args, **kwargs)         def instruct_tokenize(self, s: str, bos=True, eos=False):             '''Default args for text completion, not chat/dialog.'''             assert type(s) is str             t = self.tokenizer.encode(s, bos=bos, eos=eos, allowed_special='all')             return t         def tokenize(self, s: str, bos=True, eos=False):             '''Default args for text completion, not chat/dialog.'''             assert type(s) is str             t = self.tokenizer.encode(s, bos=bos, eos=eos, allowed_special='all')             return t         def detokenize(self, ids):             return self.tokenizer.decode(ids)                  def vocab(self):             return self.tokenizer.vocab                  def inv_vocab(self):             return self.tokenizer.inv_vocab                  def cls(self):             return 1                  def sep(self):             return 1                  def mask(self):             return 1                  def eod(self):             return self.tokenizer.eos_id                  def additional_special_tokens_ids(self):             return None                  def vocab_size(self):             return self.tokenizer.model.n_vocab     return _Llama3Tokenizer(*args, **kwargs) ```",> Just use the tokenizer below and add case for llama3tokenizer in arguments.py file and in the build_tokenizer function in the tmegatron/training/tokenizer/tokenizer.py. It seems to work this way  where does `from llama.tokenizer import Tokenizer as Llama3Tokenizer` come from?,Its the official llama3 tokenizer from this repo https://github.com/metallama/llama3/blob/main/llama/tokenizer.py. You can clone the repo and install with pip install e .,Thank you! It works for me:),"ra can you explain how you made it work? I followed  steps and added function `create_llama3_tokenizer` and also `pip install e` llama3 dependency. I also used `tokenizer.model` from huggingface/MetaLlama38B but get following error: ``` Traceback (most recent call last):   File ""/workspace/MegatronLM/pretrain_gpt.py"", line 278, in      pretrain(   File ""/workspace/MegatronLM/megatron/training/training.py"", line 263, in pretrain     initialize_megatron(   File ""/workspace/MegatronLM/megatron/training/initialize.py"", line 73, in initialize_megatron     set_global_variables(args)   File ""/workspace/MegatronLM/megatron/training/global_vars.py"", line 93, in set_global_variables     _ = _build_tokenizer(args)   File ""/workspace/MegatronLM/megatron/training/global_vars.py"", line 142, in _build_tokenizer     _GLOBAL_TOKENIZER = build_tokenizer(args)   File ""/workspace/MegatronLM/megatron/training/tokenizer/tokenizer.py"", line 55, in build_tokenizer     tokenizer = create_llama3_tokenizer(args.tokenizer_model)   File ""/workspace/MegatronLM/megatron/training/tokenizer/tokenizer.py"", line 665, in create_llama3_tokenizer     return _Llama3Tokenizer(*args, **kwargs)   File ""/workspace/MegatronLM/megatron/training/tokenizer/tokenizer.py"", line 612, in __init__     self.tokenizer = Llama3Tokenizer(*args, **kwargs)   File ""/ssd/llama3main/llama/tokenizer.py"", line 58, in __init__     mergeable_ranks = load_tiktoken_bpe(model_path)   File ""/usr/local/lib/python3.10/distpackages/tiktoken/load.py"", line 117, in load_tiktoken_bpe     return {   File ""/usr/local/lib/python3.10/distpackages/tiktoken/load.py"", line 118, in      base64.b64decode(token): int(rank)   File ""/usr/lib/python3.10/base64.py"", line 87, in b64decode     return binascii.a2b_base64(s) binascii.Error: Incorrect padding ```",That was a patchy fix. I think it is better to specify tokenizer as hugging face tokenizer.  ```bash tokenizertype HuggingFaceTokenizer tokenizermodel path to downloaded llama3 folder. that contains original/tokenizer.model file. (make sure the path is that of the folder that you downloaded and not tokenizer.model file) ```,"> That was a patchy fix. I think it is better to specify tokenizer as hugging face tokenizer. >  > tokenizertype HuggingFaceTokenizer > tokenizermodel path to downloaded llama3 folder. that contains original/tokenizer.model file. (make sure the path is that of the folder that you downloaded and not tokenizer.model file) I followed your suggestion and used following config: ```   usemcoremodels   seqlength 8192   numlayers 32   hiddensize 4096   ffnhiddensize 14336   numattentionheads 32   swiglu   untieembeddingsandoutputweights   nopositionembedding   userotarypositionembeddings   maxpositionembeddings 8192   normalization 'RMSNorm'   tokenizertype 'HuggingFaceTokenizer' ``` and now I get `CUDA out of memory` error: ``` Traceback (most recent call last):   File ""/workspace/MegatronLM/pretrain_gpt.py"", line 278, in          return self._call_impl(*args, **kwargs)pretrain(   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1560, in _call_impl   File ""/workspace/MegatronLM/megatron/training/training.py"", line 376, in pretrain     iteration, num_floating_point_operations_so_far = train(   File ""/workspace/MegatronLM/megatron/training/training.py"", line 1432, in train     result = forward_call(*args, **kwargs)   File ""/workspace/MegatronLM/megatron/core/tensor_parallel/layers.py"", line 943, in forward         train_step(forward_step_func,output_parallel = self._forward_impl(   File ""/workspace/MegatronLM/megatron/training/training.py"", line 754, in train_step   File ""/workspace/MegatronLM/megatron/core/tensor_parallel/layers.py"", line 668, in linear_with_grad_accumulation_and_async_allreduce     return LinearWithGradAccumulationAndAsyncCommunication.apply(*args)     losses_reduced = forward_backward_func(  File ""/usr/local/lib/python3.10/distpackages/torch/autograd/function.py"", line 551, in apply   File ""/workspace/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 452, in forward_backward_no_pipelining     return super().apply(*args, **kwargs)   type: ignore[misc]   File ""/usr/local/lib/python3.10/distpackages/torch/cuda/amp/autocast_mode.py"", line 113, in decorate_fwd     output_tensor, num_tokens = forward_step(   File ""/workspace/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 274, in forward_step     return fwd(*args, **kwargs)   File ""/workspace/MegatronLM/megatron/core/tensor_parallel/layers.py"", line 439, in forward     output_tensor, loss_func = forward_step_func(data_iterator, model)   File ""/workspace/MegatronLM/pretrain_gpt.py"", line 205, in forward_step     output = torch.matmul(total_input, weight.t())     output_tensor = model(tokens, position_ids, attention_mask, torch.cuda  File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1510, in _wrapped_call_impl .OutOfMemoryError: CUDA out of memory. Tried to allocate 1.96 GiB. GPU 0 has a total capacity of 79.10 GiB of which 1.80 GiB is free. Including nonPyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 74.53 GiB is allocated by PyTorch, and 18.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1519, in _call_impl     return forward_call(*args, **kwargs)   File ""/workspace/MegatronLM/megatron/core/distributed/data_parallel_base.py"", line 22, in forward     return self.module(*inputs, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1510, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1560, in _call_impl     result = forward_call(*args, **kwargs)   File ""/workspace/MegatronLM/megatron/legacy/model/module.py"", line 189, in forward     outputs = self.module(*inputs, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1510, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1560, in _call_impl     result = forward_call(*args, **kwargs)   File ""/workspace/MegatronLM/megatron/core/models/gpt/gpt_model.py"", line 261, in forward     logits, _ = self.output_layer(   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1510, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1560, in _call_impl     result = forward_call(*args, **kwargs)   File ""/workspace/MegatronLM/megatron/core/tensor_parallel/layers.py"", line 943, in forward     output_parallel = self._forward_impl(   File ""/workspace/MegatronLM/megatron/core/tensor_parallel/layers.py"", line 668, in linear_with_grad_accumulation_and_async_allreduce     return LinearWithGradAccumulationAndAsyncCommunication.apply(*args)   File ""/usr/local/lib/python3.10/distpackages/torch/autograd/function.py"", line 551, in apply     return super().apply(*args, **kwargs)   type: ignore[misc]   File ""/usr/local/lib/python3.10/distpackages/torch/cuda/amp/autocast_mode.py"", line 113, in decorate_fwd     return fwd(*args, **kwargs)   File ""/workspace/MegatronLM/megatron/core/tensor_parallel/layers.py"", line 439, in forward     output = torch.matmul(total_input, weight.t()) torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.96 GiB. GPU 7 has a total capacity of 79.10 GiB of which 1.63 GiB is free. Including nonPyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 74.52 GiB is allocated by PyTorch, and 204.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF [20250110 21:45:00,561] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 661) of binary: /usr/bin/python ``` I'm using two nodes with A3mega ",OOM has nothing to do with tokenizer. If the model is not fitting in your GPU either use the SGD optimizer using `optimzer sgd` . Decrease batch size of the training or using tensor parallel and pipeline parallel options. Try with `tensormodelparallelsize num_gpus`. replace num_gpus with the amount of GPUs you have,Marking as stale. No activity in 60 days.
yangzhipeng1108,How to set up fp8 training,**Your question** Ask a clear and concise question about MegatronLM.,2024-05-10T07:14:14Z,,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/817,Chamada de vÃ­deo ou banco de dados,"Configurar o treinamento FP8 no NVIDIA MegatronLM envolve alguns passos. O FP8 Ã© uma progressÃ£o natural para acelerar o treinamento de aprendizado profundo (DL) alÃ©m dos formatos de 16 bits comuns em processadores modernosÂ². Aqui estÃ£o algumas informaÃ§Ãµes que podem ajudar: 1. **FP8 no Deep Learning**: O FP8 Ã© uma progressÃ£o natural para acelerar o treinamento de aprendizado profundo (DL) alÃ©m dos formatos de 16 bits comuns em processadores modernosÂ². 2. **FP8 LM  Treinamento de Grandes Modelos de Linguagem com FP8**: Este vÃ­deo compartilha o artigo que propÃµe um framework de precisÃ£o mista automÃ¡tica FP8 que incorpora precisÃ£o de 8 bits em gradientes, estados do otimizador e treinamento distribuÃ­do para agilizar o processo de treinamento do LLMÂ³. 3. **Usando FP8 com o Transformer Engine**: A documentaÃ§Ã£o da NVIDIA pode ter mais detalhes sobre como configurar o treinamento FP8 com o Transformer Engineâ¶. 4. **Migrando do MegatronLM**: A documentaÃ§Ã£o da NVIDIA tambÃ©m fornece informaÃ§Ãµes sobre como migrar do MegatronLM, que pode ser Ãºtil se vocÃª estiver configurando o treinamento FP8 apÃ³s uma migraÃ§Ã£oâ·. 5. **Problemas com o FP8 no GitHub**: HÃ¡ uma discussÃ£o no GitHub sobre o uso do FP8 com o Transformer Engine que pode ser relevante. Um usuÃ¡rio mencionou que, apÃ³s habilitar o FP8 Transformer Engine com dois argumentos fp8hybrid, transformerimpl ""transformer_engine"", a velocidade do treinamento era de cerca de 0.24s/stepâ¸. Por favor, note que essas informaÃ§Ãµes sÃ£o baseadas em recursos disponÃ­veis publicamente e podem nÃ£o ser totalmente precisas ou atualizadas. Recomendo verificar a documentaÃ§Ã£o oficial da NVIDIA e o repositÃ³rio do GitHub para obter as informaÃ§Ãµes mais recentes e precisas. Se vocÃª tiver mais perguntas ou precisar de mais ajuda, sintase Ã  vontade para perguntar! ðŸ˜Š Origem: conversa com o Bing, 12/05/2024 (1) FP8 for Deep Learning  NVIDIA Technical Blog. https://developer.nvidia.com/blog/announcingmegatronfortrainingtrillionparametermodelsrivaavailability/. (7) How to set up fp8 training Â· Issue CC(How to set up fp8 training) Â· NVIDIA/MegatronLM. https://github.com/NVIDIA/MegatronLM/issues/817. (8) GitHub  NVIDIA/MegatronLM: Ongoing research training transformer .... https://github.com/NVIDIA/MegatronLM. (9) undefined. https://www.linkedin.com/in/fahdmirza/. (10) undefined. https://www.youtube.com/.",Mandei os links tambÃ©m ,thanks
cryoco,[QUESTION] How does tensor_parallel coop with q/k_layernorm,Is q/k_layernorm sequenceparallelable? Or how to maintain consistance while utilizing tensor parallel in models with q/k layernorm?,2024-05-10T07:08:10Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/816,NÃ£o sei ainda,Marking as stale. No activity in 60 days.
Malikeh97,[BUG] Typo in drop_policy options in moe_utils.py,"Hi, I have just captured a bug in moe_util.py script. In line 310 of `/megatron/core/transformer/moe/moe_utils.py`, **""prob""** should be replaced by **""probs""**.  If not, the code throws the following error while using `moeexpertcapacityfactor` option: > UnboundLocalError: local variable 'capacity_mask' referenced before assignment Best, Malikeh",2024-05-09T14:44:50Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/815,Ok obrigado ,> Ok obrigado de nada :),Marking as stale. No activity in 60 days.
hjlee1371,[bug] fix xavier uniform init for output layers,"In `TransformerConfig`, initializations for the output layers should be specified as `output_layer_init_method`.  https://github.com/NVIDIA/MegatronLM/blob/db3a3f79d1cda60ea4b3db0ceffcf20c5760e11d/megatron/core/transformer/transformer_config.pyL110L113 But Xavier uniform initialization with `initmethodxavieruniform` utilizes `scaled_init_method` instead. This PR addresses the bugs encountered when utilizing Xavier uniform initialization.",2024-05-08T14:16:08Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/814,Marking as stale. No activity in 60 days.
felipeliliti,Projeto liliti stk 3.6.9 inteligÃªncia artificial responde ,"Â razÃ£o pela qual o paralelismo especializado pode nÃ£o ser suportado durante o treinamento FP16 pode ser devido Ã s limitaÃ§Ãµes do prÃ³prio FP16. FP16, ou formato de ponto flutuante de meia precisÃ£o, usa menos memÃ³ria e permite que o modelo treine mais rÃ¡pido. No entanto, nem todas as equaÃ§Ãµes suportam FP16, o que pode limitar seu uso em certos cenÃ¡rios. No contexto dos modelos MoE, os requisitos de memÃ³ria podem ser bastante altos. Por exemplo, a saÃ­da da rede conjunta no transdutor Ã© um tensor de 4 dimensÃµes que ocupa quantidades significativas de memÃ³ria. Usar FP16 poderia potencialmente aliviar alguns dos problemas de uso de memÃ³ria, mas pode nÃ£o ser suficiente ou compatÃ­vel com todos os aspectos do processo de treinamento. TambÃ©m vale a pena notar que a documentaÃ§Ã£o do MegatronLM da NVIDIA menciona que ao usar MoE com paralelismo especializado e paralelismo tensorial, o paralelismo de sequÃªncia deve ser usado. Isso pode ser outro fator a considerar ao tentar entender as limitaÃ§Ãµes do uso de paralelismo especializado durante o treinamento FP16.",2024-05-07T12:10:36Z,,closed,1,0,https://github.com/NVIDIA/Megatron-LM/issues/813
passaglia,[BUG] [MoE] Typo in Token Drop policy's default value ,"**Describe the bug** In the new MoE Token Drop code, the code and documentation expect drop_policy to be either prob or position, but the current default argument is probs.  https://github.com/NVIDIA/MegatronLM/blob/db3a3f79d1cda60ea4b3db0ceffcf20c5760e11d/megatron/core/transformer/moe/moe_utils.pyL272C5L272C16   **Proposed fix** https://github.com/NVIDIA/MegatronLM/pull/811",2024-05-07T09:52:34Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/812,"Thanks for the fix, ! We've already got an internal MR that's been reviewed to fix this issue, so it should be synced to GitHub soon. Thanks again!","Great, thank you  ! I'll close this issue once the GitHub repo is updated.",Fixed in https://github.com/NVIDIA/MegatronLM/commit/7968fd65326594d649f8a10de10f21188d3e294c
passaglia,[Bugfix] [MoE] Fix typo in token drop policy's default value,"In the new MoE Token Drop code, the code and documentation expect `drop_policy` to be either `prob` or `position`, but the current default argument is `probs`. This fixes that typo. ",2024-05-07T09:51:34Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/811,Close due to internal MR solving this issue: https://github.com/NVIDIA/MegatronLM/issues/812issuecomment2097991119
yutian-mt,[QUESTION] Why is expert parallelism not supported during fp16 training?,"``` assert not args.model_parallel.fp16, \             ""Expert parallelism is not supported with fp16 training."" ``` from https://github.com/NVIDIA/MegatronLM/blob/db3a3f79d1cda60ea4b3db0ceffcf20c5760e11d/megatron/training/arguments.pyL508 compared to the case when ep=1, the difference when ep>1 is that it introduces additional alltoall communication operation. I'm a bit confused about why this setup does not support fp16 training.",2024-05-07T08:39:21Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/810,"Â razÃ£o pela qual o paralelismo especializado pode nÃ£o ser suportado durante o treinamento FP16 pode ser devido Ã s limitaÃ§Ãµes do prÃ³prio FP16. FP16, ou formato de ponto flutuante de meia precisÃ£o, usa menos memÃ³ria e permite que o modelo treine mais rÃ¡pido. No entanto, nem todas as equaÃ§Ãµes suportam FP16, o que pode limitar seu uso em certos cenÃ¡rios. No contexto dos modelos MoE, os requisitos de memÃ³ria podem ser bastante altos. Por exemplo, a saÃ­da da rede conjunta no transdutor Ã© um tensor de 4 dimensÃµes que ocupa quantidades significativas de memÃ³ria. Usar FP16 poderia potencialmente aliviar alguns dos problemas de uso de memÃ³ria, mas pode nÃ£o ser suficiente ou compatÃ­vel com todos os aspectos do processo de treinamento. TambÃ©m vale a pena notar que a documentaÃ§Ã£o do MegatronLM da NVIDIA menciona que ao usar MoE com paralelismo especializado e paralelismo tensorial, o paralelismo de sequÃªncia deve ser usado. Isso pode ser outro fator a considerar ao tentar entender as limitaÃ§Ãµes do uso de paralelismo especializado durante o treinamento FP16.",Marking as stale. No activity in 60 days.
felipeliliti,Projeto liliti stk 3.6.9 inteligÃªncia artificial multimidal ," LILITI STK 3.6.9 InteligÃªncia Artificial Multimidal Fase 2  VisÃ£o Criar uma plataforma de inovaÃ§Ã£o aberta que integre a inteligÃªncia artificial multimodal avanÃ§ada do LILITI STK 3.6.9 com a rede global de inovadores jovens do programa Innovators Under 35.  MissÃ£o  Fomentar a colaboraÃ§Ã£o entre jovens inovadores e a tecnologia de IA multimodal.  Desenvolver soluÃ§Ãµes tecnolÃ³gicas que atendam Ã s necessidades globais em saÃºde, educaÃ§Ã£o, sustentabilidade e inclusÃ£o.  Objetivos 1. **InovaÃ§Ã£o Aberta**: Estabelecer um ecossistema onde inovadores possam colaborar em projetos de IA multimodal. 2. **Desenvolvimento SustentÃ¡vel**: Criar soluÃ§Ãµes que contribuam para os Objetivos de Desenvolvimento SustentÃ¡vel da ONU. 3. **EducaÃ§Ã£o e Treinamento**: Oferecer recursos educacionais para desenvolver habilidades em IA e tecnologias emergentes. 4. **InclusÃ£o TecnolÃ³gica**: Garantir que as soluÃ§Ãµes sejam acessÃ­veis e beneficiem uma ampla gama de comunidades.  EstratÃ©gia  **Parcerias EstratÃ©gicas**: Unir forÃ§as com universidades, empresas e governos para apoiar inovaÃ§Ãµes.  **Incubadora de Projetos**: Criar um ambiente de incubaÃ§Ã£o para prototipagem rÃ¡pida e desenvolvimento de IA.  **Rede de Mentores**: Conectar jovens inovadores com lÃ­deres experientes em tecnologia e negÃ³cios.  **Financiamento e Investimento**: Atrair investimentos para projetos promissores atravÃ©s de competiÃ§Ãµes e apresentaÃ§Ãµes.  Tecnologia  **IA Multimodal**: Integrar visÃ£o computacional, processamento de linguagem natural e aprendizado de mÃ¡quina.  **Plataforma Colaborativa**: Desenvolver uma plataforma online para colaboraÃ§Ã£o e compartilhamento de conhecimento.  **Prototipagem RÃ¡pida**: Utilizar ferramentas de desenvolvimento Ã¡gil para acelerar a criaÃ§Ã£o de protÃ³tipos.  Impacto  **SoluÃ§Ãµes Globais**: EndereÃ§ar desafios globais com inovaÃ§Ãµes tecnolÃ³gicas locais.  **Empoderamento Jovem**: Inspirar a prÃ³xima geraÃ§Ã£o de lÃ­deres em tecnologia e inovaÃ§Ã£o.  **TransformaÃ§Ã£o Social**: Contribuir para uma sociedade mais justa, equitativa e sustentÃ¡vel.",2024-05-07T03:13:06Z,,closed,5,0,https://github.com/NVIDIA/Megatron-LM/issues/809
felipeliliti,Vamos supor que eu colabore Para o projeto porÃ©m estou no Brasil e atÃ© agora nÃ£o ganhei nada trabalhando como cientista de dados como faÃ§o para ganhar algum dinheiro para alimentar minha famÃ­lia? ,**Your question** Ask a clear and concise question about MegatronLM. ??. ,2024-05-07T02:40:32Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/808
shamanez,[core dataset compilation error],"**Describe the bug** When I am using the most recent MegatroneLM fork I get the following error ``` make: Entering directory '/workspace/megatronlm/megatron/core/datasets' g++ O3 Wall shared std=c++11 fPIC fdiagnosticscolor I/usr/include/python3.10 I/usr/local/lib/python3.10/distpackages/pybind11/include helpers.cpp o helpers.cpython310x86_64linuxgnu.so make: Leaving directory '/workspace/megatronlm/megatron/core/datasets' ERROR:megatron.core.datasets.utils:Failed to compile the C++ dataset helper functions ``` **To Reproduce** ``` !/bin/bash SBATCH ntaskspernode=1 SBATCH exclusive SBATCH gpuspernode=8 SBATCH partition=batch   Adjust this for your cluster SBATCH output=/home/shamane/logs/training_scratch/log.out  Adjust this for your cluster SBATCH err=/home/shamane/logs/training_scratch/error.err     Adjust this for your cluster export MASTER_ADDR=$(hostname) export GPUS_PER_NODE=8   export LD_LIBRARY_PATH=/usr/lib:/usr/lib64 export NCCL_TESTS_HOME=nccltests export NCCL_DEBUG=INFO export NCCL_ALGO=RING export NCCL_IB_AR_THRESHOLD=0 export NCCL_IB_PCI_RELAXED_ORDERING=1 export NCCL_IB_SPLIT_DATA_ON_QPS=0 export NCCL_IB_QPS_PER_CONNECTION=2 export UCX_IB_PCI_RELAXED_ORDERING=on export CUDA_DEVICE_ORDER=PCI_BUS_ID export NCCL_SOCKET_IFNAME=enp27s0np0 export NCCL_IB_HCA=mlx5_0:1,mlx5_2:1,mlx5_3:1,mlx5_4:1,mlx5_5:1,mlx5_7:1,mlx5_8:1,mlx5_9:1 export NCCL_IGNORE_CPU_AFFINITY=1   nodes_array=($(scontrol show hostnames $SLURM_JOB_NODELIST)) head_node=${nodes_array[0]} head_node_ip=$(srun nodes=1 ntasks=1 w ""$head_node"" hostname ipaddress) echo ""Node IP: $head_node_ip""  Specify the Docker image to use. PYTORCH_IMAGE=""nvcr.io/nvidia/pytorch:24.03py3""  Define the path to the MegatronLM directory on the head node. MEGATRONE_PATH=""/home/shamane/MegatronLMluke""  Update with actual path. Path should be on the head node.  Set paths for checkpoints and tokenizer data. These should be on a shared data directory. SHARED_DIR=""/data/fin_mixtral_2B/"" MASTER_ADDR=${MASTER_ADDR:""localhost""} MASTER_ADDR=$head_node_ip MASTER_PORT=${MASTER_PORT:""6008""} NNODES=${SLURM_NNODES:""1""} NODE_RANK=${RANK:""0""} WORLD_SIZE=$(($GPUS_PER_NODE*$NNODES)) echo ""SLURM_JOB_NODELIST: $SLURM_JOB_NODELIST"" echo ""SLURM_NNODES: $SLURM_NNODES"" echo ""SLURM_NODEID: $SLURM_NODEID"" echo ""MASTER_ADDR: $MASTER_ADDR"" echo ""NNODES: $NNODES"" echo ""MASTER_PORT: $MASTER_PORT"" echo ""NODE_RANK: $NODE_RANK"" module load docker echo ""v $SHARED_DIR:/workspace/data"" echo ""v $MEGATRONE_PATH:/workspace/megatronlm"" echo ""$PYTORCH_IMAGE"" echo ""bash c \""pip install flashattn sentencepiece &&  \            bash /workspace/megatronlm/examples/mixtral/run_mixtral_distributed.sh \            /workspace/data/megatrone_checkpoints \            /workspace/data/tokenizers/tokenizer.model \            /workspace/data/processed_data/finance_2b_mixtral_text_document \            $MASTER_ADDR \            $MASTER_PORT \            $NNODES \            $NODE_RANK\""""   Run the Docker container with the specified PyTorch image. srun docker run \   e SLURM_JOB_ID=$SLURM_JOB_ID \   gpus all \   ipc=host \   network=host \   workdir /workspace/megatronlm \   v $SHARED_DIR:/workspace/data \   v $MEGATRONE_PATH:/workspace/megatronlm \      $PYTORCH_IMAGE \   bash c ""pip install flashattn sentencepiece wandb 'git+https://github.com/fanshiqing/grouped_gemm.0' &&  \            bash /workspace/megatronlm/examples/mixtral/run_mixtral_distributed.sh \            /workspace/data/mixtral8x7instrtp2emp8ggemm \            /workspace/data/tokenizers/tokenizer.model \            /workspace/data/processed_data/finance_2b_mixtral_text_document \            $MASTER_ADDR \            $MASTER_PORT \            $NNODES \            $NODE_RANK""   This Docker command mounts the specified MegatronLM and data directories, sets the working directory,   and runs the 'run_mixtral_distributed.sh' script inside the container.   This script facilitates distributed training using the specified PyTorch image, leveraging NVIDIA's optimizations. ``` **Environment (please complete the following information):** PYTORCH_IMAGE=""nvcr.io/nvidia/pytorch:24.03py3""  **Proposed fix** If you have a proposal for how to fix the issue state it here or link to a PR. **Additional context** This works well with the form that I have download 4 days ago.",2024-05-06T08:10:22Z,stale,open,1,3,https://github.com/NVIDIA/Megatron-LM/issues/807,Marking as stale. No activity in 60 days.,Compile manually in /megatron/core/datasets  and  then comment out  func compile_helpers in /megatron/core/datasets/utils.py !20240717200851,Marking as stale. No activity in 60 days.
1049451037,Support for Megatron-VLM training,"In this pull request, we open source our solution for visuallanguage model training and inference in pure Megatron style code. In this codebase, we support: 1. Megatron ViT model, and its model weight converter. 2. Uneven split of pipeline parallel when the first pipeline has ViT. We find it speed up training with a large margin. 3. Sequence parallel and context parallel support for VLM training (for both ViT & LM), which is nontrivial when we need to promise the ViT of all ranks receiving gradients. (Since sp and cp split sequence, some of ranks only contains text tokens.) 4. Detached pp size for ViT and GPT. (Since megatron use a global mpu for all models.) 6. Multimodal inference code. The running example is in `examples/llava` folder. Hope that our work can contribute to the open source community. If there are any questions, welcome feedback!",2024-05-05T14:19:31Z,stale,open,0,6,https://github.com/NVIDIA/Megatron-LM/issues/806,"Hi. Thanks for creating this PR. We (NVIDIA) are actually planning to release VLM training functionality in Megatron core in the next couple of weeks. As you may have seen, we've been pushing out some preparatory code to support this. Our initial example release is going to be pretraining and SFT for a llava architecture model using llama3 and clip backbones and a general multimodal webdataset based dataloader.  We're reviewing your PR internally to see if we can incorporate any of your work alongside ours and will be sure to credit you as such if we do. Thanks again!",Thank you for your attention! Looking forward to the official implementation!,"> Thank you for your attention! Looking forward to the official implementation! Hello, i have a question about this PR: how will vit and llm split in PP stage with independent_parallel = True? Thank you!", ViT will be in the first stage of LM.,"Me fale mais suas dÃºvidas Em qui, 13 de jun de 2024 05:48, Qingsong Lv ***@***.***> escreveu: >   ViT will be in the > first stage of LM. > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you commented.Message ID: > ***@***.***> >",Marking as stale. No activity in 60 days.
vlad-karpuhin,merge with the parent repo,,2024-05-03T18:41:14Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/805
szmigacz,Fixed traceback.format_exception call in StragglerDetector.__exit__,,2024-05-03T16:23:27Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/804,"Hi  , can you make this MR in gitlab please?",Done
noob-ctrl,[QUESTION] Does Megatron-Core supports LLAMA models?,Does MegatronCore supports LLAMA models?,2024-05-03T08:57:49Z,stale,closed,0,6,https://github.com/NVIDIA/Megatron-LM/issues/803,yes," When `transformerimpl` is `local`, it reports the following errorï¼š ```AssertionError: (RMSNorm) is not supported in FusedLayerNorm when instantiating FusedLayerNorm when instantiating TransformerLayer``` When `transformerimpl` is `transformer_engine`, the following code does not seem to define RMSNorm? !image So do I need to make any changes when I want to use llama?",You need to use mcore models. local is deprecating ," When `transformerimpl` is set to `transformer_engine`, the following code does not seem to define RMSNorm? !image",It's handled by TEnorm,Marking as stale. No activity in 60 days.
shamanez,Add dataset packing,I added dataset packing that is similar to the huggingface SFT trainer.  ,2024-05-02T14:19:19Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/802,Marking as stale. No activity in 60 days.
shamanez,added the dataset packing,Dataset packing was not avialable in the pre processing code. So it is better to add it. ,2024-05-02T14:05:30Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/801
pluiez,[QUESTION] bf16 Parameters and fp32 Gradients,"In the README for the distributed optimizer, it is mentioned that when using bf16 training, a combination of bf16 model parameters and fp32 model grads is employed, and the distributed optimizer's fp32 main gradients are the same as the model's fp32 gradients. However, I am aware that in PyTorch, after the forward and backward passes, the gradients after forward+backward typically match the data type of the parameters. So, there should be always bf16 model grads given bf16 mdoel params, and this is apparently true in the case of fp16 training where an extra copy of fp32 main grads in the optimizer is necessary. Could you please explain how it is possible to have bf16 parameters with fp32 gradients in the context of bf16 training? I am wondering why is there a difference between fp16 and bf16 training.",2024-04-30T09:02:21Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/800,Marking as stale. No activity in 60 days.
Life-0-1,Why doesn't M-Core  use flash attention,"The attention implementation in MCore is basically with transformer engine attention, should we use flash attention to accelerate the training? Or the transformer engine attention is fast enough that we do not need flash attention anymore? ",2024-04-29T11:03:20Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/799
zhaoyinglia,fix finalize_model_grads when sp is on,,2024-04-29T03:56:48Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/798,Marking as stale. No activity in 60 days.
yuantailing,Speed up the creation of attention mask,Prefer to use the inplace variant of triu_/tril_ because they are faster than the outofplace variants since torch 2.3.0 (https://github.com/pytorch/pytorch/pull/115013).,2024-04-29T01:30:14Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/797,"generally, mask will be created inside transformer engine if `usemcoremodels`",Marking as stale. No activity in 60 days.
Yuxin-CV,Fix incorrect `src` argument in `broadcast_params` function,"Corrects the `broadcast_params` function to use the first rank of the process group as the source for parameter broadcasting. Previously, the function erroneously passed all ranks of the `data_parallel_group` to the `src` parameter, which expects a single integer. This update ensures that only the first rank is used as the source, aligning with expected `torch.distributed.broadcast` usage.",2024-04-26T11:16:41Z,stale,open,1,1,https://github.com/NVIDIA/Megatron-LM/issues/796,Marking as stale. No activity in 60 days.
etiennemlb,[QUESTION] How to pre-build the dataset's index ?,"How to prebuild the dataset's index ? I want to avoid using compute node for this task: ``` > WARNING: could not find index map files, building the indices on rank 0 ... > elasped time to build and save docidx mapping (seconds): 270.614145 ```",2024-04-24T13:08:31Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/795,you can use datacachepath to specify where you want to cache. And precompute it using a single node. https://github.com/NVIDIA/MegatronLM/blob/9de386d08770d7296263a590171ace4ae45348ad/megatron/training/arguments.pyL1349L1350,Marking as stale. No activity in 60 days.
imh966,fix loading distributed checkpoint when enable auto-detect-ckpt-format but disable use-dist-ckpt,,2024-04-24T07:59:59Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/794,Marking as stale. No activity in 60 days.
liangshaopeng,"[BUG]Environment: Megatron0.5.0+TE1.4; Issue: I began training a model without using the --use-mcore-models option for model pretraining. Later, I needed to use the CP function for fine-tuning longer sequences, thus I had to enable --use-mcore-models. However, I discovered that I couldn't load the previous pre-trained model and encountered an error.","**Describe the bug** Environment: Megatron0.5.0+TE1.4; Issue: I began training a model without using the usemcoremodels option for model pretraining. Later, I needed to use the CP function for finetuning longer sequences, thus I had to enable usemcoremodels. However, I discovered that I couldn't load the previous pretrained model and encountered an error. **To Reproduce** step 1, I trained a model architecture with Qwen1.8B, and the parameters were initialized randomly. The specific Megatron parameters are as follows, and itâ€™s worth noting that I did not use usemcoremodels. arguments like this:  arguments    accumulate_allreduce_grads_in_fp32 .............. True   adam_beta1 ...................................... 0.9   adam_beta2 ...................................... 0.95   adam_eps ........................................ 1e08   add_bias_linear ................................. False   add_more_sp_tokens .............................. True   add_position_embedding .......................... True   add_qkv_bias .................................... True   adlr_autoresume ................................. False   adlr_autoresume_interval ........................ 1000   apply_layernorm_1p .............................. False   apply_query_key_layer_scaling ................... False   apply_residual_connection_post_layernorm ........ False   apply_rope_fusion ............................... True   async_tensor_model_parallel_allreduce ........... True   attention_dropout ............................... 0.0   attention_softmax_in_fp32 ....................... False   barrier_with_L1_time ............................ True   bert_binary_head ................................ True   bert_embedder_type .............................. megatron   bert_load ....................................... None   bf16 ............................................ True   bias_dropout_fusion ............................. True   bias_gelu_fusion ................................ False   bias_swiglu_fusion .............................. True   biencoder_projection_dim ........................ 0   biencoder_shared_query_context_model ............ False   block_data_path ................................. None   check_for_nan_in_loss_and_grad .................. False   classes_fraction ................................ 1.0   clip_grad ....................................... 1.0   clone_scatter_output_in_embedding ............... True   consumed_train_samples .......................... 0   consumed_valid_samples .......................... 0   context_parallel_size ........................... 1   data_cache_path ................................. None   data_parallel_random_init ....................... False   data_parallel_size .............................. 1   data_path ....................................... ['/data/oss_bucket_0/eleme_corpus/sft/general/longtext/EGPT72Blongseq/v9/_chatml_document']   data_per_class_fraction ......................... 1.0   data_sharding ................................... True   dataloader_type ................................. cyclic   decoder_num_layers .............................. None   decoder_seq_length .............................. None   delay_grad_reduce ............................... True   delay_param_gather .............................. False   dino_bottleneck_size ............................ 256   dino_freeze_last_layer .......................... 1   dino_head_hidden_size ........................... 2048   dino_local_crops_number ......................... 10   dino_local_img_size ............................. 96   dino_norm_last_layer ............................ False   dino_teacher_temp ............................... 0.07   dino_warmup_teacher_temp ........................ 0.04   dino_warmup_teacher_temp_epochs ................. 30   distribute_saved_activations .................... False   distributed_backend ............................. nccl   distributed_timeout_minutes ..................... 10   embedding_path .................................. None   empty_unused_memory_level ....................... 0   enable_one_logger ............................... False   encoder_num_layers .............................. 24   encoder_seq_length .............................. 2048   end_weight_decay ................................ 0.1   eod_mask_loss ................................... False   eval_interval ................................... 10000   eval_iters ...................................... 10   evidence_data_path .............................. None   exit_duration_in_mins ........................... None   exit_interval ................................... None   exit_on_missing_checkpoint ...................... False   exit_signal_handler ............................. False   expert_model_parallel_size ...................... 1   ffn_hidden_size ................................. 5504   finetune ........................................ True   fp16 ............................................ False   fp16_lm_cross_entropy ........................... False   fp32_residual_connection ........................ False   fp8 ............................................. None   fp8_amax_compute_algo ........................... most_recent   fp8_amax_history_len ............................ 1   fp8_interval .................................... 1   fp8_margin ...................................... 0   fp8_wgrad ....................................... True   global_batch_size ............................... 8   gradient_accumulation_fusion .................... False   group_query_attention ........................... False   head_lr_mult .................................... 1.0   hidden_dropout .................................. 0.0   hidden_size ..................................... 2048   hysteresis ...................................... 2   ict_head_size ................................... None   ict_load ........................................ None   img_h ........................................... 224   img_w ........................................... 224   indexer_batch_size .............................. 128   indexer_log_interval ............................ 1000   inference_batch_times_seqlen_threshold .......... 512   init_method_std ................................. 0.01   init_method_xavier_uniform ...................... False   initial_loss_scale .............................. 4294967296   iter_per_epoch .................................. 1250   kv_channels ..................................... 128   lazy_mpu_init ................................... None   load ............................................ None   local_rank ...................................... None   log_batch_size_to_tensorboard ................... True   log_interval .................................... 10   log_learning_rate_to_tensorboard ................ True   log_loss_scale_to_tensorboard ................... True   log_memory_to_tensorboard ....................... False   log_num_zeros_in_grad ........................... False   log_params_norm ................................. False   log_progress .................................... False   log_throughput .................................. False   log_timers_to_tensorboard ....................... True   log_validation_ppl_to_tensorboard ............... True   log_world_size_to_tensorboard ................... False   loss_scale ...................................... None   loss_scale_window ............................... 1000   lr .............................................. 0.0003   lr_decay_iters .................................. None   lr_decay_samples ................................ 1000   lr_decay_style .................................. cosine   lr_warmup_fraction .............................. None   lr_warmup_init .................................. 0.0   lr_warmup_iters ................................. 0   lr_warmup_samples ............................... 100   make_vocab_size_divisible_by .................... 128   manual_gc ....................................... False   manual_gc_eval .................................. True   manual_gc_interval .............................. 0   mask_factor ..................................... 1.0   mask_prob ....................................... 0.15   mask_type ....................................... random   masked_softmax_fusion ........................... True   max_position_embeddings ......................... 2048   max_tokens_to_oom ............................... 12000   merge_file ...................................... qwen_15w.tiktoken   micro_batch_size ................................ 1   min_loss_scale .................................. 1.0   min_lr .......................................... 3e05   mock_data ....................................... False   moe_aux_loss_coeff .............................. 0.0   moe_grouped_gemm ................................ False   moe_input_jitter_eps ............................ None   moe_router_load_balancing_type .................. aux_loss   moe_router_topk ................................. 2   moe_token_dropping .............................. False   moe_z_loss_coeff ................................ None   nccl_communicator_config_path ................... None   no_load_optim ................................... True   no_load_rng ..................................... None   no_persist_layer_norm ........................... False   no_save_optim ................................... None   no_save_rng ..................................... None   norm_epsilon .................................... 1e05   normalization ................................... RMSNorm   num_attention_heads ............................. 16   num_channels .................................... 3   num_classes ..................................... 1000   num_experts ..................................... None   num_layers ...................................... 24   num_layers_per_virtual_pipeline_stage ........... None   num_query_groups ................................ 1   num_workers ..................................... 2   one_logger_entity ............................... hwinf_dcm   one_logger_project .............................. e2etracking   one_logger_run_name ............................. None   onnx_safe ....................................... None   openai_gelu ..................................... False   optimizer ....................................... adam   output_bert_embeddings .......................... False   overlap_grad_reduce ............................. False   overlap_p2p_comm ................................ False   overlap_param_gather ............................ False   override_opt_param_scheduler .................... False   params_dtype .................................... torch.bfloat16   patch_dim ....................................... 16   perform_initialization .......................... True   pipeline_model_parallel_size .................... 1   pipeline_model_parallel_split_rank .............. None   position_embedding_type ......................... rope   profile ......................................... False   profile_ranks ................................... [0]   profile_step_end ................................ 12   profile_step_start .............................. 10   query_in_block_prob ............................. 0.1   rampup_batch_size ............................... None   rank ............................................ 0   recompute_granularity ........................... selective   recompute_method ................................ None   recompute_num_layers ............................ None   reset_attention_mask ............................ False   reset_position_ids .............................. False   retriever_report_topk_accuracies ................ []   retriever_score_scaling ......................... False   retriever_seq_length ............................ 256   retro_add_retriever ............................. False   retro_attention_gate ............................ 1   retro_cyclic_train_iters ........................ None   retro_encoder_attention_dropout ................. 0.1   retro_encoder_hidden_dropout .................... 0.1   retro_encoder_layers ............................ 2   retro_num_neighbors ............................. 2   retro_num_retrieved_chunks ...................... 2   retro_return_doc_ids ............................ False   retro_verify_neighbor_count ..................... True   retro_workdir ................................... None   rotary_interleaved .............................. False   rotary_percent .................................. 1.0   rotary_seq_len_interpolation_factor ............. None   sample_rate ..................................... 1.0   save ............................................ /data/oss_bucket_0/LLM/Qwen1_8BChatProMegatrontrain/   save_interval ................................... 10000   scatter_gather_tensors_in_pipeline .............. True   seed ............................................ 3407   seq_length ...................................... 2048   sequence_parallel ............................... False   sgd_momentum .................................... 0.9   short_seq_prob .................................. 0.1   skip_train ...................................... False   spec ............................................ None   split ........................................... 98,2,0   squared_relu .................................... False   standalone_embedding_stage ...................... False   start_weight_decay .............................. 0.1   swiglu .......................................... True   swin_backbone_type .............................. tiny   tensor_model_parallel_size ...................... 1   tensorboard_dir ................................. None   tensorboard_log_interval ........................ 1   tensorboard_queue_size .......................... 1   test_data_path .................................. None   timing_log_level ................................ 0   timing_log_option ............................... minmax   titles_data_path ................................ None   tokenizer_model ................................. None   tokenizer_type .................................. QWenTokenizer   tp_comm_bulk_dgrad .............................. True   tp_comm_bulk_wgrad .............................. True   tp_comm_overlap ................................. False   tp_comm_overlap_cfg ............................. None   tp_comm_split_ag ................................ True   tp_comm_split_rs ................................ True   train_data_path ................................. None   train_iters ..................................... None   train_samples ................................... 1000   transformer_impl ................................ local   transformer_pipeline_model_parallel_size ........ 1   untie_embeddings_and_output_weights ............. True   use_checkpoint_args ............................. False   use_checkpoint_opt_param_scheduler .............. False   use_cpu_initialization .......................... None   use_distributed_optimizer ....................... True   use_flash_attn .................................. True   use_mcore_models ................................ False   use_one_sent_docs ............................... False   use_ring_exchange_p2p ........................... False   use_rotary_position_embeddings .................. False   valid_data_path ................................. None   variable_seq_lengths ............................ False   virtual_pipeline_model_parallel_size ............ None   vision_backbone_type ............................ vit   vision_pretraining .............................. False   vision_pretraining_type ......................... classify   vocab_extra_ids ................................. 0   vocab_file ...................................... qwen_15w.tiktoken   vocab_size ...................................... None   wandb_exp_name ..................................    wandb_project ...................................    wandb_save_dir ..................................    weight_decay .................................... 0.1   weight_decay_incr_style ......................... constant   world_size ...................................... 1  end of arguments  step 2, my plan was to use CP (contextparallel) finetuning to expand the contextLength. Therefore, I had to enable usemcoremodels. Upon continuing the training, I then found that I was unable to load the model obtained from training in step 1. ERROR: The error seems to be due to a mismatch in the naming convention of the parameters. However, I can't figure out why using usemcoremodels or not would lead to different parameter naming. Isn't this supposed to be seamlessly compatible? !image arguments like this:  arguments    accumulate_allreduce_grads_in_fp32 .............. True   adam_beta1 ...................................... 0.9   adam_beta2 ...................................... 0.95   adam_eps ........................................ 1e08   add_bias_linear ................................. False   add_more_sp_tokens .............................. True   add_position_embedding .......................... True   add_qkv_bias .................................... True   adlr_autoresume ................................. False   adlr_autoresume_interval ........................ 1000   apply_layernorm_1p .............................. False   apply_query_key_layer_scaling ................... False   apply_residual_connection_post_layernorm ........ False   apply_rope_fusion ............................... True   async_tensor_model_parallel_allreduce ........... True   attention_dropout ............................... 0.0   attention_softmax_in_fp32 ....................... False   barrier_with_L1_time ............................ True   bert_binary_head ................................ True   bert_embedder_type .............................. megatron   bert_load ....................................... None   bf16 ............................................ True   bias_dropout_fusion ............................. True   bias_gelu_fusion ................................ False   bias_swiglu_fusion .............................. True   biencoder_projection_dim ........................ 0   biencoder_shared_query_context_model ............ False   block_data_path ................................. None   check_for_nan_in_loss_and_grad .................. False   classes_fraction ................................ 1.0   clip_grad ....................................... 1.0   clone_scatter_output_in_embedding ............... True   consumed_train_samples .......................... 0   consumed_valid_samples .......................... 0   context_parallel_size ........................... 1   data_cache_path ................................. None   data_parallel_random_init ....................... False   data_parallel_size .............................. 1   data_path ....................................... ['/data/oss_bucket_0/eleme_corpus/sft/general/longtext/EGPT72Blongseq/v9/_chatml_document']   data_per_class_fraction ......................... 1.0   data_sharding ................................... True   dataloader_type ................................. cyclic   decoder_num_layers .............................. None   decoder_seq_length .............................. None   delay_grad_reduce ............................... True   delay_param_gather .............................. False   dino_bottleneck_size ............................ 256   dino_freeze_last_layer .......................... 1   dino_head_hidden_size ........................... 2048   dino_local_crops_number ......................... 10   dino_local_img_size ............................. 96   dino_norm_last_layer ............................ False   dino_teacher_temp ............................... 0.07   dino_warmup_teacher_temp ........................ 0.04   dino_warmup_teacher_temp_epochs ................. 30   distribute_saved_activations .................... False   distributed_backend ............................. nccl   distributed_timeout_minutes ..................... 10   embedding_path .................................. None   empty_unused_memory_level ....................... 0   enable_one_logger ............................... False   encoder_num_layers .............................. 24   encoder_seq_length .............................. 2048   end_weight_decay ................................ 0.1   eod_mask_loss ................................... False   eval_interval ................................... 10000   eval_iters ...................................... 10   evidence_data_path .............................. None   exit_duration_in_mins ........................... None   exit_interval ................................... None   exit_on_missing_checkpoint ...................... False   exit_signal_handler ............................. False   expert_model_parallel_size ...................... 1   ffn_hidden_size ................................. 5504   finetune ........................................ True   fp16 ............................................ False   fp16_lm_cross_entropy ........................... False   fp32_residual_connection ........................ False   fp8 ............................................. None   fp8_amax_compute_algo ........................... most_recent   fp8_amax_history_len ............................ 1   fp8_interval .................................... 1   fp8_margin ...................................... 0   fp8_wgrad ....................................... True   global_batch_size ............................... 8   gradient_accumulation_fusion .................... False   group_query_attention ........................... False   head_lr_mult .................................... 1.0   hidden_dropout .................................. 0.0   hidden_size ..................................... 2048   hysteresis ...................................... 2   ict_head_size ................................... None   ict_load ........................................ None   img_h ........................................... 224   img_w ........................................... 224   indexer_batch_size .............................. 128   indexer_log_interval ............................ 1000   inference_batch_times_seqlen_threshold .......... 512   init_method_std ................................. 0.01   init_method_xavier_uniform ...................... False   initial_loss_scale .............................. 4294967296   iter_per_epoch .................................. 1250   kv_channels ..................................... 128   lazy_mpu_init ................................... None   load ............................................ /data/oss_bucket_0/LLM/Qwen1_8BChatProMegatrontrain/   local_rank ...................................... None   log_batch_size_to_tensorboard ................... True   log_interval .................................... 10   log_learning_rate_to_tensorboard ................ True   log_loss_scale_to_tensorboard ................... True   log_memory_to_tensorboard ....................... False   log_num_zeros_in_grad ........................... False   log_params_norm ................................. False   log_progress .................................... False   log_throughput .................................. False   log_timers_to_tensorboard ....................... True   log_validation_ppl_to_tensorboard ............... True   log_world_size_to_tensorboard ................... False   loss_scale ...................................... None   loss_scale_window ............................... 1000   lr .............................................. 0.0003   lr_decay_iters .................................. None   lr_decay_samples ................................ 1000   lr_decay_style .................................. cosine   lr_warmup_fraction .............................. None   lr_warmup_init .................................. 0.0   lr_warmup_iters ................................. 0   lr_warmup_samples ............................... 100   make_vocab_size_divisible_by .................... 128   manual_gc ....................................... False   manual_gc_eval .................................. True   manual_gc_interval .............................. 0   mask_factor ..................................... 1.0   mask_prob ....................................... 0.15   mask_type ....................................... random   masked_softmax_fusion ........................... True   max_position_embeddings ......................... 2048   max_tokens_to_oom ............................... 12000   merge_file ...................................... qwen_15w.tiktoken   micro_batch_size ................................ 1   min_loss_scale .................................. 1.0   min_lr .......................................... 3e05   mock_data ....................................... False   moe_aux_loss_coeff .............................. 0.0   moe_grouped_gemm ................................ False   moe_input_jitter_eps ............................ None   moe_router_load_balancing_type .................. aux_loss   moe_router_topk ................................. 2   moe_token_dropping .............................. False   moe_z_loss_coeff ................................ None   nccl_communicator_config_path ................... None   no_load_optim ................................... True   no_load_rng ..................................... None   no_persist_layer_norm ........................... False   no_save_optim ................................... None   no_save_rng ..................................... None   norm_epsilon .................................... 1e05   normalization ................................... RMSNorm   num_attention_heads ............................. 16   num_channels .................................... 3   num_classes ..................................... 1000   num_experts ..................................... None   num_layers ...................................... 24   num_layers_per_virtual_pipeline_stage ........... None   num_query_groups ................................ 1   num_workers ..................................... 2   one_logger_entity ............................... hwinf_dcm   one_logger_project .............................. e2etracking   one_logger_run_name ............................. None   onnx_safe ....................................... None   openai_gelu ..................................... False   optimizer ....................................... adam   output_bert_embeddings .......................... False   overlap_grad_reduce ............................. False   overlap_p2p_comm ................................ False   overlap_param_gather ............................ False   override_opt_param_scheduler .................... False   params_dtype .................................... torch.bfloat16   patch_dim ....................................... 16   perform_initialization .......................... True   pipeline_model_parallel_size .................... 1   pipeline_model_parallel_split_rank .............. None   position_embedding_type ......................... rope   profile ......................................... False   profile_ranks ................................... [0]   profile_step_end ................................ 12   profile_step_start .............................. 10   query_in_block_prob ............................. 0.1   rampup_batch_size ............................... None   rank ............................................ 0   recompute_granularity ........................... selective   recompute_method ................................ None   recompute_num_layers ............................ None   reset_attention_mask ............................ False   reset_position_ids .............................. False   retriever_report_topk_accuracies ................ []   retriever_score_scaling ......................... False   retriever_seq_length ............................ 256   retro_add_retriever ............................. False   retro_attention_gate ............................ 1   retro_cyclic_train_iters ........................ None   retro_encoder_attention_dropout ................. 0.1   retro_encoder_hidden_dropout .................... 0.1   retro_encoder_layers ............................ 2   retro_num_neighbors ............................. 2   retro_num_retrieved_chunks ...................... 2   retro_return_doc_ids ............................ False   retro_verify_neighbor_count ..................... True   retro_workdir ................................... None   rotary_interleaved .............................. False   rotary_percent .................................. 1.0   rotary_seq_len_interpolation_factor ............. None   sample_rate ..................................... 1.0   save ............................................ /data/oss_bucket_0/LLM/Qwen1_8BChatProMegatrontrainTE/   save_interval ................................... 10000   scatter_gather_tensors_in_pipeline .............. True   seed ............................................ 3407   seq_length ...................................... 2048   sequence_parallel ............................... False   sgd_momentum .................................... 0.9   short_seq_prob .................................. 0.1   skip_train ...................................... False   spec ............................................ None   split ........................................... 98,2,0   squared_relu .................................... False   standalone_embedding_stage ...................... False   start_weight_decay .............................. 0.1   swiglu .......................................... True   swin_backbone_type .............................. tiny   tensor_model_parallel_size ...................... 1   tensorboard_dir ................................. None   tensorboard_log_interval ........................ 1   tensorboard_queue_size .......................... 1   test_data_path .................................. None   timing_log_level ................................ 0   timing_log_option ............................... minmax   titles_data_path ................................ None   tokenizer_model ................................. None   tokenizer_type .................................. QWenTokenizer   tp_comm_bulk_dgrad .............................. True   tp_comm_bulk_wgrad .............................. True   tp_comm_overlap ................................. False   tp_comm_overlap_cfg ............................. None   tp_comm_split_ag ................................ True   tp_comm_split_rs ................................ True   train_data_path ................................. None   train_iters ..................................... None   train_samples ................................... 1000   transformer_impl ................................ local   transformer_pipeline_model_parallel_size ........ 1   untie_embeddings_and_output_weights ............. True   use_checkpoint_args ............................. False   use_checkpoint_opt_param_scheduler .............. False   use_cpu_initialization .......................... None   use_distributed_optimizer ....................... True   use_flash_attn .................................. True   use_mcore_models ................................ True   use_one_sent_docs ............................... False   use_ring_exchange_p2p ........................... False   use_rotary_position_embeddings .................. False   valid_data_path ................................. None   variable_seq_lengths ............................ False   virtual_pipeline_model_parallel_size ............ None   vision_backbone_type ............................ vit   vision_pretraining .............................. False   vision_pretraining_type ......................... classify   vocab_extra_ids ................................. 0   vocab_file ...................................... qwen_15w.tiktoken   vocab_size ...................................... None   wandb_exp_name ..................................    wandb_project ...................................    wandb_save_dir ..................................    weight_decay .................................... 0.1   weight_decay_incr_style ......................... constant   world_size ...................................... 1  end of arguments  **Expected behavior** The error seems to be due to a mismatch in the naming convention of the parameters. However, I can't figure out why using usemcoremodels or not would lead to different parameter naming. Isn't this supposed to be seamlessly compatible? **Stack trace/logs** If applicable, add the stack trace or logs from the time of the error. **Environment (please complete the following information):**   MegatronLM commit ID   0.5.0   PyTorch version   2.1.2   CUDA version   cuda12.2ï¼Œpython3.10.13   NCCL version **Proposed fix** If you have a proposal for how to fix the issue state it here or link to a PR. **Additional context** Add any other context about the problem here.",2024-04-24T06:08:16Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/793,you need to convert checkpoint from legacy to mcore. https://github.com/NVIDIA/MegatronLM/tree/main/tools/checkpoint
okoge-kaz,[BUG] distributed optimizer doesn't work when data parallel size is odd number.,"**Describe the bug** When the data parallel size is odd while distributed optimizer is enabled, training stops with the following error. ```bash [Proxy Service 0] Failed to execute operation Connect from rank 0, retcode 3     return group.send([tensor], group_dst_rank, tag)     return pg.recv([tensor], group_src_rank, tag) torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1691, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3 ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error.  Last error: ``` **To Reproduce** The job script being used is as follows. (environment: There are 8 H100 GPUs installed in 1 node, and since 3 nodes are being used, there are a total of 3x8=24 GPUs.) ```bash !/bin/bash SBATCH jobname=llama213b SBATCH partition=a3 SBATCH exclusive SBATCH nodes 3 SBATCH gpuspernode=8 SBATCH ntaskspernode=8 SBATCH output=outputs/llama213b/%x%j.out SBATCH error=outputs/llama213b/%x%j.out set e  module load module load cuda/12.1 module load cudnn/8.9.7 module load hpcx/2.17.1  open file limit ulimit n 65536 1048576  python virtualenv source .env/bin/activate  Important TCPX environment variables UDS_PATH=""/run/tcpx${SLURM_JOB_ID}""  Only use TCPX for multinode jobs. [[ ""${SLURM_JOB_NUM_NODES}"" gt 1 ]] && export USE_TCPX=yes  head n1) export MASTER_PORT=$((10000 + ($SLURM_JOBID % 50000))) echo ""MASTER_ADDR=${MASTER_ADDR}""  hostfile export NUM_GPU_PER_NODE=8 NODE_TYPE=""H100"" NUM_NODES=$SLURM_JOB_NUM_NODES NUM_GPUS=$((${NUM_NODES} * ${NUM_GPU_PER_NODE}))  model config  llama213b: https://huggingface.co/metallama/Llama213bhf/blob/main/config.json HIDDEN_SIZE=5120 FFN_HIDDEN_SIZE=13824  intermediate size (HuggingFace) NUM_LAYERS=40 NUM_HEADS=40 SEQ_LENGTH=4096  distributed settings TENSOR_PARALLEL_SIZE=2   fixed PIPELINE_PARALLEL_SIZE=4  num layers 40: Llama2 13B CONTEXT_PARALLEL_SIZE=1 DATA_PARALLEL_SIZE=$((${NUM_GPUS} / (${TENSOR_PARALLEL_SIZE} * ${PIPELINE_PARALLEL_SIZE})))  training config MICRO_BATCH_SIZE=2 GLOBAL_BATCH_SIZE=1024 TRAIN_STEPS=500679 LR_DECAY_ITERS=452995 LR=3e4 MIN_LR=3E5 LR_WARMUP_STEPS=2000 WEIGHT_DECAY=0.1 GRAD_CLIP=1  model config TOKENIZER_MODEL=/home/ext_kazuki_fujii_rio_gsic_titech/llmjptokenizer/models/ver3.0/llmjptokenizer100k.ver3.0b1.model CHECKPOINT_SAVE_DIR=/home/ext_kazuki_fujii_rio_gsic_titech/checkpoints/Llama213b/tp${TENSOR_PARALLEL_SIZE}pp${PIPELINE_PARALLEL_SIZE}ct${CONTEXT_PARALLEL_SIZE}bench mkdir p ${CHECKPOINT_SAVE_DIR}  data config DATASET_DIR=/home/ext_kazuki_fujii_rio_gsic_titech/datasets/training_resharded_tokenize_ver3.0 TRAIN_DATA_PATH="""" TRAIN_DATA_PATH=""""  code stack TRAIN_DATA_PATH=""${TRAIN_DATA_PATH} 14486363187 ${DATASET_DIR}/train/code/stack_0000.jsonl_text_document""  en wiki TRAIN_DATA_PATH=""${TRAIN_DATA_PATH} 4744259830 ${DATASET_DIR}/train/en/wiki_0000.jsonl_text_document""  job name JOB_NAME=""llama213bbaseokazakilabcc${NODE_TYPE}${NUM_NODES}node${NUM_GPUS}gpu${SEQ_LENGTH}sDP=${DATA_PARALLEL_SIZE}TP=${TENSOR_PARALLEL_SIZE}PP=${PIPELINE_PARALLEL_SIZE}BS=${GLOBAL_BATCH_SIZE}LR=${LR}MINLR=${MIN_LR}WARMUP=${LR_WARMUP_STEPS}WD=${WEIGHT_DECAY}GC=${GRAD_CLIP}zlossoverlapparamgathergradreduce""  normepsilon 1e5 : conifg.json (RMS norm) CHECKPOINT_ARGS=""load ${CHECKPOINT_SAVE_DIR}""  run mpirun np $NUM_GPUS \   npernode $NUM_GPU_PER_NODE \   x MASTER_ADDR=$MASTER_ADDR \   x MASTER_PORT=$MASTER_PORT \   x CUDA_DEVICE_MAX_CONNECTIONS=1 \   bindto none mapby slot \   x PATH \   python pretrain_gpt.py \   tensormodelparallelsize ${TENSOR_PARALLEL_SIZE} \   pipelinemodelparallelsize ${PIPELINE_PARALLEL_SIZE} \   contextparallelsize ${CONTEXT_PARALLEL_SIZE} \   sequenceparallel \   usedistributedoptimizer \   numlayers ${NUM_LAYERS} \   hiddensize ${HIDDEN_SIZE} \   ffnhiddensize ${FFN_HIDDEN_SIZE} \   numattentionheads ${NUM_HEADS} \   seqlength ${SEQ_LENGTH} \   maxpositionembeddings ${SEQ_LENGTH} \   microbatchsize ${MICRO_BATCH_SIZE} \   globalbatchsize ${GLOBAL_BATCH_SIZE} \   trainiters ${TRAIN_STEPS} \   tokenizertype Llama2Tokenizer \   tokenizermodel ${TOKENIZER_MODEL} \   ${CHECKPOINT_ARGS} \   save ${CHECKPOINT_SAVE_DIR} \   datapath ${TRAIN_DATA_PATH} \   split 998,1,1 \   distributedbackend nccl \   initmethodstd 0.02 \   lr ${LR} \   minlr ${MIN_LR} \   lrdecaystyle cosine \   lrdecayiters ${LR_DECAY_ITERS} \   weightdecay ${WEIGHT_DECAY} \   clipgrad ${GRAD_CLIP} \   lrwarmupiters ${LR_WARMUP_STEPS} \   optimizer adam \   adambeta1 0.9 \   adambeta2 0.95 \   loginterval 1 \   saveinterval 10 \   evalinterval 100 \   evaliters 10 \   bf16 \   untieembeddingsandoutputweights \   positionembeddingtype rope \   disablebiaslinear \   usemcoremodels \   normalization RMSNorm \   normepsilon 1e5 \   nomaskedsoftmaxfusion \   attentiondropout 0.0 \   hiddendropout 0.0 \   swiglu \   useflashattn \   recomputeactivations \   recomputegranularity ""selective"" \   attentionsoftmaxinfp32 \   transformerimpl ""transformer_engine"" \   fp8format 'hybrid' \   usempi \   usezloss \   useembeddingscaling \   logthroughput \   wandbname ${JOB_NAME} \   wandbproject ""Llama213B"" \   wandbentity ""niigeniac"" ``` **Expected behavior** Please enable training to proceed even when the data parallel size is odd while using the distributed optimizer. **Stack trace/logs** ```bash > finished creating GPT datasets ... [after dataloaders are built] datetime: 20240423 05:51:42  done with setup ... training ... (min, max) time across ranks (ms):     modelandoptimizersetup ......................: (697.72, 704.61)     train/valid/testdataiteratorssetup ..........: (781007.39, 781817.93) [before the start of training step] datetime: 20240423 05:51:42  Traceback (most recent call last):   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/pretrain_gpt.py"", line 229, in  Traceback (most recent call last):   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/pretrain_gpt.py"", line 229, in  Traceback (most recent call last):   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/pretrain_gpt.py"", line 229, in  Traceback (most recent call last):   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/pretrain_gpt.py"", line 229, in  Traceback (most recent call last):   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/pretrain_gpt.py"", line 229, in      pretrain(train_valid_test_datasets_provider,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 317, in pretrain     pretrain(train_valid_test_datasets_provider,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 317, in pretrain     pretrain(train_valid_test_datasets_provider,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 317, in pretrain     pretrain(train_valid_test_datasets_provider,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 317, in pretrain     pretrain(train_valid_test_datasets_provider,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 317, in pretrain     iteration, num_floating_point_operations_so_far = train(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 1194, in train     iteration, num_floating_point_operations_so_far = train(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 1194, in train     iteration, num_floating_point_operations_so_far = train(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 1194, in train     train_step(forward_step_func,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 589, in train_step     iteration, num_floating_point_operations_so_far = train(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 1194, in train     iteration, num_floating_point_operations_so_far = train(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 1194, in train     train_step(forward_step_func,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 589, in train_step     train_step(forward_step_func,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 589, in train_step     losses_reduced = forward_backward_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1269, in forward_backward_pipelining_without_interleaving     train_step(forward_step_func,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 589, in train_step     train_step(forward_step_func,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 589, in train_step     losses_reduced = forward_backward_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1269, in forward_backward_pipelining_without_interleaving     losses_reduced = forward_backward_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1269, in forward_backward_pipelining_without_interleaving     losses_reduced = forward_backward_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1269, in forward_backward_pipelining_without_interleaving     losses_reduced = forward_backward_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1269, in forward_backward_pipelining_without_interleaving     input_tensor = recv_forward(recv_tensor_shapes, config)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1062, in recv_forward     input_tensor = recv_forward(recv_tensor_shapes, config)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1062, in recv_forward     input_tensor = recv_forward(recv_tensor_shapes, config)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1062, in recv_forward     input_tensor = recv_forward(recv_tensor_shapes, config)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1062, in recv_forward     input_tensor = recv_forward(recv_tensor_shapes, config)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1062, in recv_forward     input_tensors.append(p2p_communication.recv_forward(tensor_shape, config))   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 361, in recv_forward     input_tensors.append(p2p_communication.recv_forward(tensor_shape, config))   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 361, in recv_forward     input_tensors.append(p2p_communication.recv_forward(tensor_shape, config))   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 361, in recv_forward     input_tensors.append(p2p_communication.recv_forward(tensor_shape, config))   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 361, in recv_forward     input_tensors.append(p2p_communication.recv_forward(tensor_shape, config))   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 361, in recv_forward     input_tensor, _, _ = _communicate(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 329, in _communicate     input_tensor, _, _ = _communicate(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 329, in _communicate     input_tensor, _, _ = _communicate(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 329, in _communicate     input_tensor, _, _ = _communicate(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 329, in _communicate     input_tensor, _, _ = _communicate(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 329, in _communicate     reqs = p2p_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 162, in _batched_p2p_ops     reqs = p2p_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 162, in _batched_p2p_ops     reqs = p2p_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 162, in _batched_p2p_ops     reqs = p2p_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 162, in _batched_p2p_ops     reqs = torch.distributed.batch_isend_irecv(ops)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1865, in batch_isend_irecv     reqs = torch.distributed.batch_isend_irecv(ops)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1865, in batch_isend_irecv     reqs = torch.distributed.batch_isend_irecv(ops)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1865, in batch_isend_irecv     reqs = torch.distributed.batch_isend_irecv(ops)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1865, in batch_isend_irecv     reqs = p2p_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 162, in _batched_p2p_ops     reqs = torch.distributed.batch_isend_irecv(ops)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1865, in batch_isend_irecv     p2p_op.op(p2p_op.tensor, p2p_op.peer, p2p_op.group, p2p_op.tag)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1631, in irecv     p2p_op.op(p2p_op.tensor, p2p_op.peer, p2p_op.group, p2p_op.tag)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1631, in irecv     p2p_op.op(p2p_op.tensor, p2p_op.peer, p2p_op.group, p2p_op.tag)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1631, in irecv     p2p_op.op(p2p_op.tensor, p2p_op.peer, p2p_op.group, p2p_op.tag)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1631, in irecv     p2p_op.op(p2p_op.tensor, p2p_op.peer, p2p_op.group, p2p_op.tag)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1631, in irecv Traceback (most recent call last):   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/pretrain_gpt.py"", line 229, in      pretrain(train_valid_test_datasets_provider,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 317, in pretrain     iteration, num_floating_point_operations_so_far = train(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 1194, in train     train_step(forward_step_func,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 589, in train_step     losses_reduced = forward_backward_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1269, in forward_backward_pipelining_without_interleaving     input_tensor = recv_forward(recv_tensor_shapes, config)     return pg.recv([tensor], group_src_rank, tag)     return pg.recv([tensor], group_src_rank, tag)     return pg.recv([tensor], group_src_rank, tag) torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1691, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3 ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error.  Last error: [Proxy Service 3] Failed to execute operation Connect from rank 3, retcode 3 torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1691, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3 ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error.  Last error: [Proxy Service 3] Failed to execute operation Connect from rank 3, retcode 3   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1062, in recv_forward     input_tensors.append(p2p_communication.recv_forward(tensor_shape, config)) torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1691, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3 ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error.  Last error: [Proxy Service 3] Failed to execute operation Connect from rank 3, retcode 3   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 361, in recv_forward     input_tensor, _, _ = _communicate(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 329, in _communicate     reqs = p2p_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 162, in _batched_p2p_ops     reqs = torch.distributed.batch_isend_irecv(ops)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1865, in batch_isend_irecv     p2p_op.op(p2p_op.tensor, p2p_op.peer, p2p_op.group, p2p_op.tag)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1631, in irecv     return pg.recv([tensor], group_src_rank, tag) torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1691, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3 ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error.  Last error:     return pg.recv([tensor], group_src_rank, tag) torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1691, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3 ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error.  Last error:     return pg.recv([tensor], group_src_rank, tag) torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1691, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3 ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error.  Last error: [Proxy Service 3] Failed to execute operation Connect from rank 3, retcode 3 Traceback (most recent call last):   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/pretrain_gpt.py"", line 229, in  Traceback (most recent call last):   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/pretrain_gpt.py"", line 229, in  Traceback (most recent call last):   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/pretrain_gpt.py"", line 229, in  Traceback (most recent call last):   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/pretrain_gpt.py"", line 229, in  Traceback (most recent call last):   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/pretrain_gpt.py"", line 229, in  Traceback (most recent call last):   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/pretrain_gpt.py"", line 229, in      pretrain(train_valid_test_datasets_provider,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 317, in pretrain     pretrain(train_valid_test_datasets_provider,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 317, in pretrain     pretrain(train_valid_test_datasets_provider,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 317, in pretrain     pretrain(train_valid_test_datasets_provider,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 317, in pretrain     pretrain(train_valid_test_datasets_provider,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 317, in pretrain     pretrain(train_valid_test_datasets_provider,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 317, in pretrain     iteration, num_floating_point_operations_so_far = train(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 1194, in train     iteration, num_floating_point_operations_so_far = train(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 1194, in train     iteration, num_floating_point_operations_so_far = train(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 1194, in train     iteration, num_floating_point_operations_so_far = train(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 1194, in train     iteration, num_floating_point_operations_so_far = train(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 1194, in train     iteration, num_floating_point_operations_so_far = train(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 1194, in train     train_step(forward_step_func,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 589, in train_step     train_step(forward_step_func,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 589, in train_step     train_step(forward_step_func,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 589, in train_step     train_step(forward_step_func,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 589, in train_step     train_step(forward_step_func,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 589, in train_step     train_step(forward_step_func,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 589, in train_step     losses_reduced = forward_backward_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1244, in forward_backward_pipelining_without_interleaving     losses_reduced = forward_backward_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1244, in forward_backward_pipelining_without_interleaving     losses_reduced = forward_backward_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1244, in forward_backward_pipelining_without_interleaving     losses_reduced = forward_backward_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1244, in forward_backward_pipelining_without_interleaving     losses_reduced = forward_backward_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1244, in forward_backward_pipelining_without_interleaving     losses_reduced = forward_backward_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1244, in forward_backward_pipelining_without_interleaving     input_tensor = recv_forward(recv_tensor_shapes, config)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1062, in recv_forward     input_tensor = recv_forward(recv_tensor_shapes, config)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1062, in recv_forward     input_tensor = recv_forward(recv_tensor_shapes, config)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1062, in recv_forward     input_tensor = recv_forward(recv_tensor_shapes, config)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1062, in recv_forward     input_tensor = recv_forward(recv_tensor_shapes, config)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1062, in recv_forward     input_tensor = recv_forward(recv_tensor_shapes, config)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1062, in recv_forward     input_tensors.append(p2p_communication.recv_forward(tensor_shape, config))   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 361, in recv_forward     input_tensors.append(p2p_communication.recv_forward(tensor_shape, config))   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 361, in recv_forward     input_tensors.append(p2p_communication.recv_forward(tensor_shape, config))   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 361, in recv_forward     input_tensors.append(p2p_communication.recv_forward(tensor_shape, config))   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 361, in recv_forward     input_tensors.append(p2p_communication.recv_forward(tensor_shape, config))   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 361, in recv_forward     input_tensors.append(p2p_communication.recv_forward(tensor_shape, config))   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 361, in recv_forward     input_tensor, _, _ = _communicate(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 329, in _communicate     input_tensor, _, _ = _communicate(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 329, in _communicate     input_tensor, _, _ = _communicate(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 329, in _communicate     input_tensor, _, _ = _communicate(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 329, in _communicate     input_tensor, _, _ = _communicate(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 329, in _communicate     input_tensor, _, _ = _communicate(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 329, in _communicate     reqs = p2p_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 162, in _batched_p2p_ops     reqs = p2p_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 162, in _batched_p2p_ops     reqs = p2p_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 162, in _batched_p2p_ops     reqs = p2p_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 162, in _batched_p2p_ops     reqs = p2p_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 162, in _batched_p2p_ops     reqs = p2p_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 162, in _batched_p2p_ops     reqs = torch.distributed.batch_isend_irecv(ops)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1865, in batch_isend_irecv     reqs = torch.distributed.batch_isend_irecv(ops)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1865, in batch_isend_irecv     reqs = torch.distributed.batch_isend_irecv(ops)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1865, in batch_isend_irecv     reqs = torch.distributed.batch_isend_irecv(ops)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1865, in batch_isend_irecv     reqs = torch.distributed.batch_isend_irecv(ops)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1865, in batch_isend_irecv     reqs = torch.distributed.batch_isend_irecv(ops)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1865, in batch_isend_irecv     p2p_op.op(p2p_op.tensor, p2p_op.peer, p2p_op.group, p2p_op.tag)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1631, in irecv     p2p_op.op(p2p_op.tensor, p2p_op.peer, p2p_op.group, p2p_op.tag)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1631, in irecv     p2p_op.op(p2p_op.tensor, p2p_op.peer, p2p_op.group, p2p_op.tag)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1631, in irecv     p2p_op.op(p2p_op.tensor, p2p_op.peer, p2p_op.group, p2p_op.tag)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1631, in irecv     p2p_op.op(p2p_op.tensor, p2p_op.peer, p2p_op.group, p2p_op.tag)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1631, in irecv     p2p_op.op(p2p_op.tensor, p2p_op.peer, p2p_op.group, p2p_op.tag)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1631, in irecv     return pg.recv([tensor], group_src_rank, tag) torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1691, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3 ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error.  Last error: [Proxy Service 1] Failed to execute operation Connect from rank 1, retcode 3     return pg.recv([tensor], group_src_rank, tag)     return pg.recv([tensor], group_src_rank, tag) torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1691, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3 ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error.  Last error: [Proxy Service 1] Failed to execute operation Connect from rank 1, retcode torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1691, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3 ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error.  Last error:     return pg.recv([tensor], group_src_rank, tag)     return pg.recv([tensor], group_src_rank, tag)     return pg.recv([tensor], group_src_rank, tag) torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1691, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3 ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error.  Last error: [Proxy Service 2] Failed to execute operation Connect from rank 2, retcode 3 torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1691, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3 ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error.  Last error: torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1691, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3 ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error.  Last error: [Proxy Service 2] Failed to execute operation Connect from rank 2, retcode 3 Traceback (most recent call last):   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/pretrain_gpt.py"", line 229, in  Traceback (most recent call last):   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/pretrain_gpt.py"", line 229, in  Traceback (most recent call last):   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/pretrain_gpt.py"", line 229, in  Traceback (most recent call last):   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/pretrain_gpt.py"", line 229, in  Traceback (most recent call last):   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/pretrain_gpt.py"", line 229, in  Traceback (most recent call last):   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/pretrain_gpt.py"", line 229, in      pretrain(train_valid_test_datasets_provider,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 317, in pretrain     pretrain(train_valid_test_datasets_provider,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 317, in pretrain     pretrain(train_valid_test_datasets_provider,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 317, in pretrain     pretrain(train_valid_test_datasets_provider,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 317, in pretrain     pretrain(train_valid_test_datasets_provider,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 317, in pretrain     pretrain(train_valid_test_datasets_provider,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 317, in pretrain     iteration, num_floating_point_operations_so_far = train(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 1194, in train     iteration, num_floating_point_operations_so_far = train(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 1194, in train     iteration, num_floating_point_operations_so_far = train(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 1194, in train     iteration, num_floating_point_operations_so_far = train(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 1194, in train     iteration, num_floating_point_operations_so_far = train(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 1194, in train     train_step(forward_step_func,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 589, in train_step     train_step(forward_step_func,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 589, in train_step     train_step(forward_step_func,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 589, in train_step     train_step(forward_step_func,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 589, in train_step     train_step(forward_step_func,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 589, in train_step     losses_reduced = forward_backward_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1258, in forward_backward_pipelining_without_interleaving     losses_reduced = forward_backward_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1258, in forward_backward_pipelining_without_interleaving     losses_reduced = forward_backward_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1258, in forward_backward_pipelining_without_interleaving     losses_reduced = forward_backward_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1244, in forward_backward_pipelining_without_interleaving     losses_reduced = forward_backward_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1244, in forward_backward_pipelining_without_interleaving     iteration, num_floating_point_operations_so_far = train(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 1194, in train     train_step(forward_step_func,   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/training/training.py"", line 589, in train_step     send_forward(output_tensor, send_tensor_shapes, config)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1082, in send_forward     send_forward(output_tensor, send_tensor_shapes, config)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1082, in send_forward     send_forward(output_tensor, send_tensor_shapes, config)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1082, in send_forward     input_tensor = recv_forward(recv_tensor_shapes, config)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1062, in recv_forward     input_tensor = recv_forward(recv_tensor_shapes, config)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1062, in recv_forward     losses_reduced = forward_backward_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1258, in forward_backward_pipelining_without_interleaving     p2p_communication.send_forward(output_tensor, config)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 406, in send_forward     p2p_communication.send_forward(output_tensor, config)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 406, in send_forward     p2p_communication.send_forward(output_tensor, config)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 406, in send_forward     input_tensors.append(p2p_communication.recv_forward(tensor_shape, config))   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 361, in recv_forward     input_tensors.append(p2p_communication.recv_forward(tensor_shape, config))   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 361, in recv_forward     send_forward(output_tensor, send_tensor_shapes, config)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1082, in send_forward     p2p_communication.send_forward(output_tensor, config)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 406, in send_forward     input_tensor, _, _ = _communicate(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 329, in _communicate     _communicate(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 329, in _communicate     _communicate(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 329, in _communicate     input_tensor, _, _ = _communicate(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 329, in _communicate     _communicate(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 329, in _communicate     _communicate(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 329, in _communicate     reqs = p2p_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 162, in _batched_p2p_ops     reqs = p2p_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 162, in _batched_p2p_ops     reqs = p2p_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 162, in _batched_p2p_ops     reqs = p2p_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 162, in _batched_p2p_ops     reqs = p2p_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 162, in _batched_p2p_ops     reqs = p2p_func(   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/megatron/core/pipeline_parallel/p2p_communication.py"", line 162, in _batched_p2p_ops     reqs = torch.distributed.batch_isend_irecv(ops)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1865, in batch_isend_irecv     reqs = torch.distributed.batch_isend_irecv(ops)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1865, in batch_isend_irecv     reqs = torch.distributed.batch_isend_irecv(ops)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1865, in batch_isend_irecv     reqs = torch.distributed.batch_isend_irecv(ops)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1865, in batch_isend_irecv     reqs = torch.distributed.batch_isend_irecv(ops)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1865, in batch_isend_irecv     reqs = torch.distributed.batch_isend_irecv(ops)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1865, in batch_isend_irecv     p2p_op.op(p2p_op.tensor, p2p_op.peer, p2p_op.group, p2p_op.tag)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1591, in isend     p2p_op.op(p2p_op.tensor, p2p_op.peer, p2p_op.group, p2p_op.tag)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1591, in isend     p2p_op.op(p2p_op.tensor, p2p_op.peer, p2p_op.group, p2p_op.tag)     p2p_op.op(p2p_op.tensor, p2p_op.peer, p2p_op.group, p2p_op.tag)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1631, in irecv   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1631, in irecv     p2p_op.op(p2p_op.tensor, p2p_op.peer, p2p_op.group, p2p_op.tag)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1591, in isend     p2p_op.op(p2p_op.tensor, p2p_op.peer, p2p_op.group, p2p_op.tag)   File ""/home/ext_kazuki_fujii_rio_gsic_titech/src/MegatronLM/.env/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1591, in isend     return pg.recv([tensor], group_src_rank, tag)     return group.send([tensor], group_dst_rank, tag) torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1691, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3 ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error.  Last error:     return group.send([tensor], group_dst_rank, tag) torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1691, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3 ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error.  Last error: [Proxy Service 0] Failed to execute operation Connect from rank 0, retcode 3 torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1691, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3 ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error.  Last error: [Proxy Service 0] Failed to execute operation Connect from rank 0, retcode 3     return group.send([tensor], group_dst_rank, tag)     return group.send([tensor], group_dst_rank, tag) torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1691, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3 ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error.  Last error: [Proxy Service 0] Failed to execute operation Connect from rank 0, retcode 3     return pg.recv([tensor], group_src_rank, tag) torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1691, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3 ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error.  Last error: [Proxy Service 0] Failed to execute operation Connect from rank , res=3, cl torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1691, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3 ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error.  Last error:  Primary job  terminated normally, but 1 process returned a nonzero exit code. Per userdirection, the job has been aborted.  wandb:  0.042 MB of 0.042 MB uploaded  mpirun detected that one or more processes exited with nonzero status, thus causing the job to be terminated. The first process to do so was:   Process name: [[8911,1],11]   Exit code:    1  ``` **Environment (please complete the following information):**   MegatronLM commit ID: 2196398f5252ead6f036b06d45f7acb89b1308da   PyTorch version: 2.2.0   CUDA version: 12.1   NCCL version: 2.19.3",2024-04-23T06:02:56Z,,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/792,"Did you use a PyTorch NGC container? If so, what version?","  Sorry for the late reply, I don't use PyTorch NGC Container.","  We did not encounter the problem when using the university's supercomputer (H100: TSUBAME4 ) instead of the A3 Instance on Google Cloud Platform. I'm not sure what the issue is, but it might be due to our use of GPUDirectTCPX_v7 on GCP. Thank you for your response.","Disponha meu amigo se precisar sÃ³ chamar aqui Em seg, 6 de mai de 2024 15:00, Kazuki Fujii ***@***.***> escreveu: > Closed CC([BUG] distributed optimizer doesn't work when data parallel size is odd number.)  as > completed. > > â€” > Reply to this email directly, view it on GitHub > , or > unsubscribe >  > . > You are receiving this because you are subscribed to this thread.Message > ID: ***@***.***> >"
xju2,[BUG] Example of pretraining BERT does not work,"**Describe the bug** Runing the Pretraining *BERT* encountered two issues: 1. the ""TransformerEngine only supports softmax compute in FP32"". Need to add `attentionsoftmaxinfp32` to the model arguments. This applies to Pretraining GPT `pretrain_gpt.sh` too. 2. The attention mask is of dimension `[B, 1, max_seqlen, max_seqlen]`; however, the function `get_cu_seqlens` expects its shape to be `[B, 1, 1, max_seqlen]`. The training crashes. See the log below. **To Reproduce** run the example: `./examples/pretrain_bert.sh` in the docker image `nvcr.io/nvidia/pytorch:24.02py3` with the `main` branch of MegatronLM. The issues was found in the `core_r0.6.0` branch too. **Expected behavior** expect the example runs out of box. **Stack trace/logs** ```text [after dataloaders are built] datetime: 20240423 00:29:39  done with setup ... (min, max) time across ranks (ms):     modelandoptimizersetup ......................: (5967.29, 5967.29)     train/valid/testdataiteratorssetup ..........: (128.70, 128.70) training ... [before the start of training step] datetime: 20240423 00:29:39  torch.Size([4, 1, 512, 512]) Traceback (most recent call last):   File ""/pscratch/sd/x/xju/LLMTracking/MegatronLM/pretrain_bert.py"", line 194, in      pretrain(train_valid_test_datasets_provider, model_provider,   File ""/pscratch/sd/x/xju/LLMTracking/MegatronLM/megatron/training/training.py"", line 270, in pretrain     iteration, num_floating_point_operations_so_far = train(   File ""/pscratch/sd/x/xju/LLMTracking/MegatronLM/megatron/training/training.py"", line 990, in train     train_step(forward_step_func,   File ""/pscratch/sd/x/xju/LLMTracking/MegatronLM/megatron/training/training.py"", line 541, in train_step     losses_reduced = forward_backward_func(   File ""/pscratch/sd/x/xju/LLMTracking/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 356, in forward_backward_no_pipelining     output_tensor = forward_step(   File ""/pscratch/sd/x/xju/LLMTracking/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 192, in forward_step     output_tensor, loss_func = forward_step_func(data_iterator, model)   File ""/pscratch/sd/x/xju/LLMTracking/MegatronLM/pretrain_bert.py"", line 139, in forward_step     output_tensor = model(tokens, padding_mask,   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1520, in _call_impl     return forward_call(*args, **kwargs)   File ""/pscratch/sd/x/xju/LLMTracking/MegatronLM/megatron/core/distributed/distributed_data_parallel.py"", line 179, in forward     return self.module(*inputs, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1520, in _call_impl     return forward_call(*args, **kwargs)   File ""/pscratch/sd/x/xju/LLMTracking/MegatronLM/megatron/legacy/model/module.py"", line 190, in forward     outputs = self.module(*inputs, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1520, in _call_impl     return forward_call(*args, **kwargs)   File ""/pscratch/sd/x/xju/LLMTracking/MegatronLM/megatron/legacy/model/bert_model.py"", line 182, in forward     lm_output = self.language_model(   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1520, in _call_impl     return forward_call(*args, **kwargs)   File ""/pscratch/sd/x/xju/LLMTracking/MegatronLM/megatron/legacy/model/language_model.py"", line 493, in forward     encoder_output = self.encoder(   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1520, in _call_impl     return forward_call(*args, **kwargs)   File ""/pscratch/sd/x/xju/LLMTracking/MegatronLM/megatron/legacy/model/transformer.py"", line 1777, in forward     hidden_states = layer(   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1520, in _call_impl     return forward_call(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/transformer_engine/pytorch/transformer.py"", line 625, in forward     self_attention_outputs = self.self_attention(   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1520, in _call_impl     return forward_call(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/transformer_engine/pytorch/attention.py"", line 3461, in forward     context_layer = self.core_attention(   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1520, in _call_impl     return forward_call(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/transformer_engine/pytorch/attention.py"", line 2724, in forward     return self.fused_attention(   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1520, in _call_impl     return forward_call(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/_dynamo/eval_frame.py"", line 417, in _fn     return fn(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/transformer_engine/pytorch/attention.py"", line 2055, in forward     _cu_seqlens_q = get_cu_seqlens(attention_mask)   File ""/usr/local/lib/python3.10/distpackages/transformer_engine/pytorch/attention.py"", line 166, in get_cu_seqlens     cu_seqlens = torch.cat((zero, cu_seqlens)) RuntimeError: Tensors must have same number of dimensions: got 1 and 2 ``` **Environment (please complete the following information):** Used the docker image: `nvcr.io/nvidia/pytorch:24.02py3`.    MegatronLM commit ID: `ccfeda4`   PyTorch version: `2.3.0a0+ebedce2`   CUDA version: `12.3`   NCCL version `2.20.3` **Proposed fix** N/A **Additional context** N/A",2024-04-23T00:35:27Z,stale,open,0,7,https://github.com/NVIDIA/Megatron-LM/issues/791,"Facing the same issue, please let me know if you have found a fix! Thanks!","in MegatronDeepSpeed/megatron/model/bert_model.pyï¼Œthere is a line: ```python extended_attention_mask = bert_extended_attention_mask(attention_mask) ``` which `bert_extended_attention_mask` is define like: ```python def bert_extended_attention_mask(attention_mask):      We create a 3D attention mask from a 2D tensor mask.      [b, 1, s]     attention_mask_b1s = attention_mask.unsqueeze(1)      [b, s, 1]     attention_mask_bs1 = attention_mask.unsqueeze(2)      [b, s, s]     attention_mask_bss = attention_mask_b1s * attention_mask_bs1      [b, 1, s, s]     extended_attention_mask = attention_mask_bss.unsqueeze(1)      Convert attention mask to binary:     extended_attention_mask = (extended_attention_mask < 0.5)     return extended_attention_mask ``` the `attention_mask` is extended from [b,s] to [b,1,s,s]. Is this the cause of the problem? If so, how can I fix it? Used the docker image: nvcr.io/nvidia/pytorch:23.12py3 MegatronLM commit ID: c4d12e2","> in MegatronDeepSpeed/megatron/model/bert_model.pyï¼Œthere is a line: >  > ```python > extended_attention_mask = bert_extended_attention_mask(attention_mask) > ``` >  > which `bert_extended_attention_mask` is define like: >  > ```python > def bert_extended_attention_mask(attention_mask): >      We create a 3D attention mask from a 2D tensor mask. >      [b, 1, s] >     attention_mask_b1s = attention_mask.unsqueeze(1) >      [b, s, 1] >     attention_mask_bs1 = attention_mask.unsqueeze(2) >      [b, s, s] >     attention_mask_bss = attention_mask_b1s * attention_mask_bs1 >      [b, 1, s, s] >     extended_attention_mask = attention_mask_bss.unsqueeze(1) >  >      Convert attention mask to binary: >     extended_attention_mask = (extended_attention_mask   >     return extended_attention_mask > ``` >  > the `attention_mask` is extended from [b,s] to [b,1,s,s]. Is this the cause of the problem? If so, how can I fix it? >  > Used the docker image: nvcr.io/nvidia/pytorch:23.12py3 MegatronLM commit ID: c4d12e2 use MegatronLM branch `23.08` with docker image: nvcr.io/nvidia/pytorch:23.08py3 can avoid this problem.","Facing the same issue, please let me know if you have found a fix! Thanks!",same issue here,"It turns out that `transformer_engine` assumes the attention mask is of shape `[b, 1, 1, s]`, however, as pointed out by , MegatronLM creates an extended attention mask with shape `[b, 1, s, s]`. To resolve this, create a text file `attention.patch` with the following content: ```diff  /usr/local/lib/python3.10/distpackages/transformer_engine/pytorch/attention.py	20240802 22:24:11.000000000 +0000 +++ attention.py	20240905 17:34:58.000000000 +0000 @@ 223,6 +223,8 @@      tensor of shape [batch_size + 1] containing the cumulative sequence lengths of      the samples in a batch.      """""" +    if mask.shape[2] != 1: +        mask = mask[:, :, 0, :]      mask = mask.squeeze(1).squeeze(1)      reduced_mask = mask.logical_not().sum(dim=1)      cu_seqlens = reduced_mask.cumsum(dim=0).to(torch.int32) ``` And run `patch /usr/local/lib/python3.10/distpackages/transformer_engine/pytorch/attention.py < attention.patch`.",Marking as stale. No activity in 60 days.
shamanez,When can we have a the MOE checkpoint convert script. ,"As mentioned here, having a proper MOE/Mixtral checkpoint converter script will help us to finetune Mixtral. ",2024-04-22T11:34:22Z,stale,open,5,6,https://github.com/NVIDIA/Megatron-LM/issues/790,+1,I also strongly need this tool https://github.com/NVIDIA/MegatronLM/issues/756issuecomment2126186633,"Here is some information about converting Huggingface checkpoints to Nemo. It seems there is a conversion script available on GitHub. Although I haven't confirmed it, it might be useful. https://medium.com/karakuri/trainmoesonawstrainiuma0ebb599fbda and https://github.com/abejainc/MegatronLM","Sorry for the late response, please check: https://github.com/NVIDIA/MegatronLM/tree/main/examples/mixtral","> Sorry for the late response, please check: https://github.com/NVIDIA/MegatronLM/tree/main/examples/mixtral Does it support megatron_to_transformers? I am currently using https://github.com/alibaba/PaiMegatronPatch/blob/main/toolkits/model_checkpoints_convertor/mistral/hf2mcore.py",Marking as stale. No activity in 60 days.
shamanez,modifed the model parreleized gpt pre-trainign script,,2024-04-22T08:26:09Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/789,Marking as stale. No activity in 60 days.
Euynaheh,Add NormHead for Baichuan,"Add NormHead implementation for Baichuan model. NormHead is a training technic proposed in Chinese LLM Baichuan2. This PR adds a normalization to the output embeddings. If we want to enable NormHead in a model architecture, just set use_normhead=True in model config yaml file. ",2024-04-21T13:23:34Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/788
zhentingqi,[QUESTION] Validation loss & PPL keep going up,"Hi, so I was training 345m GPT2 using your example scripts `examples/pretrain_gpt.sh`. The validation loss and PPL, however, keep going up, while the training loss decreases as expected.  !image My hyperparameters are shown here: ``` GPT_ARGS=""     numlayers 24 \     hiddensize 1024 \     numattentionheads 16 \     seqlength 1024 \     maxpositionembeddings 1024 \     microbatchsize 2 \     globalbatchsize 4 \     lr 3.0e4 \     trainiters 300000 \     lrdecayiters 320000 \     lrdecaystyle cosine \     minlr 1.0e5 \     weightdecay 1e2 \     lrwarmupfraction .01 \     clipgrad 1.0 \     fp16 "" DATA_ARGS=""     datapath $DATA_PATH \     vocabfile $VOCAB_FILE \     mergefile $MERGE_FILE \     dataimpl mmap \     split 700,200,100 "" OUTPUT_ARGS=""     loginterval 100 \     saveinterval 50000 \     evalinterval 1000 \     evaliters 10 "" ``` Can anyone please tell me what is wrong? Should not the PPL decreases? Thanks!",2024-04-20T18:04:45Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/787,Marking as stale. No activity in 60 days.,I encountered the same problem. Have you made any progress?
cfiken,miss,,2024-04-20T05:14:03Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/786
ezioliao,[QUESTION]  Is it expected to do grad norm on dense-optimizer and moe-optimizer respectively?,"If we enable expert parallelism, there will be two optimizers for dense parameters and expert parameters. When we call `optimizer.step() ` the two optimizers perform gradnorm for their own parameters. But if we do not enable expert parallelism, all model parameter's grad will be normed as entirely.  So my question is that the behavior of grad norm is different mathematically whether expert parallelism is turned on. Is it expetced?",2024-04-19T08:51:56Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/785,Marking as stale. No activity in 60 days.,I believe this behavior is fixed now.,Marking as stale. No activity in 60 days.
malay-nagda,forward step missing arg,,2024-04-18T04:57:54Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/784,Marking as stale. No activity in 60 days.
yangzhipeng1108,One H100 will oom when using meagatron to train llama2-70b. How to use two H100 to train llama2-70b?,One H100 will oom when using meagatron to train llama270b. How to use two H100 to train llama270b?,2024-04-17T02:57:22Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/783
JanryPei,"[QUESTION] RuntimeError: Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:10:00)","**My question** I am trying to run Megatron multinode on Docker. My docker was established by the following command: ``` docker run it v ~/personal:/root/ shmsize 128g ipc=host network=host env NCCL_DEBUG=INFO env NCCL_SOCKET_IFNAME=^docker*,lo name container_name megatron_image_name ``` The pretrain.sh also had been setted like this: ``` GPUS_PER_NODE=1 MASTER_ADDR=10.x.x.x MASTER_PORT=6000 NNODES=2 NODE_RANK=1 WORLD_SIZE=$(($GPUS_PER_NODE*$NNODES)) PP=1 TP=1 MICRO_BATCH_SIZE=1 GLOBAL_BATCH_SIZE=2 ``` However, when I run the shell, the error occured: ``` using world size: 2, dataparallelsize: 2, tensormodelparallel size: 1, pipelinemodelparallel size: 1  using torch.float16 for parameters ...  arguments    accumulate_allreduce_grads_in_fp32 .............. False   adam_beta1 ...................................... 0.9   adam_beta2 ...................................... 0.999   adam_eps ........................................ 1e08   add_bias_linear ................................. True   add_position_embedding .......................... True   adlr_autoresume ................................. False   adlr_autoresume_interval ........................ 1000   apply_layernorm_1p .............................. False   apply_query_key_layer_scaling ................... True   apply_residual_connection_post_layernorm ........ False   async_tensor_model_parallel_allreduce ........... True   attention_dropout ............................... 0.1   attention_softmax_in_fp32 ....................... False   barrier_with_L1_time ............................ True   bert_binary_head ................................ True   bert_embedder_type .............................. megatron   bert_load ....................................... None   bf16 ............................................ False   bias_dropout_fusion ............................. True   bias_gelu_fusion ................................ True   biencoder_projection_dim ........................ 0   biencoder_shared_query_context_model ............ False   block_data_path ................................. None   check_for_nan_in_loss_and_grad .................. True   classes_fraction ................................ 1.0   clip_grad ....................................... 1.0   consumed_train_samples .......................... 0   consumed_valid_samples .......................... 0   data_cache_path ................................. None   data_parallel_random_init ....................... False   data_parallel_size .............................. 2   data_path ....................................... ['data/codeparrot_content_document']   data_per_class_fraction ......................... 1.0   data_sharding ................................... True   dataloader_type ................................. single   decoder_num_layers .............................. None   decoder_seq_length .............................. None   delay_grad_reduce ............................... True   dino_bottleneck_size ............................ 256   dino_freeze_last_layer .......................... 1   dino_head_hidden_size ........................... 2048   dino_local_crops_number ......................... 10   dino_local_img_size ............................. 96   dino_norm_last_layer ............................ False   dino_teacher_temp ............................... 0.07   dino_warmup_teacher_temp ........................ 0.04   dino_warmup_teacher_temp_epochs ................. 30   distribute_saved_activations .................... False   distributed_backend ............................. nccl   distributed_timeout_minutes ..................... 10   embedding_path .................................. None   empty_unused_memory_level ....................... 0   encoder_num_layers .............................. 12   encoder_seq_length .............................. 1024   end_weight_decay ................................ 0.01   eod_mask_loss ................................... False   eval_interval ................................... 100   eval_iters ...................................... 0   evidence_data_path .............................. None   exit_duration_in_mins ........................... None   exit_interval ................................... None   exit_on_missing_checkpoint ...................... False   exit_signal_handler ............................. False   expert_model_parallel_size ...................... 1   expert_parallel ................................. False   ffn_hidden_size ................................. 3072   finetune ........................................ False   fp16 ............................................ True   fp16_lm_cross_entropy ........................... False   fp32_residual_connection ........................ False   fp8 ............................................. None   fp8_amax_compute_algo ........................... most_recent   fp8_amax_history_len ............................ 1   fp8_interval .................................... 1   fp8_margin ...................................... 0   fp8_wgrad ....................................... True   global_batch_size ............................... 2   gradient_accumulation_fusion .................... True   group_query_attention ........................... False   head_lr_mult .................................... 1.0   hidden_dropout .................................. 0.1   hidden_size ..................................... 768   hysteresis ...................................... 2   ict_head_size ................................... None   ict_load ........................................ None   img_h ........................................... 224   img_w ........................................... 224   indexer_batch_size .............................. 128   indexer_log_interval ............................ 1000   inference_batch_times_seqlen_threshold .......... 512   init_method_std ................................. 0.02   init_method_xavier_uniform ...................... False   initial_loss_scale .............................. 4294967296   iter_per_epoch .................................. 1250   kv_channels ..................................... 64   lazy_mpu_init ................................... None   load ............................................ None   local_rank ...................................... None   log_batch_size_to_tensorboard ................... False   log_interval .................................... 10   log_learning_rate_to_tensorboard ................ True   log_loss_scale_to_tensorboard ................... True   log_memory_to_tensorboard ....................... False   log_num_zeros_in_grad ........................... False   log_params_norm ................................. False   log_timers_to_tensorboard ....................... False   log_validation_ppl_to_tensorboard ............... False   log_world_size_to_tensorboard ................... False   loss_scale ...................................... None   loss_scale_window ............................... 1000   lr .............................................. 0.00015   lr_decay_iters .................................. 1520000   lr_decay_samples ................................ None   lr_decay_style .................................. cosine   lr_warmup_fraction .............................. 0.01   lr_warmup_init .................................. 0.0   lr_warmup_iters ................................. 0   lr_warmup_samples ............................... 0   make_vocab_size_divisible_by .................... 128   mask_factor ..................................... 1.0   mask_prob ....................................... 0.15   mask_type ....................................... random   masked_softmax_fusion ........................... True   max_position_embeddings ......................... 1024   max_tokens_to_oom ............................... 12000   merge_file ...................................... data/merges.txt   micro_batch_size ................................ 1   min_loss_scale .................................. 1.0   min_lr .......................................... 1e05   mmap_warmup ..................................... False   model_spec ...................................... None   no_load_optim ................................... None   no_load_rng ..................................... None   no_persist_layer_norm ........................... False   no_save_optim ................................... None   no_save_rng ..................................... None   norm_epsilon .................................... 1e05   normalization ................................... LayerNorm   num_attention_heads ............................. 12   num_channels .................................... 3   num_classes ..................................... 1000   num_experts ..................................... None   num_layers ...................................... 12   num_layers_per_virtual_pipeline_stage ........... None   num_query_groups ................................ 1   num_workers ..................................... 2   onnx_safe ....................................... None   openai_gelu ..................................... False   optimizer ....................................... adam   output_bert_embeddings .......................... False   overlap_grad_reduce ............................. False   overlap_p2p_comm ................................ False   override_opt_param_scheduler .................... False   params_dtype .................................... torch.float16   patch_dim ....................................... 16   perform_initialization .......................... True   pipeline_model_parallel_size .................... 1   pipeline_model_parallel_split_rank .............. None   position_embedding_type ......................... learned_absolute   profile ......................................... False   profile_ranks ................................... [0]   profile_step_end ................................ 12   profile_step_start .............................. 10   query_in_block_prob ............................. 0.1   rampup_batch_size ............................... None   rank ............................................ 0   recompute_granularity ........................... None   recompute_method ................................ None   recompute_num_layers ............................ None   reset_attention_mask ............................ False   reset_position_ids .............................. False   retriever_report_topk_accuracies ................ []   retriever_score_scaling ......................... False   retriever_seq_length ............................ 256   retro_add_retriever ............................. False   retro_cyclic_train_iters ........................ None   retro_encoder_attention_dropout ................. 0.1   retro_encoder_hidden_dropout .................... 0.1   retro_encoder_layers ............................ 2   retro_num_neighbors ............................. 2   retro_num_retrieved_chunks ...................... 2   retro_return_doc_ids ............................ False   retro_workdir ................................... None   rotary_percent .................................. 1.0   rotary_seq_len_interpolation_factor ............. None   sample_rate ..................................... 1.0   save ............................................ None   save_interval ................................... None   scatter_gather_tensors_in_pipeline .............. True   seed ............................................ 1234   seq_length ...................................... 1024   sequence_parallel ............................... False   sgd_momentum .................................... 0.9   short_seq_prob .................................. 0.1   skip_train ...................................... False   split ........................................... 949,50,1   squared_relu .................................... False   standalone_embedding_stage ...................... False   start_weight_decay .............................. 0.01   swiglu .......................................... False   swin_backbone_type .............................. tiny   tensor_model_parallel_size ...................... 1   tensorboard_dir ................................. None   tensorboard_log_interval ........................ 1   tensorboard_queue_size .......................... 1000   test_data_path .................................. None   timing_log_level ................................ 0   timing_log_option ............................... minmax   titles_data_path ................................ None   tokenizer_model ................................. None   tokenizer_type .................................. GPT2BPETokenizer   train_data_path ................................. None   train_iters ..................................... 10   train_samples ................................... None   transformer_impl ................................ local   transformer_pipeline_model_parallel_size ........ 1   untie_embeddings_and_output_weights ............. False   use_checkpoint_args ............................. False   use_checkpoint_opt_param_scheduler .............. False   use_cpu_initialization .......................... None   use_distributed_optimizer ....................... False   use_flash_attn .................................. False   use_mcore_models ................................ False   use_one_sent_docs ............................... False   use_ring_exchange_p2p ........................... False   use_rotary_position_embeddings .................. False   valid_data_path ................................. None   variable_seq_lengths ............................ False   virtual_pipeline_model_parallel_size ............ None   vision_backbone_type ............................ vit   vision_pretraining .............................. False   vision_pretraining_type ......................... classify   vocab_extra_ids ................................. 0   vocab_file ...................................... data/vocab.json   vocab_size ...................................... None   wandb_exp_name ..................................    wandb_project ...................................    wandb_save_dir ..................................    weight_decay .................................... 0.01   weight_decay_incr_style ......................... constant   world_size ...................................... 2  end of arguments  setting number of microbatches to constant 1 > building GPT2BPETokenizer tokenizer ...  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304) > initializing torch distributed ... Traceback (most recent call last):   File ""/root/MegatronLM/pretrain_gpt.py"", line 203, in      pretrain(train_valid_test_datasets_provider,   File ""/root/MegatronLM/megatron/training.py"", line 94, in pretrain     initialize_megatron(extra_args_provider=extra_args_provider,   File ""/root/MegatronLM/megatron/initialize.py"", line 80, in initialize_megatron     finish_mpu_init()   File ""/root/MegatronLM/megatron/initialize.py"", line 60, in finish_mpu_init     _initialize_distributed()   File ""/root/MegatronLM/megatron/initialize.py"", line 196, in _initialize_distributed     torch.distributed.init_process_group(   File ""/opt/conda/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 932, in init_process_group     _store_based_barrier(rank, store, timeout)   File ""/opt/conda/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 469, in _store_based_barrier     raise RuntimeError( RuntimeError: Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:10:00) ```",2024-04-16T08:52:51Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/782,"Could you try running a simple PyTorch Distributed test program to help determine if it's something wrong with your infrastructure and job startup, or something with Megatron itself. `import os import torch import torch.distributed print('MASTER_PORT = ', os.getenv('MASTER_PORT')) print('MASTER_ADDR = ', os.getenv('MASTER_ADDR')) print('WORLD_SIZE  = ', os.getenv('WORLD_SIZE')) print('RANK        = ', os.getenv('RANK')) print('LOCAL_RANK  = ', os.getenv('LOCAL_RANK')) print('About to initialize PyTorch Distributed...', flush=True) torch.distributed.init_process_group(backend='nccl') print('Completed initialization of PyTorch Distributed', flush=True) print('Entering barrier...', flush=True) torch.distributed.barrier() print('Done with barrier', flush=True)  `",Sei la,Marking as stale. No activity in 60 days.
constroy,fix a mistake when check if num_layers dividable by vpp,fix a mistake when check `num_layers % virtual_pipeline_model_parallel == 0` `num_layers_per_stage` is actually `self.num_layers`.,2024-04-16T08:36:45Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/781,Marking as stale. No activity in 60 days.
ftgreat,[QUESTION] found NaN in local grad norm in backward pass before data-parallel communication collective,"During continuing training MoE models(loading existing ckpt), at some steps, assert errors occurred as follows: ""found NaN in local grad norm in backward pass before dataparallel communication collective"". https://github.com/NVIDIA/MegatronLM/blob/caf2007e080d65dd7488be7bd409b366e225ab5f/megatron/core/distributed/param_and_grad_buffer.pyL115  Main Settings  tp=1,pp=8,ep=2  use_mcore=True  impl=transformers_engine  distributed_optimizer=True.  Questions  1. At steps=A, an assert error occurred. however, resume training from latest ckpt, assert error would not happen at steps=A.(samples sequence is fixed).  Besides, during resume training process, except loss at the very first step, losses of all subsequent steps have tiny numeric differences. Could you explain the reasons?  2. How to figure out the above NaN error, could you give me some advice to debugging details? Thanks.",2024-04-16T05:37:21Z,,open,0,16,https://github.com/NVIDIA/Megatron-LM/issues/780,"I got a same error, when I use Megatron training deepseek model on SFT. so any body know what's the problem","> I got a same error, when I use Megatron training deepseek model on SFT. so any body know what's the problem  did you train deepseek dense model or deepseekmoe model?  Often this error happened due to data.  However, in my case, data seems ok. I am not sure whether this case is related to moe pretraining.",Same issue!,Some zero data caused it!,"> Some zero data caused it!  could you explain this root cause and what is zero data like, thanks.","> > Some zero data caused it! >  >  could you explain this root cause and what is zero data like, thanks. Hello, have you resolved this issueï¼Ÿ","> > Some zero data caused it! >  >  could you explain this root cause and what is zero data like, thanks. Met the same issue, have you resolved itï¼Ÿ",Any idea about this issue? I get the same question.,same issue,I met the same issue when I trained deepseekv2. Anyone has a solution?," sorry to bother you, have you figured out the question i and ii ï¼Ÿ","The same issue, which was fixed after I reinstalled transformer_engine from source.",unsolved.,> I met the same issue when I trained deepseekv2. Anyone has a solution? Is this code fully compatible with deepseekv2? (mla + moe) Have you tried any converting from mcore ckpt to hf ckpt?,Marking as stale. No activity in 60 days.,"I also encounter the same issue, has anyone solved it? Thank you!"
Baibaifan,[BUG]The bug of the `average_losses_across_data_parallel_group`.,"**Describe the bug** When you use `average_losses_across_data_parallel_group` in pretrain_gpt.py with ep+dp, you could meet a bug which is wrong distribution group. Such as figure 1. figure1:  **To Reproduce** N/A **Expected behavior** N/A **Stack trace/logs** N/A **Environment (please complete the following information):**   MegatronLM commit ID: ba773259dbe5735fbd91ca41e7f4ded60b335c52 **Proposed fix** N/A **Additional context** N/A",2024-04-15T03:28:22Z,,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/779,is it an error or just inaccuracy? it should average across ep+dp ranks,"> is it an error or just inaccuracy? it should average across ep+dp ranks  I think it is a bug, because ep+dp, there will be an error running here.", I think it 's not a bug since ep is within dp.,"> average_losses_across_data_parallel_group ok, I found that during initialization there was `with_context_parallel=True`ï¼Œand I changed this configuration and my problem disappeared, so I thought there was something wrong with this part. when ep+dp, `_DATA_PARALLEL_GROUP_WITH_CP` == `_DATA_PARALLEL_GROUP`. !image"
Baibaifan,"[BUG] The bug about the options of the Megatron-core, transformer-impl and flash-attention.","**Describe the bug** Open usemcoremodels and useflashattn, set transformerimpl local, and do not use flashattention. **To Reproduce** N/A **Expected behavior** N/A **Stack trace/logs** N/A **Environment (please complete the following information):**   MegatronLM commit ID : ba773259dbe5735fbd91ca41e7f4ded60b335c52 **Proposed fix** N/A **Additional context** N/A",2024-04-12T08:24:07Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/778,"when you use usemcoremodels,, you cannot use local. useflashattn decides whether to use the OSS flash attention implmentation or cudnn implmementation. ","> when you use usemcoremodels,, you cannot use local. useflashattn decides whether to use the OSS flash attention implmentation or cudnn implmementation. hi  ,I understand the process you mentioned, but currently there is a task warning in the configuration options, which is not very userfriendly.",Marking as stale. No activity in 60 days.
Victarry,Fix llama converter.,,2024-04-12T07:28:35Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/777,Marking as stale. No activity in 60 days.
BeingGod,[BUG] ConstantGradScaler and loss-scale argument not match,"**Describe the bug** The usage and description of lossscale is inconsistent. The argument of lossscale expect to get a number of positive power of 2 but ConstantGradScaler set lossscale to real scale directly rather than 2**lossscale. Argument Description:  Argument Usage:  **To Reproduce** Steps to reproduce the behavior. The easier it is to reproduce the faster it will get maintainer attention. **Expected behavior** A clear and concise description of what you expected to happen. **Stack trace/logs** If applicable, add the stack trace or logs from the time of the error. **Environment (please complete the following information):**   MegatronLM commit ID   PyTorch version   CUDA version   NCCL version **Proposed fix** If you have a proposal for how to fix the issue state it here or link to a PR. **Additional context** Add any other context about the problem here.",2024-04-12T04:47:53Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/776,Marking as stale. No activity in 60 days.
sandyhouse,[BUG] The gradient allreduce/reduce-scatter operation is performed twice when overlap_grad_reduce is False,"**Describe the bug** ```python def finish_grad_sync(self):         """"""         Finishes grad sync (allreduce or reducescatter) communication operation         for this bucket.         When overlap_grad_reduce is set to True, waits for asynchronous communication         call to complete. When overlap_grad_reduce is set to False, makes synchronous call.         """"""          If overlap_grad_reduce is False, start (and finish) synchronous communication call here.         if not self.overlap_grad_reduce:             self.start_grad_sync()             return         assert self.communication_handle is not None and self.communication_issued, (             f'Communication call has not been issued for this bucket '             f'({len(self.params_with_grad)}/{len(self.params)} params have grad available)'         )         self.communication_handle.wait() ``` The above code snippet is from the `Bucket` class in megatron/core/distributed/param_and_grad_buffer.py. The `Bucket` class performs `allreduce/reducescatter` communication for the corresponding gradients using the `start_grad_sync` and `finish_grad_sync` functions. In this setup, `start_grad_sync` initiates the communication operation, while `finish_grad_sync` waits for the communication to complete. However, in synchronous communication mode (not overlap_grad_reduce), the `finish_grad_sync` function calls the `start_grad_sync` function again, resulting in two `allreduce/reducescatter` communication operations, which is not as expected. **To Reproduce** N/A **Expected behavior** N/A **Stack trace/logs** N/A **Environment (please complete the following information):**   MegatronLM commit ID: f3a3020 **Proposed fix** N/A **Additional context** N/A",2024-04-11T06:30:41Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/775,"Hello, I don't believe this is an issue, since without overlapping grad reduce, we don't have the backward hook call `start_grad_sync` (and so `start_grad_sync` is only called once)."," Thanks for your replay, and you are right, start_grad_sync is only called when overlapping grad reduce."
sandyhouse,[BUG] Passed the wrong type of argument to torch.distributed.broadcast.,"**Describe the bug** ```python def broadcast_params(self):         """"""         Syncs parameters across all DP ranks.         """"""         for param in self.module.parameters():             is_expert_parallel = not getattr(param, 'allreduce', True)             if is_expert_parallel:                 torch.distributed.broadcast(                     param.data,                     src=torch.distributed.get_process_group_ranks(self.expert_data_parallel_group),                     group=self.expert_data_parallel_group,                 )             else:                 torch.distributed.broadcast(                     param.data,                     src=torch.distributed.get_process_group_ranks(self.data_parallel_group),                     group=self.data_parallel_group,                 ) ``` The `src` parameter of `torch.distributed.broadcast` should be of type `int`, indicating the root from which to broadcast. However, in the above code, the passed parameter is a list of all ranks in the data parallel group. The above code snippet is from the `DistributedDataParallel` class in `megatron/core/distributed/distributed_data_parallel.py`. **To Reproduce** N/A **Expected behavior** N/A **Stack trace/logs** N/A **Environment (please complete the following information):**   MegatronLM commit ID: f3a3020 **Proposed fix** The `rank0` of the data parallel group should be passed in. **Additional context** N/A",2024-04-11T06:23:11Z,stale,open,1,1,https://github.com/NVIDIA/Megatron-LM/issues/774,Marking as stale. No activity in 60 days.
uehara-mech,[QUESTION] vicuna-7b-v1.5 weight conversion from huggingface to megatron-lm format,"I am trying to convert the weight for `vicuna7bv1.5 `in huggingface transformers ( https://huggingface.co/lmsys/vicuna7bv1.5 ) to be used with megatronlm. I am using `tools/checkpoint/convert.py` to do the conversion. The command I used is as follows: ``` python tools/checkpoint/convert.py \   modeltype GPT \   loader llama2_hf \   saver megatron \   targettensorparallelsize 2 \   targetpipelineparallelsize 2 \   loaddir ${HF_CHECKPOINT_DIR} \   savedir ${MEGATRON_CHECKPOINT_DIR} \   tokenizermodel ${TOKENIZER_MODEL} ``` When I run it, I get an error like this: ``` Traceback (most recent call last):   File ""[...]/MegatronLM/tools/checkpoint/convert.py"", line 158, in      main()   File ""[...]/MegatronLM/tools/checkpoint/convert.py"", line 151, in main     loader.load_checkpoint(queue, args)   File ""[...]/MegatronLM/tools/checkpoint/loader_llama2_hf.py"", line 370, in load_checkpoint     _load_checkpoint(queue, args)   File ""[...]/MegatronLM/tools/checkpoint/loader_llama2_hf.py"", line 280, in _load_checkpoint     model = load_checkpoint_to_model(margs)   File ""[...]/MegatronLM/tools/checkpoint/loader_llama2_hf.py"", line 140, in load_checkpoint_to_model     model = model_provider(True, True).to(args.params_dtype)   File ""[...]/MegatronLM/pretrain_gpt.py"", line 84, in model_provider     model = megatron.legacy.model.GPTModel(   File ""[...]/MegatronLM/megatron/legacy/model/gpt_model.py"", line 61, in __init__     self.language_model, self._language_model_key = get_language_model(   File ""[...]/MegatronLM/megatron/legacy/model/language_model.py"", line 67, in get_language_model     language_model = TransformerLanguageModel(   File ""[...]/MegatronLM/megatron/legacy/model/language_model.py"", line 387, in __init__     self.encoder = ParallelTransformer(   File ""[...]/MegatronLM/megatron/legacy/model/transformer.py"", line 1579, in __init__     [build_layer(i + 1 + offset) for i in range(self.num_layers)])   File ""[...]/MegatronLM/megatron/legacy/model/transformer.py"", line 1579, in      [build_layer(i + 1 + offset) for i in range(self.num_layers)])   File ""[...]/MegatronLM/megatron/legacy/model/transformer.py"", line 1519, in build_layer     tp_group=mpu.get_tensor_model_parallel_group(),   File ""[...]/MegatronLM/megatron/core/parallel_state.py"", line 567, in get_tensor_model_parallel_group     assert ( AssertionError: tensor model parallel group is not initialized ``` I looked into it, and it seems this error happens here: https://github.com/NVIDIA/MegatronLM/blob/7fe863f3d94f7b64a927b04b85f5c9339d3fb784/megatron/core/parallel_state.pyL563L569 because `_TENSOR_MODEL_PARALLEL_GROUP` does not have a value set. However, I found that `_TENSOR_MODEL_PARALLEL_GROUP` is only set here in the whole code: https://github.com/NVIDIA/MegatronLM/blob/7fe863f3d94f7b64a927b04b85f5c9339d3fb784/megatron/core/parallel_state.pyL379 and this function `initialize_model_parallel` does not seem to be called during the weight conversion. How can I correctly do the weight conversion?",2024-04-10T01:37:19Z,stale,closed,3,6,https://github.com/NVIDIA/Megatron-LM/issues/773,"I'm also interested in this, and more generally how Megatron can be used to convert from HF, continue pretraining, and convert back to HF. ",same issue on different model,My understanding is that `megatron` model_type (that uses `transformerimpl=local`) is deprecated. Consider using `mcore` model_type (uses `transformerimpl=transformer_engine`): ``` saver mcore ```,"Also, if you do need `megatron` model_type, try saving first to `mcore`, then to `megatron`. Last time I checked, that worked.","> My understanding is that `megatron` model_type (that uses `transformerimpl=local`) is deprecated. Consider using `mcore` model_type (uses `transformerimpl=transformer_engine`): >  > ``` > saver mcore > ``` Thanks, man. I use `saver megatron` to convert llama213bhf  model, and  it would cause a Runtime Error when loading checkpoints with `Missing key(s) in state_dict: ""embedding.word_embeddings.weights"", ""decoder.layers.0.self_attention.linear_proj.weight""`, etc. And I change the convert command into `saver mcore`, it successfully loaded checkpoints and started finetune trainning. I read the source code of MegatronLM/tools/checkpoints/, and it seems that `llama2_hf` loader is already deprecated and `llama_mistral` is the choice. Anyway, your answer really helps me a lot, before that I checked for a long time to figure out the problems",Marking as stale. No activity in 60 days.
ocryptocode,Update pretrain_bert.py,,2024-04-09T21:39:49Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/772,Marking as stale. No activity in 60 days.
XLzed,[QUESTION] Is PackedSeqParams still under development?,"I want to use sequence packing without CrossContamination Attention like this: https://github.com/DaoAILab/flashattention/issues/432. The GPTDataset within megatron is using sequence packing by default, and I found the PackedSeqParams can be passed into coreattention, but currentlly it is not usable, when can I use it to achieve seq packing without crosscontamination attention.",2024-04-09T12:10:01Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/771,"Well, I found it's used in Nemo, so will megatronlm support sft+packedDataset? or recommended to use nemo directly?",Did you figure it out? Thanks!,"> Did you figure it out? Thanks! I use nemo to do sft training currently, the PackedSeqParams is used by NeMo sequence packing."
REIGN12,[QUESTION] Why megatron-core seems slower and use more gpu mem than legacy for gpt_pretrain?,"**Your question** I run pretrain_gpt on same arch, data, training hyperparams and same hardware, with and without using megatron_core when build the model. I notice clearly **worse wall clock time and memory usage**:  For the data I use c4_en data from huggingface and tokenize it using gpt2 tokenizer. I use the first 3.6e7(first 10%) document to conduct the experiments. **To Reproduce** megatronlm commit hash: 9de386d08770d7296263a590171ace4ae45348ad I customize a script from pretrain_gpt_distributed.sh and rename it as `pretrain_gpt_cli.sh` ```bash set x !/bin/bash  Runs the ""345M"" parameter model export CUDA_DEVICE_MAX_CONNECTIONS=1  dist GPUS_PER_NODE=4  TODO: change in future MASTER_ADDR=localhost MASTER_PORT=6000 NNODES=1 NODE_RANK=0 WORLD_SIZE=$(($GPUS_PER_NODE*$NNODES))  tokenizer TOKENIZER_DIR=""/root/model_ckps/gpt2"" VOCAB_FILE=""${TOKENIZER_DIR}/vocab.json"" MERGE_FILE=""${TOKENIZER_DIR}/merges.txt""   training hp MBS=${1:32} GBS=${2:128} LR=${3:""1e4""} MIN_LR=${4:""1e5""} WARMUP_RATIO=${5:""0.01""} WD=${6:""1e2""} SEQ_LEN=${7:1024} TRAIN_STEPS=${8:10000}  related to data? LR_DECAY_STEPS=${9:10000}  TODO: how to set this? LR_SCHED=${10:""cosine""} USE_MCORE=${11:""False""}  POSITION_EMB? learnable ape by default  model hp HIDDEN=${12:1024} ATTN_HEADS=${13:16} NUM_LAYERS=${14:24} ATTN_DROP=${15:0.0} HIDDEN_DROP=${16:0.0}  ATTN_SOFTMAX_FP32?  UNTIE?  output setting LOG_INTERVAL=${17:1} SAVE_INTERVAL=${18:10000} EVAL_STEPS=${19:1000}  EVAL_INTERVAL=${20:1000}?  data setting DATA_NAME=${20:""gpt2_10p""}  misc setting SEED=${21:""1234""}  moe setting NUM_EXPERTS=${22:""none""}  if none, no moe LOAD_BALANCER=${23:""aux_loss""}  MOE_TOPK=${24:2} AUX_LW=${25:""1e2""}  should be tuned MIX_PREC=""bf16"" DATA_PATH=/root/data/tokenized/c4_en_${DATA_NAME}_text_document EXP_NAME=""c4_${DATA_NAME}_bs${GBS}_mbs${MBS}_lr${LR}_mlr${MIN_LR}_wm${WARMUP_RATIO}_wd${WD}"" EXP_NAME=""${EXP_NAME}_${SEQ_LEN}_ts${TRAIN_STEPS}_${LR_SCHED}${LR_DECAY_STEPS}_${MIX_PREC}"" EXP_NAME=""${EXP_NAME}_h${HIDDEN}_a${ATTN_HEADS}_l${NUM_LAYERS}_ad${ATTN_DROP}_hd${HIDDEN_DROP}"" EXP_NAME=""${EXP_NAME}_${SEED}"" CHECKPOINT_PATH=""/root/model_ckps/${EXP_NAME}"" DISTRIBUTED_ARGS=""     nproc_per_node $GPUS_PER_NODE \     nnodes $NNODES \     node_rank $NODE_RANK \     master_addr $MASTER_ADDR \     master_port $MASTER_PORT "" DATA_ARGS=""     datapath $DATA_PATH \     vocabfile $VOCAB_FILE \     mergefile $MERGE_FILE \     split 949,50,1 "" OUTPUT_ARGS=""     loginterval ${LOG_INTERVAL} \     tensorboardloginterval ${LOG_INTERVAL} \     saveinterval ${SAVE_INTERVAL} \     evaliters ${EVAL_STEPS} \     tensorboarddir ${CHECKPOINT_PATH}/tb "" MOE_ARGS="""" if [ ""$NUM_EXPERTS"" != ""none"" ]; then     MOE_ARGS=""         numexperts $NUM_EXPERTS \         moerouterloadbalancingtype $LOAD_BALANCER \         moeroutertopk $MOE_TOPK \         moeauxlosscoeff $AUX_LW     ""     EXP_NAME=""${EXP_NAME}_moe${NUM_EXPERTS}${MOE_TOPK}_${LOAD_BALANCER}${AUX_LW}"" fi MCORE_ARGS="""" if [ ""$USE_MCORE"" == ""True"" ]; then     MCORE_ARGS=""usemcoremodels""     EXP_NAME=""${EXP_NAME}_mcore"" fi WANDB_PROJECT=""ScalingLaws"" WANDB_ENTITY=""reign"" \ torchrun $DISTRIBUTED_ARGS pretrain_gpt.py \     microbatchsize ${MBS} \     globalbatchsize ${GBS} \     lr ${LR} \     minlr ${MIN_LR} \     lrwarmupfraction ${WARMUP_RATIO} \     weightdecay ${WD} \     seqlength ${SEQ_LEN} \     maxpositionembeddings ${SEQ_LEN} \     trainiters ${TRAIN_STEPS} \     lrdecayiters ${LR_DECAY_STEPS} \     lrdecaystyle ${LR_SCHED} \     ${MIX_PREC} \     hiddensize ${HIDDEN} \     numattentionheads ${ATTN_HEADS} \     numlayers ${NUM_LAYERS} \     attentiondropout ${ATTN_DROP} \     hiddendropout ${HIDDEN_DROP} \     clipgrad 1.0 \     $DATA_ARGS \     $OUTPUT_ARGS \     distributedbackend nccl \     save $CHECKPOINT_PATH \     load $CHECKPOINT_PATH \     useflashattn \     wandbproject $WANDB_PROJECT \     wandbexpname ""$EXP_NAME"" \     seed $SEED \     $MOE_ARGS \     $MCORE_ARGS ``` To reproduce the experiment, please run following bash command: ```bash STEP=20000 USE_MCORE=""True""  or ""False"" to use legacy bash examples/pretrain_gpt_cli.sh 64 512 1e3 1e5 \     0.01 0.0 1024 $STEP $STEP cosine $USE_MCORE \     512 8 8 0.0 0.0 1 $STEP 100 gpt2_10p 1234 ``` Is there any reason behind this?",2024-04-09T06:28:00Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/770,A possible reason is that the local mcore model does not support flashattn. https://github.com/NVIDIA/MegatronLM/blob/core_v0.6.0/megatron/core/models/gpt/gpt_layer_specs.pyL53,Marking as stale. No activity in 60 days.
starkhu,[QUESTION]why replace F.embedding() with [] on VocabParallelEmbedding class?,"**question** barker hello, jon, I have some questions on the embedding, can you help explain? Why replace F.embedding(masked_input, self.weight) with self.weight[masked_input] in forward() function of class VocabParallelEmbedding? What is the difference between them? Why does the  F.embedding() can bring 'nondeterminism'? linkï¼šhttps://github.com/NVIDIA/MegatronLM/blob/core_r0.5.0/megatron/core/tensor_parallel/layers.pyL218",2024-04-09T02:19:05Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/769,Marking as stale. No activity in 60 days.
anmolgupt,Option to disable gradient reduce for ColumnParallelLinear Layer,"The default setting for the param disable_grad_reduce is False. When it is set to True, gradient reduction in the bprop is disabled.",2024-04-08T22:04:17Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/768
robotsp,[BUG] How to checkpoint the specific microbatch in pipeline parallelism?,"**Your question** Ask a clear and concise question about MegatronLM. I saw there is a microbatchlevel checkpointing implementation of https://arxiv.org/pdf/2205.05198.pdf in schedules.py. But I do not know how to enable it with the arguments: num_microbatches_with_partial_activation_checkpoints, checkpoint_activations_microbatch ",2024-04-07T14:05:36Z,stale,open,1,3,https://github.com/NVIDIA/Megatron-LM/issues/767,"I tried to set a batch number, but got an error "" TypeError: forward_step() takes 2 positional arguments but 3 were given"" Does it mean we have not implemented the microbatchlevel checkpointing yet?     "," Hi deepakn, do you know how to solve the problem?",Marking as stale. No activity in 60 days.
1049451037,[BUG] Bug of expert model parallel,"https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/parallel_state.pyL503 ``` 0: [rank1]:   File ""/share/home/lqs/MegatronLM/megatron/core/parallel_state.py"", line 503, in initialize_model_parallel 0: [rank1]:     group = torch.distributed.new_group( 0: [rank1]:   File ""/share/real_shared_envs/megatron/lib/python3.10/sitepackages/torch/distributed/c10d_logger.py"", line 89, in wrapper 0: [rank1]:     func_return = func(*args, **kwargs) 0: [rank1]:   File ""/share/real_shared_envs/megatron/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 3806, in new_group 0: [rank1]:     return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization) 0: [rank1]:   File ""/share/real_shared_envs/megatron/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 3877, in _new_group_with_tag 0: [rank1]:     pg, pg_store = _new_process_group_helper( 0: [rank1]:   File ""/share/real_shared_envs/megatron/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py"", line 1431, in _new_process_group_helper 0: [rank1]:     pg: ProcessGroup = ProcessGroup(prefix_store, group_rank, group_size, base_pg_options) 0: [rank1]: TypeError: __init__(): incompatible constructor arguments. The following argument types are supported: 0: [rank1]:     1. torch._C._distributed_c10d.ProcessGroup(arg0: int, arg1: int) 0: [rank1]:     2. torch._C._distributed_c10d.ProcessGroup(arg0: torch._C._distributed_c10d.Store, arg1: int, arg2: int, arg3: c10d::ProcessGroup::Options) ``` As shown in the code: ```python     tensor_and_data_group_size: int = tensor_model_parallel_size * data_parallel_size     num_tensor_and_data_groups: int = world_size // tensor_and_data_group_size     tensor_and_expert_group_size: int = tensor_model_parallel_size * expert_model_parallel_size     num_expert_groups: int = data_parallel_size // expert_model_parallel_size     for i in range(num_tensor_and_data_groups):         for j in range(num_expert_groups):              TPxEP Group             start_rank = i * tensor_and_data_group_size + j * tensor_and_expert_group_size             end_rank = i * tensor_and_data_group_size + (j + 1) * tensor_and_expert_group_size             ranks = range(start_rank, end_rank)             group = torch.distributed.new_group(                 ranks, timeout=timeout, pg_options=get_nccl_options('tp_exp', nccl_comm_cfgs)             )             if rank in ranks:                 _TENSOR_AND_EXPERT_PARALLEL_GROUP = group             for k in range(tensor_model_parallel_size * context_parallel_size):                 ranks = range(                     start_rank + k, end_rank, tensor_model_parallel_size * context_parallel_size                 )                 group = torch.distributed.new_group(                     ranks, pg_options=get_nccl_options('exp', nccl_comm_cfgs)                 )                 if rank in ranks:                     _EXPERT_MODEL_PARALLEL_GROUP = group ``` The length of `[start_rank, end_rank)` is `tp*ep`. But the `for` loop in `k` is `tp*cp`. If `cp>ep`, it will make `ranks` empty, which causes the error.",2024-04-07T08:39:01Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/766,"Thank you for let us know! We have a fix, but it's not yet merged. Temporarily WAR is replace `tensor_model_parallel_size * context_parallel_size` with just `tensor_model_parallel_size`.",This issue should has been resolved on https://github.com/NVIDIA/MegatronLM/commit/b5aba3a2f3165da8b4f6b483bf3a6da2a24718e4,Marking as stale. No activity in 60 days.
NoelBird,[very simple change] Remove duplicated code,Remove duplicated line,2024-04-03T15:43:29Z,stale,open,0,5,https://github.com/NVIDIA/Megatron-LM/issues/765,Marking as stale. No activity in 60 days.,Can you resolve the conflicts and compact your modification into one single commit? Then I can help you land your commit in the internal repo., Thank you for your help. I resolved conflict and merged to one commit. could you check it?,Your commit is squash merged in https://github.com/NVIDIA/MegatronLM/commit/2488e20d488b35259f6eb53267e8a0525a373599 and your author information is preserved.  Thanks for your contribution. You can close this PR now. ,Marking as stale. No activity in 60 days.
hannahli-nv,Enable overlapping and kernel fusion for embedding fwd,This PR aims to improve the performance of embedding fwd by doing the following two optimizations:   Using torch.compile to efficiently fuse the small kernels for reading word_embeddings when `tensor_model_parallel_size > 1`.  Overlapping the AllReduce kernel (for word_embeddings) with the indexSelectLargeIndex kernel (for position_embeddings).,2024-04-03T08:42:45Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/764
hellangleZ,[BUG] ModuleNotFoundError: No module named 'megatron.training.tokenizer'; 'megatron.training' is not a package,"**Describe the bug** A clear and concise description of what the bug is. Stonge issue /aml2/ds) root:/aml2/MegatronLM from megatron.training.tokenizer import build_tokenizer from: can't read /var/mail/megatron.training.tokenizer (/aml2/ds) root:/aml2/MegatronLM python tools/preprocess_data.py \ >        input /aml2/traindata/oscar1GB.jsonl \ >        outputprefix /aml2/traindata\ >        tokenizertype Llama2Tokenizer \ >        tokenizermodel /aml2/llama2/tokenizer.model \ >        workers 16 \ >        appendeod [20240402 08:03:42,280] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect) Traceback (most recent call last):   File ""/aml2/MegatronLM/tools/preprocess_data.py"", line 23, in      from megatron.training.tokenizer import build_tokenizer ModuleNotFoundError: No module named 'megatron.training.tokenizer'; 'megatron.training' is not a package **To Reproduce** Steps to reproduce the behavior. The easier it is to reproduce the faster it will get maintainer attention. **Expected behavior** A clear and concise description of what you expected to happen. **Stack trace/logs** If applicable, add the stack trace or logs from the time of the error. **Environment (please complete the following information):**   MegatronLM commit ID the latest   PyTorch version 2.2.1   CUDA version 12.1 **Proposed fix** If you have a proposal for how to fix the issue state it here or link to a PR. **Additional context** Add any other context about the problem here.",2024-04-02T08:11:46Z,stale,open,0,6,https://github.com/NVIDIA/Megatron-LM/issues/763,Have you tried again with the most recent version of `main`? There was a fix regarding this.,Marking as stale. No activity in 60 days.,I used the latest release version v0.7.0 and the error still happend.,Marking as stale. No activity in 60 days.,same issue,Marking as stale. No activity in 60 days.
wangxicoding,fix new bucket when param require new bucket," Problem description Suppose we have three parameters a, b, c of size 10, 4, 8, and the last parameter is `shared_embedding`, dp = 4. According to the previous implementation, the result will be (while False will be judged by `assert data_start_index % self.data_parallel_world_size == 0` during `c` param iter): ",2024-04-02T02:48:57Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/762,Marking as stale. No activity in 60 days.
guozhen1997,MOE training Loss inconsistent after resume from old checkpoint,**MOE training Loss inconsistent after resume from old checkpoint** Experimental conditions:  the latest main branch  use mcore  expertmodelparallelsize > 1  The black line runs continuously and saves a ckpt every 100 steps. The blue line is at the 100th step. ckpt is loaded and continues running. What might cause this and how to fix itï¼Ÿ,2024-04-01T02:31:42Z,stale,open,0,5,https://github.com/NVIDIA/Megatron-LM/issues/761,"Hi  , we are also debugging on this issue. I will ping you when we find the root cause ASAP.","Hi  , this issue is caused by an incorrect implementation of the dualoptimizer state loading function, the fix MR is under review and will be published soon.","Hi  , if we use the lagecy checkpointing method instead of the distributed checkpointing will we encounter this issue?","Hi  and   , this issue has already been fixed by this commit.",Marking as stale. No activity in 60 days.
Yazeed7,Updated `fused_kernels` import path,"Model conversion scripts in `tools/checkpoint` fail after the refactor with the following error: ``` Traceback (most recent call last):   File ""MegatronLM/tools/checkpoint/convert.py"", line 155, in      main()   File ""MegatronLM/tools/checkpoint/convert.py"", line 148, in main     loader.load_checkpoint(queue, args)   File ""MegatronLM/tools/checkpoint/loader_llama2_hf.py"", line 361, in load_checkpoint     _load_checkpoint(queue, args)   File ""MegatronLM/tools/checkpoint/loader_llama2_hf.py"", line 166, in _load_checkpoint     from megatron.training import fused_kernels ImportError: cannot import name 'fused_kernels' from 'megatron.training' (MegatronLM/megatron/training/__init__.py) ``` I updated the import paths to `fused_kernels`, which solved this issue.",2024-03-31T10:31:30Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/760,Marking as stale. No activity in 60 days.
Yazeed7,Updated `fused_kernels` import path,"Model conversion scripts in `tools/checkpoint` fail after the refactor with the following error: ``` Traceback (most recent call last):   File ""MegatronLM/tools/checkpoint/convert.py"", line 155, in      main()   File ""MegatronLM/tools/checkpoint/convert.py"", line 148, in main     loader.load_checkpoint(queue, args)   File ""MegatronLM/tools/checkpoint/loader_llama2_hf.py"", line 361, in load_checkpoint     _load_checkpoint(queue, args)   File ""MegatronLM/tools/checkpoint/loader_llama2_hf.py"", line 166, in _load_checkpoint     from megatron.training import fused_kernels ImportError: cannot import name 'fused_kernels' from 'megatron.training' (MegatronLM/megatron/training/__init__.py) ``` I updated the import paths to `fused_kernels`, which solved this issue.",2024-03-31T10:16:46Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/759
mayank31398,use new methods for communication,,2024-03-30T22:07:46Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/758,Marking as stale. No activity in 60 days.
mayank31398,drop redundant check,This check is already done by the above condition. Can we drop this  ?,2024-03-30T21:24:25Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/757,Marking as stale. No activity in 60 days.
ShinoharaHare,[QUESTION] Training Mixtral 8x7B on 16 x H100 only achieves low throughput of 130 TFLOPS,"As the title says, I wonder if this is normal. If not, how should I optimize it?   Logs ``` using world size: 16, dataparallel size: 4, contextparallel size: 1 tensormodelparallel size: 4, pipelinemodelparallel size: 1  WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:Llama2Tokenizer WARNING: Setting args.overlap_p2p_comm to False since noninterleaved schedule does not support overlapping p2p communication accumulate and allreduce gradients in fp32 for bfloat16 data type. using torch.bfloat16 for parameters ...  arguments    accumulate_allreduce_grads_in_fp32 .............. True   adam_beta1 ...................................... 0.9   adam_beta2 ...................................... 0.999   adam_eps ........................................ 1e08   add_bias_linear ................................. False   add_position_embedding .......................... False   add_qkv_bias .................................... False   adlr_autoresume ................................. False   adlr_autoresume_interval ........................ 1000   apply_layernorm_1p .............................. False   apply_query_key_layer_scaling ................... False   apply_residual_connection_post_layernorm ........ False   apply_rope_fusion ............................... True   async_tensor_model_parallel_allreduce ........... False   attention_dropout ............................... 0.0   attention_softmax_in_fp32 ....................... False   auto_detect_ckpt_format ......................... False   barrier_with_L1_time ............................ True   bert_binary_head ................................ True   bert_embedder_type .............................. megatron   bert_load ....................................... None   bf16 ............................................ True   bias_dropout_fusion ............................. True   bias_gelu_fusion ................................ False   bias_swiglu_fusion .............................. True   biencoder_projection_dim ........................ 0   biencoder_shared_query_context_model ............ False   block_data_path ................................. None   check_for_nan_in_loss_and_grad .................. True   classes_fraction ................................ 1.0   clip_grad ....................................... 1.0   clone_scatter_output_in_embedding ............... True   consumed_train_samples .......................... 0   consumed_valid_samples .......................... 0   context_parallel_size ........................... 1   create_attention_mask_in_dataloader ............. True   data_cache_path ................................. None   data_parallel_random_init ....................... False   data_parallel_size .............................. 4   data_path ....................................... []   data_per_class_fraction ......................... 1.0   data_sharding ................................... True   dataloader_type ................................. single   decoder_num_layers .............................. None   decoder_seq_length .............................. None   delay_grad_reduce ............................... True   delay_param_gather .............................. False   dino_bottleneck_size ............................ 256   dino_freeze_last_layer .......................... 1   dino_head_hidden_size ........................... 2048   dino_local_crops_number ......................... 10   dino_local_img_size ............................. 96   dino_norm_last_layer ............................ False   dino_teacher_temp ............................... 0.07   dino_warmup_teacher_temp ........................ 0.04   dino_warmup_teacher_temp_epochs ................. 30   dist_ckpt_format ................................ torch_dist   distribute_saved_activations .................... False   distributed_backend ............................. nccl   distributed_timeout_minutes ..................... 10   embedding_path .................................. None   empty_unused_memory_level ....................... 0   enable_one_logger ............................... False   encoder_num_layers .............................. 32   encoder_seq_length .............................. 2048   end_weight_decay ................................ 0.1   eod_mask_loss ................................... False   eval_interval ................................... 1000   eval_iters ...................................... 10   evidence_data_path .............................. None   exit_duration_in_mins ........................... None   exit_interval ................................... None   exit_on_missing_checkpoint ...................... False   exit_signal_handler ............................. False   expert_model_parallel_size ...................... 4   ffn_hidden_size ................................. 14336   finetune ........................................ False   fp16 ............................................ False   fp16_lm_cross_entropy ........................... False   fp32_residual_connection ........................ False   fp8 ............................................. None   fp8_amax_compute_algo ........................... most_recent   fp8_amax_history_len ............................ 1   fp8_interval .................................... 1   fp8_margin ...................................... 0   fp8_wgrad ....................................... True   global_batch_size ............................... 128   gradient_accumulation_fusion .................... True   group_query_attention ........................... True   head_lr_mult .................................... 1.0   hidden_dropout .................................. 0.0   hidden_size ..................................... 4096   hysteresis ...................................... 2   ict_head_size ................................... None   ict_load ........................................ None   img_h ........................................... 224   img_w ........................................... 224   indexer_batch_size .............................. 128   indexer_log_interval ............................ 1000   inference_batch_times_seqlen_threshold .......... 512   init_method_std ................................. 0.02   init_method_xavier_uniform ...................... False   initial_loss_scale .............................. 4294967296   iter_per_epoch .................................. 1250   kv_channels ..................................... 128   lazy_mpu_init ................................... None   load ............................................ custom/ckpt/mixtral8x7b   local_rank ...................................... None   log_batch_size_to_tensorboard ................... False   log_interval .................................... 1   log_learning_rate_to_tensorboard ................ True   log_loss_scale_to_tensorboard ................... True   log_memory_to_tensorboard ....................... False   log_num_zeros_in_grad ........................... False   log_params_norm ................................. False   log_progress .................................... True   log_throughput .................................. True   log_timers_to_tensorboard ....................... False   log_validation_ppl_to_tensorboard ............... False   log_world_size_to_tensorboard ................... False   loss_scale ...................................... None   loss_scale_window ............................... 1000   lr .............................................. 0.0001   lr_decay_iters .................................. 320000   lr_decay_samples ................................ None   lr_decay_style .................................. cosine   lr_warmup_fraction .............................. None   lr_warmup_init .................................. 0.0   lr_warmup_iters ................................. 500   lr_warmup_samples ............................... 0   make_vocab_size_divisible_by .................... 128   manual_gc ....................................... False   manual_gc_eval .................................. True   manual_gc_interval .............................. 0   mask_factor ..................................... 1.0   mask_prob ....................................... 0.15   mask_type ....................................... random   masked_softmax_fusion ........................... False   max_position_embeddings ......................... 32768   max_tokens_to_oom ............................... 12000   merge_file ...................................... None   micro_batch_size ................................ 1   min_loss_scale .................................. 1.0   min_lr .......................................... 1e05   mmap_bin_files .................................. True   mock_data ....................................... True   moe_aux_loss_coeff .............................. 0.01   moe_grouped_gemm ................................ True   moe_input_jitter_eps ............................ None   moe_router_load_balancing_type .................. aux_loss   moe_router_topk ................................. 2   moe_token_dropping .............................. False   moe_z_loss_coeff ................................ None   nccl_communicator_config_path ................... None   no_load_optim ................................... True   no_load_rng ..................................... True   no_persist_layer_norm ........................... False   no_save_optim ................................... None   no_save_rng ..................................... None   norm_epsilon .................................... 1e05   normalization ................................... RMSNorm   num_attention_heads ............................. 32   num_channels .................................... 3   num_classes ..................................... 1000   num_experts ..................................... 8   num_layers ...................................... 32   num_layers_per_virtual_pipeline_stage ........... None   num_query_groups ................................ 8   num_workers ..................................... 2   one_logger_entity ............................... hwinf_dcm   one_logger_project .............................. e2etracking   one_logger_run_name ............................. None   onnx_safe ....................................... None   openai_gelu ..................................... False   optimizer ....................................... adam   output_bert_embeddings .......................... False   overlap_grad_reduce ............................. False   overlap_p2p_comm ................................ False   overlap_param_gather ............................ False   override_opt_param_scheduler .................... False   params_dtype .................................... torch.bfloat16   patch_dim ....................................... 16   perform_initialization .......................... True   pipeline_model_parallel_size .................... 1   pipeline_model_parallel_split_rank .............. None   position_embedding_type ......................... rope   profile ......................................... True   profile_ranks ................................... [0]   profile_step_end ................................ 12   profile_step_start .............................. 10   qk_layernorm .................................... False   query_in_block_prob ............................. 0.1   rampup_batch_size ............................... None   rank ............................................ 0   recompute_granularity ........................... None   recompute_method ................................ None   recompute_num_layers ............................ None   reset_attention_mask ............................ False   reset_position_ids .............................. False   retriever_report_topk_accuracies ................ []   retriever_score_scaling ......................... False   retriever_seq_length ............................ 256   retro_add_retriever ............................. False   retro_attention_gate ............................ 1   retro_cyclic_train_iters ........................ None   retro_encoder_attention_dropout ................. 0.1   retro_encoder_hidden_dropout .................... 0.1   retro_encoder_layers ............................ 2   retro_num_neighbors ............................. 2   retro_num_retrieved_chunks ...................... 2   retro_project_dir ............................... None   retro_verify_neighbor_count ..................... True   rotary_interleaved .............................. False   rotary_percent .................................. 1.0   rotary_seq_len_interpolation_factor ............. None   sample_rate ..................................... 1.0   save ............................................ custom/ckpt/mixtral8x7b   save_interval ................................... 10000   scatter_gather_tensors_in_pipeline .............. True   seed ............................................ 1234   seq_length ...................................... 2048   sequence_parallel ............................... True   sgd_momentum .................................... 0.9   short_seq_prob .................................. 0.1   skip_train ...................................... False   spec ............................................ None   split ........................................... 99990,8,2   squared_relu .................................... False   standalone_embedding_stage ...................... False   start_weight_decay .............................. 0.1   swiglu .......................................... True   swin_backbone_type .............................. tiny   tensor_model_parallel_size ...................... 4   tensorboard_dir ................................. custom/ckpt/mixtral8x7b/tensorboard   tensorboard_log_interval ........................ 1   tensorboard_queue_size .......................... 1000   test_data_path .................................. None   test_mode ....................................... False   timing_log_level ................................ 0   timing_log_option ............................... minmax   titles_data_path ................................ None   tokenizer_model ................................. tokenizer.model   tokenizer_type .................................. Llama2Tokenizer   tp_comm_bulk_dgrad .............................. True   tp_comm_bulk_wgrad .............................. True   tp_comm_overlap ................................. False   tp_comm_overlap_cfg ............................. None   tp_comm_split_ag ................................ True   tp_comm_split_rs ................................ True   train_data_path ................................. None   train_iters ..................................... 500000   train_samples ................................... None   transformer_impl ................................ transformer_engine   transformer_pipeline_model_parallel_size ........ 1   untie_embeddings_and_output_weights ............. True   use_checkpoint_args ............................. False   use_checkpoint_opt_param_scheduler .............. False   use_cpu_initialization .......................... True   use_dist_ckpt ................................... False   use_distributed_optimizer ....................... True   use_flash_attn .................................. True   use_gpu_initialization .......................... None   use_mcore_models ................................ True   use_one_sent_docs ............................... False   use_ring_exchange_p2p ........................... False   use_rotary_position_embeddings .................. False   valid_data_path ................................. None   variable_seq_lengths ............................ False   virtual_pipeline_model_parallel_size ............ None   vision_backbone_type ............................ vit   vision_pretraining .............................. False   vision_pretraining_type ......................... classify   vocab_extra_ids ................................. 0   vocab_file ...................................... None   vocab_size ...................................... None   wandb_exp_name .................................. mixtral8x7b   wandb_project ................................... megatron   wandb_save_dir ..................................    weight_decay .................................... 0.1   weight_decay_incr_style ......................... constant   world_size ...................................... 16   yaml_cfg ........................................ None  end of arguments  setting number of microbatches to constant 32 > building Llama2Tokenizer tokenizer ...  > padded vocab (size: 32000) with 256 dummy tokens (new size: 32256) > initializing torch distributed ... > initialized tensor model parallel with size 4 > initialized pipeline model parallel with size 1 > setting random seeds to 1234 ... > compiling dataset index builder ... >>> done with dataset index builder. Compilation time: 0.087 seconds WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations. > compiling and loading fused kernels ... >>> done with compiling and loading fused kernels. Compilation time: 7.672 seconds [rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank8]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank9]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank10]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank11]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank12]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank13]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank14]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank15]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) time to initialize megatron (seconds): 16.718 [after megatron is initialized] datetime: 20240330 19:59:35  building GPT model ...  > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 3221491712  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 3221491712  > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 3221491712  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 3221491712 INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient allreduce / reducescatter: 1 INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (402919424 elements): INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.10.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.6.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.18.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.31.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.14.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.12.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.8.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.26.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.13.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.31.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.28.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.21.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.9.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.17.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.14.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.11.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.10.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.6.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.5.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.5.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.31.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.30.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.23.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.20.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.8.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.7.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.21.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.20.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.12.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.9.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.30.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.27.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.10.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.23.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.22.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.19.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.18.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.16.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.13.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.11.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.30.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.18.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.29.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.21.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.19.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.16.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.15.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.12.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.9.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.4.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.26.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.24.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.15.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.4.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.29.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.26.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.17.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.14.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.17.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.25.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.22.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.29.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.20.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.5.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.final_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.28.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.15.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.14.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.28.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.6.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.11.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.6.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.31.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.28.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.13.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.8.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.18.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.17.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.27.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.10.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.5.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.27.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.20.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.8.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.7.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.4.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.23.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.19.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.13.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.12.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.11.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.30.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.27.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.25.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.7.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.4.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.25.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.22.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.21.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.16.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.9.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.23.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.26.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.25.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.24.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.24.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.19.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.7.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.29.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.24.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.22.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.16.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.15.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient allreduce / reducescatter: 1 INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (2818572288 elements): INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.28.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.26.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.24.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.20.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.18.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.7.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.4.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.31.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.29.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.26.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.25.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.15.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.19.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.18.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.11.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.6.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.22.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.17.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.15.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.10.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.5.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.27.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.12.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.8.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.7.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.30.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.28.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.24.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.16.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.13.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.31.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.23.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.19.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.6.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.22.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.16.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.14.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.12.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.10.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.9.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.5.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.21.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.13.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.14.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.8.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.29.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.27.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.25.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.20.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.17.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.30.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.23.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.11.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.21.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.9.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.4.mlp.experts.weight1 INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig(fp16=False, bf16=True, params_dtype=torch.bfloat16, optimizer='adam', lr=0.0001, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, weight_decay=0.1, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e08, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=) > learning rate decay style: cosine WARNING: could not find the metadata file custom/ckpt/mixtral8x7b/latest_checkpointed_iteration.txt      will not load any checkpoints and will start from random > setting tensorboard ... (min, max) time across ranks (ms):     loadcheckpoint ................................: (0.65, 0.93) [after model, optimizer, and learning rate scheduler are built] datetime: 20240330 20:00:43  > building train, validation, and test datasets ...  > datasets target sizes (minimum size):     train:      64000000     validation: 641280     test:       1280 INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = True > building train, validation, and test datasets for GPT ... > finished creating GPT datasets ... [after dataloaders are built] datetime: 20240330 20:00:43  done with setup ... training ... (min, max) time across ranks (ms):     modelandoptimizersetup ......................: (67432.26, 67517.36)     train/valid/testdataiteratorssetup ..........: (3.74, 338.85) [before the start of training step] datetime: 20240330 20:00:43  [Rank 0] (after 1 iterations) memory (MB)  ``` ",2024-03-30T12:16:05Z,stale,closed,0,28,https://github.com/NVIDIA/Megatron-LM/issues/756,"Thank you for reporting this issue. 130 TFLOPS is indeed too low for the H100.  I quickly reviewed your script and have some suggestions: 1. Update the code to the latest main branch and upgrade grouped_gemm to v1.0. 2. Use alltoall dispathcer: moetokendispatchertype alltoall. 3. Use EP8TP2 or EP8PP4. 4. Train for a while (at least 400 steps) before checking performance, or load a pretrained checkpoint. This is because router weights in early stage are not sufficiently trained, leading to imbalanced token distribution.","Hi, thanks for the suggestions. I retested the throuput according to your suggestion. To be more specific: 1. Update MegatronLM the latest commit (https://github.com/NVIDIA/MegatronLM/commit/ba773259dbe5735fbd91ca41e7f4ded60b335c52) 2. Update grouped_gemm to v1.0.0 (https://github.com/fanshiqing/grouped_gemm/commit/7a7f0189797889e926a30b3487512f9539161060) 3. Set `moetokendispatchertype alltoall` 4. Switch to EP=8 & TP=2 5. Use the pretrained weights from Mixtral AI (converted from hf checkpoint) The throughput has indeed increased significantly, reaching around 230 TFLOP/s. However, for H100, it's still pretty low, isn't it? May I ask, theoretically, what would be a more reasonable throughput?   Here is the logs ``` using world size: 16, dataparallel size: 8, contextparallel size: 1 tensormodelparallel size: 2, pipelinemodelparallel size: 1  WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:Llama2Tokenizer WARNING: Setting args.overlap_p2p_comm to False since noninterleaved schedule does not support overlapping p2p communication accumulate and allreduce gradients in fp32 for bfloat16 data type. using torch.bfloat16 for parameters ...  arguments    accumulate_allreduce_grads_in_fp32 .............. True   adam_beta1 ...................................... 0.9   adam_beta2 ...................................... 0.999   adam_eps ........................................ 1e08   add_bias_linear ................................. False   add_position_embedding .......................... True   add_qkv_bias .................................... False   adlr_autoresume ................................. False   adlr_autoresume_interval ........................ 1000   apply_layernorm_1p .............................. False   apply_query_key_layer_scaling ................... False   apply_residual_connection_post_layernorm ........ False   apply_rope_fusion ............................... True   async_tensor_model_parallel_allreduce ........... False   attention_dropout ............................... 0.0   attention_softmax_in_fp32 ....................... False   auto_detect_ckpt_format ......................... False   barrier_with_L1_time ............................ True   bert_binary_head ................................ True   bert_embedder_type .............................. megatron   bert_load ....................................... None   bf16 ............................................ True   bias_dropout_fusion ............................. True   bias_gelu_fusion ................................ False   bias_swiglu_fusion .............................. True   biencoder_projection_dim ........................ 0   biencoder_shared_query_context_model ............ False   block_data_path ................................. None   check_for_nan_in_loss_and_grad .................. True   ckpt_fully_parallel_save ........................ False   ckpt_step ....................................... None   classes_fraction ................................ 1.0   clip_grad ....................................... 1.0   clone_scatter_output_in_embedding ............... True   consumed_train_samples .......................... 0   consumed_valid_samples .......................... 0   context_parallel_size ........................... 1   create_attention_mask_in_dataloader ............. True   data_cache_path ................................. None   data_parallel_random_init ....................... False   data_parallel_size .............................. 8   data_path ....................................... ['custom/data/wudao/wudao_mistralbpe_content_document']   data_per_class_fraction ......................... 1.0   data_sharding ................................... True   dataloader_type ................................. single   decoder_num_layers .............................. None   decoder_seq_length .............................. None   decoupled_lr .................................... None   decoupled_min_lr ................................ None   delay_grad_reduce ............................... True   delay_param_gather .............................. False   dino_bottleneck_size ............................ 256   dino_freeze_last_layer .......................... 1   dino_head_hidden_size ........................... 2048   dino_local_crops_number ......................... 10   dino_local_img_size ............................. 96   dino_norm_last_layer ............................ False   dino_teacher_temp ............................... 0.07   dino_warmup_teacher_temp ........................ 0.04   dino_warmup_teacher_temp_epochs ................. 30   dist_ckpt_format ................................ torch_dist   distribute_saved_activations .................... False   distributed_backend ............................. nccl   distributed_timeout_minutes ..................... 10   embedding_path .................................. None   empty_unused_memory_level ....................... 0   enable_one_logger ............................... False   encoder_num_layers .............................. 32   encoder_seq_length .............................. 2048   end_weight_decay ................................ 0.1   eod_mask_loss ................................... False   eval_interval ................................... 1000   eval_iters ...................................... 1   evidence_data_path .............................. None   exit_duration_in_mins ........................... None   exit_interval ................................... None   exit_on_missing_checkpoint ...................... False   exit_signal_handler ............................. False   expert_model_parallel_size ...................... 8   ffn_hidden_size ................................. 14336   finetune ........................................ False   fp16 ............................................ False   fp16_lm_cross_entropy ........................... False   fp32_residual_connection ........................ False   fp8 ............................................. None   fp8_amax_compute_algo ........................... most_recent   fp8_amax_history_len ............................ 1   fp8_interval .................................... 1   fp8_margin ...................................... 0   fp8_wgrad ....................................... True   global_batch_size ............................... 128   gradient_accumulation_fusion .................... True   group_query_attention ........................... True   head_lr_mult .................................... 1.0   hidden_dropout .................................. 0.0   hidden_size ..................................... 4096   hysteresis ...................................... 2   ict_head_size ................................... None   ict_load ........................................ None   img_h ........................................... 224   img_w ........................................... 224   indexer_batch_size .............................. 128   indexer_log_interval ............................ 1000   inference_batch_times_seqlen_threshold .......... 512   init_method_std ................................. 0.02   init_method_xavier_uniform ...................... False   initial_loss_scale .............................. 4294967296   iter_per_epoch .................................. 1250   kv_channels ..................................... 128   lazy_mpu_init ................................... None   load ............................................ custom/ckpt/mixtral8x7btp2ep8mgg   local_rank ...................................... None   log_batch_size_to_tensorboard ................... False   log_interval .................................... 1   log_learning_rate_to_tensorboard ................ True   log_loss_scale_to_tensorboard ................... True   log_memory_to_tensorboard ....................... False   log_num_zeros_in_grad ........................... False   log_params_norm ................................. False   log_progress .................................... True   log_throughput .................................. True   log_timers_to_tensorboard ....................... False   log_validation_ppl_to_tensorboard ............... False   log_world_size_to_tensorboard ................... False   loss_scale ...................................... None   loss_scale_window ............................... 1000   lr .............................................. 0.0001   lr_decay_iters .................................. 320000   lr_decay_samples ................................ None   lr_decay_style .................................. cosine   lr_warmup_fraction .............................. None   lr_warmup_init .................................. 0.0   lr_warmup_iters ................................. 500   lr_warmup_samples ............................... 0   make_vocab_size_divisible_by .................... 128   manual_gc ....................................... False   manual_gc_eval .................................. True   manual_gc_interval .............................. 0   mask_factor ..................................... 1.0   mask_prob ....................................... 0.15   mask_type ....................................... random   masked_softmax_fusion ........................... False   max_position_embeddings ......................... 32768   max_tokens_to_oom ............................... 12000   merge_file ...................................... None   micro_batch_size ................................ 1   min_loss_scale .................................. 1.0   min_lr .......................................... 1e05   mmap_bin_files .................................. True   mock_data ....................................... False   moe_aux_loss_coeff .............................. 0.01   moe_grouped_gemm ................................ True   moe_input_jitter_eps ............................ None   moe_per_layer_logging ........................... False   moe_router_load_balancing_type .................. aux_loss   moe_router_topk ................................. 2   moe_token_dispatcher_type ....................... alltoall   moe_token_dropping .............................. False   moe_z_loss_coeff ................................ None   nccl_communicator_config_path ................... None   no_load_optim ................................... True   no_load_rng ..................................... True   no_persist_layer_norm ........................... False   no_save_optim ................................... None   no_save_rng ..................................... None   norm_epsilon .................................... 1e05   normalization ................................... RMSNorm   num_attention_heads ............................. 32   num_channels .................................... 3   num_classes ..................................... 1000   num_experts ..................................... 8   num_layers ...................................... 32   num_layers_per_virtual_pipeline_stage ........... None   num_query_groups ................................ 8   num_workers ..................................... 2   one_logger_entity ............................... hwinf_dcm   one_logger_project .............................. e2etracking   one_logger_run_name ............................. None   onnx_safe ....................................... None   openai_gelu ..................................... False   optimizer ....................................... adam   output_bert_embeddings .......................... False   overlap_grad_reduce ............................. False   overlap_p2p_comm ................................ False   overlap_param_gather ............................ False   override_opt_param_scheduler .................... False   params_dtype .................................... torch.bfloat16   patch_dim ....................................... 16   perform_initialization .......................... True   pipeline_model_parallel_size .................... 1   pipeline_model_parallel_split_rank .............. None   position_embedding_type ......................... rope   pretrained_checkpoint ........................... None   profile ......................................... True   profile_ranks ................................... [0]   profile_step_end ................................ 12   profile_step_start .............................. 10   qk_layernorm .................................... False   query_in_block_prob ............................. 0.1   rampup_batch_size ............................... None   rank ............................................ 0   recompute_granularity ........................... None   recompute_method ................................ None   recompute_num_layers ............................ None   reset_attention_mask ............................ False   reset_position_ids .............................. False   retriever_report_topk_accuracies ................ []   retriever_score_scaling ......................... False   retriever_seq_length ............................ 256   retro_add_retriever ............................. False   retro_attention_gate ............................ 1   retro_cyclic_train_iters ........................ None   retro_encoder_attention_dropout ................. 0.1   retro_encoder_hidden_dropout .................... 0.1   retro_encoder_layers ............................ 2   retro_num_neighbors ............................. 2   retro_num_retrieved_chunks ...................... 2   retro_project_dir ............................... None   retro_verify_neighbor_count ..................... True   rotary_interleaved .............................. False   rotary_percent .................................. 1.0   rotary_seq_len_interpolation_factor ............. None   sample_rate ..................................... 1.0   save ............................................ custom/ckpt/mixtral8x7btp2ep8mgg   save_interval ................................... 1000   scatter_gather_tensors_in_pipeline .............. True   seed ............................................ 1234   seq_length ...................................... 2048   sequence_parallel ............................... True   sgd_momentum .................................... 0.9   short_seq_prob .................................. 0.1   skip_train ...................................... False   spec ............................................ None   split ........................................... 99990,8,2   squared_relu .................................... False   standalone_embedding_stage ...................... False   start_weight_decay .............................. 0.1   swiglu .......................................... True   swin_backbone_type .............................. tiny   tensor_model_parallel_size ...................... 2   tensorboard_dir ................................. custom/ckpt/mixtral8x7btp2ep8mgg/tensorboard   tensorboard_log_interval ........................ 1   tensorboard_queue_size .......................... 1000   test_data_path .................................. None   test_mode ....................................... False   timing_log_level ................................ 0   timing_log_option ............................... minmax   titles_data_path ................................ None   tokenizer_model ................................. custom/ckpt/mixtral8x7b/tokenizer.model   tokenizer_type .................................. Llama2Tokenizer   tp_comm_bulk_dgrad .............................. True   tp_comm_bulk_wgrad .............................. True   tp_comm_overlap ................................. False   tp_comm_overlap_ag .............................. True   tp_comm_overlap_cfg ............................. None   tp_comm_overlap_rs .............................. True   tp_comm_split_ag ................................ True   tp_comm_split_rs ................................ True   train_data_path ................................. None   train_iters ..................................... 100   train_samples ................................... None   transformer_impl ................................ transformer_engine   transformer_pipeline_model_parallel_size ........ 1   untie_embeddings_and_output_weights ............. True   use_checkpoint_args ............................. False   use_checkpoint_opt_param_scheduler .............. False   use_cpu_initialization .......................... None   use_dist_ckpt ................................... False   use_distributed_optimizer ....................... True   use_flash_attn .................................. True   use_mcore_models ................................ True   use_one_sent_docs ............................... False   use_ring_exchange_p2p ........................... False   use_rotary_position_embeddings .................. False   valid_data_path ................................. None   variable_seq_lengths ............................ False   virtual_pipeline_model_parallel_size ............ None   vision_backbone_type ............................ vit   vision_pretraining .............................. False   vision_pretraining_type ......................... classify   vocab_extra_ids ................................. 0   vocab_file ...................................... None   vocab_size ...................................... None   wandb_exp_name ..................................    wandb_project ...................................    wandb_save_dir ..................................    weight_decay .................................... 0.1   weight_decay_incr_style ......................... constant   world_size ...................................... 16   yaml_cfg ........................................ None  end of arguments  setting number of microbatches to constant 16 > building Llama2Tokenizer tokenizer ...  > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000) > initializing torch distributed ... make: Entering directory '.../MegatronLM/megatron/core/datasets' make: Nothing to be done for 'default'. make: Leaving directory '.../MegatronLM/megatron/core/datasets' > initialized tensor model parallel with size 2 > initialized pipeline model parallel with size 1 > setting random seeds to 1234 ... > compiling dataset index builder ... >>> done with dataset index builder. Compilation time: 0.104 seconds WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations. > compiling and loading fused kernels ... >>> done with compiling and loading fused kernels. Compilation time: 7.866 seconds [rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank8]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank9]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank10]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank11]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank12]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank13]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank14]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) [rank15]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) time to initialize megatron (seconds): 14.235 [after megatron is initialized] datetime: 20240406 02:54:57  building GPT model ...  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 3622047744  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 3622047744 INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient allreduce / reducescatter: 1 INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (803475456 elements): INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.29.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.23.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.final_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.28.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.18.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.14.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.28.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.14.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.8.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.31.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.25.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.24.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.19.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.17.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.12.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.17.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.8.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.31.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.25.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.22.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.19.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.16.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.11.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.10.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.27.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.21.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.15.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.10.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.27.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.22.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.21.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.30.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.20.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.13.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.30.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.24.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.7.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.29.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.28.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.22.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.16.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.13.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.12.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.6.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.4.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.16.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.15.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.12.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.9.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.8.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.6.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.18.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.26.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.25.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.12.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.4.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.19.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.18.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.9.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.5.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.4.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.28.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.26.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.23.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.20.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.7.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.29.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.23.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.11.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.7.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.5.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.31.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.19.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.14.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.31.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.25.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.24.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.22.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.17.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.16.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.14.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.11.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.24.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.17.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.11.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.10.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.27.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.21.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.20.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.15.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.13.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.13.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.10.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.5.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.4.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.30.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.29.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.27.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.21.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.15.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.6.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.6.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.30.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.9.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.26.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.9.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.26.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.20.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.8.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.5.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.23.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.18.pre_mlp_layernorm.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.7.mlp.router.weight INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient allreduce / reducescatter: 1 INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (2818572288 elements): INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.26.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.13.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.12.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.20.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.11.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.30.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.24.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.18.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.17.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.12.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.4.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.29.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.23.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.16.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.6.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.5.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.27.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.21.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.15.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.9.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.26.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.20.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.14.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.8.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.31.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.30.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.25.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.19.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.18.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.29.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.23.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.22.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.7.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.5.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.28.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.15.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.10.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.27.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.21.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.14.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.13.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.8.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.31.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.25.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.19.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.24.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.22.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.17.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.11.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.7.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.4.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.28.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.16.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.6.mlp.experts.weight1 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.10.mlp.experts.weight2 INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.9.mlp.experts.weight1 INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig(optimizer='adam', lr=0.0001, min_lr=1e05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=False, bf16=True, params_dtype=torch.bfloat16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e08, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=) > learning rate decay style: cosine  loading release checkpoint from custom/ckpt/mixtral8x7btp2ep8mgg could not find arguments in the checkpoint ...  checkpoint version 0  succesfully fixed querykeyvalues ordering for checkpoint version 0   successfully loaded checkpoint from custom/ckpt/mixtral8x7btp2ep8mgg [ t 0, p 0 ] at iteration 0 > setting tensorboard ... (min, max) time across ranks (ms):     loadcheckpoint ................................: (8126.15, 8126.65) [after model, optimizer, and learning rate scheduler are built] datetime: 20240406 02:55:06  > building train, validation, and test datasets ...  > datasets target sizes (minimum size):     train:      12800     validation: 128     test:       128 INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.9999), (0.9999, 0.99998), (0.99998, 1.0)] > building train, validation, and test datasets for GPT ... INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from custom/data/wudao/wudao_mistralbpe_content_document.idx INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 59132211 INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 59132211 INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices INFO:megatron.core.datasets.gpt_dataset:	Load the document index from cc3235b81bd7fd0fa07cabe05d15043dGPTDatasetdocument_index.npy INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from cc3235b81bd7fd0fa07cabe05d15043dGPTDatasetsample_index.npy INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from cc3235b81bd7fd0fa07cabe05d15043dGPTDatasetshuffle_index.npy INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 40201537 INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices INFO:megatron.core.datasets.gpt_dataset:	Load the document index from a625518736b8143e22f4f34c6682183eGPTDatasetdocument_index.npy INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from a625518736b8143e22f4f34c6682183eGPTDatasetsample_index.npy INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from a625518736b8143e22f4f34c6682183eGPTDatasetshuffle_index.npy INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 6204 INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 052434ed70ae721ed70b2219cf2deb88GPTDatasetdocument_index.npy INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 052434ed70ae721ed70b2219cf2deb88GPTDatasetsample_index.npy INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 052434ed70ae721ed70b2219cf2deb88GPTDatasetshuffle_index.npy INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 2332 > finished creating GPT datasets ... [after dataloaders are built] datetime: 20240406 02:55:07  done with setup ... (min, max) time across ranks (ms):     modelandoptimizersetup ......................: (8592.94, 8605.02)     train/valid/testdataiteratorssetup ..........: (569.02, 865.21) training ... [before the start of training step] datetime: 20240406 02:55:07  Number of parameters in transformer layers in billions:  46.44 Number of parameters in embedding layers in billions: 0.26 Total number of parameters in billions: 46.70 Number of parameters in most loaded shard in billions: 23.3510 Theoretical memory footprints: weight and optimizer=167019.40 MB [Rank 0] (after 1 iterations) memory (MB)  [after training is done] datetime: 20240406 03:04:54 ``` ","> Thank you for reporting this issue. 130 TFLOPS is indeed too low for the H100. I quickly reviewed your script and have some suggestions: >  > 1. Update the code to the latest main branch and upgrade grouped_gemm to v1.0. > 2. Use alltoall dispathcer: moetokendispatchertype alltoall. > 3. Use EP8TP2. > 4. Train for a while (at least 400 steps) before checking performance, or load a pretrained checkpoint. This is because router weights in early stage are not sufficiently trained, leading to imbalanced token distribution. If expert_parallel_size==num_moe_experts, the num_local_experts is 1 and GroupedMLP is same as SequentialMLP, is it right? And as I know, the communication overhead of pp is less than tp and ep if the proportion of bubble time is not too high, is MoE support pp and make it more efficient?","> Hi, thanks for the suggestions. I retested the throuput according to your suggestion. To be more specific: >  > 1. Update MegatronLM the latest commit (ba77325) > 2. Update grouped_gemm to v1.0.0 (fanshiqing/grouped_gemm) > 3. Set `moetokendispatchertype alltoall` > 4. Switch to EP=8 & TP=2 > 5. Use the pretrained weights from Mixtral AI (converted from hf checkpoint) >  > The throughput has indeed increased significantly, reaching around 230 TFLOP/s. However, for H100, it's still pretty low, isn't it? May I ask, theoretically, what would be a more reasonable throughput? >  > Here is the logs Apologies for the delayed reply. 230 TFLOPS falls below our expectations; Currently, we can exceed 330TFLOPS on the H100 and potentially higher by switching to EP8TP1 with recomputation.","Does that mean you can achieve over 330 TFLOPS in the same or similar software environment and settings? Should I then suspect hardwarerelated issues, such as network speeds between nodes?","Hi  , our env is: 1. DGX H100, 64 GPUs. 2. pytorch 24.03 image..  I doublechecked your scripts and suggest the following modifications: 1. Seq Len: 2048 > 4096 2. enable dp overlap: overlapgradreduce overlapparamgather Let's see how performance changes after these changes ^ ^.","> > Thank you for reporting this issue. 130 TFLOPS is indeed too low for the H100. I quickly reviewed your script and have some suggestions: > >  > > 1. Update the code to the latest main branch and upgrade grouped_gemm to v1.0. > > 2. Use alltoall dispathcer: moetokendispatchertype alltoall. > > 3. Use EP8TP2. > > 4. Train for a while (at least 400 steps) before checking performance, or load a pretrained checkpoint. This is because router weights in early stage are not sufficiently trained, leading to imbalanced token distribution. >  > If expert_parallel_size==num_moe_experts, the num_local_experts is 1 and GroupedMLP is same as SequentialMLP, is it right? And as I know, the communication overhead of pp is less than tp and ep if the proportion of bubble time is not too high, is MoE support pp and make it more efficient? Hi XLZed, MCore MoE does support PP, but for the Mixtral 8x7B model, we prefer EP and TP.","> > > Thank you for reporting this issue. 130 TFLOPS is indeed too low for the H100. I quickly reviewed your script and have some suggestions: > > >  > > > 1. Update the code to the latest main branch and upgrade grouped_gemm to v1.0. > > > 2. Use alltoall dispathcer: moetokendispatchertype alltoall. > > > 3. Use EP8TP2. > > > 4. Train for a while (at least 400 steps) before checking performance, or load a pretrained checkpoint. This is because router weights in early stage are not sufficiently trained, leading to imbalanced token distribution. > >  > >  > > If expert_parallel_size==num_moe_experts, the num_local_experts is 1 and GroupedMLP is same as SequentialMLP, is it right? And as I know, the communication overhead of pp is less than tp and ep if the proportion of bubble time is not too high, is MoE support pp and make it more efficient? >  > Hi XLZed, MCore MoE does support PP, but for the Mixtral 8x7B model, we prefer EP and TP. Does grouped_gemm support variable token lengths to local experts on the same rank?","> Does grouped_gemm support variable token lengths to local experts on the same rank? Yes, we support variable lengths for inputs from each local expert.","> Hi  , our env is: >  > 1. DGX H100, 64 GPUs. > 2. pytorch 24.03 image.. >  > I doublechecked your scripts and suggest the following modifications: >  > 1. Seq Len: 2048 > 4096 > 2. enable dp overlap: overlapgradreduce overlapparamgather >  > Let's see how performance changes after these changes ^ ^.   Enabling `overlapgradreduce` and `overlapparamgather` will result in a `CUDA error: uncorrectable ECC error encountered`, which seems essentially caused by OOM. I've tried setting the sequence length to 4096 before, but doing so results in a CUDA OOM directly. I've also tried adding `recomputeactivations` in both scenarios, but still get OOM.","> Hi, thanks for the suggestions. > I retested the throuput according to your suggestion. > To be more specific: >  > 1. Update MegatronLM the latest commit (https://github.com/NVIDIA/MegatronLM/commit/ba773259dbe5735fbd91ca41e7f4ded60b335c52) > 2. Update grouped_gemm to v1.0.0 (https://github.com/fanshiqing/grouped_gemm/commit/7a7f0189797889e926a30b3487512f9539161060) > 3. Set `moetokendispatchertype alltoall` > 4. Switch to EP=8 & TP=2 > 5. Use the pretrained weights from Mixtral AI (converted from hf checkpoint) >  > The throughput has indeed increased significantly, reaching around 230 TFLOP/s. > However, for H100, it's still pretty low, isn't it? > May I ask, theoretically, what would be a more reasonable throughput? >  >  >  Here is the logs >  > ``` > using world size: 16, dataparallel size: 8, contextparallel size: 1 tensormodelparallel size: 2, pipelinemodelparallel size: 1  > WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:Llama2Tokenizer > WARNING: Setting args.overlap_p2p_comm to False since noninterleaved schedule does not support overlapping p2p communication > accumulate and allreduce gradients in fp32 for bfloat16 data type. > using torch.bfloat16 for parameters ... >  arguments  >   accumulate_allreduce_grads_in_fp32 .............. True >   adam_beta1 ...................................... 0.9 >   adam_beta2 ...................................... 0.999 >   adam_eps ........................................ 1e08 >   add_bias_linear ................................. False >   add_position_embedding .......................... True >   add_qkv_bias .................................... False >   adlr_autoresume ................................. False >   adlr_autoresume_interval ........................ 1000 >   apply_layernorm_1p .............................. False >   apply_query_key_layer_scaling ................... False >   apply_residual_connection_post_layernorm ........ False >   apply_rope_fusion ............................... True >   async_tensor_model_parallel_allreduce ........... False >   attention_dropout ............................... 0.0 >   attention_softmax_in_fp32 ....................... False >   auto_detect_ckpt_format ......................... False >   barrier_with_L1_time ............................ True >   bert_binary_head ................................ True >   bert_embedder_type .............................. megatron >   bert_load ....................................... None >   bf16 ............................................ True >   bias_dropout_fusion ............................. True >   bias_gelu_fusion ................................ False >   bias_swiglu_fusion .............................. True >   biencoder_projection_dim ........................ 0 >   biencoder_shared_query_context_model ............ False >   block_data_path ................................. None >   check_for_nan_in_loss_and_grad .................. True >   ckpt_fully_parallel_save ........................ False >   ckpt_step ....................................... None >   classes_fraction ................................ 1.0 >   clip_grad ....................................... 1.0 >   clone_scatter_output_in_embedding ............... True >   consumed_train_samples .......................... 0 >   consumed_valid_samples .......................... 0 >   context_parallel_size ........................... 1 >   create_attention_mask_in_dataloader ............. True >   data_cache_path ................................. None >   data_parallel_random_init ....................... False >   data_parallel_size .............................. 8 >   data_path ....................................... ['custom/data/wudao/wudao_mistralbpe_content_document'] >   data_per_class_fraction ......................... 1.0 >   data_sharding ................................... True >   dataloader_type ................................. single >   decoder_num_layers .............................. None >   decoder_seq_length .............................. None >   decoupled_lr .................................... None >   decoupled_min_lr ................................ None >   delay_grad_reduce ............................... True >   delay_param_gather .............................. False >   dino_bottleneck_size ............................ 256 >   dino_freeze_last_layer .......................... 1 >   dino_head_hidden_size ........................... 2048 >   dino_local_crops_number ......................... 10 >   dino_local_img_size ............................. 96 >   dino_norm_last_layer ............................ False >   dino_teacher_temp ............................... 0.07 >   dino_warmup_teacher_temp ........................ 0.04 >   dino_warmup_teacher_temp_epochs ................. 30 >   dist_ckpt_format ................................ torch_dist >   distribute_saved_activations .................... False >   distributed_backend ............................. nccl >   distributed_timeout_minutes ..................... 10 >   embedding_path .................................. None >   empty_unused_memory_level ....................... 0 >   enable_one_logger ............................... False >   encoder_num_layers .............................. 32 >   encoder_seq_length .............................. 2048 >   end_weight_decay ................................ 0.1 >   eod_mask_loss ................................... False >   eval_interval ................................... 1000 >   eval_iters ...................................... 1 >   evidence_data_path .............................. None >   exit_duration_in_mins ........................... None >   exit_interval ................................... None >   exit_on_missing_checkpoint ...................... False >   exit_signal_handler ............................. False >   expert_model_parallel_size ...................... 8 >   ffn_hidden_size ................................. 14336 >   finetune ........................................ False >   fp16 ............................................ False >   fp16_lm_cross_entropy ........................... False >   fp32_residual_connection ........................ False >   fp8 ............................................. None >   fp8_amax_compute_algo ........................... most_recent >   fp8_amax_history_len ............................ 1 >   fp8_interval .................................... 1 >   fp8_margin ...................................... 0 >   fp8_wgrad ....................................... True >   global_batch_size ............................... 128 >   gradient_accumulation_fusion .................... True >   group_query_attention ........................... True >   head_lr_mult .................................... 1.0 >   hidden_dropout .................................. 0.0 >   hidden_size ..................................... 4096 >   hysteresis ...................................... 2 >   ict_head_size ................................... None >   ict_load ........................................ None >   img_h ........................................... 224 >   img_w ........................................... 224 >   indexer_batch_size .............................. 128 >   indexer_log_interval ............................ 1000 >   inference_batch_times_seqlen_threshold .......... 512 >   init_method_std ................................. 0.02 >   init_method_xavier_uniform ...................... False >   initial_loss_scale .............................. 4294967296 >   iter_per_epoch .................................. 1250 >   kv_channels ..................................... 128 >   lazy_mpu_init ................................... None >   load ............................................ custom/ckpt/mixtral8x7btp2ep8mgg >   local_rank ...................................... None >   log_batch_size_to_tensorboard ................... False >   log_interval .................................... 1 >   log_learning_rate_to_tensorboard ................ True >   log_loss_scale_to_tensorboard ................... True >   log_memory_to_tensorboard ....................... False >   log_num_zeros_in_grad ........................... False >   log_params_norm ................................. False >   log_progress .................................... True >   log_throughput .................................. True >   log_timers_to_tensorboard ....................... False >   log_validation_ppl_to_tensorboard ............... False >   log_world_size_to_tensorboard ................... False >   loss_scale ...................................... None >   loss_scale_window ............................... 1000 >   lr .............................................. 0.0001 >   lr_decay_iters .................................. 320000 >   lr_decay_samples ................................ None >   lr_decay_style .................................. cosine >   lr_warmup_fraction .............................. None >   lr_warmup_init .................................. 0.0 >   lr_warmup_iters ................................. 500 >   lr_warmup_samples ............................... 0 >   make_vocab_size_divisible_by .................... 128 >   manual_gc ....................................... False >   manual_gc_eval .................................. True >   manual_gc_interval .............................. 0 >   mask_factor ..................................... 1.0 >   mask_prob ....................................... 0.15 >   mask_type ....................................... random >   masked_softmax_fusion ........................... False >   max_position_embeddings ......................... 32768 >   max_tokens_to_oom ............................... 12000 >   merge_file ...................................... None >   micro_batch_size ................................ 1 >   min_loss_scale .................................. 1.0 >   min_lr .......................................... 1e05 >   mmap_bin_files .................................. True >   mock_data ....................................... False >   moe_aux_loss_coeff .............................. 0.01 >   moe_grouped_gemm ................................ True >   moe_input_jitter_eps ............................ None >   moe_per_layer_logging ........................... False >   moe_router_load_balancing_type .................. aux_loss >   moe_router_topk ................................. 2 >   moe_token_dispatcher_type ....................... alltoall >   moe_token_dropping .............................. False >   moe_z_loss_coeff ................................ None >   nccl_communicator_config_path ................... None >   no_load_optim ................................... True >   no_load_rng ..................................... True >   no_persist_layer_norm ........................... False >   no_save_optim ................................... None >   no_save_rng ..................................... None >   norm_epsilon .................................... 1e05 >   normalization ................................... RMSNorm >   num_attention_heads ............................. 32 >   num_channels .................................... 3 >   num_classes ..................................... 1000 >   num_experts ..................................... 8 >   num_layers ...................................... 32 >   num_layers_per_virtual_pipeline_stage ........... None >   num_query_groups ................................ 8 >   num_workers ..................................... 2 >   one_logger_entity ............................... hwinf_dcm >   one_logger_project .............................. e2etracking >   one_logger_run_name ............................. None >   onnx_safe ....................................... None >   openai_gelu ..................................... False >   optimizer ....................................... adam >   output_bert_embeddings .......................... False >   overlap_grad_reduce ............................. False >   overlap_p2p_comm ................................ False >   overlap_param_gather ............................ False >   override_opt_param_scheduler .................... False >   params_dtype .................................... torch.bfloat16 >   patch_dim ....................................... 16 >   perform_initialization .......................... True >   pipeline_model_parallel_size .................... 1 >   pipeline_model_parallel_split_rank .............. None >   position_embedding_type ......................... rope >   pretrained_checkpoint ........................... None >   profile ......................................... True >   profile_ranks ................................... [0] >   profile_step_end ................................ 12 >   profile_step_start .............................. 10 >   qk_layernorm .................................... False >   query_in_block_prob ............................. 0.1 >   rampup_batch_size ............................... None >   rank ............................................ 0 >   recompute_granularity ........................... None >   recompute_method ................................ None >   recompute_num_layers ............................ None >   reset_attention_mask ............................ False >   reset_position_ids .............................. False >   retriever_report_topk_accuracies ................ [] >   retriever_score_scaling ......................... False >   retriever_seq_length ............................ 256 >   retro_add_retriever ............................. False >   retro_attention_gate ............................ 1 >   retro_cyclic_train_iters ........................ None >   retro_encoder_attention_dropout ................. 0.1 >   retro_encoder_hidden_dropout .................... 0.1 >   retro_encoder_layers ............................ 2 >   retro_num_neighbors ............................. 2 >   retro_num_retrieved_chunks ...................... 2 >   retro_project_dir ............................... None >   retro_verify_neighbor_count ..................... True >   rotary_interleaved .............................. False >   rotary_percent .................................. 1.0 >   rotary_seq_len_interpolation_factor ............. None >   sample_rate ..................................... 1.0 >   save ............................................ custom/ckpt/mixtral8x7btp2ep8mgg >   save_interval ................................... 1000 >   scatter_gather_tensors_in_pipeline .............. True >   seed ............................................ 1234 >   seq_length ...................................... 2048 >   sequence_parallel ............................... True >   sgd_momentum .................................... 0.9 >   short_seq_prob .................................. 0.1 >   skip_train ...................................... False >   spec ............................................ None >   split ........................................... 99990,8,2 >   squared_relu .................................... False >   standalone_embedding_stage ...................... False >   start_weight_decay .............................. 0.1 >   swiglu .......................................... True >   swin_backbone_type .............................. tiny >   tensor_model_parallel_size ...................... 2 >   tensorboard_dir ................................. custom/ckpt/mixtral8x7btp2ep8mgg/tensorboard >   tensorboard_log_interval ........................ 1 >   tensorboard_queue_size .......................... 1000 >   test_data_path .................................. None >   test_mode ....................................... False >   timing_log_level ................................ 0 >   timing_log_option ............................... minmax >   titles_data_path ................................ None >   tokenizer_model ................................. custom/ckpt/mixtral8x7b/tokenizer.model >   tokenizer_type .................................. Llama2Tokenizer >   tp_comm_bulk_dgrad .............................. True >   tp_comm_bulk_wgrad .............................. True >   tp_comm_overlap ................................. False >   tp_comm_overlap_ag .............................. True >   tp_comm_overlap_cfg ............................. None >   tp_comm_overlap_rs .............................. True >   tp_comm_split_ag ................................ True >   tp_comm_split_rs ................................ True >   train_data_path ................................. None >   train_iters ..................................... 100 >   train_samples ................................... None >   transformer_impl ................................ transformer_engine >   transformer_pipeline_model_parallel_size ........ 1 >   untie_embeddings_and_output_weights ............. True >   use_checkpoint_args ............................. False >   use_checkpoint_opt_param_scheduler .............. False >   use_cpu_initialization .......................... None >   use_dist_ckpt ................................... False >   use_distributed_optimizer ....................... True >   use_flash_attn .................................. True >   use_mcore_models ................................ True >   use_one_sent_docs ............................... False >   use_ring_exchange_p2p ........................... False >   use_rotary_position_embeddings .................. False >   valid_data_path ................................. None >   variable_seq_lengths ............................ False >   virtual_pipeline_model_parallel_size ............ None >   vision_backbone_type ............................ vit >   vision_pretraining .............................. False >   vision_pretraining_type ......................... classify >   vocab_extra_ids ................................. 0 >   vocab_file ...................................... None >   vocab_size ...................................... None >   wandb_exp_name ..................................  >   wandb_project ...................................  >   wandb_save_dir ..................................  >   weight_decay .................................... 0.1 >   weight_decay_incr_style ......................... constant >   world_size ...................................... 16 >   yaml_cfg ........................................ None >  end of arguments  > setting number of microbatches to constant 16 > > building Llama2Tokenizer tokenizer ... >  > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000) > > initializing torch distributed ... > make: Entering directory '.../MegatronLM/megatron/core/datasets' > make: Nothing to be done for 'default'. > make: Leaving directory '.../MegatronLM/megatron/core/datasets' > > initialized tensor model parallel with size 2 > > initialized pipeline model parallel with size 1 > > setting random seeds to 1234 ... > > compiling dataset index builder ... > >>> done with dataset index builder. Compilation time: 0.104 seconds > WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations. > > compiling and loading fused kernels ... > >>> done with compiling and loading fused kernels. Compilation time: 7.866 seconds > [rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) > [rank8]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) > [rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) > [rank9]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) > [rank10]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) > [rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) > [rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) > [rank11]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) > [rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) > [rank12]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) > [rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) > [rank13]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) > [rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) > [rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) > [rank14]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) > [rank15]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a noop (function operator()) > time to initialize megatron (seconds): 14.235 > [after megatron is initialized] datetime: 20240406 02:54:57  > building GPT model ... >  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 3622047744 >  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 3622047744 > INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient allreduce / reducescatter: 1 > INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (803475456 elements): > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.29.self_attention.linear_proj.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.23.self_attention.linear_proj.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.final_layernorm.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.28.pre_mlp_layernorm.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.18.self_attention.linear_qkv.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.router.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.pre_mlp_layernorm.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.14.mlp.router.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.28.self_attention.linear_qkv.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.14.self_attention.linear_proj.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.8.self_attention.linear_proj.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.router.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.31.pre_mlp_layernorm.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.25.pre_mlp_layernorm.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.24.mlp.router.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.19.pre_mlp_layernorm.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.17.mlp.router.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.12.mlp.router.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.17.self_attention.linear_proj.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.8.self_attention.linear_qkv.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.31.self_attention.linear_qkv.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.25.self_attention.linear_qkv.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.22.pre_mlp_layernorm.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.19.self_attention.linear_qkv.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.16.pre_mlp_layernorm.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.11.self_attention.linear_qkv.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.10.mlp.router.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.router.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.27.mlp.router.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.21.mlp.router.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.15.mlp.router.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.10.self_attention.linear_proj.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.27.self_attention.linear_proj.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.22.self_attention.linear_qkv.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.21.self_attention.linear_proj.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.30.mlp.router.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.20.pre_mlp_layernorm.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.13.pre_mlp_layernorm.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.30.self_attention.linear_proj.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.24.self_attention.linear_proj.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.7.self_attention.linear_proj.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.29.pre_mlp_layernorm.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.28.self_attention.linear_proj.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.22.self_attention.linear_proj.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.16.self_attention.linear_proj.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.13.self_attention.linear_qkv.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.12.self_attention.linear_proj.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.6.pre_mlp_layernorm.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.4.self_attention.linear_qkv.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.pre_mlp_layernorm.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.16.self_attention.linear_qkv.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.15.self_attention.linear_proj.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.12.self_attention.linear_qkv.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.9.pre_mlp_layernorm.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.8.mlp.router.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.6.self_attention.linear_qkv.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.18.mlp.router.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.26.pre_mlp_layernorm.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.25.mlp.router.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.12.pre_mlp_layernorm.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.4.self_attention.linear_proj.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.pre_mlp_layernorm.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.19.self_attention.linear_proj.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.18.self_attention.linear_proj.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.9.self_attention.linear_qkv.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.5.self_attention.linear_qkv.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.4.pre_mlp_layernorm.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.pre_mlp_layernorm.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.router.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.28.mlp.router.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.26.self_attention.linear_qkv.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.23.pre_mlp_layernorm.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.20.self_attention.linear_qkv.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.7.pre_mlp_layernorm.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.29.self_attention.linear_qkv.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.23.self_attention.linear_qkv.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.11.mlp.router.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.7.self_attention.linear_qkv.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.5.mlp.router.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.31.mlp.router.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.19.mlp.router.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.14.pre_mlp_layernorm.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.31.self_attention.linear_proj.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.25.self_attention.linear_proj.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.24.pre_mlp_layernorm.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.22.mlp.router.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.17.pre_mlp_layernorm.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.16.mlp.router.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.14.self_attention.linear_qkv.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.11.self_attention.linear_proj.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.24.self_attention.linear_qkv.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.17.self_attention.linear_qkv.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.11.pre_mlp_layernorm.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.10.pre_mlp_layernorm.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.27.pre_mlp_layernorm.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.21.pre_mlp_layernorm.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.20.mlp.router.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.15.pre_mlp_layernorm.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.13.mlp.router.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.13.self_attention.linear_proj.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.10.self_attention.linear_qkv.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.5.self_attention.linear_proj.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.4.mlp.router.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.30.pre_mlp_layernorm.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.29.mlp.router.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.27.self_attention.linear_qkv.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.21.self_attention.linear_qkv.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.15.self_attention.linear_qkv.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.6.mlp.router.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.6.self_attention.linear_proj.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.30.self_attention.linear_qkv.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.9.mlp.router.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.26.mlp.router.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.9.self_attention.linear_proj.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.26.self_attention.linear_proj.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.20.self_attention.linear_proj.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.8.pre_mlp_layernorm.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.5.pre_mlp_layernorm.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.23.mlp.router.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.18.pre_mlp_layernorm.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.7.mlp.router.weight > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight > INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient allreduce / reducescatter: 1 > INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (2818572288 elements): > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.26.mlp.experts.weight1 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.13.mlp.experts.weight2 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.12.mlp.experts.weight1 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.experts.weight1 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.20.mlp.experts.weight1 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.11.mlp.experts.weight1 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.30.mlp.experts.weight1 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.24.mlp.experts.weight2 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.18.mlp.experts.weight1 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.17.mlp.experts.weight2 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.12.mlp.experts.weight2 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.4.mlp.experts.weight2 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.experts.weight1 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.29.mlp.experts.weight1 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.23.mlp.experts.weight1 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.16.mlp.experts.weight2 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.6.mlp.experts.weight2 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.5.mlp.experts.weight1 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.27.mlp.experts.weight2 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.21.mlp.experts.weight2 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.15.mlp.experts.weight1 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.9.mlp.experts.weight2 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.26.mlp.experts.weight2 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.20.mlp.experts.weight2 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.14.mlp.experts.weight1 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.8.mlp.experts.weight1 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.experts.weight2 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.31.mlp.experts.weight1 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.30.mlp.experts.weight2 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.25.mlp.experts.weight1 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.19.mlp.experts.weight1 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.18.mlp.experts.weight2 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.experts.weight1 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.29.mlp.experts.weight2 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.23.mlp.experts.weight2 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.22.mlp.experts.weight1 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.7.mlp.experts.weight1 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.5.mlp.experts.weight2 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.experts.weight2 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.28.mlp.experts.weight1 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.15.mlp.experts.weight2 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.experts.weight2 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.10.mlp.experts.weight1 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.27.mlp.experts.weight1 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.21.mlp.experts.weight1 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.14.mlp.experts.weight2 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.13.mlp.experts.weight1 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.8.mlp.experts.weight2 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.experts.weight1 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.31.mlp.experts.weight2 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.25.mlp.experts.weight2 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.19.mlp.experts.weight2 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.24.mlp.experts.weight1 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.22.mlp.experts.weight2 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.17.mlp.experts.weight1 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.11.mlp.experts.weight2 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.7.mlp.experts.weight2 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.4.mlp.experts.weight1 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.28.mlp.experts.weight2 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.16.mlp.experts.weight1 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.6.mlp.experts.weight1 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.experts.weight2 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.10.mlp.experts.weight2 > INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.9.mlp.experts.weight1 > INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig(optimizer='adam', lr=0.0001, min_lr=1e05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=False, bf16=True, params_dtype=torch.bfloat16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e08, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=) > > learning rate decay style: cosine >  loading release checkpoint from custom/ckpt/mixtral8x7btp2ep8mgg > could not find arguments in the checkpoint ... >  checkpoint version 0 >  succesfully fixed querykeyvalues ordering for checkpoint version 0 >   successfully loaded checkpoint from custom/ckpt/mixtral8x7btp2ep8mgg [ t 0, p 0 ] at iteration 0 > > setting tensorboard ... > (min, max) time across ranks (ms): >     loadcheckpoint ................................: (8126.15, 8126.65) > [after model, optimizer, and learning rate scheduler are built] datetime: 20240406 02:55:06  > > building train, validation, and test datasets ... >  > datasets target sizes (minimum size): >     train:      12800 >     validation: 128 >     test:       128 > INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False > INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.9999), (0.9999, 0.99998), (0.99998, 1.0)] > > building train, validation, and test datasets for GPT ... > INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from custom/data/wudao/wudao_mistralbpe_content_document.idx > INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths > INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers > INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices > INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 59132211 > INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 59132211 > INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices > INFO:megatron.core.datasets.gpt_dataset:	Load the document index from cc3235b81bd7fd0fa07cabe05d15043dGPTDatasetdocument_index.npy > INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from cc3235b81bd7fd0fa07cabe05d15043dGPTDatasetsample_index.npy > INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from cc3235b81bd7fd0fa07cabe05d15043dGPTDatasetshuffle_index.npy > INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 40201537 > INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices > INFO:megatron.core.datasets.gpt_dataset:	Load the document index from a625518736b8143e22f4f34c6682183eGPTDatasetdocument_index.npy > INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from a625518736b8143e22f4f34c6682183eGPTDatasetsample_index.npy > INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from a625518736b8143e22f4f34c6682183eGPTDatasetshuffle_index.npy > INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 6204 > INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices > INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 052434ed70ae721ed70b2219cf2deb88GPTDatasetdocument_index.npy > INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 052434ed70ae721ed70b2219cf2deb88GPTDatasetsample_index.npy > INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 052434ed70ae721ed70b2219cf2deb88GPTDatasetshuffle_index.npy > INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 2332 > > finished creating GPT datasets ... > [after dataloaders are built] datetime: 20240406 02:55:07  > done with setup ... > (min, max) time across ranks (ms): >     modelandoptimizersetup ......................: (8592.94, 8605.02) >     train/valid/testdataiteratorssetup ..........: (569.02, 865.21) > training ... > [before the start of training step] datetime: 20240406 02:55:07  > Number of parameters in transformer layers in billions:  46.44 > Number of parameters in embedding layers in billions: 0.26 > Total number of parameters in billions: 46.70 > Number of parameters in most loaded shard in billions: 23.3510 > Theoretical memory footprints: weight and optimizer=167019.40 MB > [Rank 0] (after 1 iterations) memory (MB)  > [after training is done] datetime: 20240406 03:04:54 > ``` >  which modification brings the most speed improvement? btw I encountered some error when converting mixtral from transformers to Megatron when groupedgemm is set, can you share some converting scripts?",  Could you please share your checkpoint conversion script? ,"> which modification brings the most speed improvement? btw I encountered some error when converting mixtral from transformers to Megatron when groupedgemm is set, can you share some converting scripts? The most significant performance change is achieved by resuming from a trained checkpoint. If you do not have pretrained weights, you can train from scratch for about 500 steps. We noticed that after several hundred steps, the token distribution will become quite balanced."," ,  , can you please share a conversion script for Mixtral from HF weights ?",">  ,  , can you please share a conversion script for Mixtral from HF weights ? Hi Vlad, we are working on the converter; it is already in the review process.","   Hi, I'm in a similar situation to this issue. But we also have some differences. For example, we use 8 h800, 64 experts, ep=8, tp=1, pp=1. I also encountered some training efficiency issues, but they were not a top priority. What bothers me now is that after I used ep8 and groupedgemm, my model structure changed. when I try to merge the model with ep=8 into the model with ep=1, it can be loaded by the inference program normally, indicating that the merged shape is correct. But the inference result is incorrect. I want to know if MegatronLM will develop a model convert tool that can facilitate me to merge the ep=8 model into the ep=1 model.  Or could you provide some information on how to merge a groupedgemm enabled model?","Hello  , thank you for the update. Currently, the format for the weights in GroupedGEMM for each expert is [input_size, output_size], which is different from the format used in SequentialMLP's ParallelLinear, [output_size, input_size]. Did you transpose the weight during your conversion?  can help to take a look if this issue continues. By the way, we are also working on supporting distributed checkpointing with Grouped GEMM.","> By the way, we are also working on supporting distributed checkpointing with Grouped GEMM. Yes, we have considered the order of output_size and input_size","  Hi, Could you please help me check my convert tool?","> >  ,  , can you please share a conversion script for Mixtral from HF weights ? >  > Hi Vlad, we are working on the converter; it is already in the review process. Iâ€™m excited about this. When do you plan to merge it into the main branch?",">  Hi, Could you please help me check my convert tool?   ä½ å¥½ï¼Œæˆ‘ä¹Ÿé‡åˆ°åŒæ ·çš„é—®é¢˜ï¼Œè¯·é—®çŽ°åœ¨æœ‰è§£å†³æ–¹æ³•äº†å—ï¼Ÿ","> >  Hi, Could you please help me check my convert tool? >  >  ä½ å¥½ï¼Œæˆ‘ä¹Ÿé‡åˆ°åŒæ ·çš„é—®é¢˜ï¼Œè¯·é—®çŽ°åœ¨æœ‰è§£å†³æ–¹æ³•äº†å—ï¼Ÿ æ²¡æœ‰","Hi , may I ask why you do not prefer PP? I think the PPonly setting should be the fastest one under multinode training > > > Thank you for reporting this issue. 130 TFLOPS is indeed too low for the H100. I quickly reviewed your script and have some suggestions: > > >  > > > 1. Update the code to the latest main branch and upgrade grouped_gemm to v1.0. > > > 2. Use alltoall dispathcer: moetokendispatchertype alltoall. > > > 3. Use EP8TP2. > > > 4. Train for a while (at least 400 steps) before checking performance, or load a pretrained checkpoint. This is because router weights in early stage are not sufficiently trained, leading to imbalanced token distribution. > >  > >  > > If expert_parallel_size==num_moe_experts, the num_local_experts is 1 and GroupedMLP is same as SequentialMLP, is it right? And as I know, the communication overhead of pp is less than tp and ep if the proportion of bubble time is not too high, is MoE support pp and make it more efficient? >  > Hi XLZed, MCore MoE does support PP, but for the Mixtral 8x7B model, we prefer EP and TP.","> Hi , may I ask why you do not prefer PP? I think the PPonly setting should be the fastest one under multinode training Yeah, in multinode training, using PP outperforms EP. But within a node, EP is still the goto. For instance, the best setup for Mixral 8x7B on 64xH100 is TP1EP8PP8, which hits over 400 TFLOPs. However, if there's only 16xH100, using PP2 might lead to an OOM error, so you'd have to use TP2 instead.",">  ,  , can you please share a conversion script for Mixtral from HF weights ? Sorry for the late response. We have added the converter along with the document on https://github.com/NVIDIA/MegatronLM/tree/main/examples/mixtral","Hi  ,  Thank you for the converter. I got a question regarding the current grouped GEMM experts implementation. Before, you wrote that ""By the way, we are also working on supporting distributed checkpointing with Grouped GEMM."" Did you mean saving model checkpoints by ""checkpointing"" or gradient checkpointing? Has it already been implemented or not yet?","> Hi  , Thank you for the converter. >  > I got a question regarding the current grouped GEMM experts implementation. Before, you wrote that ""By the way, we are also working on supporting distributed checkpointing with Grouped GEMM."" >  > Did you mean saving model checkpoints by ""checkpointing"" or gradient checkpointing? Has it already been implemented or not yet?  Paste a slide on distributed checkpointing (distckpt). We've also added distckpt support for GroupedMLP, enabling you to load a sequentialMLP checkpoint with GroupedMLP enabled.",Marking as stale. No activity in 60 days.
Druva24,Need a suggestion and confirmation regarding throughput calculation ,"I have a followup question for a closed issue CC(Question Regarding Throughput(Training Sequences per second)) .  I am currently running BERTLarge Pretraining for 30,000 iterations, for each iteration I am calculating throughput (seq/sec) as global_batch_size/ elapsed_time_per_iteration. Once all the iterations are done, almost all the values of throughput for each iteration are similar, so what should I take as final throughput for this experiment ? Also is this the right way to calculate throughput (seq/sec) ?, I am trying this experiment for below configurations: 1 Node * 1GPU 1 Node * 2 GPUs 1 Node * 4 GPUs 1 Node * 6 GPUs 1 Node * 8 GPUs 1 Node * 10 GPUs Do I need to change anything for calculating throughput for above configurations, or will it be global_batch_size/ elapsed_time_per_iteration ? Note that all the GPUs are H100s,micro batch size (Batch size per GPU) is 64. Reference:  https://github.com/NVIDIA/MegatronLM/blob/main/megatron/training/training.pyL764 https://github.com/NVIDIA/MegatronLM/blob/main/megatron/training/training.pyL766",2024-03-29T22:24:57Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/755,"> Once all the iterations are done, almost all the values of throughput for each iteration are similar, so what should I take as final throughput for this experiment? You can compute the average. > Do I need to change anything for calculating throughput for above configurations, or will it be global_batch_size/ elapsed_time_per_iteration ? Global batch size / elapsed periteration time will give you the throughput in sequences / second, regardless of the configuration you use.",Thank you for your confirmation and suggestion.
pilot7747,Loss mask uses torch.float32 instead of bool,"Hi! I was implementing a custom data loader and faced a deadlock caused by difference in tensors dtype. Then, I discovered that MegatronLM uses `torch.float32` instead of `bool` for loss masks: https://github.com/NVIDIA/MegatronLM/blob/9de386d08770d7296263a590171ace4ae45348ad/megatron/training/utils.pyL320 It's not directly a bug but is there any reasoning behind it? It seems that using boolean masks is more logical and probably reduces the load on communication between devices.",2024-03-29T10:40:00Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/754,Marking as stale. No activity in 60 days.
vasunvidia,CUDA graph changes for LLM,,2024-03-27T17:00:04Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/753,Closing in favor of internal MR.
XLzed,[QUESTION] attention_mask is None in middle pipeline stage?,"The attention_mask is None in middle pipeline stage when the PP_size>=4ï¼Œand the p2p communication send/recv hidden_states only, which made the results are different between different parallelism configs. Is there any workaround to solve it? ``` def get_batch(data_iterator):     """"""Generate a batch.""""""      TODO: this is pretty hacky, find a better way     if (not mpu.is_pipeline_first_stage()) and (not mpu.is_pipeline_last_stage()):         return None, None, None, None, None      get batches based on the TP rank you are on     batch = get_batch_on_this_tp_rank(data_iterator)      slice batch along sequence dimension for context parallelism     batch = get_batch_on_this_cp_rank(batch)     return batch.values() ```",2024-03-27T06:43:28Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/752,"if you use mcore model, the attention mask is generated by transformer engine","> if you use mcore model, the attention mask is generated by transformer engine Thanks! The results are same when enable transformer engine, maybe there need a cached attention_mask in pretrain_gpt.py like nemo."
HashiamKadhim,Fix typo in README.md,Typo fix,2024-03-26T01:17:10Z,stale,open,0,4,https://github.com/NVIDIA/Megatron-LM/issues/751,Marking as stale. No activity in 60 days.,Thanks. I'll include your commit in my internal fix PR.,Your commit is squash merged in https://github.com/NVIDIA/MegatronLM/commit/2488e20d488b35259f6eb53267e8a0525a373599 and your author information is preserved.  Thanks for your contribution. You can close this PR now. ,Marking as stale. No activity in 60 days.
nullnonenilNULL,[QUESTION] why pipeline-model-parallel size should be greater than 2 with interleaved schedule ï¼Ÿ,**Your question** Ask a clear and concise question about MegatronLM. !image,2024-03-25T09:46:33Z,stale,closed,0,6,https://github.com/NVIDIA/Megatron-LM/issues/750,You can't use interleaved schedule without pipeline parallel !image, I wonder whether `pipeline_model_parallel_size == 2` can be accepted?,>  I wonder whether `pipeline_model_parallel_size == 2` can be accepted?  same question.,I think that pipeline_model_parallel_size == 2 can be accepted in practice but maybe with less or no benefits in reducing bubble ?,Marking as stale. No activity in 60 days.,"It is because `tensor_send_next` and `tensor_send_prev` here are indistinguishable with PP=2: https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/pipeline_parallel/p2p_communication.pyL586. This is a nonissue with `overlap_p2p_comm` since we split forward and backward communication in steady state. We fixed this here: https://github.com/NVIDIA/MegatronLM/commit/152c562067cc0de6cbc8fba2a5095208f30d10cd. Going to mark this as closed, feel free to reopen if you have additional questions."
liuliuliu0605,[BUG] ModuleNotFoundError: No module named 'scaled_softmax_cuda',"**Describe the bug** When I try to run single GPU T5 Pretraining with the script `examples/pretrain_t5.sh`, it outputs the following error: > ModuleNotFoundError: No module named 'scaled_softmax_cuda' It seems that the code lacks of module scaled_softmax_cuda or do I need to install the relevant python module ? **Stack trace/logs**  Traceback (most recent call last):   File ""/home/ubuntu/projects/MegatronLM/pretrain_t5.py"", line 239, in      pretrain(train_valid_test_datasets_provider, model_provider, ModelType.encoder_and_decoder,   File ""/home/ubuntu/projects/MegatronLM/megatron/training.py"", line 261, in pretrain     iteration, num_floating_point_operations_so_far = train(   File ""/home/ubuntu/projects/MegatronLM/megatron/training.py"", line 967, in train     train_step(forward_step_func,   File ""/home/ubuntu/projects/MegatronLM/megatron/training.py"", line 532, in train_step     losses_reduced = forward_backward_func(   File ""/home/ubuntu/projects/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 372, in forward_backward_no_pipelining     output_tensor = forward_step(   File ""/home/ubuntu/projects/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 192, in forward_step     output_tensor, loss_func = forward_step_func(data_iterator, model)   File ""/home/ubuntu/projects/MegatronLM/pretrain_t5.py"", line 176, in forward_step     output_tensor = model(tokens_enc,   File ""/home/ubuntu/Venv/torch2.0.0cu118cp310/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1501, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/ubuntu/projects/MegatronLM/megatron/core/distributed/distributed_data_parallel.py"", line 179, in forward     return self.module(*inputs, **kwargs)   File ""/home/ubuntu/Venv/torch2.0.0cu118cp310/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1501, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/ubuntu/projects/MegatronLM/megatron/model/module.py"", line 190, in forward     outputs = self.module(*inputs, **kwargs)   File ""/home/ubuntu/Venv/torch2.0.0cu118cp310/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1501, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/ubuntu/projects/MegatronLM/megatron/model/t5_model.py"", line 118, in forward     lm_output = self.language_model(encoder_input_ids,   File ""/home/ubuntu/Venv/torch2.0.0cu118cp310/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1501, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/ubuntu/projects/MegatronLM/megatron/model/language_model.py"", line 527, in forward     decoder_output = self.decoder(   File ""/home/ubuntu/Venv/torch2.0.0cu118cp310/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1501, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/ubuntu/projects/MegatronLM/megatron/model/transformer.py"", line 1776, in forward     hidden_states = layer(   File ""/home/ubuntu/Venv/torch2.0.0cu118cp310/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1501, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/ubuntu/projects/MegatronLM/megatron/model/transformer.py"", line 1210, in forward     self.default_decoder_cross_attention(   File ""/home/ubuntu/projects/MegatronLM/megatron/model/transformer.py"", line 943, in default_decoder_cross_attention     self.inter_attention(norm_output,   File ""/home/ubuntu/Venv/torch2.0.0cu118cp310/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1501, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/ubuntu/projects/MegatronLM/megatron/model/transformer.py"", line 798, in forward     context_layer = self.core_attention(   File ""/home/ubuntu/Venv/torch2.0.0cu118cp310/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1501, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/ubuntu/projects/MegatronLM/megatron/model/transformer.py"", line 384, in forward     attention_probs = self.scale_mask_softmax(attention_scores,   File ""/home/ubuntu/Venv/torch2.0.0cu118cp310/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1501, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/ubuntu/projects/MegatronLM/megatron/model/fused_softmax.py"", line 148, in forward     return self.forward_fused_softmax(input, mask)   File ""/home/ubuntu/projects/MegatronLM/megatron/model/fused_softmax.py"", line 190, in forward_fused_softmax     return ScaledSoftmax.apply(input, scale)   File ""/home/ubuntu/Venv/torch2.0.0cu118cp310/lib/python3.10/sitepackages/torch/autograd/function.py"", line 506, in apply     return super().apply(*args, **kwargs)   type: ignore[misc]   File ""/home/ubuntu/projects/MegatronLM/megatron/model/fused_softmax.py"", line 80, in forward     import scaled_softmax_cuda ModuleNotFoundError: No module named 'scaled_softmax_cuda'  **Environment (please complete the following information):**   MegatronLM cafda9529d9956578014d4cb89b69b741702b514   PyTorch 2.0.0   CUDA 11.8",2024-03-23T07:28:55Z,stale,open,0,5,https://github.com/NVIDIA/Megatron-LM/issues/749,You may use either of the following solutions: 1. The library `scaled_softmax_cuda` is contained in apex. You may install it from https://github.com/NVIDIA/apex . 2. Add `nomaskedsoftmaxfusion` to avoid the use of fused kernel.,"> You may use either of the following solutions: >  > 1. The library `scaled_softmax_cuda` is contained in apex. You may install it from https://github.com/NVIDIA/apex . > 2. Add `nomaskedsoftmaxfusion` to avoid the use of fused kernel. Thank you for your reply. Solution 2 has fixed the problem. However, after I install apex, the ModuleNotFoundError problem still occurs.  The installing command is as follows: `pip install v disablepipversioncheck nocachedir nobuildisolation configsettings ""buildoption=cpp_ext"" configsettings ""buildoption=cuda_ext"" ./` I use `pip list|grep apex` to obtain  apex with version 0.1 and find **scaled_masked_softmax_cuda.cpython310x86_64linuxgnu.so** in the directory of torch2.0.0cu118cp310/lib/python3.10/sitepackages. Do I fail to install scaled_softmax_cuda?",  I installed apex in the same way. `scaled_softmax_cuda` should also be included in apex. !image," Thanks for providing the details. I rember when I installed apex master branch but failed. The log is install.log. Can it be caused by incompatible cuda version ? So I choose to install apex 22.04dev branch, which actually does not include `scaled_softmax_cuda.cu` file. Therefore, the module `scaled_softmax_cuda` can not be found.",Marking as stale. No activity in 60 days.
jrocmar,Support S3 checkpointing for the torch strategy in distributed checkpointing,"This PR adds support for saving checkpoints to cloud storage (e.g., S3) and loading checkpoints from cloud storage for the torch strategy in distributed checkpointing. It does so by replacing pathlib.Path with cloudpathlib.AnyPath, FileSystemReader with FsspecSystemReader, and FileSytemWriter with FsspecSystemWriter. The PR enables cloud checkpointing, but makes little attempt to optimize it.",2024-03-22T21:56:58Z,stale,open,0,4,https://github.com/NVIDIA/Megatron-LM/issues/748, Would you be the right one to review this? Thanks in advance!,"> Looks good in general, thanks. >  > Nonoptimized cloud upload is understandable, but I want to make sure that the baseline scenario does not regress in terms of performance (only worried about 2stage torch.save) and usability (added `cloudpathlib` requirement for MCore). Thanks for the review! Quick question: do you have a suite of benchmarks that you run to detect performance regressions? If not, I can run some of my own benchmarks and report back.","> > Looks good in general, thanks. > > Nonoptimized cloud upload is understandable, but I want to make sure that the baseline scenario does not regress in terms of performance (only worried about 2stage torch.save) and usability (added `cloudpathlib` requirement for MCore). >  > Thanks for the review! Quick question: do you have a suite of benchmarks that you run to detect performance regressions? If not, I can run some of my own benchmarks and report back. Please run your own benchmarks",Marking as stale. No activity in 60 days.
Mr-Philo,[BUG FIX] Fix world_size bug in QuickStart Example," BUG Description When I entered the developer guide https://docs.nvidia.com/megatroncore/developerguide/latest/userguide/index.htmlquickstart, and running the given example python file run_simple_mcore_train_loop.py, the terminal didn't respond for nearly an hour and throw an exception: ```sh /data/MegatronLM/examples/ torchrun nprocpernode=2 simple_megatron_transformer.py                             [20240322 03:18:07,974] torch.distributed.run: [WARNING]                                                                                               [20240322 03:18:07,974] torch.distributed.run: [WARNING] *****************************************                                                     [20240322 03:18:07,974] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid yo ur system being overloaded, please further tune the variable for optimal performance in your application as needed.                                      [20240322 03:18:07,974] torch.distributed.run: [WARNING] *****************************************                                                     Traceback (most recent call last):                                                                                                                         File ""/data/MegatronLM/examples/run_simple_mcore_train_loop.py"", line 102, in                                                initialize_distributed(tensor_model_parallel_size=2, pipeline_model_parallel_size=1)                                                                   File ""/data/MegatronLM/examples/run_simple_mcore_train_loop.py"", line 25, in initialize_distributed                                  torch.distributed.init_process_group(world_size=world_size, rank=rank)                                                                                 File ""/usr/local/lib/python3.10/distpackages/torch/distributed/c10d_logger.py"", line 74, in wrapper                                                       func_return = func(*args, **kwargs)                                        File ""/usr/local/lib/python3.10/distpackages/torch/distributed/distributed_c10d.py"", line 1153, in init_process_group                                     default_pg, _ = _new_process_group_helper(                                 File ""/usr/local/lib/python3.10/distpackages/torch/distributed/distributed_c10d.py"", line 1269, in _new_process_group_helper                              backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)                                                      RuntimeError: Socket Timeout ```  BUG Reason `initialize_distributed()` function in `MegatronLM/examples/run_simple_mcore_train_loop.py`, the `world_size` is set to `torch.cuda.device_count()`. **However, this activity is actually wrong** if user is running this script on a 8gpus node, but setting `torchrun nprocpernode` to any number that is not 8. This will cause the world_size is not consistent with the gpus actually used in the script. Even worse, it may cause the terminal not responding for a very long time.  BUG Reproduce Whenever you set the number of `torchrun nprocpernode` **not consistent with the total number of gpus on this machine you're using.**  BUG Fix ```diff    world_size = torch.cuda.device_count() +   world_size = int(os.environ[""WORLD_SIZE""]) ``` This change will fix this bug and avoid causing an exception without the need of changing the running command. Meanwhile, this change is adopted both in script file `examples/run_simple_mcore_train_loop.py` and doc md file `megatron/core/QuickStart.md`",2024-03-22T07:07:06Z,stale,closed,2,1,https://github.com/NVIDIA/Megatron-LM/issues/747,Marking as stale. No activity in 60 days.
wuyingjun-lucky,[QUESTION]Why does Megatron-LM using gloo backend when Creating Parrallel Group  ?,"**Your question** Why does MegatronLM using gloo backend not value paased by distributedbackend when Creating Parrallel Group  ? Ask a clear and concise question about MegatronLM. ```  for i in range(pipeline_model_parallel_size):         start_rank = i * num_pipeline_model_parallel_groups         end_rank = (i + 1) * num_pipeline_model_parallel_groups         for j in range(context_parallel_size * tensor_model_parallel_size):             ranks = range(                 start_rank + j, end_rank, context_parallel_size * tensor_model_parallel_size             )             group = torch.distributed.new_group(                 ranks, pg_options=get_nccl_options('dp', nccl_comm_cfgs)             )             group_gloo = torch.distributed.new_group(ranks, backend=""gloo"")             if rank in ranks:                 _DATA_PARALLEL_GROUP = group                 _DATA_PARALLEL_GROUP_GLOO = group_gloo                 _DATA_PARALLEL_GLOBAL_RANKS = ranks ```",2024-03-21T13:11:33Z,stale,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/746,"Most of groups use the default backend. Only a few groups use gloo backend, because gloo is needed for the communication of CPU tensors.",Marking as stale. No activity in 60 days.,"> Most of groups use the default backend. > Only a few groups use gloo backend, because gloo is needed for the communication of CPU tensors. Hi, I found some codes fixed to use gloo , for example https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/parallel_state.pyL557, But I met issue when creating gloo backend. If I manually set all the ""gloo"" to ""nccl"", it works. What is the influence? Will it be okay if we replace all the ""gloo"" to ""nccl"" ? Thank you.",Marking as stale. No activity in 60 days.
vasunvidia,Support Pipelined DGRAD GEMM + RS,,2024-03-21T10:08:59Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/745
rchardx,"[QUESTION] In RotaryEmbedding, the datatype of inv_freq and the corresponding sin/cos computations should be maintained as torch.float32?","**Your question** The RotaryEmbedding of Megatron is of nn.Module type. If the entire model is cast to another type by to(torch.bfloat16), the data type of inv_freq will change accordingly. However, maintaining float32 in subsequent sin/cos calculations seems to be a wise choice. Is this a potential precision issue that could lead to unnecessary calculation errors?",2024-03-21T04:09:10Z,stale,closed,1,1,https://github.com/NVIDIA/Megatron-LM/issues/744,Marking as stale. No activity in 60 days.
lakshya-4gp,[BUG],"**Describe the bug** The sequence length during training is different than specified, in the configs, I've specified seqlen 50016, which is divisible by the tensormodelparallelsize=4, however, during multinode training I'm seeing 50341 as the dimension. ``` lm_output = self.language_model(   File ""/opt/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/opt/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/senseifs/users/lakshya/video_gen/MegatronLM/megatron/model/language_model.py"", line 470, in forward     encoder_input = self.embedding(enc_input_ids, enc_position_ids,   File ""/opt/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/opt/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/senseifs/users/lakshya/video_gen/MegatronLM/megatron/model/language_model.py"", line 239, in forward     embeddings = tensor_parallel.scatter_to_sequence_parallel_region(embeddings)   File ""/senseifs/users/lakshya/video_gen/MegatronLM/megatron/core/tensor_parallel/mappings.py"", line 342, in scatter_to_sequence_parallel_region     return _ScatterToSequenceParallelRegion.apply(input_)   File ""/opt/venv/lib/python3.10/sitepackages/torch/autograd/function.py"", line 539, in apply     return super().apply(*args, **kwargs)   type: ignore[misc]   File ""/senseifs/users/lakshya/video_gen/MegatronLM/megatron/core/tensor_parallel/mappings.py"", line 239, in forward     return _split_along_first_dim(input_)   File ""/senseifs/users/lakshya/video_gen/MegatronLM/megatron/core/tensor_parallel/mappings.py"", line 59, in _split_along_first_dim     dim_size % world_size == 0   AssertionError: First dimension of the tensor should be divisible by tensor parallel size: 50341 % 4 != 0 ``` **To Reproduce** run the pretrain_gpt_distributed_with_mp.sh with following args: `GPUS_PER_NODE=8  Change for multinode config MASTER_ADDR=10.43.176.218 MASTER_PORT=6000 NNODES=6 NODE_RANK=$1 WORLD_SIZE=$(($GPUS_PER_NODE*$NNODES)) CHECKPOINT_PATH=./ckpts/ VOCAB_FILE=datasets/gpt2/gpt2vocab.json MERGE_FILE=datasets/gpt2/gpt2merges.txt DATA_PATH=datasets/gpt2/mygpt2_text_document DISTRIBUTED_ARGS=""     nproc_per_node $GPUS_PER_NODE \     nnodes $NNODES \     node_rank $NODE_RANK \     master_addr $MASTER_ADDR \     master_port $MASTER_PORT "" GPT_ARGS=""     tensormodelparallelsize 4 \     pipelinemodelparallelsize 2 \     sequenceparallel \     numlayers 44 \     hiddensize 1344 \     numattentionheads 24 \     seqlength 50016 \     maxpositionembeddings 50016 \     microbatchsize 1 \     globalbatchsize 12 \     lr 0.00015 \     trainiters 500000 \     lrdecayiters 320000 \     lrdecaystyle cosine \     minlr 1.0e5 \     weightdecay 1e2 \     lrwarmupfraction .01 \     clipgrad 1.0 \     fp16 \     useflashattn ""` **Expected behavior** During training the input sequence length should be 50016 **Stack trace/logs** If applicable, add the stack trace or logs from the time of the error. **Environment (please complete the following information):**   MegatronLM commit ID: using the latest main branch   PyTorch version: 2.1.0   CUDA version: 12.2   NCCL version 2.17.1 **Proposed fix** If you have a proposal for how to fix the issue state it here or link to a PR. **Additional context** Add any other context about the problem here.",2024-03-20T23:47:13Z,stale,open,0,6,https://github.com/NVIDIA/Megatron-LM/issues/743,Marking as stale. No activity in 60 days.,Same error here.,"I have got the same error, with specified dataset, global_batch_size, and sequence_parllel on.",Marking as stale. No activity in 60 days.,Same error here.,Marking as stale. No activity in 60 days.
yanminjia,[QUESTION] Why take too much time to sync up barrier information between ranks,"An issue is identified when we test megatronLM by going with 6B model with 1K GPUs. Basically, by checking the output of each iteration, we found the difference of min and max of paramsallgather with respect a specific iteration is too much as following. In this case, the TFLOPs is low.  !image By checking the code, we believe this issue is caused by the barrier in the timers as illustrated below (distrib_optimizer.py:step()).  ```         timers('paramsallgather', log_level=1).start(barrier=args.barrier_with_L1_time)         self._reset_metadata_and_sync_gather_all_model_params(force_sync=False)         timers('paramsallgather').stop() ``` To debug this issue, we added 1 line to print the timestamp after call as following: ```         timers('paramsallgather', log_level=1).start(barrier=args.barrier_with_L1_time)         print(""go out of the barrier of timer: "", time.time())         self._reset_metadata_and_sync_gather_all_model_params(force_sync=False)         timers('paramsallgather').stop() ``` By checking the timestamps printed by different ranks, we found the maximum time difference between the ranks is 200+ms which is too big.  It would be highly appreciated if any clue of this big time difference. Many thanks in adance. ",2024-03-20T15:02:37Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/742,Marking as stale. No activity in 60 days.
1049451037,[ENHANCEMENT] How to install this package with editable mode?,"I want to install this pkg with `pip install e .`. But it does not work. It's really annoying and I don't know why. I have tried to debug this for days. There are also other defects. For example, I cannot load the pkg in vscode and quick click won't work.",2024-03-19T05:43:32Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/741,Solved by adding a path manually...
OckermanSethGVSU,Update outdated method name passed to get linear_layer function to match intented method that was imported,"Currently, `model/classification.py` throws an error because line 49 attempts to pass a method that doesn't exist (`init_method`) to `get_linear_layer`. This commit updates `classification.py` to instead pass `init_method_normal` to `get_linear_layer`, which is the correct method specified by the import",2024-03-18T02:40:40Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/740,Marking as stale. No activity in 60 days.
yushengsu-thu,Fix finetune.py bug,The args should forward to core_transformer_config_from_args() args = get_args() config = core_transformer_config_from_args()  args = get_args() config = core_transformer_config_from_args(args),2024-03-17T21:01:42Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/739
WanZzzzzz,Communicate over dp group instead of dp-cp group when cp=1 for SHARP enablement,"This fix will prevent SHARP being disabled for DP communication using distributed optimizer.  In a 4node MLperf LLM training workload test, NCCL info shows DP AG/RS collectives using SHARP algo with the fix and not using SHARP without this fix. Also NCCL_SHARP_DISABLE is not used anymore in latest NCCL version. Switch to NCCL_COLLNET_ENABLE.",2024-03-15T22:13:05Z,,closed,1,0,https://github.com/NVIDIA/Megatron-LM/issues/738
tianyu-l,[BUG] cross-entropy loss not computed correctly when label_smoothing is enabled,"**Describe the bug** Currently, when `label_smoothing` is enabled, `mean_log_probs` is computed as a local mean (code pointer). This is not the expected behavior for label smoothing, and can cause the `loss` to be different on different tensor parallel ranks. **Expected behavior** An extra allreduce is needed on the `mean_log_probs` to make it right and consistent.",2024-03-14T23:06:32Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/737,Marking as stale. No activity in 60 days.
hanwen-sun,[QUESTION] why the time of one iter in nsys longer than that in the ouput log?,I want to compare the speed of training llama27b between libai(https://github.com/OneflowInc/libai) and MegatronLM in NVIDIA A800SXM480G. But I find the time of one iter in nsys is longer than the output in log when using MegatronLM;  the log time is: ```  iteration      200/    1000  ```  the nsys time is: !1710414973190 and I can't find many gap in the cuda stream. Can anyone explain this to me?,2024-03-14T11:18:35Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/736,Marking as stale. No activity in 60 days.
ZhangEnmao,[BUG] NCCL TIMEOUT ( maybe ALLREDUCE ? ),"When I use Megatron.core to train a moe model, I got the following bugs : **Output Info :** [rank2]:[E ProcessGroupNCCL.cpp:754] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=305, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600381 milliseconds before timing out. 9aee1c01f3e7:6727:7168 [1] NCCL INFO [Service thread] Connection closed by localRank 2 9aee1c01f3e7:6728:7169 [2] NCCL INFO [Service thread] Connection closed by localRank 2 9aee1c01f3e7:6729:7171 [3] NCCL INFO [Service thread] Connection closed by localRank 2 [rank0]:[E ProcessGroupNCCL.cpp:754] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=305, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600528 milliseconds before timing out. 9aee1c01f3e7:6727:7168 [1] NCCL INFO [Service thread] Connection closed by localRank 0 9aee1c01f3e7:6726:7170 [0] NCCL INFO [Service thread] Connection closed by localRank 0 9aee1c01f3e7:6729:7171 [3] NCCL INFO [Service thread] Connection closed by localRank 0 [rank1]:[E ProcessGroupNCCL.cpp:754] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=305, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600916 milliseconds before timing out. 9aee1c01f3e7:6726:7170 [0] NCCL INFO [Service thread] Connection closed by localRank 1 9aee1c01f3e7:6727:7168 [1] NCCL INFO [Service thread] Connection closed by localRank 1 9aee1c01f3e7:6728:7169 [2] NCCL INFO [Service thread] Connection closed by localRank 1 9aee1c01f3e7:6727:7025 [1] NCCL INFO comm 0x56490665efb0 rank 1 nranks 4 cudaDev 1 busId 5a000  Abort COMPLETE [rank1]:[E ProcessGroupNCCL.cpp:768] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data. [rank1]:[E ProcessGroupNCCL.cpp:774] To avoid data inconsistency, we are taking the entire process down. [rank1]:[E ProcessGroupNCCL.cpp:1282] [Rank 1] NCCL watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=305, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600916 milliseconds before timing out. Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:756 (most recent call first): frame CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string, std::allocator >) + 0x99 (0x7f475399c8f9 in /usr/local/lib/python3.10/distpackages/torch/lib/libc10.so) frame CC(Faster dataloader merge): c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(c10::optional > >) + 0x1f2 (0x7f46f5758142 in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(rename confusing arg): c10d::ProcessGroupNCCL::workCleanupLoop() + 0x178 (0x7f46f575e538 in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(Compatibility with pytorchtransformers for finetuning ): c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x8e (0x7f46f575eb2e in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(Note that as of now you need to have PySOL cloned to the directory here before building the container.):  + 0xdc253 (0x7f47534b0253 in /usr/lib/x86_64linuxgnu/libstdc++.so.6) frame CC(I do not know how to solve it.):  + 0x94ac3 (0x7f475f2a2ac3 in /usr/lib/x86_64linuxgnu/libc.so.6) frame CC(perplexity too big for gpt2 wikitext evaluation): clone + 0x44 (0x7f475f333814 in /usr/lib/x86_64linuxgnu/libc.so.6) terminate called after throwing an instance of 'c10::DistBackendError'   what():  [Rank 1] NCCL watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=305, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600916 milliseconds before timing out. Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:756 (most recent call first): frame CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string, std::allocator >) + 0x99 (0x7f475399c8f9 in /usr/local/lib/python3.10/distpackages/torch/lib/libc10.so) frame CC(Faster dataloader merge): c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(c10::optional > >) + 0x1f2 (0x7f46f5758142 in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(rename confusing arg): c10d::ProcessGroupNCCL::workCleanupLoop() + 0x178 (0x7f46f575e538 in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(Compatibility with pytorchtransformers for finetuning ): c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x8e (0x7f46f575eb2e in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(Note that as of now you need to have PySOL cloned to the directory here before building the container.):  + 0xdc253 (0x7f47534b0253 in /usr/lib/x86_64linuxgnu/libstdc++.so.6) frame CC(I do not know how to solve it.):  + 0x94ac3 (0x7f475f2a2ac3 in /usr/lib/x86_64linuxgnu/libc.so.6) frame CC(perplexity too big for gpt2 wikitext evaluation): clone + 0x44 (0x7f475f333814 in /usr/lib/x86_64linuxgnu/libc.so.6) Exception raised from ncclCommWatchdog at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1286 (most recent call first): frame CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string, std::allocator >) + 0x99 (0x7f475399c8f9 in /usr/local/lib/python3.10/distpackages/torch/lib/libc10.so) frame CC(Faster dataloader merge):  + 0xf59d3e (0x7f46f5786d3e in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(rename confusing arg):  + 0xc91879 (0x7f46f54be879 in /usr/local/lib/python3.10/distpackages/torch/lib/libtorch_cuda.so) frame CC(Compatibility with pytorchtransformers for finetuning ):  + 0xdc253 (0x7f47534b0253 in /usr/lib/x86_64linuxgnu/libstdc++.so.6) frame CC(Note that as of now you need to have PySOL cloned to the directory here before building the container.):  + 0x94ac3 (0x7f475f2a2ac3 in /usr/lib/x86_64linuxgnu/libc.so.6) frame CC(I do not know how to solve it.): clone + 0x44 (0x7f475f333814 in /usr/lib/x86_64linuxgnu/libc.so.6) 9aee1c01f3e7:6726:8015 [0] NCCL INFO [Service thread] Connection closed by localRank 1 9aee1c01f3e7:6729:8029 [3] NCCL INFO [Service thread] Connection closed by localRank 0 9aee1c01f3e7:6726:8080 [0] NCCL INFO [Service thread] Connection closed by localRank 1 9aee1c01f3e7:6729:8090 [3] NCCL INFO [Service thread] Connection closed by localRank 0 [20240314 05:21:58,497] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 6726 closing signal SIGTERM [20240314 05:21:58,497] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 6728 closing signal SIGTERM [20240314 05:21:58,498] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 6729 closing signal SIGTERM [20240314 05:21:58,663] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 6) local_rank: 1 (pid: 6727) of binary: /usr/bin/python Traceback (most recent call last):   File ""/usr/local/bin/torchrun"", line 33, in      sys.exit(load_entry_point('torch==2.2.0a0+81ea7a4', 'console_scripts', 'torchrun')())   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 351, in wrapper     return f(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/run.py"", line 806, in main     run(args)   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/run.py"", line 797, in run     elastic_launch(   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/launcher/api.py"", line 134, in __call__     return launch_agent(self._config, self._entrypoint, list(args))   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/launcher/api.py"", line 264, in launch_agent     raise ChildFailedError( torch.distributed.elastic.multiprocessing.errors.ChildFailedError **Environment (please complete the following information):**   MegatronLM 8957468   PyTorch version 3.10.12   CUDA version 12.3   NCCL : follow cuda If you can answer my question, I would be very grateful.",2024-03-14T07:23:45Z,stale,open,0,8,https://github.com/NVIDIA/Megatron-LM/issues/735,Can confirm the same issue,Marking as stale. No activity in 60 days.,I have also encountered this problem. May I ask if this problem has been resolved and how it was resolved,"I have also encountered this problem, too.","same issue, any update? How can we obtain more detailed debug info?",My bug has been  solve in this PR https://github.com/NVIDIA/TransformerEngine/pull/1031. Hope this helps you.,Marking as stale. No activity in 60 days.,Marking as stale. No activity in 60 days.
OckermanSethGVSU,Replace outdated import path of get_forward_backward_func in eval_utils.py,In `eval_utils.py` the current method of importing  `get_forward_backward_func` uses an outdated path ( `from megatron.schedules`). I replaced this import start with the updated correct path: `from megatron.core.pipeline_parallel import get_forward_backward_func`,2024-03-14T02:51:11Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/734, Thoughts?,Marking as stale. No activity in 60 days.
OckermanSethGVSU,replace outdated import in eval_utils.py ,"`eval_utils.py` currently imports `get_forward_backward_func` from `megatron.schedules`. This is outdated; instead, `get_forward_backward_func` should be imported from `megatron.core.pipeline_parallel`",2024-03-13T23:11:20Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/733
yangzhipeng1108,checkpoint_util.py,python tools/checkpoint_util.py \         modeltype GPT \         loaddir /workspace/model/megatronmodels/345minitmp\         savedir /workspace/model/megatronmodels/345minitmpout \         targettensorparallelsize 1 \         targetpipelineparallelsize 1 !image,2024-03-13T05:45:01Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/732,checkpoint_util.py  failed to run because the vocab_file argument is missing! 
JRD971000,fix torch softmax masking,elementwise multiplication with mask after torch softmax,2024-03-12T20:23:04Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/731,Marking as stale. No activity in 60 days.
yidong72,support more general inference case that query length > 1 ,"Currently the inference requires the query tensor has length 1. However, there are use cases that the query tensor length > 1. Note, this fix requires  1. TE to use the flash_atten > 2.1 so the correct attention mask is applied according to this documentchangebehaviorofcausalflag).   2. TE to enable flash attention by removing this line https://github.com/NVIDIA/TransformerEngine/blob/0fbc76af3733ae997394eaf82b78ff9c0498fe91/transformer_engine/pytorch/attention.pyL2732 3. Latest PyTorch version that has the following bug fix so the batch size 1 value tensor has correct stride after transpose ops, which is required for flash_attn. Or there is a work around by using tensor copy. ```diff  a/megatron/core/transformer/custom_layers/transformer_engine.py +++ b/megatron/core/transformer/custom_layers/transformer_engine.py @@ 457,6 +457,10 @@ class TEDotProductAttention(te.pytorch.DotProductAttention):          if self.config.apply_rope_fusion and qkv_format == 'bshd':              query, key, value = [x.transpose(0, 1).contiguous() for x in (query, key, value)] +         +        new_value = torch.zeros_like(key) +        new_value[:] = value +        value = new_value          if self.te_forward_mask_type:              core_attn_out = super().forward( ```",2024-03-12T13:41:43Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/730,Marking as stale. No activity in 60 days.
jrocmar,Support S3 data loading,"This PR introduces ~~the S3IndexedDataset, which supports loading a dataset stored in S3 in the same format as the MMapIndexedDataset~~ S3 data loading to IndexedDataset. In particular, the .idx file is downloaded to a local directory at initialization so that we can memory map it and the .bin file is streamed into memory blockbyblock.",2024-03-11T19:21:44Z,stale,closed,0,6,https://github.com/NVIDIA/Megatron-LM/issues/729,Draft implementation for CC([ENHANCEMENT] S3 data loading for MMapIndexedDataset),"> Left some comments. Thanks a bunch for the MR! Would like to see some consolidation in S3IndexedDataset before being merged.  Thank you for the comments! Based on those comments and after seeing this refactor, I refactored S3 data loading to integrate it into IndexedDataset and address your feedback."," The _S3BinReader will have poor performance when using a global random shuffle over samples (which is what GPTDataset currently does). I need to either implement ""block shuffling"" in GPTDataset as described in the ""Example"" section here (that section also describes why _S3BinReader will have poor performance) or I need to add an option to disable shuffling in GPTDataset (the user then has to be responsible for preshuffling their data). I'm inclined to just add the option to disable shuffling to start, because it's simpler. What do you think?",">  The _S3BinReader will have poor performance when using a global random shuffle over samples (which is what GPTDataset currently does). I need to either implement ""block shuffling"" in GPTDataset as described in the ""Example"" section here (that section also describes why _S3BinReader will have poor performance) or I need to add an option to disable shuffling in GPTDataset (the user then has to be responsible for preshuffling their data). I'm inclined to just add the option to disable shuffling to start, because it's simpler. What do you think? Moving the ""Example"" section from the old NeMo PR into this comment. In NeMo, a sample consists of `seq_length` tokens. For simplicity, suppose each token is 1 byte and `seq_length` is 100. Each sample then takes 100 bytes. Suppose we have a dataset with 12 samples. Sample index 0 is stored in bytes [0, 100), sample index 1 is stored in bytes [100, 200), ..., and sample index 11 is stored in bytes [1100, 1200). Currently, NeMo takes the list of sample indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11] And produces a `shuffle_idx`, which is just a permutation of those sample indices like: [11, 3, 0, 6, 1, 9, 10, 2, 5, 7, 4, 8] The `shuffle_idx` determines the order in which NeMo processes samples. We could have the `IndexedDataset` just grab the bytes for a sample at a time. The first request would be for the bytes [1100, 1200), the second request would be for the bytes [300, 400), the third request would be for the bytes [0, 100) and so on in the order determined by `shuffle_idx`. That works, but it's slow, because you're making one request for each sample. Let's try to introduce an inmemory cache. In particular, suppose the IndexedDataset does this: * If the requested bytes range [`start`, `end`) is in the cache, then extract the requested bytes range from the cache. * Otherwise, first refresh the cache by downloading the bytes range [`start`, `start` + `cache_nbytes`) and then extract the requested bytes range from the cache. Suppose the `cache_nbytes` is 400. The first request would be for the bytes [1100, 1200). The cache is initially empty, so we refresh the cache by downloading the bytes [1100, 1500) and then extract the requested bytes range from the cache. The second request would be for the bytes [300, 400). Those bytes are not in the cache, so we refresh the cache by downloading the bytes [300, 700) and then extract the requested bytes range from that cache. And so on. We actually made the problem worse. For most samples, we have to refresh the cache, so we have not reduced the number of requests much. We've just made the requests have to download a larger number of bytes. The issue is that the bytes needed for a sample index are probably not next to the bytes needed for the previous sample index. To use the cache effectively, we have to introduce some correlation in the shuffle. In particular, we divide the original list of sample indices into blocks like: * [0, 1, 2, 3] * [4, 5, 6, 7] * [8, 9, 10, 11] We then shuffle within the blocks like: * [3, 0, 2, 1] * [4, 6, 5, 7] * [11, 10, 8, 9] We then shuffle the order of the blocks like: * [11, 10, 8, 9] * [4, 6, 5, 7] * [3, 0, 2, 1] And we construct the blockshuffled `shuffle_idx` like: [11, 10, 8, 9, 4, 6, 5, 7, 3, 0, 2, 1] We also have to change which bytes we download on a cache miss. In particular, we download the bytes [`cache_start`, `cache_start` + `cache_nbytes`), where cache_start is (`start`//`cache_nbytes`) * `cache_nbytes`. The first request would be for the bytes [1100, 1200). The cache is initially empty, so we refresh the cache by downloading the bytes [800, 1200) and then extract the requested bytes range from that cache. The second request would be for the bytes [1000, 1100). We extract those bytes from the cache. The third request would be for the bytes [800, 1200). We extract those bytes from the cache. And so on. In this way, we only have to refresh cache at the start of each new block.",Marking as stale. No activity in 60 days.,"Merged in https://github.com/NVIDIA/MegatronLM/commit/a30a28dbe9063e8456ddc2f5ee1d26ede8589f63 Can mark as closed, thanks"
Ktakuya332C,[QUESTION] Why `--overlap-param-gather` is supported only with mcore models?,"I recently noticed that the `overlapparamgather` option is explicitly disabled when training models without mcore. I found the commit introduced the restriction ( daf0006 ), but cannot understand why this is necessary. As far as I can understand, the changes around that commit was innocuous to the `overlapparamgather` option, and I could not find any particular bug or inconvenience this option might introduce.  If someone knows a likely reason behind the change, I would appreciate if you leave a comment. ( By the way, thank you for sharing this great repository !  )",2024-03-11T05:17:03Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/728,"Good question. The commit you linked to fixes a bug where we had a mismatch in how tied embeddings were laid out in buckets when PP > 1. The fix was to ensure that embedding parameters reside in their own separate buckets. However, the paramgather overlap code relies on PyTorch prehooks to mark synchronization points for the allgather: these don't work for parameters that aren't wrapped in a PyTorch module. For nonMCore models, the embedding copy in the head is not in a PyTorch module, so we will see hangs if the embedding is in its own bucket (a prehook won't be able to tell us when to synchronize on a communication handle). This is the reason I put in this assertion. We are going to deprecate the nonMCore model implementations eventually, so we felt it wasn't a good use of time to fix the nonMCore model.","Thank you for your comment ! I now understands the reason behind the change.  > For nonMCore models, the embedding copy in the head is not in a PyTorch module, so we will see hangs if the embedding is in its own bucket (a prehook won't be able to tell us when to synchronize on a communication handle). I was unable to reproduce the hang (not sure why), but was able to observe that the parameters of the head does not get updated when using `overlapparamgather` on nonmcore models. > We are going to deprecate the nonMCore model implementations eventually, so we felt it wasn't a good use of time to fix the nonMCore model. This issue motivates me to move on to mcore version of the codebase. Thank you for pointing me out."
zhangsheng377,fix bug that must have transformer_engine," CC([BUG] Must have transformer_engine?)  When I use megatron but don't have transformer_engine, it will report error: ```   File ""/home/z00454081/MegatronLMmain/megatron/__init__.py"", line 16, in      from .initialize  import initialize_megatron   File ""/home/z00454081/MegatronLMmain/megatron/initialize.py"", line 18, in      from megatron.arguments import parse_args, validate_args   File ""/home/z00454081/MegatronLMmain/megatron/arguments.py"", line 16, in      from megatron.core.models.retro import RetroConfig   File ""/home/z00454081/MegatronLMmain/megatron/core/models/retro/__init__.py"", line 4, in      from .decoder_spec import get_retro_decoder_block_spec   File ""/home/z00454081/MegatronLMmain/megatron/core/models/retro/decoder_spec.py"", line 5, in      from megatron.core.models.gpt.gpt_layer_specs import (   File ""/home/z00454081/MegatronLMmain/megatron/core/models/gpt/__init__.py"", line 1, in      from .gpt_model import GPTModel   File ""/home/z00454081/MegatronLMmain/megatron/core/models/gpt/gpt_model.py"", line 17, in      from megatron.core.transformer.transformer_block import TransformerBlock   File ""/home/z00454081/MegatronLMmain/megatron/core/transformer/transformer_block.py"", line 16, in      from megatron.core.transformer.custom_layers.transformer_engine import (   File ""/home/z00454081/MegatronLMmain/megatron/core/transformer/custom_layers/transformer_engine.py"", line 7, in      import transformer_engine as te ModuleNotFoundError: No module named 'transformer_engine' ``` This is a bug I think, because I don't want to use transformer_engine, and when I remove the RetroConfig code, I can use megatron normally.",2024-03-09T19:47:30Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/727,Marking as stale. No activity in 60 days.,Aprovado ,Marking as stale. No activity in 60 days.
bescks,[QUESTION]why f and g must conjucates each other?,"**Your question** According to my understanding, there are two facts: 1. the grad op of communication op is still communication(e.g. allreduce's grad is still allreduce, allgather's grad is reducescatter) 2. without considering the gradient allreduce in data parallel, if no communication op exists in forward, neither nor in backward. So, why not use a single function g, whose both forward and backward operation are allreduce instead the two conjucation functions;",2024-03-09T13:34:22Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/726,Marking as stale. No activity in 60 days.
haidark,Add support to track consumed samples for each data shard during training," Issue: Pretraining jobs often span multiple weeks or months and it is often desirable to stop training, modify the dataset by adding/removing datashards or reweighting them, and resume training.  Currently MegatronLM does not support this (safely/without inadvertently repeating data samples) as training jobs are expected to run on a fixed set of blended datashards. The current behavior for resuming training tracks the consumed samples by a single index into the array of samples and resumes training safely if the identical dataset is rebuilt in exactly the same way.  Solution: This PR introduces functionality to track every sample consumed during training and associates the consumed samples to each data shard. When resuming training with a new checkpoint, activating a flag filters the consumed samples from each dataset before blending thereby ensuring data samples are not unintentionally repeated.  Two arguments introduced:  `overridedataloaderconsumedsamples`: allows setting the consumed samples manually  `resumewithnewdataset`: setting this flag filters the consumed samples from each datashard before building the high level datasets  Extra state added to checkpoints: `consumed_samples_per_dataset`: tracks every sample seen during training until this checkpoint for each data shard  Testing A unittest is provided for the new functionality. An integration test was run locally.",2024-03-08T14:14:01Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/725,Marking as stale. No activity in 60 days.
haidark,[Enhancement] Add support to track consumed samples per,,2024-03-08T13:35:48Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/724
acphile,Add timeout parameter for `new_group`,"This PR aims to pass `timeout` parameters to `new_group` function.  Previously, the ProcessGroups created by `new_group` does not set `timeout` parameters, which would make the communications under these ProcessGroup uses the default timeout threshold instead of what users want to set. It is crucial for failure detection.",2024-03-07T22:13:21Z,stale,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/723," , could you help review?","Hi  , thanks so much for the PR! I believe we implemented something similar in parallel: https://github.com/NVIDIA/MegatronLM/commit/e69187bc3679ea5841030a165d587bb48b56ee77. Let me know if you think otherwise!","OK. Thank you for letting me know this commit. Yes, I think the intention is the same ",Marking as stale. No activity in 60 days.
xingyaoww,[BUG] Sequence Parallel is not All-gathered when using Flash Attention?,"**Describe the bug** Does the current implementation of FlashAttention account for the cases of sequence parallelism? For example, here the `self.core_attention_flash` is called, however, the `q, k, v` passed into flash attention are *not* allgathered (correct me if I'm wrong!!), which means we only calculate attention on a chunk of sentences (say total seq_len is `L`, parallelism being 4, we only calculate attention within `L/4` sequence) which will cause issues in the trained models (i.e., the trained model might stop attending to previous content after `L/4` tokens). Their original implementation of `ParallelAttention` does not have this issue since they perform allgather in the forward pass and reducescatter in the backward pass; see this issue for details. **To Reproduce** N/A **Expected behavior** We should perform allgather across sequence parallel dimension before using flashattention, and do reducescatter in the backward pass (just like the `ParallelAttention` implementation). **Stack trace/logs** N/A",2024-03-07T10:50:29Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/722,"Never mind, I dug in deeper to do some interactive debugging and found out the `allgather` operation happens implicitly in `self.query_key_value`, a `ColumnParallelLinear` that helps take care of the allgather. The current implementation should be fine. :)",An interesting question. So does sequence parallel in deepspeed support the scaled_dot_product_attention? Thanks!
shh2000,"[BUG]  ""referenced before assignment"" when training MOE+PP","**Describe the bug** I run the sample given in `core/transformer/moe`, setting TP=1, PP=2, EP=8, on 2 DGXA1008GPUperNode. Other hyperparameters or args remain unchanged. **To Reproduce** write the sample code into `run.sh`, then use command `bash run.sh` on 2 node. **Expected behavior** when TP=2, PP=1, EP=8, the task can be trained well. but when TPPPEP=1/2/8, i got `""UnboundLocalError: local variable 'loss' referenced before assignment""` **Additional context** I see the code at` MegatronLM/megatron/core/pipeline_parallel/schedules.py, line 216`: ``` if parallel_state.is_pipeline_last_stage():         if not collect_non_loss_data:             output_tensor = loss_func(output_tensor)             loss, loss_reduced = output_tensor             output_tensor = loss / num_microbatches             forward_data_store.append(loss_reduced)         else:             data = loss_func(output_tensor, non_loss_data=True)             forward_data_store.append(data)     if config.timers is not None:         config.timers('forwardcompute').stop()      Set the loss scale for the auxiliary loss of the MoE layer.      Since we use a trick to do backward on the auxiliary loss, we need to set the scale explicitly.     if hasattr(config, 'num_moe_experts') and config.num_moe_experts is not None:          Calculate the loss scale based on the grad_scale_func if available, else default to 1.         loss_scale = (             config.grad_scale_func(torch.tensor(1.0, device=loss.device))             if config.grad_scale_func is not None             else torch.tensor(1.0)         )          Set the loss scale         MoEAuxLossAutoScaler.set_loss_scale(loss_scale / num_microbatches) ``` if not last PP stage, there won't be a ""loss"" variable, which is referenced in the next part""device=loss.device"".",2024-03-07T10:06:17Z,stale,closed,0,5,https://github.com/NVIDIA/Megatron-LM/issues/721,You can try this: https://github.com/NVIDIA/MegatronLM/pull/700,"> You can try this: CC(Fix the bug of using loss before assignment) thanks, i'll use this solution locally. Looking forward to PR merged(or NV official update commits)","We have an internal fix for this, we will push soon.",Marking as stale. No activity in 60 days.,"proved, the bug had been fixed, thanks!"
1049451037,[BUG] Inference for multiple nodes not working.,"I can run llama270b with mcore model normally with tensor parallel size = 4. However, when I use tensor parallel size = 8, the inference server hangs forever...",2024-03-07T07:16:05Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/720,Solved... Just set parallel_output=False... I have met this problem several times...
chenfengshijie, why Megatron's ParallelAttention currently only supports SelfAttention with a 'causal' MaskType?,"Could you please explain why Megatron's ParallelAttention currently only supports SelfAttention with a 'causal' MaskType? Also, is there potential for flashAttention support in cases where the Mask is 'None' for both Multihead Self Attention (MSA) and Multihead Cross Attention (MCA)? In Parallel class,I noticed this: !image It appears that the utilization of flashattention is currently confined to causal selfattention. However, considering that flashattention is compatible with padding mask types and crossattention, could you elucidate on any technical challenges or distinctions that might prevent its extension to these contexts?",2024-03-06T02:43:28Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/719,Marking as stale. No activity in 60 days.,you can use MaskType None when `usemcoremodels`
mayank31398,Is this assertion correct?,https://github.com/NVIDIA/MegatronLM/blob/53a350eddbce38f036fe5884c3fec955cf710b9e/megatron/core/datasets/indexed_dataset.pyL293 I have a bunch of datasets that used to work with older megatron (when data logic for GPTDatasets was not moved to core). Now this assertion creates problem for them. Is this correct? The 2 assertions above this make sense but I can't figure out why this must be true?  sorry to tag you since I don't know who to tag,2024-03-05T19:51:28Z,,closed,0,5,https://github.com/NVIDIA/Megatron-LM/issues/718,"Hi Mayank, Thanks for opening the issue. The `document_indices` are the indices which bookend the documents. As such, the final document index should correspond to the final sequence, whose index would be the length of the `sequence_lengths`. Can you please provide prints to help bottom out on this issue? Starting perhaps will all values in the three assertions?",Yeah but is this also true if we do splitting by sentences?,The execution path is very similar. It would help to have the values and to see what kind of difference there is.,"My bad, this is due to an internal bug closing ", Can you disclose what caused your error? We'd like to make sure other users are aware of any potential pitfalls here.
TING2938,add argument `--rotary-base` for gpt model,add argument `rotarybase` for gpt model,2024-03-03T07:19:06Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/717,Marking as stale. No activity in 60 days.
Teng-xu,[BUG] Permormance drop while training with MoE,"**Describe the bug** During our training sessions utilizing Megatron's Mixture of Experts (MoE) layers, we observed a decline in performance occurring at specific steps, with this deterioration manifesting sporadically and inconsistently throughout the training process. We have also done some memory profiling and found that the execution time is predominantly occupied by the all gather and reduce scatter calls, accounting for 99% of the time spent during the lowperformance steps. Hence, we seek insights into potential causes of this performance issue. **To Reproduce** ``` config = TransformerConfig(     tensor_model_parallel_size=1, context_parallel_size=1, pipeline_model_parallel_size=1,     expert_model_parallel_size=2, num_layers=32, hidden_size=4096,      num_attention_heads=32, layernorm_epsilon=1e05,     add_bias_linear=False, activation_func=F.silu, num_moe_experts=8,     fp8=None, normalization='RMSNorm', moe_router_load_balancing_type='sinkhorn',     moe_router_topk=2, moe_grouped_gemm=True, moe_aux_loss_coeff=0.0,     moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False ) ``` **Expected behavior** The throughput should be stable across the training steps. **Stack trace/logs** ``` Batch 138 Loss: 6.425852298736572, Speed: 6.26 samples/sec, Model TFLOPS/GPU: 320.16 Batch 139 Loss: 6.429311275482178, Speed: 6.30 samples/sec, Model TFLOPS/GPU: 321.91 Batch 140 Loss: 6.296842575073242, Speed: 0.05 samples/sec, Model TFLOPS/GPU: 2.39 Batch 141 Loss: 6.297295570373535, Speed: 0.26 samples/sec, Model TFLOPS/GPU: 13.24 ``` !image(1) **Environment (please complete the following information):**   MegatronLM commit ID 5f9c870f9f24b482509699d206a9dbb00958f6fc   PyTorch version PT2.1   CUDA version CUDA12.1   NCCL version 2.18.3",2024-02-29T00:42:32Z,stale,open,0,8,https://github.com/NVIDIA/Megatron-LM/issues/716,"Also encountered the same problem with nonMoE model. I tried to run training job of Llama 13B model on two DGX A100 nodes, but the time breakdown shows: ```     forwardbackward ...............................: (5662.82, 5666.72)     forwardcompute ................................: (2146.30, 2210.56)     backwardcompute ...............................: (3431.20, 3509.58)     batchgenerator ................................: (17.31, 33.45)     layernormgradsallreduce .....................: (5.24, 218.94)     embeddinggradsallreduce .....................: (0.06, 0.11)     allgradssync .................................: (215891.91, 225072.22)     optimizercopytomaingrad ....................: (9.13, 9.19)     optimizerunscaleandcheckinf ................: (9.69, 9.88)     optimizerclipmaingrad .......................: (14.55, 14.77)     optimizercountzeros ..........................: (0.02, 0.07)     optimizerinnerstep ...........................: (31.58, 32.33)     optimizercopymaintomodelparams ............: (9.36, 9.57)     optimizer ......................................: (77.15, 77.37) ``` (I disabled all `overlap*` optimizations and `distributedoptimizer` for more accurate time breakdown. Gradient `AllReduce` takes more than 200 seconds while `forwardbackward` takes just 5.6 seconds. The problem occurs regardless of using `distributedoptimizer`: ```     forwardbackward ...............................: (6640.79, 6647.08)     forwardcompute ................................: (3118.90, 3181.81)     backwardcompute ...............................: (3428.96, 3512.83)     batchgenerator ................................: (16.72, 34.26)     layernormgradsallreduce .....................: (4.97, 11.08)     embeddinggradsallreduce .....................: (0.06, 0.12)     allgradssync .................................: (77025.69, 112368.28)     paramsallgather ..............................: (77461.61, 112343.53)     optimizercopytomaingrad ....................: (4.65, 4.82)     optimizerunscaleandcheckinf ................: (5.37, 5.39)     optimizerclipmaingrad .......................: (7.70, 7.74)     optimizercountzeros ..........................: (0.02, 0.03)     optimizerinnerstep ...........................: (15.89, 16.30)     optimizercopymaintomodelparams ............: (4.53, 4.56)     optimizer ......................................: (77502.36, 112384.28) ``` When I run the same job on a single node, the problem disappears. My environment is  Megatron commit ID: `core_v0.4.0`  NGC PyTorch container version: `23.04`",My issue has been resolved by passing `device=/dev/infiniband` in docker run argument.,"ktaebum's issue is unrelated. We only notice slowdown in some steps, and due to intra node AllGather calls which are surprisingly high for those steps","I have encountered the same problem with MoE, when route type is sinkhorn and topK > 1. !image From my log, I found the main comsumption is from `sinkhorn` function ```python norm_logits = sinkhorn(                     logits.to(dtype=torch.float32)                 ) ```","When topk > 1 and route type is sinkhorn, the sinkhorn function inner code loop thousands times for some `logits` cases. But I didn't found any clue on those `logits`, look similar with normal ones.",xu chen  Thanks for reporting this issue. This could be due to too many iterations in Sinkhorn on some ranks. You can try adding an early stop to Sinkhorn or using aux_loss for load balancing.,how to get Model TFLOPS/GPU?,Marking as stale. No activity in 60 days.
zhipeng93,Fix wrong key for output_layer_init_method,"Given that we aim  to use mcore to do the training, we have a function to parse the args from MegatronLM to mcore. Howover, the key of `output_layer_init_method ` is incorrect. [1] This PR fix this. [1] https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/transformer/transformer_config.pyL95",2024-02-28T10:08:37Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/715,Marking as stale. No activity in 60 days.
zhangsheng377,fix bug that must have transformer_engine," CC([BUG] Must have transformer_engine?)  When I use megatron but don't have transformer_engine, it will report error: ```   File ""/home/z00454081/MegatronLMmain/megatron/__init__.py"", line 16, in      from .initialize  import initialize_megatron   File ""/home/z00454081/MegatronLMmain/megatron/initialize.py"", line 18, in      from megatron.arguments import parse_args, validate_args   File ""/home/z00454081/MegatronLMmain/megatron/arguments.py"", line 16, in      from megatron.core.models.retro import RetroConfig   File ""/home/z00454081/MegatronLMmain/megatron/core/models/retro/__init__.py"", line 4, in      from .decoder_spec import get_retro_decoder_block_spec   File ""/home/z00454081/MegatronLMmain/megatron/core/models/retro/decoder_spec.py"", line 5, in      from megatron.core.models.gpt.gpt_layer_specs import (   File ""/home/z00454081/MegatronLMmain/megatron/core/models/gpt/__init__.py"", line 1, in      from .gpt_model import GPTModel   File ""/home/z00454081/MegatronLMmain/megatron/core/models/gpt/gpt_model.py"", line 17, in      from megatron.core.transformer.transformer_block import TransformerBlock   File ""/home/z00454081/MegatronLMmain/megatron/core/transformer/transformer_block.py"", line 16, in      from megatron.core.transformer.custom_layers.transformer_engine import (   File ""/home/z00454081/MegatronLMmain/megatron/core/transformer/custom_layers/transformer_engine.py"", line 7, in      import transformer_engine as te ModuleNotFoundError: No module named 'transformer_engine' ``` This is a bug I think, because I don't want to use transformer_engine, and when I remove the RetroConfig code, I can use megatron normally.",2024-02-28T02:50:13Z,,closed,1,0,https://github.com/NVIDIA/Megatron-LM/issues/714
zhangsheng377,[BUG] Must have transformer_engine?,"When I use megatron but don't have transformer_engine, it will report error: ```   File ""/home/z00454081/MegatronLMmain/megatron/__init__.py"", line 16, in      from .initialize  import initialize_megatron   File ""/home/z00454081/MegatronLMmain/megatron/initialize.py"", line 18, in      from megatron.arguments import parse_args, validate_args   File ""/home/z00454081/MegatronLMmain/megatron/arguments.py"", line 16, in      from megatron.core.models.retro import RetroConfig   File ""/home/z00454081/MegatronLMmain/megatron/core/models/retro/__init__.py"", line 4, in      from .decoder_spec import get_retro_decoder_block_spec   File ""/home/z00454081/MegatronLMmain/megatron/core/models/retro/decoder_spec.py"", line 5, in      from megatron.core.models.gpt.gpt_layer_specs import (   File ""/home/z00454081/MegatronLMmain/megatron/core/models/gpt/__init__.py"", line 1, in      from .gpt_model import GPTModel   File ""/home/z00454081/MegatronLMmain/megatron/core/models/gpt/gpt_model.py"", line 17, in      from megatron.core.transformer.transformer_block import TransformerBlock   File ""/home/z00454081/MegatronLMmain/megatron/core/transformer/transformer_block.py"", line 16, in      from megatron.core.transformer.custom_layers.transformer_engine import (   File ""/home/z00454081/MegatronLMmain/megatron/core/transformer/custom_layers/transformer_engine.py"", line 7, in      import transformer_engine as te ModuleNotFoundError: No module named 'transformer_engine' ``` This is a bug I think, because I don't want to use transformer_engine, and when I remove the RetroConfig code, I can use megatron normally.",2024-02-28T02:48:00Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/713,This happens when trying to run  `compute_memory_usage.py` as well,Marking as stale. No activity in 60 days.
ZihaoLin0123,[QUESTION] What is the retrieval datasets when evaluating downstream tasks?,"I read the InstructRetro paper and it said  > For downstream task evaluation, we follow Retro (Borgeaud et al., 2022) and use taskspecific corpus and stateoftheart retrievers and DRAGON+ (Lin et al., 2023)) to retrieve the most relevant and highquality information for the task. For NQ and TriviaQA, we use DPR to retrieve from Wikipedia. For doc2dial and other longform QA datasets, we use DRAGON+. Does this mean that for each QA dataset, you only use its corpus as retrieval data? For example, when you evaluate NewsQA, you first collect all the articles of NewsQA **without considering other tasks' corpus such as SQuAD and Wikipedia**, chunk them, and embed them. Then you input a question, and the retriever will retrieve related article chunks from the corpus. Finally, concatenate the retrieved chunks with the input and feed them into LLM. Is that correct?",2024-02-27T19:20:26Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/712,Marking as stale. No activity in 60 days.
yinzhijian,[BUG] Enabling --fp16 and --tp-comm-overlap leads to GEMM reporting inconsistent types,"**Describe the bug** When MegatronLM enables tpcommoverlap, if fp16 is set, subsequent GEMM calculations report errors due to the mismatch in the type of userbuffer being set to bfloat16. The problematic code is as follows: ```python def initialize_ub(     shape: list,     tp_size: int,     use_fp8: bool = False,     ub_cfgs: Optional[dict] = None ) > None: ....     def add_ub(         name: str,         method: str,         num_sm: int = 16,         cga_size: int = 2,         set_sm_margin: int = 0,         num_splits: int = 4,         aggregate: int = 0,     ) > None:         dtype = torch.uint8 if (use_fp8 and name in fp8_buf) else torch.bfloat16         ...``` Traceback (most recent call last):   File ""/workspace/MegatronLM/pretrain_gpt.py"", line 207, in      pretrain(train_valid_test_datasets_provider,   File ""/workspace/MegatronLM/megatron/training.py"", line 258, in pretrain     iteration, num_floating_point_operations_so_far = train(   File ""/workspace/MegatronLM/megatron/training.py"", line 970, in train     train_step(forward_step_func,   File ""/workspace/MegatronLM/megatron/training.py"", line 535, in train_step     losses_reduced = forward_backward_func(   File ""/workspace/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 356, in forward_backward_no_pipelining     output_tensor = forward_step(   File ""/workspace/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 192, in forward_step     output_tensor, loss_func = forward_step_func(data_iterator, model)   File ""/workspace/MegatronLM/pretrain_gpt.py"", line 144, in forward_step     output_tensor = model(tokens, position_ids, attention_mask,   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1510, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1519, in _call_impl     return forward_call(*args, **kwargs)   File ""/workspace/MegatronLM/megatron/core/distributed/distributed_data_parallel.py"", line 138, in forward     return self.module(*inputs, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1510, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1519, in _call_impl     return forward_call(*args, **kwargs)   File ""/workspace/MegatronLM/megatron/model/module.py"", line 189, in forward     outputs = self.module(*inputs, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1510, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1519, in _call_impl     return forward_call(*args, **kwargs)   File ""/workspace/MegatronLM/megatron/core/models/gpt/gpt_model.py"", line 170, in forward     hidden_states = self.decoder(   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1510, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1519, in _call_impl     return forward_call(*args, **kwargs)   File ""/workspace/MegatronLM/megatron/core/transformer/transformer_block.py"", line 370, in forward     hidden_states, context = layer(   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1510, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1519, in _call_impl     return forward_call(*args, **kwargs)   File ""/workspace/MegatronLM/megatron/core/transformer/transformer_layer.py"", line 176, in forward     attention_output_with_bias = self.self_attention(   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1510, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1519, in _call_impl     return forward_call(*args, **kwargs)   File ""/workspace/MegatronLM/megatron/core/transformer/attention.py"", line 254, in forward     query, key, value = self.get_query_key_value_tensors(hidden_states, key_value_states)   File ""/workspace/MegatronLM/megatron/core/transformer/attention.py"", line 370, in get_query_key_value_tensors     mixed_qkv, _ = self.linear_qkv(hidden_states)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1510, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1519, in _call_impl     return forward_call(*args, **kwargs)   File ""/workspace/MegatronLM/megatron/core/transformer/custom_layers/transformer_engine.py"", line 238, in forward     out = super().forward(x, is_first_microbatch=self.is_first_microbatch)   File ""/usr/local/lib/python3.10/distpackages/torch/_dynamo/eval_frame.py"", line 410, in _fn     return fn(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/transformer_engine/pytorch/module/layernorm_linear.py"", line 1005, in forward     out = fwd_fn(*args)   File ""/usr/local/lib/python3.10/distpackages/torch/autograd/function.py"", line 551, in apply     return super().apply(*args, **kwargs)   type: ignore[misc]   File ""/usr/local/lib/python3.10/distpackages/transformer_engine/pytorch/module/layernorm_linear.py"", line 231, in forward     out, _, _ = tex.gemm(   File ""/usr/local/lib/python3.10/distpackages/transformer_engine/pytorch/cpp_extensions/gemm.py"", line 178, in gemm     assert A.dtype == dtype and B.dtype == dtype, \ AssertionError: Expected dtype=torch.float16, but found A.dtype=torch.float16 and B.dtype=torch.bfloat16 **To Reproduce** GPT_ARGS=""     usemcoremodels \     numlayers 40 \     hiddensize 5120 \     numattentionheads 40 \     seqlength 4096 \     maxpositionembeddings 4096 \     microbatchsize 1 \     globalbatchsize 32 \     lr 0.00015 \     recomputeactivations \     trainiters 500000 \     lrdecayiters 320000 \     lrdecaystyle cosine \     minlr 1.0e5 \     weightdecay 1e2 \     lrwarmupfraction .01 \     clipgrad 1.0 \     usedistributedoptimizer \     useflashattn \     untieembeddingsandoutputweights \     noasynctensormodelparallelallreduce \     sequenceparallel \     tensormodelparallelsize 2 \     tpcommoverlap \     fp16 "" python pretrain_gpt.py \     $GPT_ARGS \ **Expected behavior** tpcommoverlap supports fp16 **Stack trace/logs** If applicable, add the stack trace or logs from the time of the error. **Environment (please complete the following information):**   transformerengine        1.2.1+bbafb02   MegatronLM commit ID   PyTorch version   CUDA version   NCCL version **Proposed fix** If you have a proposal for how to fix the issue state it here or link to a PR. **Additional context** Add any other context about the problem here.",2024-02-27T11:24:19Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/711,Marking as stale. No activity in 60 days.
1049451037,[BUG] text generation not working for --position-embedding-type rope,"When I use the official `examples/run_text_generation` scripts, I can run normally without rope. But when I use rope, the text generation will raise 500 error: ```     return self._call_impl(*args, **kwargs)   File ""/envs//megatron/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1538, in _call_impl     return forward_call(*args, **kwargs)   File ""/envs//MegatronLM/megatron/core/models/gpt/gpt_model.py"", line 183, in forward     hidden_states = self.decoder(   File ""/envs//megatron/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1529, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/envs//megatron/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1538, in _call_impl     return forward_call(*args, **kwargs)   File ""/envs//MegatronLM/megatron/core/transformer/transformer_block.py"", line 382, in forward     hidden_states, context = layer(   File ""/envs//megatron/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1529, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/envs//megatron/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1538, in _call_impl     return forward_call(*args, **kwargs)   File ""/envs//MegatronLM/megatron/core/transformer/transformer_layer.py"", line 176, in forward     attention_output_with_bias = self.self_attention(   File ""/envs//megatron/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1529, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/envs//megatron/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1538, in _call_impl     return forward_call(*args, **kwargs)   File ""/envs//MegatronLM/megatron/core/transformer/attention.py"", line 305, in forward     core_attn_out = self.core_attention(   File ""/envs//megatron/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1529, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/envs//megatron/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1538, in _call_impl     return forward_call(*args, **kwargs)   File ""/envs//MegatronLM/megatron/core/transformer/custom_layers/transformer_engine.py"", line 466, in forward     core_attn_out = super().forward(   File ""/envs//megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/attention.py"", line 2552, in forward     qkv_layout, query_layer, key_layer, value_layer = _get_qkv_layout(   File ""/envs//megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/attention.py"", line 1466, in _get_qkv_layout     raise Exception(""The provided qkv memory layout is not supported!"") Exception: The provided qkv memory layout is not supported! ```",2024-02-26T07:24:34Z,stale,open,0,6,https://github.com/NVIDIA/Megatron-LM/issues/710,I solved the problem by hardcode... https://github.com/NVIDIA/MegatronLM/issues/703issuecomment1965759788,I found that the reason is because transpose_output_memory was set to True.,No. changing transpose_output_memory to False will cause training raising error.,Marking as stale. No activity in 60 days., This simply because you were using a mcore model (mcore model has bugs). You switch to legacy model : uselegacymodel (now mcore is the default),Marking as stale. No activity in 60 days.
okoge-kaz,Fix: checkpoint load bug (TE)," Issue When using TransformerEngine with MegatronLM for training, I encountered an issue where the Loss Curve would significantly change after loading a checkpoint. This problem did not occur when TransformerEngine was not utilized. With TransformerEngine !W B Chart 2_26_2024, 3_49_43 PM Without TransformerEngine !W B Chart 2_26_2024, 3_56_54 PM  Overview Upon investigation to resolve this issue, I noticed that during load_state_dict, the strict parameter was set to False. This appears to be a measure to prevent errors during loading due to the name of keys in TransformerEngine `transformer_engine.pytorch.TransformerLayer` being different from those in the MegatronLM implementation. However, bypassing the error by setting strict=False resulted in checkpoints not being loaded correctly. To correct this, I made the necessary changes.  Result After changes ( With TransformerEngine) !image !image",2024-02-26T07:05:32Z,stale,closed,1,1,https://github.com/NVIDIA/Megatron-LM/issues/709,Marking as stale. No activity in 60 days.
1049451037,[BUG] Cannot train model with seq_length > 1024,"For GPT training, everything works fine when I run it with seqlength 1024. But when I change 1024 to 2048, scripts cannot run. This is the error reported when I run `examples/pretrain_gpt_distributed_with_mp.sh` with `usemcoremodels`, `maxpositionembeddings 2048` and `seqlength 2048`: ``` > finished creating GPT datasets ...                                                                                                               [after dataloaders are built] datetime: 20240226 14:15:27                                                                                        done with setup ...                                                                                                                                (min, max) time across ranks (ms):                                                                                                                     modelandoptimizersetup ......................: (341.91, 359.40)                                                                                 train/valid/testdataiteratorssetup ..........: (163.57, 333.68)                                                                             training ...                                                                                                                                       [before the start of training step] datetime: 20240226 14:15:28                                                                                  [rank0]: Traceback (most recent call last):                                                                                                        [rank0]:   File ""/envs//MegatronLM/pretrain_gpt.py"", line 209, in                                                                [rank0]:     pretrain(train_valid_test_datasets_provider,                                                                                          [rank0]:   File ""/envs//MegatronLM/megatron/training.py"", line 258, in pretrain                                                          [rank0]:     iteration, num_floating_point_operations_so_far = train(                                                                              [rank0]:   File ""/envs//MegatronLM/megatron/training.py"", line 970, in train                                                             [rank0]:     train_step(forward_step_func,                                                                                                         [rank0]:   File ""/envs//MegatronLM/megatron/training.py"", line 535, in train_step                                                        [rank0]:     losses_reduced = forward_backward_func(                                                                                               [rank0]:   File ""/envs//MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 1211, in forward_backward_pipelining_without_inte rleaving                                                                                                                                           [rank0]:     output_tensor = forward_step(                                                                                                         [rank0]:   File ""/envs//MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 192, in forward_step                              [rank0]:     output_tensor, loss_func = forward_step_func(data_iterator, model)                                                                    [rank0]:   File ""/envs//MegatronLM/pretrain_gpt.py"", line 142, in forward_step                                                           [rank0]:     tokens, labels, loss_mask, attention_mask, position_ids = get_batch(                                                                  [rank0]:   File ""/envs//MegatronLM/pretrain_gpt.py"", line 91, in get_batch                                                               [rank0]:     batch = get_batch_on_this_tp_rank(data_iterator)                                                                                      [rank0]:   File ""/envs//MegatronLM/megatron/utils.py"", line 285, in get_batch_on_this_tp_rank                                            [rank0]:     data = next(data_iterator)                                                                                                            [rank0]:   File ""/envs/megatron/lib/python3.10/sitepackages/torch/utils/data/dataloader.py"", line 631, in __next__              [rank0]:     data = self._next_data()                                                                                                              [rank0]:   File ""/envs/megatron/lib/python3.10/sitepackages/torch/utils/data/dataloader.py"", line 1346, in _next_data           [rank0]:     return self._process_data(data)                                                                                                       [rank0]:   File ""/envs/megatron/lib/python3.10/sitepackages/torch/utils/data/dataloader.py"", line 1372, in _process_data        [rank0]:     data.reraise()                                                                                                                        [rank0]:   File ""/envs/megatron/lib/python3.10/sitepackages/torch/_utils.py"", line 705, in reraise                              [rank0]:     raise exception                                                                                                                       [rank0]: RuntimeError: Caught RuntimeError in DataLoader worker process 0.                                                                         [rank0]: Original Traceback (most recent call last):                                                                                               [rank0]:   File ""/envs/megatron/lib/python3.10/sitepackages/torch/utils/data/_utils/worker.py"", line 308, in _worker_loop       [rank0]:     data = fetcher.fetch(index)   type: ignore[possiblyundefined]                                                                       [rank0]:   File ""/envs/megatron/lib/python3.10/sitepackages/torch/utils/data/_utils/fetch.py"", line 54, in fetch                [rank0]:     return self.collate_fn(data)                                                                                                          [rank0]:   File ""/envs/megatron/lib/python3.10/sitepackages/torch/utils/data/_utils/collate.py"", line 277, in default_collate   [rank0]:     return collate(batch, collate_fn_map=default_collate_fn_map)                                                                          [rank0]:   File ""/envs/megatron/lib/python3.10/sitepackages/torch/utils/data/_utils/collate.py"", line 129, in collate           [rank0]:     return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})                              [rank0]:   File ""/envs/megatron/lib/python3.10/sitepackages/torch/utils/data/_utils/collate.py"", line 129, in  [rank0]:     return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})            [rank0]:   File ""/envs/megatron/lib/python3.10/sitepackages/torch/utils/data/_utils/collate.py"", line 121, in collate [rank0]:     return collate_fn_mapelem_type                                              [rank0]:   File ""/envs/megatron/lib/python3.10/sitepackages/torch/utils/data/_utils/collate.py"", line 173, in collate_tensor_fn [rank0]:     out = elem.new(storage).resize_(len(batch), *list(elem.size()))        [rank0]: RuntimeError: Trying to resize storage that is not resizable ```",2024-02-26T06:21:50Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/708," I encountered the same error, have you found the reason?","Hi ctrl , it's not a bug. Just ensure that your seq_length of your data labels are right. Especially when you set `contextparallelsize` > 1."
anlongfei,Does Megatron has plan to support Gemmaï¼Ÿ,**Your question** Does Megatron has plan to support Gemmaï¼Ÿ,2024-02-26T03:32:05Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/707,nemo (using megatroncore) supports it https://github.com/NVIDIA/NeMo/blob/f005f1323eaf9a23ad6dc4bc326dc95bf0002e8d/examples/nlp/language_modeling/conf/megatron_gemma_config.yamlL4,also looking for example of gemma in megatronlm,Marking as stale. No activity in 60 days.
TING2938,add megatron_mcore saver/loader,add saver/loader for mcore checkpoint convert,2024-02-25T07:36:41Z,stale,open,2,13,https://github.com/NVIDIA/Megatron-LM/issues/706,"Hi, I think your conversation code has some potential bugs: https://github.com/NVIDIA/MegatronLM/issues/703","> Hi, I think your conversation code has some potential bugs: >  > CC(Is there any tool to convert hf llama2 to Megatron with **mcore** model instead of legacy model?) I try it with vicuna 7b model, the inference is ok !image !image you can add some conversation template for inference",I changed the model from llama27b to llama27bchat. Now it works! Thank you!,"However, when I try llama270bchat, it doesn't work anymore... (I run with tensor parallel = 2 and pipeline parallel = 2) Is there any problem with transformation or MegatronLM? The problem is that the numbers are never shown in terminal:  Enter prompt: What is the answer of 1+1=? Enter number of tokens to generate: 100 Megatron Response:  `[INST] >` You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. `>` What is the answer of 1+1=? [/INST]   The answer to the equation is ""The answer to the equation is two (or ""the result is two""). **The calculation works like this because when you add one plus one (or +), the result is two (or =).** Therefore the answer is two."" Is there anything else I can help you with ?","I find that there is something wrong with tensor parallel for transformation. Here is the experiments: 1. TP=4, PP=1 âŒ 2. TP=2, PP=2 âŒ 3. TP=1, PP=4 âœ… So... how to fix it?","Finally, I figured it out that I have to set `parallel_output=False` to make inference work with tensor parallel.","> Finally, I figured it out that I have to set `parallel_output=False` to make inference work with tensor parallel. Thanks for your tests, text_generation_server on main branch does not support mcore inference, maybe we can fix it https://github.com/NVIDIA/MegatronLM/blob/ad53b1e38689a0ceed75ade7821f4e6c7554abb4/tools/run_text_generation_server.pyL22",I just use this model provider by setting parallel_output=False: https://github.com/NVIDIA/MegatronLM/blob/ad53b1e38689a0ceed75ade7821f4e6c7554abb4/pretrain_gpt.pyL54L66 It works fine in text_generation_server.,"Moreover, I find that padding vocab is necessary to finetune llama70b with tensor parallel >= 4.", Is there a tool available for converting checkpoints that include optimizer states while changing the tensor parallelism (TP) and pipeline parallelism (PP) configurations? Thank you.,Marking as stale. No activity in 60 days.,">  Is there a tool available for converting checkpoints that include optimizer states while changing the tensor parallelism (TP) and pipeline parallelism (PP) configurations? Thank you. The repo already include tools to produces weights in partition (tp/pp mode), you can include optimization states when doing sft/continue train jobs. ",Marking as stale. No activity in 60 days.
ghost,[ENHANCEMENT] BlendedDataset in extreme cases can lead to an IndexError.,"> âš ï¸ This discussion is based on the old version of Megatron. This problem still exists in the new version of Megatron.  1. Background Megatron's data blending algorithm is based on a function in helpers.cpp: `build_blending_indices`. This function **IS NOT ROBUST**, and users can construct extreme test cases to cause bugs in Megatron's training. Specifically, in the `__getitem__` method of the `BlendableDataset` class, `sample_idx >= len(self.datasets[dataset_idx])` may appear(Because there is no limit on the growth of `current_samples`), causing the index to go out of bounds. Consider **two identical** datasets being blended, but the weights assigned to them are all extreme, one approaching 0 and one approaching 1, such as 0.001 and 0.999. In this case, according to the algorithm in `build_blending_indices`, the dataset with the smaller weight will be sampled for more than one epoch, thus causing an `IndexError`.  2. How to trigger this bug? Assume that we have generated the corresponding binary files: `data_text_document.bin` and `data_text_document.idx`. The files do not need to be too large, just appropriate. Use the following code to examine this dataset: ```python from megatron.data.indexed_dataset import make_dataset data_prefix = '/path/to/data_text_document' indexed_dataset = make_dataset(data_prefix, 'mmap', True) sizes = indexed_dataset.sizes all_token_counts = np.sum(sizes) print(""document count:"", len(sizes) / 1000000, ""M"") print(""all token count:"", all_token_counts / 1000000000, ""B"") print(""average token count:"", all_token_counts / len(sizes)) print(""max token count:"", np.max(sizes)) print(""min token count:"", np.min(sizes)) print(""tp50 token count:"", np.percentile(sizes, 50)) print(""tp90 token count:"", np.percentile(sizes, 90)) print(""tp99 token count:"", np.percentile(sizes, 99)) print(""tp999 token count:"", np.percentile(sizes, 99.9)) ``` Output: ``` document count: 0.0001 M all token count: 0.000108625 B average token count: 1086.25 max token count: 4970 min token count: 146 tp50 token count: 793.5 tp90 token count: 2316.4000000000005 tp99 token count: 4306.7000000000035 tp999 token count: 4903.670000000007 ``` It can be seen that this dataset has a total of 100 documents. Consider blending two identical datasets and set the `traindatapath` parameter in the training script like this: ```bash traindatapath 0.999 /path/to/data_text_document 0.001 /path/to/data_text_document ``` Then print the following information in the source code of `BlendableDataset` for observation: ```python class BlendableDataset(torch.utils.data.Dataset):     def __init__(self, datasets, weights):         self.datasets = datasets         num_datasets = len(datasets)         assert num_datasets == len(weights)         for i, dataset in enumerate(self.datasets):             print_rank_0(f""dataset {i}: {len(dataset)}"")   print information         self.size = 0         for dataset in self.datasets:             self.size += len(dataset)          Normalize weights.         weights = np.array(weights, dtype=np.float64)         sum_weights = np.sum(weights)         assert sum_weights > 0.0         weights /= sum_weights          Build indecies.         start_time = time.time()         assert num_datasets  elapsed time for building blendable dataset indices: '                      '{:.2f} (sec)'.format(time.time()  start_time))         print_rank_0(f""dataset_index: {self.dataset_index}"")   print information         print_rank_0(f""dataset_sample_index: {self.dataset_sample_index}"")   print information ``` Then run the training script and first observe the output: ``` dataset 0: 1060 dataset 1: 106 dataset_index: [0 1 0 ... 0 0 0] dataset_sample_index: [   0    0    1 ... 1161 1162 1163] ``` It is obvious that the size of the first dataset is only 1060, but `sample_idx` has exceeded this value in several places(1161, 1162, 1163), which will inevitably cause the index to go out of bounds. If we fix to only take the last sample of the `BlendableDataset` each time, **the bug can be triggered stably**: ```python def __getitem__(self, idx):     idx = self.size  1    Always take the last sample of BlendableDataset     dataset_idx = self.dataset_index[idx]     sample_idx = self.dataset_sample_index[idx]     return {         ""dataset_idx"" : dataset_idx,         **self.datasets[dataset_idx][sample_idx],   ""text"": ndarray     } ```  3. How to fix? Fixing this bug is not difficult, just let `sample_idx` take the modulus of the length of the corresponding dataset.",2024-02-24T09:48:57Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/705,Marking as stale. No activity in 60 days.
condy0919,Fix typos.,Those typos are reported by codespell.,2024-02-23T17:17:08Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/704,Marking as stale. No activity in 60 days.
1049451037,Is there any tool to convert hf llama-2 to Megatron with **mcore** model instead of legacy model?,"I have converted the model followed by https://github.com/NVIDIA/MegatronLM/blob/main/docs/llama2.mdhuggingfaceformat , but the generated model is legacy GPT model. I want to use mcore model.",2024-02-23T03:01:58Z,,closed,1,11,https://github.com/NVIDIA/Megatron-LM/issues/703,"I write the model conversion code, but it hasn't been fully tested yet, maybe you can have a try.  https://github.com/TING2938/MegatronLM/tree/main/tools/checkpoint",Thank you for your reply! I have transformed the model and got a checkpoint file. How can I run your converted model for inference?,I think there is something wrong... I use the official text generation server and cli in MegatronLM to run inference. The output of llama27b is weird: ,"Now there is a new problem... ```   File ""/megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/attention.py"", line 2552, in forward     qkv_layout, query_layer, key_layer, value_layer = _get_qkv_layout(  File ""/megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/attention.py"", line 1466, in _get_qkv_layout     raise Exception(""The provided qkv memory layout is not supported!"") Exception: The provided qkv memory layout is not supported! ```","> Now there is a new problem... >  > ``` >   File ""/megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/attention.py"", line 2552, in forward >     qkv_layout, query_layer, key_layer, value_layer = _get_qkv_layout(  File ""/megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/attention.py"", line 1466, in _get_qkv_layout >     raise Exception(""The provided qkv memory layout is not supported!"") > Exception: The provided qkv memory layout is not supported! > ``` what is the version of your transformer_engin? you can update to stable, pip install git+https://github.com/NVIDIA/TransformerEngine.git",My version is stable exactly.,"I also tried the main branch of TransformerEngine. However, same problem occurs.","Finally, I make it work by adding lines of hardcode after https://github.com/NVIDIA/MegatronLM/blob/5f9c870f9f24b482509699d206a9dbb00958f6fc/megatron/core/transformer/attention.pyL289 ```python             assert key.shape == value.shape             value = torch.as_strided(value, value.shape, key.stride()) ``` However, the inference result is not right... So your transformation code has some bugs... ``` Enter prompt: Q: What is the capital of China? A: Enter number of tokens to generate: 10 Megatron Response:  Q: What is the capital of China? A: Confusion. Anyone can answer the question: $ ```",I changed the model from llama27b to llama27bchat. Now it works! Thank you!,"> > Now there is a new problem... > > ``` > >   File ""/megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/attention.py"", line 2552, in forward > >     qkv_layout, query_layer, key_layer, value_layer = _get_qkv_layout(  File ""/megatron/lib/python3.10/sitepackages/transformer_engine/pytorch/attention.py"", line 1466, in _get_qkv_layout > >     raise Exception(""The provided qkv memory layout is not supported!"") > > Exception: The provided qkv memory layout is not supported! > > ``` >  > what is the version of your transformer_engin? >  > you can update to stable, pip install git+https://github.com/NVIDIA/TransformerEngine.git I encounter with a bug when using your saver_llama2_hf to convert mcore llama270b to hf  in saver_llama2_hf, the queue module report that cannot find package megatron.  I solved it by adding the following code in the start of save_checkpoint function `sys.path.append(os.path.abspath(         os.path.join(os.path.dirname(__file__),                      os.path.pardir,                      os.path.pardir)))     sys.path.insert(0, 'pathtoyourmegatrondir')`","> I changed the model from llama27b to llama27bchat. Now it works! Thank you! Yes you should use sft model (chat) to do the QA test. These SFT models are instructed to answer questions. Pretrain models is just learn the ""capacity"" or knowledge, they don't know how to ""use"" them."
ghtaro,[QUESTION] Megatron-LM installation with CUDA 11.6,"Hi, I am using singlenode multiGPU cluster (A100 * 6) and would like to use MegatronLM to learn llama2 model on that. On the computational environment, I am not able to use docker and the CUDA version is fixed at 11.6.  My question is if it is possible to install MegatronLM on my environment. After installing required packages (including apex), I did ``` pip install git+https://github.com/NVIDIA/MegatronLM.git cd MegatronLM pip install . ``` and run a shell script basically just calling pretrain_gpt.py. It fails with the following error message saying ""no module named transformer_engine"". ``` ~/pretrain/MegatronLM$ sh examples/pretrain_llama2.sh  + DATASET_1=./dataset/arxiv_train_text_sentence + DATASET=1 ./dataset/arxiv_train_text_sentence + CHECKPOINT_PATH=./outputs/llama21.3b/checkpoints/ + TOKENIZER_PATH=./tokenizer/code10k_en20k_ja30k.ver2.1.model + TP=1 + PP=2 + GPUS_PER_NODE=2 + MASTER_ADDR=localhost + MASTER_PORT=6000 + NNODES=1 + NODE_RANK=0 + HIDDEN_SIZE=2048 + FFN_HIDDEN_SIZE=5504 + NUM_LAYERS=24 + NUM_HEADS=16 + SEQ_LENGTH=2048 + NUM_KV_HEADS=4 + MICRO_BATCH_SIZE=4 + GLOBAL_BATCH_SIZE=32 + TRAIN_STEPS=2500 + LR=3e4 + MIN_LR=3e5 + LR_WARMUP_STEPS=20 + WEIGHT_DECAY=0.1 + GRAD_CLIP=1 + activation_checkpoint=false + DISTRIBUTED_ARGS=nproc_per_node 2 nnodes 1 node_rank 0 master_addr localhost master_port 6000 + torchrun nproc_per_node 2 nnodes 1 node_rank 0 master_addr localhost master_port 6000 pretrain_gpt.py tensormodelparallelsize 1 pipelinemodelparallelsize 2 numlayers 24 hiddensize 2048 ffnhiddensize 5504 numattentionheads 16 microbatchsize 4 globalbatchsize 32 seqlength 2048 maxpositionembeddings 2048 trainiters 2500 save ./outputs/llama21.3b/checkpoints/ load ./outputs/llama21.3b/checkpoints/ datapath 1 ./dataset/arxiv_train_text_sentence dataimpl mmap tokenizertype SentencePieceTokenizer tokenizermodel ./tokenizer/code10k_en20k_ja30k.ver2.1.model split 949,50,1 distributedbackend nccl lr 3e4 lrdecaystyle cosine minlr 3e5 weightdecay 0.1 clipgrad 1 lrwarmupiters 20 optimizer adam adambeta1 0.9 adambeta2 0.95 loginterval 1 saveinterval 10000 evalinterval 1000 evaliters 10 bf16 noquerykeylayerscaling attentiondropout 0 hiddendropout 0 userotarypositionembeddings untieembeddingsandoutputweights swiglu normalization rmsnorm disablebiaslinear numkeyvalueheads 4 WARNING:torch.distributed.run: ***************************************** Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.  ***************************************** Zarrbased strategies will not be registered because of missing packages Traceback (most recent call last):   File ""/home/mais/pretrain/MegatronLM/pretrain_gpt.py"", line 8, in  Traceback (most recent call last):   File ""/home/mais/pretrain/MegatronLM/pretrain_gpt.py"", line 8, in      from megatron import get_args   File ""/home/mais/pretrain/MegatronLM/megatron/__init__.py"", line 16, in      from megatron import get_args   File ""/home/mais/pretrain/MegatronLM/megatron/__init__.py"", line 16, in      from .initialize  import initialize_megatron   File ""/home/mais/pretrain/MegatronLM/megatron/initialize.py"", line 18, in      from .initialize  import initialize_megatron   File ""/home/mais/pretrain/MegatronLM/megatron/initialize.py"", line 18, in      from megatron.arguments import parse_args, validate_args   File ""/home/mais/pretrain/MegatronLM/megatron/arguments.py"", line 16, in      from megatron.arguments import parse_args, validate_args   File ""/home/mais/pretrain/MegatronLM/megatron/arguments.py"", line 16, in      from megatron.core.models.retro import RetroConfig   File ""/home/mais/pretrain/MegatronLM/megatron/core/models/retro/__init__.py"", line 4, in      from megatron.core.models.retro import RetroConfig   File ""/home/mais/pretrain/MegatronLM/megatron/core/models/retro/__init__.py"", line 4, in      from .decoder_spec import get_retro_decoder_block_spec   File ""/home/mais/pretrain/MegatronLM/megatron/core/models/retro/decoder_spec.py"", line 5, in      from .decoder_spec import get_retro_decoder_block_spec   File ""/home/mais/pretrain/MegatronLM/megatron/core/models/retro/decoder_spec.py"", line 5, in      from megatron.core.models.gpt.gpt_layer_specs import (   File ""/home/mais/pretrain/MegatronLM/megatron/core/models/gpt/__init__.py"", line 1, in      from megatron.core.models.gpt.gpt_layer_specs import (   File ""/home/mais/pretrain/MegatronLM/megatron/core/models/gpt/__init__.py"", line 1, in      from .gpt_model import GPTModel   File ""/home/mais/pretrain/MegatronLM/megatron/core/models/gpt/gpt_model.py"", line 17, in      from .gpt_model import GPTModel   File ""/home/mais/pretrain/MegatronLM/megatron/core/models/gpt/gpt_model.py"", line 17, in      from megatron.core.transformer.transformer_block import TransformerBlock   File ""/home/mais/pretrain/MegatronLM/megatron/core/transformer/transformer_block.py"", line 16, in      from megatron.core.transformer.transformer_block import TransformerBlock   File ""/home/mais/pretrain/MegatronLM/megatron/core/transformer/transformer_block.py"", line 16, in      from megatron.core.transformer.custom_layers.transformer_engine import (   File ""/home/mais/pretrain/MegatronLM/megatron/core/transformer/custom_layers/transformer_engine.py"", line 7, in      from megatron.core.transformer.custom_layers.transformer_engine import (   File ""/home/mais/pretrain/MegatronLM/megatron/core/transformer/custom_layers/transformer_engine.py"", line 7, in      import transformer_engine as te ModuleNotFoundError: No module named 'transformer_engine'     import transformer_engine as te ModuleNotFoundError: No module named 'transformer_engine' ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 1544321) of binary: /home/mais/.mlm_pretrain/bin/python Traceback (most recent call last):   File ""/home/mais/.mlm_pretrain/bin/torchrun"", line 8, in      sys.exit(main())   File ""/home/mais/.mlm_pretrain/lib/python3.10/sitepackages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper     return f(*args, **kwargs)   File ""/home/mais/.mlm_pretrain/lib/python3.10/sitepackages/torch/distributed/run.py"", line 762, in main     run(args)   File ""/home/mais/.mlm_pretrain/lib/python3.10/sitepackages/torch/distributed/run.py"", line 753, in run     elastic_launch(   File ""/home/mais/.mlm_pretrain/lib/python3.10/sitepackages/torch/distributed/launcher/api.py"", line 132, in __call__     return launch_agent(self._config, self._entrypoint, list(args))   File ""/home/mais/.mlm_pretrain/lib/python3.10/sitepackages/torch/distributed/launcher/api.py"", line 246, in launch_agent     raise ChildFailedError( torch.distributed.elastic.multiprocessing.errors.ChildFailedError:  ``` Then I found https://github.com/NVIDIA/TransformerEngine, but looks like we need to have CUDA>=11.8. It would be very helpful if you give me advice to sort this out.",2024-02-22T16:24:04Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/702,Marking as stale. No activity in 60 days.
TING2938,[QUESTION] What is the difference between with/without mcore model in pretrain_gpt.py?,What is the difference between with/without mcore model in pretrain_gpt.py? pretrain_gpt.pyL33,2024-02-22T07:50:08Z,stale,closed,2,2,https://github.com/NVIDIA/Megatron-LM/issues/701,mcore model uses modules under megatron.core. Most layers are optimized with transformer engine,Marking as stale. No activity in 60 days.
LiuXTao,Fix the bug of using loss before assignment,"**Bug Description** A bug is triggered when using **MoE** in conjunction with **pipeline_parallel_size > 1**, resulting in a 'referenced before assignment' error. The complete error report is as follows ``` Traceback (most recent call last):     File ""/mnt/nanjing3cephfs/mmbaseplt2/devxtl/temptest/MegatronLMfix/pretrain_gpt.py"", line 207, in        pretrain(train_valid_test_datasets_provider,   File ""/mnt/nanjing3cephfs/mmbaseplt2/devxtl/temptest/MegatronLMfix/megatron/training.py"", line 258, in pretrain     iteration, num_floating_point_operations_so_far = train(   File ""/mnt/nanjing3cephfs/mmbaseplt2/devxtl/temptest/MegatronLMfix/megatron/training.py"", line 970, in train     train_step(forward_step_func,   File ""/mnt/nanjing3cephfs/mmbaseplt2/devxtl/temptest/MegatronLMfix/megatron/training.py"", line 535, in train_step     losses_reduced = forward_backward_func(   File ""/mnt/nanjing3cephfs/mmbaseplt2/devxtl/temptest/MegatronLMfix/megatron/core/pipeline_parallel/schedules.py"", line 1212, in forward_backward_pipelining_without_interleaving     output_tensor = forward_step(   File ""/mnt/nanjing3cephfs/mmbaseplt2/devxtl/temptest/MegatronLMfix/megatron/core/pipeline_parallel/schedules.py"", line 216, in forward_step     config.grad_scale_func(torch.tensor(1.0, device=loss.device)) UnboundLocalError: local variable 'loss' referenced before assignment ``` **Script** The above BUG can be reproduced using my script _examples/pretrain_gpt_moe_demo.sh_ **Solution** Upon examining the error code, I noticed a potential issue in line 216 of _megatron/core/pipeline_parallel/schedules.py_: `config.grad_scale_func(torch.tensor(1.0, device=loss.device))` The variable loss runs the risk of being referenced before assignment. Therefore, I suggest modifying it as per my pull request: `config.grad_scale_func(torch.tensor(1.0, device=output_tensor.device))`",2024-02-22T06:38:08Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/700,Marking as stale. No activity in 60 days.
zhentingqi,[BUG] Faiss RuntimeError,"**Describe the bug** I am running step 3 on one 80G A100 GPU to ""Build index for similarity search"". My ""DATA_BLEND"" is the first 10000 scraped text items from openwebtext created with the steps. I only want to build an index using these 10000 text items. But when I run `bash tools/retro/examples/preprocess_data.sh indextrain`,  I encountered the following error: ``` Traceback (most recent call last):   File ""tools/retro/main.py"", line 224, in      train_index()  train only   File ""/n/home06/zhentingqi/LLM_safety/MegatronLMretro/./tools/retro/index/build.py"", line 137, in train_index     train_on_embeddings()   File ""/n/home06/zhentingqi/LLM_safety/MegatronLMretro/./tools/retro/index/build.py"", line 112, in train_on_embeddings     index.train()   File ""/n/home06/zhentingqi/LLM_safety/MegatronLMretro/./tools/retro/index/indexes/faiss_base.py"", line 81, in train     self._train()   File ""/n/home06/zhentingqi/LLM_safety/MegatronLMretro/./tools/retro/index/indexes/faiss_base.py"", line 71, in _train     index.train(inp)   File ""/n/home06/zhentingqi/.local/lib/python3.8/sitepackages/faiss/__init__.py"", line 280, in replacement_train     self.train_c(n, swig_ptr(x))   File ""/n/home06/zhentingqi/.local/lib/python3.8/sitepackages/faiss/swigfaiss.py"", line 3605, in train     return _swigfaiss.IndexPreTransform_train(self, n, x) RuntimeError: Error in void faiss::Clustering::train_encoded(faiss::Clustering::idx_t, const uint8_t*, const faiss::Index*, faiss::Index&, const float*) at /project/faiss/faiss/Clustering.cpp:283: Error: 'nx >= k' failed: Number of training points (4850) should be at least as large as number of clusters (65536) ``` Why is the number of training points 4850? And how can I reset the limit 65536 to fit my number of training points? Thanks! **Environment (please complete the following information):**   MegatronLM commit ID: bd6f4ead41dac8aa8d50f46253630b7eba84bcdf   PyTorch version: 2.1.0a0+fe05266   CUDA version: 12.2 **Script: `preprocess_data.sh`**: ``` !/bin/bash set u unset NCCL_DEBUG  Megatron, Retro dirs.  REPO_DIR=""MegatronLM"" RETRO_WORKDIR=""big_data/openwebtext/retro_workdir_10000""  Task (e.g., db, index, query).   This script takes a single argument, which specifies the retro task to be performed.  The available tasks are: dbbuild, indextrain, indexadd, and querypretrainingneighbors.  RETRO_TASKS=""dbbuild""                       Build the retrieval database  RETRO_TASKS=""indextrain""                    Train the index  RETRO_TASKS=""indexadd""                      Add data to the index  RETRO_TASKS=""querypretrainingneighbors""    Perform query pretraining for neighbors  You can also provide the task as a commandline argument when executing the script.  Example: ./preprocess_data.sh indexadd RETRO_TASKS=$1  Data.  DATA_BLEND="" \     1 big_data/openwebtext/scraped_10000/gpt2_text_document \ ""  Index.  RETRO_INDEX_STR=""OPQ32_64,IVF65536_HNSW8,PQ32"" RETRO_INDEX_NTRAIN=1000000  ! qzt: adjust this if training samples are few RETRO_INDEX_TRAIN_LOAD_FRACTION=0.97 RETRO_INDEX_ADD_LOAD_FRACTION=0.95  BERT.  BERT_CKPT=""big_models/megatronbert345mcased"" BERT_VOCAB=""big_models/megatronbert345mcased/bertcasedvocab.txt""  GPT.  RETRO_GPT_SEED=1234 RETRO_GPT_SPLIT=""98,2,0"" RETRO_GPT_DATA_PATH=${DATA_BLEND} RETRO_GPT_DATALOADER_TYPE=single RETRO_GPT_EVAL_INTERVAL=2000 RETRO_GPT_EVAL_ITERS=50 RETRO_GPT_TRAIN_SAMPLES=200000 RETRO_GPT_LR_DECAY_SAMPLES=175000 RETRO_GPT_LR_WARMUP_SAMPLES=10000 RETRO_GPT_SEQ_LENGTH=512 RETRO_GPT_GLOBAL_BATCH_SIZE=256 RETRO_GPT_CHUNK_LENGTH=64 GPT_VOCAB=""big_models/megatrongpt345m/gpt2vocab.json"" GPT_MERGE=""big_models/megatrongpt345m/gpt2merges.txt""  Query.  RETRO_QUERY_NUM_NEIGHBORS_QUERY=200 RETRO_QUERY_NUM_NEIGHBORS_SAVE=20 RETRO_QUERY_EF_SEARCH=32 RETRO_QUERY_NPROBE=4096  Args.  ARGS="" \     distributedtimeoutminutes 600 \     tensormodelparallelsize 1 \     pipelinemodelparallelsize 1 \     numlayers 24 \     hiddensize 1024 \     numattentionheads 16 \     microbatchsize 1 \     globalbatchsize ${RETRO_GPT_GLOBAL_BATCH_SIZE} \     seqlength 512 \     maxpositionembeddings 512 \     load ${BERT_CKPT} \     exitonmissingcheckpoint \     noloadoptim \     noloadrng \     datapath ${RETRO_GPT_DATA_PATH} \     tokenizertype BertWordPieceLowerCase \     vocabfile ${BERT_VOCAB} \     split ${RETRO_GPT_SPLIT} \     distributedbackend nccl \     lr 0.0001 \     lrdecaystyle linear \     minlr 1.0e5 \     trainsamples ${RETRO_GPT_TRAIN_SAMPLES} \     lrdecaysamples ${RETRO_GPT_LR_DECAY_SAMPLES} \     lrwarmupsamples ${RETRO_GPT_LR_WARMUP_SAMPLES} \     weightdecay 1e2 \     clipgrad 1.0 \     evalinterval ${RETRO_GPT_EVAL_INTERVAL} \     evaliters ${RETRO_GPT_EVAL_ITERS} \     fp16 \     dataloadertype ${RETRO_GPT_DATALOADER_TYPE} \     nodatasharding \     nogradientaccumulationfusion \     noasynctensormodelparallelallreduce \     bertembeddertype megatron \     outputbertembeddings \     \     retroworkdir ${RETRO_WORKDIR} \     retrotasks ${RETRO_TASKS} \     retroreturndocids \     retrobertvocabfile ${BERT_VOCAB} \     retroberttokenizertype BertWordPieceLowerCase \     retrogptseed ${RETRO_GPT_SEED} \     retrogpttokenizertype GPT2BPETokenizer \     retrogptvocabfile ${GPT_VOCAB} \     retrogptmergefile ${GPT_MERGE} \     retrogptseqlength ${RETRO_GPT_SEQ_LENGTH} \     retrogptchunklength ${RETRO_GPT_CHUNK_LENGTH} \     retrogptglobalbatchsize ${RETRO_GPT_GLOBAL_BATCH_SIZE} \     retrogptevalinterval ${RETRO_GPT_EVAL_INTERVAL} \     retrogptevaliters ${RETRO_GPT_EVAL_ITERS} \     retrogptsplit ${RETRO_GPT_SPLIT} \     retrogptdatapath ${RETRO_GPT_DATA_PATH} \     retroindexstr ${RETRO_INDEX_STR} \     retroindexntrain ${RETRO_INDEX_NTRAIN} \     retroindextrainloadfraction ${RETRO_INDEX_TRAIN_LOAD_FRACTION} \     retroindexaddloadfraction ${RETRO_INDEX_ADD_LOAD_FRACTION} \     retroindexnodeletetrainingembeddings \     retroindexnodeleteaddedcodes \     retroquerynumneighborsquery ${RETRO_QUERY_NUM_NEIGHBORS_QUERY} \     retroquerynumneighborssave ${RETRO_QUERY_NUM_NEIGHBORS_SAVE} \     retroqueryefsearch ${RETRO_QUERY_EF_SEARCH} \     retroquerynprobe ${RETRO_QUERY_NPROBE} \ ""  Command.  NPROCS=1 ! Number of GPUs. CMD=""\     cd ${REPO_DIR} && pwd && \     python m torch.distributed.run \     nproc_per_node ${NPROCS} \     nnodes 1 \     master_port 6000 \     tools/retro/main.py ${ARGS} \ "" echo ""~~~~~~~~~~~~~~~~~~~~~~~~~~"" echo ""CMD = '$CMD'."" echo ""~~~~~~~~~~~~~~~~~~~~~~~~~~"" eval $CMD ```",2024-02-20T20:09:40Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/699,Marking as stale. No activity in 60 days.
jrocmar,[ENHANCEMENT] S3 data loading for MMapIndexedDataset,"**Is your feature request related to a problem? Please describe.** A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] MegatronLM does not support loading a dataset from S3. **Describe the solution you'd like** A clear and concise description of what you want to happen. I would like to extend MMapIndexedDataset to support S3 data loading. **Describe alternatives you've considered** A clear and concise description of any alternative solutions or features you've considered. A user can download the dataset from S3 to the local file system at the start of training, but that blocks training until the download is complete. Alternatively, a user can store their dataset in a cloud file system. That can work well, but requires managing the cloud file system in addition to S3. **Proposed implementation** If you have a proposed implementation for the feature state it here or link to a PR. I implemented S3 data loading in a private fork of NeMo initially. I have a public, draft PR with that implementation here. However, NeMo now uses MMapIndexedDataset in MegatronLM directly, so ~~I would like to port a similar implementation to MegatronLM~~ I also ported a similar implementation to MegatronLM here. In particular, the index file is downloaded to the local file system so that it can be memory mapped and the bin file is streamed in chunks from S3. Note that the block shuffling functionality described in the NeMo PR is optional (we can just assume that the user has preshuffled the dataset). **Additional context** Add any other context or screenshots about the feature request here.",2024-02-20T18:42:52Z,stale,open,3,4,https://github.com/NVIDIA/Megatron-LM/issues/698,,: I'd love to get your feedback on this approach. Thanks in advance!,"I have been also training models using S3 data streaming by customizing the code as  did, and it is interesting to see that other people have been doing the same. I think this would indeed be a nice feature to be supported officially.",Marking as stale. No activity in 60 days.
argitrage,Incorrect shuffling of documents across epochs in GPTDataset,"**Incorrect Dataset Shuffling**  Currently, in `gpt_dataset.py`, the dataset is being globally shuffled across epochs rather than within epoch shuffling which is the standard.  Both shuffle index code and document index code are being shuffled across epochs.  **Question** Has this been done on purpose? Is there any reason to prefer global shuffling over perepoch shuffling? **Solution** Shuffle data per epoch instead of shuffling the full data. Implementation is straightforward. However, we need to fix both document and shuffle index to fix the overall problem.",2024-02-20T03:25:23Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/697,"I did some preliminary experiments with the same   1. One the simulate (or directly measure this shuffling), and a significant fraction (~30%) of the samples are never seen in a given epoch due to shuffling across epochs. 2. On smallscale training a GPTstyle model with Wikipedia, correcting this shuffling does not seem to lead to any performance improvements  I estimate because LMs continue to improve over even after repeating a few epochs on input data, so this issue of shuffling does not impact them significantly negatively.",Marking as stale. No activity in 60 days.
zhentingqi,[BUG] AttributeError: module 'transformer_engine' has no attribute 'pytorch',"**Describe the bug** I am running InstructRetro and start with data preprocessing, with `bash tools/retro/examples/preprocess_data.sh dbbuild` **Stack trace/logs** Due to torchrun's multiprocessing, the output stack trace is messy. I manually extract the error message below: ``` in  class TELinear(te.pytorch.Linear): AttributeError: module 'transformer_engine' has no attribute 'pytorch' ``` **Environment (please complete the following information):**   MegatronLM commit ID: bd6f4ead41dac8aa8d50f46253630b7eba84bcdf   PyTorch version: 2.1.1   CUDA version: 12.2",2024-02-19T20:34:54Z,stale,open,16,7,https://github.com/NVIDIA/Megatron-LM/issues/696,"Same problem, Pytorch version 2.1.0 and CUDA version 12.0 But pytorch dir do exist in transformer_engine package dir.",Same problem.   MegatronLM: 3709708ad233a0c2140de146ca6aaf3ecc05e66c   Pytorch version 2.1.0    CUDA version 12.0,Same problem * MegatronLM: 89574689447d694bb19dd86fc8a6153b4467ba9d * Pytorch version 2.2.1 * CUDA version 11.8,"It seems that the issue is caused by the missing `libtorch_cuda_cpp.so` during the import of dependencies in `__init__.py`, and this error is caught and passed by a trycatch block. ``` import flash_attn_2_cuda as flash_attn_cuda ImportError: libtorch_cuda_cpp.so: cannot open shared object file: No such file or directory ``` In my env, I can only find these two files: ``` /home/aep/miniconda3/envs/deepspeed/lib/python3.8/sitepackages/torch/lib/libtorch_cuda_linalg.so /home/aep/miniconda3/envs/deepspeed/lib/python3.8/sitepackages/torch/lib/libtorch_cuda.so ``` Discussions indicate that in newer versions of torch, `libtorch_cuda_cpp.so` is no longer generated. https://discuss.pytorch.org/t/nolibtorchcudacppsoavailablewhenbuildpytorchfromsource/159864/6 Thus, the issue seems to be that the `TransformerEngine`'s default installation of `flashattn` has not yet been adapted to the new version of pytorch. I resolved this problem by recompiling `flashattn` from the source: ``` git clone https://github.com/DaoAILab/flashattention b v2.4.2 MAX_JOBS=8 pip install e . ```",same problem even with recompiling flashattn from the source Pytorch version 2.1.1 CUDA version 12.1,"Pytorch version 2.1.0 CUDA version 12.1 It didn't work with the stable version of Transformer_engine (i.e., transformerengine 1.5.0+6a9edc3). I don't have any idea, but somehow it worked with installing the latest version of Transformer_engine from source (i.e., transformerengine1.7.0.dev0+9709147 ). $ git clone recursive https://github.com/NVIDIA/TransformerEngine.git $ cd TransformerEngine $ export NVTE_FRAMEWORK=pytorch    Optionally set framework $ pip install . ...... Stored in directory: /tmp/pipephemwheelcache10dvr64i/wheels/9d/cf/7f/d14555553b5b30698dae0a4159fdd058157e7021cec565ecaa Successfully built transformerengine flashattn Installing collected packages: flashattn, transformerengine Successfully installed flashattn2.4.2 transformerengine1.7.0.dev0+9709147 wor Following seems to work as well. $ pip install git+https://github.com/NVIDIA/TransformerEngine.git",Marking as stale. No activity in 60 days.
zhentingqi,[BUG] No Module Error,"**Describe the bug** I am running data preprocessing script using the following command: ``` python tools/preprocess_data.py \        input ./openwebtext/scraped_100/train_data.json \        outputprefix ./openwebtext/scraped_100/my_gpt2 \        vocabfile ./big_models/megatrongpt345m/gpt2vocab.json \        datasetimpl mmap \        tokenizertype GPT2BPETokenizer \        mergefile ./big_models/megatrongpt345m/gpt2merges.txt \        appendeod \        workers 20 \        chunksize 25 ``` And I run into the following error: ``` Zarrbased strategies will not be registered because of missing packages Traceback (most recent call last):   File ""tools/preprocess_data.py"", line 23, in      from megatron.tokenizer import build_tokenizer   File ""/n/home06/zhentingqi/LLM_safety/MegatronLM/megatron/__init__.py"", line 16, in      from .initialize  import initialize_megatron   File ""/n/home06/zhentingqi/LLM_safety/MegatronLM/megatron/initialize.py"", line 18, in      from megatron.arguments import parse_args, validate_args   File ""/n/home06/zhentingqi/LLM_safety/MegatronLM/megatron/arguments.py"", line 16, in      from megatron.core.models.retro import RetroConfig   File ""/n/home06/zhentingqi/LLM_safety/MegatronLM/megatron/core/models/retro/__init__.py"", line 4, in      from .decoder_spec import get_retro_decoder_block_spec   File ""/n/home06/zhentingqi/LLM_safety/MegatronLM/megatron/core/models/retro/decoder_spec.py"", line 5, in      from megatron.core.models.gpt.gpt_layer_specs import (   File ""/n/home06/zhentingqi/LLM_safety/MegatronLM/megatron/core/models/gpt/__init__.py"", line 1, in      from .gpt_model import GPTModel   File ""/n/home06/zhentingqi/LLM_safety/MegatronLM/megatron/core/models/gpt/gpt_model.py"", line 17, in      from megatron.core.transformer.transformer_block import TransformerBlock   File ""/n/home06/zhentingqi/LLM_safety/MegatronLM/megatron/core/transformer/transformer_block.py"", line 16, in      from megatron.core.transformer.custom_layers.transformer_engine import (   File ""/n/home06/zhentingqi/LLM_safety/MegatronLM/megatron/core/transformer/custom_layers/transformer_engine.py"", line 7, in      import transformer_engine as te ModuleNotFoundError: No module named 'transformer_engine' ``` **Environment:**   MegatronLM commit ID: 5f9c870f9f24b482509699d206a9dbb00958f6fc   PyTorch version: 2.2.0   CUDA version: 12.2   NCCL version: 2.19.3",2024-02-18T16:59:20Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/695,you need to install https://github.com/NVIDIA/TransformerEngine,Marking as stale. No activity in 60 days.
xiaojunjie,adjust the keys of attention in checkpoint,"I try to convert gpt checkpoint from **local** to **transformer_engine** according to following map ` {                 'input_layernorm.': 'self_attention.linear_qkv.layer_norm_',                 'pre_mlp_layernorm.': 'mlp.linear_fc1.layer_norm_',   } ` It works well only when the optimizer is not loaded, because: In **local** or **transformer_engine**  checkpoint,  **linear_proj** is in front of  **linear_qkv**, due to the order of initialization  local     layernorm  > linear_proj  > linear_qkv  transformer_engine    linear_proj > layernorm > linear_qkv For optimizer, it is necessary to swap sequential stored weights. Another method, as this pr do, build **linear_qkv** first, as same as the forward order     layernorm > linear_qkv > linear_proj",2024-02-18T12:08:38Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/694, Please correct me if I'm wrong,Marking as stale. No activity in 60 days.
ByronHsu,[doc] `--workers` is missing in data preprocessing example script,"`workers` is required as in preprocess_data.py, but it is missing in readme.",2024-02-13T06:24:07Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/693,Marking as stale. No activity in 60 days.
ByronHsu,[Logging] Surface up loader module import error," Why? The loader import errors is swallowed if the root cause is not loader_X.py not found. For example, if i don't have `transformers` installed, it also printed loader_X not found, but the actual error is `transformers` not found.",2024-02-13T06:19:05Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/692,Marking as stale. No activity in 60 days.
robotsp,[QUESTION] how to profile bubble time in pipeline parallelism?,**Your question** how can we profile bubble time in pipeline parallelism accurately?,2024-02-07T02:00:45Z,stale,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/691,Marking as stale. No activity in 60 days.,same question,Same question,Marking as stale. No activity in 60 days.
wuxibin89,"[QUESTION] For DDP, why map parameter's main_grad to grad buffer instead of grad?"," Hi, I'm diving deep into MegatronLM's implementation. For DDP wrapper, the current implementation maps each parameter's `main_grad` to grad buffer. https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/distributed/grad_buffer.pyL272L280 And then in backward hook, add `grad` to `main_grad` manually. https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/distributed/distributed_data_parallel.pyL151L154 My question is why not just map parameter's `grad` to grad buffer directly and let torch accumulating them automatically?",2024-02-05T09:28:54Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/690,Marking as stale. No activity in 60 days.,"PyTorchâ€™s autograd functionality assumes that a model parameter and its corresponding gradient have the same data type. However, while lowprecision data types like FP8 are sufficient for evaluating a neural networkâ€™s forward and backward passes, the optimization step typically requires full FP32 precision to avoid significant learning degradation. In addition, Tensor Cores on Hopper GPUs have the option to accumulate matrix products directly into FP32, resulting in better numerical accuracy and avoiding the need for a separate casting kernel. Thus, Transformer Engine provides an option to directly generate FP32 gradients for weight tensors. The FP32 gradients are not output to the parameterâ€™s grad tensor, but rather to a main_grad tensor that must be initialized before the backward pass. (https://docs.nvidia.com/deeplearning/transformerengine/userguide/examples/advanced_optimizations.html) "," Got it, thanks for your kind reply!"
ZhangEnmao,[QUESTION]Zarr-based strategies will not be registered because of missing packages,"**Your question** Ask a clear and concise question about MegatronLM. When I transfer the hf_weights to megatron_weights using ""tools/checkpoint/util.py modeltype GPT"", I got the output informations as follows, and the ""targettensorparallelsize 2 targetpipelineparallelsize 2"" are not used I think. !image",2024-02-05T05:13:51Z,stale,closed,0,7,https://github.com/NVIDIA/Megatron-LM/issues/689,"what's the full command you're using. The warning ""Zarrbased strategies will not be registered because of missing packages"" is irrelevant. ",Marking as stale. No activity in 60 days.,Bim,Projeto liliti stk 3.6.9 inteligÃªncia artificial multimidal ,"Have you solved the problem? > **Your question** Ask a clear and concise question about MegatronLM. When I transfer the hf_weights to megatron_weights using ""tools/checkpoint/util.py modeltype GPT"", I got the output informations as follows, and the ""targettensorparallelsize 2 targetpipelineparallelsize 2"" are not used I think. !image",CadÃª o projeto liliti stk 3.6.9 inteligÃªncia artificial multimidal? ,Marking as stale. No activity in 60 days.
zhangsheng377,fix weight don't have main_grad,"When we use the LoRA with Megatron, we find that some weight do not require_grad, so they don't have main_grad, but the code does not judge that and throw error.",2024-02-04T10:24:07Z,stale,open,1,1,https://github.com/NVIDIA/Megatron-LM/issues/688,Marking as stale. No activity in 60 days.
zhouyiyuan-mt,[QUESTION]Why forward_backward_pipelining_without_interleaving cannot open config.overlap_p2p_comm?,"forward_backward_pipelining_with_interleaving has a branch of opening config.overlap_p2p_comm, why does forward_backward_pipelining_without_interleaving not have?",2024-02-04T09:01:41Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/687,"It can, we just haven't had the engineering resources to get around to it.",Marking as stale. No activity in 60 days.
robotsp,[QUESTION] How to re-initialize process group after destroy_process_group() ?,**Your question** Ask a clear and concise question about MegatronLM. How to initialize process group twice in one torch.run bash I tried to destroy the original one and reinitialize it but failed...,2024-02-02T11:45:43Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/686,Marking as stale. No activity in 60 days.
JYXL1,[QUESTION]which torch version can work with `ring_exchange`?,"**Your question** Ask a clear and concise question about MegatronLM. i have search the method from torch, but cannot find it, can you tell me which version of torch works with it?",2024-02-02T03:24:24Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/685,Marking as stale. No activity in 60 days.
jubick1337,Neft,,2024-02-01T19:19:25Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/684,Marking as stale. No activity in 60 days.
lxg2015,[FIx] add params_dtype to router layer,,2024-02-01T03:42:49Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/683,Marking as stale. No activity in 60 days.
haidark,Add support for ffn_hidden_size in throughput logging,"The existing calculation for throughput assumes `ffn_hidden_size` is fixed at `4 * hidden_size` resulting in incorrect TFLOP/s/GPU when using `args.swiglu` or specifying a custom `args.ffn_hidden_size`. Depending on how different the custom `ffn_hidden_size` setting is this can result in reported throughput difference of 30 TFLOP/s/GPU. This PR adjusts the throughput calculation to account for `ffn_hidden_size`.  Calculation Details: The calculation follows the notation and procedure of the FLOPs approximation in the Appendix of the paper ""Efficient LargeScale Language Model Training on GPU Clusters Using MegatronLM"". Beyond the variables described in the appendix, we introduce `f`= ffn hidden size, `a`=  of attention heads, and `g`:=  query groups for GQA where `g == a` is vanilla MHA. Attention FLOPs: `6bsh**2` (key, value, output proj) + `2bsh(h//a)g` (query proj) + `4bhs**2` (attn compute) FFN FLOPs: `4bshf` (f = 4h is the default Megatron setting) Output FLOPs: `6bshV` (unembedding compute) Total FLOPs: `6bsh(L(3h + (h//a)g + 2s + 2f) + V)`",2024-01-27T12:18:25Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/682,"This is a small fix but important, is there a plan to address this?","Yes, will try to get to this this week.","This is fixed now, we also added support for MoEs in the formulae."
eagle705,[QUESTION] How to set `--rotary-seq-len-interpolation-factor` for rope scaling?,"Hi,  I'm curious if it's possible to set `rotaryseqleninterpolationfactor` in MegatronLM to match huggingface's rope_scaling setting (`{""type"": ""dynamic"", ""factor"": 2.0}`).  Is there any information you can share on how the `rotaryseqleninterpolationfactor` option is used?",2024-01-26T14:04:36Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/681,https://github.com/NVIDIA/MegatronLM/blob/4bd4e7426c8def582eb80ae53f296a803643d2f5/megatron/core/models/common/embeddings/rotary_pos_embedding.pyL52,Marking as stale. No activity in 60 days.
kebe7jun,Fix the wrong exit code when some errors occur,"The process exits with a status code of 0 when the file is not found, making the job appear as 'Completed'. ```  loading checkpoint from /workspace/checkpoints at iteration 200 could not load the checkpoint [Errno 2] No such file or directory: '/workspace/checkpoints/iter_0000200/mp_rank_00/model_optim_rng.pt' rootkebe6fb6f6c96dh7ljs:/workspace/MegatronLM echo $? 0 ``` This is an incorrect return.",2024-01-26T03:44:12Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/680,Marking as stale. No activity in 60 days.
siddharth9820,Tiny LLaMA data-loaders and a training script in Megatron-AxoNN,,2024-01-25T11:37:04Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/679
aoyulong,[REGRESSION] get_batch in pretrain_gpt.py is much slower than the old impl,"**Describe the regression** The new code is much slower than the old one when increasing the sequence length. **To Reproduce** The code was tested on a A80080GB server equipped with NVLINK. The configuration is as following, and only the `seqlength `and `maxpositionembeddings` are changed. ``` DISTRIBUTED_ARGS=""     nproc_per_node 8     nnodes 1     node_rank 0     master_addr $MASTER_ADDR \     master_port $MASTER_PORT  "" TRAINING_ARGS=""     trainsamples 10000 \     evaliters 0 \     evalinterval 2000 \     tensormodelparallelsize 4 \     pipelinemodelparallelsize 2 \     makevocabsizedivisibleby 64 \     microbatchsize 1 \     globalbatchsize 4 \     disablebiaslinear \     sequenceparallel \     useflashattn "" MIXED_PRECISION_ARGS=""     bf16 \     attentionsoftmaxinfp32 \     accumulateallreducegradsinfp32 "" DATA_ARGS=""     datapath $DATA_PATH \     tokenizertype GPT2BPETokenizer \     vocabfile $VOCAB_FILE \     vocabsize 100008\     mergefile $MERGE_FILE \     split 1 ""      specialtokensfile $SPECIAL_TOKENS_FILE \ NETWORK_ARGS=""     numlayers 24 \     hiddensize 2048 \     numattentionheads 16 \     seqlength 4096 \     maxpositionembeddings 4096 \     normalization RMSNorm \     userotarypositionembeddings \     nopositionembedding \     swiglu \     untieembeddingsandoutputweights "" INITIALIZATION_ARGS=""     initmethodstd 0.02 \     seed 42 "" REGULARIZATION_ARGS=""     attentiondropout 0.0 \     hiddendropout 0.0 \     weightdecay 0.1 \     adambeta1 0.9 \     adambeta2 0.95 \     clipgrad 1.0 "" LEARNING_RATE_ARGS=""     lr 1.0e3 \     lrdecaystyle cosine \     lrwarmupsamples 100 \     minlr 1.0e5 "" CHECKPOINTING_ARGS=""     saveinterval 20000 \     save $CHECKPOINT_PATH \ "" LOGGING_ARGS=""     loginterval 1 \     timingloglevel 2 \     tensorboarddir $TB_PATH \     tensorboardloginterval 1 \     wandbsavedir $WB_PATH "" cmd=""torchrun $DISTRIBUTED_ARGS pretrain_gpt.py \               $TRAINING_ARGS \               $MIXED_PRECISION_ARGS \               $DATA_ARGS \               $NETWORK_ARGS \               $INITIALIZATION_ARGS \               $REGULARIZATION_ARGS \               $LEARNING_RATE_ARGS \               $CHECKPOINTING_ARGS \               $LOGGING_ARGS     "" echo $cmd eval $cmd ``` **Performance comparison**  **Environment (please complete the following information):**   Previous PyTorch version: 2.1.0a0+4136153   New PyTorch version: 2.1.0a0+4136153   Previous NGC container version: PyTorch Release 23.06   New NGC container version: PyTorch Release 23.06 **Proposed fix** Change the `get_batch` function to the original implementation.",2024-01-25T04:19:11Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/678,Marking as stale. No activity in 60 days.
GSSBMW,[BUG] TypeError: FastLayerNormFN.forward() missing 1 required positional argument: 'memory_efficient',"**Describe the bug** Follow the steps of readme to run pretrain of gpt 345M model. But give error  `TypeError: FastLayerNormFN.forward() missing 1 required positional argument: 'memory_efficient'` **To Reproduce** Follow the step of readme. **Stack trace/logs** [before the start of training step] datetime: 20240124 05:41:28  ``` Traceback (most recent call last):   File ""/home/sanshang/021_debug/megatronlm/pretrain_gpt.py"", line 198, in      pretrain(train_valid_test_datasets_provider,   File ""/home/sanshang/021_debug/megatronlm/megatron/training.py"", line 246, in pretrain     iteration, num_floating_point_operations_so_far = train(   File ""/home/sanshang/021_debug/megatronlm/megatron/training.py"", line 908, in train     train_step(forward_step_func,   File ""/home/sanshang/021_debug/megatronlm/megatron/training.py"", line 517, in train_step     losses_reduced = forward_backward_func(   File ""/home/sanshang/021_debug/megatronlm/megatron/core/pipeline_parallel/schedules.py"", line 343, in forward_backward_no_pipelining     output_tensor = forward_step(   File ""/home/sanshang/021_debug/megatronlm/megatron/core/pipeline_parallel/schedules.py"", line 191, in forward_step     output_tensor, loss_func = forward_step_func(data_iterator, model)   File ""/home/sanshang/021_debug/megatronlm/pretrain_gpt.py"", line 145, in forward_step     output_tensor = model(tokens, position_ids, attention_mask,   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1510, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1519, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/sanshang/021_debug/megatronlm/megatron/core/distributed/distributed_data_parallel.py"", line 136, in forward     return self.module(*inputs, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1510, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1519, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/sanshang/021_debug/megatronlm/megatron/model/module.py"", line 185, in forward     outputs = self.module(*inputs, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1510, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1519, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/sanshang/021_debug/megatronlm/megatron/model/gpt_model.py"", line 82, in forward     lm_output = self.language_model(   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1510, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1519, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/sanshang/021_debug/megatronlm/megatron/model/language_model.py"", line 493, in forward     encoder_output = self.encoder(   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1510, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1519, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/sanshang/021_debug/megatronlm/megatron/model/transformer.py"", line 1776, in forward     hidden_states = layer(   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1510, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1519, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/sanshang/021_debug/megatronlm/megatron/model/transformer.py"", line 1159, in forward     norm_output = self.input_norm(hidden_states)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1510, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1519, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/sanshang/021_debug/megatronlm/megatron/model/fused_layer_norm.py"", line 86, in forward     output = FastLayerNormFN.apply(input, weight, self.bias, self.eps)   File ""/usr/local/lib/python3.10/distpackages/torch/autograd/function.py"", line 551, in apply     return super().apply(*args, **kwargs)   type: ignore[misc] TypeError: FastLayerNormFN.forward() missing 1 required positional argument: 'memory_efficient' ``` **Environment (please complete the following information):**   MegatronLM commit ID: 2c3468a49ed51324ae9b442e0d88416f1b29422b   Container: nvcr.ionvidia/pytorch:23.12py3   PyTorch version: 2.2.0a0+81ea7a4   CUDA version: Cuda compilation tools, release 12.3, V12.3.107, given by `nvcc version`   NCCL version: 2.19.3   Python: 3.10.12 **Proposed fix** If you have a proposal for how to fix the issue state it here or link to a PR. **Additional context** Add any other context about the problem here.",2024-01-24T13:53:32Z,,closed,1,9,https://github.com/NVIDIA/Megatron-LM/issues/677,"I encountered a similar problem.  My error is reported as: TypeError: forward() missing 1 required positional argument: 'memory_efficient' ``` Traceback (most recent call last):   File ""/fs1/home/huangth/git_dir/MegatronLM/pretrain_gpt.py"", line 204, in      pretrain(train_valid_test_datasets_provider,   File ""/fs1/home/huangth/git_dir/MegatronLM/megatron/training.py"", line 162, in pretrain     iteration = train(forward_step_func,   File ""/fs1/home/huangth/git_dir/MegatronLM/megatron/training.py"", line 740, in train     train_step(forward_step_func,   File ""/fs1/home/huangth/git_dir/MegatronLM/megatron/training.py"", line 424, in train_step     losses_reduced = forward_backward_func(   File ""/fs1/home/huangth/git_dir/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 368, in forward_backward_no_pipelining     output_tensor = forward_step(   File ""/fs1/home/huangth/git_dir/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 193, in forward_step     output_tensor, loss_func = forward_step_func(data_iterator, model)   File ""/fs1/home/huangth/git_dir/MegatronLM/pretrain_gpt.py"", line 167, in forward_step     output_tensor = model(tokens, position_ids, attention_mask,   File ""/fs1/home/huangth/.conda/envs/torch13/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/fs1/home/huangth/.conda/envs/torch13/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1529, in _call_impl     return forward_call(*args, **kwargs)   File ""/fs1/home/huangth/git_dir/MegatronLM/megatron/core/distributed.py"", line 289, in forward     return self.module(*inputs, **kwargs)   File ""/fs1/home/huangth/.conda/envs/torch13/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/fs1/home/huangth/.conda/envs/torch13/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1529, in _call_impl     return forward_call(*args, **kwargs)   File ""/fs1/home/huangth/git_dir/MegatronLM/megatron/model/module.py"", line 181, in forward     outputs = self.module(*inputs, **kwargs)   File ""/fs1/home/huangth/.conda/envs/torch13/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/fs1/home/huangth/.conda/envs/torch13/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1529, in _call_impl     return forward_call(*args, **kwargs)   File ""/fs1/home/huangth/git_dir/MegatronLM/megatron/model/gpt_model.py"", line 82, in forward     lm_output = self.language_model(   File ""/fs1/home/huangth/.conda/envs/torch13/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/fs1/home/huangth/.conda/envs/torch13/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1529, in _call_impl     return forward_call(*args, **kwargs)   File ""/fs1/home/huangth/git_dir/MegatronLM/megatron/model/language_model.py"", line 487, in forward     encoder_output = self.encoder(   File ""/fs1/home/huangth/.conda/envs/torch13/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/fs1/home/huangth/.conda/envs/torch13/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1529, in _call_impl     return forward_call(*args, **kwargs)   File ""/fs1/home/huangth/git_dir/MegatronLM/megatron/model/transformer.py"", line 1747, in forward     hidden_states = layer(   File ""/fs1/home/huangth/.conda/envs/torch13/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/fs1/home/huangth/.conda/envs/torch13/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1529, in _call_impl     return forward_call(*args, **kwargs)   File ""/fs1/home/huangth/git_dir/MegatronLM/megatron/model/transformer.py"", line 1132, in forward     norm_output = self.input_norm(hidden_states)   File ""/fs1/home/huangth/.conda/envs/torch13/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/fs1/home/huangth/.conda/envs/torch13/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1529, in _call_impl     return forward_call(*args, **kwargs)   File ""/fs1/home/huangth/git_dir/MegatronLM/megatron/model/fused_layer_norm.py"", line 84, in forward     return FusedLayerNormAffineFunction.apply(input, weight, self.bias, self.normalized_shape, self.eps)   File ""/fs1/home/huangth/.conda/envs/torch13/lib/python3.9/sitepackages/torch/autograd/function.py"", line 539, in apply     return super().apply(*args, **kwargs)   type: ignore[misc] TypeError: forward() missing 1 required positional argument: 'memory_efficient' [20240126 11:11:23,747] torch.distributed.elastic.multiprocessing.api: [ERROR] failed ```",I think the team already has one MR in review to fix it.,"yeah I encountered the same problem too today: ``` TypeError: FastLayerNormFN.forward() missing 1 required positional argument: 'memory_efficient'     return forward_call(*args, **kwargs)   File ""/workspace/azysxf/MegatronLM/megatron/model/module.py"", line 194, in forward     outputs = self.module(*inputs, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1510, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1519, in _call_impl     return forward_call(*args, **kwargs)   File ""/workspace/azysxf/MegatronLM/megatron/model/gpt_model.py"", line 82, in forward     lm_output = self.language_model(   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1510, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1519, in _call_impl     return forward_call(*args, **kwargs)   File ""/workspace/azysxf/MegatronLM/megatron/model/language_model.py"", line 501, in forward     encoder_output = self.encoder(   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1510, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1519, in _call_impl     return forward_call(*args, **kwargs)   File ""/workspace/azysxf/MegatronLM/megatron/model/transformer.py"", line 1668, in forward     hidden_states = layer(   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1510, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1519, in _call_impl     return forward_call(*args, **kwargs)   File ""/workspace/azysxf/MegatronLM/megatron/model/transformer.py"", line 1053, in forward     norm_output = self.input_norm(hidden_states)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1510, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1519, in _call_impl     return forward_call(*args, **kwargs)   File ""/workspace/azysxf/MegatronLM/megatron/model/fused_layer_norm.py"", line 86, in forward     output = FastLayerNormFN.apply(input, weight, self.bias, self.eps)   File ""/usr/local/lib/python3.10/distpackages/torch/autograd/function.py"", line 551, in apply     return super().apply(*args, **kwargs)   type: ignore[misc] TypeError: FastLayerNormFN.forward() missing 1 required positional argument: 'memory_efficient' ```",> I think the team already has one MR in review to fix it. did you find the one?,"maybe the NGC version is too new, try 23 series version",+1 ,Running on nvcr.io/nvidia/pytorch:23.05py3 resolved it for me,> Running on nvcr.io/nvidia/pytorch:23.05py3 resolved it for me That Solved it for me as well!,This issue should have been resolved. Close.
Druva24,How to split the dataset when running pretrain_bert.py,"Hi there, I am trying to run pretrain_bert.py using a small Wikipedia corpus consisting of 502 documents. I have set the split ratio as 449, 50, 1 for train, valid, test. With train_iters as 20, eval_iters set to 1 and eval_interval as 1, I encountered the following message in the generated log file: > building train, validation, and test datasets ...  > datasets target sizes (minimum size):     train:      20     validation: 21     test:       1     I expected the dataset split process not to hang, as my split reached the target sizes for all three portions. However, the mapping was successful for the training and validation sets, but it hung during the test mapping. When I adjusted the split to 472, 30, 20, the run was successful. How should one assess the target sizes (minimum sizes) for the dataset split?     Adding reference code for above log:     https://github.com/NVIDIA/MegatronLM/blob/de4028a9d45bd65c67e1a201d9e0690bd6cb4304/megatron/training.pyL1079     Thanks",2024-01-23T17:26:26Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/676,   ,Hello   barker     Can someone kindly respond to my question. Its kind of very important to me figure out above issue. I am waiting on the above and halted some of my daily work duties. Thanks,Marking as stale. No activity in 60 days.
zhouyiyuan-mt,[QUESTION]num_warmup_microbatch = (pipeline_parallel_size - pipeline_parallel_rank - 1) * 2,"https://github.com/NVIDIA/MegatronLM/blob/de4028a9d45bd65c67e1a201d9e0690bd6cb4304/megatron/core/pipeline_parallel/schedules.pyL503 I am confused about why num_warmup_microbatch = (pipeline_parallel_size  pipeline_parallel_rank  1) * 2 + num_chunk * pipeline_parallel_size, rather than  num_warmup_microbatch = (pipeline_parallel_size  pipeline_parallel_rank  1) + num_chunk * pipeline_parallel_size, where the latter looks like more natural.  Wait for your answer, thank you.",2024-01-23T09:40:17Z,,closed,0,7,https://github.com/NVIDIA/Megatron-LM/issues/675," The number of warmup microbatches is the number of microbatches needed until a pipeline stage can move to its ""steady state"" behavior of 1 forward and then 1 backward microbatch.",">  > The number of warmup microbatches is the number of microbatches needed until a pipeline stage can move to its ""steady state"" behavior of 1 forward and then 1 backward microbatch. https://github.com/NVIDIA/MegatronLM/blob/de4028a9d45bd65c67e1a201d9e0690bd6cb4304/megatron/core/pipeline_parallel/schedules.pyL503 https://github.com/NVIDIA/MegatronLM/blob/de4028a9d45bd65c67e1a201d9e0690bd6cb4304/megatron/core/pipeline_parallel/schedules.pyL1138 I know the meaning of  num_warmup_microbatch. I want to know why it is different to calculate  num_warmup_microbatch between forward_backward_pipelining_without_interleaving and forward_backward_pipelining_with_interleaving, and what is the meanning of the factor ""2"" in forward_backward_pipelining_with_interleaving.","For the interleaved schedule, the number of warmup microbatches is 10, 8, 6, 4 (difference of 2). For the noninterleaved schedule, the number of warmup microbatches is 3, 2, 1, 0 (difference of 1). Our interleaved schedule is trying to make sure all pipeline stages are running forward and backward passes in the same time step; the noninterleaved schedule does not have that restriction.","> For the interleaved schedule, the number of warmup microbatches is 10, 8, 6, 4 (difference of 2). For the noninterleaved schedule, the number of warmup microbatches is 3, 2, 1, 0 (difference of 1). Our interleaved schedule is trying to make sure all pipeline stages are running forward and backward passes in the same time step; the noninterleaved schedule does not have that restriction. What are advantages to make all pipeline stages running forward and backward passes in the same time step?","We needed this since we have wraparound with the interleaved schedule: the last rank may need to send tensors to the first rank. If the different ranks in a pipeline are offset, then the implementation gets more complicated to schedule that lasttofirst (or firsttolast when in backward pass) communication.",Thanks! Great idea!,"Closing this, feel free to reopen if needed."
xu-song,Add worker option for preprocess_data.py,`workers` is an required option for `preprocess_data.py` https://github.com/NVIDIA/MegatronLM/blob/de4028a9d45bd65c67e1a201d9e0690bd6cb4304/tools/preprocess_data.pyL223L226,2024-01-23T07:46:49Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/674,Marking as stale. No activity in 60 days.
Infi-zc,[BUG] Incorrect loss scaling in context parallel code logic,"**Describe the bug** Hi,      I think that there is a bug with the loss calculation in the context parallel code logic, and we could discuss it. When using context parallel, there is an additional loss scale to the sum result of local loss cross cp group.  refer to: https://github.com/NVIDIA/MegatronLM/blob/de4028a9d45bd65c67e1a201d9e0690bd6cb4304/pretrain_gpt.pyL126 the main logic compared between data parallel and context parallel ps. use tp for tensorparallel, dp for data parallel, cp for context parallel **data parallel:** 1. sum local dp loss, get per token loss 2. scale grad buffer by 1/dp_size, refer to: https://github.com/NVIDIA/MegatronLM/blob/de4028a9d45bd65c67e1a201d9e0690bd6cb4304/megatron/core/distributed/grad_buffer.pyL98 3. allreduce grads across dp_group and **current logic in cp** : 1. sum local cp loss, allreduce within the cp_group, get per token loss, which is similar to the operations at per dp rank 2. **multiply loss by the cp_size** 3. scale grad buffer by 1/(dp+cp_size), also refer to https://github.com/NVIDIA/MegatronLM/blob/de4028a9d45bd65c67e1a201d9e0690bd6cb4304/megatron/core/distributed/grad_buffer.pyL98 4. allreduce grads across (dp+cp) group we can regard context parallel as an special data parallelï¼Œand show users with the equivalent loss to the full sequence length loss. In this case, scale grad buffer by 1/(dp+cp)_size and allreduce grad between cp+dp group are reasonable. But the loss used to scale grad should be the loss of corresponding chunk of sequence other than allreduced loss between cp_group. which means the loss should be the **result of 1st step in current cp logic** other than after it times cp_size. so I think the **multiply loss by the cp_size** logic should be removed. On the other hand, I compared the grad_output(the backward grad parameter at https://github.com/NVIDIA/MegatronLM/blob/de4028a9d45bd65c67e1a201d9e0690bd6cb4304/megatron/core/tensor_parallel/layers.pyL363) of the output_layer(lm logits layer) in a toy llamalike model with tensor_model_parallel_size = 2 with two dp_cp settings. one is tp2dp1, the other is tp2cp2dp1. The grad_outputs are different when multiply loss by the cp_size. But if remove the mul cp_size logic, the grad_outputs are exactly the same. **To Reproduce** 1. config a toy model with dp1cp1 and dp1cp2, and initialize then with the same parameter, be sure to use deterministic training feature. 2. dump the gradient output of the output_layer from the first backward pass and compare them.(be sure to totally avoid the numeric issues caused by flash_attn and linear layer, due to using bf16 calculation and the rerank of cp input chunks ) **Expected behavior** If remove the logic of **multiply loss by the cp_size**, the grad_output should be the same. **Proposed fix** I created a pull request to fix this issue. https://github.com/NVIDIA/MegatronLM/pull/672",2024-01-23T06:36:38Z,,closed,0,47,https://github.com/NVIDIA/Megatron-LM/issues/673,"zc Thanks for reporting this. However, I am not sure I get the issue clearly. First of all, weight grads should be scaled by 1/(dp*cp), not 1/(dp+cp). Assume we have two GPUs, and you have two microbatches, you can run with either DP2CP1 or DP1CP2. get_data_parallel_group(with_context_parallel=True) in parallel_state.py return same results for DP2CP1 and DP1CP2. With DP2CP1, each GPU gets 1 microbatch with full seqlen. With DP1CP2, each GPU gets 2 microbatches with half seqlen for each of them, two GPUs cooperate with each to process microbatches one by one. With DP2CP1, (loss in loss_func) is full_seqlen_loss / full_seqlen_loss_mask_sum. In each GPU, we only have 1 microbatch, so loss does not get scaled at here. Then in optimizer, weight grads are scaled by 2 With DP1CP2, before allreduce, loss in each GPU is for half seqlen. After allreduce, loss in each GPU is for full seqlen. However, unlike DP2CP1 case, DP1CP2 has two microbatches per GPU, so loss is scaled by 2 at here. Hence, we multiple loss by context_parallel_size to recover it. This will make sure the calculated weight grads of each microbatch in each GPU is same as the case of DP2CP1. Then finally, optimizer scale weight grads by 2 will be correct. To be clear, for both cases of DP2CP1 and DP1CP2, self.data_parallel_world_size in grad_buffer.py is same, it's 2 for both cases."," Thanks for the replay and sorry for the confusion caused by my improper way of notation.  The 'dp+cp group' used above is to represent the dist group got by calling  get_data_parallel_group(with_context_parallel=True).  Firstly, I think all the operations of loss, including sum loss*loss_mask and then divide by num_of_active_tokens(loss_mask.sum()), multiply by 1/num_microbatches, aim to get the averaged per token loss.  With DP1CP2, the number of micro batches are 2, so in each cp_rank, it should divide micro_batch_size(which here is 2) to get the averaged per sequence loss, because it does have 2 sequences in total 2 micro batches now. And also, the full sequence length loss is divided by full token nums. After these operations to loss are finished, the 2 cp_ranks will have the same per token loss, and it is the same as DP2CP1.  As for the weight grad,  DP1CP2 also sum the two grad_weights, and also divide by 2(with_context_parallel=True). Comparing DP1CP1 (with full seq length, 1 micro batch) with DP1CP2( each with half seq length, 1 micro batch), the corresponding grad_weight of the 2 cp_ranks should add up so as to be the same as DP1CP1â€™s weight grad. With DP1CP2 and DP2CP1, the difference is in the order in which the weight gradients are accumulated. DP1CP2, ((cp0_mb1 + cp0_mb2) + (cp1_mb1 + cp1_mb2)) / 2,  DP2CP1, (dp0_mb1 + dp1_mb1)/2. mb is short for micro_batch, cp0 is short for cp_rank=0, dp0 is short for dp_rank = 0. So I still think 'multiply 2 to recover it"" is not necessary. What do you think about it?","> With DP1CP2, the number of micro batches are 2, so in each cp_rank, it should divide micro_batch_size(which here is 2) to get the averaged per sequence loss, because it does have 2 sequences in total 2 micro batches now. And also, the full sequence length loss is divided by full token nums. After these operations to loss are finished, the 2 cp_ranks will have the same per token loss, and it is the same as DP2CP1. But optimizer still scale weight grads by 2 at here , wouldn't this make optimizer update with wrong gradients? Basically, in DP2CP1 case, you only have optimizer weight grads scaling. In DP1CP2 case, you scale each weight grads  twice, num_microbatches and optimizer, so we need to multiply loss by context_parallel _size to offset one of them. Regards to your example of DP1CP1 (1GPU running) vs. DP1CP2 (2GPU running), each GPU should have 1 microbatch in both cases, so neither of them have scaling by num_microbatches. The difference is that DP1CP1 optimizer scale weight grad by 1, but DP1CP2 scale weight grads by 2. Hence, multiplying loss by context_paralleli_size in DP1CP2 case is still necessary, otherwise, optimizer weight grads scaling will make wgrads wrong. FYI, here is the test that I ran before , curves of TP2CP2DP2 match with TP2CP1DP4."," Thank you very much, you are right. By the way, personally, I think it's better to divide by the pure dp world size (with_context_parallel=False) before grad reduce rather than multiplying by the cp_size on the loss. It's just a minor suggestion."," Thanks for your explanation. However, I wonder how might loss backprop through all_reduce, which isn't part of the computation graph? I feel we should also divide by cp_size to get pertoken loss over the whole sequence. https://github.com/NVIDIA/MegatronLM/blob/de4028a9d45bd65c67e1a201d9e0690bd6cb4304/pretrain_gpt.pyL110",">  Thanks for your explanation. However, I wonder how might loss backprop through all_reduce, which isn't part of the computation graph? I feel we should also divide by cp_size to get pertoken loss over the whole sequence. >  > https://github.com/NVIDIA/MegatronLM/blob/de4028a9d45bd65c67e1a201d9e0690bd6cb4304/pretrain_gpt.pyL110 Hi  The loss in this allreduce has two elements. The second element is `loss_mask.sum()`. After allreduce, the second element will be the total number of valid tokens in the full sequence. Then `loss = loss[0] / loss[1]` is the average over full sequence.","Will this problem cause grad norm bigger?   In my projects, The grad norm in dp = 0.5, but 2.0 in CP=2, 16 in CP=4. Thanks ","Hi  , I have not tested this with the latest MCore distributed optimizer yet, but I do not remember I saw this problem in my old test runnings. After wgrads reducescatter and averaging in optimizer, wgrads should be same as CP1, so I do not quite expect you see grads norm becomes so much bigger. It could be different than CP1 due to some nondeterministic running. I guess you may have some wgrads scaling issue or grads norm calculation did not take some scaling / averaging into consideration.","Hi, , I've checked and found a solution about my grad norm error.  My megatronlm code is based on MCore0.7.0, and this bug is fixed in https://github.com/NVIDIA/MegatronLM/commit/3bdcbbbe5d2a455a75e28969be7250cd4bd27bae. Before this commit,  gradient data does'not divide by context_parallel_size(2), which causes my grad norm is 4* compared to the correct one.  Your intuition is very impressive! Thank you very much!","Hi  No problem. Yeah, this was a bug. I did not know you use old MCore. Glad that you have fixed it."," I am not sure I totally follow your compute, MCore also scale loss by num_microbatches, which also can impact the wgrads value in bwd. Anyway, we need to make sure loss and grad_norm are same as CP1 no matter what CP size is. If you get matches loss curve and similar grad_norm curve, you should be fine.","Sorry about the misguide, My opinion is loss * args.context_parallel_size  might be wrong. In my setting, num_microbatches will always be 1 no matter CP_SIZE=1 or CP_SIZE=2 or CP_SIZE=4","It should not be wrong, we have tested it. It means you increase the number of GPUs by CP_size? If so, I do not think the math in your prior message is correct. With CP1DP1 (i.e., single GPU), you do not need to do any AR and scaling. With CP1DP2 (i.e., 2 GPUs), each GPU runs with half sequence length and only save activations of half sequence. It means that each GPU will get wgrads based on the half sequence length. After you AR across the 2 GPUs, each GPU gets wgrads of full sequence length. The AR results are equivalent to CP1. However, the gradient scaling you pointed out can make the AR results 2x smaller, right? Then the wagrds in each GPU is 2x smaller than CP1, right? Multiplying loss by 2 (i.e., CP_size) makes the wgrads with CP2 same as CP1.","With this comment  ` Regards to your example of DP1CP1 (1GPU running) vs. DP1CP2 (2GPU running), each GPU should have 1 microbatch in both cases, so neither of them have scaling by num_microbatches. The difference is that DP1CP1 optimizer scale weight grad by 1, but DP1CP2 scale weight grads by 2. Hence, multiplying loss by context_paralleli_size in DP1CP2 case is still necessary, otherwise, optimizer weight grads scaling will make wgrads wrong. ` `DP1CP1 optimizer scale weight grad by 1, but DP1CP2 scale weight grads by 2.`.  In DP1CP2 , we scale weight grads by 2, but we will all reduce them across DP*CP group to sum them back instead of using loss * CP_SIZE","It seems you are right. But I use random model and random input, in GPU1, DP=1 and GPU2, DP1CP2, The grad norm in CP2 is right 2*grad_norm in CP1. Could you show me how to test this? Thanks very much.","Since we multiply loss ahead and delay context_parallel_sub_sequence_grad reduce in DDP allreduce. In the middle, we collect the norm of grad, will this be incoerect? Grad norm is crucial because of grad clip.",We mainly just ran a model (such as GPT or Llama) and check out loss curves and other metrics with CP=1 and CP>1.,"OK, Thank you very much, you are correct! But the calculation of grad_norm need to be fixed, which will impact grad clip. Thank you again!"," , we can fix it, could you please elaborate the bug of grad_norm calculation? thanks","  Absolutely! Take GPU1CP1DP1 [CP1] and GPU2CP2DP1[CP2] for example. ``` CP2:  1. loss will be all reduce across cp size, so loss is intact 2. we will multiply loss by 2, then we get 2 * loss  3. seq in CP0 or CP1 is not intact, we call them seq0 and seq1. 4. calculate wgrad, this is not intact because of subseq0 and subseq1, we will get 2 * wgrad0 and 2 * wgrad1 5. calculate gradnorm,  formula is sqrt(sum(grad**2)),  we will get sqrt(sum((2 * wgrad0) **2)) and sqrt(sum((2 * wgrad0) **2)), which are 2 * sqrt(sum((wgrad0**2))) and  2 * sqrt(sum(wgrad0**2)) CP1: the correct grad norm is sqrt(sum((wgrad0 + wgrad1)**2)), which is unaligned with CP2. ```","did you enable `calculate_per_token_loss`? I found the same conclusion as . In my case, I turn on `calculate_per_token_loss` and remove `*cp_size` in loss_func. Then the loss and grad_norm curve fits well.","Hi, yt  I don't turn on `calculate_per_token_loss`, because my loss is MSE in per sample. Which setting are you working on?  A pretrained model or a totally scratched model? Remove `*cp_size` is very dangurous because you are dividing grad by 2.","> [](https://github.com/xrennvidia) Absolutely! >  > Take GPU1CP1DP1 [CP1] and GPU2CP2DP1[CP2] for example. >  > ``` > CP2:  > 1. loss will be all reduce across cp size, so loss is intact > 2. we will multiply loss by 2, then we get 2 * loss  > 3. seq in CP0 or CP1 is not intact, we call them seq0 and seq1. > 4. calculate wgrad, this is not intact because of subseq0 and subseq1, we will get 2 * wgrad0 and 2 * wgrad1 > 5. calculate gradnorm,  formula is sqrt(sum(grad**2)),  we will get sqrt(sum((2 * wgrad0) **2)) and sqrt(sum((2 * wgrad0) **2)), which are 2 * sqrt(sum((wgrad0**2))) and  2 * sqrt(sum(wgrad0**2)) >  > CP1: > the correct grad norm is sqrt(sum((wgrad0 + wgrad1)**2)), which is unaligned with CP2. > ``` Sorry for the delayed reply, why is it like this? it should do wgrads scaling and reduction first, then calculate grad_norm, right?"," I've seen the code. It indeed do wgrads scaling and reduction first. Then the cp grad norm should not be cp_size * dp_grad_norm. Right now, I can't figure it out why my grad norm is wrong. But somehow, I remove the `loss * cp_size` and  add cp grad reduce after `main_grad` in ddp grad reduce_scatter, then grad norm works well. I think this also makes sense.","> Hi, yt I don't turn on `calculate_per_token_loss`, because my loss is MSE in per sample. Which setting are you working on? A pretrained model or a totally scratched model? >  Both from scratch and continuepretrain. > Remove `*cp_size` is very dangurous because you are dividing grad by 2. But you said you removed it? I test it on a small random init model, and a repeated dummy data: tp=1 pp=1 dp=1 cp=4 ```  [20250220 11:18:36] iteration        1/250000000  ``` the log shows it works well. full arguments ```  arguments    accumulate_allreduce_grads_in_fp32 .............. True   adam_beta1 ...................................... 0.9   adam_beta2 ...................................... 0.95   adam_eps ........................................ 1e08   add_bias_linear ................................. False   add_position_embedding .......................... True   add_qkv_bias .................................... True   adlr_autoresume ................................. False   adlr_autoresume_interval ........................ 1000   app_tag_run_name ................................ None   app_tag_run_version ............................. 0.0.0   apply_layernorm_1p .............................. False   apply_query_key_layer_scaling ................... False   apply_residual_connection_post_layernorm ........ False   apply_rope_fusion ............................... True   async_save ...................................... True   async_tensor_model_parallel_allreduce ........... True   attention_dropout ............................... 0.0   attention_softmax_in_fp32 ....................... False   auto_detect_ckpt_format ......................... False   barrier_with_L1_time ............................ True   bert_binary_head ................................ True   bert_embedder_type .............................. megatron   bert_load ....................................... None   bf16 ............................................ True   bias_dropout_fusion ............................. True   bias_gelu_fusion ................................ False   bias_swiglu_fusion .............................. True   biencoder_projection_dim ........................ 0   biencoder_shared_query_context_model ............ False   block_data_path ................................. None   calculate_per_token_loss ........................ True   check_for_nan_in_loss_and_grad .................. True   check_weight_hash_across_dp_replicas_interval ... None   ckpt_assume_constant_structure .................. False   ckpt_fully_parallel_load ........................ False   ckpt_fully_parallel_save ........................ True   ckpt_fully_parallel_save_deprecated ............. False   ckpt_step ....................................... None   classes_fraction ................................ 1.0   clip_grad ....................................... 1.0   clone_scatter_output_in_embedding ............... True   consumed_train_samples .......................... 0   consumed_valid_samples .......................... 0   context_parallel_size ........................... 1   create_attention_mask_in_dataloader ............. True   cross_entropy_loss_fusion ....................... False   data_cache_path ................................. None   data_parallel_random_init ....................... False   data_parallel_size .............................. 4   data_path ....................................... None   data_per_class_fraction ......................... 1.0   data_sharding ................................... True   dataloader_type ................................. single   ddp_average_in_collective ....................... False   ddp_bucket_size ................................. None   decoder_num_layers .............................. None   decoder_seq_length .............................. None   decoupled_lr .................................... None   decoupled_min_lr ................................ None   defer_embedding_wgrad_compute ................... False   delay_grad_reduce ............................... True   delay_param_gather .............................. False   deprecated_use_mcore_models ..................... False   deterministic_mode .............................. False   dino_bottleneck_size ............................ 256   dino_freeze_last_layer .......................... 1   dino_head_hidden_size ........................... 2048   dino_local_crops_number ......................... 10   dino_local_img_size ............................. 96   dino_norm_last_layer ............................ False   dino_teacher_temp ............................... 0.07   dino_warmup_teacher_temp ........................ 0.04   dino_warmup_teacher_temp_epochs ................. 30   disable_straggler_on_startup .................... False   dist_ckpt_format ................................ torch_dist   dist_ckpt_strictness ............................ assume_ok_unexpected   distribute_saved_activations .................... False   distributed_backend ............................. nccl   distributed_timeout_minutes ..................... 10   embedding_path .................................. None   empty_unused_memory_level ....................... 0   enable_one_logger ............................... True   encoder_num_layers .............................. 4   encoder_seq_length .............................. 2048   end_weight_decay ................................ 0.1   eod_mask_loss ................................... False   epochs .......................................... 1   eval_interval ................................... 1000   eval_iters ...................................... 100   evidence_data_path .............................. None   exit_duration_in_mins ........................... None   exit_interval ................................... None   exit_on_missing_checkpoint ...................... False   exit_signal_handler ............................. True   expert_model_parallel_size ...................... 1   ffn_hidden_size ................................. 1024   finetune ........................................ False   fp16 ............................................ False   fp16_lm_cross_entropy ........................... False   fp32_residual_connection ........................ False   fp8 ............................................. None   fp8_amax_compute_algo ........................... most_recent   fp8_amax_history_len ............................ 1   fp8_interval .................................... 1   fp8_margin ...................................... 0   fp8_wgrad ....................................... True   freeze_llm ...................................... 0   freeze_vit ...................................... 0   global_batch_size ............................... 4   gradient_accumulation_fusion .................... True   group_query_attention ........................... True   head_lr_mult .................................... 1.0   hidden_dropout .................................. 0.0   hidden_size ..................................... 512   hybrid_attention_ratio .......................... 0.0   hybrid_mlp_ratio ................................ 0.0   hybrid_override_pattern ......................... None   hysteresis ...................................... 2   ict_head_size ................................... None   ict_load ........................................ None   image_root ......................................   img_h ........................................... 224   img_w ........................................... 224   indexer_batch_size .............................. 128   indexer_log_interval ............................ 1000   inference_batch_times_seqlen_threshold .......... 512   init_method_std ................................. 0.02   init_method_xavier_uniform ...................... False   initial_loss_scale .............................. 4294967296   iter_per_epoch .................................. 1250   kv_channels ..................................... 128   lazy_mpu_init ................................... None   load ............................................ None   local_rank ...................................... None   log_batch_size_to_tensorboard ................... False   log_interval .................................... 1   log_learning_rate_to_tensorboard ................ True   log_loss_scale_to_tensorboard ................... False   log_memory_to_tensorboard ....................... False   log_num_zeros_in_grad ........................... False   log_params_norm ................................. False   log_progress .................................... False   log_straggler ................................... False   log_throughput .................................. True   log_timers_to_tensorboard ....................... True   log_validation_ppl_to_tensorboard ............... False   log_world_size_to_tensorboard ................... False   logging_level ................................... None   loss_scale ...................................... None   loss_scale_window ............................... 1000   lr .............................................. 1e05   lr_decay_iters .................................. None   lr_decay_samples ................................ None   lr_decay_style .................................. cosine   lr_warmup_fraction .............................. None   lr_warmup_init .................................. 0.0   lr_warmup_iters ................................. 0   lr_warmup_samples ............................... 0   lr_wsd_decay_iters .............................. None   lr_wsd_decay_samples ............................ None   lr_wsd_decay_style .............................. exponential   make_vocab_size_divisible_by .................... 128   manual_gc ....................................... False   manual_gc_eval .................................. True   manual_gc_interval .............................. 0   mask_factor ..................................... 1.0   mask_prob ....................................... 0.15   mask_type ....................................... random   masked_softmax_fusion ........................... True   max_position_embeddings ......................... 2048   max_tokens_to_oom ............................... 12000   merge_file ...................................... None   micro_batch_size ................................ 1   min_loss_scale .................................. 1.0   min_lr .......................................... 1e05   mmap_bin_files .................................. True   mock_data ....................................... False   moe_aux_loss_coeff .............................. 0.0   moe_expert_capacity_factor ...................... None   moe_extended_tp ................................. False   moe_grouped_gemm ................................ False   moe_input_jitter_eps ............................ None   moe_layer_recompute ............................. False   moe_pad_expert_input_to_capacity ................ False   moe_per_layer_logging ........................... False   moe_router_load_balancing_type .................. aux_loss   moe_router_pre_softmax .......................... False   moe_router_topk ................................. 2   moe_token_dispatcher_type ....................... allgather   moe_token_drop_policy ........................... probs   moe_z_loss_coeff ................................ None   nccl_communicator_config_path ................... None   no_load_optim ................................... None   no_load_rng ..................................... None   no_persist_layer_norm ........................... False   no_save_optim ................................... None   no_save_rng ..................................... None   norm_epsilon .................................... 1e06   normalization ................................... RMSNorm   num_attention_heads ............................. 4   num_channels .................................... 3   num_classes ..................................... 1000   num_dataset_builder_threads ..................... 1   num_experts ..................................... None   num_layers ...................................... 4   num_layers_per_virtual_pipeline_stage ........... None   num_query_groups ................................ 2   num_workers ..................................... 5   one_logger_async ................................ False   one_logger_project .............................. megatronlm   one_logger_run_name ............................. None   onnx_safe ....................................... None   openai_gelu ..................................... False   optimizer ....................................... adam   output_bert_embeddings .......................... False   overlap_grad_reduce ............................. True   overlap_p2p_comm ................................ False   overlap_param_gather ............................ True   override_opt_param_scheduler .................... False   padded_vocab_size ............................... 512   params_dtype .................................... torch.bfloat16   patch_dim ....................................... 16   perform_initialization .......................... True   pipeline_model_parallel_size .................... 1   pipeline_model_parallel_split_rank .............. None   position_embedding_type ......................... rope   pretrain_data ...................................   pretrained_checkpoint ........................... None   profile ......................................... False   profile_ranks ................................... [0]   profile_step_end ................................ 12   profile_step_start .............................. 10   qk_layernorm .................................... False   query_in_block_prob ............................. 0.1   rampup_batch_size ............................... None   rank ............................................ 0   recompute_granularity ........................... full   recompute_method ................................ uniform   recompute_num_layers ............................ 1   reset_attention_mask ............................ False   reset_position_ids .............................. False   retriever_report_topk_accuracies ................ []   retriever_score_scaling ......................... False   retriever_seq_length ............................ 256   retro_add_retriever ............................. False   retro_attention_gate ............................ 1   retro_cyclic_train_iters ........................ None   retro_encoder_attention_dropout ................. 0.1   retro_encoder_hidden_dropout .................... 0.1   retro_encoder_layers ............................ 2   retro_num_neighbors ............................. 2   retro_num_retrieved_chunks ...................... 2   retro_project_dir ............................... None   retro_verify_neighbor_count ..................... True   rotary_base ..................................... 1000000   rotary_interleaved .............................. False   rotary_percent .................................. 1.0   rotary_seq_len_interpolation_factor ............. None   s3_cache_path ................................... None   sample_rate ..................................... 1.0   save ............................................ None   save_epoch_interval ............................. 0   save_interval ................................... 1000   scatter_gather_tensors_in_pipeline .............. True   seed ............................................ 42   seq_length ...................................... 2048   sequence_parallel ............................... False   sgd_momentum .................................... 0.9   short_seq_prob .................................. 0.1   skip_train ...................................... False   spec ............................................ None   split ........................................... None   squared_relu .................................... False   standalone_embedding_stage ...................... False   start_weight_decay .............................. 0.1   straggler_ctrlr_port ............................ 65535   straggler_minmax_count .......................... 1   swiglu .......................................... True   swin_backbone_type .............................. tiny   tensor_model_parallel_size ...................... 1   tensorboard_dir ................................. None   tensorboard_log_interval ........................ 1   tensorboard_queue_size .......................... 5   test_data_path .................................. None   test_mode ....................................... False   tiktoken_num_special_tokens ..................... 1000   tiktoken_pattern ................................ None   tiktoken_special_tokens ......................... None   timing_log_level ................................ 0   timing_log_option ............................... minmax   titles_data_path ................................ None   tokenizer_model ................................. None   tokenizer_type .................................. None   tp_comm_bulk_dgrad .............................. True   tp_comm_bulk_wgrad .............................. True   tp_comm_overlap ................................. False   tp_comm_overlap_ag .............................. True   tp_comm_overlap_cfg ............................. None   tp_comm_overlap_rs .............................. True   tp_comm_overlap_rs_dgrad ........................ False   tp_comm_split_ag ................................ True   tp_comm_split_rs ................................ True   train_data ......................................   train_data_path ................................. None   train_iters ..................................... None   train_samples ................................... None   transformer_impl ................................ transformer_engine   transformer_pipeline_model_parallel_size ........ 1   untie_embeddings_and_output_weights ............. True   use_checkpoint_args ............................. False   use_checkpoint_opt_param_scheduler .............. True   use_cpu_initialization .......................... None   use_dist_ckpt ................................... True   use_distributed_optimizer ....................... True   use_flash_attn .................................. True   use_legacy_models ............................... False   use_one_sent_docs ............................... False   use_ring_exchange_p2p ........................... False   use_rotary_position_embeddings .................. False   use_tp_pp_dp_mapping ............................ False   val_data ........................................   valid_data_path ................................. None   variable_seq_lengths ............................ False   virtual_pipeline_model_parallel_size ............ None   vision_backbone_type ............................ vit   vision_pretraining .............................. False   vision_pretraining_type ......................... classify   vision_tower_recompute .......................... True   vocab_extra_ids ................................. 0   vocab_file ...................................... None   vocab_size ...................................... None   wandb_exp_name ..................................   wandb_project ...................................   wandb_save_dir ..................................   weight_decay .................................... 0.1   weight_decay_incr_style ......................... constant   wgrad_deferral_limit ............................ 0   world_size ...................................... 4   yaml_cfg ........................................ None  end of arguments  I tried version both at mcore_r0.8.0 and mcore_r0.10.0, it shows the same result. I removed the `*cp_size`.","I removed the `*cp_size`, because I reduce grad across CP group explicitly. Could you show the log in `don't remove loss * cp_size` setting? Grad norm and loss will not diverge so much, because the difference of grad is not that large.","> Could you show the log in `don't remove loss * cp_size` setting? Grad norm and loss will not diverge so much, because the difference of grad is not that large. tp=1 pp=1 dp=4 cp=1 ```  [20250220 11:27:02] iteration        1/250000000  ```",after `loss * cp_size` grad norm is cp times larger.,"The perfect loss is very impressive.  So if you don't use grad_clip,  please turn on `loss * cp_size`.","> The perfect loss is very impressive. So if you don't use grad_clip, please turn on `loss * cp_size`. Thank you! I will turn on it in my further experiments. I also test continuepretrain on Qwen2.57B with real data. tp=4 pp=1 dp=2 cp=1 ```  [20250220 12:01:39] iteration        1/ 1412026  ```"
Infi-zc,fix loss scale in loss func with cp,refer to the issue https://github.com/NVIDIA/MegatronLM/issues/673,2024-01-23T06:22:55Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/672
yaoyu-33,[ENHANCEMENT] Add support for HF style Bert,"**Is your feature request related to a problem? Please describe.** PostLN is not supported in MCore transformer at the moment (everything is PreLN). In order to convert HF checkpoints, we want to add postLN style to MCore as well. PostLN requires an additional LN in the beginning of the network as well. **Describe the solution you'd like** Please refer to NeMo code update here for HF Bert support: https://github.com/NVIDIA/NeMo/pull/8088",2024-01-22T23:39:07Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/671,Marking as stale. No activity in 60 days.
wangbo233,[QUESTION] Why write a special LinearWithFrozenWeight?,"**Your question**  Is there any explanation about this? Seems the difference between LinearWithFrozenWeight and  LinearWithGradAccumulationAndAsyncCommunication is that LinearWithFrozenWeight does not calculate the gradient of weight. This looks like a performance optimization, but turns out this could cause different results.",2024-01-22T07:25:54Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/670,Marking as stale. No activity in 60 days.
lxg2015,[Fix] moe router layer missing dtype argument,"**Description** I build a moe model, and construct input for it like below,  ```   from megatron.core.models.gpt import GPTModel  model = GPTModel(         config=config,         transformer_layer_spec=transformer_layer_spec,         vocab_size=args.padded_vocab_size,         max_sequence_length=args.max_position_embeddings,         pre_process=pre_process,         post_process=post_process,         fp16_lm_cross_entropy=args.fp16_lm_cross_entropy,         parallel_output=True,         share_embeddings_and_output_weights=not args.untie_embeddings_and_output_weights,         position_embedding_type=args.position_embedding_type,         rotary_percent=args.rotary_percent,         rotary_base=args.rotary_base,     ) ... model(input_ids=input_ids, attention_mask=attention_mask, position_ids=position_ids) ``` then when enable bf16, the model forward will encounter an param dtype RunTimeError. This PR will fix it  >   File ""/megatron/core/transformer/switch_mlp.py"", line 94, in forward     route = self.router(hidden_states)   File ""/usr/local/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1501, in _call_impl     return forward_call(*args, **kwargs)   File ""/usr/local/lib/python3.9/sitepackages/torch/nn/modules/linear.py"", line 114, in forward     return F.linear(input, self.weight, self.bias) RuntimeError: expected scalar type BFloat16 but found Float",2024-01-22T05:04:43Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/669
ethanhe42,add core_attention_bias_type to TransformerConfig,`core_attention_bias_type` is needed to use alibi from transformer engine https://docs.nvidia.com/deeplearning/transformerengine/userguide/api/pytorch.html?highlight=alibitransformer_engine.pytorch.DotProductAttention.forward,2024-01-21T10:05:59Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/668,33 ,Marking as stale. No activity in 60 days.,Marking as stale. No activity in 60 days.
matrixssy,Support Mixtral 8*7B MOE ," Support Mixtral 8*7B MOE  model structure and weight converter from huggingface. You can refer to this script to convert the huggingface weight to megatron: ```shell python tools/checkpoint/util.py modeltype GPT loader mixtral_hf saver mixtral loaddir ../models/Mixtral8x7BInstructv0.1 savedir ../models/Mixtral8x7BInstructv0.1tp2pp4 tokenizermodel ../models/Mixtral8x7BInstructv0.1/tokenizer.model targettensorparallelsize 2 targetpipelineparallelsize 4 ``` To activate mixtral moe in training: ```shell numexperts 8 \ moetype mixtral \ ``` Note that: To implement the load balancing loss of huggingface equivalently on megatron requires a lot of modifications for returning router  logits. Therefore, in order to simplify the work, I choose to use the original sinkhorn algorithm to balance the voting probability of each expert instead of using the load_balancing_loss_func in huggingface.",2024-01-18T11:29:44Z,,closed,3,19,https://github.com/NVIDIA/Megatron-LM/issues/667, CC([ENHANCEMENT] Do you have a plan that supports Mixtral 8x7B?),Great work! Could you provide a script of convert megatron mixtral to hf ?,> Great work! Could you provide a script of convert megatron mixtral to hf ? Still working on it.,> Great work! Great work! Look forward to a script of convert megatron mixtral to hf.,"Hi, I wonder if the loss is normal after converting and training mixtral with megatron at your computer.  I apply this PR and the initial loss is quite high, which seems to indicate the forward step is not aligned with huggingface version, especially in TP>1.  ","> Hi, I wonder if the loss is normal after converting and training mixtral with megatron at your computer. I apply this PR and the initial loss is quite high, which seems to indicate the forward step is not aligned with huggingface version, especially in TP>1.  Hello, first of all, thank you for giving it a try. In my example, I trained Mixtral 8x7Bv0.1 on the gpt4 alpaca zh dataset, and the loss decreased from 4.5 to around 1.0 after 300 iterations; if I train Mixtral 8x7BInstructionv0.1 on the gpt4 alpaca zh dataset, the loss decreases from 1.5 to around 1.0 after 300 iterations. Could you share your HF training loss curve? Although this PR has not achieved load balancing loss (around 1e2 level), the loss should not differ significantly from HF.","> Hi, I wonder if the loss is normal after converting and training mixtral with megatron at your computer. I apply this PR and the initial loss is quite high, which seems to indicate the forward step is not aligned with huggingface version, especially in TP>1.  By the way, I have verified that the relative error of the average forward logits between the converted megatron model and the HF (Hugging Face) model is within 1%, and the cosine similarity is 0.9999.","Hi, I fix a bug in my script and now the initial loss is normal *(around 2.3 in arxiv dataset). Thanks for your contribution! also, I have an extra question, as the gate linear is shared across TP groups, why not define it as a tensor_parallel.RowParallelLinear?","> Hi, I fix a bug in my script and now the initial loss is normal *(around 2.3 in arxiv dataset). Thanks for your contribution! >  > also, I have an extra question, as the gate linear is shared across TP groups, why not define it as a tensor_parallel.RowParallelLinear? Good question! My initial idea was that the shape of the router would be (hidden state, n_experts), which is not particularly large and only has one per layer, so the benefits of parallelization are not significant. Additionally, when implementing loadbalancing loss in the future, obtaining route logits will become difficult.","Hi, . Thanks for your contribution, there are some ongoing efforts in NVIDIA internally working on the Mixtral 8x7B example. We will support convert HF checkpoint to MCore checkpoint with different EP/TP/PP size. The code will be released with some code refactor soon.","> Hi, . Thanks for your contribution, there are some ongoing efforts in NVIDIA internally working on the Mixtral 8x7B example. We will support convert HF checkpoint to MCore checkpoint with different EP/TP/PP size. The code will be released with some code refactor soon. Cool! How much longer will this task take (convert HF checkpoint to MCore checkpoint in EP), and are there any preceding pull requests?","Actually, the functionality of changing EP size has been implemented. But there is a preceding MR (on gitlab internally) still being reviewed which implement MLMlegacy to MCore model converter. After that MR finished, I need some time for code refactor. I think the MR for Mixtral checkpoint convert will be released this month.","Hi, when I run your code, I got two errors. Could you help me and give some advises ? !image !image","Hi, when I set targettensorparallelsize > 1 , I got the following errors. only setting targettensorparallelsize = 1 works. Is it possible that it is related to the following warning ? I use the latest docker with pytorch and nvidia, What can I do to resolve this   missing packages problem. Thanks very much. !image !image","> Hi, when I set targettensorparallelsize > 1 , I got the following errors. only setting targettensorparallelsize = 1 works. Is it possible that it is related to the following warning ? I use the latest docker with pytorch and nvidia, What can I do to resolve this missing packages problem. Thanks very much. !image >  > !image Yes, you need to set sequenceparallel",Marking as stale. No activity in 60 days.,Any update on this ?,"  Have the plans to release a checkpointconverter that supports MoE (mentioned here as ""Coming Soon"")  changed?","> Actually, the functionality of changing EP size has been implemented. >  > But there is a preceding MR (on gitlab internally) still being reviewed which implement MLMlegacy to MCore model converter. After that MR finished, I need some time for code refactor. I think the MR for Mixtral checkpoint convert will be released this month. Hi Victarry, is there some progress about this?"
shaonan1993,[QUESTION] What's the different between implementation of GPT model in megatron/model/gpt_model.py and megatron/core/model/gpt/gpt_model.py?,"Hi, I find there are two implementation of gpt model in the repo. I want to know to know what's the different between them?",2024-01-18T08:48:49Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/666,They should be similar. `megatron/model/gpt_model.py` will eventually be deprecated.,Going to close this.
loofahcus,fix: fix wrong index slicing in _get_ltor_masks_and_position_ids,remove the index slicing `b` in `_get_ltor_masks_and_position_ids` since the `data` tensor is no longer a 2D tensor.,2024-01-18T03:48:40Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/665,Marking as stale. No activity in 60 days.
zyksir,[QUESTION] Why use CPU to gather distributed optimizer,"Can we use GPU to gather the state and then save it? Since with GPU we can use NCCL and rdma, and now we have to use gloo and TCP.",2024-01-16T12:23:50Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/664,"We use gloo / CPU to collect the distributed optimizer state across DP ranks, which might not all fit in GPU memory.","Going to close this, feel free to reopen if you still have questions."
jindajia,[BUG] loss can not decrease when using overlap-param-gather,"**Describe the bug** Hello, I've encountered an issue where using `overlapgatherparameters` and `overlapreducegradients` simultaneously seems to prevent the loss from decreasing as expected. It appears that in the `optimizer.step()` function, the `found_inf_flag` is consistently true, leading to the learning rate remaining at zero. To investigate this, I experimented with three different settings: 1. Without overlapping both gradient reduction and parameter gathering, the loss decreases normally. 2. With overlapping gradient reduction but without overlapping parameter gathering, the loss still decreases normally. 3. However, when both gradient reduction and parameter gathering are overlapped, the loss does not decrease. It seems there might be an issue with the overlapping parameter gathering. Any insights or suggestions on this matter would be greatly appreciated. **To Reproduce** Steps to reproduce the behavior. The easier it is to reproduce the faster it will get maintainer attention. Training Arguments: ``` MODEL_ARGS=""     numlayers 12 \     hiddensize 768 \     numattentionheads 12 \     seqlength 1024 \     maxpositionembeddings 1024 \ "" OPTIMIZER_ARGS=""     lr 0.0006 \     lrdecayiters 70000 \     lrdecaystyle cosine \     minlr 0.00006 \     adambeta1 0.9 \     adambeta2 0.95 \     adameps 1e08 \     weightdecay .1 \     lrwarmupfraction 0.01 \     clipgrad 1.0 \     fp16 \     usedistributedoptimizer \ "" TRAING_ARGS=""     tensormodelparallelsize 1 \     pipelinemodelparallelsize 1 \     microbatchsize 2 \     globalbatchsize 256 \     trainiters 80000 \ "" OUTPUT_ARGS=""     loginterval 10 \     timingloglevel 2 \     saveinterval 1000 \     evalinterval 100 \     evaliters 10 \     logtimerstotensorboard \     logvalidationppltotensorboard \     logthroughput \     tensorboarddir ${TENSORBOARD_DIR} \     tensorboardloginterval 1 \     wandbproject zeroppquantization\     wandbsavedir ${WANDB_DIR} \     wandbexpname qtgradients \ "" PERFORMANCE_ARGS=""     noasynctensormodelparallelallreduce \     recomputeactivations \     recomputegranularity selective \     overlapparamgather \     overlapgradreduce \     nodelaygradreduce \ "" ``` **Environment (please complete the following information):**   MegatronLM commit ID: a22f6723   PyTorch version 2.0.1   CUDA 11.8   NCCL version 2.14.3 **Additional context** output log ``` (min, max) time across ranks (ms):     loadcheckpoint ................................: (0.28, 26.66) /ocean/projects/asc200010p/jjia1/TOOLS/mambaforge/envs/megatroncuda_118/lib/python3.10/sitepackages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.   warnings.warn(_create_warning_msg( (min, max) time across ranks (ms):     modelandoptimizersetup ......................: (416.54, 452.18)     train/valid/testdataiteratorssetup ..........: (4135.60, 4811.10)  iteration       10/   80000  (min, max) time across ranks (ms):     forwardbackward ...............................: (3397.56, 3422.50)     forwardcompute ................................: (1938.00, 2043.37)     backwardcompute ...............................: (1342.57, 1452.97)     batchgenerator ................................: (24.89, 35.75)     layernormgradsallreduce .....................: (0.03, 0.03)     embeddinggradsallreduce .....................: (0.04, 0.05)     allgradssync .................................: (0.05, 0.06)     optimizercopytomaingrad ....................: (0.78, 2.44)     optimizerunscaleandcheckinf ................: (11.96, 22.84)     optimizer ......................................: (45.09, 56.44) ```",2024-01-15T23:30:45Z,,closed,0,8,https://github.com/NVIDIA/Megatron-LM/issues/663,Will look into this.,Thank you! Have you reproduced this issue?,"I just reproduced it, thanks. Trying to figure out what's wrong, and then will implement a fix.","I have a fix, it just needs to get through internal peer review and then it will be pushed here.","Awesome, thank you so much. In the past few days, I also found an informal way to bypass this bug, which is to manually set the `initiallossscale`. This can also temporarily circumvent the issue.",: The fix has been pushed (https://github.com/NVIDIA/MegatronLM/commit/75120dbf069703d21e34f0cb031fb4a3c1ac5315). Please let me know if you are still running into issues.,Thank you for your repair!! I will try it and let you know if there are anything issues.," Hi, I encountered an issue. I set overlapgradreduce and overlapparamgather. I use both train and eval. After eval finishes, the first step of the next train has a gradient of 0, but the subsequent train steps have normal gradients.  Do you have any ideas? If I set force_sync=True here, the gradients become normal."
robotsp,[QUESTION] How to release the model and optimizer memory manually?,"**Your question** How to release the model and optimizer memory manually? **What I have tried**  set zero_grad() method  set None method  del method  gc.collect()  torch.cuda.empty_cached() But the ways above are not worked, please help, thanks!",2024-01-15T13:35:33Z,stale,closed,0,7,https://github.com/NVIDIA/Megatron-LM/issues/662,What's the use case for releasing the model memory? Trying to delete the optimizer object might help with releasing the optimizer memory (so something like `del optimizer` in `megatron/training.py`).,"> What's the use case for releasing the model memory? Trying to delete the optimizer object might help with releasing the optimizer memory (so something like `del optimizer` in `megatron/training.py`). I have a use case to call the training function twice, but I found some memory fragment that cannot release by del and torch.cuda.empty_cache() after the first dumpy training.  ","I see, makes sense. You probably don't want to release model memory still. Presumably you are not doing a backward pass the first time you run the training function? You could try activation recomputation.","Actually, the use case is required to change the original neural network structure and that's why I want to release the model and optimizer memory from the original one. I tried recomputation to release activation in the dummy training already. But model and optimizer should occupy more memory, I think? I have clear model parameters and optimizer state by setting tensor.storage.resize_(0). It did drop some of the memory fragment but not that much.  Thanks for your quick answer in advance!","Hmm, not sure. `del model` and `del optimizer` seem like the ""right"" things to do here."," One more question, I found I trained the model in the second time after the dummy first training, its loss curve is different from that one in the training from scratch. Do you know why? Should I destroy the process group and reinitialize it?",Marking as stale. No activity in 60 days.
haytham918,Duplicate GPU detected when running pretrain_gpt_distributed_with_mp.sh,"Hello, When I am running pretrain_gpt_distributed_with_mp.sh script, it always gives me this error, and I can't find any reference to fix it. I do not change the script except modifying the checkpoint_path and dataset file location. ``` torch.distributed  File ""/run/megatron/initialize.py"", line 156, in _compile_dependencies .DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1727, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.19.3 ncclInvalidUsage: This usually reflects invalid usage of NCCL library. Last error: Duplicate GPU detected : rank 3 and rank 0 both on CUDA device f000     torch.distributed.barrier()   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/c10d_logger.py"", line 72, in wrapper     work = default_pg.barrier(opts=opts) torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1727, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.19.3 ncclInvalidUsage: This usually reflects invalid usage of NCCL library. Last error: Duplicate GPU detected : rank 7 and rank 0 both on CUDA device f000     return func(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/distributed_c10d.py"", line 3744, in barrier     work = default_pg.barrier(opts=opts) torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1727, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.19.3 ncclInvalidUsage: This usually reflects invalid usage of NCCL library. ```",2024-01-14T08:07:52Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/661,Can you post the script you are running here? Seems like multiple processes are being run on the same rank. How many GPUs does the machine you are running on have? 4?,"Never mind. I didn't realize the node I was using only has 1 GPU, which causes the problem for distributed learning.","Great! As a note, when using `torchrun`, you need to set `nproc_per_node` to the number of GPUs you have a on a single server. Going to close this."
echoyinke,re-padding the num of elements ,"repadding the num of elements , it is uesfull when you load a checkpoint to a new DP size",2024-01-12T09:31:37Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/660,"Thanks for the PR, we already have an internal MR for this that should go in soon.",We just merged it: https://github.com/NVIDIA/MegatronLM/commit/66bba1d66540f345eeffb947ec69d69408d90f55.,"Closing this, hopefully the above commit works for you!"
lzyubermensch,"[BUG] checkpoint saved on two nodes cannot be loaded on three nodes, but it can be loaded on four nodes.","**Describe the bug** I encountered a bug while using distributed optimizer on Megatron main branch. After training with two nodes(16 GPU cards) and saving the checkpoints, I attempted to load the same checkpoints on three nodes(24 GPU cards), but an error occurred. However, loading the checkpoints with four nodes(32 GPU cards) worked without any issues. **To Reproduce** 1.  Train with two nodes(16 GPU cards) on Megatron main branch with the parameters(TP = 1, PP =1, usedistributedoptimizer=True, overlapparamgather=True, overlapgradreduce=True, accumulate_allreduce_grads_in_fp32=True). 2. Save the checkpoints after training. 3. Load the same checkpoints on three nodes(24 GPU cards) with the same parameters(TP = 1, PP =1, usedistributedoptimizer=True, overlapparamgather=True, overlapgradreduce=True, accumulate_allreduce_grads_in_fp32=True). 4. Load the same checkpoints on four nodes(32 GPU cards) with the same parameters(TP = 1, PP =1, usedistributedoptimizer=True, overlapparamgather=True, overlapgradreduce=True, accumulate_allreduce_grads_in_fp32=True). **Expected behavior** The checkpoint should be loadable and runnable on any number of machines. **Stack trace/logs** > Traceback (most recent call last):   File ""/megatron/code/MegatronLM/megatron/checkpointing.py"", line 611, in load_checkpoint     optimizer.load_parameter_state(optim_checkpoint_name)   File ""/megatron/code/MegatronLM/megatron/optimizer/distrib_optimizer.py"", line 755, in load_parameter_state     assert self.per_bucket_numel == per_bucket_numel_in_checkpoint, \ AssertionError: Number of elements in each bucket need to be the same in current run ([{torch.float32: [44576784, 89150736, 67133448, 44576784, 89150736, 67133448, 44576784, 89150736, 67133448, 44576784, 89150736, 67133448, 206053392]}]) and checkpoint ([{torch.float32: [44576768, 89150720, 67133440, 44576768, 89150720, 67133440, 44576768, 89150720, 67133440, 44576768, 89150720, 67133440, 206053376]}]) **Environment (please complete the following information):**   MegatronLM commit ID: bcce6f54e075e3c3374ea67adefe54f3f2da2b07 (main branch)   PyTorch version: py310   CUDA version: cu121 **Proposed fix** It seems like a grad_buffer issue caused by DistributedDataParallel(MegatronModule).",2024-01-11T03:20:08Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/659," Maybe you can help me, thanks!",Marking as stale. No activity in 60 days.
ken-arf,How to convert Llama-2 huggingface checkpoint to the megatron format ,"What is the proper way to convert the Llama2 huggingface checkpoint format to the Megatron? I followed the instructions in the docs/llama2.md, but got the following errors. I don't understand why transformer_engine in core/transformer/custom_layers imports itself as te at line 6,  and in that module, there is no attribute for pytorch. MODEL_SIZE=7B TP=1 TOP=/mnt MEGATRON_DIR=$TOP/Megatron/MegatronLM HF_FORMAT_DIR=$TOP/LLaMa/llama_workarea/hf_llama_models/$MODEL_SIZE MEGATRON_FORMAT_DIR=$TOP/Megatron/workspace.MegatronLM/weights/$MODEL_SIZE TOKENIZER_MODEL=$TOP/LLaMa/llama_workarea/hf_llama_models/7B/$MODEL_SIZE/tokenizer.model export PYTHONPATH=""$PWD:$PWD/tools/checkpoint"" echo $PYTHONPATH python3 tools/checkpoint/util.py \     modeltype GPT \     loader llama2_hf \     saver megatron \     targettensorparallelsize ${TP} \     loaddir ${HF_FORMAT_DIR} \     savedir ${MEGATRON_FORMAT_DIR} \     tokenizermodel ${TOKENIZER_MODEL}  Loaded loader_llama2_hf as the loader. Loaded saver_megatron as the saver. Starting saver... Starting loader... Zarrbased strategies will not be registered because of missing packages Zarrbased strategies will not be registered because of missing packages   File ""/home/aae14935wb/Share/Megatron/MegatronLM/megatron/core/models/gpt/gpt_model.py"", line 15, in      from megatron.core.transformer.transformer_block import TransformerBlock   File ""/home/aae14935wb/Share/Megatron/MegatronLM/megatron/core/transformer/transformer_block.py"", line 13, in      from megatron.core.transformer.custom_layers.transformer_engine import TENorm   File ""/home/aae14935wb/Share/Megatron/MegatronLM/megatron/core/transformer/custom_layers/transformer_engine.py"", line 71, in      class TELinear(te.pytorch.Linear): AttributeError: module 'transformer_engine' has no attribute 'pytorch'",2024-01-10T06:28:10Z,stale,closed,0,6,https://github.com/NVIDIA/Megatron-LM/issues/658,try pip uninstall transformerengine,"I faced the same issue when converting a Hugging Face checkpoint to Transformer format. When I run the util.py script, I get an error like this: $ tools/checkpoint/util.py modeltype GPT {blah blah....... } Loaded loader_llama2_hf as the loader. Loaded saver_megatron as the saver. Starting saver... Starting loader... Unable to import Megatron, please specify the path to Megatron using megatronpath. Exiting. No module named 'megatron.arguments' Unable to import Megatron, please specify the path to Megatron using megatronpath. Exiting. I added an error print to check the module name and checked it, and I get an error that megatron.arguments does not exist as shown above. However, if you additionally import megatron, it will show that there is no transformer engine as shown below. $ python Python 3.8.6 (default, Jan 22 2021, 11:41:28) [GCC 8.4.1 20200928 (Red Hat 8.4.11)] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import megatron ...... ModuleNotFoundError: No module named 'transformer_engine' The method suggested by CaesarWWK, ""pip uninstall transformerengine,"" does not work because transformerengine is not installed in my environment. Could you suggest a better solution?","> I faced the same issue when converting a Hugging Face checkpoint to Transformer format. When I run the util.py script, I get an error like this: >  > $ tools/checkpoint/util.py modeltype GPT {blah blah....... } Loaded loader_llama2_hf as the loader. Loaded saver_megatron as the saver. Starting saver... Starting loader... Unable to import Megatron, please specify the path to Megatron using megatronpath. Exiting. No module named 'megatron.arguments' Unable to import Megatron, please specify the path to Megatron using megatronpath. Exiting. >  > I added an error print to check the module name and checked it, and I get an error that megatron.arguments does not exist as shown above. However, if you additionally import megatron, it will show that there is no transformer engine as shown below. >  > $ python Python 3.8.6 (default, Jan 22 2021, 11:41:28) [GCC 8.4.1 20200928 (Red Hat 8.4.11)] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >  > > > > import megatron > > > > ...... > > > > ModuleNotFoundError: No module named 'transformer_engine' >  > The method suggested by CaesarWWK, ""pip uninstall transformerengine,"" does not work because transformerengine is not installed in my environment. >  > Could you suggest a better solution? Hi, Have you already solved this problem ? I got a problem like you. But I can succeed in importing megatron, and my error informations are as follows: !image","> What is the proper way to convert the Llama2 huggingface checkpoint format to the Megatron? I followed the instructions in the docs/llama2.md, but got the following errors. I don't understand why transformer_engine in core/transformer/custom_layers imports itself as te at line 6, and in that module, there is no attribute for pytorch. >  > MODEL_SIZE=7B TP=1 TOP=/mnt MEGATRON_DIR=$TOP/Megatron/MegatronLM HF_FORMAT_DIR=$TOP/LLaMa/llama_workarea/hf_llama_models/$MODEL_SIZE MEGATRON_FORMAT_DIR=$TOP/Megatron/workspace.MegatronLM/weights/$MODEL_SIZE TOKENIZER_MODEL=$TOP/LLaMa/llama_workarea/hf_llama_models/7B/$MODEL_SIZE/tokenizer.model >  > export PYTHONPATH=""$PWD:$PWD/tools/checkpoint"" echo $PYTHONPATH >  > python3 tools/checkpoint/util.py modeltype GPT loader llama2_hf saver megatron targettensorparallelsize ${TP} loaddir ${HF_FORMAT_DIR} savedir ${MEGATRON_FORMAT_DIR} tokenizermodel ${TOKENIZER_MODEL} >  >  Loaded loader_llama2_hf as the loader. Loaded saver_megatron as the saver. Starting saver... Starting loader... Zarrbased strategies will not be registered because of missing packages Zarrbased strategies will not be registered because of missing packages File ""/home/aae14935wb/Share/Megatron/MegatronLM/megatron/core/models/gpt/gpt_model.py"", line 15, in from megatron.core.transformer.transformer_block import TransformerBlock File ""/home/aae14935wb/Share/Megatron/MegatronLM/megatron/core/transformer/transformer_block.py"", line 13, in from megatron.core.transformer.custom_layers.transformer_engine import TENorm File ""/home/aae14935wb/Share/Megatron/MegatronLM/megatron/core/transformer/custom_layers/transformer_engine.py"", line 71, in class TELinear(te.pytorch.Linear): AttributeError: module 'transformer_engine' has no attribute 'pytorch' I got the same problem like you! All informations are same. Have you already solved this problem ? ","> > What is the proper way to convert the Llama2 huggingface checkpoint format to the Megatron? I followed the instructions in the docs/llama2.md, but got the following errors. I don't understand why transformer_engine in core/transformer/custom_layers imports itself as te at line 6, and in that module, there is no attribute for pytorch. > > MODEL_SIZE=7B TP=1 TOP=/mnt MEGATRON_DIR=$TOP/Megatron/MegatronLM HF_FORMAT_DIR=$TOP/LLaMa/llama_workarea/hf_llama_models/$MODEL_SIZE MEGATRON_FORMAT_DIR=$TOP/Megatron/workspace.MegatronLM/weights/$MODEL_SIZE TOKENIZER_MODEL=$TOP/LLaMa/llama_workarea/hf_llama_models/7B/$MODEL_SIZE/tokenizer.model > > export PYTHONPATH=""$PWD:$PWD/tools/checkpoint"" echo $PYTHONPATH > > python3 tools/checkpoint/util.py modeltype GPT loader llama2_hf saver megatron targettensorparallelsize ${TP} loaddir ${HF_FORMAT_DIR} savedir ${MEGATRON_FORMAT_DIR} tokenizermodel ${TOKENIZER_MODEL} > >  Loaded loader_llama2_hf as the loader. Loaded saver_megatron as the saver. Starting saver... Starting loader... Zarrbased strategies will not be registered because of missing packages Zarrbased strategies will not be registered because of missing packages File ""/home/aae14935wb/Share/Megatron/MegatronLM/megatron/core/models/gpt/gpt_model.py"", line 15, in from megatron.core.transformer.transformer_block import TransformerBlock File ""/home/aae14935wb/Share/Megatron/MegatronLM/megatron/core/transformer/transformer_block.py"", line 13, in from megatron.core.transformer.custom_layers.transformer_engine import TENorm File ""/home/aae14935wb/Share/Megatron/MegatronLM/megatron/core/transformer/custom_layers/transformer_engine.py"", line 71, in class TELinear(te.pytorch.Linear): AttributeError: module 'transformer_engine' has no attribute 'pytorch' >  > I got the same problem like you! All informations are same. Have you already solved this problem ?  I got the same problem. Have you solved it?",Marking as stale. No activity in 60 days.
Richie-yan,The implementation of pad bucket size causes the loss to be inconsistent with before,"After with the pad bucket size code, I noticed a difference in the training loss compared to before.  the code below caused a diff order in the opt params generated   The generated param_range_map, which is orderdependent, is used for sharding optimizer parameters. Therefore, when using the pad bucket code, the order of the resulting optimizer parameters differs from the previous one. which in turn led to an inconsistency in the order of gradient norm accumulation when using clip grad. Consequently, the final loss differed from what it was when the pad bucket size code was not used.   ",2024-01-10T02:20:42Z,,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/657,"Yes, we were aware of this when making the change; it is unfortunate the losses no longer match exactly, but the convergence curves should be similar (i.e., the convergence rate is unchanged). Let me know if you are seeing something different.","I haven't seen a change in the convergence rate for the time being, so for now, this loss difference is acceptable. Or, can it be changed to be consistent with the previous order?",That unfortunately will be difficult and is not a priority right now.,"I am going to close the issue, feel free to reopen if you run into any other issues."
mayank31398,[BUG] LM head weights get untied while training with overlap,LM head weights get untied during training even when they are supposed to be tied. This is happening when overlap parameters are set to true. cc:  ,2024-01-09T17:07:02Z,,closed,0,23,https://github.com/NVIDIA/Megatron-LM/issues/656,Can you provide an example script? And how are you inspecting the parameters?," I used 4 GPUs with a 3B param model. I added the following statements to the `train_step` function to print the tensors: ```python     if mpu.get_tensor_model_parallel_rank() == 0:         if mpu.is_pipeline_first_stage():             print(model[0].module.module.language_model.embedding.word_embeddings.weight)     torch.distributed.barrier()     if mpu.get_tensor_model_parallel_rank() == 0:         if mpu.is_pipeline_last_stage():             print(model[0].module.module.word_embeddings.weight)     torch.distributed.barrier()     print("""" * 50)     torch.distributed.barrier() ``` Note that this only happens when pipeline parallel is enabled. I have started seeing this after the introduction of overlapping backward pass in this repo. you can use this script to reproduce, also attaching logs ```shell  A100 80GB export NCCL_SOCKET_IFNAME=""ib,bond"" export NCCL_IB_CUDA_SUPPORT=1 export NCCL_IB_PCI_RELAXED_ORDERING=1 export UCX_IB_PCI_RELAXED_ORDERING=on export CUDA_DEVICE_ORDER=PCI_BUS_ID export NCCL_SOCKET_NTHREADS=2 export NCCL_NSOCKS_PERTHREAD=4 export CUDA_DEVICE_MAX_CONNECTIONS=1 MASTER_ADDR=$(echo ${LSB_MCPU_HOSTS}  cut d':' f1)1)) DISTRIBUTED_ARGS=""\ nproc_per_node $GPUS_PER_NODE \ nnodes $NNODES \ node_rank $NODE_RANK \ master_addr $MASTER_ADDR \ master_port $MASTER_PORT \ "" GPT_ARGS=""tensormodelparallelsize 1 \ pipelinemodelparallelsize 2 \ numlayers 32 \ hiddensize 3072 \ numattentionheads 32 \ initmethodstd 0.01275 \ seqlength 2048 \ maxpositionembeddings 2048 \ attentiondropout 0.1 \ hiddendropout 0.1 \ microbatchsize 1 \ globalbatchsize 2 \ lr 0.0003 \ minlr 0.00003 \ trainiters 510000 \ lrdecayiters 510000 \ lrdecaystyle constant \ weightdecay .1 \ adambeta2 .95 \ clipgrad 1.0 \ bf16 \ useflashattn \ loginterval 10 \ saveinterval 2000 \ evalinterval 5000000000 \ evaliters 2 \ usedistributedoptimizer \ tokenizertype NullTokenizer \ save $CHECKPOINT_PATH \ load $CHECKPOINT_PATH \ datacachepath ./cache \ sequenceparallel \ distributedtimeoutminutes 120 \ finetune \ vocabsize 49152"" torchrun $DISTRIBUTED_ARGS \     pretrain_gpt.py \     $GPT_ARGS \     save $CHECKPOINT_PATH \     load $CHECKPOINT_PATH \     datapath /dataset/bluepile/g20bc_starcoder_tokens2_megatron/lang=Python ``` at the beginning (they match exactly), the tensors are: ```shell tensor([[ 0.0103, 0.0074,  0.0206,  ...,  0.0034, 0.0028, 0.0205],         [ 0.0076, 0.0069, 0.0001,  ...,  0.0129,  0.0171, 0.0015],         [0.0142, 0.0120, 0.0104,  ..., 0.0024,  0.0121, 0.0005],         ...,         [ 0.0126, 0.0068,  0.0016,  ..., 0.0097,  0.0049,  0.0047],         [ 0.0025,  0.0100, 0.0010,  ..., 0.0078, 0.0209, 0.0128],         [0.0222,  0.0206, 0.0101,  ..., 0.0168,  0.0177,  0.0025]],        device='cuda:1', dtype=torch.bfloat16, requires_grad=True) Parameter containing: tensor([[ 0.0103, 0.0074,  0.0206,  ...,  0.0034, 0.0028, 0.0205],         [ 0.0076, 0.0069, 0.0001,  ...,  0.0129,  0.0171, 0.0015],         [0.0142, 0.0120, 0.0104,  ..., 0.0024,  0.0121, 0.0005],         ...,         [ 0.0126, 0.0068,  0.0016,  ..., 0.0097,  0.0049,  0.0047],         [ 0.0025,  0.0100, 0.0010,  ..., 0.0078, 0.0209, 0.0128],         [0.0222,  0.0206, 0.0101,  ..., 0.0168,  0.0177,  0.0025]],        device='cuda:0', dtype=torch.bfloat16, requires_grad=True) Parameter containing: tensor([[ 0.0103, 0.0074,  0.0206,  ...,  0.0034, 0.0028, 0.0205],         [ 0.0076, 0.0069, 0.0001,  ...,  0.0129,  0.0171, 0.0015],         [0.0142, 0.0120, 0.0104,  ..., 0.0024,  0.0121, 0.0005],         ...,         [ 0.0126, 0.0068,  0.0016,  ..., 0.0097,  0.0049,  0.0047],         [ 0.0025,  0.0100, 0.0010,  ..., 0.0078, 0.0209, 0.0128],         [0.0222,  0.0206, 0.0101,  ..., 0.0168,  0.0177,  0.0025]],        device='cuda:3', dtype=torch.bfloat16, requires_grad=True) Parameter containing: tensor([[ 0.0103, 0.0074,  0.0206,  ...,  0.0034, 0.0028, 0.0205],         [ 0.0076, 0.0069, 0.0001,  ...,  0.0129,  0.0171, 0.0015],         [0.0142, 0.0120, 0.0104,  ..., 0.0024,  0.0121, 0.0005],         ...,         [ 0.0126, 0.0068,  0.0016,  ..., 0.0097,  0.0049,  0.0047],         [ 0.0025,  0.0100, 0.0010,  ..., 0.0078, 0.0209, 0.0128],         [0.0222,  0.0206, 0.0101,  ..., 0.0168,  0.0177,  0.0025]],        device='cuda:2', dtype=torch.bfloat16, requires_grad=True) ``` after 220 steps: ```shell Parameter containing: tensor([[ 0.0133, 0.0061,  0.0311,  ..., 0.0013,  0.0018, 0.0094],         [ 0.0045, 0.0047, 0.0002,  ...,  0.0128,  0.0019, 0.0061],         [0.0176, 0.0069, 0.0096,  ..., 0.0018, 0.0137, 0.0058],         ...,         [ 0.0095, 0.0048,  0.0002,  ..., 0.0094, 0.0166, 0.0002],         [0.0006,  0.0121, 0.0013,  ..., 0.0071, 0.0427, 0.0184],         [0.0261,  0.0259, 0.0095,  ..., 0.0173, 0.0046, 0.0029]],        device='cuda:0', dtype=torch.bfloat16, requires_grad=True)Parameter containing: tensor([[ 0.0133, 0.0061,  0.0311,  ..., 0.0013,  0.0018, 0.0094],         [ 0.0045, 0.0047, 0.0002,  ...,  0.0128,  0.0019, 0.0061],         [0.0176, 0.0069, 0.0096,  ..., 0.0018, 0.0137, 0.0058],         ...,         [ 0.0095, 0.0048,  0.0002,  ..., 0.0094, 0.0166, 0.0002],         [0.0006,  0.0121, 0.0013,  ..., 0.0071, 0.0427, 0.0184],         [0.0261,  0.0259, 0.0095,  ..., 0.0173, 0.0046, 0.0029]],        device='cuda:1', dtype=torch.bfloat16, requires_grad=True) Parameter containing: tensor([[ 1.6235e02, 7.6904e03,  3.1128e02,  ..., 2.7084e04,          1.8768e03, 1.2207e02],         [ 4.0588e03, 3.8605e03, 8.6308e05,  ...,  1.3123e02,           1.6022e03, 6.1340e03],         [1.7944e02, 5.2795e03, 8.8501e03,  ..., 8.0109e04,          1.5259e02, 5.7678e03],         ...,         [ 9.1553e03, 3.4790e03,  6.8283e04,  ..., 8.3618e03,          1.8677e02, 1.8501e04],         [1.0529e03,  1.3611e02, 8.6975e04,  ..., 6.1340e03,          4.4434e02, 1.8311e02],         [2.6611e02,  2.7344e02, 9.0332e03,  ..., 1.6602e02,          5.4932e03, 2.9449e03]], device='cuda:2', dtype=torch.bfloat16,        requires_grad=True)Parameter containing: tensor([[ 1.6235e02, 7.6904e03,  3.1128e02,  ..., 2.7084e04,          1.8768e03, 1.2207e02],         [ 4.0588e03, 3.8605e03, 8.6308e05,  ...,  1.3123e02,           1.6022e03, 6.1340e03],         [1.7944e02, 5.2795e03, 8.8501e03,  ..., 8.0109e04,          1.5259e02, 5.7678e03],         ...,         [ 9.1553e03, 3.4790e03,  6.8283e04,  ..., 8.3618e03,          1.8677e02, 1.8501e04],         [1.0529e03,  1.3611e02, 8.6975e04,  ..., 6.1340e03,          4.4434e02, 1.8311e02],         [2.6611e02,  2.7344e02, 9.0332e03,  ..., 1.6602e02,          5.4932e03, 2.9449e03]], device='cuda:3', dtype=torch.bfloat16,        requires_grad=True) ``` This is only happening when pipeline parallel is enabled. Looking forward to your reply ðŸ˜ƒ ","Will look into it, thanks. A few more questions:  What version of the codebase are you using?  I don't see you using `overlapgradreduce`, though the issue says ""while training with overlap""?  Could you see if you run into the same issue with this argument: https://github.com/NVIDIA/MegatronLM/blob/main/megatron/arguments.pyL917C25L917C58?", I saw issues regardless of `overlapgradreduce`. The above tensor logs are without overlap though.,Ack. Commit hash?,"latest commit. I did a fresh clone a few hours ago :). you can use this: `6140bf8deb6f944f3bf1b71e5341b9489b42217a`. Also, the issue doesn't happen when data parallel is off (case of 2 GPUs).",Sounds good. Will get back to you as soon as I can.,"Hey, did you get a chance to look into this?","Sorry, was traveling yesterday from India. Will get to this best cast next week, following week more likely.","Hey Deepak, following up on this any progress?","Haven't looked at this in earnest yet. Will look this week. Have you tried just turning off weight tying? This is what we do by default now, which is partially why this fell through the cracks.",The training is progressing fine. There are no issues with that. Its just that the parameter is not getting tied working correctly.,"I am able to reproduce (including the fact that tying works without data parallelism, but doesn't work with data parallelism). Will work on a fix. Thanks for pointing this out!","Also seems like the mismatch only happens with `usedistributedoptimizer`? If I remove this flag, things seem to work as expected? Is this your experience as well,  ?",Yeah I think it was working without distributed optimizer but I can't say for sure.,Hi  any updates on this one?,"Hi , I figured out the issue. The commit that introduced overlapping of reducescatter with the backward pass changed the order of the reducescatter of gradients across dataparallel replicas and the allreduce of the embedding gradients across the first and last pipeline stage (we used to do the embedding allreduce before the dataparallel reducescatter). Since the reducescatter now runs overlapped with the backward pass, we need the dataparallel reducescatter to run before the embedding allreduce, but this leads to issues if the embeddings in the first and last pipeline stages are not partitioned across dataparallel replicas for the distributed optimizer in the same way. This is a figure that illustrates this issue: !image I have a fix: assign the embedding in each pipeline stage to its own separate bucket to ensure that the partitioning strategy is the same across both pipeline stages. Still working on cleaning it up and then it will need to go through our internal review process before it makes its way to Github. Let me know if any of this doesn't make sense!","Yup, I had the same understanding. I think the explanation makes sense. Thanks a lot for this. I will wait for the fix :)","Fix is here: https://github.com/NVIDIA/MegatronLM/commit/db2040f7ebdda99c18125936376fe30119267e6b. Please let me know if it works for you too. Thanks, !","Hey, thanks I will take a look over this weekend.","Hi , are you still seeing this issue? If not, can I close this?","Hey , I am currently running really busy so haven't gotten a chance to try this. You can close this for now. I will test it sometime soon and reopen if there are issues.","Sounds good, let me know."
TriLoo,[BUG] Function IndexPutBackward0 returned an invalid gradient at index.,"**Describe the bug** Add dropout layer to tensor model parallel context, but assure different mp rank got same dropout mask, then dropout layer will raise this error:  **To Reproduce** Steps to reproduce the behavior. The easier it is to reproduce the faster it will get maintainer attention. **Expected behavior** Different mp rank within same mp group will got same dropout mask for input_parallel at line 186: !image **Stack trace/logs** !image **Environment (please complete the following information):**   MegatronLM commit ID: 2bc6cd307a11423928c675f741e79e03df23e721   PyTorch version: 2.0.1+cu117   CUDA version: 11.8   NCCL version: 8.9.0",2024-01-08T10:20:26Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/655,Marking as stale. No activity in 60 days.,Won't fix. Closing.
jkunchen,[QUESTION] Question about compute theoretical memory usage of weight and optimizer.,"Hi, I'm a bit uncertain about the computation of `num_bytes_per_parameter`. From what I understand in MixedPrecision training, there are typically 4 bytes allocated for storing fp16 gradients and parameters and an extra 12 bytes for the optimizer. However, in this script, it appears to be computed as 18 bytes. Could you help me understand this apparent discrepancy? https://github.com/NVIDIA/MegatronLM/blob/3b27d30eecc5b0857e6652d872666d9a141b36e3/megatron/theoretical_memory_usage.pyL62L64",2024-01-08T01:59:35Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/654,"2 bytes for model parameters used for fprop and bprop, 4 bytes for main parameters, 4 bytes for gradients, 8 bytes for Adam states. 2 + 4 + 4 + 8 = 18. The model parameters and gradients are *not* partitioned when using the distributed optimizer."
JimmyZhang12,Add is_first_microbatch arg,,2024-01-05T18:16:33Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/653,Marking as stale. No activity in 60 days.
wangxicoding,question about test_global_memory_buffer,"why `obtained_tensor` equal with `expected_tensor`. If the value of memory is random, I don't think it will always be equal. https://github.com/NVIDIA/MegatronLM/blob/2bc6cd307a11423928c675f741e79e03df23e721/tests/unit_tests/test_utils.pyL13L17",2024-01-05T15:03:16Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/652,Marking as stale. No activity in 60 days.
workingloong,[ENHANCEMENT] Asynchronously save the checkpoint to storage with minimizing the paused time of training.,"**Is your feature request related to a problem? Please describe.** Fault often occurs during the training of a large LLM.  To recover the training, users usually periodically save checkpoints. However,   the time to save a checkpoint to storage needs minutes. Highfrequency checkpoints will waste an amount of time on GPUs. If using lowfrequency checkpoints, the latest checkpoint is far away from the training break point, which also wastes an amount of iterations.  It is important to shorten the checkpoint time to improve the goodput of training. **Describe the solution you'd like** We can synchronously copy the state dict into the shared memory of training processes and the main process of `torchrun`. The main process can asynchronously save the state dict from the shared memory to storage without any interference of training. **Describe alternatives you've considered** A clear and concise description of any alternative solutions or features you've considered. **Proposed implementation** We have implemented asynchronous persistence in DLRover Flash Checkpoint.  The APIs to support MegatronLM is same as the `save_checkpoint` and `load_checkpoint` like ```python from dlrover.trainer.torch.flash_checkpoint.megatron import save_checkpoint from dlrover.trainer.torch.flash_checkpoint.megatron import load_checkpoint from dlrover.trainer.torch.flash_checkpoint.megatron import StorageType if args.save and iteration % save_memory_interval == 0:     save_checkpoint(iteration, model, optimizer,                     opt_param_scheduler, storage_type=StorageType.MEMORY,) ``` We are happy to contribute the asynchronous persistence to the MegatronLM. **Additional context** We have conducted experiments to test the performance of asynchronous persistence with GPT21.5B on 2 * A100, the time to pause training decreases to 2.1s from 11s to save checkpoint to MVMe SSD. ",2024-01-05T02:48:19Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/651,Marking as stale. No activity in 60 days.,Feature is available in latest version. Closing the ticket.
TaekyungHeo,[BUG] Docker Build Fails at `pip install megatron-core==0.4.0`,"**Describe the bug** There is an error in building the Docker image for a project dependent on MegatronLM (https://github.com/NVIDIA/NeMoMegatronLauncher). The build process gets stuck during the package installation phase, specifically at `pip install megatroncore==0.4.0`. **To Reproduce** Steps to reproduce the behavior: 1. Clone NeMoMegatronLauncher ``` $ git clone recursesubmodules https://github.com/NVIDIA/NeMoMegatronLauncher.git ``` 2. Build a docker image and observe the failure during the `pip install megatroncore==0.4.0` step. ``` $ cd NeMoMegatronLauncher $ docker build . ``` **Expected behavior** The Docker build should proceed without errors and successfully install all required packages, including `megatroncore==0.4.0`. **Stack trace/logs** ``` $ pip install megatroncore==0.4.0 Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com Collecting megatroncore==0.4.0   Downloading megatron_core0.4.0.tar.gz (154 kB)       154 kB 15.1 MB/s    Installing build dependencies ... done   WARNING: Missing build requirements in pyproject.toml for megatroncore==0.4.0 from https://files.pythonhosted.org/packages/fd/b9/e85da25f4de43dad70d6fd1c21b88db085f471d5348c51cce05dc9e4b0ef/megatron_core0.4.0.tar.gzsha256=bb2cd1f4c5746b31a8b4abd676820ddceec272f002873801a519dbbf1352d8ef.   WARNING: The project does not specify a build backend, and pip cannot fall back to setuptools without 'wheel'.   Getting requirements to build wheel ... error   ERROR: Command errored out with exit status 1:    command: /usr/bin/python /usr/local/lib/python3.8/distpackages/pip/_vendor/pep517/in_process/_in_process.py get_requires_for_build_wheel /tmp/tmp1gpfom00        cwd: /tmp/pipinstall_fo_70fz/megatroncore_b051d2fdd6d846beb6e755037509a79a   Complete output (18 lines):   Traceback (most recent call last):     File ""/usr/local/lib/python3.8/distpackages/pip/_vendor/pep517/in_process/_in_process.py"", line 349, in        main()     File ""/usr/local/lib/python3.8/distpackages/pip/_vendor/pep517/in_process/_in_process.py"", line 331, in main       json_out['return_val'] = hook(**hook_input['kwargs'])     File ""/usr/local/lib/python3.8/distpackages/pip/_vendor/pep517/in_process/_in_process.py"", line 117, in get_requires_for_build_wheel       return hook(config_settings)     File ""/usr/local/lib/python3.8/distpackages/setuptools/build_meta.py"", line 338, in get_requires_for_build_wheel       return self._get_build_requires(config_settings, requirements=['wheel'])     File ""/usr/local/lib/python3.8/distpackages/setuptools/build_meta.py"", line 320, in _get_build_requires       self.run_setup()     File ""/usr/local/lib/python3.8/distpackages/setuptools/build_meta.py"", line 483, in run_setup       super(_BuildMetaLegacyBackend,     File ""/usr/local/lib/python3.8/distpackages/setuptools/build_meta.py"", line 335, in run_setup       exec(code, locals())     File """", line 52, in      File """", line 45, in req_file   FileNotFoundError: [Errno 2] No such file or directory: 'megatron/core/requirements.txt'    WARNING: Discarding https://files.pythonhosted.org/packages/fd/b9/e85da25f4de43dad70d6fd1c21b88db085f471d5348c51cce05dc9e4b0ef/megatron_core0.4.0.tar.gzsha256=bb2cd1f4c5746b31a8b4abd676820ddceec272f002873801a519dbbf1352d8ef (from https://pypi.org/simple/megatroncore/). Command errored out with exit status 1: /usr/bin/python /usr/local/lib/python3.8/distpackages/pip/_vendor/pep517/in_process/_in_process.py get_requires_for_build_wheel /tmp/tmp1gpfom00 Check the logs for full command output. ERROR: Could not find a version that satisfies the requirement megatroncore==0.4.0 (from versions: 0.1.0, 0.2.0, 0.3.0, 0.4.0) ERROR: No matching distribution found for megatroncore==0.4.0 WARNING: You are using pip version 21.2.4; however, version 23.3.2 is available. You should consider upgrading via the '/usr/bin/python m pip install upgrade pip' command. ``` **Environment:**  MegatronLM commit ID: Unknown (depends on the Dockerfile configuration)  PyTorch version: Unknown (depends on the Dockerfile configuration)  CUDA version: Unknown (depends on the Dockerfile configuration, if applicable)  NCCL version: Unknown (depends on the Dockerfile configuration, if applicable) **Proposed fix** Currently, I do not have a proposed fix. I am hoping the maintainers can provide insight or a fix for this issue. **Additional context**  The issue appears to be specific to `megatron_core` version 0.4.0. Notably, when using `megatron_core` version 0.3.0, the build process completes successfully. This suggests that the problem may be isolated to changes introduced in version 0.4.0 of `megatron_core`.",2024-01-04T18:49:34Z,stale,closed,0,8,https://github.com/NVIDIA/Megatron-LM/issues/650,Related issue: https://github.com/NVIDIA/NeMoMegatronLauncher/issues/184,"I believe it could be fixed by: ``` diff git a/MANIFEST.in b/MANIFEST.in new file mode 100644 index 00000000..b3356b76  /dev/null +++ b/MANIFEST.in @@ 0,0 +1 @@ +include megatron/core/requirements.txt``` `requirements.txt` is not packed to the source distribution package. ","Not that this is a valid solution, however I was facing the same issue while installing nemo_toolkit[all] and I reverted the version of the package to the previous one nemotoolkit==1.21.0 released in 2023 as opposed to the current one which released in Jan 2024.  ",Encountered the same issue as the OP  indeed it seems this repo is missing a `MANIFEST.in` file that tells distutils which extra noncode files need to be included in the PyPI release.  https://setuptools.pypa.io/en/latest/userguide/miscellaneous.html,  I think is has been fixed https://github.com/NVIDIA/MegatronLM/commit/f6b0f4e41bf762676a2f01c944c733a8af06b7db. `NeMoMegatronLauncher` seems to have other problems as well.,"Thanks  youâ€™re right, I missed that.",Marking as stale. No activity in 60 days.,Closing bug. Will track the issue on the new ticket.
matrixssy,[ENHANCEMENT] Do you have a plan that supports Mixtral 8x7B?,,2024-01-04T01:43:44Z,stale,closed,4,7,https://github.com/NVIDIA/Megatron-LM/issues/649,The same question.,"> The same question. I am working for it, but I am not sure if it will be accepted.", any progress ? thank you.,see https://github.com/NVIDIA/MegatronLM/pull/667,"Hi, Please refer to this script for MoE/Mixtral training.","> Hi, Please refer to this script for MoE/Mixtral training. Great! But I would like to know how to convert Hugging Face (HF) weights to Megatron (MG) format, and if it's possible to convert them back after training?",Marking as stale. No activity in 60 days.
vksastry,Vit Classify,"I am trying to run examples/pretrain_vision_classify.sh. I am wondering if tensor parallelism and pipeline parallelism are supported for vision models ? In other words, can I use tensormodelparallelsize or pipelinemodelparallelsize greater than 1? ",2023-12-30T00:58:35Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/648,Marking as stale. No activity in 60 days.
young-chao,"[BUG]When loading from checkpoint to continue training, it will hang during the validationâ€˜s forward.","**Describe the bug** When I load the model from checkpoint and continue training, it always hangs during the validation process. It should be noted that it only hangs when the ""noloadoptim"" and ""noloadrng"" parameters are not used.  **To Reproduce** Below is my startup scriptï¼š ` torchrun nnodes=2 nproc_per_node=8 node_rank=$node_rank \     master_addr=10.90.0.2 master_port=12375  \     anypath/MegatronLM/pretrain_gpt.py \     usecheckpointargs \     exitonmissingcheckpoint \     load=anypath/megatron/13B_10B \     save=anypath/megatron/13B_10B \     traindatapath=anypath/CC_megatron_merged \     validdatapath=anypath/CC2_text_document \     tensormodelparallelsize=2 \     pipelinemodelparallelsize=4 \     recomputeactivations \     usedistributedoptimizer \     sequenceparallel \     useflashattn \     overlapgradreduce \     bf16 \     numlayers=40 \     hiddensize=5120 \     numattentionheads=40 \     ffnhiddensize=13824 \     attentiondropout=0.0 \     hiddendropout=0.0 \     swiglu \     normalization RMSNorm \     userotarypositionembeddings \     nopositionembedding \     nomaskedsoftmaxfusion \     untieembeddingsandoutputweights \     disablebiaslinear \     tokenizertype=Llama2Tokenizer \     tokenizermodel=anypath/tokenizer.model \     trainiters=160000 \     microbatchsize=1 \     globalbatchsize=32 \     seqlength=2048 \     lr=3e4 \     minlr=0 \     lrdecaystyle=linear \     adambeta1=0.9 \     adambeta2=0.95 \     weightdecay=0.1 \     clipgrad=1.0 \     initmethodstd=0.006 \     lrdecayiters=157709 \     lrwarmupiters=15771 \     maxpositionembeddings=4096 \     loginterval=1 \     evalinterval=5 \     evaliters=1 \     saveinterval=1000 \     tensorboardqueuesize=1 \     tensorboarddir=anypath \     logtimerstotensorboard \     logbatchsizetotensorboard \     logvalidationppltotensorboard \     numworkers=8 \     seed=1234 ` **Expected behavior** When the program loads from checkpoint and continues training for 5 steps, it will hang until it times out. **Environment (please complete the following information):** I used the image from NGC, and upgraded nccl version.   MegatronLM 2bc6cd3   PyTorch 2.1.0a0+b5021ba   CUDA 12.1.1   NCCL 2.18.6 **Additional context** It is worth noting that this error is sporadic. When training on two machines, training can continue normally in about half of the cases. However, when training on 64 machines, it will hang after trying many times.",2023-12-28T07:27:29Z,,closed,0,8,https://github.com/NVIDIA/Megatron-LM/issues/647,What I found was that the training got stuck in the first or last stage of PP where it was trying to get the data. !image,Marking as stale. No activity in 60 days.,ä½ å¥½ï¼Œä½ çš„é‚®ä»¶å·²æ”¶åˆ°ï¼Œè°¢è°¢ï¼,Marking as stale. No activity in 60 days.,ä½ å¥½ï¼Œä½ çš„é‚®ä»¶å·²æ”¶åˆ°ï¼Œè°¢è°¢ï¼,Marking as stale. No activity in 60 days.,ä½ å¥½ï¼Œä½ çš„é‚®ä»¶å·²æ”¶åˆ°ï¼Œè°¢è°¢ï¼,ä½ å¥½ï¼Œä½ çš„é‚®ä»¶å·²æ”¶åˆ°ï¼Œè°¢è°¢ï¼
SVEEu,add assert for overlap_param_gather,,2023-12-26T12:26:10Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/646
itsliupeng,Dynamic dp size,Pad the optimizer state (param/exp_avg/exp_avg_sq in `distrib_optim.pt`) to meet different data parallelism sizes during loading.,2023-12-26T09:30:50Z,stale,open,0,6,https://github.com/NVIDIA/Megatron-LM/issues/645,Thanks for the contribution; we already have an internal MR that will fix this. It should be pushed some time this week.,We just merged it: https://github.com/NVIDIA/MegatronLM/commit/66bba1d66540f345eeffb947ec69d69408d90f55.,"> We just merged it: 66bba1d. Thanks for the information. I  find this commit introduced a new `per_bucket_numel_unpadded` field in `distrib_optim.pt` file , which results in incompatibility with existing checkpoint. Additionally, both truncation and padding processes can be efficiently executed in a single line of code by utilizing the F.pad function, with the padding_size parameter set to either a positive or negative value.","`numel_unpadded` not in old checkpoints should not result in issues though since we check for its existence in the checkpoint under an if condition. `F.pad` looked interesting, but I need to test it and I haven't gotten a chance to since I am busy with other stuff and had already implemented this version. Will put it on the backburner and come back to it when I get time.",Marking as stale. No activity in 60 days.,Marking as stale. No activity in 60 days.
wangxicoding,fix param_buffer peak memory may be OOM," Problem description As shown in Figure 2, using `torch.tensor(storage)` will increase the peak memory. `torch.tensor(storage)` may create a new tensor first, and then replace the storage of the new tensor with the incoming storage, resulting in a problem of high peak memory, and may cause OOM. (For example, if we have 16GB param and 32GB grad, and when obtaining the param_buffer 32GB storage is needed, 16+32+32 = 80GB, it may cause OOM)  Solution As shown in Figure 1, we can use `tensor.view(dtype)` to share tensor, there will be no problem with high peak memory. In addition, as shown in Figure 3, `.view()` does not ignore views/slices, so there is no need to calculate offset. !ubFd36nrJe",2023-12-26T08:56:27Z,,closed,1,5,https://github.com/NVIDIA/Megatron-LM/issues/644,Did you try running this with `usedistributedoptimizer overlapgradreduce`?,"> Did you try running this with `usedistributedoptimizer overlapgradreduce`? I have tested `len(grad_buffer.buckets) > 1`, as expected. Megatron doesn't have CI on GitHub, maybe you can use the internal CI to test it again.","Hi  there are codes in megatron where you can make variable inplace version (loss layer for example), which could save you around activation 20 GB. Besides tensor view, you can reuse some tensor's memory inplace. Wish this  helps you.","Thanks a lot for this patch,  ! We merged it internally, and it should be pushed to Github soon!","> Thanks a lot for this patch,  ! We merged it internally, and it should be pushed to Github soon! ok, thanks"
MoFHeka,"[BUG] RotaryEmbedding return float32 emb, make apply_rotary_pos_emb cast half to float32.",Main branch. It will make TransformerEngine assert fails.,2023-12-26T03:36:22Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/643,Marking as stale. No activity in 60 days.
RUAN-ZX,fix: solve problem of accessing pad eos bos property of _Llama2Tokenizer,fix: solve problem of accessing pad eos bos property of _Llama2Tokenizer See details in CC([BUG] Get 'self._eos_id' error when accessing LlamaTokenizer.).,2023-12-25T03:12:25Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/642,Marking as stale. No activity in 60 days.
RUAN-ZX,[BUG] Get 'self._eos_id' error when accessing LlamaTokenizer.,"**Describe the bug** Failed to access property `pad`, `eos`, `bos` of LlamaTokenizer due to incorrect initialization of `self._pad_id`, `self._eos_id`, `self._bos_id`. ``` AttributeError: '_Llama2Tokenizer' object has no attribute '_eos_id' ``` **To Reproduce** tokenizertype=Llama2Tokenizer **Expected behavior** Not produce error and successfully return `_eos_id` of LlamaTokenizer. **Environment (please complete the following information):**   MegatronLM latest version from main branch   PyTorch 2.1.0 **Proposed fix** Here are my pr CC(fix: solve problem of accessing pad eos bos property of _Llama2Tokenizer). **Additional context** As you can see, `_GPTSentencePieceTokenizer` initalize `self._pad_id`, `self._eos_id`, `self._bos_id` and we manage to access property like `pad`, `eos`, `bos` through its parent class `_SentencePieceTokenizer`. Hence, I assume that `LlamaTokenizer` has the same logic as `_GPTSentencePieceTokenizer` since they both are the inherited from `_SentencePieceTokenizer`.",2023-12-25T03:10:06Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/641,Marking as stale. No activity in 60 days.,Marking as stale. No activity in 60 days.
Rememberz,"[QUESTION] During pre training, the first global batch can run, but the second global batch reports the error ""cuda of memory"".","Hello, I have encountered an issue: during pre training, the first global batch can run, but the second global batch reports the error ""cuda of memory"". Specifically, I tested a set of runnable configuration: 4 nodes with 16 GPUs, 4 GPUs/node, tp=1, pp=8, dp=2, global_batch_size=1024, micro_batch_size=1, and the micro_batch_num = 1024/2=512, 1F1B strategy. I have summarized the memory changes of the GPU15 in the last stage of PP (stable 1F1B stage)ï¼š !tp1pp8dp2_memory_change It can be seen that in the first minibatch stage, forward propagation increases the memory of 3G, while backward propagation reduces the memory of 3G, with the highest memory usage of 17.5G. In the second minibatch stage, the first forward propagation increased **10G** of memory, but the first backward propagation only reduced 3G of memory, with the highest memory usage of 24.5G. Subsequently, both forward and backward directions steadily increased by 3G and decreased by 3G, resulting in an inexplicable increase of 7G of memory. May I ask if anyone knows the reason for this abnormal increase in GPU memory footprint? How to solve this problem?",2023-12-23T03:02:34Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/640,"It is expected: the optimizer state is allocated lazily the first time the optimizer runs. You can try to measure the footprint after the optimizer step, and you should see a jump the first time the optimizer step runs.",Marking as stale. No activity in 60 days.
apupneja,[BUG] Building the db for retro needs args.json,"**Describe the bug** On the second step for building the db for Retro, the `dbbuild` should produce the args.json, however, I get the following error on running `bash tools/retro/examples/preprocess_data.sh dbbuild `: **To Reproduce** Run `bash tools/retro/examples/preprocess_data.sh dbbuild ` **Expected behavior** This step should have created the file, however, it seems that an assert statement in the validation step requires that it is created first. **Stack trace/logs** ``` Traceback (most recent call last):   File ""/workspace/MegatronLM/tools/retro/main.py"", line 192, in      initialize_megatron(extra_args_provider=add_retro_args)   File ""/workspace/MegatronLM/megatron/initialize.py"", line 50, in initialize_megatron     validate_args(args, args_defaults)   File ""/workspace/MegatronLM/megatron/arguments.py"", line 371, in validate_args     assert os.path.exists(retro_args_path), ""retro workdir missing args.json"" AssertionError: retro workdir missing args.json ``` **Environment:** I used the docker environment provided, along with the latest commit of Megatron=LM",2023-12-22T12:08:49Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/639,Same problem here. Could anyone please tackle this issue?,This file is provided in https://drive.google.com/file/d/121GqAdMvf8bJEBZRtSD4uhWSRWgI3s/view?pli=1,Marking as stale. No activity in 60 days.
THULiusj,[QUESTION] How to use nsys profile for Megatron,"**Your question** I tried to profile the application. I use **nsys profile** command, the application can successfully train and evaluate, but never returned (just hang there), which made the profiling file cannot be generated.",2023-12-20T03:49:11Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/638,"Here is a simple example: ``` NSYS_ITER={NSYS_ITER:1} NSYS_ITER_RANGE={NSYS_ITER_RANGE:2} if (( $NSYS_ITER >= 0 )); then     mkdir p ${LOG_DIR}/nsys_reports     NSYS_CMD=""nsys profile forceoverwrite true o ${LOG_DIR}nsys_reports/${JOB_NAME}${RANK_ID} capturerange=cudaProfilerApi""     NSYS_ARGS=""         profile profilestepstart $NSYS_ITER profilestepend $((NSYS_ITER + NSYS_ITER_RANGE))     "" else     NSYS_CMD=""""     NSYS_ARGS="""" fi $NSYS_CMD torchrun $DISTRIBUTED_ARGS pretrain_gpt.py \     $GPT_ARGS \     $MOE_ARGS \     $DATA_ARGS \     $OUTPUT_ARGS \     $LOGGING_ARGS \     $NSYS_ARGS \     $EXTRA_ARGS \     $RECOMPUTE_ARGS \     distributedbackend nccl \     &> ${LOG_DIR}/log/{RANK_ID}.log ``` simply export NSYS_ITER to number greater than 1. Good luck  .",Marking as stale. No activity in 60 days.
THULiusj,[QUESTION] Report OOM after 10 iterations when running pretrain_gpt_distributed_with_mp.sh,"**Your question** I run pretrain_gpt_distributed_with_mp.sh script, and reported OOM after 10 iterations. Anyone knows why it suddenly reported OOM? detailed configuratioins as below: ``` GPT30B model num_nodes=2 num_gpu_per_node=8 mini_batch_size=2 global_batch_size=2048 enable fp8 transformer engine enable recomputeactivations enable distributed optimizer enable flash attention ``` ``` Apex=22.04 TransformerEngine=0.12 Megatron=latest commit cuda=12.2 NCCL=2.18.5 ```",2023-12-20T03:47:48Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/637,It probably ran OOM as soon as it tried to allocate optimizer state. Can you paste the relevant output log lines here? (after it sets up the data loader),Marking as stale. No activity in 60 days.
liqing9399,[QUESTION] The impact of CUDA_DEVICE_MAX_CONNECTIONS on the overlap of computation and communicationï¼Ÿ,"Why can the overlap between computation and data parallel (DP) communicationï¼ˆreduce_scatterï¼‰ still be achieved when CUDA_DEVICE_MAX_CONNECTIONS is set to 1?Why is it that some parts are overlap, while others are blocked? ![Uploading blocking compute.pngâ€¦]()",2023-12-19T03:48:10Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/636,![Uploading blocking compute.pngâ€¦](),Marking as stale. No activity in 60 days.
hwdef,Update log_string format in training.py,Unified log output format,2023-12-17T16:03:16Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/635
lingjiew93,Any plan on supportting LLM pretraining with FSDP?,"Hi, Want to konw that do you have any plan on fsdp to replace 3d parallel? Since there are zero3 from microsoft and fsdp from meta which all target on data parallel. And I saw there is fsdp in NEMO but not in LLM pretraing scenario.  Not sure do you have comparison on 3d and fsdp and future plan? Thanks",2023-12-16T06:05:02Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/634,Closed as I saw it will be supported in Nemo,"> Closed as I saw it will be supported in Nemo Is there a link available? Regarding what you mentioned about ""will be supported in Nemo.""",https://docs.nvidia.com/nemoframework/userguide/latest/nemotoolkit/features/parallelisms.htmlfullyshareddataparallelismfsdp
akoumpa,Sliding window attention/akoumparouli,Pending https://github.com/NVIDIA/TransformerEngine/pull/551 ,2023-12-15T17:04:06Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/633
ENg-122,"Error when loading checkpoint: no file common.pt, and my container does not exist this file",**Your question** Ask a clear and concise question about MegatronLM.,2023-12-15T06:24:50Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/632,æˆ‘æ²¡æœ‰æ­£ç¡®ä¿å­˜checkpointæ–‡ä»¶
akoumpa,Small bugfix for rotary embeddings.,"The input vector needs to be split into odd/even parts before applying transformations, since that's the format the transformations expect the input in. **Longer explanation:** From https://kexue.fm/archives/8265   Rewriting the first two rows from the above image (naming â€œAâ€ the output vector): A[0] = q[0] * cos mth[0]  q[1] * sin mth[0] A[1] = q[1] * cos mth[0] + q[0] * sin mth[0] Which is the same if q and mth are viewed as complex numbers qâ€™ = (q[0], i q[1]) mthâ€™ = (cos mth[0], i sin mth[0]) qâ€™ * mth* = (q[0] cos mth[0] + (i^2) * q[1] sin mth[0], i (q[1] * cos mth[0] + q[0] * sin mth[0] qâ€™ * mth* = (q[0] cos mth[0]  1 * q[1] sin mth[0], i (q[1] * cos mth[0] + q[0] * sin mth[0])) A[0] = Real[qâ€™ * mthâ€™] = q[0] cos mth[0]  1 * q[1] sin mth[0] A[1] = Im[qâ€™ * mthâ€™] = q[1] * cos mth[0] + q[0] * sin mth[0] But megatron does A[0] = q[0] * cos mth[0]  q[d/21] * sin mth[0] A[1] = q[1] * cos mth[1]  q[d/2] * sin mth[1] However, if q is also split in odd/even parts then it works because q[d/21] corresponds to q[1], Update: it's better to perform these transposes on the weight matrices themselves, to save runtime costs. As a result this PR is no longer required. **Compatibility:** Any model trained with the current Megatron RoPe implementation, should be fine with what's merged, and so merging this PR would break compatibility with these models, but enable compatibility with models trained using the correct implementation (e.g. Mistral).",2023-12-14T02:20:30Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/631
stu1130,[BUG] AttributeError: 'NoneType' object has no attribute 'requires_grad' on 'full' activation recomputation,"**Describe the bug** When we enable full activation recomputation, we ran into the following error ``` ... File ""/workspace/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 253, in backward_step     torch.autograd.backward(output_tensor[0], grad_tensors=output_tensor_grad[0])   File ""/usr/local/lib/python3.10/distpackages/torch/autograd/__init__.py"", line 251, in backward     Variable._execution_engine.run_backward(   Calls into the C++ engine to run the backward pass   File ""/usr/local/lib/python3.10/distpackages/torch/autograd/function.py"", line 288, in apply     return user_fn(self, *args)   File ""/workspace/src/MegatronLM/megatron/core/tensor_parallel/random.py"", line 261, in backward     torch.autograd.backward(outputs, args)   File ""/usr/local/lib/python3.10/distpackages/torch/autograd/__init__.py"", line 244, in backward     grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)   File ""/usr/local/lib/python3.10/distpackages/torch/autograd/__init__.py"", line 115, in _make_grads     if out.requires_grad: AttributeError: 'NoneType' object has no attribute 'requires_grad' ``` We are using MegatronLM branch r0.4.0/TransformerEngine branch release_v1.1 and trigger the training job from NeMo (on latest main branch). **To Reproduce** Should be able to reproduce the error using megatroncore r0.4.0 and latest NeMo branch on arbitrary size of GPT model. **Expected behavior** Succeeds to run the training job. **Stack trace/logs** Here is the full logs ``` Traceback (most recent call last):   File ""/workspace/src/src/main_pretrain_local_cp.py"", line 31, in      main()   File ""/workspace/src/NeMo/nemo/core/config/hydra_runner.py"", line 126, in wrapper     _run_hydra(   File ""/usr/local/lib/python3.10/distpackages/hydra/_internal/utils.py"", line 394, in _run_hydra     _run_app(   File ""/usr/local/lib/python3.10/distpackages/hydra/_internal/utils.py"", line 457, in _run_app     run_and_report(   File ""/usr/local/lib/python3.10/distpackages/hydra/_internal/utils.py"", line 223, in run_and_report     raise ex   File ""/usr/local/lib/python3.10/distpackages/hydra/_internal/utils.py"", line 220, in run_and_report     return func()   File ""/usr/local/lib/python3.10/distpackages/hydra/_internal/utils.py"", line 458, in      lambda: hydra.run(   File ""/usr/local/lib/python3.10/distpackages/hydra/_internal/hydra.py"", line 132, in run     _ = ret.return_value   File ""/usr/local/lib/python3.10/distpackages/hydra/core/utils.py"", line 260, in return_value     raise self._return_value   File ""/usr/local/lib/python3.10/distpackages/hydra/core/utils.py"", line 186, in run_job     ret.return_value = task_function(task_cfg)   File ""/workspace/src/src/main_pretrain_local_cp.py"", line 27, in main     trainer.fit(model)   File ""/usr/local/lib/python3.10/distpackages/pytorch_lightning/trainer/trainer.py"", line 532, in fit     call._call_and_handle_interrupt(   File ""/usr/local/lib/python3.10/distpackages/pytorch_lightning/trainer/call.py"", line 43, in _call_and_handle_interrupt     return trainer_fn(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/pytorch_lightning/trainer/trainer.py"", line 571, in _fit_impl     self._run(model, ckpt_path=ckpt_path)   File ""/usr/local/lib/python3.10/distpackages/pytorch_lightning/trainer/trainer.py"", line 980, in _run     results = self._run_stage()   File ""/usr/local/lib/python3.10/distpackages/pytorch_lightning/trainer/trainer.py"", line 1023, in _run_stage     self.fit_loop.run()   File ""/usr/local/lib/python3.10/distpackages/pytorch_lightning/loops/fit_loop.py"", line 202, in run     self.advance()   File ""/usr/local/lib/python3.10/distpackages/pytorch_lightning/loops/fit_loop.py"", line 355, in advance     self.epoch_loop.run(self._data_fetcher)   File ""/usr/local/lib/python3.10/distpackages/pytorch_lightning/loops/training_epoch_loop.py"", line 133, in run     self.advance(data_fetcher)   File ""/usr/local/lib/python3.10/distpackages/pytorch_lightning/loops/training_epoch_loop.py"", line 219, in advance     batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)   File ""/usr/local/lib/python3.10/distpackages/pytorch_lightning/loops/optimization/automatic.py"", line 188, in run     self._optimizer_step(kwargs.get(""batch_idx"", 0), closure)   File ""/usr/local/lib/python3.10/distpackages/pytorch_lightning/loops/optimization/automatic.py"", line 266, in _optimizer_step     call._call_lightning_module_hook(   File ""/usr/local/lib/python3.10/distpackages/pytorch_lightning/trainer/call.py"", line 145, in _call_lightning_module_hook     output = fn(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/pytorch_lightning/core/module.py"", line 1270, in optimizer_step     optimizer.step(closure=optimizer_closure)   File ""/usr/local/lib/python3.10/distpackages/pytorch_lightning/core/optimizer.py"", line 161, in step     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/pytorch_lightning/strategies/strategy.py"", line 231, in optimizer_step     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/pytorch_lightning/plugins/precision/amp.py"", line 73, in optimizer_step     return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/pytorch_lightning/plugins/precision/precision_plugin.py"", line 116, in optimizer_step     return optimizer.step(closure=closure, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/optim/lr_scheduler.py"", line 68, in wrapper     return wrapped(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/optim/optimizer.py"", line 373, in wrapper     out = func(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/apex/optimizers/fused_adam.py"", line 140, in step     loss = closure()   File ""/usr/local/lib/python3.10/distpackages/pytorch_lightning/plugins/precision/precision_plugin.py"", line 103, in _wrap_closure     closure_result = closure()   File ""/usr/local/lib/python3.10/distpackages/pytorch_lightning/loops/optimization/automatic.py"", line 142, in __call__     self._result = self.closure(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/utils/_contextlib.py"", line 115, in decorate_context     return func(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/pytorch_lightning/loops/optimization/automatic.py"", line 128, in closure     step_output = self._step_fn()   File ""/usr/local/lib/python3.10/distpackages/pytorch_lightning/loops/optimization/automatic.py"", line 315, in _training_step     training_step_output = call._call_strategy_hook(trainer, ""training_step"", *kwargs.values())   File ""/usr/local/lib/python3.10/distpackages/pytorch_lightning/trainer/call.py"", line 293, in _call_strategy_hook     output = fn(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/pytorch_lightning/strategies/fsdp.py"", line 327, in training_step     return self.model(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/pytorch_lightning/overrides/base.py"", line 90, in forward     output = self._forward_module.training_step(*inputs, **kwargs)   File ""/workspace/src/NeMo/nemo/utils/model_utils.py"", line 381, in wrap_training_step     output_dict = wrapped(*args, **kwargs)   File ""/workspace/src/NeMo/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py"", line 609, in training_step     loss_mean = self.fwd_bwd_step(dataloader_iter, batch_idx, False)   File ""/workspace/src/NeMo/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py"", line 511, in fwd_bwd_step     losses_reduced_per_micro_batch = fwd_bwd_function(   File ""/workspace/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 354, in forward_backward_no_pipelining     backward_step(input_tensor, output_tensor, output_tensor_grad, model_type, config)   File ""/workspace/src/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 253, in backward_step     torch.autograd.backward(output_tensor[0], grad_tensors=output_tensor_grad[0])   File ""/usr/local/lib/python3.10/distpackages/torch/autograd/__init__.py"", line 251, in backward     Variable._execution_engine.run_backward(   Calls into the C++ engine to run the backward pass   File ""/usr/local/lib/python3.10/distpackages/torch/autograd/function.py"", line 288, in apply     return user_fn(self, *args)   File ""/workspace/src/MegatronLM/megatron/core/tensor_parallel/random.py"", line 261, in backward     torch.autograd.backward(outputs, args)   File ""/usr/local/lib/python3.10/distpackages/torch/autograd/__init__.py"", line 244, in backward     grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)   File ""/usr/local/lib/python3.10/distpackages/torch/autograd/__init__.py"", line 115, in _make_grads     if out.requires_grad: AttributeError: 'NoneType' object has no attribute 'requires_grad' ``` **Environment (please complete the following information):**   MegatronLM commit ID: branch r0.4.0   PyTorch version: NGC pytorch 23.10py3 **Additional context** Let me know if having the issue to reproduce the issue. Thanks!",2023-12-14T01:24:55Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/630,How did you fix this issue? 
Victarry,[BUG] Redundant cast_transpose_kernel with fp8 enabled in MCore,"**Describe the bug** For each microbatch between `optimizer.step()` where the weights don't change, MCore model will repeatedly cast the weight from bf16 to fp8.  **From nsys timeline, with this fixed, the perf can increase about 5%.** **To Reproduce** Using this NeMo config. **Expected behavior** The `cast_transpose_kernel` only need to be called for the first microbatch. **Environment (please complete the following information):**  Docker: `nvcr.io/eabignlp/gaparticipants/nemofwtraining:23.08.03` **Proposed fix** Pass `is_first_microbatch` into TEModule.  `is_first_microbatch` in forward params from TE  the forward call without `is_first_microbatch` in MCore **Additional context** Contact denliu.com for additional questions.",2023-12-13T03:23:34Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/629,Marking as stale. No activity in 60 days.
hwdef,"Optimization output log, add colons to iteration",Unified log output format,2023-12-12T08:29:55Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/628, Please take a look., Could you please review this?,Marking as stale. No activity in 60 days.
weixiao-huang,[Enhancement] fused kernels support torch ddp,See https://github.com/NVIDIA/MegatronLM/issues/626 This PR must be merged after https://github.com/NVIDIA/MegatronLM/pull/625,2023-12-11T13:54:34Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/627,Marking as stale. No activity in 60 days.
weixiao-huang,[ENHANCEMENT] Make test_fused_kernels work in torch ddp,"**Is your feature request related to a problem? Please describe.** When I run `torchrun nnodes 1 nproc_per_node 8 node_rank 0 master_addr 127.0.0.1 master_port 12345 m megatron.fused_kernels.tests.test_fused_kernels`, fused_kernels will only use `cuda:0` for data computation, which will cause test only run in gpu 0 instead of other gpu cards for matching the real run for using fused_kernels **Describe the solution you'd like** Add device argument in test scripts and make torch ddp work in `test_fused_kernels.py` **Describe alternatives you've considered** No **Proposed implementation** https://github.com/NVIDIA/MegatronLM/pull/627 **Additional context** No",2023-12-11T13:28:32Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/626,Marking as stale. No activity in 60 days.
weixiao-huang,[Bug] fix: make fused_kernels test work,Fixes https://github.com/NVIDIA/MegatronLM/issues/624,2023-12-11T13:21:57Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/625,Marking as stale. No activity in 60 days.
weixiao-huang,[BUG] test_fused_kernels could not pass,"**Describe the bug** `python3 m megatron.fused_kernels.tests.test_fused_kernels` got an error ```bash Traceback (most recent call last):   File ""/opt/conda/lib/python3.10/runpy.py"", line 196, in _run_module_as_main     return _run_code(code, main_globals, None,   File ""/opt/conda/lib/python3.10/runpy.py"", line 86, in _run_code     exec(code, run_globals)   File ""/root/github.com/NVIDIA/MegatronLM/megatron/fused_kernels/tests/test_fused_kernels.py"", line 380, in      load() TypeError: load() missing 1 required positional argument: 'args' ``` **To Reproduce** Just use `python3 m megatron.fused_kernels.tests.test_fused_kernels` can reproduce **Expected behavior** Test successfully **Stack trace/logs** If applicable, add the stack trace or logs from the time of the error. **Environment (please complete the following information):**   MegatronLM commit ID     23.05   PyTorch version     2.0.0+cu118   CUDA version     11.8   NCCL version **Proposed fix** https://github.com/NVIDIA/MegatronLM/pull/625 **Additional context** Add any other context about the problem here.",2023-12-11T13:11:29Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/624,Marking as stale. No activity in 60 days.
SeaOfOcean,[BUG] embedding table OOB check,"**Describe the bug** In this commit https://github.com/NVIDIA/MegatronLM/commit/b30f553ff7566da865b9c27b552da79d5f09cfed add embedding table OOB test ``` assert not torch.any(             (input_ = self.num_embeddings)         ), ""An input token is out of bounds of the embedding table"" ``` but I found that the Llama2Tokenizer has pad_id == 1, when we pad the data with pad_id, the check will fail.  ",2023-12-08T07:10:57Z,stale,closed,1,2,https://github.com/NVIDIA/Megatron-LM/issues/623,ok thanks Ill remove the lower check than I guess to support llama,Marking as stale. No activity in 60 days.
xiaojunjie,Fix bug communicating shapes when variable_seq_lengths enable,"This PR addresses the issue of communicating shapes when variable_seq_lengths enable, as detailed in the related issue https://github.com/NVIDIA/MegatronLM/issues/620.  Description This problem occurs when variable_seq_lengths=true and **num_pipeline_model_parallel_groups**>1. Without pipeline_model_parallel_group specified for communication, the tensor shape would be communicated among different pipeline_model_parallel_groups.  Fix To fix this issue, it is relatively simple: assign pipeline_model_parallel_group for P2POp when communicating shapes. Just like when communicating tensor",2023-12-08T03:53:51Z,,open,3,2,https://github.com/NVIDIA/Megatron-LM/issues/622,Marking as stale. No activity in 60 days.,Marking as stale. No activity in 60 days.
Hyungyo1,[QUESTION] RuntimeError: CUDA error: no kernel image is available for execution on the device,"Your question: I'm trying to run a GPT pretraining script using examples/pretrain_gpt.sh and the following is the error message I get. ``` Traceback (most recent call last):   File ""/workspace/megatron/pretrain_gpt.py"", line 230, in      pretrain(train_valid_test_datasets_provider,   File ""/workspace/megatron/megatron/training.py"", line 180, in pretrain     iteration = train(forward_step_func,   File ""/workspace/megatron/megatron/training.py"", line 784, in train     train_step(forward_step_func,   File ""/workspace/megatron/megatron/training.py"", line 447, in train_step     losses_reduced = forward_backward_func(   File ""/workspace/megatron/megatron/core/pipeline_parallel/schedules.py"", line 327, in forward_backward_no_pipelining     output_tensor = forward_step(   File ""/workspace/megatron/megatron/core/pipeline_parallel/schedules.py"", line 183, in forward_step     output_tensor, loss_func = forward_step_func(data_iterator, model)   File ""/workspace/megatron/pretrain_gpt.py"", line 181, in forward_step     output_tensor = model(tokens, position_ids, attention_mask,   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/workspace/megatron/megatron/core/distributed/distributed_data_parallel.py"", line 136, in forward     return self.module(*inputs, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/workspace/megatron/megatron/model/module.py"", line 181, in forward     outputs = self.module(*inputs, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/workspace/megatron/megatron/model/gpt_model.py"", line 82, in forward     lm_output = self.language_model(   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/workspace/megatron/megatron/model/language_model.py"", line 493, in forward     encoder_output = self.encoder(   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/workspace/megatron/megatron/model/transformer.py"", line 1761, in forward     hidden_states = layer(   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/workspace/megatron/megatron/model/transformer.py"", line 1150, in forward     self.self_attention(   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/workspace/megatron/megatron/model/transformer.py"", line 666, in forward     mixed_x_layer, _ = self.query_key_value(hidden_states)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/workspace/megatron/megatron/core/tensor_parallel/layers.py"", line 747, in forward     output_parallel = self._forward_impl(   File ""/workspace/megatron/megatron/core/tensor_parallel/layers.py"", line 528, in linear_with_grad_accumulation_and_async_allreduce     return LinearWithGradAccumulationAndAsyncCommunication.apply(*args)   File ""/usr/local/lib/python3.10/distpackages/torch/autograd/function.py"", line 539, in apply     return super().apply(*args, **kwargs)   type: ignore[misc]   File ""/usr/local/lib/python3.10/distpackages/torch/cuda/amp/autocast_mode.py"", line 113, in decorate_fwd     return fwd(*args, **kwargs)   File ""/workspace/megatron/megatron/core/tensor_parallel/layers.py"", line 336, in forward     output = torch.matmul(total_input, weight.t()) RuntimeError: CUDA error: no kernel image is available for execution on the device CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect. For debugging consider passing CUDA_LAUNCH_BLOCKING=1. Compile with `TORCH_USE_CUDA_DSA` to enable deviceside assertions. [20231207 20:27:40,447] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 400) of binary: /usr/bin/python Traceback (most recent call last):   File ""/usr/local/bin/torchrun"", line 8, in      sys.exit(main())   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper     return f(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/run.py"", line 806, in main     run(args)   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/run.py"", line 797, in run     elastic_launch(   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/launcher/api.py"", line 134, in __call__     return launch_agent(self._config, self._entrypoint, list(args))   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/launcher/api.py"", line 264, in launch_agent     raise ChildFailedError( torch.distributed.elastic.multiprocessing.errors.ChildFailedError:  ``` I am using an NVIDIA Docker (nvcr.io/nvidia/pytorch:23.10py3) and my GPU device is NVIDIA Titan Xp. I also tried with multiple different NVIDIA Docker versions but none of them works. With the docker versions of >=pytorch:23.03, I get the above error message and for docker versions of      from megatron import get_args   File ""/workspace/megatron/megatron/__init__.py"", line 15, in      from .initialize  import initialize_megatron   File ""/workspace/megatron/megatron/initialize.py"", line 18, in      from megatron.arguments import parse_args, validate_args   File ""/workspace/megatron/megatron/arguments.py"", line 16, in      from megatron.core.models.retro import RetroConfig   File ""/workspace/megatron/megatron/core/models/retro/__init__.py"", line 4, in      from .decoder_spec import get_retro_decoder_block_spec   File ""/workspace/megatron/megatron/core/models/retro/decoder_spec.py"", line 5, in      from megatron.core.models.gpt.gpt_layer_specs import (   File ""/workspace/megatron/megatron/core/models/gpt/__init__.py"", line 1, in      from .gpt_model import GPTModel   File ""/workspace/megatron/megatron/core/models/gpt/gpt_model.py"", line 15, in      from megatron.core.transformer.transformer_block import TransformerBlock   File ""/workspace/megatron/megatron/core/transformer/transformer_block.py"", line 13, in      from megatron.core.transformer.custom_layers.transformer_engine import TENorm   File ""/workspace/megatron/megatron/core/transformer/custom_layers/transformer_engine.py"", line 337, in      class TEDotProductAttention(te.pytorch.DotProductAttention): AttributeError: module 'transformer_engine.pytorch' has no attribute 'DotProductAttention' ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 386) of binary: /usr/bin/python Traceback (most recent call last):   File ""/usr/local/bin/torchrun"", line 33, in      sys.exit(load_entry_point('torch==1.14.0a0+44dac51', 'console_scripts', 'torchrun')())   File ""/usr/local/lib/python3.8/distpackages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper     return f(*args, **kwargs)   File ""/usr/local/lib/python3.8/distpackages/torch/distributed/run.py"", line 762, in main     run(args)   File ""/usr/local/lib/python3.8/distpackages/torch/distributed/run.py"", line 753, in run     elastic_launch(   File ""/usr/local/lib/python3.8/distpackages/torch/distributed/launcher/api.py"", line 132, in __call__     return launch_agent(self._config, self._entrypoint, list(args))   File ""/usr/local/lib/python3.8/distpackages/torch/distributed/launcher/api.py"", line 246, in launch_agent     raise ChildFailedError( torch.distributed.elastic.multiprocessing.errors.ChildFailedError:  ``` I tried to manually reinstall transformer_engine from source, but it wasn't able to successfully install. Here is my system environment when I run with NVIDIA Docker (nvcr.io/nvidia/pytorch:23.10py3): ``` PyTorch version: 2.1.0a0+32f93b1 Is debug build: False CUDA used to build PyTorch: 12.2 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.27.6 Libc version: glibc2.35 Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64bit runtime) Python platform: Linux5.15.089genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 12.2.140 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA TITAN Xp GPU 1: NVIDIA TITAN Xp Nvidia driver version: 535.129.03 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.5 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.5 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.5 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.5 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.5 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.5 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.5 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.22.2 [pip3] pytorchquantization==2.1.2 [pip3] torch==2.1.0a0+32f93b1 [pip3] torchtensorrt==0.0.0 [pip3] torchdata==0.7.0a0 [pip3] torchtext==0.16.0a0 [pip3] torchvision==0.16.0a0 [pip3] triton==2.1.0+e621604 ``` Any help would be very much appreciated. Thank you.",2023-12-07T20:47:30Z,,closed,0,7,https://github.com/NVIDIA/Megatron-LM/issues/621,Have you solved this problem? I have encountered the same issue.,"No, I have not yet",Marking as stale. No activity in 60 days.,+1,"the environment of docker container  cause this problem ,please use newest container from ngc",Marking as stale. No activity in 60 days.,"maybe some thing mismatch between your gpu and pytorchï¼Œtry to build env locally by torch==2.1 like bellowï¼š ``` pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 indexurl https://download.pytorch.org/whl/cu118 pip install 'numpy<2' pip install psutil git clone https://github.com/NVIDIA/apex cd apex pip install v disablepipversioncheck nocachedir nobuildisolation configsettings ""buildoption=cpp_ext"" configsettings ""buildoption=cuda_ext"" ./ ``` then try to run train script"
xiaojunjie,[BUG] communicate shapes when variable_seq_lengths enabled,"**Describe the bug** When variable_seq_lengths=true and **tp>1**ï¼Œ tensor_shape was not correctly communicated among ranks. **To Reproduce** world_size = 8 tp = 2 pp = 4 variable_seq_lengths = true **Expected behavior** ranks of parallel_groups arranged as follows: group0:   0    2    4    6 group1:     1    3    5    7 rank0 send tensor to rank2, success rank1 send  tensor to rank3, hang **Environment (please complete the following information):**   MegatronLM commit ID Lastest   PyTorch version 1.14.0a0+410ce96   CUDA version 11.4   NCCL version  2.15.5 **Proposed fix** Assign **pipeline_model_parallel_group** for P2POp when communicating shapes. Just like when communicating tensor !image",2023-12-07T12:23:20Z,stale,closed,0,6,https://github.com/NVIDIA/Megatron-LM/issues/620,Hey! Correct me if I am wrong  ,Sounds good to me. Can you please submit a merge request.,> Sounds good to me. Can you please submit a merge request. [ CC(Fix bug communicating shapes when variable_seq_lengths enable) ]  ,"Hi, , do you have any idea why the hang occurs? And why using pipeline_model_parallel_group for P2POp can solve this problem? It seems that the default communication group should work, right?","> Hi, , do you have any idea why the hang occurs? And why using pipeline_model_parallel_group for P2POp can solve this problem? It seems that the default communication group should work, right? As seen in the case above, shape should be communicated separately in two pp groups, because the rank given in line 73 is within a pp group, values 0~3. The default communication group can work if the rank is global. You are right. I think mismatched rank and group caused the communication hang.  ",Marking as stale. No activity in 60 days.
LiuXTao,[Fix] Fix router sync bug when moe and sequence parallelism are enable,"This PR addresses the issue of unsynchronized router weights in switchmlp when using sequence parallelism with moe enabled, as detailed in the related issue https://github.com/NVIDIA/MegatronLM/issues/599. **Description** We discovered that this problem occurs when sequence parallelism, tensor parallelism, and moe are all enabled simultaneously. Within the same tensor parallel group, different tp rank workers' routers receive input that has been partitioned by sequence parallelism. When the router calculates and updates its weights, there is no synchronization within the tp groups, leading to differences in the router's weights as training progresses. **Fix** To fix this issue, it is relatively simple: just add the sequence parallel attribute to the router's weights, allowing them to be synchronized during parameter updates. ``` if config.sequence_parallel:     setattr(self.router.weight, 'sequence_parallel', True) ``` **Test script** We tested the fix using the `examples/pretrain_gpt_moe_1B.sh`",2023-12-07T02:52:48Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/619,Marking as stale. No activity in 60 days.
ufotalent,Use different FBW memory settings for ZB schedules,,2023-12-06T08:50:41Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/618
Ethan-yt,Time breakdown analysis tool,"This pull request introduces a utility that breaks down the time spent in a single iteration, offering a powerful means of projecting theoretical time in model training. This tool enables users to derive theoretical FLOPs, enabling the optimization of hyperparameter adjustments for maximum performance, thereby eliminating the need for manual experimentation. Some of the calculation methodologies employed in this tool are inspired by https://github.com/NVIDIA/MegatronLM/pull/482. Please note the following limitations: 1. It is primarily based on llama models and has not been tested yet on other architectures. 2. It utilizes common settings such as bf16 but currently does not support gradient checkpoint and other features. If any inconsistency arises between the provided formula and the actual training process, feel free to correct me. Thank you. example output: ``` notation                                          description  value        a Number of microbatches / gradient accumulation steps    120        b                                     Global batchsize   1200        s                                      Sequence length   4096        h                                          Hidden size   8192        i                  Intermediate size / FFN hidden size  28672        l                                     Number of layers     80        v                                           Vocab size  32256       nq                      Number of query attention heads     64      nkv                  Number of key/value attention heads      8        d                                   Data parallel size     10        t                                 Tensor parallel size      4        p                               Pipeline parallel size      4 Model parameters:                name          param           Attention 12,079,595,520                 FFN 56,371,445,760           Layernorm      1,318,912 Embedding & LM Head    528,482,304               Total 68,980,842,496 Total FLOPs per iteration                                 name                     flops Attention Q, K, V, O Transformations   356,241,767,399,424,000              Attention Score, Values   158,329,674,399,744,000                                  FFN 1,662,461,581,197,312,000                              LM Head     7,792,788,661,862,400                                Total 2,184,825,811,658,342,400 Communication per GPU: parallelism  size(GB)  count  time(seconds)          TP    12.080 80.000          5.685          DP    15.521  1.000          0.183          PP     8.053  4.000          1.516 Time breakdown:               name  time(seconds) Forward / Backward         27.310          PP Bubble          0.683   PP Communication          1.516   TP Communication          5.685   DP Communication          0.183              Total         35.376 Estimated FLOPs per second: 386.000 TFLOPs ```",2023-12-05T08:46:27Z,stale,closed,1,3,https://github.com/NVIDIA/Megatron-LM/issues/617, ,ping  barker ,Marking as stale. No activity in 60 days.
taozhiwei,use _all_gather_base instead of all_gather,"when using allgather, the output is a list, and in the implementation of torch, the list will be flattened and unflattened, which will result in additional allocation of GPU memory and D2D operations. But these all gather operations already have a flat GPU memory, using _all_gather_base replaces all_gather will save GPU memory allocation and additional D2D operations.",2023-12-05T07:57:02Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/616,Marking as stale. No activity in 60 days.
Yang-QW,[BUG]Initialization stuck,"When I set WORLD_SIZE=2, tensormodelparallelsize=2, I can not load checkpoint using ""tools/run_text_generation_server.py"". The tensormodelparallelsize is 2 when I trained the checkpoint. It got stuck during initialization, just as shown below. And finally it terminated due to timeout. !image When I set WORLD_SIZE=1, tensormodelparallelsize=1, it could run successfully. (The model is also trained with tensormodelparallelsize=1). Is there anything wrong with the code or my configuration?",2023-12-05T03:07:04Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/615,"!image I mistakenly set the number of processes to 1. If there were any documentation, that would be great."
ydshi0,[QUESTION] Failed to run megatron vit. pretrain_vision_classify.sh:  TypeError: init() got an unexpected keyword argument 'post_layer_norm,"***Your question***   The environment I use is nvcr.io/nvidia/pytorch:20.04py3 official image, and I can also run gpt, bert. But when I run vit it fails.**TypeError: init() got an unexpected keyword argument 'post_layer_norm**.please help me, thank you very muchï¼ ***log***ï¼š root:/workspace/MegatronLM bash examples/pretrain_vision_classify.sh  Zarrbased strategies will not be registered because of missing packages using world size: 1, dataparallelsize: 1, tensormodelparallel size: 1, pipelinemodelparallel size: 1  WARNING: overriding default arguments for vision_pretraining:True                        with vision_pretraining:False using torch.float16 for parameters ...  arguments  setting number of microbatches to constant 256 > building NullTokenizer tokenizer ...  > padded vocab (size: 1) with 127 dummy tokens (new size: 128) > initializing torch distributed ... > initialized tensor model parallel with size 1 > initialized pipeline model parallel with size 1 > setting random seeds to 1234 ... > compiling dataset index builder ... make: Entering directory '/workspace/MegatronLM/megatron/data' make: Nothing to be done for 'default'. make: Leaving directory '/workspace/MegatronLM/megatron/data' >>> done with dataset index builder. Compilation time: 0.174 seconds > compiling and loading fused kernels ... >>> done with compiling and loading fused kernels. Compilation time: 1.699 seconds [GPU:0],allreducehere,torch.Size([1])  time to initialize megatron (seconds): 3.106 [after megatron is initialized] datetime: 20231204 07:32:40  building VIT model ... Traceback (most recent call last):   File ""pretrain_vision_classify.py"", line 99, in      pretrain(   File ""/workspace/MegatronLM/megatron/training.py"", line 114, in pretrain     model, optimizer, opt_param_scheduler = setup_model_and_optimizer(   File ""/workspace/MegatronLM/megatron/training.py"", line 377, in setup_model_and_optimizer     model = get_model(model_provider_func, model_type)   File ""/workspace/MegatronLM/megatron/training.py"", line 261, in get_model     model = model_provider_func(   File ""pretrain_vision_classify.py"", line 25, in model_provider     model = VitClassificationModel(config=config,   File ""/workspace/MegatronLM/megatron/model/vision/classification.py"", line 27, in __init__     self.backbone = VitBackbone(   File ""/workspace/MegatronLM/megatron/model/vision/vit_backbone.py"", line 199, in __init__     self.transformer = ParallelTransformer( ***TypeError: __init__() got an unexpected keyword argument 'post_layer_norm'*** ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 123086) of binary: /opt/conda/bin/python ***pretrain_vision_classify.sh*** ! /bin/bash export CUDA_DEVICE_MAX_CONNECTIONS=1 export NCCL_IB_SL=1 DATA_PATH_TRAIN=/workspace/MegatronLM/train_data DATA_PATH_VAL=/workspace/MegatronLM/val_data CHECKPOINT_PATH= CLASSIFIER_ARGS=""    	   tensormodelparallelsize 1 \         numlayers 12 \         hiddensize 768 \         numattentionheads 12 \         patchdim 4 \         seqlength 3136 \         maxpositionembeddings 3136 \         imgh 224 \         imgw 224 \         maskfactor 1.0 \         fp16 \         trainiters 750000 \         lrdecaystyle cosine \         microbatchsize 4 \         globalbatchsize 1024 \         lr 0.0005 \         minlr 0.00001 \         attentiondropout 0.0 \         weightdecay 0.05 \         lrwarmupiters 12500 \         clipgrad 1.0 \         nogradientaccumulationfusion \         numworkers 4      "" DDPimpl torch  DATA_ARGS=""      tokenizertype NullTokenizer \      vocabsize 0 \      datapath $DATA_PATH_TRAIN $DATA_PATH_VAL \      nodatasharding \      split 8,1,1  "" OUTPUT_ARG=""      loginterval 1 \      saveinterval 10000 \      evalinterval 2500 \      evaliters 1 \ "" tensorboarddir ${CHECKPOINT_PATH} \ torchrun pretrain_vision_classify.py \      $CLASSIFIER_ARGS \      $DATA_ARGS \      $OUTPUT_ARGS \      save $CHECKPOINT_PATH \      load $CHECKPOINT_PATH ***env*** root:/workspace/MegatronLM python m torch.utils.collect_env  Collecting environment information... PyTorch version: 1.12.0a0+bd13bc6 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.2) 9.4.0 Clang version: Could not collect CMake version: version 3.22.3 Libc version: glibc2.31 Python version: 3.8.13  (default, Mar 25 2022, 06:04:10)  [GCC 10.3.0] (64bit runtime) Python platform: Linux5.4.0166genericx86_64withglibc2.10 Is CUDA available: True CUDA runtime version: 11.6.124 GPU models and configuration:  GPU 0: NVIDIA GeForce GTX 1080 Ti GPU 1: NVIDIA GeForce GTX 1080 Ti GPU 2: NVIDIA GeForce GTX 1080 Ti GPU 3: NVIDIA GeForce GTX 1080 Ti Nvidia driver version: 535.129.03",2023-12-04T07:43:38Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/614,"Have you solved this problem, I met the same problem. Thanks for any reply","Hi, what you need is to change vit_backbone.py: ```         self.transformer = ParallelTransformer(             config,             model_type=args.model_type,             pre_process=self.pre_process,             post_process=self.post_process,             post_norm=self.post_layer_norm,  INSTEAD OF: post_layer_norm=self.post_layer_norm,             drop_path_rate=self.drop_path_rate         ) ```"
r11922151,[QUESTION] Abnormal Increase in GPU Memory Consumption when Using torch.autograd.graph.saved_tensors_hooks,"Hi, I am using torch.autograd.graph.saved_tensors_hooks to register pack_hooks and unpack_hooks for handling saved tensors in the pytorch/pytorch:2.0.1cuda11.7cudnn8devel container.  However, I noticed that once I use torch.autograd.graph.saved_tensors_hooks in megatronlm, the GPU memory consumption significantly increases, such as from 10GB to 20GB, even though my registered pack and unpack simply return the tensor. I also noticed that the custom forward and backward in megatronLM use ctx.save_for_backward, but it seems to only directly store tensors for backward use. Is this related to the abnormal increase in GPU memory consumption that I observed? I need to use torch.autograd.graph.saved_tensors_hooks to implement some features. If it is related, are there any recommended methods?",2023-12-03T16:53:59Z,stale,closed,1,1,https://github.com/NVIDIA/Megatron-LM/issues/613,Marking as stale. No activity in 60 days.
ilml,Why can't we just use rank!=0 to make it use the input tensor?,"https://github.com/NVIDIA/MegatronLM/blob/bcce6f54e075e3c3374ea67adefe54f3f2da2b07/megatron/core/pipeline_parallel/schedules.pyL174 Hi team, i have a dumb question for this line.  So looks like this line is saying that we're going to set the input tensor mandatorily, and if it's the first rank in PP then we will use the data iterator in the following lines. Can we just use the rank/stage information in PP and do something like `if this is the first stage, then input is from data iter , else it's from the input tensor`? Thanks a lot!",2023-12-03T04:31:33Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/612,Marking as stale. No activity in 60 days.
13416157913,"[QUESTION] Hello, a consumed samples means how many token in the training? And json file convert to .bin and .idx file","Hello,  ï¼ˆ1ï¼‰A consumed samples means how many token in the training?  ï¼ˆ2ï¼‰How compute all token number after json file convert to .bin and .idx file?",2023-12-02T06:34:41Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/611,Marking as stale. No activity in 60 days.,(1) tokens = seq_len * consumed samples,Marking as stale. No activity in 60 days.
eltociear,Update arguments.py,suppored > supported,2023-11-29T16:23:16Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/610,Marking as stale. No activity in 60 days.
BerenMillidge,Evaluation code,,2023-11-29T04:04:05Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/609
zhang662817,[BUG] Crash when enable --tp-comm-overlap,**Describe the bug** crash when enable tpcommoverlap in examples/pretrain_gpt_distributed_with_mp.sh !image **To Reproduce** !image **Environment (please complete the following information):**   MegatronLM commit ID: 9290c730d04b482be8fae92a4186fe4ff0c95270   PyTorch Docker: nvcr.io/nvidia/pytorch 23.10py3 ,2023-11-28T07:58:41Z,,closed,0,10,https://github.com/NVIDIA/Megatron-LM/issues/608,And how to config tpcommoverlapcfg?,same problem,"  hello, Can you run it successfully now?",Marking as stale. No activity in 60 days.,"I have the same problem, have you solved it?",I have the same problem,I have the same problem. :( ,Marking as stale. No activity in 60 days.,  Can you takea look into this. ,Can you try `mip=pmix` in your srun script? tensorparallel communication overlap uses MPI bootstrapping. We are trying to move on to using NCCL.
jingjie01ai,"[BUG] FP8+PP+Recompute+GA>1, loss = nan","**Describe the bug** FP8+PP+**Recompute**+GA>1, loss = nan FP8+PP+GA>1, loss is normal FP8+PP+Recompute+**GA=1**, loss is normal FP8+**TP**+Recompute+GA>1, loss is normal",2023-11-28T01:52:22Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/607,refer: https://github.com/NVIDIA/TransformerEngine/issues/539
SefaZeng,[QUESTION] CUDA OOM error when finetuning LLAMA2-7b with A100 40G and seq_length 64.,"**Your question** Ask a clear and concise question about MegatronLM. I try to follow the scripts for finetune LLAMA. So I convert the HF format checkpoint to megatron format. Then this is my finetune scripts: ``` TP=1 PP=1 GPUS_PER_NODE=8 NODE_RANK=$RANK export LOCAL_RANK=$RANK NNODES=${WORLD_SIZE} export NCCL_SOCKET_IFNAME=eth,en,em,bond export NCCL_SOCKET_IFNAME=bond1 export TORCH_DISTRIBUTED_DEBUG=OFF INFO export NCCL_DEBUG= export CUDA_LAUNCH_BLOCKING=1 export LD_LIBRARY_PATH=/usr/local/nccl/lib:$LD_LIBRARY_PATH export TRANSFORMERS_CACHE=$WORK_PATH/cache export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:32 export PYTHONPATH=${TOOL_PATH}:${PYTHONPATH} export OMP_NUM_THREADS=20 export NCCL_IB_GID_INDEX=3 export NCCL_ASYNC_ERROR_HANDLING=1 export CUDA_DEVICE_MAX_CONNECTIONS=1  7 B HIDDEN_SIZE=4096  e.g. llama13b: 5120 FFN_HIDDEN_SIZE=11008  e.g. llama13b: 13824 NUM_LAYERS=32  e.g. llama13b: 40 NUM_HEADS=32  e.g. llama13b: 40 SEQ_LENGTH=64 NUM_KV_HEADS=32  llama2 70B uses GQA MICRO_BATCH_SIZE=1 GLOBAL_BATCH_SIZE=8  TRAIN_STEPS=2  LR=3e4 MIN_LR=3e5 LR_WARMUP_STEPS=1 WEIGHT_DECAY=0.1 GRAD_CLIP=1 torchrun $DISTRIBUTED_ARGS \        ${CODE_PATH}/pretrain_gpt.py \        tensormodelparallelsize $TP \        pipelinemodelparallelsize $PP \        numlayers $NUM_LAYERS \        hiddensize $HIDDEN_SIZE \        ffnhiddensize $FFN_HIDDEN_SIZE \        numattentionheads $NUM_HEADS \        microbatchsize $MICRO_BATCH_SIZE \        globalbatchsize $GLOBAL_BATCH_SIZE \        seqlength $SEQ_LENGTH \                                                                                                                                                                      maxpositionembeddings $SEQ_LENGTH \        trainiters $TRAIN_STEPS \        save $CHECKPOINT_PATH \        load $PREMODEL \        traindatapath $DATASET \        validdatapath $DATASET_VALID \        distributedbackend nccl \        lr $LR \        lrdecaystyle cosine \        minlr $MIN_LR \        weightdecay $WEIGHT_DECAY \        clipgrad $GRAD_CLIP \        lrwarmupiters $LR_WARMUP_STEPS \        optimizer adam \        adambeta1 0.9 \        adambeta2 0.95 \        loginterval 1 \         saveinterval 1 \         evalinterval 10000000 \        evaliters 1000 \        tokenizertype Llama2Tokenizer \        tokenizermodel ${TOKENIZER_PATH} \        exitonmissingcheckpoint \        usecheckpointargs \        noloadoptim \        noloadrng \        fp16 \        untieembeddingsandoutputweights \        userotarypositionembeddings \        normalization RMSNorm \        nopositionembedding \        nomaskedsoftmaxfusion \        useflashattn \        makevocabsizedivisibleby 512 ``` and this last several lines of  log: ``` 20231126T14:15:35.547888511Z INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.25.post_attention_norm.weight 20231126T14:15:35.547898149Z INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.mlp.dense_4h_to_h.weight 20231126T14:15:35.547927875Z INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.input_norm.weight 20231126T14:15:35.547934518Z INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.self_attention.query_key_value.weight 20231126T14:15:35.547936632Z INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.self_attention.query_key_value.weight 20231126T14:15:35.969218289Z Traceback (most recent call last): 20231126T14:15:35.969235181Z   File ""/MegatronLMgit/pretrain_gpt.py"", line 230, in  20231126T14:15:35.970025202Z     pretrain(train_valid_test_datasets_provider, 20231126T14:15:35.970047654Z   File ""/MegatronLMgit/megatron/training.py"", line 118, in pretrain 20231126T14:15:35.970538099Z     model, optimizer, opt_param_scheduler = setup_model_and_optimizer( 20231126T14:15:35.970548109Z   File ""MegatronLMgit/megatron/training.py"", line 386, in setup_model_and_optimizer 20231126T14:15:35.970944406Z     optimizer = get_megatron_optimizer(model, no_wd_decay_cond, 20231126T14:15:35.970952391Z   File ""/MegatronLMgit/megatron/optimizer/__init__.py"", line 125, in get_megatron_optimizer 20231126T14:15:35.972337735Z     return opt_ty(optimizer, 20231126T14:15:35.972344188Z   File ""/MegatronLMgit/megatron/optimizer/optimizer.py"", line 412, in __init__ 20231126T14:15:35.973971719Z     main_param = param.detach().clone().float() 20231126T14:15:35.973976198Z torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 7 has a total capacty of 39.59 GiB of which 176.19 MiB is free. Process 3153921 has 39.41 GiB memory in use. Of the allocated memory 37.56 GiB is allocated by PyTorch, and 1.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF ``` I think this is not right. As finetune LLAMA27B with only seq_length 64 using transformers works well. ",2023-11-26T14:36:20Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/606,world_size in Megatron_lm equals to node*gpu per node.  I have the same problem using your script and fix it by changing the world size,Marking as stale. No activity in 60 days.
StephennFernandes,[QUESTION] how to train model with your custom tokenizer. ,"hey there, i am pretty new to using megatron LM. i have come here from the bigscience fork. i plan to pretrain GPT BERT and T5 style models on multilingual datasets. for which i have trained my own sentencepiece tokenizer model and also have huggingface tokenizer version of the same.  could someone please help me on how should i used my spm.model file to preprocess and train GPT, BERT and T5 model. ",2023-11-24T11:45:52Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/605,Marking as stale. No activity in 60 days.
yiakwy-xpu-ml-framework-team,[ENHANCEMENT] enable 16bit IO for larger vercab_size,"During bench testing, I found unusual thing : > the dataset is actually dump data in int32 bytes The codes was written with multiple process with python generator (e.g.: pool.imap), make this issue hard to uncover. ```          def optimal_dtype(cardinality: Optional[int]) > Type[numpy.number]:         """"""Get the dtype to use for an index of a certain cardinality         Args:             cardinality (Optional[int]): The number of elements to be indexed         Returns:             Type[numpy.number]: The dtype to use for the index         """"""         if cardinality is not None and cardinality  65500 (",2023-11-24T11:04:13Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/604,Marking as stale. No activity in 60 days.
Ultramarine,[BUG]ModuleNotFoundError: No module named 'change_data_ptr',"**Describe the bug** I use the [[preprocess_data.py]](https://github.com/NVIDIA/MegatronLM/blob/main/tools/preprocess_data.py)to process the data of enwiki ,then it appeared.  **To Reproduce** I try to find  the package  in NNIDIA/apex and pypi, but I nothing , and seems no others meet it . **Stack trace/logs** ...lib/python3.7/sitepackages/apex/contrib/combine_tensors/combine_tensors.py"", line 19, in  from change_data_ptr import change_data_ptr ModuleNotFoundError: No module named 'change_data_ptr' **Environment (please complete the following information):**   MegatronLM main?   PyTorch 1.81   pythion >=3.7.0",2023-11-24T07:09:07Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/603,It looks like you have an old Apex (which is used by migatron for kernel fusion impl). change_data_ptr.so is compiled and found (located in ${YOUR_ANACONDA_HOME}/envs/${your_env:llamapy38}/lib/${python_version:python3.8}/sitepackages/apex).   Wish this helps.,Marking as stale. No activity in 60 days.
Chandler-Bing,[BUG] optimizer state is empty when save-interval=20,"**Describe the bug** optimizer KeyError: 'exp_avg'    when saving the checkpoint at iter 20 while `saveinterval=20` , but when I just change to `saveinterval=50` and at iter 50, everything is ok     it's so weird, anyone could help me with that? **To Reproduce** ``` here is my argument     arguments    accumulate_allreduce_grads_in_fp32 .............. False   adam_beta1 ...................................... 0.9   adam_beta2 ...................................... 0.95   adam_eps ........................................ 1e08   add_bias_linear ................................. False   add_position_embedding .......................... False   adlr_autoresume ................................. False   adlr_autoresume_interval ........................ 1000   apply_layernorm_1p .............................. False   apply_query_key_layer_scaling ................... False   apply_residual_connection_post_layernorm ........ False   async_tensor_model_parallel_allreduce ........... True   attention_dropout ............................... 0.0   attention_softmax_in_fp32 ....................... True   barrier_with_L1_time ............................ True   bert_binary_head ................................ True   bert_embedder_type .............................. megatron   bert_load ....................................... None   bf16 ............................................ False   bias_dropout_fusion ............................. True   bias_gelu_fusion ................................ False   biencoder_projection_dim ........................ 0   biencoder_shared_query_context_model ............ False   block_data_path ................................. None   check_for_nan_in_loss_and_grad .................. True   classes_fraction ................................ 1.0   clip_grad ....................................... 0.5   consumed_train_samples .......................... 0   consumed_valid_samples .......................... 0   context_parallel_size ........................... 1   data_cache_path ................................. None   data_parallel_random_init ....................... False   data_parallel_size .............................. 8   data_path ....................................... ['/app/nfs_share_dir/1/archive/v2/all/all.shuf_text_document']   data_per_class_fraction ......................... 1.0   data_sharding ................................... True   dataloader_type ................................. single   decoder_num_layers .............................. None   decoder_seq_length .............................. None   delay_grad_reduce ............................... True   dino_bottleneck_size ............................ 256   dino_freeze_last_layer .......................... 1   dino_head_hidden_size ........................... 2048   dino_local_crops_number ......................... 10   dino_local_img_size ............................. 96   dino_norm_last_layer ............................ False   dino_teacher_temp ............................... 0.07   dino_warmup_teacher_temp ........................ 0.04   dino_warmup_teacher_temp_epochs ................. 30   distribute_saved_activations .................... False   distributed_backend ............................. nccl   distributed_timeout_minutes ..................... 100   embedding_path .................................. None   empty_unused_memory_level ....................... 0   encoder_num_layers .............................. 32   encoder_seq_length .............................. 4096   end_weight_decay ................................ 0.1   eod_mask_loss ................................... False   eval_interval ................................... 1000   eval_iters ...................................... 200   evidence_data_path .............................. None   exit_duration_in_mins ........................... None   exit_interval ................................... None   exit_on_missing_checkpoint ...................... False   exit_signal_handler ............................. False   expert_model_parallel_size ...................... 1   expert_parallel ................................. False   ffn_hidden_size ................................. 11008   finetune ........................................ True   fp16 ............................................ True   fp16_lm_cross_entropy ........................... False   fp32_residual_connection ........................ False   fp8 ............................................. None   fp8_amax_compute_algo ........................... most_recent   fp8_amax_history_len ............................ 1   fp8_interval .................................... 1   fp8_margin ...................................... 0   fp8_wgrad ....................................... True   global_batch_size ............................... 8   gradient_accumulation_fusion .................... True   group_query_attention ........................... False   head_lr_mult .................................... 1.0   hidden_dropout .................................. 0.0   hidden_size ..................................... 4096   hysteresis ...................................... 2   ict_head_size ................................... None   ict_load ........................................ None   img_h ........................................... 224   img_w ........................................... 224   indexer_batch_size .............................. 128   indexer_log_interval ............................ 1000   inference_batch_times_seqlen_threshold .......... 512   init_method_std ................................. 0.02   init_method_xavier_uniform ...................... False   initial_loss_scale .............................. 4294967296   iter_per_epoch .................................. 1250   kv_channels ..................................... 128   lazy_mpu_init ................................... None   load ............................................ /app/nfs_share_dir/1/llm_model/megatronatom   local_rank ...................................... None   log_batch_size_to_tensorboard ................... True   log_interval .................................... 10   log_learning_rate_to_tensorboard ................ True   log_loss_scale_to_tensorboard ................... True   log_memory_to_tensorboard ....................... False   log_num_zeros_in_grad ........................... False   log_params_norm ................................. False   log_path ........................................ output/ckpt   log_save ........................................ output/ckpt/loss.txt   log_timers_to_tensorboard ....................... True   log_validation_ppl_to_tensorboard ............... False   log_world_size_to_tensorboard ................... False   loss_scale ...................................... None   loss_scale_window ............................... 1000   lr .............................................. 0.0002   lr_decay_iters .................................. None   lr_decay_samples ................................ 17624440   lr_decay_style .................................. cosine   lr_warmup_fraction .............................. None   lr_warmup_init .................................. 6e05   lr_warmup_iters ................................. 0   lr_warmup_samples ............................... 220305   make_vocab_size_divisible_by .................... 1   manual_gc ....................................... False   manual_gc_eval .................................. True   manual_gc_interval .............................. 0   mask_factor ..................................... 1.0   mask_prob ....................................... 0.15   mask_type ....................................... random   masked_softmax_fusion ........................... True   max_position_embeddings ......................... 4096   max_tokens_to_oom ............................... 12000   merge_file ...................................... None   micro_batch_size ................................ 1   min_loss_scale .................................. 1.0   min_lr .......................................... 6e05   model_spec ...................................... None   nccl_communicator_config_path ................... None   no_load_optim ................................... True   no_load_rng ..................................... True   no_persist_layer_norm ........................... False   no_save_optim ................................... None   no_save_rng ..................................... None   norm_epsilon .................................... 1e05   normalization ................................... RMSNorm   num_attention_heads ............................. 32   num_channels .................................... 3   num_classes ..................................... 1000   num_experts ..................................... None   num_layers ...................................... 32   num_layers_per_virtual_pipeline_stage ........... None   num_query_groups ................................ 1   num_workers ..................................... 16   onnx_safe ....................................... None   openai_gelu ..................................... False   optimizer ....................................... adam   output_bert_embeddings .......................... False   overlap_grad_reduce ............................. False   overlap_p2p_comm ................................ False   override_opt_param_scheduler .................... False   params_dtype .................................... torch.float16   patch_dim ....................................... 16   perform_initialization .......................... True   pipeline_model_parallel_size .................... 1   pipeline_model_parallel_split_rank .............. None   position_embedding_type ......................... rope   profile ......................................... False   profile_ranks ................................... [0]   profile_step_end ................................ 12   profile_step_start .............................. 10   query_in_block_prob ............................. 0.1   rampup_batch_size ............................... None   rank ............................................ 0   recompute_granularity ........................... None   recompute_method ................................ None   recompute_num_layers ............................ None   reset_attention_mask ............................ False   reset_position_ids .............................. False   retriever_report_topk_accuracies ................ []   retriever_score_scaling ......................... False   retriever_seq_length ............................ 256   retro_add_retriever ............................. False   retro_cyclic_train_iters ........................ None   retro_encoder_attention_dropout ................. 0.1   retro_encoder_hidden_dropout .................... 0.1   retro_encoder_layers ............................ 2   retro_num_neighbors ............................. 2   retro_num_retrieved_chunks ...................... 2   retro_return_doc_ids ............................ False   retro_workdir ................................... None   rotary_percent .................................. 1.0   rotary_seq_len_interpolation_factor ............. None   sample_rate ..................................... 1.0   save ............................................ output/ckpt   save_interval ................................... 20   save_total_limit ................................ 2   saved_checkpoint_list ........................... []   scatter_gather_tensors_in_pipeline .............. True   seed ............................................ 3407   seq_length ...................................... 4096   sequence_parallel ............................... False   sgd_momentum .................................... 0.9   short_seq_prob .................................. 0.1   skip_train ...................................... False   split ........................................... 969, 30, 1   squared_relu .................................... False   standalone_embedding_stage ...................... False   start_weight_decay .............................. 0.1   swiglu .......................................... True   swin_backbone_type .............................. tiny   tensor_model_parallel_size ...................... 1   tensorboard_dir ................................. output/tensorboard   tensorboard_log_interval ........................ 1   tensorboard_queue_size .......................... 1000   test_data_path .................................. None   timing_log_level ................................ 0   timing_log_option ............................... minmax   titles_data_path ................................ None   tokenizer_model ................................. /app/nfs_share_dir/1/llm_model/atom7b1117/tokenizer.model   tokenizer_type .................................. Llama2Tokenizer   tp_comm_bulk_dgrad .............................. True   tp_comm_bulk_wgrad .............................. True   tp_comm_overlap ................................. False   tp_comm_overlap_cfg ............................. None   tp_comm_split_ag ................................ True   tp_comm_split_rs ................................ True   train_data_path ................................. None   train_iters ..................................... None   train_samples ................................... 22030551   transformer_impl ................................ local   transformer_pipeline_model_parallel_size ........ 1   untie_embeddings_and_output_weights ............. True   use_checkpoint_args ............................. False   use_checkpoint_opt_param_scheduler .............. False   use_cpu_initialization .......................... None   use_distributed_optimizer ....................... True   use_flash_attn .................................. True   use_mcore_models ................................ False   use_one_sent_docs ............................... False   use_ring_exchange_p2p ........................... False   use_rotary_position_embeddings .................. False   valid_data_path ................................. None   variable_seq_lengths ............................ False   virtual_pipeline_model_parallel_size ............ None   vision_backbone_type ............................ vit   vision_pretraining .............................. False   vision_pretraining_type ......................... classify   vocab_extra_ids ................................. 0   vocab_file ...................................... None   vocab_size ...................................... None   wandb_exp_name ..................................    wandb_project ...................................    wandb_save_dir ..................................    weight_decay .................................... 0.1   weight_decay_incr_style ......................... constant   world_size ...................................... 8  end of arguments  ``` **Expected behavior** A clear and concise description of what you expected to happen. **Stack trace/logs** Traceback (most recent call last):   File ""/app/nfs_share_dir/1/projects/megatron/pretrain_gpt.py"", line 230, in      pretrain(train_valid_test_datasets_provider,   File ""/app/nfs_share_dir/1/projects/megatron/megatron/training.py"", line 160, in pretrain     iteration = train(forward_step_func,   File ""/app/nfs_share_dir/1/projects/megatron/megatron/training.py"", line 807, in train     checkpoint_name = save_checkpoint_and_time(iteration, model, optimizer,   File ""/app/nfs_share_dir/1/projects/megatron/megatron/training.py"", line 684, in save_checkpoint_and_time     checkpoint_name = save_checkpoint(iteration, model, optimizer, opt_param_scheduler)   File ""/app/nfs_share_dir/1/projects/megatron/megatron/checkpointing.py"", line 262, in save_checkpoint     optimizer.save_parameter_state(optim_checkpoint_name)   File ""/app/nfs_share_dir/1/projects/megatron/megatron/optimizer/distrib_optimizer.py"", line 656, in save_parameter_state     .data.copy_(tensors[key].detach().cpu()) KeyError: 'exp_avg' **Environment (please complete the following information):**   MegatronLM commit ID  :19afb90    PyTorch version: 2.1.0a0+32f93b1   CUDA version:12.2   NCCL version: single node **Proposed fix** I have tried to change lr to make sure lr not equal to 0 when saving checkpoint, and I check every code about argument` saveinterval`    why this argument would affect the optimizer state?   when 'saveinterval=20' and at iter 20, I print the adam opt state dict, it's empty **Additional context** Add any other context about the problem here.",2023-11-24T02:16:45Z,stale,closed,0,10,https://github.com/NVIDIA/Megatron-LM/issues/602,"The issue is that the optimizer state hasn't been initialized yet at step 20 (since the grad scaler is still searching for the right value). If you set `lossscale 16384` or something, then you should be able to save a checkpoint at step 20 (or even earlier) since the optimizer state will then be initialized right after the first backward pass.","> The issue is that the optimizer state hasn't been initialized yet at step 20 (since the grad scaler is still searching for the right value). If you set `lossscale 16384` or something, then you should be able to save a checkpoint at step 20 (or even earlier) since the optimizer state will then be initialized right after the first backward pass.   Same error  when I set `lossscale 16384` or bigger ... ",Can you provide the full command line you are using?,"``` !/bin/bash export CUDA_DEVICE_MAX_CONNECTIONS=1  NCCL options export NCCL_DEBUG=INFO export NCCL_SOCKET_IFNAME=eth0 export NCCL_IB_HCA=mlx5 export NCCL_NET_GDR_READ=1 export NCCL_IB_DISABLE=0 TP_SIZE=1 PP_SIZE=1  Change for multinode config MASTER_ADDR= MASTER_PORT=2244 N_NODES=1 N_GPUS=8 NODE_RANK=1 DP_SIZE=$((${N_NODES} * ${N_GPUS} / ${TP_SIZE} / ${PP_SIZE})) MBS=1 GAS=1 SEQ_LENGTH=4096 DATA_PATH= TOKENIZER=Llama2Tokenizer TOKENIZER_MODEL= EXP=""ckpt"" SAVE_PATH=""output"" LOAD_PATH= CHECKPOINT_PATH=${SAVE_PATH}/${EXP} TENSORBOARD_PATH=${SAVE_PATH}/tensorboard TRAIN_LOG=${SAVE_PATH}/${EXP}/train_${NODE_RANK}.log LOSS_LOG=${SAVE_PATH}/${EXP}/loss.txt PARAM_PATH=${SAVE_PATH}/${EXP} mkdir p ${SAVE_PATH}/${EXP} SEED=3407 TRAIN_TOKENS=90237136896 TRAIN_SAMPLES=$((TRAIN_TOKENS / SEQ_LENGTH)) GBS=$((MBS * DP_SIZE * GAS))   global_batch_size MIN_LR=5e6 LR=2e5 LR_DECAY_STYLE=cosine LR_DECAY_SAMPLES=$(($TRAIN_SAMPLES * 80 / 100)) LR_WARMUP_SAMPLES=$(($TRAIN_SAMPLES * 1 / 100)) llama2  NUM_LAYER=32 HIDDEN_SIZE=4096 NUM_AH=32 INTERMEDIATE_SIZE=11008 POSTION_ENCODE=""rope"" LAYER_NORM=""RMSNorm"" DROPOUT=0 DISTRIBUTED_ARGS=""     standalone \     nproc_per_node ${N_GPUS} \     nnodes ${N_NODES} \     node_rank ${NODE_RANK} \     master_addr ${MASTER_ADDR} \     master_port ${MASTER_PORT} \ "" GPT_ARGS=""     tensormodelparallelsize ${TP_SIZE} \     pipelinemodelparallelsize ${PP_SIZE} \     numlayers ${NUM_LAYER} \     hiddensize ${HIDDEN_SIZE} \     numattentionheads ${NUM_AH} \     seqlength ${SEQ_LENGTH} \     ffnhiddensize ${INTERMEDIATE_SIZE} \     tokenizertype ${TOKENIZER} \     tokenizermodel ${TOKENIZER_MODEL} \     nopositionembedding \     noloadoptim \     noloadrng \     makevocabsizedivisibleby 1 \     positionembeddingtype ${POSTION_ENCODE} \     normalization ${LAYER_NORM} \     attentiondropout ${DROPOUT} \     hiddendropout ${DROPOUT} \     attentionsoftmaxinfp32 \     untieembeddingsandoutputweights \     disablebiaslinear \     useflashattn \     usedistributedoptimizer \     swiglu \     maxpositionembeddings ${SEQ_LENGTH} \ "" TRAIN_ARGS="" \     seed ${SEED} \     microbatchsize ${MBS} \     globalbatchsize ${GBS} \     trainsamples ${TRAIN_SAMPLES} \     optimizer adam \     adambeta1 0.9 \     adambeta2 0.95 \     adameps 1e8 \     lr $LR \     minlr $MIN_LR \     lrdecaystyle ${LR_DECAY_STYLE} \     lrdecaysamples ${LR_DECAY_SAMPLES} \     lrwarmupsamples ${LR_WARMUP_SAMPLES} \     clipgrad 0.5 \     lossscale 1638400 \     finetune \     weightdecay 1e1 \     numworkers ${NUM_WORKERS} \     fp16 \     distributedtimeoutminutes 100 \ "" DATA_ARGS=""     datapath ${DATA_PATH} "" OUTPUT_ARGS=""     loginterval 10 \     logpath ${PARAM_PATH} \     logsave ${LOSS_LOG} \     logbatchsizetotensorboard \     logtimerstotensorboard \     tensorboarddir ${TENSORBOARD_PATH} \     saveinterval 20 \     evaliters 200 "" torchrun ${DISTRIBUTED_ARGS} pretrain_gpt.py \     ${TRAIN_ARGS} \     ${GPT_ARGS} \     ${DATA_ARGS} \     ${OUTPUT_ARGS} \     distributedbackend nccl \     save ${CHECKPOINT_PATH} \     load ${LOAD_PATH} 2>&1 | tee ${TRAIN_LOG} ```   Here is my full run script . thanks  ","I presume you also tried `loss scale 16384` (this should be a perfect power of 2)? Can you paste the relevant lines from the logs too? Basically everything starting with ""iteration""? Should look like this (use `loginterval 1` too): ``` 383:  iteration     7800/  472541  ```","```  iteration        1/ 2753818  saving checkpoint at iteration      20 to output/ckpt Traceback (most recent call last):   File ""/app/nfs_share_dir/1/projects/megatron/pretrain_gpt.py"", line 230, in      pretrain(train_valid_test_datasets_provider,   File ""/app/nfs_share_dir/1/projects/megatron/megatron/training.py"", line 160, in pretrain     iteration = train(forward_step_func,   File ""/app/nfs_share_dir/1/projects/megatron/megatron/training.py"", line 805, in train     checkpoint_name = save_checkpoint_and_time(iteration, model, optimizer,   File ""/app/nfs_share_dir/1/projects/megatron/megatron/training.py"", line 684, in save_checkpoint_and_time     checkpoint_name = save_checkpoint(iteration, model, optimizer, opt_param_scheduler)   File ""/app/nfs_share_dir/1/projects/megatron/megatron/checkpointing.py"", line 262, in save_checkpoint     optimizer.save_parameter_state(optim_checkpoint_name)   File ""/app/nfs_share_dir/1/projects/megatron/megatron/optimizer/distrib_optimizer.py"", line 653, in save_parameter_state     .data.copy_(tensors[key].detach().cpu()) KeyError: 'exp_avg' Traceback (most recent call last):   File ""/app/nfs_share_dir/1/projects/megatron/pretrain_gpt.py"", line 230, in      pretrain(train_valid_test_datasets_provider,   File ""/app/nfs_share_dir/1/projects/megatron/megatron/training.py"", line 160, in pretrain     iteration = train(forward_step_func,   File ""/app/nfs_share_dir/1/projects/megatron/megatron/training.py"", line 805, in train     checkpoint_name = save_checkpoint_and_time(iteration, model, optimizer,   File ""/app/nfs_share_dir/1/projects/megatron/megatron/training.py"", line 684, in save_checkpoint_and_time     checkpoint_name = save_checkpoint(iteration, model, optimizer, opt_param_scheduler)   File ""/app/nfs_share_dir/1/projects/megatron/megatron/checkpointing.py"", line 262, in save_checkpoint     optimizer.save_parameter_state(optim_checkpoint_name)   File ""/app/nfs_share_dir/1/projects/megatron/megatron/optimizer/distrib_optimizer.py"", line 653, in save_parameter_state     .data.copy_(tensors[key].detach().cpu()) KeyError: 'exp_avg' ```    sure,     here is my total log, seems no` grad norm` there (not know it matters or not)","Very strange, not sure why you are always seeing skipped iterations. Perhaps something weird with your dataset?","I will check my dataset later. as I did not see any problems when preprocess row text to .bin and .idx file, I did not pay much attention on dataset. Another question:   `consumed samples`     A consumed sample means a json text line?     if No, ` consumed token` = `consumed samples`    *4096  if Yes, how should I know info like ` consumed token` to get my training throughput?  (becauseof different length of text)    Thanks for helping! ","The same issue happened to me. It disappears if I set ""fp16=false"" and ""bf16=true"" or set ""lossscale 16384"". The reason is that gradients overflow when using fp16, and hence parameter updating is skipped. That's why you see ""number of skipped iterations:   1"" at logging. The optimizer states are never initialized, nor is the field 'exp_avg'.",Marking as stale. No activity in 60 days.
neuronblack,typo,typo,2023-11-23T08:55:20Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/601,Marking as stale. No activity in 60 days.
okuchaiev,[BUG] Unittests for NLP require data on internal CI machines to not fail.,"**Describe the bug** A lot of unit tests in NLP collection (over 10) require correct version of ``/home/TestData`` folder (from internal CI machines) to be present to run successfully. **This makes it impossible to run unittests successfully anywhere but on internal NVIDIA CI machines.** **To Reproduce** Clone NeMo on new machine in clean environment and try running ``pytest tests/collections/nlp`` Make sure you do not have ``/home/TestData`` folder on the machine. **Expected behavior** Unittest run by ``pytest `` command should run successfully, not only on CI machines. E.g. external developer/contributor should be able to run unit tests. **Stack trace/logs** ``` library = 'sentencepiece', model_name = None, tokenizer_model = '/home/TestData/nlp/megatron_sft/tokenizer.model', vocab_file = None, merges_file = None, special_tokens = None, use_fast = False, bpe_dropout = 0.0, r2l = False, legacy = False, delimiter = None     def get_nmt_tokenizer(         library: str = 'yttm',         model_name: Optional[str] = None,         tokenizer_model: Optional[str] = None,         vocab_file: Optional[str] = None,         merges_file: Optional[str] = None,         special_tokens: Optional[Dict[str, str]] = None,         use_fast: Optional[bool] = False,         bpe_dropout: Optional[float] = 0.0,         r2l: Optional[bool] = False,         legacy: Optional[bool] = False,         delimiter: Optional[str] = None,     ):         """"""         Args:             model_name: if using a pretrained model from NeMo, HuggingFace, or Megatron             tokenizer_model: tokenizer model file of sentencepiece or youtokentome             special_tokens: dict of special tokens             vocab_file: path to vocab file             use_fast: (only for HuggingFace AutoTokenizer) set to True to use fast HuggingFace tokenizer             bpe_dropout: (only supported by YTTM tokenizer) BPE dropout tries to corrupt the standard segmentation procedure                 of BPE to help model better learn word compositionality and become robust to segmentation errors.                 It has empirically been shown to improve inference time BLEU scores.             r2l: Whether to return subword IDs from right to left         """"""         if special_tokens is None:             special_tokens_dict = {}         else:             special_tokens_dict = special_tokens         if (library != 'bytelevel') and (             model_name is None and (tokenizer_model is None or not os.path.isfile(tokenizer_model))         ): >           raise ValueError(""No Tokenizer path provided or file does not exist!"") E           ValueError: No Tokenizer path provided or file does not exist! nemo/collections/nlp/modules/common/tokenizer_utils.py:176: ValueError ``` **Environment (please complete the following information):**   MegatronLM commit ID   PyTorch version 2.*   CUDA version   NCCL version **Proposed fix** I proposed tests to either skip if  ``/home/TestData`` folder isn't found, or to be rewritten.  **Additional context** Add any other context about the problem here.",2023-11-22T02:52:14Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/600,oops. wrong repo
RookieHong,[BUG] SwitchMLP router weights are not synchronized within the tensor parallelism group,"**Describe the bug** The router in SwitchMLP is defined as a torch.nn.Linear.  ```python class SwitchMLP(MegatronModule):     """"""     Routes input to one of N MLP ""experts""     """"""     def __init__(self, config):         super(SwitchMLP, self).__init__()         args = get_args()         self.router = torch.nn.Linear(args.hidden_size, args.num_experts) ``` When tensor parallelism is enabled, the weights of router are not split according to tensor parallelism and are not synchronized within the same tensor parallelism group. As a result, routers within the same tensor parallelism group have different weights, making it impossible to export the weights trained on MegatronLM to HuggingFace. **To Reproduce** Just make sure tensormodelparallelsize > 1 and numexperts >= 1. **Expected behavior** The weights of the router in SwitchMLP should be consistent within the tensor parallelism group. **Stack trace/logs** If applicable, add the stack trace or logs from the time of the error. **Environment (please complete the following information):**   MegatronLM commit ID: e122536b7645edcb7ebf099b5c92a443f7dbf8e7   PyTorch version: 2.0.1+cu118   CUDA version: 11.8   NCCL version **Proposed fix** The router in SwitchMLP should be defined using tensor_parallel.RowParallelLinear or tensor_parallel.ColumnParallelLinear. By doing so, the weights of the router within the tensor parallelism group can be merged, allowing the trained weights to be exported successfully. **Additional context** Add any other context about the problem here.",2023-11-20T09:57:00Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/599
CurryRice233,fix data helpers overflow bug,"It can produce this error when the training iteration is a large number and the dataset is a short sentence dataset. ``` Traceback (most recent call last):   File ""pretrain_gpt.py"", line 121, in      args_defaults={'tokenizer_type': 'GPT2BPETokenizer'}   File ""/home/mauser/modelarts/userjobdir/GPT3kernel_ID2728_for_PyTorch_zgcl/megatron/training.py"", line 150, in pretrain     process_non_loss_data_func)   File ""/home/mauser/modelarts/userjobdir/GPT3kernel_ID2728_for_PyTorch_zgcl/megatron/training.py"", line 689, in train     opt_param_scheduler)   File ""/home/mauser/modelarts/userjobdir/GPT3kernel_ID2728_for_PyTorch_zgcl/megatron/training.py"", line 417, in train_step     optimizer, fwd_bwd_timers, forward_only=False)   File ""/home/mauser/modelarts/userjobdir/GPT3kernel_ID2728_for_PyTorch_zgcl/megatron/schedules.py"", line 654, in forward_backward_pipelining_without_interleaving     timers, collect_non_loss_data)   File ""/home/mauser/modelarts/userjobdir/GPT3kernel_ID2728_for_PyTorch_zgcl/megatron/schedules.py"", line 118, in forward_step     output_tensor, loss_func = forward_step_func(data_iterator, model)   File ""pretrain_gpt.py"", line 84, in forward_step     data_iterator)   File ""pretrain_gpt.py"", line 45, in get_batch     data = next(data_iterator)   File ""/home/mauser/anaconda/lib/python3.7/sitepackages/torch/utils/data/dataloader.py"", line 530, in __next__     data = self._next_data()   File ""/home/mauser/anaconda/lib/python3.7/sitepackages/torch/utils/data/dataloader.py"", line 570, in _next_data     data = self._dataset_fetcher.fetch(index)   may raise StopIteration   File ""/home/mauser/anaconda/lib/python3.7/sitepackages/torch/utils/data/_utils/fetch.py"", line 52, in fetch     return self.collate_fn(data)   File ""/home/mauser/anaconda/lib/python3.7/sitepackages/torch/utils/data/_utils/collate.py"", line 157, in default_collate     return elem_type({key: default_collate([d[key] for d in batch]) for key in elem})   File ""/home/mauser/anaconda/lib/python3.7/sitepackages/torch/utils/data/_utils/collate.py"", line 157, in      return elem_type({key: default_collate([d[key] for d in batch]) for key in elem})   File ""/home/mauser/anaconda/lib/python3.7/sitepackages/torch/utils/data/_utils/collate.py"", line 146, in default_collate     return default_collate([torch.as_tensor(b) for b in batch])   File ""/home/mauser/anaconda/lib/python3.7/sitepackages/torch/utils/data/_utils/collate.py"", line 138, in default_collate     return torch.stack(batch, 0, out=out) RuntimeError: stack expects each tensor to be equal size, but got [8193] at entry 0 and [8246] at entry 1 ``` The reason is `sample_idx[2 * sample_index] = doc_idx_index;`, that `doc_idx_index`(int64) is assigned to `sample_idx`(int32), when the value of `doc_idx_index` is greater than the range of int32, overflow may occur.",2023-11-20T02:15:28Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/598,Marking as stale. No activity in 60 days.
13416157913,"[QUESTION] Hello, how to accurately calculate memory usage based on operating parameters?","Hello, how to accurately calculate memory usage based on operating parameters ? For example, TP_SIZE=1 \ PP_SIZE=1 \ WORLD_SIZE=8 \ MICRO_BATCH_SIZE=2 \ GLOBAL_BATCH_SIZE=128 \ finetune \ sequenceparallel \ numlayers 32 \ hiddensize 4096 \ numattentionheads 32 \ seqlength 4096 \ maxpositionembeddings 4096 \ nopositionembedding \ userotarypositionembeddings \ swiglu \ ffnhiddensize 11008\ disablebiaslinear \ RMSNorm \ layernormepsilon 1e6 \ causallm \ distributedoptimizer \ useflashattn",2023-11-20T01:20:47Z,stale,closed,0,5,https://github.com/NVIDIA/Megatron-LM/issues/597,https://github.com/NVIDIA/MegatronLM/pull/482 Please check this,> CC(Analysis Tool) Please check this Thank you very much.,We also now have a `report_theoretical_memory.py` script now that should take the same set of arguments as `pretrain_gpt.py`. You can use like this: ```bash CUDA_DEVICE_MAX_CONNECTIONS=1 WORLD_SIZE= python u report_theoretical_memory.py ${options} ```,Marking as stale. No activity in 60 days.,Going to close this. Feel free to reopen if you are still running into issues.
lushizijizoude,Invalid config blocking attribute value -2147483648[QUESTION],"When I switched the model from distributed training to singlemachine training, I encountered the following error: ```shell Building extension module rotary_pos_emb_cuda... Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N) ninja: no work to do. Loading extension module rotary_pos_emb_cuda... dsw22664b7566f8bxz9j6:71868:71868 [0] NCCL INFO Bootstrap : Using eth0:10.207.8.104 dsw22664b7566f8bxz9j6:71868:71868 [0] NCCL INFO Plugin name set by env to libncclnetnone.so dsw22664b7566f8bxz9j6:71868:71868 [0] NCCL INFO NET/Plugin : No plugin found (libncclnetnone.so), using internal implementation dsw22664b7566f8bxz9j6:71868:71868 [0] init.cc:1270 NCCL WARN Invalid config blocking attribute value 2147483648 Traceback (most recent call last):   File ""scripts/sft_gpt.py"", line 151, in      sft(   File ""/cpfs/2926428ee2463e44/user/xinghai/eas/infer/7b/ProjectACE/ace/sft/training.py"", line 60, in sft     initialize_megatron(   File ""/cpfs/2926428ee2463e44/user/xinghai/eas/infer/7b/MegatronLMagi33b/megatron/initialize.py"", line 82, in initialize_megatron     _compile_dependencies()   File ""/cpfs/2926428ee2463e44/user/xinghai/eas/infer/7b/MegatronLMagi33b/megatron/initialize.py"", line 131, in _compile_dependencies     torch.distributed.barrier()   File ""/cpfs/2926428ee2463e44/user/xinghai/opt/miniconda/envs/py38/lib/python3.8/sitepackages/torch/distributed/c10d_logger.py"", line 47, in wrapper     return func(*args, **kwargs)   File ""/cpfs/2926428ee2463e44/user/xinghai/opt/miniconda/envs/py38/lib/python3.8/sitepackages/torch/distributed/distributed_c10d.py"", line 3696, in barrier     work = default_pg.barrier(opts=opts) torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/NCCLUtils.hpp:219, invalid argument, NCCL version 2.14.3 ncclInvalidArgument: Invalid value for an argument. Last error: Invalid config blocking attribute value 2147483648 [20231119 03:09:25,027] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 71868) of binary: /cpfs/2926428ee2463e44/user/xinghai/opt/miniconda/envs/py38/bin/python Traceback (most recent call last):   File ""/cpfs/2926428ee2463e44/user/xinghai/opt/miniconda/envs/py38/bin/torchrun"", line 8, in      sys.exit(main())   File ""/cpfs/2926428ee2463e44/user/xinghai/opt/miniconda/envs/py38/lib/python3.8/sitepackages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper     return f(*args, **kwargs)   File ""/cpfs/2926428ee2463e44/user/xinghai/opt/miniconda/envs/py38/lib/python3.8/sitepackages/torch/distributed/run.py"", line 806, in main     run(args)   File ""/cpfs/2926428ee2463e44/user/xinghai/opt/miniconda/envs/py38/lib/python3.8/sitepackages/torch/distributed/run.py"", line 797, in run     elastic_launch(   File ""/cpfs/2926428ee2463e44/user/xinghai/opt/miniconda/envs/py38/lib/python3.8/sitepackages/torch/distributed/launcher/api.py"", line 134, in __call__     return launch_agent(self._config, self._entrypoint, list(args))   File ""/cpfs/2926428ee2463e44/user/xinghai/opt/miniconda/envs/py38/lib/python3.8/sitepackages/torch/distributed/launcher/api.py"", line 264, in launch_agent     raise ChildFailedError( torch.distributed.elastic.multiprocessing.errors.ChildFailedError:  ============================================================ scripts/sft_gpt.py FAILED  Failures:     Root Cause (first observed failure): [0]:   time      : 20231119_03:09:25   host      : dsw22664b7566f8bxz9j6   rank      : 0 (local_rank: 0)   exitcode  : 1 (pid: 71868)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html ============================================================ ``` ",2023-11-18T19:11:22Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/596,How are you running singlenode training?,Marking as stale. No activity in 60 days.
drxmy,[ENHANCEMENT] convert HF llama with target-pipeline-parallel-size larger than 1,"**Describe the solution you'd like** When converting HF llama,  can i also set targetpipelineparallelsize larger than 1?",2023-11-17T07:31:39Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/595, do you solve this issue? 
zhipeng93,[QUESTION] TE and MLM version,"**Your question** I need to use MLM and TE to enable FP8 optimization, however the codebase of MLM seems unstable  There is no clear guideline for users to choose a specific MLM version and the corresponding TE. Is there a document or a roadmap for this?",2023-11-16T08:57:18Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/594,Marking as stale. No activity in 60 days.
ChenYuHo,FusedLayerNormAffineFunction requires memory_efficient argument,FusedLayerNormAffineFunction requires memory_efficient argument https://github.com/NVIDIA/apex/blob/08f740290f999296d319ed2e3f21cd00b810918a/apex/normalization/fused_layer_norm.pyL34 but MegatronLM's FusedLayerNormAffineFunction usage does not have it and causes errors. https://github.com/NVIDIA/MegatronLM/blob/443ce9f3f98fdc5a53c6b480c6e21b79944d198e/megatron/core/fusions/fused_layer_norm.pyL116 Should this be exposed to users?,2023-11-16T08:43:16Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/593,"I believe the issue is this commit  https://github.com/NVIDIA/apex/commit/6ff45486f432f91eb86937a0def5eb5f2cf792ae Using an older version of apex (from before this commit) may help fix this problem  I am testing if this works. Alternatively, latest NGC container is from before this commit, and does not seem to have this issue too.",Marking as stale. No activity in 60 days.
Mrzhang-dada,[QUESTION]for model,"**Your question** Ask a clear and concise question about MegatronLM. After carefully reading the readme, I only saw Megatron LM's user manual for Bert, GPT, etc., and did not see the extension effect that Megatron LM can have on other models. May I ask if Megatron LM supports the use of other models and how to call interfaces to adapt to them?",2023-11-16T08:17:22Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/592,"I am not sure I fully follow: what do you mean by ""did not see the extension effect that Megatron LM can have on other models""?","> I am not sure I fully follow: what do you mean by ""did not see the extension effect that Megatron LM can have on other models""? What I mean is whether Megatron LM can be used on other large models besides BERT, GPT, T5, etc. If so, what should I do?",We only support the models that have corresponding `pretrain_*.py` scripts in the root directory.
zhipeng93,[ENHANCEMENT] Support converting checkpoint with interleaved schedule enabled,"**Is your feature request related to a problem? Please describe.** Suppose we have trained an LLM with parallel strategy `A` (interleaved schedule not enabled) and saved a checkpoint `C`.  Then we found that `A` could be more efficient by enabling interleaved schedule (say strategy `B`). However, the current megatron codebase cannot use strategy `B` to continue training from checkpoint `C`. **Describe the solution you'd like** Support `interleaved schedule` in [1]. [1] https://github.com/NVIDIA/MegatronLM/tree/main/tools/checkpoint",2023-11-15T12:13:29Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/591,Marking as stale. No activity in 60 days.
loubnabnl,Humaneval greedy generations,Run generations for humaneval problems using Megatron (sequential generation for greedy mode) ```bash  deploy the server on a GPU node (adapt paths/ports..) bash examples/run_text_generation_starcoder.sh  run and save generations (on the same node) to be executed with bigcodeevaluationharness python /fsx/loubna/bigcode_2/code/pr/MegatronLM/tools/run_requests_humaneval.py ```,2023-11-14T15:04:40Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/590
hwdef,[ENHANCEMENT] support zero 2 distributed optimize,"**Is your feature request related to a problem? Please describe.** As far as I know, the current distributed optimizer of megatronlm implements zero1, but zero1 does not save enough GPU memory. When I train a model, I use the `usedistributedoptimizer` parameter to only make each GPU memory usage has dropped from 49GB to 47GB. I conducted the same test on megatrondeepspeed. When megatrondeepspeed doubled the `MBS`, the GPU memory usage was only 36GB. In megatrondeepspeed, I used zero2. I think The reduction of GPU memory is related to my use of zero2. **Describe the solution you'd like** I hope megatronlm can also implement zero2 **Describe alternatives you've considered** https://github.com/microsoft/MegatronDeepSpeed",2023-11-14T06:43:10Z,stale,closed,3,7,https://github.com/NVIDIA/Megatron-LM/issues/589,"nvidia  Hi, Could you please help me about this?",Marking as stale. No activity in 60 days.,Any progress?,Marking as stale. No activity in 60 days., can you please let us know how to enable ZeRO 1/2/3 ? Thanks, please let know how can i enable ZeRO 1/2/3 feature ? raised CC([QUESTION] How to enable ZeRO 2/3 stages ?) ,We do not currently support Zero 2/3. But it is possible that we will support this in the future.
ia3leonidshad,[BUG] RoPE embeddings lead to error with distributed training,"**Describe the bug** Using RoPE embeddings lead to NCCL error when training on 2 GPUs or more. Bug was introduced in this commit: https://github.com/NVIDIA/MegatronLM/commit/0c2074e2bdfca3a2a1ad5957838e4209e141a93cdiffa76c01a5dcf342ac5c484ff276e6cd91de4756f1fc17125131e7c8a2badb1fee When rolling back before it (and fixing some import and args bugs) RoPE works. **To Reproduce** ``` GPT_ARGS=""     numlayers 24 \     hiddensize 1024 \     numattentionheads 16 \     seqlength 1024 \     maxpositionembeddings 1024 \     positionembeddingtype rope \     microbatchsize 4 \     globalbatchsize 64 \     lr 0.00015 \     trainiters 500000 \     lrdecayiters 320000 \     lrdecaystyle cosine \     minlr 1.0e5 \     weightdecay 1e2 \     lrwarmupfraction .01 \     clipgrad 1.0 \     fp16 \     datacachepath /home/lekimov/experiments/indexfolder \ "" ``` ``` DISTRIBUTED_ARGS=""     nproc_per_node 2 \     nnodes 1 \     node_rank 0 \     master_addr localhost \     master_port 2020 "" ``` **Stack trace/logs** ``` Traceback (most recent call last):   File ""pretrain_gpt.py"", line 230, in      pretrain(train_valid_test_datasets_provider,   File ""/home/lekimov/MegatronLMlatest/megatron/training.py"", line 160, in pretrain Traceback (most recent call last):   File ""pretrain_gpt.py"", line 230, in      pretrain(train_valid_test_datasets_provider,   File ""/home/lekimov/MegatronLMlatest/megatron/training.py"", line 160, in pretrain     iteration = train(forward_step_func,   File ""/home/lekimov/MegatronLMlatest/megatron/training.py"", line 748, in train     train_step(forward_step_func,   File ""/home/lekimov/MegatronLMlatest/megatron/training.py"", line 424, in train_step     losses_reduced = forward_backward_func(   File ""/home/lekimov/MegatronLMlatest/megatron/core/pipeline_parallel/schedules.py"", line 362, in forward_backward_no_pipelining     iteration = train(forward_step_func,   File ""/home/lekimov/MegatronLMlatest/megatron/training.py"", line 748, in train     config.finalize_model_grads_func([model])   File ""/home/lekimov/MegatronLMlatest/megatron/core/distributed/finalize_model_grads.py"", line 129, in finalize_model_grads     train_step(forward_step_func,   File ""/home/lekimov/MegatronLMlatest/megatron/training.py"", line 424, in train_step     model_chunk.finish_grad_sync()   File ""/home/lekimov/MegatronLMlatest/megatron/core/distributed/distributed_data_parallel.py"", line 192, in finish_grad_sync     losses_reduced = forward_backward_func(   File ""/home/lekimov/MegatronLMlatest/megatron/core/pipeline_parallel/schedules.py"", line 362, in forward_backward_no_pipelining     grad_buffer.finish_grad_sync()   File ""/home/lekimov/MegatronLMlatest/megatron/core/distributed/grad_buffer.py"", line 383, in finish_grad_sync     config.finalize_model_grads_func([model])   File ""/home/lekimov/MegatronLMlatest/megatron/core/distributed/finalize_model_grads.py"", line 129, in finalize_model_grads     bucket.finish_grad_sync()   File ""/home/lekimov/MegatronLMlatest/megatron/core/distributed/grad_buffer.py"", line 123, in finish_grad_sync     model_chunk.finish_grad_sync()   File ""/home/lekimov/MegatronLMlatest/megatron/core/distributed/distributed_data_parallel.py"", line 192, in finish_grad_sync     self.start_grad_sync()   File ""/home/lekimov/MegatronLMlatest/megatron/core/distributed/grad_buffer.py"", line 108, in start_grad_sync     grad_buffer.finish_grad_sync()   File ""/home/lekimov/MegatronLMlatest/megatron/core/distributed/grad_buffer.py"", line 383, in finish_grad_sync     bucket.finish_grad_sync()   File ""/home/lekimov/MegatronLMlatest/megatron/core/distributed/grad_buffer.py"", line 123, in finish_grad_sync     self.start_grad_sync()   File ""/home/lekimov/MegatronLMlatest/megatron/core/distributed/grad_buffer.py"", line 108, in start_grad_sync     self.communication_handle = torch.distributed.all_reduce(   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/c10d_logger.py"", line 47, in wrapper     return func(*args, **kwargs)   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/distributed_c10d.py"", line 2050, in all_reduce     work = group.allreduce([tensor], opts) torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1331, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.18.5 ncclUnhandledCudaError: Call to CUDA function failed. Last error: Cuda failure 'invalid argument' ``` **Environment (please complete the following information):**   MegatronLM commit ID: 443ce9f3f98fdc5a53c6b480c6e21b79944d198e   PyTorch version: 2.1.0+cu11   CUDA version: cuda_11.8.r11.8/compiler.31833905_0   NCCL version: 2.18.5   2x V100 GPUs (32GB) **Proposed fix** Might be connected to: https://github.com/NVIDIA/MegatronLM/issues/560 **Additional context** Without rotary embeddings everything works fine.",2023-11-13T12:52:31Z,,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/588,could you run with `NCCL_DEBUG=INFO`? We have been able to run RoPe without issues.,I cannot reproduce it anymore. I'll close the issue.,"  I met same error.I installed transformerEngine and it does not work.Then I uninstalled it.And I got this error. ``` [20240301 00:58:32,151] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect) [20240301 00:58:42,368] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only. [20240301 00:58:42,369] [INFO] [runner.py:570:main] cmd = /home/www/anaconda3/envs/cuda11/bin/python u m deepspeed.launcher.launch world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMywgNCwgNSwgNiwgOCwgOV19 master_addr=127.0.0.1 master_port=29500 enable_each_rank_log=None /home/www/models/gpt/megatron_lm/trains/train_scaled_v55.py tensormodelparallelsize 2 sequenceparallel useflashattn optimizer adam recomputeactivations numlayers 64 hiddensize 3072 numattentionheads 32 seqlength 3400 maxpositionembeddings 4000 microbatchsize 1 globalbatchsize 120 lr 0.0001 trainiters 500000 lrdecayiters 320000 lrdecaystyle cosine minlr 1.0e5 weightdecay 1e2 clipgrad 1.0 bf16 datapath $DATA_PATH vocabfile $VOCAB_FILE mergefile $MERGE_FILE split 999,1,1 loginterval 10 saveinterval 1000 evalinterval 10000 evaliters 1 tokenizertype t5 untieembeddingsandoutputweights userotarypositionembeddings swiglu save /data3/www/checkpoints/dones/10b_4_2 dataloadertype cyclic load /data3/www/checkpoints/dones/10b_4_1 finetune initiallossscale 8192 tensorboardqueuesize 1 logtimerstotensorboard logbatchsizetotensorboard logvalidationppltotensorboard tensorboarddir /data3/www/checkpoints/dones/10b_4_1 usedistributedoptimizer spec local [20240301 00:58:44,665] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect) [20240301 00:58:45,605] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 3, 4, 5, 6, 8, 9]} [20240301 00:58:45,605] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0 [20240301 00:58:45,606] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}) [20240301 00:58:45,606] [INFO] [launch.py:163:main] dist_world_size=8 [20240301 00:58:45,606] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,3,4,5,6,8,9 no transformer engine no transformer engine no transformer engine no transformer engine no transformer engine no transformer engine no transformer engine no transformer engine no transformer engine no transformer engine no transformer engine no transformer engine no transformer engine no transformer engine no transformer engine no transformer engine [20240301 00:58:49,556] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect) [20240301 00:58:49,557] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect) [20240301 00:58:49,608] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect) [20240301 00:58:49,611] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)  None  None  None  None You are using the default legacy behaviour of the . This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 You are using the default legacy behaviour of the . This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 [20240301 00:58:49,929] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect) You are using the default legacy behaviour of the . This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 > setting tensorboard ... [20240301 00:58:49,991] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect) You are using the default legacy behaviour of the . This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 [20240301 00:58:50,038] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect) [20240301 00:58:50,116] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)  None  None  None  None You are using the default legacy behaviour of the . This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 You are using the default legacy behaviour of the . This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 You are using the default legacy behaviour of the . This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 using world size: 8, dataparallel size: 4, contextparallel size: 1 tensormodelparallel size: 2, pipelinemodelparallel size: 1  WARNING: Setting args.overlap_p2p_comm to False since noninterleaved schedule does not support overlapping p2p communication accumulate and allreduce gradients in fp32 for bfloat16 data type. using torch.bfloat16 for parameters ...  arguments    accumulate_allreduce_grads_in_fp32 .............. True   adam_beta1 ...................................... 0.9   adam_beta2 ...................................... 0.999   adam_eps ........................................ 1e08   add_bias_linear ................................. True   add_position_embedding .......................... True   add_qkv_bias .................................... False   adlr_autoresume ................................. False   adlr_autoresume_interval ........................ 1000   apply_layernorm_1p .............................. False   apply_query_key_layer_scaling ................... False   apply_residual_connection_post_layernorm ........ False   apply_rope_fusion ............................... True   async_tensor_model_parallel_allreduce ........... False   attention_dropout ............................... 0.1   attention_softmax_in_fp32 ....................... False   barrier_with_L1_time ............................ True   bert_binary_head ................................ True   bert_embedder_type .............................. megatron   bert_load ....................................... None   bf16 ............................................ True   bias_dropout_fusion ............................. True   bias_gelu_fusion ................................ True   bias_swiglu_fusion .............................. True   biencoder_projection_dim ........................ 0   biencoder_shared_query_context_model ............ False   block_data_path ................................. None   check_for_nan_in_loss_and_grad .................. True   classes_fraction ................................ 1.0   clip_grad ....................................... 1.0   clone_scatter_output_in_embedding ............... True   consumed_train_samples .......................... 0   consumed_valid_samples .......................... 0   context_parallel_size ........................... 1   data_cache_path ................................. None   data_parallel_random_init ....................... False   data_parallel_size .............................. 4   data_path ....................................... ['$DATA_PATH']   data_per_class_fraction ......................... 1.0   data_sharding ................................... True   dataloader_type ................................. cyclic   decoder_num_layers .............................. None   decoder_seq_length .............................. None   delay_grad_reduce ............................... True   delay_param_gather .............................. False   dino_bottleneck_size ............................ 256   dino_freeze_last_layer .......................... 1   dino_head_hidden_size ........................... 2048   dino_local_crops_number ......................... 10   dino_local_img_size ............................. 96   dino_norm_last_layer ............................ False   dino_teacher_temp ............................... 0.07   dino_warmup_teacher_temp ........................ 0.04   dino_warmup_teacher_temp_epochs ................. 30   distribute_saved_activations .................... False   distributed_backend ............................. nccl   distributed_timeout_minutes ..................... 10   embedding_path .................................. None   empty_unused_memory_level ....................... 0   enable_one_logger ............................... False   encoder_num_layers .............................. 64   encoder_seq_length .............................. 3400   end_weight_decay ................................ 0.01   eod_mask_loss ................................... False   eval_interval ................................... 10000   eval_iters ...................................... 1   evidence_data_path .............................. None   exit_duration_in_mins ........................... None   exit_interval ................................... None   exit_on_missing_checkpoint ...................... False   exit_signal_handler ............................. False   expert_model_parallel_size ...................... 1   ffn_hidden_size ................................. 8192   finetune ........................................ True   fp16 ............................................ False   fp16_lm_cross_entropy ........................... False   fp32_residual_connection ........................ False   fp8 ............................................. None   fp8_amax_compute_algo ........................... most_recent   fp8_amax_history_len ............................ 1   fp8_interval .................................... 1   fp8_margin ...................................... 0   fp8_wgrad ....................................... True   global_batch_size ............................... 120   gradient_accumulation_fusion .................... True   group_query_attention ........................... False   head_lr_mult .................................... 1.0   hidden_dropout .................................. 0.1   hidden_size ..................................... 3072   hysteresis ...................................... 2   ict_head_size ................................... None   ict_load ........................................ None   img_h ........................................... 224   img_w ........................................... 224   indexer_batch_size .............................. 128   indexer_log_interval ............................ 1000   inference_batch_times_seqlen_threshold .......... 512   init_method_std ................................. 0.02   init_method_xavier_uniform ...................... False   initial_loss_scale .............................. 8192.0   iter_per_epoch .................................. 1250   kv_channels ..................................... 96   lazy_mpu_init ................................... None   load ............................................ /data3/www/checkpoints/dones/10b_4_1   local_rank ...................................... 0   log_batch_size_to_tensorboard ................... True   log_interval .................................... 10   log_learning_rate_to_tensorboard ................ True   log_loss_scale_to_tensorboard ................... True   log_memory_to_tensorboard ....................... False   log_num_zeros_in_grad ........................... False   log_params_norm ................................. False   log_progress .................................... False   log_throughput .................................. False   log_timers_to_tensorboard ....................... True   log_validation_ppl_to_tensorboard ............... True   log_world_size_to_tensorboard ................... False   loss_scale ...................................... None   loss_scale_window ............................... 1000   lr .............................................. 0.0001   lr_decay_iters .................................. 320000   lr_decay_samples ................................ None   lr_decay_style .................................. cosine   lr_warmup_fraction .............................. None   lr_warmup_init .................................. 0.0   lr_warmup_iters ................................. 0   lr_warmup_samples ............................... 0   make_vocab_size_divisible_by .................... 128   manual_gc ....................................... False   manual_gc_eval .................................. True   manual_gc_interval .............................. 0   mask_factor ..................................... 1.0   mask_prob ....................................... 0.15   mask_type ....................................... random   masked_softmax_fusion ........................... True   max_position_embeddings ......................... 4000   max_tokens_to_oom ............................... 12000   merge_file ...................................... $MERGE_FILE   micro_batch_size ................................ 1   min_loss_scale .................................. 1.0   min_lr .......................................... 1e05   mock_data ....................................... False   moe_aux_loss_coeff .............................. 0.0   moe_grouped_gemm ................................ False   moe_input_jitter_eps ............................ None   moe_router_load_balancing_type .................. aux_loss   moe_router_topk ................................. 2   moe_token_dropping .............................. False   moe_z_loss_coeff ................................ None   nccl_communicator_config_path ................... None   no_load_optim ................................... None   no_load_rng ..................................... None   no_persist_layer_norm ........................... False   no_save_optim ................................... None   no_save_rng ..................................... None   norm_epsilon .................................... 1e05   normalization ................................... LayerNorm   num_attention_heads ............................. 32   num_channels .................................... 3   num_classes ..................................... 1000   num_experts ..................................... None   num_layers ...................................... 64   num_layers_per_virtual_pipeline_stage ........... None   num_query_groups ................................ 1   num_workers ..................................... 2   one_logger_entity ............................... hwinf_dcm   one_logger_project .............................. e2etracking   one_logger_run_name ............................. None   onnx_safe ....................................... None   openai_gelu ..................................... False   optimizer ....................................... adam   output_bert_embeddings .......................... False   overlap_grad_reduce ............................. False   overlap_p2p_comm ................................ False   overlap_param_gather ............................ False   override_opt_param_scheduler .................... False   params_dtype .................................... torch.bfloat16   patch_dim ....................................... 16   perform_initialization .......................... True   pipeline_model_parallel_size .................... 1   pipeline_model_parallel_split_rank .............. None   position_embedding_type ......................... rope   profile ......................................... False   profile_ranks ................................... [0]   profile_step_end ................................ 12   profile_step_start .............................. 10   query_in_block_prob ............................. 0.1   rampup_batch_size ............................... None   rank ............................................ 0   recompute_granularity ........................... selective   recompute_method ................................ None   recompute_num_layers ............................ None   reset_attention_mask ............................ False   reset_position_ids .............................. False   retriever_report_topk_accuracies ................ []   retriever_score_scaling ......................... False   retriever_seq_length ............................ 256   retro_add_retriever ............................. False   retro_attention_gate ............................ 1   retro_cyclic_train_iters ........................ None   retro_encoder_attention_dropout ................. 0.1   retro_encoder_hidden_dropout .................... 0.1   retro_encoder_layers ............................ 2   retro_num_neighbors ............................. 2   retro_num_retrieved_chunks ...................... 2   retro_return_doc_ids ............................ False   retro_verify_neighbor_count ..................... True   retro_workdir ................................... None   rotary_interleaved .............................. False   rotary_percent .................................. 1.0   rotary_seq_len_interpolation_factor ............. None   sample_rate ..................................... 1.0   save ............................................ /data3/www/checkpoints/dones/10b_4_2   save_interval ................................... 1000   scatter_gather_tensors_in_pipeline .............. True   seed ............................................ 1234   seq_length ...................................... 3400   sequence_parallel ............................... True   sgd_momentum .................................... 0.9   short_seq_prob .................................. 0.1   skip_train ...................................... False   spec ............................................ ['local']   split ........................................... 999,1,1   squared_relu .................................... False   standalone_embedding_stage ...................... False   start_weight_decay .............................. 0.01   swiglu .......................................... True   swin_backbone_type .............................. tiny   tensor_model_parallel_size ...................... 2   tensorboard_dir ................................. /data3/www/checkpoints/dones/10b_4_1   tensorboard_log_interval ........................ 1   tensorboard_queue_size .......................... 1   test_data_path .................................. None   timing_log_level ................................ 0   timing_log_option ............................... minmax   titles_data_path ................................ None   tokenizer_model ................................. None   tokenizer_type .................................. t5   tp_comm_bulk_dgrad .............................. True   tp_comm_bulk_wgrad .............................. True   tp_comm_overlap ................................. False   tp_comm_overlap_cfg ............................. None   tp_comm_split_ag ................................ True   tp_comm_split_rs ................................ True   train_data_path ................................. None   train_iters ..................................... 500000   train_samples ................................... None   transformer_impl ................................ local   transformer_pipeline_model_parallel_size ........ 1   untie_embeddings_and_output_weights ............. True   use_checkpoint_args ............................. False   use_checkpoint_opt_param_scheduler .............. False   use_cpu_initialization .......................... None   use_distributed_optimizer ....................... True   use_flash_attn .................................. True   use_mcore_models ................................ False   use_one_sent_docs ............................... False   use_ring_exchange_p2p ........................... False   use_rotary_position_embeddings .................. True   valid_data_path ................................. None   variable_seq_lengths ............................ False   virtual_pipeline_model_parallel_size ............ None   vision_backbone_type ............................ vit   vision_pretraining .............................. False   vision_pretraining_type ......................... classify   vocab_extra_ids ................................. 0   vocab_file ...................................... $VOCAB_FILE   vocab_size ...................................... None   wandb_exp_name ..................................    wandb_project ...................................    wandb_save_dir ..................................    weight_decay .................................... 0.01   weight_decay_incr_style ......................... constant   world_size ...................................... 8  end of arguments  setting number of microbatches to constant 30 > building t5 tokenizer ... You are using the default legacy behaviour of the . This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565  > padded vocab (size: 32596) with 172 dummy tokens (new size: 32768) > initializing torch distributed ... > done: initializing torch distributed ... > initialized tensor model parallel with size 2 > initialized pipeline model parallel with size 1 > setting random seeds to 1234 ... > compiling dataset index builder ... make: Entering directory '/home/www/models/gpt/megatron_lm/megatron/core/datasets' make: Nothing to be done for 'default'. make: Leaving directory '/home/www/models/gpt/megatron_lm/megatron/core/datasets' >>> done with dataset index builder. Compilation time: 0.062 seconds > compiling and loading fused kernels ... psSYS420GPTNR:2816644:2816644 [0] NCCL INFO Bootstrap : Using eno1:192.168.223.11 psSYS420GPTNR:2816644:2816644 [0] NCCL INFO NET/Plugin : Plugin load (libncclnet.so) returned 2 : libncclnet.so: cannot open shared object file: No such file or directory psSYS420GPTNR:2816644:2816644 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation psSYS420GPTNR:2816644:2816644 [0] NCCL INFO cudaDriverVersion 12020 NCCL version 2.18.5+cuda11.8 psSYS420GPTNR:2816651:2816651 [7] NCCL INFO cudaDriverVersion 12020 psSYS420GPTNR:2816647:2816647 [3] NCCL INFO cudaDriverVersion 12020 psSYS420GPTNR:2816649:2816649 [5] NCCL INFO cudaDriverVersion 12020 psSYS420GPTNR:2816645:2816645 [1] NCCL INFO cudaDriverVersion 12020 psSYS420GPTNR:2816648:2816648 [4] NCCL INFO cudaDriverVersion 12020 psSYS420GPTNR:2816646:2816646 [2] NCCL INFO cudaDriverVersion 12020 psSYS420GPTNR:2816650:2816650 [6] NCCL INFO cudaDriverVersion 12020 psSYS420GPTNR:2816648:2816648 [4] NCCL INFO Bootstrap : Using eno1:192.168.223.11 psSYS420GPTNR:2816649:2816649 [5] NCCL INFO Bootstrap : Using eno1:192.168.223.11 psSYS420GPTNR:2816651:2816651 [7] NCCL INFO Bootstrap : Using eno1:192.168.223.11 psSYS420GPTNR:2816650:2816650 [6] NCCL INFO Bootstrap : Using eno1:192.168.223.11 psSYS420GPTNR:2816645:2816645 [1] NCCL INFO Bootstrap : Using eno1:192.168.223.11 psSYS420GPTNR:2816647:2816647 [3] NCCL INFO Bootstrap : Using eno1:192.168.223.11 psSYS420GPTNR:2816646:2816646 [2] NCCL INFO Bootstrap : Using eno1:192.168.223.11 psSYS420GPTNR:2816648:2816648 [4] NCCL INFO NET/Plugin : Plugin load (libncclnet.so) returned 2 : libncclnet.so: cannot open shared object file: No such file or directory psSYS420GPTNR:2816648:2816648 [4] NCCL INFO NET/Plugin : No plugin found, using internal implementation psSYS420GPTNR:2816651:2816651 [7] NCCL INFO NET/Plugin : Plugin load (libncclnet.so) returned 2 : libncclnet.so: cannot open shared object file: No such file or directory psSYS420GPTNR:2816651:2816651 [7] NCCL INFO NET/Plugin : No plugin found, using internal implementation psSYS420GPTNR:2816649:2816649 [5] NCCL INFO NET/Plugin : Plugin load (libncclnet.so) returned 2 : libncclnet.so: cannot open shared object file: No such file or directory psSYS420GPTNR:2816649:2816649 [5] NCCL INFO NET/Plugin : No plugin found, using internal implementation psSYS420GPTNR:2816650:2816650 [6] NCCL INFO NET/Plugin : Plugin load (libncclnet.so) returned 2 : libncclnet.so: cannot open shared object file: No such file or directory psSYS420GPTNR:2816650:2816650 [6] NCCL INFO NET/Plugin : No plugin found, using internal implementation psSYS420GPTNR:2816645:2816645 [1] NCCL INFO NET/Plugin : Plugin load (libncclnet.so) returned 2 : libncclnet.so: cannot open shared object file: No such file or directory psSYS420GPTNR:2816647:2816647 [3] NCCL INFO NET/Plugin : Plugin load (libncclnet.so) returned 2 : libncclnet.so: cannot open shared object file: No such file or directory psSYS420GPTNR:2816645:2816645 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation psSYS420GPTNR:2816647:2816647 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation psSYS420GPTNR:2816646:2816646 [2] NCCL INFO NET/Plugin : Plugin load (libncclnet.so) returned 2 : libncclnet.so: cannot open shared object file: No such file or directory psSYS420GPTNR:2816646:2816646 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation psSYS420GPTNR:2816647:2817099 [3] NCCL INFO NET/IB : No device found. psSYS420GPTNR:2816647:2817099 [3] NCCL INFO NET/Socket : Using [0]eno1:192.168.223.11 [1]usb0:169.254.3.1 psSYS420GPTNR:2816647:2817099 [3] NCCL INFO Using network Socket psSYS420GPTNR:2816648:2817097 [4] NCCL INFO NET/IB : No device found. psSYS420GPTNR:2816648:2817097 [4] NCCL INFO NET/Socket : Using [0]eno1:192.168.223.11 [1]usb0:169.254.3.1 psSYS420GPTNR:2816648:2817097 [4] NCCL INFO Using network Socket psSYS420GPTNR:2816644:2817094 [0] NCCL INFO NET/IB : No device found. psSYS420GPTNR:2816644:2817094 [0] NCCL INFO NET/Socket : Using [0]eno1:192.168.223.11 [1]usb0:169.254.3.1 psSYS420GPTNR:2816644:2817094 [0] NCCL INFO Using network Socket psSYS420GPTNR:2816651:2817095 [7] NCCL INFO NET/IB : No device found. psSYS420GPTNR:2816649:2817098 [5] NCCL INFO NET/IB : No device found. psSYS420GPTNR:2816651:2817095 [7] NCCL INFO NET/Socket : Using [0]eno1:192.168.223.11 [1]usb0:169.254.3.1 psSYS420GPTNR:2816651:2817095 [7] NCCL INFO Using network Socket psSYS420GPTNR:2816649:2817098 [5] NCCL INFO NET/Socket : Using [0]eno1:192.168.223.11 [1]usb0:169.254.3.1 psSYS420GPTNR:2816649:2817098 [5] NCCL INFO Using network Socket psSYS420GPTNR:2816645:2817101 [1] NCCL INFO NET/IB : No device found. psSYS420GPTNR:2816645:2817101 [1] NCCL INFO NET/Socket : Using [0]eno1:192.168.223.11 [1]usb0:169.254.3.1 psSYS420GPTNR:2816645:2817101 [1] NCCL INFO Using network Socket psSYS420GPTNR:2816646:2817100 [2] NCCL INFO NET/IB : No device found. psSYS420GPTNR:2816646:2817100 [2] NCCL INFO NET/Socket : Using [0]eno1:192.168.223.11 [1]usb0:169.254.3.1 psSYS420GPTNR:2816646:2817100 [2] NCCL INFO Using network Socket psSYS420GPTNR:2816650:2817096 [6] NCCL INFO NET/IB : No device found. psSYS420GPTNR:2816650:2817096 [6] NCCL INFO NET/Socket : Using [0]eno1:192.168.223.11 [1]usb0:169.254.3.1 psSYS420GPTNR:2816650:2817096 [6] NCCL INFO Using network Socket psSYS420GPTNR:2816650:2817096 [6] NCCL INFO comm 0xa49d350 rank 6 nranks 8 cudaDev 6 nvmlDev 8 busId d5000 commId 0xcb4c9dccde6a2dc9  Init START psSYS420GPTNR:2816649:2817098 [5] NCCL INFO comm 0xaadd840 rank 5 nranks 8 cudaDev 5 nvmlDev 6 busId d1000 commId 0xcb4c9dccde6a2dc9  Init START psSYS420GPTNR:2816648:2817097 [4] NCCL INFO comm 0xa5829e0 rank 4 nranks 8 cudaDev 4 nvmlDev 5 busId ce000 commId 0xcb4c9dccde6a2dc9  Init START psSYS420GPTNR:2816647:2817099 [3] NCCL INFO comm 0xa019500 rank 3 nranks 8 cudaDev 3 nvmlDev 4 busId 57000 commId 0xcb4c9dccde6a2dc9  Init START psSYS420GPTNR:2816646:2817100 [2] NCCL INFO comm 0xb59fb40 rank 2 nranks 8 cudaDev 2 nvmlDev 3 busId 56000 commId 0xcb4c9dccde6a2dc9  Init START psSYS420GPTNR:2816645:2817101 [1] NCCL INFO comm 0xa0f80d0 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 52000 commId 0xcb4c9dccde6a2dc9  Init START psSYS420GPTNR:2816651:2817095 [7] NCCL INFO comm 0x9aa9c50 rank 7 nranks 8 cudaDev 7 nvmlDev 9 busId d6000 commId 0xcb4c9dccde6a2dc9  Init START psSYS420GPTNR:2816644:2817094 [0] NCCL INFO comm 0x9a01be0 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 4f000 commId 0xcb4c9dccde6a2dc9  Init START psSYS420GPTNR:2816644:2817094 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to NVL psSYS420GPTNR:2816644:2817094 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,00000000,ffffffff psSYS420GPTNR:2816645:2817101 [1] NCCL INFO NCCL_P2P_LEVEL set by environment to NVL psSYS420GPTNR:2816645:2817101 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,00000000,ffffffff psSYS420GPTNR:2816646:2817100 [2] NCCL INFO NCCL_P2P_LEVEL set by environment to NVL psSYS420GPTNR:2816650:2817096 [6] NCCL INFO NCCL_P2P_LEVEL set by environment to NVL psSYS420GPTNR:2816650:2817096 [6] NCCL INFO Setting affinity for GPU 8 to ffffffff,00000000,ffffffff,00000000 psSYS420GPTNR:2816646:2817100 [2] NCCL INFO Setting affinity for GPU 3 to ffffffff,00000000,ffffffff psSYS420GPTNR:2816647:2817099 [3] NCCL INFO NCCL_P2P_LEVEL set by environment to NVL psSYS420GPTNR:2816647:2817099 [3] NCCL INFO Setting affinity for GPU 4 to ffffffff,00000000,ffffffff psSYS420GPTNR:2816649:2817098 [5] NCCL INFO NCCL_P2P_LEVEL set by environment to NVL psSYS420GPTNR:2816651:2817095 [7] NCCL INFO NCCL_P2P_LEVEL set by environment to NVL psSYS420GPTNR:2816649:2817098 [5] NCCL INFO Setting affinity for GPU 6 to ffffffff,00000000,ffffffff,00000000 psSYS420GPTNR:2816648:2817097 [4] NCCL INFO NCCL_P2P_LEVEL set by environment to NVL psSYS420GPTNR:2816651:2817095 [7] NCCL INFO Setting affinity for GPU 9 to ffffffff,00000000,ffffffff,00000000 psSYS420GPTNR:2816648:2817097 [4] NCCL INFO Setting affinity for GPU 5 to ffffffff,00000000,ffffffff,00000000 psSYS420GPTNR:2816645:2817101 [1] NCCL INFO Trees [0] 2/1/1>1>0 [1] 2/1/1>1>0 psSYS420GPTNR:2816651:2817095 [7] NCCL INFO Trees [0] 1/1/1>7>6 [1] 1/1/1>7>6 psSYS420GPTNR:2816645:2817101 [1] NCCL INFO P2P Chunksize set to 524288 psSYS420GPTNR:2816651:2817095 [7] NCCL INFO P2P Chunksize set to 524288 psSYS420GPTNR:2816650:2817096 [6] NCCL INFO Trees [0] 7/1/1>6>5 [1] 7/1/1>6>5 psSYS420GPTNR:2816646:2817100 [2] NCCL INFO Trees [0] 3/1/1>2>1 [1] 3/1/1>2>1 psSYS420GPTNR:2816650:2817096 [6] NCCL INFO P2P Chunksize set to 524288 psSYS420GPTNR:2816649:2817098 [5] NCCL INFO Trees [0] 6/1/1>5>4 [1] 6/1/1>5>4 psSYS420GPTNR:2816646:2817100 [2] NCCL INFO P2P Chunksize set to 524288 psSYS420GPTNR:2816648:2817097 [4] NCCL INFO Trees [0] 5/1/1>4>3 [1] 5/1/1>4>3 psSYS420GPTNR:2816649:2817098 [5] NCCL INFO P2P Chunksize set to 524288 psSYS420GPTNR:2816648:2817097 [4] NCCL INFO P2P Chunksize set to 524288 psSYS420GPTNR:2816647:2817099 [3] NCCL INFO Trees [0] 4/1/1>3>2 [1] 4/1/1>3>2 psSYS420GPTNR:2816647:2817099 [3] NCCL INFO P2P Chunksize set to 524288 psSYS420GPTNR:2816644:2817094 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7 psSYS420GPTNR:2816644:2817094 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7 psSYS420GPTNR:2816644:2817094 [0] NCCL INFO Trees [0] 1/1/1>0>1 [1] 1/1/1>0>1 psSYS420GPTNR:2816644:2817094 [0] NCCL INFO P2P Chunksize set to 524288 psSYS420GPTNR:2816651:2817095 [7] NCCL INFO Channel 00 : 7[9] > 0[0] via SHM/direct/direct psSYS420GPTNR:2816645:2817101 [1] NCCL INFO Channel 00 : 1[1] > 2[3] via SHM/direct/direct psSYS420GPTNR:2816647:2817099 [3] NCCL INFO Channel 00 : 3[4] > 4[5] via SHM/direct/direct psSYS420GPTNR:2816651:2817095 [7] NCCL INFO Channel 01 : 7[9] > 0[0] via SHM/direct/direct psSYS420GPTNR:2816645:2817101 [1] NCCL INFO Channel 01 : 1[1] > 2[3] via SHM/direct/direct psSYS420GPTNR:2816647:2817099 [3] NCCL INFO Channel 01 : 3[4] > 4[5] via SHM/direct/direct psSYS420GPTNR:2816649:2817098 [5] NCCL INFO Channel 00 : 5[6] > 6[8] via SHM/direct/direct psSYS420GPTNR:2816649:2817098 [5] NCCL INFO Channel 01 : 5[6] > 6[8] via SHM/direct/direct psSYS420GPTNR:2816648:2817097 [4] NCCL INFO Channel 00/0 : 4[5] > 5[6] via P2P/IPC psSYS420GPTNR:2816646:2817100 [2] NCCL INFO Channel 00/0 : 2[3] > 3[4] via P2P/IPC psSYS420GPTNR:2816644:2817094 [0] NCCL INFO Channel 00/0 : 0[0] > 1[1] via P2P/IPC psSYS420GPTNR:2816648:2817097 [4] NCCL INFO Channel 01/0 : 4[5] > 5[6] via P2P/IPC psSYS420GPTNR:2816650:2817096 [6] NCCL INFO Channel 00/0 : 6[8] > 7[9] via P2P/IPC psSYS420GPTNR:2816646:2817100 [2] NCCL INFO Channel 01/0 : 2[3] > 3[4] via P2P/IPC psSYS420GPTNR:2816644:2817094 [0] NCCL INFO Channel 01/0 : 0[0] > 1[1] via P2P/IPC psSYS420GPTNR:2816650:2817096 [6] NCCL INFO Channel 01/0 : 6[8] > 7[9] via P2P/IPC psSYS420GPTNR:2816648:2817097 [4] NCCL INFO Connected all rings psSYS420GPTNR:2816650:2817096 [6] NCCL INFO Connected all rings psSYS420GPTNR:2816648:2817097 [4] NCCL INFO Channel 00 : 4[5] > 3[4] via SHM/direct/direct psSYS420GPTNR:2816648:2817097 [4] NCCL INFO Channel 01 : 4[5] > 3[4] via SHM/direct/direct psSYS420GPTNR:2816644:2817094 [0] NCCL INFO Connected all rings psSYS420GPTNR:2816649:2817098 [5] NCCL INFO Connected all rings psSYS420GPTNR:2816650:2817096 [6] NCCL INFO Channel 00 : 6[8] > 5[6] via SHM/direct/direct psSYS420GPTNR:2816646:2817100 [2] NCCL INFO Connected all rings psSYS420GPTNR:2816651:2817095 [7] NCCL INFO Connected all rings psSYS420GPTNR:2816651:2817095 [7] NCCL INFO Channel 00/0 : 7[9] > 6[8] via P2P/IPC psSYS420GPTNR:2816650:2817096 [6] NCCL INFO Channel 01 : 6[8] > 5[6] via SHM/direct/direct psSYS420GPTNR:2816647:2817099 [3] NCCL INFO Connected all rings psSYS420GPTNR:2816645:2817101 [1] NCCL INFO Connected all rings psSYS420GPTNR:2816651:2817095 [7] NCCL INFO Channel 01/0 : 7[9] > 6[8] via P2P/IPC psSYS420GPTNR:2816651:2817095 [7] NCCL INFO Connected all trees psSYS420GPTNR:2816651:2817095 [7] NCCL INFO threadThresholds 8/8/64  512 psSYS420GPTNR:2816647:2822081 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer psSYS420GPTNR:2816649:2822080 [5] NCCL INFO comm 0x241e5240 rank 2 nranks 4 cudaDev 5 nvmlDev 6 busId d1000 commId 0x5d713e669f1b99f1  Init COMPLETE psSYS420GPTNR:2816651:2822079 [7] NCCL INFO comm 0x2647dab0 rank 3 nranks 4 cudaDev 7 nvmlDev 9 busId d6000 commId 0x5d713e669f1b99f1  Init COMPLETE psSYS420GPTNR:2816645:2822078 [1] NCCL INFO comm 0x1ddf8f60 rank 0 nranks 4 cudaDev 1 nvmlDev 1 busId 52000 commId 0x5d713e669f1b99f1  Init COMPLETE psSYS420GPTNR:2816647:2822081 [3] NCCL INFO comm 0x237d3820 rank 1 nranks 4 cudaDev 3 nvmlDev 4 busId 57000 commId 0x5d713e669f1b99f1  Init COMPLETE psSYS420GPTNR:2816644:2816644 [0] NCCL INFO comm 0x1aafabb0 rank 0 nranks 4 cudaDev 0 busId 4f000  Abort COMPLETE psSYS420GPTNR:2816644:2821495 [0] NCCL INFO [Service thread] Connection closed by localRank 0 psSYS420GPTNR:2816644:2816644 [0] NCCL INFO comm 0x219770f0 rank 0 nranks 2 cudaDev 0 busId 4f000  Abort COMPLETE psSYS420GPTNR:2816644:2817122 [0] NCCL INFO [Service thread] Connection closed by localRank 0 psSYS420GPTNR:2816644:2816644 [0] NCCL INFO comm 0x9a01be0 rank 0 nranks 8 cudaDev 0 busId 4f000  Abort COMPLETE /home/www/anaconda3/envs/cuda11/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown   warnings.warn('resource_tracker: There appear to be %d ' [20240301 01:02:57,902] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2816644 [20240301 01:02:57,904] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2816645 [20240301 01:02:58,225] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2816646 /home/www/anaconda3/envs/cuda11/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 16 leaked semaphore objects to clean up at shutdown   warnings.warn('resource_tracker: There appear to be %d ' [20240301 01:02:59,101] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2816647 [20240301 01:02:59,379] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2816648 /home/www/anaconda3/envs/cuda11/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 16 leaked semaphore objects to clean up at shutdown   warnings.warn('resource_tracker: There appear to be %d ' [20240301 01:03:00,320] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2816649 [20240301 01:03:00,640] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2816650 /home/www/anaconda3/envs/cuda11/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 16 leaked semaphore objects to clean up at shutdown   warnings.warn('resource_tracker: There appear to be %d ' [20240301 01:03:01,528] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2816651 [20240301 01:03:01,846] [ERROR] [launch.py:321:sigkill_handler] ['/home/www/anaconda3/envs/cuda11/bin/python', 'u', '/home/www/models/gpt/megatron_lm/trains/train_scaled_v55.py', 'local_rank=7', 'tensormodelparallelsize', '2', 'sequenceparallel', 'useflashattn', 'optimizer', 'adam', 'recomputeactivations', 'numlayers', '64', 'hiddensize', '3072', 'numattentionheads', '32', 'seqlength', '3400', 'maxpositionembeddings', '4000', 'microbatchsize', '1', 'globalbatchsize', '120', 'lr', '0.0001', 'trainiters', '500000', 'lrdecayiters', '320000', 'lrdecaystyle', 'cosine', 'minlr', '1.0e5', 'weightdecay', '1e2', 'clipgrad', '1.0', 'bf16', 'datapath', '$DATA_PATH', 'vocabfile', '$VOCAB_FILE', 'mergefile', '$MERGE_FILE', 'split', '999,1,1', 'loginterval', '10', 'saveinterval', '1000', 'evalinterval', '10000', 'evaliters', '1', 'tokenizertype', 't5', 'untieembeddingsandoutputweights', 'userotarypositionembeddings', 'swiglu', 'save', '/data3/www/checkpoints/dones/10b_4_2', 'dataloadertype', 'cyclic', 'load', '/data3/www/checkpoints/dones/10b_4_1', 'finetune', 'initiallossscale', '8192', 'tensorboardqueuesize', '1', 'logtimerstotensorboard', 'logbatchsizetotensorboard', 'logvalidationppltotensorboard', 'tensorboarddir', '/data3/www/checkpoints/dones/10b_4_1', 'usedistributedoptimizer', 'spec', 'local'] exits with return code = 1 ```",can you run with the nccl debug flag above. Also are those no_transformer_engine prints from your code?
Hongjie1Chu,"What do you mean by ""subgraphs"" and ""relevant files""?",,2023-11-12T08:16:53Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/587
Hongjie1Chu,Inter-process and Inter-layer Communication in Model Parallelism within Megatron,"In the context of model parallelism, each layer's input for training is derived from the output of the previous layer. How is this information passed from one layer to the next? Moreover, after an individual process completes its training during an iteration, how does it communicate with other processes within the same tensor parallelism group? For instance, in the train() method within the training.py file, there is a loop while iteration < args.train_iters. After the current process finishes its training iteration, which specific interface does it use to communicate with other processes within the same tensor parallelism group? Additionally, which interface is used for passing the output to the subsequent layer? I'm keen to understand the underlying mechanisms for both interprocess communication and the transfer of results between layers in the Megatron framework. Any insights or pointers towards relevant sections of the codebase would be greatly appreciated. Thank you for your assistance.",2023-11-12T05:29:21Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/586,"Specifically, referring to the model parallelism diagram, how do processes 3 and 4 obtain the computation results from processes 1 and 2 respectively? After 3 and 4 complete their individual computations, how do they communicate with each other? Furthermore, after this communication, which interface is used to pass the results to processes 5 and 6? !v2708c01105de92567824bd9d3456b9459_720w","(Copying partially from a comment on another issue) The tensorparallel communication is embedded in the relevant linear layers: https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/tensor_parallel/layers.py. For example, rowparallel linear layers are implemented here: https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/tensor_parallel/layers.pyL764. In the corresponding forward method, you can see the relevant communication calls: https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/tensor_parallel/layers.pyL933L936. The pipelineparallel communication is embedded in schedules.py: https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/pipeline_parallel/schedules.pyL1042. This method has relevant communication methods like `recv_forward`, `send_forward`, etc. Hope this helps, let me know if you still have questions.","Closing, feel free to reopen if you still have questions."
jrt-20,How Megatron trains multiple subgraphs on multiple GPUs and where the relevant files are,**Your question** ```[tasklist]  Tasks ``` ```[tasklist]  Tasks ```,2023-11-11T05:16:08Z,,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/585,"What do you mean by ""subgraphs"" and ""relevant files""?","In the context of model parallelism, each layer's input for training is derived from the output of the previous layer. How is this information passed from one layer to the next? Moreover, after an individual process completes its training during an iteration, how does it communicate with other processes within the same tensor parallelism group? For instance, in the train() method within the training.py file, there is a loop while iteration < args.train_iters. After the current process finishes its training iteration, which specific interface does it use to communicate with other processes within the same tensor parallelism group? Additionally, which interface is used for passing the output to the subsequent layer?","The tensorparallel communication is embedded in the relevant linear layers: https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/tensor_parallel/layers.py. For example, rowparallel linear layers are implemented here: https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/tensor_parallel/layers.pyL764.  In the corresponding `forward` method, you can see the relevant communication calls: https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/tensor_parallel/layers.pyL933L936. Hope this helps, let me know if you still have questions.","> The tensorparallel communication is embedded in the relevant linear layers: https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/tensor_parallel/layers.py. For example, rowparallel linear layers are implemented here: https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/tensor_parallel/layers.pyL764. In the corresponding `forward` method, you can see the relevant communication calls: https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/tensor_parallel/layers.pyL933L936. >  > Hope this helps, let me know if you still have questions. Thanks, lgtm!"
nikit-srivastava,[QUESTION] How to preprocess data for lustre file system?,"When I train a megatron GPT model on computing cluster with lustre file system, I observe frequent fileopen and fileclose operations (which causes performance degradation in lustre) even though the read/write bandwidth is barely utilized for them. !image I preprocess data using mmap, as described in the readme (https://github.com/NVIDIA/MegatronLMdatapreprocessing). What would be an ideal way to resolve this issue?",2023-11-10T22:03:54Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/584,"On further analysis, we discovered that the frequent opening/closing requests were made to the temporary files. This can be solved by placing those temporary files on nonlustre directory such as `$HOME`."
loubnabnl,Add file level fim,,2023-11-10T13:30:53Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/583
tailover,RL PPO Algorithm Support,"Hi, dou you have a plan to support the RL algorithm to finetune the base model? ",2023-11-10T07:07:51Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/582,Marking as stale. No activity in 60 days.
xealml,[QUESTION]How to serving Llama2,"**Your question** Ask a clear and concise question about MegatronLM. in this doc  https://github.com/NVIDIA/MegatronLM/blob/main/docs/llama2.mdlaunchmegatron says  If loading for either inference or finetuning, use the following arguments: which  python file is for config infer ```[tasklist]  Tasks ```",2023-11-10T06:23:52Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/581,??? same question 
Richie-yan,Clip_grad_norm when using use-distributed-optimizer,"When using clip_grad_norm, the model will execute the get_parameters function, which fetches parameters from optimizer.param_groups, and then takes out the corresponding gradient for norm. However, when using usedistributedoptimizer, optimizer.param_groups is sharded according to data_parallel. In this case, the gradient obtained is no longer global compared to when not using usedistributedoptimizer.  This seems to be a problem?",2023-11-09T04:26:48Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/580,distributed optimizer overried get_model_parallel_group 
xealml,[QUESTION]how to inference on llama2 with megatron-lm,**Your question** Ask a clear and concise question about MegatronLM.,2023-11-08T14:23:43Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/579,how to ? any ideas 
zhangsheng377,Fix main_grad,"When the DDP class creates the main_grad attribute, it is judged that only the params that required_grad are created. However, when doing allreduce here, it was used directly without judging whether weight has the main_grad attribute, causing an exception. In addition, distributed layers such as ColumnParallelLinear use super for initialization, which will lead to circular dependencies when using Lora. Lora is initialized by passing the class name with self.",2023-11-07T09:47:06Z,stale,open,1,2,https://github.com/NVIDIA/Megatron-LM/issues/578,    ,Marking as stale. No activity in 60 days.
janelu9,[BUG]ModuleNotFoundError: No module named 'transformer_engine',"``` from megatron.core.transformer.custom_layers.transformer_engine import (   TEDotProductAttention,   TELayerNormColumnParallelLinear,   TERowParallelLinear,   ) ```",2023-11-07T03:41:13Z,,closed,0,9,https://github.com/NVIDIA/Megatron-LM/issues/577,How are you running the code? Are you using a Docker container?,"Hi, I'm facing the same issue running the preprocessing code natively on my machine. CUDA 11.8 Python 3.11.5 Rocky Linux 9.2 ``` python3 tools/preprocess_data.py  input processed_afr.json \ outputprefix mygpt2 \ vocabfile gpt2vocab.json \ tokenizertype GPT2BPETokenizer \ mergefile gpt2merges.txt \ appendeod ``` results in  ``` Zarrbased strategies will not be registered because of missing packages Traceback (most recent call last):   File ""/home/aszablew/multilingualllmsexperiments/MegatronLM/tools/preprocess_data.py"", line 23, in      from megatron.tokenizer import build_tokenizer   File ""/home/aszablew/multilingualllmsexperiments/MegatronLM/megatron/__init__.py"", line 15, in      from .initialize  import initialize_megatron   File ""/home/aszablew/multilingualllmsexperiments/MegatronLM/megatron/initialize.py"", line 18, in      from megatron.arguments import parse_args, validate_args   File ""/home/aszablew/multilingualllmsexperiments/MegatronLM/megatron/arguments.py"", line 16, in      from megatron.core.models.retro import RetroConfig   File ""/home/aszablew/multilingualllmsexperiments/MegatronLM/megatron/core/models/retro/__init__.py"", line 4, in      from .decoder_spec import get_retro_decoder_block_spec   File ""/home/aszablew/multilingualllmsexperiments/MegatronLM/megatron/core/models/retro/decoder_spec.py"", line 5, in      from megatron.core.models.gpt.gpt_layer_specs import (   File ""/home/aszablew/multilingualllmsexperiments/MegatronLM/megatron/core/models/gpt/__init__.py"", line 1, in      from .gpt_model import GPTModel   File ""/home/aszablew/multilingualllmsexperiments/MegatronLM/megatron/core/models/gpt/gpt_model.py"", line 15, in      from megatron.core.transformer.transformer_block import TransformerBlock   File ""/home/aszablew/multilingualllmsexperiments/MegatronLM/megatron/core/transformer/transformer_block.py"", line 13, in      from megatron.core.transformer.custom_layers.transformer_engine import TENorm   File ""/home/aszablew/multilingualllmsexperiments/MegatronLM/megatron/core/transformer/custom_layers/transformer_engine.py"", line 6, in      import transformer_engine as te ModuleNotFoundError: No module named 'transformer_engine' ``` I'd really appreciate some support with it! Thanks!","You need TransformerEngine too. Is it possible for you to use a docker container? For example, our code should work with this PyTorch container: https://docs.nvidia.com/deeplearning/frameworks/pytorchreleasenotes/rel2304.html.","Thank you . I'm afraid I won't be able to use the docker container due to some limitations of the shared system I'm using.  In this case, would it help to install TransformerEngine following:  https://docs.nvidia.com/deeplearning/transformerengine/userguide/installation.html ?","Yes, we have done some testing of the latest TE release (https://github.com/nvidia/transformerEngine/tree/release_v1.1) with our `main` branch, so I recommend trying that.","Thanks, will give it a go! ðŸš€",I managed to run the docker container with singularity. Works perfectly fine  thanks again!,"Ok, great! Going to close this.",who can share a wheel installation package here?
sbmaruf,Add Llama2Tokenizer in the data processing tools,Please check the simple edits.,2023-11-06T17:20:11Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/576,Marking as stale. No activity in 60 days.,updated in the new version.
aeeeeeep,fix synchronization issue when save checkpoint,,2023-11-06T15:33:13Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/575,Marking as stale. No activity in 60 days.
johndolgov,[QUESTION] Why rope_base is not an aruments' parameter like in HF?,"Why is not rope_base an arguments' parameter like in HF? I see in HF parameter rope_theta/rope_base, should we add the same in MegatronLM?",2023-11-06T12:25:29Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/574,Marking as stale. No activity in 60 days.,It is now
610lyn,Update saver_megatron to support setting output virtual pipeline parallel size.,"Currently, when we use tools/checkpoint/util.py to convert a checkpoint, we cannot specify the output virtual pipeline parallel world size. I implemented this function by update saver_megatron.py.",2023-11-06T03:23:00Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/573,Marking as stale. No activity in 60 days.
sbmaruf,Update training.py,Naming consistency of the variable. Please check the simple edits.   ,2023-11-04T21:12:14Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/572,Marking as stale. No activity in 60 days.
Neo-Zhangjiajie,Fix error in pipeline-parallel schedules to support context parallel,The original `get_tensor_shapes()` in `megatron/core/pipeline_parallel/schedules.py` doesn't consider context parallel size.,2023-11-04T14:48:39Z,stale,open,1,3,https://github.com/NVIDIA/Megatron-LM/issues/571,Marking as stale. No activity in 60 days.,why this merging is blockedï¼Ÿ,Marking as stale. No activity in 60 days.
SeaOfOcean,Fix not contiguous error when generation with return_output_log_probs=True,"Fix not contiguous error when generation with return_output_log_probs=True and  pipeline_model_parallel_size>1 In `generation.generate_tokens_probs_and_return_on_first_stage` , when `return_output_log_probs=True`,  `output_log_probs[:, :context_length]` makes it not contiguous, which raise nonecontiguous error in the following `broadcast_from_last_to_first_pipeline_stage`, which requires the tensor to be contiguous.",2023-11-03T09:18:19Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/570, could you please help review the fix?, ,Marking as stale. No activity in 60 days.
Kingsleyandher,Update cross_entropy.py,The inplace operation reduces intermediate memory usage.,2023-11-02T03:06:47Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/569
mayank31398,fix repeat_interleave,related: https://github.com/NVIDIA/MegatronLM/issues/543,2023-11-01T09:45:48Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/568,Is this behavior of automatic repetition implemented in any FlashAttention2 version?,Marking as stale. No activity in 60 days.
Davidgzx,We Need A NEW RELEASE,"The last release is about 4 month ago. There are many updates since that time, Could you please tag a new stable release",2023-11-01T09:23:42Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/567,Marking as stale. No activity in 60 days.
huangyf530,"[BUG] SwitchMLP is not compatible with distributed_optimizer, but there is no assertion.","**Describe the bug** SwitchMLP used with distributed_optimizer will cause parameters in SwitchMLP not updated by optimizer. The **main_grad of parameters in SwitchMLP is stored in discreate buffers which are not in model.grad_buffers**, and therefore they will not be used in distributed_optimizer and will never update during training process. **To Reproduce** Add following arguments in training script: ```shell bf16 \ numexperts 4 \ numexpertlayers 2 \ expertmodelparallelsize 2 \ usedistributedoptimizer \ ``` Then you can compare parameters of SwitchNLP among checkpoints for different iterations. **Expected behavior** The paramsters should be different among different iterations. **Stack trace/logs** N/A **Environment (please complete the following information):**   MegatronLM commit ID   PyTorch version   CUDA version   NCCL version **Proposed fix** 1. Maybe we can add parameters of SwichMLP into model's grad_buffers but in different buckets with other parameters. And when all_reduce/reduce_scatter model's gradients, the used parallel_group should be different for different buckets (data_parallel_group for dense parameters and data_modulo_expert_parallel_group for parameters in expert). I can imagine this will still nedd other modifications on distributed_optimizer to make it work. And I am very happy to make my contirbutions to solve this problem. 2. Another simple method may disable distributed_optimizer when SwitchMLP is used. **Additional context** N/A",2023-10-30T13:10:43Z,stale,closed,2,9,https://github.com/NVIDIA/Megatron-LM/issues/565, Thank for reporting the issue. Could you please provide the parallel setting in your case?," I run it on a toy model with llama's tokenizer on 8 A800 GPU.  Some related arguments are in the following: ```shell DISTRIBUTED_ARGS="" \        tensormodelparallelsize 2 \        pipelinemodelparallelsize 1 \        distributedbackend nccl \        usedistributedoptimizer \        "" NETWORK_SIZE_ARGS="" \        numlayers 4 \        hiddensize 128 \        numattentionheads 4 \        groupqueryattention \        numquerygroups 4 \        ffnhiddensize 512 \        positionembeddingtype rope \        maxpositionembeddings 4096 \        makevocabsizedivisibleby 1 \        normepsilon 1e5 \        normalization RMSNorm \        swiglu \        untieembeddingsandoutputweights \        useflashattn \        sequenceparallel \        "" REGULATIZATION_ARGS="" \        attentiondropout 0.1 \        hiddendropout 0.1 \        weightdecay 1e1 \        clipgrad 1.0 \        adambeta1 0.9 \        adambeta2 0.95 \        adameps 1e8 \        "" TRAINING_ARGS="" \        microbatchsize 1 \        globalbatchsize 32 \        trainiters 10000 \        loginterval 1 \        disablebiaslinear \        nobiasgelufusion \        optimizer adam \        exitinterval 10 \        "" MIXED_PRECISION_ARGS="" \        bf16 \        noquerykeylayerscaling \        "" MOE_ARGS=""        numexperts 4 \        expertmodelparallelsize 2 \        "" ```","Thanks for the response!  I believe the issue lies in Megatron does not support DP for experts at present (DP size should <= EP size). In your situation, you can utilize the command `expertmodelparallelsize 4` to run the model."," Thank you for your response! I have tried to utilize `expertmodelparallelsize 4`.  But the parameters of SwithMLP still don't update when I use `distributedoptimizer`. And when I remove `distributedoptimizer`, the parameters can be updated. I use the following code to compare experts in different checkpoints. ```python import torch filepath1 = ""checkpoint/debug_moe/iter_0000005/mp_rank_00_000/model_optim_rng.pt"" filepath2 = ""checkpoint/debug_moe/iter_0000010/mp_rank_00_000/model_optim_rng.pt"" model1 = torch.load(filepath1, map_location='cpu')['model']['language_model']['encoder'] model2 = torch.load(filepath2, map_location='cpu')['model']['language_model']['encoder'] print(model1.keys())  compare router router1 = model1['layers.0.mlp.router.weight'] router2 = model2['layers.0.mlp.router.weight'] print(""layer 0 router"") print(torch.max(torch.abs(router1  router2)))  compare expert expert1 = model1['layers.0.mlp.local_experts.0.dense_h_to_4h.weight'] expert2 = model2['layers.0.mlp.local_experts.0.dense_h_to_4h.weight'] print(""layer 0 expert"") print(torch.max(torch.abs(expert1  expert2)))  attention dense mlp1 = model1['layers.0.self_attention.dense.weight'] mlp2 = model2['layers.0.self_attention.dense.weight'] print(""layer 0 attention dense"") print(torch.max(torch.abs(mlp1  mlp2)))  attention qkv mlp1 = model1['layers.0.self_attention.query_key_value.weight'] mlp2 = model2['layers.0.self_attention.query_key_value.weight'] print(""layer 0 attention qkv"") print(torch.max(torch.abs(mlp1  mlp2))) ``` With `distributedoptimizer`, I will get the following output: ``` layer 0 router tensor(1.9073e06, dtype=torch.bfloat16) layer 0 expert tensor(0., dtype=torch.bfloat16) layer 0 attention dense tensor(1.9073e06, dtype=torch.bfloat16) layer 0 attention qkv tensor(1.9073e06, dtype=torch.bfloat16) ``` Without `distributedoptimizer` will get the following output: ``` layer 0 router tensor(1.9073e06, dtype=torch.bfloat16) layer 0 expert tensor(1.9073e06, dtype=torch.bfloat16) layer 0 attention dense tensor(1.9073e06, dtype=torch.bfloat16) layer 0 attention qkv tensor(1.9073e06, dtype=torch.bfloat16) ```",Marking as stale. No activity in 60 days.,"Is this still a problem? In the newly released documentation, I found that the example for training Mixtral uses the following parameters: ``` expertmodelparallelsize usedistributedoptimizer ``` If it remains an issue, it could be quite misleading.","> Is this still a problem? In the newly released documentation, I found that the example for training Mixtral uses the following parameters: >  > ``` > expertmodelparallelsize > usedistributedoptimizer > ``` >  > If it remains an issue, it could be quite misleading. Thank you for the reminder, DistOpt should now works well with MoE, including Expert Parallelism.","> > Is this still a problem? In the newly released documentation, I found that the example for training Mixtral uses the following parameters: > > ``` > > expertmodelparallelsize > > usedistributedoptimizer > > ``` > >  > >  > >      > >        > >      > >  > >        > >      > >  > >      > >    > > If it remains an issue, it could be quite misleading. >  > Thank you for the reminder, DistOpt should now works well with MoE, including Expert Parallelism. Oh, I see. Thank you for clarifying. Could I also ask why this issue and the related issues haven't been closed yet?",Marking as stale. No activity in 60 days.
jordgedu,'distributed-timeout-minutes' does not take effect,"**Is your feature request related to a problem? Please describe.**     I have observed that MegatronLM already includes the 'distributedtimeoutminutes' parameter, allowing users to set the timeout for PyTorch collective communication. However, it appears that this setting is not effective in many cases. The primary reason for this is that the 'distributedtimeoutminutes' parameter is not being passed to another widely used API, 'torch.distributed.new_group', which causes the user's specified timeout to have no effect. I kindly request prompt attention to address this issue. **Describe the solution you'd like**     To ensure that the distributedtimeoutminutes setting takes effect for both torch.distributed.new_group and torch.distributed.init_process_group, you need to pass it as a parameter to both functions. **Proposed implementation**  pass â€˜distributedtimeoutminutes â€™ as a parameter to â€˜torch.distributed.new_groupâ€™ and â€˜torch.distributed.init_process_groupâ€™.  https://github.com/NVIDIA/MegatronLM/blob/main/tests/unit_tests/test_parallel_state.py",2023-10-30T03:24:43Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/564,Marking as stale. No activity in 60 days.
wuyujiji,[QUESTION] Is there a plan to introduce torch.compile into Megatron?,**Your question** Is there a plan to introduce `torch.compile` into Megatron to speed up the training performance?,2023-10-30T02:14:30Z,stale,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/563,  I have the same question,Marking as stale. No activity in 60 days.,"+1, according to this, torch.compile can speed up the megatron server inference speed",Marking as stale. No activity in 60 days.
mayank31398,[QUESTION] Seeking documentation/paper for expert and context parallel,same as title,2023-10-26T17:30:37Z,,closed,9,3,https://github.com/NVIDIA/Megatron-LM/issues/562,Marking as stale. No activity in 60 days.,https://github.com/NVIDIA/NeMo/pull/9184 introduced some documentation into NeMo regarding these which are rendered here: https://docs.nvidia.com/nemoframework/userguide/latest/nemotoolkit/features/parallelisms.html,thanks
jiayulu,[QUESTION] how is the grad bucket size ensured to be a multiple of dp world size?,"  could you please point to the code that ensures the bucket size is divisible by the dp world size? from what i can see, a new bucket is created whenever the cumulative number of elements is bigger than or equal to the specified bucket size. ``` def shard_buffer(buffer):     """"""     Shard buffer into dp_size chunks of equal size.     """"""     data_parallel_world_size = parallel_state.get_data_parallel_world_size()     assert buffer.numel() % data_parallel_world_size == 0     < how is this ensured?     shard_size = buffer.numel() // data_parallel_world_size     sharded_buffer = [         buffer[(r * shard_size) : ((r + 1) * shard_size)] for r in range(data_parallel_world_size)     ]     return sharded_buffer ``` https://github.com/NVIDIA/MegatronLM/blob/feac76a79148622d8f2a45d46c08a972a24784a3/megatron/core/distributed.pyL18L28",2023-10-26T05:40:05Z,,closed,0,21,https://github.com/NVIDIA/Megatron-LM/issues/561,"There is no such code right now; we are depending on ""good numbers"" for this to hold (e.g., if dp_size is a power of 2, this usually will not be an issue). But the assertion is needed for correctness. I am working on a patch to fix this."," thanks for the quick reply. another question: before your recent change, the ` build_model_gbuf_range` function uses `grad_buffer.numel` to calculate each dp rank's range: ```     def build_model_gbuf_range(cls, model, dtype):         data_parallel_rank = mpu.get_data_parallel_rank()         data_parallel_world_size = mpu.get_data_parallel_world_size()          Grad buffer range.         grad_buffer = model.grad_buffers[dtype]         gbuf_size = grad_buffer.numel       < here         max_gbuf_range_size = int(math.ceil(gbuf_size / data_parallel_world_size))          All world ranges. (i.e., across all data parallel ranks)         gbuf_world_all_ranges = []         for r in range(data_parallel_world_size):             gbuf_world_start = r * max_gbuf_range_size             gbuf_world_end = min(gbuf_size, gbuf_world_start+max_gbuf_range_size)             gbuf_world_range = Range(gbuf_world_start, gbuf_world_end)             gbuf_world_all_ranges.append(gbuf_world_range)          Local DP's ranges.         gbuf_world_range = gbuf_world_all_ranges[data_parallel_rank]         gbuf_local_range = gbuf_world_range.normalize() ``` yet the `reduce_scatter` uses `grad_buffer.numel_padded` to split the grad buffer to dp ranks: ```     def _get_local_view(self, buf):         assert buf.numel() % self.data_parallel_size == 0         shard_size = buf.numel() // self.data_parallel_size    < here         return buf[             (self.data_parallel_rank * shard_size) : ((self.data_parallel_rank + 1) * shard_size)         ]     def all_reduce(self):         assert (             self.allreduce_handle is None and not self.allreduce_issued         ), 'Should not have multiple allreduces in flight at once'         self.data /= self.data_parallel_size          Use async_op only when overlap_grad_reduce is True.         if self.reduce_scatter:             local_data_view = self._get_local_view(self.data)             self.allreduce_handle = torch.distributed._reduce_scatter_base(                 local_data_view,                 self.data,                 group=self.data_parallel_group,                 async_op=self.overlap_grad_reduce,             )         else:             self.allreduce_handle = torch.distributed.all_reduce(                 self.data, group=self.data_parallel_group, async_op=self.overlap_grad_reduce             )         self.allreduce_issued = True ``` this would cause the optimize to use the wrong gradients to update the params it owns, is that right? https://github.com/NVIDIA/MegatronLM/blob/0203a13faddd1a91f8d9f53fd858d73e9d3b973e/megatron/optimizer/distrib_optimizer.pyL123L168 https://github.com/NVIDIA/MegatronLM/blob/0203a13faddd1a91f8d9f53fd858d73e9d3b973e/megatron/model/distributed.pyL78L104","`buf` here is each bucket's buffer actually. We ran extensive tests to check for correctness, so I think the implementation is correct (when the assertion you pointed out doesn't fail)."," it was the same before bucketing was added, so that too relies on `grad_buffer.numel_padded` being equal to `grad_buffer.numel`? `get_model_buffer_dp_views` uses `grad_buffer.numel_padded` https://github.com/NVIDIA/MegatronLM/blob/115b6776813eb182237da6c8b9c78c6ae05d8ab1/megatron/optimizer/distrib_optimizer.pyL791 `build_model_gbuf_range` uses `grad_buffer.numel` https://github.com/NVIDIA/MegatronLM/blob/115b6776813eb182237da6c8b9c78c6ae05d8ab1/megatron/optimizer/distrib_optimizer.pyL140", could you please comment on the question above?,"I had the same high level question as we're actually running into this being a problem (dp is not always a nice power of 2). But I don't understand the second line of questions, as the linked lines by  seem to both point to the numel ?",+1.,"In my scenario, dp_size cannot be a power of 2, which leads to my problem when i use distributed optimizer: assert gbuf_size % data_parallel_world_size == 0, \ AssertionError: Each bucket's buffer size should be divisible by 112","  I think the resolution to this paradox is that .numel() is the size of the underlying data tensor, which is allocated in MemoryBuffer with the numel_padded and is not the same as the raw .numel field of the MemoryBuffer. .numel() != .numel Some better naming could have gone a long way here.","Yup, that's right. I have a PR open internally right now to fix the root cause (i.e., in the PR, I pad each bucket individually to make sure this assertion never fires); I am also removing other field names that are confusing.",Thanks Deepak!  For this and all the great work on the distributed optimizer generally.,">  >  > I think the resolution to this paradox is that .numel() is the size of the underlying data tensor, which is allocated in MemoryBuffer with the numel_padded and is not the same as the raw .numel field of the MemoryBuffer. >  > .numel() != .numel >  > Some better naming could have gone a long way here.  yes, that's what I mean. when `.numel` != `.numel()`, the distributed optimizer would produce wrong results. In the commit I linked above there's no assertion.",Won't this assertion fail though? https://github.com/NVIDIA/MegatronLM/blob/main/megatron/optimizer/distrib_optimizer.pyL146,> Won't this assertion fail though? https://github.com/NVIDIA/MegatronLM/blob/main/megatron/optimizer/distrib_optimizer.pyL146 `gbuf_size` is padded to be a multiple of `data_parallel_world_size` https://github.com/NVIDIA/MegatronLM/blob/115b6776813eb182237da6c8b9c78c6ae05d8ab1/megatron/model/distributed.pyL136L137,"No, that's the full buffer. Each bucket is not padded: ```python         bucket = model.grad_buffers[dtype].buckets[bucket_index]         bucket_buffer = bucket.data         gbuf_size = bucket_buffer.numel() ```","I am talking about the commit i linked above: 115b6776813e, not the tip of main. there's no buckets in that commit",Oh I see. I don't think it's incorrect. `max_gbuf_range_size = int(math.ceil(gbuf_size / data_parallel_world_size))` should give you the same thing as `numel_padded / data_parallel_world_size`.,"indeed, you are right. i totally missed that.","> Yup, that's right. I have a PR open internally right now to fix the root cause (i.e., in the PR, I pad each bucket individually to make sure this assertion never fires); I am also removing other field names that are confusing. Can you send your PR link? I would like to refer to it. Thanks.","This is on our internal Gitlab, I'll link to the merge commit here once it gets merged and pushed to this publicfacing repository.",Perbucket padding code pushed here: https://github.com/NVIDIA/MegatronLM/commit/443ce9f3f98fdc5a53c6b480c6e21b79944d198e. Please let me know if you run into any issues.
mayank31398,[QUESTION] Issue regarding correctness of RoPE in context parallel,https://github.com/NVIDIA/MegatronLM/blob/feac76a79148622d8f2a45d46c08a972a24784a3/megatron/core/models/common/embeddings/rotary_pos_embedding.pyL23L25 Is this logic actually correct? I am unsure if this is splitting by sequence length correctly. Can you explain this? I think it needs to have sin and cos values corresponding to each rank right? are we sure that this logic is implemented correctly?,2023-10-25T18:15:27Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/560,Marking as stale. No activity in 60 days.
joshlk,[QUESTION] All-Reduce placement in backwards for Tensor Parallel MLP blocks,"In the 2019 paper, Megatron introduces how to apply tensor parallel to MLP transformer blocks. This diagram is taken from the paper: !Screenshot 20231024 at 14 27 01 Here the function `f` is an identity for forward computation and an allreduce in the backward computation. `g` is an allreduce in the forward computation and an identity in the backward computation. However, usually the gradient of an allreduce is also an allreduce (see pytorch)  why is that not the case above? I have verified that the above multidevice MLP block is numerically equivalent to that of a singledevice MLP block, and I understand the intuition; however, I am struggling to show this is equivalent using matrix calculus mathematically. Can you please help? ðŸ™‚",2023-10-24T14:59:31Z,stale,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/559,"Thanks for the question! For simplicity, I am going to consider 2 scalars, but the math should translate to $n$ vectors as well. Let $y = x_1 + x_2$ (allreducing two scalars $x_1$ and $x_2$). In the backward pass, we have the derivative of the loss with respect to $y$ ($dL/dy$) and want to compute the derivatives of the loss with respect to $x_1$ and $x_2$ ($dL/dx_1$ and $dL/dx_2$). Using the chain rule, $dL/dx_1 = dL/dy \cdot dy/dx_1 = dL/dy \cdot 1 = dL/dy$. Similarly, $dL/dx_2 = dL/dy$ as well. Hope this helps!","Hi, thanks for the help ðŸ™‚  If I understand correctly, I think the maths above models this computation graph:  However, we are using multiple devices and the allreduce replicates $y$ like so:  If we consider the above, to calculate the backwards pass, we first need the gradient of $\hat{y}$ with respect to $\hat{x}$: $$ \begin{aligned} \frac{\mathrm{d} \hat{y} }{\mathrm{d} \hat{x}} &= \begin{pmatrix} \mathrm{d} y_1 / \mathrm{d} x_1 & \mathrm{d} y_1 / \mathrm{d} x_2 \\  \mathrm{d} y_2 / \mathrm{d} x_1 & \mathrm{d} y_2 / \mathrm{d} x_2 \end{pmatrix} \\ &= \begin{pmatrix} 1 & 1 \\  1 & 1 \end{pmatrix} \end{aligned} $$ Then to obtain $\mathrm{d} L / \mathrm{d} \hat{x}$ given $\mathrm{d} L / \mathrm{d} \hat{y}$: $$ \begin{aligned} \frac{\mathrm{d} L }{\mathrm{d} \hat{x}} &= \frac{\mathrm{d} L }{\mathrm{d} \hat{y}}\frac{\mathrm{d} \hat{y} }{\mathrm{d} \hat{x}} \\ &= \begin{pmatrix} \mathrm{d} L / \mathrm{d} y_1 & \mathrm{d} L / \mathrm{d} y_2  \end{pmatrix} \begin{pmatrix} 1 & 1 \\  1 & 1 \end{pmatrix} \\ &=\begin{pmatrix} \mathrm{d} L / \mathrm{d} y_1 + \mathrm{d} L / \mathrm{d} y_2 & \mathrm{d} L / \mathrm{d} y_1 + \mathrm{d} L / \mathrm{d} y_2  \end{pmatrix} \end{aligned} $$ So $\mathrm{d} L / \mathrm{d} x_1$ and $\mathrm{d} L / \mathrm{d} x_2$ are the sum of the two $y$ grad contributions, which is an allreduce. I believe this is how PyTorch also derived the backwards to an allreduce is an allreduce. I am doing something wrong here, do you know what? In general, is an allreduce not the backwards to an allreduce?","Hi , I think I now know where the discrepancy comes from. You need to consider the loss as vector of different functions on each device: $\hat{L}$ instead of $L$, and so $\mathrm{d} \hat{L}  / \mathrm{d} \hat{x}$ and not $\mathrm{d} L  / \mathrm{d} \hat{x}$. I have written up my argument here. I would welcome your comments: https://github.com/pytorch/pytorch/issues/58005issuecomment1783782272",Marking as stale. No activity in 60 days.
hwdef,[QUESTION]can not enbale --overlap-grad-reduce," environments: 2 nodes 8 GPU each node TP 1 PP 1 DP 16 == world_size nvidia A100  Problem: When I enable `overlapgradreduce`, the program will break. ``` Traceback (most recent call last):   File ""/data/edward/MegatronLM/pretrain_gpt.py"", line 187, in      pretrain(train_valid_test_datasets_provider,   File ""/data/edward/MegatronLM/megatron/training.py"", line 116, in pretrain     model, optimizer, opt_param_scheduler = setup_model_and_optimizer(   File ""/data/edward/MegatronLM/megatron/training.py"", line 382, in setup_model_and_optimizer     optimizer = get_megatron_optimizer(model, no_wd_decay_cond,   File ""/data/edward/MegatronLM/megatron/optimizer/__init__.py"", line 125, in get_megatron_optimizer     return opt_ty(optimizer,   File ""/data/edward/MegatronLM/megatron/optimizer/distrib_optimizer.py"", line 397, in __init__     self.model_gbuf_ranges.append(self.build_model_gbuf_range_map(model))   File ""/data/edward/MegatronLM/megatron/optimizer/distrib_optimizer.py"", line 190, in build_model_gbuf_range_map     return {   File ""/data/edward/MegatronLM/megatron/optimizer/distrib_optimizer.py"", line 191, in      dtype : [cls.build_model_gbuf_range(model, dtype, bucket_index)   File ""/data/edward/MegatronLM/megatron/optimizer/distrib_optimizer.py"", line 191, in      dtype : [cls.build_model_gbuf_range(model, dtype, bucket_index)   File ""/data/edward/MegatronLM/megatron/optimizer/distrib_optimizer.py"", line 149, in build_model_gbuf_range     assert gbuf_size % data_parallel_world_size == 0, \ AssertionError: Each bucket's buffer size should be divisible by 16 ``` I would like to know how to set the parameters in a reasonable way, which can be verified by the code here.",2023-10-24T12:57:40Z,,closed,0,17,https://github.com/NVIDIA/Megatron-LM/issues/558,"  I see you guys have submitted these codes, can you provide some help, I would appreciate it!","What is the full commandline you are using? Including model size parameters (`hiddensize`, `numattentionheads`, `numlayers`, etc.)?","```shell torchrun nproc_per_node 8 \     nnodes 2 \     node_rank 1 \     master_addr  \     master_port  \     pretrain_gpt.py \     lr 4e4 \     minlr 2e5 \     lrdecaystyle cosine \     lrdecaysamples 43945312 \     lrwarmupsamples 244140 \     weightdecay 1e1 \     optimizer adam \     adambeta1 0.9 \     adambeta2 0.95 \     adameps 1e8 \     normalization RMSNorm \     tokenizertype  \     userotarypositionembeddings \     numworkers 8 \     seed 1234 \     tensormodelparallelsize 1 \     pipelinemodelparallelsize 1 \     trainsamples 48828125 \     numlayers 24 \     maxpositionembeddings 2048 \     hiddensize 1024 \     numattentionheads 16 \     usedistributedoptimizer \     seqlength 2048 \     microbatchsize 7 \     globalbatchsize 960 \     clipgrad 1.0 \     fp16 \     resetpositionids \     resetattentionmask \     eodmaskloss \     swiglu \     nobiasgelufusion \     ffnhiddensize 2730 \     logtimerstotensorboard \     logbatchsizetotensorboard \     logvalidationppltotensorboard \     tensorboardqueuesize 5 \     datapath  \     tensorboarddir  \     split 950,25,25 \     loginterval 5 \     saveinterval 500 \     evalinterval 500 \     evaliters 2 \     distributedbackend nccl \     useflashattn \     overlapgradreduce \     save $CHECKPOINT_PATH \     load $CHECKPOINT_PATH ```","Thanks, I'll try to get back to you by the end of the week.","We have a fix for this internally, which is making its way through review. Should hopefully be pushed to Github sometime next week.","> We have a fix for this internally, which is making its way through review. Should hopefully be pushed to Github sometime next week. Thanks. Please remind me after commit.",Perbucket padding code pushed here: https://github.com/NVIDIA/MegatronLM/commit/443ce9f3f98fdc5a53c6b480c6e21b79944d198e. Please let me know if you run into any issues.,Thank you!,I will try this patch,"I have a new error: ``` /edward/MegatronLMnew/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()   storage = bucket.data.storage()._untyped() /edward/MegatronLMnew/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()   storage = bucket.data.storage()._untyped() setting training iterations to 87193 > learning rate decay style: cosine  loading checkpoint from /edward/MegatronLM/checkpoint at iteration 2500  checkpoint version 3.0 Traceback (most recent call last):   File ""/edward/MegatronLMnew/pretrain_gpt.py"", line 230, in      pretrain(train_valid_test_datasets_provider,   File ""/edward/MegatronLMnew/megatron/training.py"", line 118, in pretrain     model, optimizer, opt_param_scheduler = setup_model_and_optimizer(   File ""/edward/MegatronLMnew/megatron/training.py"", line 393, in setup_model_and_optimizer     args.iteration = load_checkpoint(model, optimizer, opt_param_scheduler)   File ""/edward/MegatronLMnew/megatron/checkpointing.py"", line 611, in load_checkpoint     optimizer.load_parameter_state(optim_checkpoint_name)   File ""/edward/MegatronLMnew/megatron/optimizer/distrib_optimizer.py"", line 739, in load_parameter_state     world_tensor = loaded_state[model_idx][dtype][key][bucket_idx] IndexError: list index out of range lixin26codegenmaster0:16947:17791 [6] NCCL INFO [Service thread] Connection closed by localRank 0 lixin26codegenmaster0:16943:17784 [2] NCCL INFO [Service thread] Connection closed by localRank 0 lixin26codegenmaster0:16945:17787 [4] NCCL INFO [Service thread] Connection closed by localRank 0 Traceback (most recent call last):   File ""/edward/MegatronLMnew/pretrain_gpt.py"", line 230, in  Traceback (most recent call last):   File ""/edward/MegatronLMnew/pretrain_gpt.py"", line 230, in      pretrain(train_valid_test_datasets_provider,   File ""/edward/MegatronLMnew/megatron/training.py"", line 118, in pretrain     pretrain(train_valid_test_datasets_provider,   File ""/edward/MegatronLMnew/megatron/training.py"", line 118, in pretrain     model, optimizer, opt_param_scheduler = setup_model_and_optimizer(   File ""/edward/MegatronLMnew/megatron/training.py"", line 393, in setup_model_and_optimizer     model, optimizer, opt_param_scheduler = setup_model_and_optimizer(   File ""/edward/MegatronLMnew/megatron/training.py"", line 393, in setup_model_and_optimizer     args.iteration = load_checkpoint(model, optimizer, opt_param_scheduler)   File ""/edward/MegatronLMnew/megatron/checkpointing.py"", line 611, in load_checkpoint     args.iteration = load_checkpoint(model, optimizer, opt_param_scheduler)       File ""/edward/MegatronLMnew/megatron/checkpointing.py"", line 611, in load_checkpoint optimizer.load_parameter_state(optim_checkpoint_name)   File ""/edward/MegatronLMnew/megatron/optimizer/distrib_optimizer.py"", line 748, in load_parameter_state Traceback (most recent call last):   File ""/edward/MegatronLMnew/pretrain_gpt.py"", line 230, in  Traceback (most recent call last):   File ""/edward/MegatronLMnew/pretrain_gpt.py"", line 230, in      torch.distributed.scatter(   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/c10d_logger.py"", line 47, in wrapper     optimizer.load_parameter_state(optim_checkpoint_name)   File ""/edward/MegatronLMnew/megatron/optimizer/distrib_optimizer.py"", line 748, in load_parameter_state     return func(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/distributed_c10d.py"", line 3172, in scatter Traceback (most recent call last):     pretrain(train_valid_test_datasets_provider,   File ""/edward/MegatronLMnew/pretrain_gpt.py"", line 230, in    File ""/edward/MegatronLMnew/megatron/training.py"", line 118, in pretrain     pretrain(train_valid_test_datasets_provider,   File ""/edward/MegatronLMnew/megatron/training.py"", line 118, in pretrain Traceback (most recent call last):   File ""/edward/MegatronLMnew/pretrain_gpt.py"", line 230, in  Traceback (most recent call last):   File ""/edward/MegatronLMnew/pretrain_gpt.py"", line 230, in          model, optimizer, opt_param_scheduler = setup_model_and_optimizer(torch.distributed.scatter(       File ""/edward/MegatronLMnew/megatron/training.py"", line 393, in setup_model_and_optimizer   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/c10d_logger.py"", line 47, in wrapper model, optimizer, opt_param_scheduler = setup_model_and_optimizer(   File ""/edward/MegatronLMnew/megatron/training.py"", line 393, in setup_model_and_optimizer     return func(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/distributed_c10d.py"", line 3172, in scatter         args.iteration = load_checkpoint(model, optimizer, opt_param_scheduler)work.wait()   File ""/edward/MegatronLMnew/megatron/checkpointing.py"", line 611, in load_checkpoint RuntimeError                : args.iteration = load_checkpoint(model, optimizer, opt_param_scheduler)pretrain(train_valid_test_datasets_provider,pretrain(train_valid_test_datasets_provider,pretrain(train_valid_test_datasets_provider,[/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:534] Connection closed by peer [10.178.164.177]:5850   File ""/edward/MegatronLMnew/megatron/checkpointing.py"", line 611, in load_checkpoint optimizer.load_parameter_state(optim_checkpoint_name)   File ""/edward/MegatronLMnew/megatron/training.py"", line 118, in pretrain   File ""/edward/MegatronLMnew/megatron/training.py"", line 118, in pretrain   File ""/edward/MegatronLMnew/megatron/training.py"", line 118, in pretrain   File ""/edward/MegatronLMnew/megatron/optimizer/distrib_optimizer.py"", line 748, in load_parameter_state     model, optimizer, opt_param_scheduler = setup_model_and_optimizer(   File ""/edward/MegatronLMnew/megatron/training.py"", line 393, in setup_model_and_optimizer     optimizer.load_parameter_state(optim_checkpoint_name)   File ""/edward/MegatronLMnew/megatron/optimizer/distrib_optimizer.py"", line 748, in load_parameter_state     torch.distributed.scatter(   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/c10d_logger.py"", line 47, in wrapper     work.wait() RuntimeError        model, optimizer, opt_param_scheduler = setup_model_and_optimizer(         : args.iteration = load_checkpoint(model, optimizer, opt_param_scheduler)return func(*args, **kwargs)  File ""/edward/MegatronLMnew/megatron/training.py"", line 393, in setup_model_and_optimizer model, optimizer, opt_param_scheduler = setup_model_and_optimizer(    [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:534] Connection closed by peer [10.178.164.177]:8583 torch.distributed.scatter(   File ""/edward/MegatronLMnew/megatron/training.py"", line 393, in setup_model_and_optimizer   File ""/edward/MegatronLMnew/megatron/checkpointing.py"", line 611, in load_checkpoint   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/distributed_c10d.py"", line 3172, in scatter   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/c10d_logger.py"", line 47, in wrapper     return func(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/distributed_c10d.py"", line 3172, in scatter         args.iteration = load_checkpoint(model, optimizer, opt_param_scheduler)optimizer.load_parameter_state(optim_checkpoint_name)   File ""/edward/MegatronLMnew/megatron/checkpointing.py"", line 611, in load_checkpoint   File ""/edward/MegatronLMnew/megatron/optimizer/distrib_optimizer.py"", line 748, in load_parameter_state     args.iteration = load_checkpoint(model, optimizer, opt_param_scheduler)   File ""/edward/MegatronLMnew/megatron/checkpointing.py"", line 611, in load_checkpoint     optimizer.load_parameter_state(optim_checkpoint_name)   File ""/edward/MegatronLMnew/megatron/optimizer/distrib_optimizer.py"", line 748, in load_parameter_state     optimizer.load_parameter_state(optim_checkpoint_name)     torch.distributed.scatter(  File ""/edward/MegatronLMnew/megatron/optimizer/distrib_optimizer.py"", line 748, in load_parameter_state   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/c10d_logger.py"", line 47, in wrapper     work.wait() RuntimeError: [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:534] Connection closed by peer [10.178.164.177]:8583     torch.distributed.scatter(   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/c10d_logger.py"", line 47, in wrapper     return func(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/distributed_c10d.py"", line 3172, in scatter         torch.distributed.scatter(return func(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/c10d_logger.py"", line 47, in wrapper   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/distributed_c10d.py"", line 3172, in scatter     work.wait() RuntimeError    : return func(*args, **kwargs)[/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:534] Connection closed by peer [10.178.164.177]:8583   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/distributed_c10d.py"", line 3172, in scatter     work.wait() RuntimeError: [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:534] Connection closed by peer [10.178.164.177]:8583     work.wait() RuntimeError: [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:534] Connection closed by peer [10.178.164.177]:8583     work.wait() RuntimeError: [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:534] Connection closed by peer [10.178.164.177]:8583 [20240110 03:13:37,220] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 16941) of binary: /usr/bin/python Traceback (most recent call last):   File ""/usr/local/bin/torchrun"", line 8, in      sys.exit(main())   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper     return f(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/run.py"", line 806, in main     run(args)   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/run.py"", line 797, in run     elastic_launch(   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/launcher/api.py"", line 134, in __call__     return launch_agent(self._config, self._entrypoint, list(args))   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/launcher/api.py"", line 264, in launch_agent     raise ChildFailedError( ``` I'm a little confused, for example, I use nccl as the backend, why is there a gloo log in the log?","I think you are trying to load a checkpoint that was created using the old code (i.e., the code without buckets). Can you try to not do this? (i.e., start from a fresh initialization). You should be able to load the resulting checkpoint. I will create a patch to make it possible to load checkpoints in the old format.",We use Gloo to collect state on the CPU when loading / saving checkpoints: https://github.com/NVIDIA/MegatronLM/blob/main/megatron/optimizer/distrib_optimizer.pyL705.," Thanks,I ran it successfully.","By the way, I wonder if this parameter can reduce the occupation of GPU memory?",Which parameter? `usedistributedoptimizer` does reduce memory usage. `overlapgradreduce` shouldn't affect memory usage.,"To load old checkpoints, you can use this patch (will also get this merged in eventually): ```diff commit 639f12808d7f641764abb9eb9d368733777b05ad Author: Deepak Narayanan  Date:   Fri Nov 10 11:08:25 2023 0800     Make checkpoint loading somewhat backwards compatible     If current run only creates one bucket, then it is possible to load an old     checkpoint. If current run uses overlapgradreduce and splits the GradBuffer     into multiple buckets, then an AssertionError is thrown diff git a/megatron/optimizer/distrib_optimizer.py b/megatron/optimizer/distrib_optimizer.py index 9875d192d..50eb385a6 100644  a/megatron/optimizer/distrib_optimizer.py +++ b/megatron/optimizer/distrib_optimizer.py @@ 736,7 +736,14 @@ class DistributedOptimizer(MixedPrecisionOptimizer):                           Scatter tensor list.                          if data_parallel_rank == 0:                             world_tensor = loaded_state[model_idx][dtype][key][bucket_idx] +                            world_tensor_for_all_buckets = loaded_state[model_idx][dtype][key] +                            if not isinstance(world_tensor_for_all_buckets, list): +                                world_tensor_for_all_buckets = [world_tensor_for_all_buckets] +                            assert bucket_idx < len(world_tensor_for_all_buckets), \ +                                (f""Trying to load state for bucket_id {bucket_idx} (out of "" +                                 f""{len(gbuf_range_map_for_all_buckets)} buckets) from checkpoint; "" +                                 f""checkpoint only has {len(world_tensor_for_all_buckets)} bucket(s)"") +                            world_tensor = world_tensor_for_all_buckets[bucket_idx]                              gbuf_start_idxs = \                                  list(range(0, gbuf_world_numel, gbuf_local_numel))                              send_tensors = [world_tensor[i:(i+gbuf_local_numel)] ```",Thanks for all your replay!
cckao,[QUESTION] Is it possible to enable sequence parallel in inference time,"**Your question** To be specific, text generation of Llama 2. I've seen examples show how to use sequence parallelism in training, but I don't see any for inference. Can anyone tell me how to do it? Thank you.",2023-10-24T10:45:28Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/557,It should be the same way: using the `sequenceparallel` commandline argument.,Marking as stale. No activity in 60 days.
bisunny,[QUESTION] Whether to consider optimizing TP scheduling?,"The performance when using tensor parallel is restricted by communication. Have you considered adding some enhancement strategies to Megatron, such as paper [Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models]",2023-10-24T03:46:21Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/556,"TransformerEngine has implemented the UserBuffer for overlapping and can be used for Megaton. However, I suppose it will work fine to just overlap with matmul in Linear class. ","Thank you for your reply, I will study it.",Marking as stale. No activity in 60 days.
V3RGANz,[QUESTION] run_text_generation_server arguments,Text generation server scripts like this https://github.com/NVIDIA/MegatronLM/blob/main/examples/run_text_generation_server_345M.sh contain following arguments: ```        outseqlength 1024  \        temperature 1.0  \        top_p 0.9  \ ``` But script run failed because it does not recognise them. I also could not find them anywhere in source code. I have only seen that text generation configuration can be passed directly in request json (https://github.com/NVIDIA/MegatronLM/blob/main/megatron/text_generation_server.pyL86).  Are these args wrong or I am missing something?,2023-10-23T18:35:47Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/555,"hi thanks for pointing this out, these were recently removed because they were not actually doing anything and the sampling args are controlled in the payload. I will update the examples.",Marking as stale. No activity in 60 days.
lingjiew93,Support of LoRA/QLoRA?,"Hi, I saw megatron already included llama2 inference and finetune but I didn't find LoRA. Do you have plan to support LoRA/QLoRA for llama2 in future? Thanks,",2023-10-23T09:01:58Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/554,We have support for LoRA and other adapters in NeMo: https://docs.nvidia.com/nemoframework/userguide/latest/playbooks/llama2peft.html.,> We have support for LoRA and other adapters in NeMo: https://docs.nvidia.com/nemoframework/userguide/latest/playbooks/llama2peft.html. Will check that. Thanks.
okoge-kaz,[QUESTION] Is there support for megatron -> HF checkpoint?,"I am training Llama2 by converting llama2 checkpoints in HuggingFace format to megatron format checkpoints with reference to https://github.com/NVIDIA/MegatronLM/blob/main/docs/llama2.md . I would like to do a megatron > HF checkpoint conversion and do inference to see if the training is going well. Is there a tool to convert to HF checkpoint? I tried MegatronLLaMA https://github.com/alibaba/MegatronLLaMA/blob/main/tools/checkpoint_conversion/llama_checkpoint_conversion.py but it did not work as the inference result is as follows. ``` inference result : ""'atsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsatsats"" ```",2023-10-21T15:50:53Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/553,it's supported in our fork in case: https://github.com/epfLLM/MegatronLLM
Infi-zc,fix the bug when load from dist opt with changed dp size,"When using dist opt and saving ckpt, if we load it again and want to change the dp_size, loading from distrib_opt.pt may cause an error. This is because the loaded tensor (world_tensor) may not be long enough and cannot be scattered into equal length (gbuf_local_numel) dp_world_size parts. At this time, scatter will report an error because the last shard requires a receiving length of gbuf_local_numel, and the sending length is world_tensor_numel  (dp_size  1) * gbuf_local_numel, which will be less than gbuf_local_numel, causing an error. This PR has fixed this issue. This typically occurs when the number of available cards decreases or increases, and we want to continue training, then we would face this issue.  ",2023-10-20T13:51:02Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/552,"My feeling is that other parts of the code will break if a bucket's size is not divisible by the new dp_size. Concretely, doesn't the assertion here fail: https://github.com/NVIDIA/MegatronLM/blob/main/megatron/optimizer/distrib_optimizer.pyL146?",Marking as stale. No activity in 60 days.
flower-with-safe,add max z loss,"in baichuan2 tech report:  Baichuan 2: Open Largescale Language Models https://arxiv.org/abs/2309.10305 they mentioned max z loss, which helped stabilize training and made the inference more robust to hyperparameters. I implemented the Megatron version of max z loss.",2023-10-19T09:04:49Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/551, ,Marking as stale. No activity in 60 days.
siddharth9820,Add example bash script,`bash examples/run_axonn.sh` should run GPT350M on 4 GPUs with 2x2 tensor parallelism.,2023-10-17T20:08:40Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/550
13416157913,"[QUESTION] Hello, when finetune llama2-7B, it's error: RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`","Hello, when finetune llama27B, it's error: RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)` like this: Traceback (most recent call last):   File ""/home/dengkaibiao/MegatronLM/./tasks/main.py"", line 102, in      main()   File ""/home/dengkaibiao/MegatronLM/tasks/glue/finetune.py"", line 81, in main     glue_classification(num_classes, Dataset, name_from_datapath)   File ""/home/dengkaibiao/MegatronLM/tasks/glue/finetune.py"", line 52, in glue_classification     finetune(train_valid_datasets_provider, model_provider,   File ""/home/dengkaibiao/MegatronLM/tasks/finetune_utils.py"", line 298, in finetune     _train(model, optimizer, opt_param_scheduler, forward_step,   File ""/home/dengkaibiao/MegatronLM/tasks/finetune_utils.py"", line 186, in _train     out = train_step(forward_step, batch, model, optimizer, opt_param_scheduler,config)   File ""/home/dengkaibiao/MegatronLM/megatron/training.py"", line 416, in train_step     losses_reduced = forward_backward_func(   File ""/home/dengkaibiao/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 341, in forward_backward_no_pipelining     output_tensor = forward_step(   File ""/home/dengkaibiao/MegatronLM/megatron/core/pipeline_parallel/schedules.py"", line 184, in forward_step     output_tensor, loss_func = forward_step_func(data_iterator, model)   File ""/home/dengkaibiao/MegatronLM/tasks/finetune_utils.py"", line 68, in _cross_entropy_forward_step     output_tensor = model(tokens, attention_mask, tokentype_ids=types)   File ""/home/dengkaibiao/anaconda3/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1194, in _call_impl     return forward_call(*input, **kwargs)   File ""/home/dengkaibiao/MegatronLM/megatron/model/distributed.py"", line 236, in forward     return self.module(*inputs, **kwargs)   File ""/home/dengkaibiao/anaconda3/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1194, in _call_impl     return forward_call(*input, **kwargs)   File ""/home/dengkaibiao/MegatronLM/megatron/model/module.py"", line 181, in forward     outputs = self.module(*inputs, **kwargs)   File ""/home/dengkaibiao/anaconda3/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1194, in _call_impl     return forward_call(*input, **kwargs)   File ""/home/dengkaibiao/MegatronLM/megatron/model/classification.py"", line 58, in forward     lm_output = self.language_model(   File ""/home/dengkaibiao/anaconda3/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1194, in _call_impl     return forward_call(*input, **kwargs)   File ""/home/dengkaibiao/MegatronLM/megatron/model/language_model.py"", line 501, in forward     encoder_output = self.encoder(   File ""/home/dengkaibiao/anaconda3/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1194, in _call_impl     return forward_call(*input, **kwargs)   File ""/home/dengkaibiao/MegatronLM/megatron/model/transformer.py"", line 1668, in forward     hidden_states = layer(   File ""/home/dengkaibiao/anaconda3/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1194, in _call_impl     return forward_call(*input, **kwargs)   File ""/home/dengkaibiao/MegatronLM/megatron/model/transformer.py"", line 1057, in forward     self.self_attention(   File ""/home/dengkaibiao/anaconda3/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1194, in _call_impl     return forward_call(*input, **kwargs)   File ""/home/dengkaibiao/MegatronLM/megatron/model/transformer.py"", line 573, in forward     mixed_x_layer, _ = self.query_key_value(hidden_states)   File ""/home/dengkaibiao/anaconda3/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1194, in _call_impl     return forward_call(*input, **kwargs)   File ""/home/dengkaibiao/MegatronLM/megatron/core/tensor_parallel/layers.py"", line 733, in forward     output_parallel = self._forward_impl(   File ""/home/dengkaibiao/MegatronLM/megatron/core/tensor_parallel/layers.py"", line 534, in linear_with_grad_accumulation_and_async_allreduce     return LinearWithGradAccumulationAndAsyncCommunication.apply(*args)   File ""/home/dengkaibiao/anaconda3/lib/python3.10/sitepackages/torch/cuda/amp/autocast_mode.py"", line 97, in decorate_fwd     return fwd(*args, **kwargs)   File ""/home/dengkaibiao/MegatronLM/megatron/core/tensor_parallel/layers.py"", line 342, in forward     output = torch.matmul(total_input, weight.t()) RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)` Traceback (most recent call last):",2023-10-17T10:01:56Z,stale,closed,1,1,https://github.com/NVIDIA/Megatron-LM/issues/549,Marking as stale. No activity in 60 days.
gmittal,Support frozen tensor parallel layers when gradient accumulation fusion is enabled,I found that when setting `.requires_grad = False` for some TP layers during training (with grad accumulation fusion on) that the backward assumes the layer has a gradient. This patch fixes this by avoiding gradient accumulation fusion when the layer is frozen.,2023-10-16T18:46:10Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/548,Marking as stale. No activity in 60 days.
torshie,[QUESTION] Options to blend multiple datasets evenly.,"I have two datasets `A` and `B`, assume `A` has 10 samples, `B` has 20 samples. I want to megatron to sample from the two datasets evenly across all 30 samples (no up sampling or down sampling). Should the option to blend the datasets be `datapath 1 A 1 B` or `datapath 10 A 20 B` ?",2023-10-16T11:17:05Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/547,Marking as stale. No activity in 60 days.,same question,"The blend will be calculated depending on the number of _training samples_, completely independent of the number of samples in the datasets. So indeed it would be the second option, where you sample from `B` with an increased probability."
flower-with-safe,"[BUG] when training models in tp=1 and pp=1, lm_head weights won't be saved.","**Describe the bug** A clear and concise description of what the bug is. when we pretrain gpt model using megatron. we initialize our model use megatron/training.py get_model func. If tp=1 and pp=1, then pre_process and post_process is always True. When the model need to be saved, we use func state_dict_for_save_checkpoint in megatron/model/gpt_model.py `        def state_dict_for_save_checkpoint(self, prefix='', keep_vars=False):         state_dict_ = {}         state_dict_[self._language_model_key] \             = self.language_model.state_dict_for_save_checkpoint(                 prefix=prefix, keep_vars=keep_vars)          Save word_embeddings.         if self.post_process and not self.pre_process and not self.untie_embeddings_and_output_weights:             state_dict_[self._word_embeddings_for_head_key] \                 = self.word_embeddings.state_dict(prefix=prefix,                                                   keep_vars=keep_vars)         return state_dict_` and the lm_head will never be saved. **To Reproduce** Steps to reproduce the behavior. The easier it is to reproduce the faster it will get maintainer attention. **Expected behavior** A clear and concise description of what you expected to happen. **Stack trace/logs** If applicable, add the stack trace or logs from the time of the error. **Environment (please complete the following information):**   MegatronLM commit ID   PyTorch version   CUDA version   NCCL version **Proposed fix** If you have a proposal for how to fix the issue state it here or link to a PR. **Additional context** Add any other context about the problem here.",2023-10-16T07:23:55Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/546,"This is by design and not a bug. With only one pipeline stage, we reuse the same weights for the embedding and the lm_head and don't need two copies.","Closing this issue for now, feel free to reopen if you have more questions."
SeaOfOcean,[ENHANCEMENT] relax assert gbuf_size % data_parallel_world_size == 0 when using use-distributed-optimizer,"**Is your feature request related to a problem? Please describe.** relax assert gbuf_size % data_parallel_world_size == 0 when using usedistributedoptimizer. In some setting, when I enable usedistributedoptimizer, the assertion failed  ``` AssertionError: Each bucket's buffer size should be divisible by 2 ``` **Describe the solution you'd like** relax the assert",2023-10-16T06:47:16Z,stale,closed,0,5,https://github.com/NVIDIA/Megatron-LM/issues/545, Can you help check it? Why do we need this assert?,I found that this PR  https://github.com/NVIDIA/MegatronLM/commit/efb2e25595bcced494da3566b248dfeed55f27f6 introduced the change,"Can you share the commandline arguments you used to produce this assertion error? The assertion itself is important since with the distributed optimizer, we want to give every DP rank ""ownership"" on an equalsized shard of the parameters.",Perbucket padding code pushed here: https://github.com/NVIDIA/MegatronLM/commit/443ce9f3f98fdc5a53c6b480c6e21b79944d198e. Please let me know if you run into any issues.,Marking as stale. No activity in 60 days.
Azure-Tang,"first commit, modify transformer to add cache attention calculation, â€¦","Megatron integrate terapipe, first commit. Modify transformer to add cache attention calculation. Correctness verification is required.",2023-10-15T16:58:59Z,stale,open,0,4,https://github.com/NVIDIA/Megatron-LM/issues/544,Marking as stale. No activity in 60 days.,"Tang hi tang, Thanks for your work of the implementation of terapipe on MegatronLM. Did you try to compare the performance versus no terapipinig? How much benefit it brings?","> Tang hi tang, Thanks for your work of the implementation of terapipe on MegatronLM. Did you try to compare the performance versus no terapipinig? How much benefit it brings? hi, i completed the modify, which you can check in my repo. And it seems got error of about two decimal places. But I didn't further check the result so far.",Marking as stale. No activity in 60 days.
bisunny,[QUESTION] Performance degradation issue,"I used the latest version(branch 'main') and found some performance issues, There are some unclear synchronizations operations(repeat_interleave) here that lead to inefficiency in GPU utilization, as shown below, and these will not appear in older version. !image",2023-10-14T10:31:06Z,stale,closed,0,12,https://github.com/NVIDIA/Megatron-LM/issues/543,What commit hash are you using? What command line?,"commit hash is 'f7727433293427bef04858f67b2889fe9b177d88'. The command line refers to the blog https://huggingface.co/blog/megatrontraining,  I just used multiple gpus to distribute training and changed some parameters(like hidden_size,number_layer,seq_length) to make the model larger.","> commit hash is 'f7727433293427bef04858f67b2889fe9b177d88'. The command line refers to the blog https://huggingface.co/blog/megatrontraining, I just used multiple gpus to distribute training and changed some parameters(like hidden_size,number_layer,seq_length) to make the model larger.","> Author export CUDA_DEVICE_MAX_CONNECTIONS=1 export CUDA_VISIBLE_DEVICES=0,1,2,3 GPUS_PER_NODE=4 MASTER_ADDR=localhost MASTER_PORT=6001 NNODES=1 NODE_RANK=0 WORLD_SIZE=$(($GPUS_PER_NODE*$NNODES)) DISTRIBUTED_ARGS=""nproc_per_node $GPUS_PER_NODE nnodes $NNODES node_rank $NODE_RANK master_addr $MASTER_ADDR master_port $MASTER_PORT"" CHECKPOINT_PATH=/mnt/workspace/flash_v2/codeparrot/checkpoint2 VOCAB_FILE=/mnt/workspace/flash_v2/codeparrot/vocab.json MERGE_FILE=/mnt/workspace/flash_v2/codeparrot/merges.txt DATA_PATH=/mnt/workspace/flash_v2/codeparrot_content_document GPT_ARGS=""numlayers 40 hiddensize 5120 numattentionheads 40 seqlength 4096 maxpositionembeddings 4096 microbatchsize 2 globalbatchsize 8 lr 0.0005 trainiters 10000 swiglu lrdecayiters 150000 lrdecaystyle cosine lrwarmupiters 2000 weightdecay .1 adambeta2 .999 bf16 loginterval 10 saveinterval 3000 evalinterval 3000 evaliters 3000 useflashattn usedistributedoptimizer logtimerstotensorboard logparamsnorm lognumzerosingrad timingloglevel 2 sequenceparallel overlapgradreduce "" TENSORBOARD_ARGS=""tensorboarddir experiments/tensorboard"" torchrun $DISTRIBUTED_ARGS \         pretrain_gpt.py \         tensormodelparallelsize 4 \         pipelinemodelparallelsize 1 \         $GPT_ARGS \         vocabfile $VOCAB_FILE \         mergefile $MERGE_FILE \         save $CHECKPOINT_PATH \         datapath $DATA_PATH \         $TENSORBOARD_ARGS > log 2>&1& ","commit hash is 'f7727433293427bef04858f67b2889fe9b177d88'. The command line refers to the blog https://huggingface.co/blog/megatrontraining, I just used multiple gpus to distribute training and changed some parameters(like hidden_size,number_layer,seq_length) to make the model larger.",I think this should be resolved with dd74ea0b9a40b4dd5c8eacf8306bc0d63c94e54c.,"Thanks, I will try it","  Thank you very much, the previous problem has been solved. And I have another question. When using gqa( self.num_attention_heads_per_partition // self.num_query_groups_per_partition > 1 = True, if training llama 2 70b model), the above situation will still occur (synchronous operation). Can the problem be fundamentally solved(If it is a problem)?",We are looking into this. Stay tuned.," I have opened a PR for this ^^ Can you take a look? Its working fine for me for a 3B parameter model on 8x H100s I don't have access to A100s so can't test on those I think this has been fixed with PyTorch 2.1, at least I am unable to replicate it","also, the repeat can be avoided when using flash 2, since Flash 2 does that itself",Marking as stale. No activity in 60 days.
cuichenx,Fix variable type for layernorm1p ,"False should be a boolean type, not string (see https://github.com/NVIDIA/TransformerEngine/blob/main/transformer_engine/pytorch/module/layernorm_linear.pyL648)",2023-10-14T00:38:16Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/542
Druva24,Question Regarding Throughput(Training Sequences per second),"Hi, We are using MegatronLM source code for running benchmarking experiments of BERTLarge and GPT2 pretraining on H100s. However in the script, there is no calculation of throughput, so can we calculate the throughput as global_batch_size/consumed_samples_per_iteration Reference code:  https://github.com/NVIDIA/MegatronLM/blob/79a9feef261352ac1ee80b36f2cf73c20f864965/megatron/training.pyL609 https://github.com/NVIDIA/MegatronLM/blob/79a9feef261352ac1ee80b36f2cf73c20f864965/megatron/training.pyL611 Thank you",2023-10-13T17:10:14Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/541,"global_batch_size / iteration_time will give you the throughput in sequences / second. Is that what you want? If you want throughput in ""floating point operations / second"", then the numerator should be the number of floatingpoint operations in an iteration instead of the global batch size. https://arxiv.org/pdf/2104.04473.pdf has some details on the right formula to use for this.","yes  , I want throughput in sequences / second","Ok, then global_batch_size / iteration_time should be good. Closing this."
thincal,[QUESTION] How to avoid build megatron/fused_kernels during training,"To speed up the training process, what's the way to save the prebuilt stuff of megatron/fused_kernels and avoid the build during training? any suggestion is appreciated.",2023-10-13T13:02:42Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/540,solved by `importlib.import_module`.
robotsp,[ENHANCEMENT]Does BPipe implement in Megatron-LM framework?,"**Is your feature request related to a problem? Please describe.** I read a paper named: **bpipe: memorybalanced pipeline parallelism for training large language models**. The author said they have implemented in Megatron which I cannot find. **Describe the solution you'd like** My question is, Does BPipe implement in MegatronLM framework?       ```[tasklist]  Tasks ```",2023-10-13T07:41:02Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/539,"No, this is not implemented in this repository. The authors probably meant they implemented their techniques in a fork of Megatron."
mcrts,fix: rotary position embedding missing argument,Missing Import for Rotary Position Embedding. Missing rotary_percent argument when instanciating RotaryEmbeddings,2023-10-12T18:22:26Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/538,"looks like that was missed, taking a look now"
SeaOfOcean,Remove unnecessary repeat_interleave to fix performance drop,"There are unnecessary repeat_interleave when  `self.num_attention_heads_per_partition == self.num_query_groups_per_partition` in attention.py and transformer.py, which will trigger torch.repeat_interleave even when repeat==1, which hurt the performance when `self.num_attention_heads_per_partition == self.num_query_groups_per_partition`. !image",2023-10-12T12:02:17Z,,closed,0,5,https://github.com/NVIDIA/Megatron-LM/issues/537, Can you help review the code?,This is being addressed in a PR coming.,Should be addressed by https://github.com/NVIDIA/MegatronLM/commit/993aa0f0f7e1c92b04eab27f5abeea4b94644751., https://github.com/NVIDIA/MegatronLM/blob/main/megatron/model/transformer.pyL756 We should also fix it in transformer.py,"Thanks, ! Merged into main."
kisseternity,[QUESTION] Should llama or gpt-like models have padding attention mask?,"**Your question** Hello, as far as I know about Megatron, I've only seen padding mask for bert implementation. Yet in Huggingface transformers library, the llama model should also take in the padding mask,  while the Megatron's attention mask is the causal mask. Am I right or do I miss something? Please take a look. thanks.",2023-10-11T12:37:00Z,stale,closed,1,6,https://github.com/NVIDIA/Megatron-LM/issues/536,Marking as stale. No activity in 60 days.,Do you figure it out?  I also only see causal mask for training. Inference has padding but the attention mask computed by get_ltor_masks_and_position_ids does not consider padding.,"> Do you figure it out? I also only see causal mask for training. Inference has padding but the attention mask computed by get_ltor_masks_and_position_ids does not consider padding. It turns out the default dataloader in Megatron is designed for pretraining, meaning that every sample is expected to be the same max seq len. So no padding is needed. If you want to do finetuning, I think code needs to be added for padding. You may look the code in DeepSpeedchat for reference.","> > Do you figure it out? I also only see causal mask for training. Inference has padding but the attention mask computed by get_ltor_masks_and_position_ids does not consider padding. >  > It turns out the default dataloader in Megatron is designed for pretraining, meaning that every sample is expected to be the same max seq len. So no padding is needed. If you want to do finetuning, I think code needs to be added for padding. You may look the code in DeepSpeedchat for reference. Thank you! I was hoping the padding code was just not found by me","The megatron GPTDataset is implemented as batchpacking like https://huggingface.co/docs/trl/sft_trainerpackingdatasetconstantlengthdataset which provides samples with constant length, but I don't know the performance effect of packing compared to padding, or maybe resetattentionmask should be used for small dataset to avoid CrossContamination Attention.",Marking as stale. No activity in 60 days.
nrailg,[BUG] RoPE `seq` loses fraction bits with `fp16` enabled,"**Describe the bug** With FP16 enabled, the dtype of `self.inv_freq` is actually fp16 in `forward`. If `max_seq_len` = 32768, it exceeds fp16 range. `seq.type_as(self.inv_freq)` will cast seq to f16, and lose fraction bits, making all tail token embeddings the same, and LM models won't be able to tell its position. ```python class RotaryEmbedding(nn.Module):     def __init__(self, dim, seq_len_interpolation_factor=None):         super().__init__()         self.seq_len_interpolation_factor = seq_len_interpolation_factor         inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))         self.register_buffer('inv_freq', inv_freq, persistent=False)     def forward(self, max_seq_len, offset=0):         seq = torch.arange(max_seq_len, device=self.inv_freq.device) + offset         if self.seq_len_interpolation_factor is not None:             seq = seq.type_as(self.inv_freq)             seq *= 1 / self.seq_len_interpolation_factor         freqs = einsum('i , j > i j', seq.type_as(self.inv_freq), self.inv_freq)          first part even vector components, second part odd vector components,           2 * dim in dimension size         emb = torch.cat((freqs, freqs), dim=1)          emb [seq_length, .., dim]         return emb[:, None, None, :] ```",2023-10-10T10:35:47Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/535,Marking as stale. No activity in 60 days.
yfsong0709,[BUG] Import error in clip_grads.py,"**Describe the bug** In file ""megatron/optimizer/clip_grads.py"", line 19, there is an import ""torch._six import inf"". **Environment (please complete the following information):** CUDA==cu118 torch==2.0.0 **Proposed fix** This import should be ""torch import inf"".",2023-10-10T03:55:54Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/534
Infi-zc,[QUESTION] Why should CUDA_DEVICE_MAX_CONNECTIONS=1 should be set when using seq_parallel or async comm?,"I have a question, why is it necessary to set CUDA_DEVICE_MAX_CONNECTIONS=1 after enabling seq_parallel? This note is written in the bwd_compute function, and it says it is to launch communication first, so as to achieve overlap with calculation. But I don't understand the inevitable connection between these two. Can anyone help explain? Thank you.",2023-10-09T12:53:11Z,stale,closed,0,5,https://github.com/NVIDIA/Megatron-LM/issues/533,It enforces the order of kernel execution on GPU as the kernel queuing order from host. Its for GEMM and TP communication overlap it allows for scheduling the communication kernel in GPU ahead of the GEMM to have the communication kernel have GPU resources allocated before GEMM takes all of them. From my understanding its like essentially having a single stream, Thanks for your reply!  I have some further questions about that env var. 1. Does this environment variable have an upper limit?  2. What is the default value?,Marking as stale. No activity in 60 days.,>  Thanks for your reply! I have some further questions about that env var. >  >     1. Does this environment variable have an upper limit? >  >     2. What is the default value? See here: https://docs.nvidia.com/cuda/cudacprogrammingguide/index.htmlcudaenvironmentvariables,Marking as stale. No activity in 60 days.
sanandaraj5597,Bias-Dropout-Add fusion fix / Avoided an unwanted interleave operation / Added support for UB/TP communication overlap,"BiasDropoutAdd fusion: The if statement to check for presence of Bias does the Bias addition, while the DropoutAdd is done generically outside the if block. This causes Torch to not fuse these operations. Patched the code to make Torch fuse the operations for better performance. Attention's torch.interleave operation: The Attention block uses a torch.interleave operation before Flash Attention. This inserts unwanted kernels even when the interleaving factor is 1 for GPT based models. An idempotent operation is inserting kernels hurting some performance. So, did this interleaving based on the interleaving factor. Added support for user buffer/tensor parallel communication overlap to improve performance of these models.",2023-10-09T09:25:10Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/532,"Move to gitlab, closing this."
MoFHeka,Fix GQA bug when pipeline parallel. And fix RotaryEmbedding precision bug.," Fix GQA bug Fix CC([BUG] Fail to apply GQA when using pipeline parallel.). The **View** operator couldn't recognize the proper shape of tensor. **Reshape** operator is fine.  Fix RoPe bug At present, the mainstream position coding implementations have the problem of collision in mixed precision due to the lack of precision of floatingpoint number representation of low precision (float16/bfloat16). Especially when the context length becomes longer and longer during model training (inference), the problem of position coding collision caused by low precision representation becomes more and more serious, which affects the effect of the model. !coding collision Refer: https://cdn.baichuanai.com/paper/Baichuan2technicalreport.pdf",2023-10-09T08:26:10Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/531,"seq.type_as(torch.float32) leads to the  `TypeError: type_as(): argument 'other' (position 1) must be Tensor, not torch.dtype` it's probably should be seq.type(torch.float32)","> seq.type_as(torch.float32) leads to the > `TypeError: type_as(): argument 'other' (position 1) must be Tensor, not torch.dtype` > it's probably should be seq.type(torch.float32) Sorry, I fixed it.",Marking as stale. No activity in 60 days.
siddharth9820,Parallelizing Attention with AxoNN's TP,,2023-10-04T20:54:54Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/530
PeiqinSun,[QUESTION] Broken pipe in preprocess_data,"**Your question** I want to convert my custom pretrainning data via tools/preprocess_data.py. But I often encountered ""broken pipe"" in running.  The configure is  ``` 20231004T11:18:01.444981769+08:00 stdout F  input /xxx/train_15.json 20231004T11:18:01.444981769+08:00 stdout F json_keys ['text'] 20231004T11:18:01.444984737+08:00 stdout F split_sentences False 20231004T11:18:01.444986873+08:00 stdout F keep_newlines False 20231004T11:18:01.444989827+08:00 stdout F tokenizer_type HuggingfaceTokenizer 20231004T11:18:01.444993447+08:00 stdout F model_path_or_name xxx 20231004T11:18:01.444995844+08:00 stdout F append_eod True 20231004T11:18:01.444998186+08:00 stdout F lang english 20231004T11:18:01.445000254+08:00 stdout F output_prefix xxx 20231004T11:18:01.445002526+08:00 stdout F dataset_impl mmap 20231004T11:18:01.4450045+08:00 stdout F workers 48 20231004T11:18:01.445006683+08:00 stdout F partitions 1 20231004T11:18:01.445008721+08:00 stdout F log_interval 1000 20231004T11:18:01.445010802+08:00 stdout F keep_empty False 20231004T11:18:01.445013106+08:00 stdout F rank 1 20231004T11:18:01.445015385+08:00 stdout F make_vocab_size_divisible_by 128 20231004T11:18:01.445017724+08:00 stdout F tensor_model_parallel_size 1 20231004T11:18:01.445019888+08:00 stdout F vocab_extra_ids 0 ``` The detail of Error is  ``` 20231004T15:37:14.21580373+08:00 stderr F Traceback (most recent call last): 20231004T15:37:14.215807119+08:00 stderr F   File ""/usr/lib/python3.10/multiprocessing/process.py"", line 314, in _bootstrap 20231004T15:37:14.215820708+08:00 stderr F     self.run() 20231004T15:37:14.215823543+08:00 stderr F   File ""/usr/lib/python3.10/multiprocessing/process.py"", line 108, in run 20231004T15:37:14.215826665+08:00 stderr F     self._target(*self._args, **self._kwargs) 20231004T15:37:14.215829287+08:00 stderr F   File ""/usr/lib/python3.10/multiprocessing/pool.py"", line 136, in worker 20231004T15:37:14.215832042+08:00 stderr F     put((job, i, (False, wrapped))) 20231004T15:37:14.215837155+08:00 stderr F   File ""/usr/lib/python3.10/multiprocessing/queues.py"", line 377, in put 20231004T15:37:14.215840519+08:00 stderr F     self._writer.send_bytes(obj) 20231004T15:37:14.215843229+08:00 stderr F   File ""/usr/lib/python3.10/multiprocessing/connection.py"", line 200, in send_bytes 20231004T15:37:14.215846096+08:00 stderr F     self._send_bytes(m[offset:offset + size]) 20231004T15:37:14.215848774+08:00 stderr F   File ""/usr/lib/python3.10/multiprocessing/connection.py"", line 404, in _send_bytes 20231004T15:37:14.215851029+08:00 stderr F     self._send(header) 20231004T15:37:14.215853939+08:00 stderr F   File ""/usr/lib/python3.10/multiprocessing/connection.py"", line 368, in _send 20231004T15:37:14.215856408+08:00 stderr F     n = write(self._handle, buf) 20231004T15:37:14.215861943+08:00 stderr F BrokenPipeError: [Errno 32] Broken pipe ``` And I try to decrease the workers, but I don't think it's a make sense solution. The maximum filehandlers in my systems is 1043786. I think this is enough big. So, have anyone encounter? And how to fix?",2023-10-04T11:55:45Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/529,Marking as stale. No activity in 60 days.
ppt0011,[QUESTION] which script to use to launch llama 2 fine tuning?,which script should we pass the paramaters defined here to in order to launch the model? https://github.com/NVIDIA/MegatronLM/blob/main/docs/llama2.mdlaunchmegatron,2023-10-04T09:41:28Z,stale,closed,4,4,https://github.com/NVIDIA/Megatron-LM/issues/528,Marking as stale. No activity in 60 days., I have the same question. Do you have any ideas?,same question please !!!!!!,Marking as stale. No activity in 60 days.
siddharth9820,Parallelizing MLP layers with AxoNN's tensor parallelism,,2023-10-03T18:40:40Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/527
siddharth9820,initialize torch distributed using ranks and world size from MPI,"This initialization scheme is easier to adapt to other clusters compared to using torchrun, since most clusters have some version of MPI.  The changes here only use rank and worldsize data from MPI for initializing the torch process group. All communication still happens via NCCL.",2023-10-02T21:01:39Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/526
jindajia,"[QUESTION] Even training a small model, Megatron-LM need huge memory space, OOM error","I'm trying to train a gpt2 large(774M) model on V10032GB GPU, however even this model is not big, I cant' fit it into a single gpu. It will always show this error, attache with my terminal output. ``` ""torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 786.00 MiB (GPU 0; 31.74 GiB total capacity; 28.82 GiB already allocated; 577.12 MiB free; 29.97 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"" ``` Below is my training config and model size calculation. Based on my calculation, and especially with the help of  from this tool, I figured out this model should only need 23.3GB which is much smaller than 32GB GPU memory. However, I still encounter OOM errors. So, I'm confused why Megatron need so much memory during training. Besides, I didn't use ```useflashatten``` method to save memory because I can only access to V100 GPU which is not support it. Is this a reason why memory bigger than theoretical situation? ``` DISTRIBUTED_ARGS=""     nproc_per_node $GPUS_PER_NODE \     nnodes $NNODES \     node_rank $NODE_RANK \     master_addr $MASTER_ADDR \     master_port $MASTER_PORT "" GPT_ARGS=""     tensormodelparallelsize 1 \     pipelinemodelparallelsize 1 \     numlayers 36 \     hiddensize 1280 \     numattentionheads 20 \     seqlength 1024 \     maxpositionembeddings 1024 \     microbatchsize 4 \     globalbatchsize 16 \     lr 0.00015 \     trainiters 500000 \     lrdecayiters 320000 \     lrdecaystyle cosine \     minlr 1.0e5 \     weightdecay 1e2 \     lrwarmupfraction .01 \     clipgrad 1.0 \     fp16 \     swiglu \     usedistributedoptimizer "" ``` Model Memory Size ```  ***Full Model without Parallel*** =========================================================================================================== Layer                                      Param.(shape)           Param.(Mem. MB)  Act.(Mem. MB)          GPTModel                                                          â”œâ”€TransformerLanguageModel                  â”‚    â””â”€Embedding                                                    	               	15.0           	 â”‚    â”‚    â””â”€word_embeddings                w=[50304,1280]           	122.8          	   â”‚    â”‚    â””â”€position_embeddings            w=[1024,1280]            	2.5            	 â”‚    â””â”€ParallelTransformer: X 36(layer_num)                                        	206.0/layer    	 â”‚    â”‚    â””â”€input_layernorm                w=[1280],b=[1280]        	0.0            	10.0           	 â”‚    â”‚    â””â”€self_attention                                          	               	60.0           	 â”‚    â”‚         â””â”€dense_4h_to_h            w=[1280,3392],b=[1280]   	8.3            	10.0           	 â”‚    â”‚    â””â”€drop_add_fusion                                         	               	15.0           	  Amount of Parameters: 771,106,304   Parameters: 1.4GB Gradients: 1.4GB Optimizers(Adam) States: 11.5GB Activations: 9.0GB Total memory demand: 23.3GB ============================================================================================================== ```",2023-09-29T14:43:07Z,stale,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/525,"What happens if you half the microbatch size? I am wondering if the tool you are using calculates memory estimates for the activations correctly. Also, unfortunately PyTorch consumes a fair bit of auxiliary memory that is not reflected in any of these calculations.","The default output result of this tool is set to ""useflashatten"", which does not match your usage scenario. Please refer to the ""Limitations"" section in Analysis Tool.",We also now have a `report_theoretical_memory.py` script now that should take the same set of arguments as `pretrain_gpt.py`. You can use like this: ```bash CUDA_DEVICE_MAX_CONNECTIONS=1 WORLD_SIZE= python u report_theoretical_memory.py ${options} ```,Marking as stale. No activity in 60 days.
kkranen,removed argument to sharded_state_dict that was not present in signature,,2023-09-28T20:39:15Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/524,Closing this since there is an internal one now.
Muennighoff,"[BUG] FLOPs computation in ""Reducing Activation Recomputation in Large Transformer Models""","In your paper (https://arxiv.org/pdf/2205.05198.pdf) you write that: `Selective activation recomputation requires an additional forward pass attention matrix computation (2Bs2h operations) and attention over values (2Bs2h operations). Adding these required FLOPs to Equation 7, the total number of FLOPs we require per iteration (denoted by hardware FLOPs) is..` However the first formula you give is `72BLs(h**2) * (1 + s/6h + v/12hL) = 72BLs(h**2) + 12BL(s**2)h + 6BLshv` while the second formula is: `72BLs(h**2) * (1 + s/3h + v/12hL) = 72BLs(h**2) + 24BL(s**2)h + 6BLshv` The difference is `12BL(s**2)h` not `4BL(s**2)h` as I would have expected given your explanation. Could you clarify why that is the case? Since ths is forward only, the additional 2x cannot come from a backward pass I guess. Thanks! Sorry if this is the wrong place but you don't provide any email in your paper.    ",2023-09-28T18:37:26Z,,closed,1,1,https://github.com/NVIDIA/Megatron-LM/issues/523,"Yes, this equation is incorrect, there are 16 BL(s**2)h FLOPs, not 24, so the second term should be 2/9 not 1/3."
kkranen,fixed issue with PEFT in which sharded_state_dict was called with badâ€¦,â€¦ input,2023-09-28T16:29:56Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/522
Lena-Jurkschat,Indented torch.init_distributed(),"Indented the init_distributed in initialize.py, to the else path of `if torch.distributed.is_initialized(),` such that it's only executed if torch distributed is really not initialized. When initializing the process group manually, it will be executed a second time due to the missing indent. This Leads to a `RuntimeError: trying to initialize the default process group twice!`",2023-09-28T11:15:29Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/521
mikolajblaz,Fix SwiGLU with dist checkpoint for inference,"With SwiGLU, the weights of `dense_h_to_4h` (linear_fc1) must be first chunked into 2 and then TPsharded. This MR implements it for **inference**. The optimizer state requires some additional handling.",2023-09-26T09:48:39Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/520
gouchangjiang,[BUG] no_sync_func in forward_backward_pipelining_without_interleaving of file schedules.py,"**Describe the bug** line 1062 of file schedules.py, duplicates the logic of setting no_sync_func. It will override logic between lines 1060 and 1061. No matter what the typing of the model is, no_sync_func will be selected as contextlib.nullcontext if it is not set in the arguments. **Proposed fix** maybe write it like  if no_sync_func is None:    if isinstance(model, torchDDP):         no_sync_func = model.no_sync    else:         no_sync_func = contextlib.nullcontext",2023-09-25T03:38:05Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/519,"I think this may be fine how it is because if the first condition is met it sets `no_sync_func = model.no_sync` so it won't be set to `contextlib.nullcontext` unless `model.no_sync` returns None ``` if no_sync_func is None and isinstance(model, torchDDP):         no_sync_func = model.no_sync if no_sync_func is None:        no_sync_func = contextlib.nullcontext ```","Yup,  is correct.",thanks   my bad.
forrestjgq,why `expand_as` is required in param backprop hook registering?,"**Your question** In DDP tensor backprop processing, the param is expanded to get grad_fn and register a hook to that fn, why is that operation required? can we just get the param.grad_fn and register hook to that? see code: https://github.com/NVIDIA/MegatronLM/blob/main/megatron/model/distributed.pyL350",2023-09-25T02:53:05Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/518,Marking as stale. No activity in 60 days.
etoilestar,[QUESTION] question about vit,**Your question** Ask a clear and concise question about MegatronLM. Why do I find that vit training imagenet does not converge well? i use the dataset tinyimagenet as well as imagenet2012,2023-09-24T03:41:21Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/517,This question is too broad to be able to help. Please reopen with more details about what you are trying to do and where it is not working as expected.
awsankur,[BUG],"**Describe the bug** Single node (8 A100 GPUs) training with pretrain_gpt.py errors out. **To Reproduce** Steps to reproduce the behavior. The easier it is to reproduce the faster it will get maintainer attention. 1. Pull Docker Image: docker pull nvcr.io/nvidia/pytorch:23.05py3 2. Run Container: docker run gpus all name pytorchcontainer d i t v /home/ubuntu/data:/data  nvcr.io/nvidia/pytorch:23.05py3 /bin/bash 3. Run Training:  torchrun standalone nnodes=1 nproc_per_node=8 /workspace/MegatronLM/pretrain_gpt.py numlayers 24 hiddensize 1024 numattentionheads 16 seqlength 1024 maxpositionembeddings 1024 microbatchsize 8 globalbatchsize 64 lr 0.00015 trainiters 500000 lrdecayiters 320000 lrdecaystyle cosine minlr 1.0e5 weightdecay 1e2 lrwarmupfraction .01 clipgrad 1.0 fp16 datapath /data/fsx/gpt2/mygpt2_text_document vocabfile /data/fsx/gpt2/gpt2vocab.json mergefile /data/fsx/gpt2/gpt2merges.txt split 949,50,1 loginterval 1 saveinterval 10000 evalinterval 1000 evaliters 40 **Expected behavior** A clear and concise description of what you expected to happen. Something like: ``` 1:  iteration        3/  286102  ``` **Stack trace/logs** If applicable, add the stack trace or logs from the time of the error. ============================================================ root:/workspace export CUDA_DEVICE_MAX_CONNECTIONS=1 root:/workspace torchrun standalone nnodes=1 nproc_per_node=8 /workspace/MegatronLM/pretrain_gpt.py numlayers 24 hiddensize 1024 numattentionheads 16 seqlength 1024 maxpositionembeddings 1024 microbatchsize 8 globalbatchsize 64 lr 0.00015 trainiters 500000 lrdecayiters 320000 lrdecaystyle cosine minlr 1.0e5 weightdecay 1e2 lrwarmupfraction .01 clipgrad 1.0 fp16 datapath /data/fsx/gpt2/mygpt2_text_document vocabfile /data/fsx/gpt2/gpt2vocab.json mergefile /data/fsx/gpt2/gpt2merges.txt split 949,50,1 loginterval 1 saveinterval 10000 evalinterval 1000 evaliters 40 master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified. WARNING:torch.distributed.run: ***************************************** Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. ***************************************** Zarrbased strategies will not be registered because of missing packages Zarrbased strategies will not be registered because of missing packages Zarrbased strategies will not be registered because of missing packages Zarrbased strategies will not be registered because of missing packages Zarrbased strategies will not be registered because of missing packages Zarrbased strategies will not be registered because of missing packages Zarrbased strategies will not be registered because of missing packages Zarrbased strategies will not be registered because of missing packages using world size: 8, dataparallelsize: 8, tensormodelparallel size: 1, pipelinemodelparallel size: 1 using torch.float16 for parameters ...  arguments    accumulate_allreduce_grads_in_fp32 .............. False   adam_beta1 ...................................... 0.9   adam_beta2 ...................................... 0.999   adam_eps ........................................ 1e08   add_bias_linear ................................. True   add_position_embedding .......................... True   adlr_autoresume ................................. False   adlr_autoresume_interval ........................ 1000   apply_layernorm_1p .............................. False   apply_query_key_layer_scaling ................... True   apply_residual_connection_post_layernorm ........ False   async_tensor_model_parallel_allreduce ........... True   attention_dropout ............................... 0.1   attention_softmax_in_fp32 ....................... False   barrier_with_L1_time ............................ True   bert_binary_head ................................ True   bert_embedder_type .............................. megatron   bert_load ....................................... None   bf16 ............................................ False   bias_dropout_fusion ............................. True   bias_gelu_fusion ................................ True   biencoder_projection_dim ........................ 0   biencoder_shared_query_context_model ............ False   block_data_path ................................. None   check_for_nan_in_loss_and_grad .................. True   classes_fraction ................................ 1.0   clip_grad ....................................... 1.0   consumed_train_samples .......................... 0   consumed_valid_samples .......................... 0   data_cache_path ................................. None   data_parallel_random_init ....................... False   data_parallel_size .............................. 8   data_path ....................................... ['/data/fsx/gpt2/mygpt2_text_document']   data_per_class_fraction ......................... 1.0   data_sharding ................................... True   dataloader_type ................................. single   decoder_num_layers .............................. None   decoder_seq_length .............................. None   dino_bottleneck_size ............................ 256   dino_freeze_last_layer .......................... 1   dino_head_hidden_size ........................... 2048   dino_local_crops_number ......................... 10   dino_local_img_size ............................. 96   dino_norm_last_layer ............................ False   dino_teacher_temp ............................... 0.07   dino_warmup_teacher_temp ........................ 0.04   dino_warmup_teacher_temp_epochs ................. 30   distribute_saved_activations .................... False   distributed_backend ............................. nccl   distributed_timeout_minutes ..................... 10   embedding_path .................................. None   embedding_weights_in_fp32 ....................... False   empty_unused_memory_level ....................... 0   encoder_num_layers .............................. 24   encoder_seq_length .............................. 1024   end_weight_decay ................................ 0.01   eod_mask_loss ................................... False   eval_interval ................................... 1000   eval_iters ...................................... 40   evidence_data_path .............................. None   exit_duration_in_mins ........................... None   exit_interval ................................... None   exit_on_missing_checkpoint ...................... False   exit_signal_handler ............................. False   ffn_hidden_size ................................. 4096   finetune ........................................ False   fp16 ............................................ True   fp16_lm_cross_entropy ........................... False   fp32_residual_connection ........................ False   fp8 ............................................. None   fp8_amax_compute_algo ........................... most_recent   fp8_amax_history_len ............................ 1   fp8_interval .................................... 1   fp8_margin ...................................... 0   fp8_wgrad ....................................... True   global_batch_size ............................... 64   gradient_accumulation_fusion .................... True   group_query_attention ........................... False   head_lr_mult .................................... 1.0   hidden_dropout .................................. 0.1   hidden_size ..................................... 1024   hysteresis ...................................... 2   ict_head_size ................................... None   ict_load ........................................ None   img_h ........................................... 224   img_w ........................................... 224   indexer_batch_size .............................. 128   indexer_log_interval ............................ 1000   inference_batch_times_seqlen_threshold .......... 512   init_method_std ................................. 0.02   init_method_xavier_uniform ...................... False   initial_loss_scale .............................. 4294967296   iter_per_epoch .................................. 1250   kv_channels ..................................... 64   lazy_mpu_init ................................... None   load ............................................ None   local_rank ...................................... None   log_batch_size_to_tensorboard ................... False   log_interval .................................... 1   log_learning_rate_to_tensorboard ................ True   log_loss_scale_to_tensorboard ................... True   log_memory_to_tensorboard ....................... False   log_num_zeros_in_grad ........................... False   log_params_norm ................................. False   log_timers_to_tensorboard ....................... False   log_validation_ppl_to_tensorboard ............... False   log_world_size_to_tensorboard ................... False   loss_scale ...................................... None   loss_scale_window ............................... 1000   lr .............................................. 0.00015   lr_decay_iters .................................. 320000   lr_decay_samples ................................ None   lr_decay_style .................................. cosine   lr_warmup_fraction .............................. 0.01   lr_warmup_init .................................. 0.0   lr_warmup_iters ................................. 0   lr_warmup_samples ............................... 0   make_vocab_size_divisible_by .................... 128   mask_factor ..................................... 1.0   mask_prob ....................................... 0.15   mask_type ....................................... random   masked_softmax_fusion ........................... True   max_position_embeddings ......................... 1024   max_tokens_to_oom ............................... 12000   merge_file ...................................... /data/fsx/gpt2/gpt2merges.txt   micro_batch_size ................................ 8   min_loss_scale .................................. 1.0   min_lr .......................................... 1e05   mmap_warmup ..................................... False   no_load_optim ................................... None   no_load_rng ..................................... None   no_persist_layer_norm ........................... False   no_save_optim ................................... None   no_save_rng ..................................... None   norm_epsilon .................................... 1e05   normalization ................................... LayerNorm   num_attention_heads ............................. 16   num_channels .................................... 3   num_classes ..................................... 1000   num_experts ..................................... None   num_layers ...................................... 24   num_layers_per_virtual_pipeline_stage ........... None   num_query_groups ................................ 1   num_workers ..................................... 2   onnx_safe ....................................... None   openai_gelu ..................................... False   optimizer ....................................... adam   output_bert_embeddings .......................... False   overlap_grad_reduce ............................. False   overlap_p2p_comm ................................ False   override_opt_param_scheduler .................... False   params_dtype .................................... torch.float16   patch_dim ....................................... 16   perform_initialization .......................... True   pipeline_model_parallel_size .................... 1   pipeline_model_parallel_split_rank .............. None   position_embedding_type ......................... learned_absolute   profile ......................................... False   profile_ranks ................................... [0]   profile_step_end ................................ 12   profile_step_start .............................. 10   query_in_block_prob ............................. 0.1   rampup_batch_size ............................... None   rank ............................................ 0   recompute_granularity ........................... None   recompute_method ................................ None   recompute_num_layers ............................ None   reset_attention_mask ............................ False   reset_position_ids .............................. False   retriever_report_topk_accuracies ................ []   retriever_score_scaling ......................... False   retriever_seq_length ............................ 256   retro_add_retriever ............................. False   retro_cyclic_train_iters ........................ None   retro_encoder_attention_dropout ................. 0.1   retro_encoder_hidden_dropout .................... 0.1   retro_encoder_layers ............................ 2   retro_num_neighbors ............................. 2   retro_num_retrieved_chunks ...................... 2   retro_return_doc_ids ............................ False   retro_workdir ................................... None   rotary_percent .................................. 1.0   rotary_seq_len_interpolation_factor ............. None   sample_rate ..................................... 1.0   save ............................................ None   save_interval ................................... 10000   scatter_gather_tensors_in_pipeline .............. True   seed ............................................ 1234   seq_length ...................................... 1024   sequence_parallel ............................... False   sgd_momentum .................................... 0.9   short_seq_prob .................................. 0.1   skip_train ...................................... False   split ........................................... 949,50,1   squared_relu .................................... False   standalone_embedding_stage ...................... False   start_weight_decay .............................. 0.01   swiglu .......................................... False   swin_backbone_type .............................. tiny   tensor_model_parallel_size ...................... 1   tensorboard_dir ................................. None   tensorboard_log_interval ........................ 1   tensorboard_queue_size .......................... 1000   test_data_path .................................. None   timing_log_level ................................ 0   timing_log_option ............................... minmax   titles_data_path ................................ None   tokenizer_model ................................. None   tokenizer_type .................................. GPT2BPETokenizer   train_data_path ................................. None   train_iters ..................................... 500000   train_samples ................................... None   transformer_impl ................................ local   transformer_pipeline_model_parallel_size ........ 1   untie_embeddings_and_output_weights ............. False   use_checkpoint_args ............................. False   use_checkpoint_opt_param_scheduler .............. False   use_cpu_initialization .......................... None   use_distributed_optimizer ....................... False   use_flash_attn .................................. False   use_one_sent_docs ............................... False   use_ring_exchange_p2p ........................... False   use_rotary_position_embeddings .................. False   valid_data_path ................................. None   variable_seq_lengths ............................ False   virtual_pipeline_model_parallel_size ............ None   vision_backbone_type ............................ vit   vision_pretraining .............................. False   vision_pretraining_type ......................... classify   vocab_extra_ids ................................. 0   vocab_file ...................................... /data/fsx/gpt2/gpt2vocab.json   vocab_size ...................................... None   weight_decay .................................... 0.01   weight_decay_incr_style ......................... constant   world_size ...................................... 8  end of arguments  setting number of microbatches to constant 1 > building GPT2BPETokenizer tokenizer ...  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304) > initializing torch distributed ... > initialized tensor model parallel with size 1 > initialized pipeline model parallel with size 1 > setting random seeds to 1234 ... > compiling dataset index builder ... make: Entering directory '/workspace/MegatronLM/megatron/data' g++ O3 Wall shared std=c++11 fPIC fdiagnosticscolor I/usr/include/python3.10 I/usr/local/lib/python3.10/distpackages/pybind11/include helpers.cpp o helpers.cpython310x86_64linuxgnu.so make: Leaving directory '/workspace/MegatronLM/megatron/data' >>> done with dataset index builder. Compilation time: 5.231 seconds > compiling and loading fused kernels ... >>> done with compiling and loading fused kernels. Compilation time: 11.328 seconds time to initialize megatron (seconds): 18.750 [after megatron is initialized] datetime: 20230922 21:36:50 building GPT model ...  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 354871296 > buckets for gradient allreduce:     params for bucket 1       module.language_model.encoder.layers.21.post_attention_norm.bias       module.language_model.encoder.layers.13.mlp.dense_4h_to_h.weight       module.language_model.encoder.layers.8.post_attention_norm.weight       module.language_model.encoder.layers.3.self_attention.query_key_value.weight       module.language_model.encoder.layers.20.mlp.dense_4h_to_h.weight       module.language_model.encoder.layers.14.mlp.dense_h_to_4h.weight       module.language_model.encoder.layers.9.self_attention.dense.weight       module.language_model.encoder.layers.4.self_attention.dense.bias       module.language_model.encoder.layers.22.mlp.dense_4h_to_h.weight       module.language_model.encoder.layers.19.input_norm.weight       module.language_model.encoder.layers.15.mlp.dense_4h_to_h.bias       module.language_model.encoder.layers.10.mlp.dense_h_to_4h.bias       module.language_model.encoder.layers.17.self_attention.query_key_value.bias       module.language_model.encoder.layers.11.post_attention_norm.bias       module.language_model.encoder.layers.6.self_attention.dense.bias       module.language_model.encoder.layers.1.input_norm.bias       module.language_model.embedding.word_embeddings.weight       module.language_model.encoder.layers.18.input_norm.weight       module.language_model.encoder.layers.23.post_attention_norm.bias       module.language_model.encoder.layers.21.self_attention.query_key_value.bias       module.language_model.encoder.layers.12.mlp.dense_4h_to_h.weight       module.language_model.encoder.layers.7.post_attention_norm.weight       module.language_model.encoder.layers.2.self_attention.query_key_value.weight       module.language_model.encoder.layers.13.mlp.dense_h_to_4h.weight       module.language_model.encoder.layers.8.self_attention.dense.weight       module.language_model.encoder.layers.3.self_attention.dense.bias       module.language_model.encoder.layers.22.self_attention.query_key_value.weight       module.language_model.encoder.layers.14.mlp.dense_4h_to_h.bias       module.language_model.encoder.layers.9.mlp.dense_h_to_4h.bias       module.language_model.encoder.layers.4.post_attention_norm.weight       module.language_model.encoder.layers.16.self_attention.query_key_value.bias       module.language_model.encoder.layers.10.post_attention_norm.bias       module.language_model.encoder.layers.20.self_attention.dense.weight       module.language_model.encoder.layers.17.input_norm.weight       module.language_model.encoder.layers.11.mlp.dense_4h_to_h.weight       module.language_model.encoder.layers.6.post_attention_norm.weight       module.language_model.encoder.layers.1.self_attention.query_key_value.weight       module.language_model.encoder.layers.23.mlp.dense_4h_to_h.weight       module.language_model.encoder.layers.18.input_norm.bias       module.language_model.encoder.layers.12.mlp.dense_h_to_4h.weight       module.language_model.encoder.layers.7.self_attention.dense.weight       module.language_model.encoder.layers.2.self_attention.dense.bias       module.language_model.encoder.layers.0.self_attention.query_key_value.bias       module.language_model.encoder.layers.0.input_norm.weight       module.language_model.encoder.layers.21.mlp.dense_4h_to_h.weight       module.language_model.encoder.layers.13.mlp.dense_4h_to_h.bias       module.language_model.encoder.layers.8.mlp.dense_h_to_4h.bias       module.language_model.encoder.layers.3.post_attention_norm.weight       module.language_model.encoder.layers.19.mlp.dense_h_to_4h.bias       module.language_model.encoder.layers.15.self_attention.query_key_value.bias       module.language_model.encoder.layers.9.post_attention_norm.bias       module.language_model.encoder.layers.4.self_attention.dense.weight       module.language_model.encoder.layers.0.input_norm.bias       module.language_model.encoder.layers.21.self_attention.query_key_value.weight       module.language_model.encoder.layers.19.mlp.dense_4h_to_h.bias       module.language_model.encoder.layers.16.input_norm.weight       module.language_model.encoder.layers.10.mlp.dense_4h_to_h.weight       module.language_model.encoder.layers.5.self_attention.dense.bias       module.language_model.encoder.layers.21.self_attention.dense.weight       module.language_model.encoder.layers.17.input_norm.bias       module.language_model.encoder.layers.11.mlp.dense_h_to_4h.weight       module.language_model.encoder.layers.6.self_attention.dense.weight       module.language_model.encoder.layers.1.self_attention.dense.bias       module.language_model.encoder.layers.0.mlp.dense_4h_to_h.weight       module.language_model.encoder.layers.18.self_attention.query_key_value.weight       module.language_model.encoder.layers.23.mlp.dense_h_to_4h.weight       module.language_model.encoder.layers.12.mlp.dense_4h_to_h.bias       module.language_model.encoder.layers.7.mlp.dense_h_to_4h.bias       module.language_model.encoder.layers.2.post_attention_norm.weight       module.language_model.encoder.layers.14.self_attention.query_key_value.bias       module.language_model.encoder.layers.8.post_attention_norm.bias       module.language_model.encoder.layers.3.self_attention.dense.weight       module.language_model.encoder.layers.22.self_attention.dense.bias       module.language_model.encoder.layers.21.input_norm.bias       module.language_model.encoder.layers.15.input_norm.weight       module.language_model.encoder.layers.9.mlp.dense_4h_to_h.weight       module.language_model.encoder.layers.4.mlp.dense_h_to_4h.bias       module.language_model.encoder.layers.20.post_attention_norm.bias       module.language_model.encoder.layers.16.input_norm.bias       module.language_model.encoder.layers.10.mlp.dense_h_to_4h.weight       module.language_model.encoder.layers.5.self_attention.dense.weight       module.language_model.encoder.layers.0.self_attention.dense.bias       module.language_model.encoder.layers.21.input_norm.weight       module.language_model.encoder.layers.17.self_attention.query_key_value.weight       module.language_model.encoder.layers.11.mlp.dense_4h_to_h.bias       module.language_model.encoder.layers.6.mlp.dense_h_to_4h.bias       module.language_model.encoder.layers.1.post_attention_norm.weight       module.language_model.encoder.layers.23.mlp.dense_4h_to_h.bias       module.language_model.encoder.layers.19.post_attention_norm.bias       module.language_model.encoder.layers.18.self_attention.dense.bias       module.language_model.encoder.layers.13.self_attention.query_key_value.bias       module.language_model.encoder.layers.7.post_attention_norm.bias       module.language_model.encoder.layers.2.self_attention.dense.weight       module.language_model.encoder.layers.21.mlp.dense_h_to_4h.weight       module.language_model.encoder.layers.14.input_norm.weight       module.language_model.encoder.layers.8.mlp.dense_4h_to_h.weight       module.language_model.encoder.layers.3.mlp.dense_h_to_4h.bias       module.language_model.encoder.layers.15.input_norm.bias       module.language_model.encoder.layers.9.mlp.dense_h_to_4h.weight       module.language_model.encoder.layers.4.post_attention_norm.bias       module.language_model.encoder.layers.16.self_attention.query_key_value.weight       module.language_model.encoder.layers.10.mlp.dense_4h_to_h.bias       module.language_model.encoder.layers.5.mlp.dense_h_to_4h.bias       module.language_model.encoder.layers.0.post_attention_norm.weight       module.language_model.encoder.layers.22.mlp.dense_4h_to_h.bias       module.language_model.encoder.layers.17.self_attention.dense.bias       module.language_model.encoder.layers.12.self_attention.query_key_value.bias       module.language_model.encoder.layers.6.post_attention_norm.bias       module.language_model.encoder.layers.1.self_attention.dense.weight       module.language_model.encoder.layers.19.post_attention_norm.weight       module.language_model.encoder.layers.18.post_attention_norm.weight       module.language_model.encoder.layers.13.input_norm.weight       module.language_model.encoder.layers.7.mlp.dense_4h_to_h.weight       module.language_model.encoder.layers.2.mlp.dense_h_to_4h.bias       module.language_model.encoder.layers.14.input_norm.bias       module.language_model.encoder.layers.8.mlp.dense_h_to_4h.weight       module.language_model.encoder.layers.3.post_attention_norm.bias       module.language_model.encoder.layers.22.post_attention_norm.weight       module.language_model.encoder.layers.15.self_attention.query_key_value.weight       module.language_model.encoder.layers.9.mlp.dense_4h_to_h.bias       module.language_model.encoder.layers.4.mlp.dense_4h_to_h.weight       module.language_model.encoder.layers.0.self_attention.query_key_value.weight       module.language_model.encoder.layers.5.post_attention_norm.bias       module.language_model.encoder.layers.0.self_attention.dense.weight       module.language_model.encoder.final_norm.bias       module.language_model.encoder.layers.21.self_attention.dense.bias       module.language_model.encoder.layers.16.self_attention.dense.bias       module.language_model.encoder.layers.11.self_attention.query_key_value.bias       module.language_model.embedding.position_embeddings.weight       module.language_model.encoder.layers.23.self_attention.query_key_value.bias       module.language_model.encoder.layers.17.post_attention_norm.weight       module.language_model.encoder.layers.12.input_norm.weight       module.language_model.encoder.layers.6.mlp.dense_4h_to_h.weight       module.language_model.encoder.layers.1.mlp.dense_h_to_4h.bias       module.language_model.encoder.layers.19.input_norm.bias       module.language_model.encoder.layers.18.self_attention.dense.weight       module.language_model.encoder.layers.13.input_norm.bias       module.language_model.encoder.layers.7.mlp.dense_h_to_4h.weight       module.language_model.encoder.layers.2.post_attention_norm.bias       module.language_model.encoder.layers.21.mlp.dense_4h_to_h.bias       module.language_model.encoder.layers.19.mlp.dense_h_to_4h.weight       module.language_model.encoder.layers.14.self_attention.query_key_value.weight       module.language_model.encoder.layers.8.mlp.dense_4h_to_h.bias       module.language_model.encoder.layers.3.mlp.dense_4h_to_h.weight       module.language_model.encoder.layers.20.input_norm.bias       module.language_model.encoder.layers.15.self_attention.dense.bias       module.language_model.encoder.layers.10.self_attention.query_key_value.bias       module.language_model.encoder.layers.4.mlp.dense_h_to_4h.weight       module.language_model.encoder.layers.16.post_attention_norm.weight       module.language_model.encoder.layers.11.input_norm.weight       module.language_model.encoder.layers.5.mlp.dense_4h_to_h.weight       module.language_model.encoder.layers.0.mlp.dense_h_to_4h.bias       module.language_model.encoder.layers.23.input_norm.weight       module.language_model.encoder.layers.17.self_attention.dense.weight       module.language_model.encoder.layers.12.input_norm.bias       module.language_model.encoder.layers.6.mlp.dense_h_to_4h.weight       module.language_model.encoder.layers.1.post_attention_norm.bias       module.language_model.encoder.layers.20.input_norm.weight       module.language_model.encoder.layers.18.mlp.dense_h_to_4h.bias       module.language_model.encoder.layers.13.self_attention.query_key_value.weight       module.language_model.encoder.layers.7.mlp.dense_4h_to_h.bias       module.language_model.encoder.layers.2.mlp.dense_4h_to_h.weight       module.language_model.encoder.layers.20.self_attention.query_key_value.weight       module.language_model.encoder.layers.14.self_attention.dense.bias       module.language_model.encoder.layers.9.self_attention.query_key_value.bias       module.language_model.encoder.layers.3.mlp.dense_h_to_4h.weight       module.language_model.encoder.layers.22.self_attention.dense.weight       module.language_model.encoder.layers.15.post_attention_norm.weight       module.language_model.encoder.layers.10.input_norm.weight       module.language_model.encoder.layers.4.mlp.dense_4h_to_h.bias       module.language_model.encoder.layers.16.self_attention.dense.weight       module.language_model.encoder.layers.11.input_norm.bias       module.language_model.encoder.layers.5.mlp.dense_h_to_4h.weight       module.language_model.encoder.layers.0.post_attention_norm.bias       module.language_model.encoder.layers.17.mlp.dense_h_to_4h.bias       module.language_model.encoder.layers.23.input_norm.bias       module.language_model.encoder.layers.12.self_attention.query_key_value.weight       module.language_model.encoder.layers.6.mlp.dense_4h_to_h.bias       module.language_model.encoder.layers.1.mlp.dense_4h_to_h.weight       module.language_model.encoder.layers.18.post_attention_norm.bias       module.language_model.encoder.layers.13.self_attention.dense.bias       module.language_model.encoder.layers.8.self_attention.query_key_value.bias       module.language_model.encoder.layers.2.mlp.dense_h_to_4h.weight       module.language_model.encoder.layers.22.self_attention.query_key_value.bias       module.language_model.encoder.layers.19.self_attention.query_key_value.weight       module.language_model.encoder.layers.14.post_attention_norm.weight       module.language_model.encoder.layers.9.input_norm.weight       module.language_model.encoder.layers.3.mlp.dense_4h_to_h.bias       module.language_model.encoder.layers.15.self_attention.dense.weight       module.language_model.encoder.layers.10.input_norm.bias       module.language_model.encoder.layers.5.self_attention.query_key_value.bias       module.language_model.encoder.layers.21.post_attention_norm.weight       module.language_model.encoder.layers.16.mlp.dense_h_to_4h.bias       module.language_model.encoder.layers.11.self_attention.query_key_value.weight       module.language_model.encoder.layers.5.mlp.dense_4h_to_h.bias       module.language_model.encoder.layers.0.mlp.dense_h_to_4h.weight       module.language_model.encoder.layers.23.self_attention.query_key_value.weight       module.language_model.encoder.layers.17.post_attention_norm.bias       module.language_model.encoder.layers.12.self_attention.dense.bias       module.language_model.encoder.layers.7.self_attention.query_key_value.bias       module.language_model.encoder.layers.1.mlp.dense_h_to_4h.weight       module.language_model.encoder.layers.18.mlp.dense_4h_to_h.weight       module.language_model.encoder.layers.19.self_attention.dense.weight       module.language_model.encoder.layers.13.post_attention_norm.weight       module.language_model.encoder.layers.8.input_norm.weight       module.language_model.encoder.layers.2.mlp.dense_4h_to_h.bias       module.language_model.encoder.layers.14.self_attention.dense.weight       module.language_model.encoder.layers.9.input_norm.bias       module.language_model.encoder.layers.4.self_attention.query_key_value.bias       module.language_model.encoder.layers.22.mlp.dense_h_to_4h.bias       module.language_model.encoder.layers.20.self_attention.query_key_value.bias       module.language_model.encoder.layers.15.mlp.dense_h_to_4h.bias       module.language_model.encoder.layers.10.self_attention.query_key_value.weight       module.language_model.encoder.layers.5.input_norm.weight       module.language_model.encoder.layers.16.post_attention_norm.bias       module.language_model.encoder.layers.11.self_attention.dense.bias       module.language_model.encoder.layers.6.self_attention.query_key_value.bias       module.language_model.encoder.layers.23.self_attention.dense.bias       module.language_model.encoder.layers.20.mlp.dense_h_to_4h.bias       module.language_model.encoder.layers.17.mlp.dense_4h_to_h.weight       module.language_model.encoder.layers.12.post_attention_norm.weight       module.language_model.encoder.layers.7.input_norm.weight       module.language_model.encoder.layers.1.mlp.dense_4h_to_h.bias       module.language_model.encoder.layers.18.mlp.dense_h_to_4h.weight       module.language_model.encoder.layers.13.self_attention.dense.weight       module.language_model.encoder.layers.8.input_norm.bias       module.language_model.encoder.layers.3.self_attention.query_key_value.bias       module.language_model.encoder.layers.22.input_norm.weight       module.language_model.encoder.layers.14.mlp.dense_h_to_4h.bias       module.language_model.encoder.layers.9.self_attention.query_key_value.weight       module.language_model.encoder.layers.4.input_norm.weight       module.language_model.encoder.layers.15.post_attention_norm.bias       module.language_model.encoder.layers.10.self_attention.dense.bias       module.language_model.encoder.layers.5.input_norm.bias       module.language_model.encoder.layers.16.mlp.dense_4h_to_h.weight       module.language_model.encoder.layers.11.post_attention_norm.weight       module.language_model.encoder.layers.6.input_norm.weight       module.language_model.encoder.layers.0.mlp.dense_4h_to_h.bias       module.language_model.encoder.layers.23.post_attention_norm.weight       module.language_model.encoder.layers.20.post_attention_norm.weight       module.language_model.encoder.layers.17.mlp.dense_h_to_4h.weight       module.language_model.encoder.layers.12.self_attention.dense.weight       module.language_model.encoder.layers.7.input_norm.bias       module.language_model.encoder.layers.2.self_attention.query_key_value.bias       module.language_model.encoder.layers.21.mlp.dense_h_to_4h.bias       module.language_model.encoder.layers.18.mlp.dense_4h_to_h.bias       module.language_model.encoder.layers.13.mlp.dense_h_to_4h.bias       module.language_model.encoder.layers.8.self_attention.query_key_value.weight       module.language_model.encoder.layers.3.input_norm.weight       module.language_model.encoder.layers.20.self_attention.dense.bias       module.language_model.encoder.layers.14.post_attention_norm.bias       module.language_model.encoder.layers.9.self_attention.dense.bias       module.language_model.encoder.layers.4.input_norm.bias       module.language_model.encoder.layers.22.post_attention_norm.bias       module.language_model.encoder.layers.15.mlp.dense_4h_to_h.weight       module.language_model.encoder.layers.10.post_attention_norm.weight       module.language_model.encoder.layers.5.self_attention.query_key_value.weight       module.language_model.encoder.layers.16.mlp.dense_h_to_4h.weight       module.language_model.encoder.layers.11.self_attention.dense.weight       module.language_model.encoder.layers.6.input_norm.bias       module.language_model.encoder.layers.1.self_attention.query_key_value.bias       module.language_model.encoder.layers.23.self_attention.dense.weight       module.language_model.encoder.layers.17.mlp.dense_4h_to_h.bias       module.language_model.encoder.layers.12.mlp.dense_h_to_4h.bias       module.language_model.encoder.layers.7.self_attention.query_key_value.weight       module.language_model.encoder.layers.2.input_norm.weight       module.language_model.encoder.layers.3.input_norm.bias       module.language_model.encoder.final_norm.weight       module.language_model.encoder.layers.20.mlp.dense_h_to_4h.weight       module.language_model.encoder.layers.19.mlp.dense_4h_to_h.weight       module.language_model.encoder.layers.19.self_attention.query_key_value.bias       module.language_model.encoder.layers.13.post_attention_norm.bias       module.language_model.encoder.layers.8.self_attention.dense.bias       module.language_model.encoder.layers.22.input_norm.bias       module.language_model.encoder.layers.19.self_attention.dense.bias       module.language_model.encoder.layers.14.mlp.dense_4h_to_h.weight       module.language_model.encoder.layers.9.post_attention_norm.weight       module.language_model.encoder.layers.4.self_attention.query_key_value.weight       module.language_model.encoder.layers.15.mlp.dense_h_to_4h.weight       module.language_model.encoder.layers.10.self_attention.dense.weight       module.language_model.encoder.layers.5.post_attention_norm.weight       module.language_model.encoder.layers.22.mlp.dense_h_to_4h.weight       module.language_model.encoder.layers.16.mlp.dense_4h_to_h.bias       module.language_model.encoder.layers.11.mlp.dense_h_to_4h.bias       module.language_model.encoder.layers.6.self_attention.query_key_value.weight       module.language_model.encoder.layers.1.input_norm.weight       module.language_model.encoder.layers.23.mlp.dense_h_to_4h.bias       module.language_model.encoder.layers.20.mlp.dense_4h_to_h.bias       module.language_model.encoder.layers.18.self_attention.query_key_value.bias       module.language_model.encoder.layers.12.post_attention_norm.bias       module.language_model.encoder.layers.7.self_attention.dense.bias       module.language_model.encoder.layers.2.input_norm.bias      total number of elements: 354871296 > learning rate decay style: cosine [after model, optimizer, and learning rate scheduler are built] datetime: 20230922 21:36:50 > building train, validation, and test datasets ...  > datasets target sizes (minimum size):     train:      32000000     validation: 1282560     test:       2560 > building train, validation, and test datasets for GPT ... Single data path provided for train, valid & test  > building dataset index ...     reading sequence lengths...     reading sequence pointers...     reading document indices...     creating np buffer of mmap...     creating memory view of np buffer...  > finished creating indexed dataset in 0.001909 seconds     number of documents: 79000  > dataset split:     train:      document indices in [0, 74971) total of 74971 documents     validation:      document indices in [74971, 78921) total of 3950 documents     test:      document indices in [78921, 79000) total of 79 documents [173632d0f02d:986  :0:1411] Caught signal 7 (Bus error: nonexistent physical address) ==== backtrace (tid:   1411) ====  0 0x0000000000042520 __sigaction()  ???:0  1 0x00000000001afbba __nss_database_lookup()  ???:0  2 0x000000000008e1ca ncclGroupEnd()  ???:0  3 0x000000000007cec7 ncclGroupEnd()  ???:0  4 0x000000000007f232 ncclGroupEnd()  ???:0  5 0x0000000000094b43 pthread_condattr_setpshared()  ???:0  6 0x0000000000125bb4 clone()  ???:0 ================================= WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 984 closing signal SIGTERM WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 985 closing signal SIGTERM WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 987 closing signal SIGTERM WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 988 closing signal SIGTERM WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 989 closing signal SIGTERM WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 990 closing signal SIGTERM WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 991 closing signal SIGTERM ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 7) local_rank: 2 (pid: 986) of binary: /usr/bin/python Traceback (most recent call last):   File ""/usr/local/bin/torchrun"", line 33, in      sys.exit(load_entry_point('torch==2.0.0', 'console_scripts', 'torchrun')())   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper     return f(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/run.py"", line 794, in main     run(args)   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/run.py"", line 785, in run     elastic_launch(   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/launcher/api.py"", line 134, in __call__     return launch_agent(self._config, self._entrypoint, list(args))   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/launcher/api.py"", line 250, in launch_agent     raise ChildFailedError( torch.distributed.elastic.multiprocessing.errors.ChildFailedError: =================================================== /workspace/MegatronLM/pretrain_gpt.py FAILED  Failures:     Root Cause (first observed failure): [0]:   time      : 20230922_21:37:03   host      : 173632d0f02d   rank      : 2 (local_rank: 2)   exitcode  : 7 (pid: 986)   error_file:    traceback : Signal 7 (SIGBUS) received by PID 986 =================================================== **Environment (please complete the following information):**   MegatronLM commit ID: main branch   PyTorch version: 2.0.0   CUDA version: 12.2   NCCL version: 2.18.1 **Proposed fix** If you have a proposal for how to fix the issue state it here or link to a PR. **Additional context** Add any other context about the problem here.",2023-09-22T21:45:56Z,,closed,0,15,https://github.com/NVIDIA/Megatron-LM/issues/516,Are you seeing these errors within a particular docker container?,Yes . Getting these errors in the container nvcr.io/nvidia/pytorch:23.05py3,Can you check if you get the same error with `nvcr.io/nvidia/pytorch:23.04py3`?,Just tried it. I get the exact same error.,"Interesting, this works for us locally. I think this is related to your NCCL setup. Are you able to run `nccl_tests` in the same setup? Or something simple that uses `torch.distributed`: https://pytorch.org/tutorials/intermediate/dist_tuto.htmlsetup.","I am able to run NCCL tests on my node. Here is the result I get: ``` root:/opt/nccltests/build ./all_reduce_perf b 8 e 128M f 2 g 8  nThread 1 nGpus 8 minBytes 8 maxBytes 134217728 step: 2(factor) warmup iters: 5 iters: 20 agg iters: 1 validation: 1 graph: 0   Using devices   Rank  0 Group  0 Pid     66 on f218865125ae device  0 [0x10] NVIDIA A100SXM480GB   Rank  1 Group  0 Pid     66 on f218865125ae device  1 [0x10] NVIDIA A100SXM480GB   Rank  2 Group  0 Pid     66 on f218865125ae device  2 [0x20] NVIDIA A100SXM480GB   Rank  3 Group  0 Pid     66 on f218865125ae device  3 [0x20] NVIDIA A100SXM480GB   Rank  4 Group  0 Pid     66 on f218865125ae device  4 [0x90] NVIDIA A100SXM480GB   Rank  5 Group  0 Pid     66 on f218865125ae device  5 [0x90] NVIDIA A100SXM480GB   Rank  6 Group  0 Pid     66 on f218865125ae device  6 [0xa0] NVIDIA A100SXM480GB   Rank  7 Group  0 Pid     66 on f218865125ae device  7 [0xa0] NVIDIA A100SXM480GB                                                                outofplace                       inplace        size         count      type   redop    root     time   algbw   busbw wrong     time   algbw   busbw wrong         (B)    (elements)                               (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)            8             2     float     sum      1    75.64    0.00    0.00      0    74.20    0.00    0.00      0           16             4     float     sum      1    73.47    0.00    0.00      0    73.32    0.00    0.00      0           32             8     float     sum      1    73.25    0.00    0.00      0    74.40    0.00    0.00      0           64            16     float     sum      1    74.39    0.00    0.00      0    73.35    0.00    0.00      0          128            32     float     sum      1    73.84    0.00    0.00      0    73.68    0.00    0.00      0          256            64     float     sum      1    74.51    0.00    0.01      0    74.40    0.00    0.01      0          512           128     float     sum      1    73.45    0.01    0.01      0    73.21    0.01    0.01      0         1024           256     float     sum      1    75.96    0.01    0.02      0    76.13    0.01    0.02      0         2048           512     float     sum      1    86.42    0.02    0.04      0    83.34    0.02    0.04      0         4096          1024     float     sum      1    93.53    0.04    0.08      0    91.42    0.04    0.08      0         8192          2048     float     sum      1    94.95    0.09    0.15      0    94.23    0.09    0.15      0        16384          4096     float     sum      1    97.16    0.17    0.30      0    97.85    0.17    0.29      0        32768          8192     float     sum      1    111.5    0.29    0.51      0    111.8    0.29    0.51      0        65536         16384     float     sum      1    117.1    0.56    0.98      0    116.0    0.56    0.99      0       131072         32768     float     sum      1    124.3    1.05    1.84      0    123.7    1.06    1.85      0       262144         65536     float     sum      1    126.0    2.08    3.64      0    127.2    2.06    3.61      0       524288        131072     float     sum      1    138.8    3.78    6.61      0    132.1    3.97    6.95      0      1048576        262144     float     sum      1    141.3    7.42   12.99      0    143.7    7.30   12.77      0      2097152        524288     float     sum      1    152.4   13.76   24.09      0    152.6   13.74   24.05      0      4194304       1048576     float     sum      1    169.6   24.73   43.27      0    167.8   25.00   43.75      0      8388608       2097152     float     sum      1    190.1   44.13   77.23      0    192.5   43.59   76.28      0     16777216       4194304     float     sum      1    221.4   75.79  132.63      0    217.2   77.23  135.15      0     33554432       8388608     float     sum      1    356.8   94.05  164.59      0    355.7   94.33  165.08      0     67108864      16777216     float     sum      1    574.8  116.76  204.32      0    574.5  116.80  204.41      0    134217728      33554432     float     sum      1   1158.2  115.89  202.81      0   1154.2  116.29  203.51      0  Out of bounds values : 0 OK  Avg bus bandwidth    : 35.1127  ``` I am also able to train another another model with DDP using torchrun without any issues.  The issue arises only when running MegatronLM code. Since it works for you locally, how can I help you debug this?",Can you run your Megatron command with `NCCL_DEBUG=INFO` and send the logfile here?,"Here it is: root:/workspace torchrun standalone nnodes=1 nproc_per_node=8 /workspace/MegatronLM/pretrain_gpt.py numlayers 24 hiddensize 1024 numattentionheads 16 seqlength 1024 maxpositionembeddings 1024 microbatchsize 8 globalbatchsize 64 lr 0.00015 trainiters 500000 lrdecayiters 320000 lrdecaystyle cosine minlr 1.0e5 weightdecay 1e2 lrwarmupfraction .01 clipgrad 1.0 fp16 datapath /data/gpt2/mygpt2_text_document vocabfile /data/gpt2/gpt2vocab.json mergefile /data/gpt2/gpt2merges.txt split 949,50,1 loginterval 1 saveinterval 10000 evalinterval 1000 evaliters 40 master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified. WARNING:torch.distributed.run: ***************************************** Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. ***************************************** Zarrbased strategies will not be registered because of missing packages Zarrbased strategies will not be registered because of missing packages Zarrbased strategies will not be registered because of missing packages Zarrbased strategies will not be registered because of missing packages Zarrbased strategies will not be registered because of missing packages Zarrbased strategies will not be registered because of missing packages Zarrbased strategies will not be registered because of missing packages Zarrbased strategies will not be registered because of missing packages using world size: 8, dataparallelsize: 8, tensormodelparallel size: 1, pipelinemodelparallel size: 1 using torch.float16 for parameters ...  arguments    accumulate_allreduce_grads_in_fp32 .............. False   adam_beta1 ...................................... 0.9   adam_beta2 ...................................... 0.999   adam_eps ........................................ 1e08   add_bias_linear ................................. True   add_position_embedding .......................... True   adlr_autoresume ................................. False   adlr_autoresume_interval ........................ 1000   apply_layernorm_1p .............................. False   apply_query_key_layer_scaling ................... True   apply_residual_connection_post_layernorm ........ False   async_tensor_model_parallel_allreduce ........... True   attention_dropout ............................... 0.1   attention_softmax_in_fp32 ....................... False   barrier_with_L1_time ............................ True   bert_binary_head ................................ True   bert_embedder_type .............................. megatron   bert_load ....................................... None   bf16 ............................................ False   bias_dropout_fusion ............................. True   bias_gelu_fusion ................................ True   biencoder_projection_dim ........................ 0   biencoder_shared_query_context_model ............ False   block_data_path ................................. None   check_for_nan_in_loss_and_grad .................. True   classes_fraction ................................ 1.0   clip_grad ....................................... 1.0   consumed_train_samples .......................... 0   consumed_valid_samples .......................... 0   data_cache_path ................................. None   data_parallel_random_init ....................... False   data_parallel_size .............................. 8   data_path ....................................... ['/data/gpt2/mygpt2_text_document']   data_per_class_fraction ......................... 1.0   data_sharding ................................... True   dataloader_type ................................. single   decoder_num_layers .............................. None   decoder_seq_length .............................. None   dino_bottleneck_size ............................ 256   dino_freeze_last_layer .......................... 1   dino_head_hidden_size ........................... 2048   dino_local_crops_number ......................... 10   dino_local_img_size ............................. 96   dino_norm_last_layer ............................ False   dino_teacher_temp ............................... 0.07   dino_warmup_teacher_temp ........................ 0.04   dino_warmup_teacher_temp_epochs ................. 30   distribute_saved_activations .................... False   distributed_backend ............................. nccl   distributed_timeout_minutes ..................... 10   embedding_path .................................. None   embedding_weights_in_fp32 ....................... False   empty_unused_memory_level ....................... 0   encoder_num_layers .............................. 24   encoder_seq_length .............................. 1024   end_weight_decay ................................ 0.01   eod_mask_loss ................................... False   eval_interval ................................... 1000   eval_iters ...................................... 40   evidence_data_path .............................. None   exit_duration_in_mins ........................... None   exit_interval ................................... None   exit_on_missing_checkpoint ...................... False   exit_signal_handler ............................. False   ffn_hidden_size ................................. 4096   finetune ........................................ False   fp16 ............................................ True   fp16_lm_cross_entropy ........................... False   fp32_residual_connection ........................ False   fp8 ............................................. None   fp8_amax_compute_algo ........................... most_recent   fp8_amax_history_len ............................ 1   fp8_interval .................................... 1   fp8_margin ...................................... 0   fp8_wgrad ....................................... True   global_batch_size ............................... 64   gradient_accumulation_fusion .................... True   group_query_attention ........................... False   head_lr_mult .................................... 1.0   hidden_dropout .................................. 0.1   hidden_size ..................................... 1024   hysteresis ...................................... 2   ict_head_size ................................... None   ict_load ........................................ None   img_h ........................................... 224   img_w ........................................... 224   indexer_batch_size .............................. 128   indexer_log_interval ............................ 1000   inference_batch_times_seqlen_threshold .......... 512   init_method_std ................................. 0.02   init_method_xavier_uniform ...................... False   initial_loss_scale .............................. 4294967296   iter_per_epoch .................................. 1250   kv_channels ..................................... 64   lazy_mpu_init ................................... None   load ............................................ None   local_rank ...................................... None   log_batch_size_to_tensorboard ................... False   log_interval .................................... 1   log_learning_rate_to_tensorboard ................ True   log_loss_scale_to_tensorboard ................... True   log_memory_to_tensorboard ....................... False   log_num_zeros_in_grad ........................... False   log_params_norm ................................. False   log_timers_to_tensorboard ....................... False   log_validation_ppl_to_tensorboard ............... False   log_world_size_to_tensorboard ................... False   loss_scale ...................................... None   loss_scale_window ............................... 1000   lr .............................................. 0.00015   lr_decay_iters .................................. 320000   lr_decay_samples ................................ None   lr_decay_style .................................. cosine   lr_warmup_fraction .............................. 0.01   lr_warmup_init .................................. 0.0   lr_warmup_iters ................................. 0   lr_warmup_samples ............................... 0   make_vocab_size_divisible_by .................... 128   mask_factor ..................................... 1.0   mask_prob ....................................... 0.15   mask_type ....................................... random   masked_softmax_fusion ........................... True   max_position_embeddings ......................... 1024   max_tokens_to_oom ............................... 12000   merge_file ...................................... /data/gpt2/gpt2merges.txt   micro_batch_size ................................ 8   min_loss_scale .................................. 1.0   min_lr .......................................... 1e05   mmap_warmup ..................................... False   no_load_optim ................................... None   no_load_rng ..................................... None   no_persist_layer_norm ........................... False   no_save_optim ................................... None   no_save_rng ..................................... None   norm_epsilon .................................... 1e05   normalization ................................... LayerNorm   num_attention_heads ............................. 16   num_channels .................................... 3   num_classes ..................................... 1000   num_experts ..................................... None   num_layers ...................................... 24   num_layers_per_virtual_pipeline_stage ........... None   num_query_groups ................................ 1   num_workers ..................................... 2   onnx_safe ....................................... None   openai_gelu ..................................... False   optimizer ....................................... adam   output_bert_embeddings .......................... False   overlap_grad_reduce ............................. False   overlap_p2p_comm ................................ False   override_opt_param_scheduler .................... False   params_dtype .................................... torch.float16   patch_dim ....................................... 16   perform_initialization .......................... True   pipeline_model_parallel_size .................... 1   pipeline_model_parallel_split_rank .............. None   position_embedding_type ......................... learned_absolute   profile ......................................... False   profile_ranks ................................... [0]   profile_step_end ................................ 12   profile_step_start .............................. 10   query_in_block_prob ............................. 0.1   rampup_batch_size ............................... None   rank ............................................ 0   recompute_granularity ........................... None   recompute_method ................................ None   recompute_num_layers ............................ None   reset_attention_mask ............................ False   reset_position_ids .............................. False   retriever_report_topk_accuracies ................ []   retriever_score_scaling ......................... False   retriever_seq_length ............................ 256   retro_add_retriever ............................. False   retro_cyclic_train_iters ........................ None   retro_encoder_attention_dropout ................. 0.1   retro_encoder_hidden_dropout .................... 0.1   retro_encoder_layers ............................ 2   retro_num_neighbors ............................. 2   retro_num_retrieved_chunks ...................... 2   retro_return_doc_ids ............................ False   retro_workdir ................................... None   rotary_percent .................................. 1.0   rotary_seq_len_interpolation_factor ............. None   sample_rate ..................................... 1.0   save ............................................ None   save_interval ................................... 10000   scatter_gather_tensors_in_pipeline .............. True   seed ............................................ 1234   seq_length ...................................... 1024   sequence_parallel ............................... False   sgd_momentum .................................... 0.9   short_seq_prob .................................. 0.1   skip_train ...................................... False   split ........................................... 949,50,1   squared_relu .................................... False   standalone_embedding_stage ...................... False   start_weight_decay .............................. 0.01   swiglu .......................................... False   swin_backbone_type .............................. tiny   tensor_model_parallel_size ...................... 1   tensorboard_dir ................................. None   tensorboard_log_interval ........................ 1   tensorboard_queue_size .......................... 1000   test_data_path .................................. None   timing_log_level ................................ 0   timing_log_option ............................... minmax   titles_data_path ................................ None   tokenizer_model ................................. None   tokenizer_type .................................. GPT2BPETokenizer   train_data_path ................................. None   train_iters ..................................... 500000   train_samples ................................... None   transformer_impl ................................ local   transformer_pipeline_model_parallel_size ........ 1   untie_embeddings_and_output_weights ............. False   use_checkpoint_args ............................. False   use_checkpoint_opt_param_scheduler .............. False   use_cpu_initialization .......................... None   use_distributed_optimizer ....................... False   use_flash_attn .................................. False   use_one_sent_docs ............................... False   use_ring_exchange_p2p ........................... False   use_rotary_position_embeddings .................. False   valid_data_path ................................. None   variable_seq_lengths ............................ False   virtual_pipeline_model_parallel_size ............ None   vision_backbone_type ............................ vit   vision_pretraining .............................. False   vision_pretraining_type ......................... classify   vocab_extra_ids ................................. 0   vocab_file ...................................... /data/gpt2/gpt2vocab.json   vocab_size ...................................... None   weight_decay .................................... 0.01   weight_decay_incr_style ......................... constant   world_size ...................................... 8  end of arguments  setting number of microbatches to constant 1 > building GPT2BPETokenizer tokenizer ...  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304) > initializing torch distributed ... > initialized tensor model parallel with size 1 > initialized pipeline model parallel with size 1 > setting random seeds to 1234 ... > compiling dataset index builder ... make: Entering directory '/workspace/MegatronLM/megatron/data' make: Nothing to be done for 'default'. make: Leaving directory '/workspace/MegatronLM/megatron/data' >>> done with dataset index builder. Compilation time: 0.075 seconds > compiling and loading fused kernels ... 09677202c889:5779:5779 [0] NCCL INFO Bootstrap : Using eth0:172.17.0.3 09677202c889:5779:5779 [0] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v6 symbol. 09677202c889:5779:5779 [0] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin (v5) 09677202c889:5779:5779 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol. 09677202c889:5779:5779 [0] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v5) 09677202c889:5779:5779 [0] NCCL INFO cudaDriverVersion 12020 NCCL version 2.17.1+cuda12.1 09677202c889:5780:5780 [1] NCCL INFO cudaDriverVersion 12020 09677202c889:5780:5780 [1] NCCL INFO Bootstrap : Using eth0:172.17.0.3 09677202c889:5787:5787 [7] NCCL INFO cudaDriverVersion 12020 09677202c889:5787:5787 [7] NCCL INFO Bootstrap : Using eth0:172.17.0.3 09677202c889:5783:5783 [4] NCCL INFO cudaDriverVersion 12020 09677202c889:5783:5783 [4] NCCL INFO Bootstrap : Using eth0:172.17.0.3 09677202c889:5785:5785 [6] NCCL INFO cudaDriverVersion 12020 09677202c889:5782:5782 [3] NCCL INFO cudaDriverVersion 12020 09677202c889:5785:5785 [6] NCCL INFO Bootstrap : Using eth0:172.17.0.3 09677202c889:5782:5782 [3] NCCL INFO Bootstrap : Using eth0:172.17.0.3 09677202c889:5781:5781 [2] NCCL INFO cudaDriverVersion 12020 09677202c889:5784:5784 [5] NCCL INFO cudaDriverVersion 12020 09677202c889:5781:5781 [2] NCCL INFO Bootstrap : Using eth0:172.17.0.3 09677202c889:5784:5784 [5] NCCL INFO Bootstrap : Using eth0:172.17.0.3 09677202c889:5783:5783 [4] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v6 symbol. 09677202c889:5783:5783 [4] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin (v5) 09677202c889:5783:5783 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol. 09677202c889:5783:5783 [4] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v5) 09677202c889:5785:5785 [6] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v6 symbol. 09677202c889:5785:5785 [6] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin (v5) 09677202c889:5782:5782 [3] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v6 symbol. 09677202c889:5785:5785 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol. 09677202c889:5785:5785 [6] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v5) 09677202c889:5782:5782 [3] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin (v5) 09677202c889:5782:5782 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol. 09677202c889:5782:5782 [3] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v5) 09677202c889:5781:5781 [2] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v6 symbol. 09677202c889:5781:5781 [2] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin (v5) 09677202c889:5781:5781 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol. 09677202c889:5781:5781 [2] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v5) 09677202c889:5787:5787 [7] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v6 symbol. 09677202c889:5787:5787 [7] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin (v5) 09677202c889:5787:5787 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol. 09677202c889:5787:5787 [7] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v5) 09677202c889:5780:5780 [1] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v6 symbol. 09677202c889:5780:5780 [1] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin (v5) 09677202c889:5780:5780 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol. 09677202c889:5780:5780 [1] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v5) 09677202c889:5784:5784 [5] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v6 symbol. 09677202c889:5784:5784 [5] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin (v5) 09677202c889:5784:5784 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol. 09677202c889:5784:5784 [5] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v5) 09677202c889:5779:6160 [0] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libncclnet.so 09677202c889:5779:6160 [0] NCCL INFO P2P plugin IBext 09677202c889:5779:6160 [0] NCCL INFO NET/IB : No device found. 09677202c889:5779:6160 [0] NCCL INFO NET/IB : No device found. 09677202c889:5779:6160 [0] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.3 09677202c889:5779:6160 [0] NCCL INFO Using network Socket 09677202c889:5783:6165 [4] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libncclnet.so 09677202c889:5783:6165 [4] NCCL INFO P2P plugin IBext 09677202c889:5783:6165 [4] NCCL INFO NET/IB : No device found. 09677202c889:5783:6165 [4] NCCL INFO NET/IB : No device found. 09677202c889:5783:6165 [4] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.3 09677202c889:5783:6165 [4] NCCL INFO Using network Socket 09677202c889:5781:6168 [2] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libncclnet.so 09677202c889:5781:6168 [2] NCCL INFO P2P plugin IBext 09677202c889:5781:6168 [2] NCCL INFO NET/IB : No device found. 09677202c889:5781:6168 [2] NCCL INFO NET/IB : No device found. 09677202c889:5781:6168 [2] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.3 09677202c889:5781:6168 [2] NCCL INFO Using network Socket 09677202c889:5787:6171 [7] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libncclnet.so 09677202c889:5787:6171 [7] NCCL INFO P2P plugin IBext 09677202c889:5787:6171 [7] NCCL INFO NET/IB : No device found. 09677202c889:5787:6171 [7] NCCL INFO NET/IB : No device found. 09677202c889:5787:6171 [7] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.3 09677202c889:5787:6171 [7] NCCL INFO Using network Socket 09677202c889:5782:6167 [3] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libncclnet.so 09677202c889:5782:6167 [3] NCCL INFO P2P plugin IBext 09677202c889:5782:6167 [3] NCCL INFO NET/IB : No device found. 09677202c889:5782:6167 [3] NCCL INFO NET/IB : No device found. 09677202c889:5782:6167 [3] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.3 09677202c889:5782:6167 [3] NCCL INFO Using network Socket 09677202c889:5780:6172 [1] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libncclnet.so 09677202c889:5780:6172 [1] NCCL INFO P2P plugin IBext 09677202c889:5780:6172 [1] NCCL INFO NET/IB : No device found. 09677202c889:5780:6172 [1] NCCL INFO NET/IB : No device found. 09677202c889:5780:6172 [1] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.3 09677202c889:5780:6172 [1] NCCL INFO Using network Socket 09677202c889:5785:6166 [6] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libncclnet.so 09677202c889:5785:6166 [6] NCCL INFO P2P plugin IBext 09677202c889:5785:6166 [6] NCCL INFO NET/IB : No device found. 09677202c889:5785:6166 [6] NCCL INFO NET/IB : No device found. 09677202c889:5785:6166 [6] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.3 09677202c889:5785:6166 [6] NCCL INFO Using network Socket 09677202c889:5784:6174 [5] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libncclnet.so 09677202c889:5784:6174 [5] NCCL INFO P2P plugin IBext 09677202c889:5784:6174 [5] NCCL INFO NET/IB : No device found. 09677202c889:5784:6174 [5] NCCL INFO NET/IB : No device found. 09677202c889:5784:6174 [5] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.3 09677202c889:5784:6174 [5] NCCL INFO Using network Socket 09677202c889:5787:6171 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000 09677202c889:5784:6174 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000 09677202c889:5783:6165 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000 09677202c889:5782:6167 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff 09677202c889:5780:6172 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff 09677202c889:5781:6168 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff 09677202c889:5779:6160 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff 09677202c889:5785:6166 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000 09677202c889:5779:6160 [0] NCCL INFO Channel 00/24 :    0   1   2   3   4   5   6   7 09677202c889:5779:6160 [0] NCCL INFO Channel 01/24 :    0   1   2   3   4   5   6   7 09677202c889:5787:6171 [7] NCCL INFO Trees [0] 1/1/1>7>6 [1] 1/1/1>7>6 [2] 1/1/1>7>6 [3] 1/1/1>7>6 [4] 1/1/1>7>6 [5] 1/1/1>7>6 [6] 1/1/1>7>6 [7] 1/1/1>7>6 [8] 1/1/1>7>6 [9] 1/1/1>7>6 [10] 1/1/1>7>6 [11] 1/1/1>7>6 [12] 1/1/1>7>6 [13] 1/1/1>7>6 [14] 1/1/1>7>6 [15] 1/1/1>7>6 [16] 1/1/1>7>6 [17] 1/1/1>7>6 [18] 1/1/1>7>6 [19] 1/1/1>7>6 [20] 1/1/1>7>6 [21] 1/1/1>7>6 [22] 1/1/1>7>6 [23] 1/1/1>7>6 09677202c889:5785:6166 [6] NCCL INFO Trees [0] 7/1/1>6>5 [1] 7/1/1>6>5 [2] 7/1/1>6>5 [3] 7/1/1>6>5 [4] 7/1/1>6>5 [5] 7/1/1>6>5 [6] 7/1/1>6>5 [7] 7/1/1>6>5 [8] 7/1/1>6>5 [9] 7/1/1>6>5 [10] 7/1/1>6>5 [11] 7/1/1>6>5 [12] 7/1/1>6>5 [13] 7/1/1>6>5 [14] 7/1/1>6>5 [15] 7/1/1>6>5 [16] 7/1/1>6>5 [17] 7/1/1>6>5 [18] 7/1/1>6>5 [19] 7/1/1>6>5 [20] 7/1/1>6>5 [21] 7/1/1>6>5 [22] 7/1/1>6>5 [23] 7/1/1>6>5 09677202c889:5779:6160 [0] NCCL INFO Channel 02/24 :    0   1   2   3   4   5   6   7 09677202c889:5784:6174 [5] NCCL INFO Trees [0] 6/1/1>5>4 [1] 6/1/1>5>4 [2] 6/1/1>5>4 [3] 6/1/1>5>4 [4] 6/1/1>5>4 [5] 6/1/1>5>4 [6] 6/1/1>5>4 [7] 6/1/1>5>4 [8] 6/1/1>5>4 [9] 6/1/1>5>4 [10] 6/1/1>5>4 [11] 6/1/1>5>4 [12] 6/1/1>5>4 [13] 6/1/1>5>4 [14] 6/1/1>5>4 [15] 6/1/1>5>4 [16] 6/1/1>5>4 [17] 6/1/1>5>4 [18] 6/1/1>5>4 [19] 6/1/1>5>4 [20] 6/1/1>5>4 [21] 6/1/1>5>4 [22] 6/1/1>5>4 [23] 6/1/1>5>4 09677202c889:5787:6171 [7] NCCL INFO P2P Chunksize set to 524288 09677202c889:5785:6166 [6] NCCL INFO P2P Chunksize set to 524288 09677202c889:5779:6160 [0] NCCL INFO Channel 03/24 :    0   1   2   3   4   5   6   7 09677202c889:5780:6172 [1] NCCL INFO Trees [0] 2/1/1>1>0 [1] 2/1/1>1>0 [2] 2/1/1>1>0 [3] 2/1/1>1>0 [4] 2/1/1>1>0 [5] 2/1/1>1>0 [6] 2/1/1>1>0 [7] 2/1/1>1>0 [8] 2/1/1>1>0 [9] 2/1/1>1>0 [10] 2/1/1>1>0 [11] 2/1/1>1>0 [12] 2/1/1>1>0 [13] 2/1/1>1>0 [14] 2/1/1>1>0 [15] 2/1/1>1>0 [16] 2/1/1>1>0 [17] 2/1/1>1>0 [18] 2/1/1>1>0 [19] 2/1/1>1>0 [20] 2/1/1>1>0 [21] 2/1/1>1>0 [22] 2/1/1>1>0 [23] 2/1/1>1>0 09677202c889:5783:6165 [4] NCCL INFO Trees [0] 5/1/1>4>3 [1] 5/1/1>4>3 [2] 5/1/1>4>3 [3] 5/1/1>4>3 [4] 5/1/1>4>3 [5] 5/1/1>4>3 [6] 5/1/1>4>3 [7] 5/1/1>4>3 [8] 5/1/1>4>3 [9] 5/1/1>4>3 [10] 5/1/1>4>3 [11] 5/1/1>4>3 [12] 5/1/1>4>3 [13] 5/1/1>4>3 [14] 5/1/1>4>3 [15] 5/1/1>4>3 [16] 5/1/1>4>3 [17] 5/1/1>4>3 [18] 5/1/1>4>3 [19] 5/1/1>4>3 [20] 5/1/1>4>3 [21] 5/1/1>4>3 [22] 5/1/1>4>3 [23] 5/1/1>4>3 09677202c889:5781:6168 [2] NCCL INFO Trees [0] 3/1/1>2>1 [1] 3/1/1>2>1 [2] 3/1/1>2>1 [3] 3/1/1>2>1 [4] 3/1/1>2>1 [5] 3/1/1>2>1 [6] 3/1/1>2>1 [7] 3/1/1>2>1 [8] 3/1/1>2>1 [9] 3/1/1>2>1 [10] 3/1/1>2>1 [11] 3/1/1>2>1 [12] 3/1/1>2>1 [13] 3/1/1>2>1 [14] 3/1/1>2>1 [15] 3/1/1>2>1 [16] 3/1/1>2>1 [17] 3/1/1>2>1 [18] 3/1/1>2>1 [19] 3/1/1>2>1 [20] 3/1/1>2>1 [21] 3/1/1>2>1 [22] 3/1/1>2>1 [23] 3/1/1>2>1 09677202c889:5784:6174 [5] NCCL INFO P2P Chunksize set to 524288 09677202c889:5779:6160 [0] NCCL INFO Channel 04/24 :    0   1   2   3   4   5   6   7 09677202c889:5780:6172 [1] NCCL INFO P2P Chunksize set to 524288 09677202c889:5782:6167 [3] NCCL INFO Trees [0] 4/1/1>3>2 [1] 4/1/1>3>2 [2] 4/1/1>3>2 [3] 4/1/1>3>2 [4] 4/1/1>3>2 [5] 4/1/1>3>2 [6] 4/1/1>3>2 [7] 4/1/1>3>2 [8] 4/1/1>3>2 [9] 4/1/1>3>2 [10] 4/1/1>3>2 [11] 4/1/1>3>2 [12] 4/1/1>3>2 [13] 4/1/1>3>2 [14] 4/1/1>3>2 [15] 4/1/1>3>2 [16] 4/1/1>3>2 [17] 4/1/1>3>2 [18] 4/1/1>3>2 [19] 4/1/1>3>2 [20] 4/1/1>3>2 [21] 4/1/1>3>2 [22] 4/1/1>3>2 [23] 4/1/1>3>2 09677202c889:5783:6165 [4] NCCL INFO P2P Chunksize set to 524288 09677202c889:5781:6168 [2] NCCL INFO P2P Chunksize set to 524288 09677202c889:5779:6160 [0] NCCL INFO Channel 05/24 :    0   1   2   3   4   5   6   7 09677202c889:5782:6167 [3] NCCL INFO P2P Chunksize set to 524288 09677202c889:5779:6160 [0] NCCL INFO Channel 06/24 :    0   1   2   3   4   5   6   7 09677202c889:5779:6160 [0] NCCL INFO Channel 07/24 :    0   1   2   3   4   5   6   7 09677202c889:5779:6160 [0] NCCL INFO Channel 08/24 :    0   1   2   3   4   5   6   7 09677202c889:5779:6160 [0] NCCL INFO Channel 09/24 :    0   1   2   3   4   5   6   7 09677202c889:5779:6160 [0] NCCL INFO Channel 10/24 :    0   1   2   3   4   5   6   7 09677202c889:5779:6160 [0] NCCL INFO Channel 11/24 :    0   1   2   3   4   5   6   7 09677202c889:5779:6160 [0] NCCL INFO Channel 12/24 :    0   1   2   3   4   5   6   7 09677202c889:5779:6160 [0] NCCL INFO Channel 13/24 :    0   1   2   3   4   5   6   7 09677202c889:5779:6160 [0] NCCL INFO Channel 14/24 :    0   1   2   3   4   5   6   7 09677202c889:5779:6160 [0] NCCL INFO Channel 15/24 :    0   1   2   3   4   5   6   7 09677202c889:5779:6160 [0] NCCL INFO Channel 16/24 :    0   1   2   3   4   5   6   7 09677202c889:5779:6160 [0] NCCL INFO Channel 17/24 :    0   1   2   3   4   5   6   7 09677202c889:5779:6160 [0] NCCL INFO Channel 18/24 :    0   1   2   3   4   5   6   7 09677202c889:5779:6160 [0] NCCL INFO Channel 19/24 :    0   1   2   3   4   5   6   7 09677202c889:5779:6160 [0] NCCL INFO Channel 20/24 :    0   1   2   3   4   5   6   7 09677202c889:5779:6160 [0] NCCL INFO Channel 21/24 :    0   1   2   3   4   5   6   7 09677202c889:5779:6160 [0] NCCL INFO Channel 22/24 :    0   1   2   3   4   5   6   7 09677202c889:5779:6160 [0] NCCL INFO Channel 23/24 :    0   1   2   3   4   5   6   7 09677202c889:5779:6160 [0] NCCL INFO Trees [0] 1/1/1>0>1 [1] 1/1/1>0>1 [2] 1/1/1>0>1 [3] 1/1/1>0>1 [4] 1/1/1>0>1 [5] 1/1/1>0>1 [6] 1/1/1>0>1 [7] 1/1/1>0>1 [8] 1/1/1>0>1 [9] 1/1/1>0>1 [10] 1/1/1>0>1 [11] 1/1/1>0>1 [12] 1/1/1>0>1 [13] 1/1/1>0>1 [14] 1/1/1>0>1 [15] 1/1/1>0>1 [16] 1/1/1>0>1 [17] 1/1/1>0>1 [18] 1/1/1>0>1 [19] 1/1/1>0>1 [20] 1/1/1>0>1 [21] 1/1/1>0>1 [22] 1/1/1>0>1 [23] 1/1/1>0>1 09677202c889:5779:6160 [0] NCCL INFO P2P Chunksize set to 524288 09677202c889:6257:6638 [6] NCCL INFO Channel 00/0 : 6[a01c0] > 7[a01d0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 00/0 : 1[101d0] > 2[201c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 00/0 : 4[901c0] > 5[901d0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 00/0 : 3[201d0] > 4[901c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 00/0 : 7[a01d0] > 0[101c0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 00/0 : 2[201c0] > 3[201d0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 00/0 : 5[901d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6251:6632 [0] NCCL INFO Channel 00/0 : 0[101c0] > 1[101d0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 01/0 : 1[101d0] > 2[201c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 01/0 : 4[901c0] > 5[901d0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 01/0 : 3[201d0] > 4[901c0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 01/0 : 6[a01c0] > 7[a01d0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 01/0 : 7[a01d0] > 0[101c0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 01/0 : 2[201c0] > 3[201d0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 01/0 : 5[901d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6251:6632 [0] NCCL INFO Channel 01/0 : 0[101c0] > 1[101d0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 02/0 : 6[a01c0] > 7[a01d0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 02/0 : 4[901c0] > 5[901d0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 02/0 : 3[201d0] > 4[901c0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 02/0 : 1[101d0] > 2[201c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 02/0 : 7[a01d0] > 0[101c0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 02/0 : 2[201c0] > 3[201d0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 02/0 : 5[901d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6251:6632 [0] NCCL INFO Channel 02/0 : 0[101c0] > 1[101d0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 03/0 : 1[101d0] > 2[201c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 03/0 : 4[901c0] > 5[901d0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 03/0 : 6[a01c0] > 7[a01d0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 03/0 : 3[201d0] > 4[901c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 03/0 : 7[a01d0] > 0[101c0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 03/0 : 2[201c0] > 3[201d0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 03/0 : 5[901d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6251:6632 [0] NCCL INFO Channel 03/0 : 0[101c0] > 1[101d0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 04/0 : 1[101d0] > 2[201c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 04/0 : 4[901c0] > 5[901d0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 04/0 : 6[a01c0] > 7[a01d0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 04/0 : 3[201d0] > 4[901c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 04/0 : 7[a01d0] > 0[101c0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 04/0 : 2[201c0] > 3[201d0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 04/0 : 5[901d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6251:6632 [0] NCCL INFO Channel 04/0 : 0[101c0] > 1[101d0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 05/0 : 1[101d0] > 2[201c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 05/0 : 4[901c0] > 5[901d0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 05/0 : 6[a01c0] > 7[a01d0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 05/0 : 3[201d0] > 4[901c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 05/0 : 7[a01d0] > 0[101c0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 05/0 : 2[201c0] > 3[201d0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 05/0 : 5[901d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 06/0 : 6[a01c0] > 7[a01d0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 06/0 : 4[901c0] > 5[901d0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 06/0 : 1[101d0] > 2[201c0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 06/0 : 3[201d0] > 4[901c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 06/0 : 7[a01d0] > 0[101c0] via P2P/IPC/read 09677202c889:6251:6632 [0] NCCL INFO Channel 05/0 : 0[101c0] > 1[101d0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 06/0 : 2[201c0] > 3[201d0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 07/0 : 4[901c0] > 5[901d0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 06/0 : 5[901d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 07/0 : 1[101d0] > 2[201c0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 07/0 : 3[201d0] > 4[901c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 07/0 : 7[a01d0] > 0[101c0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 07/0 : 6[a01c0] > 7[a01d0] via P2P/IPC/read 09677202c889:6251:6632 [0] NCCL INFO Channel 06/0 : 0[101c0] > 1[101d0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 07/0 : 2[201c0] > 3[201d0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 08/0 : 4[901c0] > 5[901d0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 08/0 : 1[101d0] > 2[201c0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 08/0 : 3[201d0] > 4[901c0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 07/0 : 5[901d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 08/0 : 7[a01d0] > 0[101c0] via P2P/IPC/read 09677202c889:6251:6632 [0] NCCL INFO Channel 07/0 : 0[101c0] > 1[101d0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 08/0 : 2[201c0] > 3[201d0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 08/0 : 6[a01c0] > 7[a01d0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 09/0 : 4[901c0] > 5[901d0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 09/0 : 1[101d0] > 2[201c0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 09/0 : 3[201d0] > 4[901c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 09/0 : 7[a01d0] > 0[101c0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 09/0 : 2[201c0] > 3[201d0] via P2P/IPC/read 09677202c889:6251:6632 [0] NCCL INFO Channel 08/0 : 0[101c0] > 1[101d0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 09/0 : 6[a01c0] > 7[a01d0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 08/0 : 5[901d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 10/0 : 4[901c0] > 5[901d0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 10/0 : 1[101d0] > 2[201c0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 10/0 : 3[201d0] > 4[901c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 10/0 : 7[a01d0] > 0[101c0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 10/0 : 2[201c0] > 3[201d0] via P2P/IPC/read 09677202c889:6251:6632 [0] NCCL INFO Channel 09/0 : 0[101c0] > 1[101d0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 09/0 : 5[901d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 10/0 : 6[a01c0] > 7[a01d0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 11/0 : 4[901c0] > 5[901d0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 11/0 : 1[101d0] > 2[201c0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 11/0 : 3[201d0] > 4[901c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 11/0 : 7[a01d0] > 0[101c0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 11/0 : 2[201c0] > 3[201d0] via P2P/IPC/read 09677202c889:6251:6632 [0] NCCL INFO Channel 10/0 : 0[101c0] > 1[101d0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 11/0 : 6[a01c0] > 7[a01d0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 10/0 : 5[901d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 12/0 : 4[901c0] > 5[901d0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 12/0 : 3[201d0] > 4[901c0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 12/0 : 1[101d0] > 2[201c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 12/0 : 7[a01d0] > 0[101c0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 12/0 : 2[201c0] > 3[201d0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 11/0 : 5[901d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 12/0 : 6[a01c0] > 7[a01d0] via P2P/IPC/read 09677202c889:6251:6632 [0] NCCL INFO Channel 11/0 : 0[101c0] > 1[101d0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 13/0 : 1[101d0] > 2[201c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 13/0 : 4[901c0] > 5[901d0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 13/0 : 3[201d0] > 4[901c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 13/0 : 7[a01d0] > 0[101c0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 13/0 : 2[201c0] > 3[201d0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 13/0 : 6[a01c0] > 7[a01d0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 12/0 : 5[901d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6251:6632 [0] NCCL INFO Channel 12/0 : 0[101c0] > 1[101d0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 14/0 : 4[901c0] > 5[901d0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 14/0 : 1[101d0] > 2[201c0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 14/0 : 3[201d0] > 4[901c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 14/0 : 7[a01d0] > 0[101c0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 14/0 : 2[201c0] > 3[201d0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 14/0 : 6[a01c0] > 7[a01d0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 13/0 : 5[901d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6251:6632 [0] NCCL INFO Channel 13/0 : 0[101c0] > 1[101d0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 15/0 : 4[901c0] > 5[901d0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 15/0 : 1[101d0] > 2[201c0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 15/0 : 3[201d0] > 4[901c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 15/0 : 7[a01d0] > 0[101c0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 15/0 : 2[201c0] > 3[201d0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 15/0 : 6[a01c0] > 7[a01d0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 14/0 : 5[901d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6251:6632 [0] NCCL INFO Channel 14/0 : 0[101c0] > 1[101d0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 16/0 : 1[101d0] > 2[201c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 16/0 : 4[901c0] > 5[901d0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 16/0 : 3[201d0] > 4[901c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 16/0 : 7[a01d0] > 0[101c0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 16/0 : 2[201c0] > 3[201d0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 16/0 : 6[a01c0] > 7[a01d0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 15/0 : 5[901d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6251:6632 [0] NCCL INFO Channel 15/0 : 0[101c0] > 1[101d0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 17/0 : 1[101d0] > 2[201c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 17/0 : 4[901c0] > 5[901d0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 17/0 : 3[201d0] > 4[901c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 17/0 : 7[a01d0] > 0[101c0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 17/0 : 2[201c0] > 3[201d0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 17/0 : 6[a01c0] > 7[a01d0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 16/0 : 5[901d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6251:6632 [0] NCCL INFO Channel 16/0 : 0[101c0] > 1[101d0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 18/0 : 4[901c0] > 5[901d0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 18/0 : 1[101d0] > 2[201c0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 18/0 : 3[201d0] > 4[901c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 18/0 : 7[a01d0] > 0[101c0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 18/0 : 2[201c0] > 3[201d0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 18/0 : 6[a01c0] > 7[a01d0] via P2P/IPC/read 09677202c889:6251:6632 [0] NCCL INFO Channel 17/0 : 0[101c0] > 1[101d0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 17/0 : 5[901d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 19/0 : 4[901c0] > 5[901d0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 19/0 : 1[101d0] > 2[201c0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 19/0 : 3[201d0] > 4[901c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 19/0 : 7[a01d0] > 0[101c0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 19/0 : 2[201c0] > 3[201d0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 19/0 : 6[a01c0] > 7[a01d0] via P2P/IPC/read 09677202c889:6251:6632 [0] NCCL INFO Channel 18/0 : 0[101c0] > 1[101d0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 18/0 : 5[901d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 20/0 : 4[901c0] > 5[901d0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 20/0 : 1[101d0] > 2[201c0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 20/0 : 3[201d0] > 4[901c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 20/0 : 7[a01d0] > 0[101c0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 20/0 : 2[201c0] > 3[201d0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 20/0 : 6[a01c0] > 7[a01d0] via P2P/IPC/read 09677202c889:6251:6632 [0] NCCL INFO Channel 19/0 : 0[101c0] > 1[101d0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 19/0 : 5[901d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 21/0 : 4[901c0] > 5[901d0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 21/0 : 1[101d0] > 2[201c0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 21/0 : 3[201d0] > 4[901c0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 21/0 : 2[201c0] > 3[201d0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 21/0 : 7[a01d0] > 0[101c0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 21/0 : 6[a01c0] > 7[a01d0] via P2P/IPC/read 09677202c889:6251:6632 [0] NCCL INFO Channel 20/0 : 0[101c0] > 1[101d0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 22/0 : 1[101d0] > 2[201c0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 22/0 : 3[201d0] > 4[901c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 22/0 : 4[901c0] > 5[901d0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 20/0 : 5[901d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 22/0 : 7[a01d0] > 0[101c0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 22/0 : 2[201c0] > 3[201d0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 22/0 : 6[a01c0] > 7[a01d0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 23/0 : 4[901c0] > 5[901d0] via P2P/IPC/read 09677202c889:6251:6632 [0] NCCL INFO Channel 21/0 : 0[101c0] > 1[101d0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 23/0 : 1[101d0] > 2[201c0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 23/0 : 3[201d0] > 4[901c0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 21/0 : 5[901d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 23/0 : 7[a01d0] > 0[101c0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 23/0 : 2[201c0] > 3[201d0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 23/0 : 6[a01c0] > 7[a01d0] via P2P/IPC/read 09677202c889:6251:6632 [0] NCCL INFO Channel 22/0 : 0[101c0] > 1[101d0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 22/0 : 5[901d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Connected all rings 09677202c889:6254:6636 [3] NCCL INFO Connected all rings 09677202c889:6251:6632 [0] NCCL INFO Channel 23/0 : 0[101c0] > 1[101d0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 23/0 : 5[901d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6251:6632 [0] NCCL INFO Connected all rings 09677202c889:6256:6637 [5] NCCL INFO Connected all rings 09677202c889:6259:6640 [7] NCCL INFO Connected all rings 09677202c889:6259:6640 [7] NCCL INFO Channel 00/0 : 7[a01d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Connected all rings 09677202c889:6255:6646 [4] NCCL INFO Connected all rings 09677202c889:6257:6638 [6] NCCL INFO Connected all rings 09677202c889:6259:6640 [7] NCCL INFO Channel 01/0 : 7[a01d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 02/0 : 7[a01d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 03/0 : 7[a01d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 04/0 : 7[a01d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 05/0 : 7[a01d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 06/0 : 7[a01d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 07/0 : 7[a01d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 08/0 : 7[a01d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 09/0 : 7[a01d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 10/0 : 7[a01d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 11/0 : 7[a01d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 12/0 : 7[a01d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 13/0 : 7[a01d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 14/0 : 7[a01d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 15/0 : 7[a01d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 16/0 : 7[a01d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 17/0 : 7[a01d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 18/0 : 7[a01d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 19/0 : 7[a01d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 00/0 : 3[201d0] > 2[201c0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 00/0 : 2[201c0] > 1[101d0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 20/0 : 7[a01d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 01/0 : 2[201c0] > 1[101d0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 01/0 : 3[201d0] > 2[201c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 21/0 : 7[a01d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 02/0 : 2[201c0] > 1[101d0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 02/0 : 3[201d0] > 2[201c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 22/0 : 7[a01d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 00/0 : 1[101d0] > 0[101c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 00/0 : 4[901c0] > 3[201d0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 03/0 : 2[201c0] > 1[101d0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 03/0 : 3[201d0] > 2[201c0] via P2P/IPC/read 09677202c889:6259:6640 [7] NCCL INFO Channel 23/0 : 7[a01d0] > 6[a01c0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 00/0 : 6[a01c0] > 5[901d0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 01/0 : 1[101d0] > 0[101c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 01/0 : 4[901c0] > 3[201d0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 04/0 : 2[201c0] > 1[101d0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 04/0 : 3[201d0] > 2[201c0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 00/0 : 5[901d0] > 4[901c0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 02/0 : 1[101d0] > 0[101c0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 01/0 : 6[a01c0] > 5[901d0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 02/0 : 4[901c0] > 3[201d0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 05/0 : 2[201c0] > 1[101d0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 05/0 : 3[201d0] > 2[201c0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 03/0 : 1[101d0] > 0[101c0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 01/0 : 5[901d0] > 4[901c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 03/0 : 4[901c0] > 3[201d0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 02/0 : 6[a01c0] > 5[901d0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 06/0 : 2[201c0] > 1[101d0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 06/0 : 3[201d0] > 2[201c0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 04/0 : 1[101d0] > 0[101c0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 02/0 : 5[901d0] > 4[901c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 04/0 : 4[901c0] > 3[201d0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 03/0 : 6[a01c0] > 5[901d0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 07/0 : 3[201d0] > 2[201c0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 07/0 : 2[201c0] > 1[101d0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 03/0 : 5[901d0] > 4[901c0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 05/0 : 1[101d0] > 0[101c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 05/0 : 4[901c0] > 3[201d0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 08/0 : 2[201c0] > 1[101d0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 08/0 : 3[201d0] > 2[201c0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 04/0 : 6[a01c0] > 5[901d0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 06/0 : 1[101d0] > 0[101c0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 04/0 : 5[901d0] > 4[901c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 06/0 : 4[901c0] > 3[201d0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 09/0 : 2[201c0] > 1[101d0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 09/0 : 3[201d0] > 2[201c0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 05/0 : 6[a01c0] > 5[901d0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 07/0 : 1[101d0] > 0[101c0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 05/0 : 5[901d0] > 4[901c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 07/0 : 4[901c0] > 3[201d0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 10/0 : 2[201c0] > 1[101d0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 10/0 : 3[201d0] > 2[201c0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 06/0 : 6[a01c0] > 5[901d0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 06/0 : 5[901d0] > 4[901c0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 08/0 : 1[101d0] > 0[101c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 08/0 : 4[901c0] > 3[201d0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 11/0 : 2[201c0] > 1[101d0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 11/0 : 3[201d0] > 2[201c0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 07/0 : 6[a01c0] > 5[901d0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 09/0 : 1[101d0] > 0[101c0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 07/0 : 5[901d0] > 4[901c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 09/0 : 4[901c0] > 3[201d0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 12/0 : 2[201c0] > 1[101d0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 12/0 : 3[201d0] > 2[201c0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 08/0 : 6[a01c0] > 5[901d0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 10/0 : 1[101d0] > 0[101c0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 08/0 : 5[901d0] > 4[901c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 10/0 : 4[901c0] > 3[201d0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 13/0 : 2[201c0] > 1[101d0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 13/0 : 3[201d0] > 2[201c0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 09/0 : 6[a01c0] > 5[901d0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 11/0 : 1[101d0] > 0[101c0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 09/0 : 5[901d0] > 4[901c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 11/0 : 4[901c0] > 3[201d0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 14/0 : 2[201c0] > 1[101d0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 14/0 : 3[201d0] > 2[201c0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 10/0 : 6[a01c0] > 5[901d0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 12/0 : 1[101d0] > 0[101c0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 10/0 : 5[901d0] > 4[901c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 12/0 : 4[901c0] > 3[201d0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 15/0 : 2[201c0] > 1[101d0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 15/0 : 3[201d0] > 2[201c0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 11/0 : 6[a01c0] > 5[901d0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 13/0 : 1[101d0] > 0[101c0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 11/0 : 5[901d0] > 4[901c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 13/0 : 4[901c0] > 3[201d0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 16/0 : 2[201c0] > 1[101d0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 16/0 : 3[201d0] > 2[201c0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 12/0 : 6[a01c0] > 5[901d0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 12/0 : 5[901d0] > 4[901c0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 14/0 : 1[101d0] > 0[101c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 14/0 : 4[901c0] > 3[201d0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 17/0 : 2[201c0] > 1[101d0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 17/0 : 3[201d0] > 2[201c0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 13/0 : 6[a01c0] > 5[901d0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 13/0 : 5[901d0] > 4[901c0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 15/0 : 1[101d0] > 0[101c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 15/0 : 4[901c0] > 3[201d0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 18/0 : 2[201c0] > 1[101d0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 18/0 : 3[201d0] > 2[201c0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 14/0 : 6[a01c0] > 5[901d0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 14/0 : 5[901d0] > 4[901c0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 16/0 : 1[101d0] > 0[101c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 16/0 : 4[901c0] > 3[201d0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 19/0 : 2[201c0] > 1[101d0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 19/0 : 3[201d0] > 2[201c0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 17/0 : 1[101d0] > 0[101c0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 15/0 : 6[a01c0] > 5[901d0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 15/0 : 5[901d0] > 4[901c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 17/0 : 4[901c0] > 3[201d0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 20/0 : 2[201c0] > 1[101d0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 20/0 : 3[201d0] > 2[201c0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 18/0 : 1[101d0] > 0[101c0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 16/0 : 5[901d0] > 4[901c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 18/0 : 4[901c0] > 3[201d0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 16/0 : 6[a01c0] > 5[901d0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 21/0 : 3[201d0] > 2[201c0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 21/0 : 2[201c0] > 1[101d0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 19/0 : 1[101d0] > 0[101c0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 17/0 : 5[901d0] > 4[901c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 19/0 : 4[901c0] > 3[201d0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 17/0 : 6[a01c0] > 5[901d0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 22/0 : 3[201d0] > 2[201c0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 22/0 : 2[201c0] > 1[101d0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 20/0 : 1[101d0] > 0[101c0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 18/0 : 5[901d0] > 4[901c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 20/0 : 4[901c0] > 3[201d0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 18/0 : 6[a01c0] > 5[901d0] via P2P/IPC/read 09677202c889:6254:6636 [3] NCCL INFO Channel 23/0 : 3[201d0] > 2[201c0] via P2P/IPC/read 09677202c889:6253:6644 [2] NCCL INFO Channel 23/0 : 2[201c0] > 1[101d0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 21/0 : 1[101d0] > 0[101c0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 19/0 : 5[901d0] > 4[901c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 21/0 : 4[901c0] > 3[201d0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 19/0 : 6[a01c0] > 5[901d0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 20/0 : 5[901d0] > 4[901c0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 22/0 : 1[101d0] > 0[101c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 22/0 : 4[901c0] > 3[201d0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 20/0 : 6[a01c0] > 5[901d0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 21/0 : 5[901d0] > 4[901c0] via P2P/IPC/read 09677202c889:6252:6642 [1] NCCL INFO Channel 23/0 : 1[101d0] > 0[101c0] via P2P/IPC/read 09677202c889:6255:6646 [4] NCCL INFO Channel 23/0 : 4[901c0] > 3[201d0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 21/0 : 6[a01c0] > 5[901d0] via P2P/IPC/read 09677202c889:6256:6637 [5] NCCL INFO Channel 22/0 : 5[901d0] > 4[901c0] via P2P/IPC/read 09677202c889:6257:6638 [6] NCCL INFO Channel 22/0 : 6[a01c0] > 5[901d0] via P2P/IPC/read 09677202c889:5275:5656 [0] NCCL INFO Connected all trees 09677202c889:5275:5656 [0] NCCL INFO NVLS multicast support is not available on dev 0 09677202c889:5275:5656 [0] NCCL INFO threadThresholds 8/8/64  512 09677202c889:5278:5702 [3] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer [09677202c889:5278 :0:5705] Caught signal 7 (Bus error: nonexistent physical address) ==== backtrace (tid:   5707) ====  0 0x0000000000043090 killpg()  ???:0  1 0x000000000008170d ncclGroupEnd()  ???:0  2 0x00000000000742f0 ncclGroupEnd()  ???:0  3 0x0000000000008609 start_thread()  ???:0  4 0x000000000011f133 clone()  ???:0 ================================= ==== backtrace (tid:   5709) ====  0 0x0000000000043090 killpg()  ???:0  1 0x000000000008170d ncclGroupEnd()  ???:0  2 0x00000000000742f0 ncclGroupEnd()  ???:0  3 0x0000000000008609 start_thread()  ???:0  4 0x000000000011f133 clone()  ???:0 ================================= ==== backtrace (tid:   5705) ====  0 0x0000000000043090 killpg()  ???:0  1 0x000000000008170d ncclGroupEnd()  ???:0  2 0x00000000000742f0 ncclGroupEnd()  ???:0  3 0x0000000000008609 start_thread()  ???:0  4 0x000000000011f133 clone()  ???:0 ================================= ==== backtrace (tid:   5708) ====  0 0x0000000000043090 killpg()  ???:0  1 0x000000000008170d ncclGroupEnd()  ???:0  2 0x00000000000742f0 ncclGroupEnd()  ???:0  3 0x0000000000008609 start_thread()  ???:0  4 0x000000000011f133 clone()  ???:0 ================================= ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 7) local_rank: 0 (pid: 5275) of binary: /usr/bin/python Traceback (most recent call last):   File ""/usr/local/bin/torchrun"", line 33, in      sys.exit(load_entry_point('torch==2.1.0a0+fe05266', 'console_scripts', 'torchrun')())   File ""/usr/local/lib/python3.8/distpackages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper     return f(*args, **kwargs)   File ""/usr/local/lib/python3.8/distpackages/torch/distributed/run.py"", line 794, in main     run(args)   File ""/usr/local/lib/python3.8/distpackages/torch/distributed/run.py"", line 785, in run     elastic_launch(   File ""/usr/local/lib/python3.8/distpackages/torch/distributed/launcher/api.py"", line 134, in __call__     return launch_agent(self._config, self._entrypoint, list(args))   File ""/usr/local/lib/python3.8/distpackages/torch/distributed/launcher/api.py"", line 250, in launch_agent     raise ChildFailedError( torch.distributed.elastic.multiprocessing.errors.ChildFailedError: ==================================================== /workspace/MegatronLM/pretrain_gpt.py FAILED  Failures: [1]:   time      : 20230924_17:31:35   host      : 09677202c889   rank      : 1 (local_rank: 1)   exitcode  : 7 (pid: 5276)   error_file:    traceback : Signal 7 (SIGBUS) received by PID 5276 [2]:   time      : 20230924_17:31:35   host      : 09677202c889   rank      : 2 (local_rank: 2)   exitcode  : 7 (pid: 5277)   error_file:    traceback : Signal 7 (SIGBUS) received by PID 5277 [3]:   time      : 20230924_17:31:35   host      : 09677202c889   rank      : 3 (local_rank: 3)   exitcode  : 7 (pid: 5278)   error_file:    traceback : Signal 7 (SIGBUS) received by PID 5278 [4]:   time      : 20230924_17:31:35   host      : 09677202c889   rank      : 4 (local_rank: 4)   exitcode  : 7 (pid: 5279)   error_file:    traceback : Signal 7 (SIGBUS) received by PID 5279 [5]:   time      : 20230924_17:31:35   host      : 09677202c889   rank      : 5 (local_rank: 5)   exitcode  : 7 (pid: 5280)   error_file:    traceback : Signal 7 (SIGBUS) received by PID 5280 [6]:   time      : 20230924_17:31:35   host      : 09677202c889   rank      : 6 (local_rank: 6)   exitcode  : 7 (pid: 5281)   error_file:    traceback : Signal 7 (SIGBUS) received by PID 5281 [7]:   time      : 20230924_17:31:35   host      : 09677202c889   rank      : 7 (local_rank: 7)   exitcode  : 7 (pid: 5283)   error_file:    traceback : Signal 7 (SIGBUS) received by PID 5283  Root Cause (first observed failure): [0]:   time      : 20230924_17:31:35   host      : 09677202c889   rank      : 0 (local_rank: 0)   exitcode  : 7 (pid: 5275)   error_file:    traceback : Signal 7 (SIGBUS) received by PID 5275 ==================================================== root:/workspace","Can you try running this (one CPU process per GPU, instead of a single CPU process for all 8 GPUs on the node)? ``` mpirun np 8 ./all_reduce_perf_mpi b 8 e 128M f 2 g 1 ```",Can you also add `â€“shmsize=1g â€“ulimit memlock=1` to your `docker run` command?,Running the container like below solves the issue. Training works! `docker run gpus all ipc=host ulimit memlock=1 ulimit stack=67108864 it v /home/ubuntu/data:/data megatrontraining:latest /bin/bash`,Thank you for your help,Great to hear! Going to close this.,"Hi, I found the same output information with me. Do you know the possible reason ? !image","> Hi, I found the same output information with me. Do you know the possible reason ? !image I think it may cause some errors."
jomayeri,Moe Loss for Transformer Engine,Add an dummy moe loss value so activation checkpointing with Transformer Engine will run.,2023-09-22T18:06:41Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/515
Fernandogrove,[BUG],"**Describe the bug** A clear and concise description of what the bug is. **To Reproduce** Steps to reproduce the behavior. The easier it is to reproduce the faster it will get maintainer attention. **Expected behavior** A clear and concise description of what you expected to happen. **Stack trace/logs** If applicable, add the stack trace or logs from the time of the error. **Environment (please complete the following information):**   MegatronLM commit ID   PyTorch version   CUDA version   NCCL version **Proposed fix** If you have a proposal for how to fix the issue state it here or link to a PR. **Additional context** Add any other context about the problem here. ```[tasklist]  Tasks  [x] Pix 61995957627 ```",2023-09-22T11:07:59Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/514
DavideHe,[QUESTION]  both are right?,"https://github.com/NVIDIA/MegatronLM/blob/main/megatron/model/transformer.pyL1441 https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/transformer/transformer_layer.pyL94 two  way  calculate offset ,but they are different .    both are right?",2023-09-22T08:37:39Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/513,"Yup, both should be computing the same set of offsets: ``` With 8 layers, 2 stages, and 4 model chunks, we want an assignment of layers to stages like (each list is a model chunk): Stage 0: [0]  [2]  [4]  [6] Stage 1: [1]  [3]  [5]  [7] With 8 layers, 2 stages, and 2 virtual stages, we want an assignment of layers to stages like (each list is a model chunk): Stage 0: [0, 1]  [4, 5] Stage 1: [2, 3]  [6, 7]```","Going to close this, feel free to reopen if you still have questions."
jindajia,"[BUG]Tensorboard doesn't have information related to timers with  ""--log-timers-to-tensorboard"" argument","**Describe the bug** I want to see more detailed times related to forward process and collective communication, so I add logtimertotensorboard argument in my script. However, there is no information related to timers_to_log in my tensorboard.  The whole script is below.  ``` !/bin/bash Runs the ""345M"" parameter model export CUDA_DEVICE_MAX_CONNECTIONS=1 GPUS_PER_NODE=2 Change for multinode config MASTER_ADDR=localhost MASTER_PORT=6000 NNODES=1 NODE_RANK=0 WORLD_SIZE=$(($GPUS_PER_NODE*$NNODES)) CHECKPOINT_PATH=.... VOCAB_FILE=.... MERGE_FILE=.... DATA_PATH=.... TENSORBOARD_DIR=.... DISTRIBUTED_ARGS=""     nproc_per_node $GPUS_PER_NODE \     nnodes $NNODES \     node_rank $NODE_RANK \     master_addr $MASTER_ADDR \     master_port $MASTER_PORT "" GPT_ARGS=""     tensormodelparallelsize 1 \     pipelinemodelparallelsize 1 \     sequenceparallel \     numlayers 24 \     hiddensize 1024 \     numattentionheads 16 \     seqlength 1024 \     maxpositionembeddings 1024 \     microbatchsize 4 \     globalbatchsize 16 \     lr 0.00015 \     trainiters 300 \     lrdecayiters 320000 \     lrdecaystyle cosine \     minlr 1.0e5 \     weightdecay 1e2 \     lrwarmupfraction .01 \     clipgrad 1.0 \     fp16 \     usedistributedoptimizer "" DATA_ARGS=""     datapath $DATA_PATH \     vocabfile $VOCAB_FILE \     mergefile $MERGE_FILE \     split 949,50,1 "" OUTPUT_ARGS=""     loginterval 10 \     saveinterval 10000 \     evalinterval 1000 \     evaliters 10 \     logvalidationppltotensorboard \     logtimerstotensorboard \     tensorboarddir ${TENSORBOARD_DIR} \     tensorboardloginterval 5 "" torchrun $DISTRIBUTED_ARGS ../pretrain_gpt.py \     $GPT_ARGS \     $DATA_ARGS \     $OUTPUT_ARGS \     distributedbackend nccl \      save $CHECKPOINT_PATH \      load $CHECKPOINT_PATH ```  and my tensorboard chart screenshot is belowed  **To Reproduce** Steps to reproduce the behavior. The easier it is to reproduce the faster it will get maintainer attention. **Expected behavior** A clear and concise description of what you expected to happen. **Stack trace/logs** If applicable, add the stack trace or logs from the time of the error. **Environment (please complete the following information):**   MegatronLM commit ID   52f13005148afa47a6f37b082083fa2c6675ae3e   PyTorch version   CUDA version   NCCL version **Proposed fix** If you have a proposal for how to fix the issue state it here or link to a PR. **Additional context** Add any other context about the problem here.",2023-09-21T13:18:36Z,,closed,0,9,https://github.com/NVIDIA/Megatron-LM/issues/512,Hmm. Is there any data under `iterationtime`?,"!Screenshot 20230921 at 2 20 15 PM https://tensorboard.dev/experiment/JRNE6NxURkyY4WoQO11JiA Yes, iterationtime has data, but it doesn't have detailed time data.",This is what I see from a recent Tensorboard run I did with `logtimerstotensorboard`: ,"OK, Thank you, I will try another time.","Actually, I tried a lot of times with your latest code, but still have the same problem. There are no timer logs when using ```logtimerstotensorboard``` Below are my tensorboard result: https://tensorboard.dev/experiment/igldecmKSnKLjps25C87FQ/ Below are my arguments: using world size: 4, dataparallelsize: 2, tensormodelparallel size: 2, pipelinemodelparallel size: 1  using torch.float16 for parameters ...  arguments    accumulate_allreduce_grads_in_fp32 .............. False   adam_beta1 ...................................... 0.9   adam_beta2 ...................................... 0.999   adam_eps ........................................ 1e08   add_bias_linear ................................. True   add_position_embedding .......................... True   adlr_autoresume ................................. False   adlr_autoresume_interval ........................ 1000   apply_layernorm_1p .............................. False   apply_query_key_layer_scaling ................... True   apply_residual_connection_post_layernorm ........ False   async_tensor_model_parallel_allreduce ........... False   attention_dropout ............................... 0.1   attention_softmax_in_fp32 ....................... False   barrier_with_L1_time ............................ True   bert_binary_head ................................ True   bert_embedder_type .............................. megatron   bert_load ....................................... None   bf16 ............................................ False   bias_dropout_fusion ............................. True   bias_gelu_fusion ................................ True   biencoder_projection_dim ........................ 0   biencoder_shared_query_context_model ............ False   block_data_path ................................. None   check_for_nan_in_loss_and_grad .................. True   classes_fraction ................................ 1.0   clip_grad ....................................... 1.0   consumed_train_samples .......................... 0   consumed_valid_samples .......................... 0   data_cache_path ................................. None   data_parallel_random_init ....................... False   data_parallel_size .............................. 2   data_path ....................................... ['/ocean/projects/asc200010p/jjia1/Developer/LLM/gpt2/datapath/gpt2_text_document']   data_per_class_fraction ......................... 1.0   data_sharding ................................... True   dataloader_type ................................. single   decoder_num_layers .............................. None   decoder_seq_length .............................. None   dino_bottleneck_size ............................ 256   dino_freeze_last_layer .......................... 1   dino_head_hidden_size ........................... 2048   dino_local_crops_number ......................... 10   dino_local_img_size ............................. 96   dino_norm_last_layer ............................ False   dino_teacher_temp ............................... 0.07   dino_warmup_teacher_temp ........................ 0.04   dino_warmup_teacher_temp_epochs ................. 30   distribute_saved_activations .................... False   distributed_backend ............................. nccl   distributed_timeout_minutes ..................... 10   embedding_path .................................. None   embedding_weights_in_fp32 ....................... False   empty_unused_memory_level ....................... 0   encoder_num_layers .............................. 24   encoder_seq_length .............................. 1024   end_weight_decay ................................ 0.01   eod_mask_loss ................................... False   eval_interval ................................... 1000   eval_iters ...................................... 10   evidence_data_path .............................. None   exit_duration_in_mins ........................... None   exit_interval ................................... None   exit_on_missing_checkpoint ...................... False   exit_signal_handler ............................. False   ffn_hidden_size ................................. 4096   finetune ........................................ False   fp16 ............................................ True   fp16_lm_cross_entropy ........................... False   fp32_residual_connection ........................ False   fp8 ............................................. None   fp8_amax_compute_algo ........................... most_recent   fp8_amax_history_len ............................ 1   fp8_interval .................................... 1   fp8_margin ...................................... 0   fp8_wgrad ....................................... True   global_batch_size ............................... 16   gradient_accumulation_fusion .................... True   group_query_attention ........................... False   head_lr_mult .................................... 1.0   hidden_dropout .................................. 0.1   hidden_size ..................................... 1024   hysteresis ...................................... 2   ict_head_size ................................... None   ict_load ........................................ None   img_h ........................................... 224   img_w ........................................... 224   indexer_batch_size .............................. 128   indexer_log_interval ............................ 1000   inference_batch_times_seqlen_threshold .......... 512   init_method_std ................................. 0.02   init_method_xavier_uniform ...................... False   initial_loss_scale .............................. 4294967296   iter_per_epoch .................................. 1250   kv_channels ..................................... 64   lazy_mpu_init ................................... None   load ............................................ /ocean/projects/asc200010p/jjia1/Developer/LLM/gpt2/checkpoint/demo   local_rank ...................................... None   log_batch_size_to_tensorboard ................... False   log_interval .................................... 10   log_learning_rate_to_tensorboard ................ True   log_loss_scale_to_tensorboard ................... True   log_memory_to_tensorboard ....................... False   log_num_zeros_in_grad ........................... False   log_params_norm ................................. False   log_timers_to_tensorboard ....................... True   log_validation_ppl_to_tensorboard ............... False   log_world_size_to_tensorboard ................... False   loss_scale ...................................... None   loss_scale_window ............................... 1000   lr .............................................. 0.00015   lr_decay_iters .................................. 320000   lr_decay_samples ................................ None   lr_decay_style .................................. cosine   lr_warmup_fraction .............................. 0.01   lr_warmup_init .................................. 0.0   lr_warmup_iters ................................. 0   lr_warmup_samples ............................... 0   make_vocab_size_divisible_by .................... 128   mask_factor ..................................... 1.0   mask_prob ....................................... 0.15   mask_type ....................................... random   masked_softmax_fusion ........................... True   max_position_embeddings ......................... 1024   max_tokens_to_oom ............................... 12000   merge_file ...................................... /ocean/projects/asc200010p/jjia1/Developer/LLM/gpt2/merge_file/gpt2merges.txt   micro_batch_size ................................ 4   min_loss_scale .................................. 1.0   min_lr .......................................... 1e05   mmap_warmup ..................................... False   no_load_optim ................................... None   no_load_rng ..................................... None   no_persist_layer_norm ........................... False   no_save_optim ................................... None   no_save_rng ..................................... None   norm_epsilon .................................... 1e05   normalization ................................... LayerNorm   num_attention_heads ............................. 16   num_channels .................................... 3   num_classes ..................................... 1000   num_experts ..................................... None   num_layers ...................................... 24   num_layers_per_virtual_pipeline_stage ........... None   num_query_groups ................................ 1   num_workers ..................................... 2   onnx_safe ....................................... None   openai_gelu ..................................... False   optimizer ....................................... adam   output_bert_embeddings .......................... False   overlap_grad_reduce ............................. False   overlap_p2p_comm ................................ False   override_opt_param_scheduler .................... False   params_dtype .................................... torch.float16   patch_dim ....................................... 16   perform_initialization .......................... True   pipeline_model_parallel_size .................... 1   pipeline_model_parallel_split_rank .............. None   position_embedding_type ......................... learned_absolute   profile ......................................... False   profile_ranks ................................... [0]   profile_step_end ................................ 12   profile_step_start .............................. 10   query_in_block_prob ............................. 0.1   rampup_batch_size ............................... None   rank ............................................ 0   recompute_granularity ........................... None   recompute_method ................................ None   recompute_num_layers ............................ None   reset_attention_mask ............................ False   reset_position_ids .............................. False   retriever_report_topk_accuracies ................ []   retriever_score_scaling ......................... False   retriever_seq_length ............................ 256   retro_add_retriever ............................. False   retro_cyclic_train_iters ........................ None   retro_encoder_attention_dropout ................. 0.1   retro_encoder_hidden_dropout .................... 0.1   retro_encoder_layers ............................ 2   retro_num_neighbors ............................. 2   retro_num_retrieved_chunks ...................... 2   retro_return_doc_ids ............................ False   retro_workdir ................................... None   rotary_percent .................................. 1.0   rotary_seq_len_interpolation_factor ............. None   sample_rate ..................................... 1.0   save ............................................ /ocean/projects/asc200010p/jjia1/Developer/LLM/gpt2/checkpoint/demo   save_interval ................................... 10000   scatter_gather_tensors_in_pipeline .............. True   seed ............................................ 1234   seq_length ...................................... 1024   sequence_parallel ............................... True   sgd_momentum .................................... 0.9   short_seq_prob .................................. 0.1   skip_train ...................................... False   split ........................................... 949,50,1   squared_relu .................................... False   standalone_embedding_stage ...................... False   start_weight_decay .............................. 0.01   swiglu .......................................... False   swin_backbone_type .............................. tiny   tensor_model_parallel_size ...................... 2   tensorboard_dir ................................. /ocean/projects/asc200010p/jjia1/Developer/tensorboard   tensorboard_log_interval ........................ 5   tensorboard_queue_size .......................... 1000   test_data_path .................................. None   timing_log_level ................................ 0   timing_log_option ............................... minmax   titles_data_path ................................ None   tokenizer_model ................................. None   tokenizer_type .................................. GPT2BPETokenizer   train_data_path ................................. None   train_iters ..................................... 500000   train_samples ................................... None   transformer_impl ................................ local   transformer_pipeline_model_parallel_size ........ 1   untie_embeddings_and_output_weights ............. False   use_checkpoint_args ............................. False   use_checkpoint_opt_param_scheduler .............. False   use_cpu_initialization .......................... None   use_distributed_optimizer ....................... False   use_flash_attn .................................. False   use_one_sent_docs ............................... False   use_ring_exchange_p2p ........................... False   use_rotary_position_embeddings .................. False   valid_data_path ................................. None   variable_seq_lengths ............................ False   virtual_pipeline_model_parallel_size ............ None   vision_backbone_type ............................ vit   vision_pretraining .............................. False   vision_pretraining_type ......................... classify   vocab_extra_ids ................................. 0   vocab_file ...................................... /ocean/projects/asc200010p/jjia1/Developer/LLM/gpt2/vocab_file/gpt2vocab.json   vocab_size ...................................... None   weight_decay .................................... 0.01   weight_decay_incr_style ......................... constant   world_size ...................................... 4  end of arguments ","Hmm, I can't reproduce this on my end. Did you try this with a fresh tensorboard directory? Maybe also upgrade your tensorboard version?","I finally found the reason why it has no information related to timers, that is because I didn't specify      ```timingloglevel 2```",Oh right! Nice catch.,"> Oh right! Nice catch. Thank you, Dr. Narayanan, I have read your paper, it's really awesome!"
sincerity-being,[BUG],"using world size: 1, dataparallelsize: 1, tensormodelparallel size: 1, pipelinemodelparallel size: 1  using torch.float16 for parameters ...  arguments    accumulate_allreduce_grads_in_fp32 .............. False   adam_beta1 ...................................... 0.9   adam_beta2 ...................................... 0.999   adam_eps ........................................ 1e08   add_bias_linear ................................. True   add_position_embedding .......................... True   adlr_autoresume ................................. False   adlr_autoresume_interval ........................ 1000   apply_layernorm_1p .............................. False   apply_query_key_layer_scaling ................... True   apply_residual_connection_post_layernorm ........ False   async_tensor_model_parallel_allreduce ........... True   attention_dropout ............................... 0.1   attention_softmax_in_fp32 ....................... False   barrier_with_L1_time ............................ True   bert_binary_head ................................ True   bert_embedder_type .............................. megatron   bert_load ....................................... None   bf16 ............................................ False   bias_dropout_fusion ............................. True   bias_gelu_fusion ................................ True   biencoder_projection_dim ........................ 0   biencoder_shared_query_context_model ............ False   block_data_path ................................. None   check_for_nan_in_loss_and_grad .................. True   classes_fraction ................................ 1.0   clip_grad ....................................... 1.0   consumed_train_samples .......................... 0   consumed_valid_samples .......................... 0   data_cache_path ................................. None   data_parallel_random_init ....................... False   data_parallel_size .............................. 1   data_path ....................................... ['./data/bert_test.json']   data_per_class_fraction ......................... 1.0   data_sharding ................................... True   dataloader_type ................................. single   decoder_num_layers .............................. None   decoder_seq_length .............................. None   dino_bottleneck_size ............................ 256   dino_freeze_last_layer .......................... 1   dino_head_hidden_size ........................... 2048   dino_local_crops_number ......................... 10   dino_local_img_size ............................. 96   dino_norm_last_layer ............................ False   dino_teacher_temp ............................... 0.07   dino_warmup_teacher_temp ........................ 0.04   dino_warmup_teacher_temp_epochs ................. 30   distribute_saved_activations .................... False   distributed_backend ............................. nccl   distributed_timeout_minutes ..................... 10   embedding_path .................................. None   embedding_weights_in_fp32 ....................... False   empty_unused_memory_level ....................... 0   encoder_num_layers .............................. 24   encoder_seq_length .............................. 512   end_weight_decay ................................ 0.01   eod_mask_loss ................................... False   eval_interval ................................... 1000   eval_iters ...................................... 10   evidence_data_path .............................. None   exit_duration_in_mins ........................... None   exit_interval ................................... None   exit_on_missing_checkpoint ...................... False   exit_signal_handler ............................. False   ffn_hidden_size ................................. 4096   finetune ........................................ False   fp16 ............................................ True   fp16_lm_cross_entropy ........................... False   fp32_residual_connection ........................ False   fp8 ............................................. None   fp8_amax_compute_algo ........................... most_recent   fp8_amax_history_len ............................ 1   fp8_interval .................................... 1   fp8_margin ...................................... 0   fp8_wgrad ....................................... True   global_batch_size ............................... 8   gradient_accumulation_fusion .................... True   group_query_attention ........................... False   head_lr_mult .................................... 1.0   hidden_dropout .................................. 0.1   hidden_size ..................................... 1024   hysteresis ...................................... 2   ict_head_size ................................... None   ict_load ........................................ None   img_h ........................................... 224   img_w ........................................... 224   indexer_batch_size .............................. 128   indexer_log_interval ............................ 1000   inference_batch_times_seqlen_threshold .......... 512   init_method_std ................................. 0.02   init_method_xavier_uniform ...................... False   initial_loss_scale .............................. 4294967296   iter_per_epoch .................................. 1250   kv_channels ..................................... 64   lazy_mpu_init ................................... None   load ............................................ None   local_rank ...................................... None   log_batch_size_to_tensorboard ................... False   log_interval .................................... 100   log_learning_rate_to_tensorboard ................ True   log_loss_scale_to_tensorboard ................... True   log_memory_to_tensorboard ....................... False   log_num_zeros_in_grad ........................... False   log_params_norm ................................. False   log_timers_to_tensorboard ....................... False   log_validation_ppl_to_tensorboard ............... False   log_world_size_to_tensorboard ................... False   loss_scale ...................................... None   loss_scale_window ............................... 1000   lr .............................................. 0.0001   lr_decay_iters .................................. 990000   lr_decay_samples ................................ None   lr_decay_style .................................. linear   lr_warmup_fraction .............................. 0.01   lr_warmup_init .................................. 0.0   lr_warmup_iters ................................. 0   lr_warmup_samples ............................... 0   make_vocab_size_divisible_by .................... 128   mask_factor ..................................... 1.0   mask_prob ....................................... 0.15   mask_type ....................................... random   masked_softmax_fusion ........................... True   max_position_embeddings ......................... 512   max_tokens_to_oom ............................... 12000   merge_file ...................................... None   micro_batch_size ................................ 4   min_loss_scale .................................. 1.0   min_lr .......................................... 1e05   mmap_warmup ..................................... False   no_load_optim ................................... None   no_load_rng ..................................... None   no_persist_layer_norm ........................... False   no_save_optim ................................... None   no_save_rng ..................................... None   norm_epsilon .................................... 1e05   normalization ................................... LayerNorm   num_attention_heads ............................. 16   num_channels .................................... 3   num_classes ..................................... 1000   num_experts ..................................... None   num_layers ...................................... 24   num_layers_per_virtual_pipeline_stage ........... None   num_query_groups ................................ 1   num_workers ..................................... 2   onnx_safe ....................................... None   openai_gelu ..................................... False   optimizer ....................................... adam   output_bert_embeddings .......................... False   overlap_grad_reduce ............................. False   overlap_p2p_comm ................................ False   override_opt_param_scheduler .................... False   params_dtype .................................... torch.float16   patch_dim ....................................... 16   perform_initialization .......................... True   pipeline_model_parallel_size .................... 1   pipeline_model_parallel_split_rank .............. None   position_embedding_type ......................... learned_absolute   profile ......................................... False   profile_ranks ................................... [0]   profile_step_end ................................ 12   profile_step_start .............................. 10   query_in_block_prob ............................. 0.1   rampup_batch_size ............................... None   rank ............................................ 0   recompute_granularity ........................... None   recompute_method ................................ None   recompute_num_layers ............................ None   reset_attention_mask ............................ False   reset_position_ids .............................. False   retriever_report_topk_accuracies ................ []   retriever_score_scaling ......................... False   retriever_seq_length ............................ 256   retro_add_retriever ............................. False   retro_cyclic_train_iters ........................ None   retro_encoder_attention_dropout ................. 0.1   retro_encoder_hidden_dropout .................... 0.1   retro_encoder_layers ............................ 2   retro_num_neighbors ............................. 2   retro_num_retrieved_chunks ...................... 2   retro_return_doc_ids ............................ False   retro_workdir ................................... None   rotary_percent .................................. 1.0   rotary_seq_len_interpolation_factor ............. None   sample_rate ..................................... 1.0   save ............................................ ./checkpoint   save_interval ................................... 10000   scatter_gather_tensors_in_pipeline .............. True   seed ............................................ 1234   seq_length ...................................... 512   sequence_parallel ............................... False   sgd_momentum .................................... 0.9   short_seq_prob .................................. 0.1   skip_train ...................................... False   split ........................................... 949,50,1   squared_relu .................................... False   standalone_embedding_stage ...................... False   start_weight_decay .............................. 0.01   swiglu .......................................... False   swin_backbone_type .............................. tiny   tensor_model_parallel_size ...................... 1   tensorboard_dir ................................. None   tensorboard_log_interval ........................ 1   tensorboard_queue_size .......................... 1000   test_data_path .................................. None   timing_log_level ................................ 0   timing_log_option ............................... minmax   titles_data_path ................................ None   tokenizer_model ................................. None   tokenizer_type .................................. BertWordPieceLowerCase   train_data_path ................................. None   train_iters ..................................... 2000000   train_samples ................................... None   transformer_impl ................................ local   transformer_pipeline_model_parallel_size ........ 1   untie_embeddings_and_output_weights ............. False   use_checkpoint_args ............................. False   use_checkpoint_opt_param_scheduler .............. False   use_cpu_initialization .......................... None   use_distributed_optimizer ....................... False   use_flash_attn .................................. False   use_one_sent_docs ............................... False   use_ring_exchange_p2p ........................... False   use_rotary_position_embeddings .................. False   valid_data_path ................................. None   variable_seq_lengths ............................ False   virtual_pipeline_model_parallel_size ............ None   vision_backbone_type ............................ vit   vision_pretraining .............................. False   vision_pretraining_type ......................... classify   vocab_extra_ids ................................. 0   vocab_file ...................................... ./data/bertvocab.txt   vocab_size ...................................... None   weight_decay .................................... 0.01   weight_decay_incr_style ......................... constant   world_size ...................................... 1  end of arguments  setting number of microbatches to constant 2 > building BertWordPieceLowerCase tokenizer ...  > padded vocab (size: 30524) with 68 dummy tokens (new size: 30592) > initializing torch distributed ... > initialized tensor model parallel with size 1 > initialized pipeline model parallel with size 1 > setting random seeds to 1234 ... > compiling dataset index builder ... Traceback (most recent call last):   File ""/home/dawnlab/sinceritybeing/code/finetuning/MegatronLm/pretrain_bert.py"", line 151, in      pretrain(train_valid_test_datasets_provider, model_provider,   File ""/home/dawnlab/sinceritybeing/code/finetuning/MegatronLm/megatron/training.py"", line 89, in pretrain     initialize_megatron(extra_args_provider=extra_args_provider,   File ""/home/dawnlab/sinceritybeing/code/finetuning/MegatronLm/megatron/initialize.py"", line 86, in initialize_megatron     _compile_dependencies()   File ""/home/dawnlab/sinceritybeing/code/finetuning/MegatronLm/megatron/initialize.py"", line 105, in _compile_dependencies     compile_helper()   File ""/home/dawnlab/sinceritybeing/code/finetuning/MegatronLm/megatron/data/dataset_utils.py"", line 89, in compile_helper     ret = subprocess.run(['make', 'C', path])   File ""/home/dawnlab/anaconda3/envs/megatronLM/lib/python3.8/subprocess.py"", line 493, in run     with Popen(*popenargs, **kwargs) as process:   File ""/home/dawnlab/anaconda3/envs/megatronLM/lib/python3.8/subprocess.py"", line 858, in __init__     self._execute_child(args, executable, preexec_fn, close_fds,   File ""/home/dawnlab/anaconda3/envs/megatronLM/lib/python3.8/subprocess.py"", line 1720, in _execute_child     raise child_exception_type(errno_num, err_msg, err_filename) FileNotFoundError: [Errno 2] No such file or directory: 'make'",2023-09-21T13:16:20Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/511,I think this is because you don't have the `make` package installed on your system.,"Going to close this, feel free to reopen if this is not the issue."
MoFHeka,[ENHANCEMENT] Which code file would be the best to merge unit-scaling feature into for Float8 training?,"**Is your feature request related to a problem? Please describe.** Unitscaling is a convenient method to implement FP8 training. And according  to its theory, it's much faster than transformerengine. Relevant introduction: https://www.graphcore.ai/posts/makinglowprecisionnumberformatsfp16andfp8easytousewithournewunitscalinglibrary Git: https://github.com/graphcoreresearch/unitscaling **Describe the solution you'd like** It seems that add new implementation into megatron/model/transformer.py and megatron/core/tensor_parallel/layers.py would be easier? Or just building a new package like transformerengine? **Describe alternatives you've considered** W = scaled(W, beta=beta_W)? I have no idea, please give me some suggestion.  **Proposed implementation** Unitscaling need to analysis the whole model to find the best scaling factors for each layer. So also we have to add a addon warmup process. **Additional context** Something like 'LinearWithGradAccumulationAndAsyncCommunication' is not easy to rewrite, so...maybe we can rewrite the final computation graph?",2023-09-20T14:00:57Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/510,Marking as stale. No activity in 60 days.
SefaZeng,[QUESTION] Same global_batch_size but a smaller micro_batch_size is faster?,"**Your question** I try to set different micro_batch_size with the same global_batch_size to find a max micro_batch_size fits the GPU memory. But I find when the micro_batch_size goes smaller the training time for one iteration is faster. Is this correct? Or did I get something wrong here? As for my experience, a larger batch size in each GPU will be faster as it will decrease the time for gradient accumulation.  Specifical data for the batch_size and time: ``` global_batch_size=256 micro_batch_size=16 steps: 2 iter time (s): 2.261  micro_batch_size=8 steps: 2 iter time (s): 1.412  micro_batch_size=4 steps: 2 iter time (s): 0.989 ```",2023-09-20T11:51:44Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/509,This does not seem right. Your intuition is right: larger microbatch sizes should be slightly faster. Increasing batch size keeping the DP size the same should also increase throughput (samples / second) since dataparallel communication is less frequent.,"> This does not seem right. Your intuition is right: larger microbatch sizes should be slightly faster. Increasing batch size keeping the DP size the same should also increase throughput (samples / second) since dataparallel communication is less frequent. I checked this issue in both the 7b model and the 600M model, it seems true for both settings.  The scripts is like: ``` TP=2 PP=2 ZERO_STAGE=0 GPUS_PER_NODE=8  7 B    HIDDEN_SIZE=4096  e.g. llama13b: 5120 FFN_HIDDEN_SIZE=11008  e.g. llama13b: 13824 NUM_LAYERS=32  e.g. llama13b: 40 NUM_HEADS=32  e.g. llama13b: 40 SEQ_LENGTH=4096 NUM_KV_HEADS=32  llama2 70B uses GQA MICRO_BATCH_SIZE=1 GLOBAL_BATCH_SIZE=1024  e.g. llama: 4M tokens TRAIN_STEPS=250000  e.g. llama: 1T tokens / 4M tokens_per_batch = 250000 steps LR=3e4  MIN_LR=3e5 LR_WARMUP_STEPS=2000 WEIGHT_DECAY=0.1 GRAD_CLIP=1 ds_args="""" ds_args="" deepspeed ${ds_args}"" ds_args="" deepspeed_config=$DS_CONFIG ${ds_args}"" ds_args="" zerostage=$ZERO_STAGE ${ds_args}"" ds_args="" deepspeedactivationcheckpointing ${ds_args}"" DISTRIBUTED_ARGS=""nproc_per_node $GPUS_PER_NODE nnodes $NNODES node_rank $NODE_RANK master_addr $MASTER_ADDR master_port $MASTER_PORT"" $DISTRIBUTED_ARGS torchrun $DISTRIBUTED_ARGS \        ${CODE_PATH}/pretrain_gpt.py \        tensormodelparallelsize $TP \        pipelinemodelparallelsize $PP \        numlayers $NUM_LAYERS \        hiddensize $HIDDEN_SIZE \        ffnhiddensize $FFN_HIDDEN_SIZE \        numattentionheads $NUM_HEADS \        microbatchsize $MICRO_BATCH_SIZE \        globalbatchsize $GLOBAL_BATCH_SIZE \        seqlength $SEQ_LENGTH \        maxpositionembeddings $SEQ_LENGTH \        trainiters $TRAIN_STEPS \        save $CHECKPOINT_PATH \        load $CHECKPOINT_PATH \        traindatapath $DATASET \        validdatapath $DATASET_VALID \        testdatapath $DATASET_VALID \        dataimpl mmap \        tokenizertype MT5Tokenizer \        tokenizermodel $TOKENIZER_PATH \        distributedbackend nccl \        lr $LR \        lrdecaystyle cosine \        minlr $MIN_LR \        weightdecay $WEIGHT_DECAY \        clipgrad $GRAD_CLIP \        lrwarmupiters $LR_WARMUP_STEPS \        optimizer adam \        adambeta1 0.9 \        adambeta2 0.95 \                                                                                                                                    loginterval 1 \        saveinterval 10000 \        evalinterval 1000 \        evaliters 1000 \        fp16 \        noquerykeylayerscaling \        attentiondropout 0 \        hiddendropout 0 \        disablebiaslinear \        $ds_args ```","I found the MICRO_BATCH_SIZE and GLOBAL_BATCH_SIZE in the deepspeed_config.json is not the same, which is the reason for this issue..."
KwangryeolPark,[QUESTION] Is it possible to use other optimizer or custom optimizer?,I'm researching for improved optimzier. I have my own PyTorch optimizer code and I want to use it in MegatronLM project. How can I adapt my optimizer?,2023-09-16T08:30:28Z,stale,closed,0,14,https://github.com/NVIDIA/Megatron-LM/issues/508,You could probably modify the code here to use your own optimizer: https://github.com/NVIDIA/MegatronLM/blob/main/megatron/optimizer/__init__.pyL74.,"> You could probably modify the code here to use your own optimizer: https://github.com/NVIDIA/MegatronLM/blob/main/megatron/optimizer/__init__.pyL74. Thank you for comment. I also saw the code and edit the original Adam and SGD code to use Adam and SGD provided by PyTorch. However, while I training a model, the performance is so difficult when use optimizers from PyTorch. Are there any configurations for the same performance?",What do you mean by performance? Convergence rate or throughput?,"> What do you mean by performance? Convergence rate or throughput? I mean the loss. When I use PyTorch Adam, the loss graph is so flucuated with same configuration.",I don't follow. Using the PyTorch optimizers instead of the apex optimizers gives you unstable losses? Do you have a graph that you can share?,"> I don't follow. Using the PyTorch optimizers instead of the apex optimizers gives you unstable losses? Do you have a graph that you can share? Sure. Here is my research result about T5, GPT3 and BERT model on wikitext dataset mentioned here This is my modified optimizer/__init__.py. I just change Adam to torch.optim.Adam and other configurations are same. ```python if args.optimizer == 'adam':         optimizer = torch.optim.Adam(param_groups,                          lr=args.lr,                          weight_decay=args.weight_decay,                          betas=(args.adam_beta1, args.adam_beta2),                          eps=args.adam_eps)     elif args.optimizer == 'sgd':         optimizer = torch.optim.SGD(param_groups,                         lr=args.lr,                         weight_decay=args.weight_decay,                         momentum=args.sgd_momentum) ``` And below is the loss graph. As you can see, it is extremely unstable. !W B Chart 2023  9  27  ì˜¤í›„ 12_20_45 !W B Chart 2023  9  27  ì˜¤í›„ 12_19_55 (1) !W B Chart 2023  9  27  ì˜¤í›„ 12_19_55",What is step on the xaxis? The number of batches seen so far or something else?,> What is step on the xaxis? The number of batches seen so far or something else? The xaxis (Step) is the number of iterations and the yaxis is loss.,Something looks extremely wrong here...the loss should get to near 3.0 with that many steps. Do you have a graph with the apex optimizer?,"> Something looks extremely wrong here...the loss should get to near 3.0 with that many steps. Do you have a graph with the apex optimizer? I deleted the result with apex (because it was intended research). However, the loss graph was stable. ",I am wondering if there's an issue with your data or something else; did the loss decrease to something around 3.0?,> I am wondering if there's an issue with your data or something else; did the loss decrease to something around 3.0? I did not run many iterations but here is my GPT2 lm loss. It is very stable and the second figure is comparison. The red graph is apex optimizer(quantized) and purple one is original torch optim !image !image,There is something wrong in lossscale graph and gradnorm graph. The lossscale is small 1048576(quantized) vs 16384(original torch optim). And gradnorm much bigger than quantized optimizer(apex).,Marking as stale. No activity in 60 days.
CurryRice233,fix data helpers overflow bug,"It can produce this error when the training iteration is a large number and the dataset is a short sentence dataset. ``` Traceback (most recent call last):   File ""pretrain_gpt.py"", line 121, in      args_defaults={'tokenizer_type': 'GPT2BPETokenizer'}   File ""/home/mauser/modelarts/userjobdir/GPT3kernel_ID2728_for_PyTorch_zgcl/megatron/training.py"", line 150, in pretrain     process_non_loss_data_func)   File ""/home/mauser/modelarts/userjobdir/GPT3kernel_ID2728_for_PyTorch_zgcl/megatron/training.py"", line 689, in train     opt_param_scheduler)   File ""/home/mauser/modelarts/userjobdir/GPT3kernel_ID2728_for_PyTorch_zgcl/megatron/training.py"", line 417, in train_step     optimizer, fwd_bwd_timers, forward_only=False)   File ""/home/mauser/modelarts/userjobdir/GPT3kernel_ID2728_for_PyTorch_zgcl/megatron/schedules.py"", line 654, in forward_backward_pipelining_without_interleaving     timers, collect_non_loss_data)   File ""/home/mauser/modelarts/userjobdir/GPT3kernel_ID2728_for_PyTorch_zgcl/megatron/schedules.py"", line 118, in forward_step     output_tensor, loss_func = forward_step_func(data_iterator, model)   File ""pretrain_gpt.py"", line 84, in forward_step     data_iterator)   File ""pretrain_gpt.py"", line 45, in get_batch     data = next(data_iterator)   File ""/home/mauser/anaconda/lib/python3.7/sitepackages/torch/utils/data/dataloader.py"", line 530, in __next__     data = self._next_data()   File ""/home/mauser/anaconda/lib/python3.7/sitepackages/torch/utils/data/dataloader.py"", line 570, in _next_data     data = self._dataset_fetcher.fetch(index)   may raise StopIteration   File ""/home/mauser/anaconda/lib/python3.7/sitepackages/torch/utils/data/_utils/fetch.py"", line 52, in fetch     return self.collate_fn(data)   File ""/home/mauser/anaconda/lib/python3.7/sitepackages/torch/utils/data/_utils/collate.py"", line 157, in default_collate     return elem_type({key: default_collate([d[key] for d in batch]) for key in elem})   File ""/home/mauser/anaconda/lib/python3.7/sitepackages/torch/utils/data/_utils/collate.py"", line 157, in      return elem_type({key: default_collate([d[key] for d in batch]) for key in elem})   File ""/home/mauser/anaconda/lib/python3.7/sitepackages/torch/utils/data/_utils/collate.py"", line 146, in default_collate     return default_collate([torch.as_tensor(b) for b in batch])   File ""/home/mauser/anaconda/lib/python3.7/sitepackages/torch/utils/data/_utils/collate.py"", line 138, in default_collate     return torch.stack(batch, 0, out=out) RuntimeError: stack expects each tensor to be equal size, but got [8193] at entry 0 and [8246] at entry 1 ``` The reason is that `doc_idx_index`(int64) is assigned to `sample_idx`(int32), when the value of `doc_idx_index` is greater than the range of int32, overflow may occur.",2023-09-16T06:44:33Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/507,"  Sorry for annoying, can you guys review this PR ?",Marking as stale. No activity in 60 days.
lawchingman,[ENHANCEMENT],**Is your feature request related to a problem? Please describe.** A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] **Describe the solution you'd like** A clear and concise description of what you want to happen. **Describe alternatives you've considered** A clear and concise description of any alternative solutions or features you've considered. **Proposed implementation** If you have a proposed implementation for the feature state it here or link to a PR. **Additional context** Add any other context or screenshots about the feature request here.,2023-09-16T01:31:58Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/506
lawchingman,[ENHANCEMENT],**Is your feature request related to a problem? Please describe.** A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] **Describe the solution you'd like** A clear and concise description of what you want to happen. **Describe alternatives you've considered** A clear and concise description of any alternative solutions or features you've considered. **Proposed implementation** If you have a proposed implementation for the feature state it here or link to a PR. **Additional context** Add any other context or screenshots about the feature request here.,2023-09-15T17:41:13Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/505
RuiWang1998,Avoid transpose large tensors in embedding,Move the [b s] > [s b] transpose earlier to only transpose ids instead of dense tensors.,2023-09-15T16:11:43Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/504,Marking as stale. No activity in 60 days.
KwangryeolPark,Copy,,2023-09-15T10:42:06Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/503
huhu0823,[QUESTION]What is the reason why bfloat16 requires gradient accumulation and allreductions need to be completed in fp32,"Hi,I am currently studying the Megatron framework. I noticed that bfloat16 in megatron requires gradient accumulation, and allreductions need to be completed in fp32. Then the gradient communicates in fp32 format. However, fp16 requires gradient accumulation, and allreductions can be completed in fp16.Then the gradient communicates in fp16 format. I want to know what are the special reasons for these two different calculation methods Line 159160 of the megatron/arguments. py file !lQLPJxda3cZZmQjNApzNBeiwwTfxEK6Mif0E95eXr1raAA_1512_668",2023-09-15T08:26:26Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/502,Marking as stale. No activity in 60 days.,"Same question, anyone knows the reason?",Marking as stale. No activity in 60 days.
forrestjgq,[BUG]preprocess_data.py Partition.process_json_file() use undefined var `key` in last line,"**Describe the bug** ```     def process_json_file(self, file_name):        ...         for key in self.args.json_keys:                  ....         for i, (doc, sentence_lens, bytes_processed) in enumerate(encoded_docs, start=1):             for key in doc.keys():                 builders[key].add_doc(doc[key], sentence_lens[key])          key in this line is not defined         builders[key].finalize(output_idx_files[key]) ``` **To Reproduce** code logical issue **Expected behavior** has clear definition of key **Stack trace/logs** no need **Environment (please complete the following information):** no need **Proposed fix** should this line be: ``` for key in self.args.json_keys:     builders[key].finalize(output_idx_files[key]) ``` ? **Additional context** nothing",2023-09-15T01:42:45Z,stale,closed,1,3,https://github.com/NVIDIA/Megatron-LM/issues/501,Marking as stale. No activity in 60 days., I have same issue.Did you find the solution?,Marking as stale. No activity in 60 days.
thuhujin,[QUESTION] Status & Plan for Distributed Checkpointing in Megatron Repo,"Hi there, I noticed that distributed checkpointing was recently added in the repo under megatron/core/dist_checkpointing directory. From the current implementation available, I find it a good match for my use case. However, it seems that the code still need some polishing and it's not yet enabled in production code path. I have a few questions regarding the current status and future plans: 1. Could someone please provide an overall update on the status of the feature and when it might be ready?  2. Will dist checkpointing be implemented for both transformer enginebased models and nonTE based models? Currently it's only available for TE based models. 3. I see tensorstore introduced in the changes. Will that be the primary or recommended approach in Megatron repo to deal with checkpoint storage? From some of the papers by Google, it seems to be the case for Google.   4. There is a similar functionality offered by pytorch. Pytorch also introduced concepts like a load plan, sharded tensor etc. How will the changes in Megatron repo interact with pytorch's?  5. Finally, is there any roadmap in terms of checkpointing for Megatron? Having clearer plans would make it easier for contributors to align their efforts and make meaningful contributions. Thanks!",2023-09-14T09:26:08Z,stale,closed,3,5,https://github.com/NVIDIA/Megatron-LM/issues/500,Marking as stale. No activity in 60 days.,Any update on this? Thanks!,Marking as stale. No activity in 60 days.,"+1 for this issue, this seems to be an interesting and useful feature. Besides, sth like async checkpointing is also sth useful, will the megatron team consider implementing this?",Marking as stale. No activity in 60 days.
RichardScottOZ,so as to?,or even to?,2023-09-14T03:42:29Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/499,Marking as stale. No activity in 60 days.
aeeeeeep,[BUG] mpu.destroy_tensor_model_parallel() is not defined,**mpu.destroy_tensor_model_parallel() is not defined** **Additional context** in `megatron/mpu/tests/test_data.py:59` and `megatron/mpu/tests/test_cross_entropy.py:79` ```python ...      Reset groups     mpu.destroy_tensor_model_parallel() ... ```,2023-09-14T03:28:38Z,stale,closed,2,1,https://github.com/NVIDIA/Megatron-LM/issues/498,Marking as stale. No activity in 60 days.
erhoo82,Add interface to specify NCCL configs of each communicator,"The NCCL communicator configs are set as default values. So, when not specifying the config values using env vars, communicators become initialized with default NCCL communicator settings.",2023-09-13T17:50:33Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/497
cinqs,[QUESTION] Can we  change the name to optimus'?,"**Your question** I like this project and currently working on it. But I dont like megatron the character, should we change the name to optimus' ?",2023-09-13T07:31:06Z,,closed,2,1,https://github.com/NVIDIA/Megatron-LM/issues/496,There are no plans to change the name of the project
17889982022,[BUG] â€˜Distributed Optimizerâ€™ reports an error when the number of training nodes exceeds 10,"**Describe the bug**     I am planning to train a GPTlike LLM with 13B parameters. And the ""distributed_optimizer"" functionality in MegatronLM can operate successfully with node counts such as 2, 6, and 8. However, when the node count exceeds 10, it encounters an error, and the error message is as follows: !1d9b2d45e261f2e147e467eb32f509c7 **Environment**   MegatronLM commit ID : dc21350806   PyTorch version:  1.14.0a0+44dac51   CUDA version: 12.0   NCCL version:  2.16.5+cuda12.0 **To Reproduce** 1. **Training environment**      A800s cluster with 8 A800 GPUs and 8 IB NICs per node 2. **Some main arguments**         Parallel environment: world_size=128(8*16), TP=4, PP=1, SP=False         Model structure: numlayers 40, hiddensize 5120, numattentionheads 40,  seqlength 2048 \         Training configuration: micro_batch_size=4, global_batch_size=3840         Acceleration configuration: bf16=True, recomputeactivations=True 3. **Whole arguments** ```    arguments        accumulate_allreduce_grads_in_fp32 .............. False       adam_beta1 ...................................... 0.9       adam_beta2 ...................................... 0.999       adam_eps ........................................ 1e08       add_bias_linear ................................. False       add_position_embedding .......................... True       adlr_autoresume ................................. False       adlr_autoresume_interval ........................ 1000       apply_layernorm_1p .............................. False       apply_query_key_layer_scaling ................... True       apply_residual_connection_post_layernorm ........ False       async_tensor_model_parallel_allreduce ........... True       attention_dropout ............................... 0.1       attention_softmax_in_fp32 ....................... False       barrier_with_L1_time ............................ True       bert_binary_head ................................ True       bert_embedder_type .............................. megatron       bert_load ....................................... None       bf16 ............................................ False       bias_dropout_fusion ............................. True       bias_gelu_fusion ................................ False       biencoder_projection_dim ........................ 0       biencoder_shared_query_context_model ............ False       block_data_path ................................. None       check_for_nan_in_loss_and_grad .................. True       classes_fraction ................................ 1.0       clip_grad ....................................... 1.0       consumed_train_samples .......................... 0       consumed_valid_samples .......................... 0       data_cache_path ................................. None       data_parallel_random_init ....................... False       data_parallel_size .............................. 32       data_path .......................................        data_per_class_fraction ......................... 1.0       data_sharding ................................... True       dataloader_type ................................. single       decoder_num_layers .............................. None       decoder_seq_length .............................. None       dino_bottleneck_size ............................ 256       dino_freeze_last_layer .......................... 1       dino_head_hidden_size ........................... 2048       dino_local_crops_number ......................... 10       dino_local_img_size ............................. 96       dino_norm_last_layer ............................ False       dino_teacher_temp ............................... 0.07       dino_warmup_teacher_temp ........................ 0.04       dino_warmup_teacher_temp_epochs ................. 30       distribute_saved_activations .................... False       distributed_backend ............................. nccl       distributed_timeout_minutes ..................... 100       embedding_path .................................. None       embedding_weights_in_fp32 ....................... False       empty_unused_memory_level ....................... 0       encoder_num_layers .............................. 40       encoder_seq_length .............................. 2048       end_weight_decay ................................ 0.1       eod_mask_loss ................................... False       eval_interval ................................... 500       eval_iters ...................................... 10       evidence_data_path .............................. None       exit_duration_in_mins ........................... None       exit_interval ................................... None       exit_on_missing_checkpoint ...................... False       exit_signal_handler ............................. False       ffn_hidden_size ................................. 20480       finetune ........................................ False       fp16 ............................................ True       fp16_lm_cross_entropy ........................... False       fp32_residual_connection ........................ False       fp8 ............................................. None       fp8_amax_compute_algo ........................... most_recent       fp8_amax_history_len ............................ 1       fp8_interval .................................... 1       fp8_margin ...................................... 0       fp8_wgrad ....................................... True       global_batch_size ............................... 3840       gradient_accumulation_fusion .................... True       group_query_attention ........................... False       head_lr_mult .................................... 1.0       hidden_dropout .................................. 0.1       hidden_size ..................................... 5120       hysteresis ...................................... 2       ict_head_size ................................... None       ict_load ........................................ None       img_h ........................................... 224       img_w ........................................... 224       indexer_batch_size .............................. 128       indexer_log_interval ............................ 1000       inference_batch_times_seqlen_threshold .......... 512       init_method_std ................................. 0.02       init_method_xavier_uniform ...................... False       initial_loss_scale .............................. 4294967296       iter_per_epoch .................................. 1250       kv_channels ..................................... 128       lazy_mpu_init ................................... None       load ............................................        local_rank ...................................... None       log_batch_size_to_tensorboard ................... True       log_interval .................................... 5       log_learning_rate_to_tensorboard ................ True       log_loss_scale_to_tensorboard ................... True       log_memory_to_tensorboard ....................... True       log_num_zeros_in_grad ........................... False       log_params_norm ................................. False       log_timers_to_tensorboard ....................... True       log_validation_ppl_to_tensorboard ............... True       log_world_size_to_tensorboard ................... False       loss_scale ...................................... None       loss_scale_window ............................... 1000       lr .............................................. 0.0003       lr_decay_iters .................................. None       lr_decay_samples ................................ None       lr_decay_style .................................. cosine       lr_warmup_fraction .............................. 0.01       lr_warmup_init .................................. 0.0       lr_warmup_iters ................................. 0       lr_warmup_samples ............................... 0       make_vocab_size_divisible_by .................... 128       mask_factor ..................................... 1.0       mask_prob ....................................... 0.15       mask_type ....................................... random       masked_softmax_fusion ........................... True       max_position_embeddings ......................... 2048       max_tokens_to_oom ............................... 12000       merge_file ...................................... None       micro_batch_size ................................ 4       min_loss_scale .................................. 1.0       min_lr .......................................... 3e06       mmap_warmup ..................................... False       no_load_optim ................................... None       no_load_rng ..................................... None       no_persist_layer_norm ........................... False       no_save_optim ................................... None       no_save_rng ..................................... None       norm_epsilon .................................... 1e05       normalization ................................... LayerNorm       num_attention_heads ............................. 40       num_channels .................................... 3       num_classes ..................................... 1000       num_experts ..................................... None       num_layers ...................................... 40       num_layers_per_virtual_pipeline_stage ........... None       num_query_groups ................................ 1       num_workers ..................................... 2       onnx_safe ....................................... None       openai_gelu ..................................... False       optimizer ....................................... adam       output_bert_embeddings .......................... False       overlap_grad_reduce ............................. False       overlap_p2p_comm ................................ False       override_opt_param_scheduler .................... False       params_dtype .................................... torch.float16       patch_dim ....................................... 16       perform_initialization .......................... True       pipeline_model_parallel_size .................... 1       pipeline_model_parallel_split_rank .............. None       position_embedding_type ......................... learned_absolute       profile ......................................... False       profile_ranks ................................... [0]       profile_step_end ................................ 12       profile_step_start .............................. 10       query_in_block_prob ............................. 0.1       rampup_batch_size ............................... None       rank ............................................ 0       recompute_granularity ........................... selective       recompute_method ................................ None       recompute_num_layers ............................ None       reset_attention_mask ............................ True       reset_position_ids .............................. False       retriever_report_topk_accuracies ................ []       retriever_score_scaling ......................... False       retriever_seq_length ............................ 256       retro_add_retriever ............................. False       retro_cyclic_train_iters ........................ None       retro_encoder_attention_dropout ................. 0.1       retro_encoder_hidden_dropout .................... 0.1       retro_encoder_layers ............................ 2       retro_num_neighbors ............................. 2       retro_num_retrieved_chunks ...................... 2       retro_return_doc_ids ............................ False       retro_workdir ................................... None       rotary_percent .................................. 1.0       rotary_seq_len_interpolation_factor ............. None       sample_rate ..................................... 1.0       save ............................................         save_interval ................................... 500       scatter_gather_tensors_in_pipeline .............. True       seed ............................................ 1234       seq_length ...................................... 2048       sequence_parallel ............................... False       sgd_momentum .................................... 0.9       short_seq_prob .................................. 0.1       skip_train ...................................... False       split ........................................... 998,2,1       squared_relu .................................... False       standalone_embedding_stage ...................... False       start_weight_decay .............................. 0.1       swiglu .......................................... False       swin_backbone_type .............................. tiny       tensor_model_parallel_size ...................... 4       tensorboard_dir .................................        tensorboard_log_interval ........................ 5       tensorboard_queue_size .......................... 1000       test_data_path .................................. None       timing_log_level ................................ 2       timing_log_option ............................... max       titles_data_path ................................ None       tokenizer_model .................................       tokenizer_type .................................. SentencePieceTokenizer       train_data_path ................................. None       train_iters ..................................... 190317       train_samples ................................... None       transformer_impl ................................ local       transformer_pipeline_model_parallel_size ........ 1       untie_embeddings_and_output_weights ............. False       use_checkpoint_args ............................. False       use_checkpoint_opt_param_scheduler .............. False       use_cpu_initialization .......................... None       use_distributed_optimizer ....................... True       use_flash_attn .................................. False       use_one_sent_docs ............................... False       use_ring_exchange_p2p ........................... False       use_rotary_position_embeddings .................. False       valid_data_path ................................. None       variable_seq_lengths ............................ False       virtual_pipeline_model_parallel_size ............ None       vision_backbone_type ............................ vit       vision_pretraining .............................. False       vision_pretraining_type ......................... classify       vocab_extra_ids ................................. 0       vocab_file ...................................... None       vocab_size ...................................... None       weight_decay .................................... 0.1       weight_decay_incr_style ......................... constant       world_size ...................................... 128              end of arguments  ```",2023-09-12T04:10:33Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/495,Using the latest NGC Pytorch container solves this problem ! :smile::smile:,"Hmm, interesting that the new container solves the problem. Which container did you use?",The NGC Pytorch Container  Tag: 23.08py3
haoliuhl,[ENHANCEMENT] Support scan over sequence dimension when computing FFN ,"**Is your feature request related to a problem? Please describe.** It's difficult to train large context LLMs without outofmemory errors, e.g., when training LLMs on repository level code. **Describe the solution you'd like** Blockwise scan a rematerialized FFN over sequence dimension, ie, computing FFN block by block. This can reduce memory cost by up to 4x over stateoftheart Memeff / Flashattention as proposed in BPT.  As Section 3.1 of the paper shows, blockwise scan of FFN reduces memory cost by 4 times. **Describe alternatives you've considered** Alternative solution is sequence parallelism of selfattention and FFN. Applying it on selfattention incurs extra communication cost, on FFN it's OK. Sequence parallelism shards sequence across hosts, and BPT reduces memory cost on each host. **Proposed implementation**  A Jax implementation of BPT is available and the most relevant part. The code allows us to train models on small HBM TPUv3, it would be great to if MegatronLM supports the feature.",2023-09-10T06:30:20Z,stale,closed,2,1,https://github.com/NVIDIA/Megatron-LM/issues/494,Marking as stale. No activity in 60 days.
superzhangmch,"[QUESTION]why use the [s, b, h] data layout instead of [b, s, h]?","> Efficient LargeScale Language Model Training on GPU Clusters > Using MegatronLM >  > 4.2 Computation Optimizations > We implemented three modelspecific optimizations to the computation graph to attain high performance.  > First, we changed the data layout in the transformer layer to avoid memoryintensive transpose operations,  > and to enable the use of strided batched GEMM kernels. Specifically, we changed the data layout  > from [ð‘, ð‘ , ð‘Ž, â„Ž] to [ð‘ , ð‘, ð‘Ž, â„Ž], where ð‘, ð‘ , ð‘Ž, and â„Ž are batch, sequence, attentionhead, and hiddensize  > dimensions, respectively. It is mentioned in the above paper. what (and where) are the memoryintensive transpose operations if we don't do so?  why it enables the use of strided batched GEMM kernels?",2023-09-10T03:20:17Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/493,for efficiency of seq_parallel,"> for efficiency of seq_parallel according to the timeline of metatronlm paper series, seq_parallel did not appear in this paper(Efficient LargeScale Language Model Training on GPU Clusters Using MegatronLM). It appeared firstly in the subsequent paper: Reducing Activation Recomputation in Large Transformer Models",Marking as stale. No activity in 60 days.
adammoody,[ENHANCEMENT] Tools for distributed data preprocessing,"For the BigScience effort, I developed ``preprocess_data_dist.py`` to parallelize data preprocessing of large datasets.  As an example, by using 32 compute nodes, this reduces the time it takes to preprocess the 1TiB OSCAR dataset from days to an hour.  It produces ``indexed_dataset`` files that are identical to those created by the ``preprocess_data.py`` script.  An overview of the usage is written up in the ""distributed data preprocessing"" subsection here: https://github.com/adammoody/MegatronDeepSpeed1/tree/distdatadatapreprocessingdistributed A pointer to the actual script: https://github.com/adammoody/MegatronDeepSpeed1/blob/distdata/tools/preprocess_data_dist.py There is a related, simpler script that merges multiple ``indexed_dataset`` files into a single file.  This is handy if one has preprocessed a large dataset in pieces using some other method and wants to efficiently join those into a single dataset: https://github.com/bigscienceworkshop/MegatronDeepSpeed/blob/main/tools/merge_preprocessed_data.py Additionally, I have an ``indexed_json.py`` class that creates an index for JSONL files that allows for random access to different lines in the file.  It uses multiple processes to read different regions of the source file and collectively scans and records the location of newline characters that delimit lines in the source file: https://github.com/adammoody/MegatronDeepSpeed1/blob/distdata/tools/indexed_json.py All three of these rely on a new ``DistData`` class that abstracts a number of collective operations helpful for accessing shared files.  The version contributed to BigScience only uses torch.distributed, though I also have a version that can optionally use mpi4py.  I personally prefer to use mpi4py, which is easier to launch on my system and is a bit more robust. https://github.com/adammoody/MegatronDeepSpeed1/blob/distdata/tools/distdata.py These scripts are particularly useful for those who have access to systems with multiple compute nodes that are connected to a highperformance parallel file system.  Since people running MegatronLM likely have access to such resources, I expect this could be useful to other MegatronLM users. The ``indexed_dataset`` file format has been updated recently, and the above work needs to be refreshed to match to that new file format. Would this be of interest?",2023-09-09T00:41:27Z,stale,closed,6,1,https://github.com/NVIDIA/Megatron-LM/issues/492,Marking as stale. No activity in 60 days.
sighingnow,"""Lazy"" import symbols from apex to make data preprocess can run without GPUs","The data preprocessing step doesn't require GPUs, but it requires some utility modules which unnecessarily import `apex` package and then fails where there's only CPU available. Separately deploying data preprocessing and training on different environments would be a nice feature for developers. This pull request fixes that by lazily import (evaluate the availability) those symbols and suggests installing apex when they are required but not found.",2023-09-08T06:38:19Z,,closed,0,6,https://github.com/NVIDIA/Megatron-LM/issues/491,"The change set is minor, and should have no effect on the training process.",Polite ping , I was able to run inside the pytorch container without attaching any gpus and run preprocessing.  Could you share the error that you see?,"Hi , I'm running the data preprocessing step in an environment where nvidia GPUs is not available and cuda is not installed. Where I first install apex without `cuda_ext`: ``` pip install v disablepipversioncheck nocachedir nobuildisolation configsettings ""buildoption=cpp_ext"" ./ ``` where `amp_C` won't be built as there's no CUDA. Then, using the following command for data preprocessing: ``` python3 tools/preprocess_data.py \     workers 1 \     vocabfile /opt/tao/llm/gpt2/gpt2vocab.json \     datasetimpl mmap \     tokenizertype GPT2BPETokenizer \     mergefile /opt/tao/llm/gpt2/gpt2merges.txt \     appendeod \     input /opt/tao/llm/wiki10k.json \     outputprefix /opt/tao/llm/wiki10k ``` The complaints: ``` No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda' Zarrbased strategies will not be registered because of missing packages Traceback (most recent call last):   File ""/opt/tao/llm/MegatronLM/tools/preprocess_data.py"", line 23, in      from megatron.tokenizer import build_tokenizer   File ""/opt/tao/llm/MegatronLM/megatron/__init__.py"", line 14, in      from .initialize  import initialize_megatron   File ""/opt/tao/llm/MegatronLM/megatron/initialize.py"", line 19, in      from megatron.checkpointing import load_args_from_checkpoint   File ""/opt/tao/llm/MegatronLM/megatron/checkpointing.py"", line 15, in      from .utils import (unwrap_model,   File ""/opt/tao/llm/MegatronLM/megatron/utils.py"", line 11, in      import amp_C ModuleNotFoundError: No module named 'amp_C' ```","Hmm I see, this works fine without gpus if using the recommended path of the NGC pytorch container since its already inside.  I guess the path for building manually should work as well","> Hmm I see, this works fine without gpus if using the recommended path of the NGC pytorch container since its already inside. I guess the path for building manually should work as well It works inside the NGC pytorch container because the container is built with CUDA available and with apex preinstalled. Where CUDA is not available and users want to run the preprocessing steps on their own host s or other containers, this patch will be required."
erhoo82,Use SHARP to AMAX reduction,Use SHARP to TP+DP AMAX reduction. Env vars to set to apply SHARP to only AMAX reduction group. `MEGATRON_CORE_USE_SHARP_DP=0` `MEGATRON_CORE_USE_SHARP_AMAX=1`,2023-09-07T17:40:28Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/490
adammoody,indexed_dataset: define size of uint16,The ``np.uint16`` type is a valid dtype used in ``indexed_dataset.py``.  This PR adds the type to the ``element_sizes`` dictionary.,2023-09-06T19:10:21Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/489
yxyOo,[ENHANCEMENT]Analysis Tool,"I'm always frustrated that I can't estimate the amount of resources the model will consume during the training of large language models, or determine whether my training configuration will lead to outofmemory error. It's equally frustrating not knowing the minimum number of GPU cards needed, which prevents appropriate resource allocation. Running the model to solve these issues is both timeconsuming and ineffective. Moreover, I desire to understand more detailed information in during the training process, such as communication information and mappings between GPU and model.   To tackle these issues, I've developed the Analysis Tool for offline analysis of memory requirements and communication data during MegatronLM GPTModel training under hybrid parallel strategies. What do you think of this tool?",2023-09-06T09:49:08Z,stale,closed,1,3,https://github.com/NVIDIA/Megatron-LM/issues/488,Marking as stale. No activity in 60 days.,"This tool is awesome, I really need this analysis tool",Marking as stale. No activity in 60 days.
singleheart,Add RMSNorm,"This is a different version from PR CC(Add RMSNorm). Since the submission of PR CC(Add RMSNorm), RMSNorm has been added to MegatronLM. However, it currently requires transformer_engine. This version adds support for RMSNorm without transformer_engine.  Therefore, it can be used with the following configuration: ```bash transformerimpl local normalization RMSNorm ```",2023-09-05T09:37:44Z,stale,open,2,2,https://github.com/NVIDIA/Megatron-LM/issues/487,"Hey, PR CC(Add RMSNorm) also adds RMSNorm without TransformerEngine; our PRs don't differ in that regard. :) While CC(Add RMSNorm) modifies the code base more generally than this PR, I like your introduction of a new `MixedFusedNorm` class. This is a much cleaner API than I introduced.",Marking as stale. No activity in 60 days.
PinzhengWang322,Fix bug passing encoder_decoder_attn_mask,I noticed an issue with the `encoder_decoder_attn_mask`. It appears that it's not being passed correctly to the `forward` method of the `megatron.model.language_model.TransformerLanguageModel` class when using T5 model.,2023-09-05T03:08:33Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/486,Marking as stale. No activity in 60 days.
guyueh1,Fix pipeline parallel hang under FP8,"This PR fixes a program hang issue when using FP8 TE and pipeline parallel.  As shown in commit 588ef65 and previous commits, correct usage of FP8 context requires a `fp8_group` argument which should be initialized by the `get_amax_reduction_group()` API. Refer to the nonmcore version in Nemo https://github.com/NVIDIA/NeMo/blob/458f630724a77b2dcd928004a6c078a25cc0a627/nemo/collections/nlp/modules/common/megatron/transformer.pyL1455",2023-09-05T01:57:27Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/485,Will close this and draft the pr in internal repo.
60wanjinbing, megatron-lm==1.1.5 and  megatron-lm==2.2.0,how to download  megatronlm==1.1.5 and  megatronlm==2.2.0 in python ?,2023-09-03T14:08:37Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/484,You can see all tagged versions here: https://github.com/NVIDIA/MegatronLM/tags. Doing something like: `git fetch origin; git checkout ` should then give you the right version that you want.,Going to close this; feel free to reopen if you run into difficulties.
mikolajblaz,Fix bug in distopt allgathers with interleaved pipeline parallelism for old MCore,"This is an equivalent of this commit, but applied to old MCore",2023-09-01T17:40:28Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/483
yxyOo,Analysis Tool," Introduction Offline analysis of memory requirements and communication information of MegatronLM GPTModel training under hybrid parallel strategies  Features Given the GPT model configuration and parallel training configuration, this tool will output the following: * Detail the memory requirements for Parameters, Gradients, Optimizer States and Activations at the Transformer granularity level on each GPU. * Provide an estimate predicting the least amount of memory a GPU needs to train the GPT model without causing OutofMemory (OOM) errors. * Describe the communication requirements when implementing Data Parallelism, Pipeline Parallelism and Tensor Parallelism. State how many times each dimension needs to communicate, the amount of data transmitted each time and the members of the communication group, among others. * Describe the changes in the size of the Transformer model before and after parallel and how these changes impact GPU utilization. We randomly selected some parallel configurations and used the ""Memory Requirement"" output in this tool as the estimated value, and the output of torch.cuda.max_memory_allocated() in MegatronLM report_memory after training several iterations as the actual value. The parallel configurations in the xaxis of the following figure correspond to the four model parallel configurations in the table below in order. This can give users insight into whether their planned parallel configuration is trainable, and if it potentially could trigger OOM errors.     Model      â””â”€dense_4h_to_h            w=[4096,5440],b=[4096]   	85.0           	64.0           	[4,2048,4096]            	64.0           	                         	               	allreduce                	 â”‚    â”‚    â””â”€drop_add_fusion                                         	               	96.0           	  8 Tensor Parallel Communication Groups: â”‚    â””â”€[n0_g0 n0_g1] â”‚    â””â”€[n0_g2 n0_g3] â”‚    â””â”€[n0_g4 n0_g5] â”‚    â””â”€[n0_g6 n0_g7] â”‚    â””â”€[n1_g0 n1_g1] â”‚    â””â”€[n1_g2 n1_g3] â”‚    â””â”€[n1_g4 n1_g5] â”‚    â””â”€[n1_g6 n1_g7] Communication in Tensor Parallel â”‚    â””â”€each gpu: â”‚    â”‚    â””â”€each micro_batch: â”‚    â”‚    â”‚    â””â”€frequency: 64 â”‚    â”‚    â”‚    â””â”€volume: 4.0GB â”‚    â”‚    â”‚    â””â”€each transformer: â”‚    â”‚    â”‚    â”‚    â””â”€frequency: 2(forward)+2(backward)=4 â”‚    â”‚    â”‚    â”‚    â””â”€volume: 0.25GB â”‚    â”‚    â””â”€each iteration: â”‚    â”‚    â”‚    â””â”€frequency: 2048 â”‚    â”‚    â”‚    â””â”€volume: 128.0GB â”‚    â””â”€cluster: â”‚    â”‚    â””â”€each micro_batch: â”‚    â”‚    â”‚    â””â”€frequency: 1024 â”‚    â”‚    â”‚    â””â”€volume: 64.0GB â”‚    â”‚    â””â”€each iteration: â”‚    â”‚    â”‚    â””â”€frequency: 32768 â”‚    â”‚    â”‚    â””â”€volume: 2048.0GB ======================================================================================================================================================================================================================= ```",2023-09-01T12:45:20Z,stale,open,6,14,https://github.com/NVIDIA/Megatron-LM/issues/482,This looks interesting! How accurate is it?,Really awesome!!,"> This looks interesting! How accurate is it? We randomly selected several parallel configurations and conducted ""Memory Requirement"" tests on the 7B llama2 model using a single H800 machine with eight cards. The results showed that the error was within 1% for all measurements. All other values output by the tool were theoretical.",+1 for Really awesome!,"> > This looks interesting! How accurate is it? >  > We randomly selected several parallel configurations and conducted ""Memory Requirement"" tests on the 7B llama2 model using a single H800 machine with eight cards. The results showed that the error was within 1% for all measurements. All other values output by the tool were theoretical. Is this verification based on the code base here or that used `torch.cuda.CUDAPluggableAllocator`? I am wondering whether we really need to execute the code to get the peak memory usage.","Hi : I have a few questions about total_parameters computing. Since you mentioned your experiments on llama, but I find some inconsistency: 1. llama doesn't have bias 2. llama doesn't use shared embeddings 3. llama doesn't have position_embeddings Thanks","Hi,  this is a great feature! While my suggestion might seem a bit much, I believe it would be beneficial to use the default argument parser from training. This way, you could simply replace the training executable name with this tool and receive an analysis without additional effort (Or even print it before training if you want).  Moreover, it's particularly handy for LLaMa checkpoints, as most arguments are read directly from the checkpoints (`usecheckpointargs`).","> Hi,  this is a great feature! While my suggestion might seem a bit much, I believe it would be beneficial to use the default argument parser from training. This way, you could simply replace the training executable name with this tool and receive an analysis without additional effort (Or even print it before training if you want). Here is a simple training script that computes the ""theoretical"" memory usage of a model: https://github.com/NVIDIA/MegatronLM/blob/main/compute_memory_usage.py. It reuses the existing argument parser so we can easily do precisely what you ask for. It is under active development and should get better in the coming days.","> Hi : I have a few questions about total_parameters computing. Since you mentioned your experiments on llama, but I find some inconsistency: >  > 1. llama doesn't have bias > 2. llama doesn't use shared embeddings > 3. llama doesn't have position_embeddings >  > Thanks Thank you for pointing out that it should be the GPT model.","> > > This looks interesting! How accurate is it? > >  > >  > > We randomly selected several parallel configurations and conducted ""Memory Requirement"" tests on the 7B llama2 model using a single H800 machine with eight cards. The results showed that the error was within 1% for all measurements. All other values output by the tool were theoretical. >  > Is this verification based on the code base here or that used `torch.cuda.CUDAPluggableAllocator`? I am wondering whether we really need to execute the code to get the peak memory usage. It is based on the code base. Before training your model, you can use this tool to determine the minimum amount of memory the model will consume.","> Hi,  this is a great feature! While my suggestion might seem a bit much, I believe it would be beneficial to use the default argument parser from training. This way, you could simply replace the training executable name with this tool and receive an analysis without additional effort (Or even print it before training if you want). >  > Moreover, it's particularly handy for LLaMa checkpoints, as most arguments are read directly from the checkpoints (`usecheckpointargs`). > itiona Thank you for your suggestion, I did it this way at the time to quickly develop this tool, haha. If needed, I will consider supporting related features in the future.",Marking as stale. No activity in 60 days.,"> > Hi : I have a few questions about total_parameters computing. Since you mentioned your experiments on llama, but I find some inconsistency: > >  > > 1. llama doesn't have bias > > 2. llama doesn't use shared embeddings > > 3. llama doesn't have position_embeddings > >  > > Thanks >  > Thank you for pointing out that it should be the GPT model. Note if flash attention used, memory cost is O(b*h*s*d) not O(b*h*s*s*d). ",Marking as stale. No activity in 60 days.
shjwudp,Automatically find and install all packages under megatron/core,Fixed the issue where `megatron.core.dist_checkpointing` cannot be found when using the latest version of nemo.,2023-09-01T09:09:07Z,stale,open,1,1,https://github.com/NVIDIA/Megatron-LM/issues/481,Marking as stale. No activity in 60 days.
eagle705,[QUESTION] MHA to GQA Converter for LLaMA2 Support,"Hello! I've been reviewing the process of training the LLaMA2 architecture from MegatronLM recently and I'm interested in trying to convert MHA to GQA. I've confirmed that it's possible to train GQA from scratch, but I'd like to know `if there are plans for a conversion script to transform existing MHA checkpoints into GQA checkpoints`. I attempted to convert the key and value weights using the following logic, but it didn't seem to work well: ```python key = torch.mean(key.view([num_query_groups_per_partition, 1, hidden_size_per_attention_head, hidden_size]), dim=1).view([1, hidden_size]) ```",2023-08-29T02:35:00Z,,closed,4,3,https://github.com/NVIDIA/Megatron-LM/issues/480,I solved it," hi, how did you solve itï¼Ÿ",Any information on this?
HelloWorldBeginner,This line of code is repeated.,This line of code is repeated.,2023-08-25T07:50:38Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/479
Barber0,[ENHANCEMENT] Enhance data efficiency with efficient sequence packing,"As far as I know, MegatronLM requires the input sequence length to be fixed and padded to the `seqlength`. However, for some SFT datasets like tatsulab/alpaca or OpenOrca/OpenOrca, whose encoded sequence length rarely reach to 2048tokens, that would cause waste of computing resource. Does Megatron team have plan to apply some efficient sequence packing methods like this paper: Efficient Sequence Packing without Crosscontamination: Accelerating Large Language Models without Impacting Performance? Maybe the technique mentioned above can be added to the script: preprocess_data.py.",2023-08-24T08:16:25Z,,closed,4,2,https://github.com/NVIDIA/Megatron-LM/issues/478,Marking as stale. No activity in 60 days.,Wonder the same thing
benkerd22,Fix inference_params bug when using transformer engine,"During inference, the current version of TransformerEngine will try to get property `max_sequence_len` of `inference_params`, which will result in an error. https://github.com/NVIDIA/TransformerEngine/blob/main/transformer_engine/pytorch/attention.pyL1580 We should add backward compatibility for this legacy name.",2023-08-24T07:16:51Z,stale,open,0,4,https://github.com/NVIDIA/Megatron-LM/issues/477,"Hi  , could you review this PR? Thanks a lot!",Marking as stale. No activity in 60 days.,I have the same error. How do i resolve it?,Marking as stale. No activity in 60 days.
hwdef,[QUESTION] how to use SHARP with Megatron,"I checked the source code and found the following message: https://github.com/NVIDIA/MegatronLM/blob/f24fac4ed0dcf0522056521a93445d9a82f501a9/megatron/core/parallel_state.pyL200C14L200C14 ```python  Apply SHARP to DP process groups     if use_sharp:         if rank == 0:             print(                 ""The number of process groups to use SHARP with depends on the type ""                 ""of the network switch. Nvidia QM1 switch supports SAHRP up to 8 ""                 ""process groups and QM2 supports up to 256 process groups. We apply ""                 ""SHARP to the communications of the dataparallel domain. If the ""                 ""number of dataparallel process groups is larger than the max ""                 ""process groups that the network switch supports, the communication ""                 ""will fall back to nonSHARP operators. To enable SHARP, ""                 ""`SBATCH_NETWORK=sharp` should be set in the sbatch script.""             )         torch.distributed.barrier(             group=get_data_parallel_group(), device_ids=[torch.cuda.current_device()]         )          Set `NCCL_SHARP_DISABLE=1` to restrict SHARP application to DP process groups         os.environ[""NCCL_SHARP_DISABLE""] = ""1"" ``` Megatron wants to use sharp when DP, so it turns off SHARP globally. I'm guessing that Megatron turns SHARP on at DP time, but I haven't found the code for that. Searching for NCCL_SHARP_DISABLE globally, I only found one. It's the above global disable SHARP !image Right now I'm in a situation where Megatron is requesting too many pg, resulting in insufficient resource SHARP resources. I want to know what to do in this situation",2023-08-22T06:41:09Z,,closed,0,12,https://github.com/NVIDIA/Megatron-LM/issues/476,  Could you help me ?,"```         torch.distributed.barrier(             group=get_data_parallel_group(), device_ids=[torch.cuda.current_device()]         ) ``` This line calls nccl allreduce, which triggers the initialization of the nccl comm used by the dp group, with sharp enabled.","> ``` >         torch.distributed.barrier( >             group=get_data_parallel_group(), device_ids=[torch.cuda.current_device()] >         ) > ``` >  > This line calls nccl allreduce, which triggers the initialization of the nccl comm used by the dp group, with sharp enabled. Do I need to set any additional environment variables?","As the comment says, you need to set `SBATCH_NETWORK=sharp` in your sbatch script assuming you are using slurm. COLLNET_DIRECT algorithm is supposed to be used for the collective kernels when all conditions for SHARP are met. However, you can add these env vars to help enabling SHARP `NCCL_ALGO=CollNetDirect` and `NCCL_SHARP_GROUP_SIZE_THRESH=2`.","> As the comment says, you need to set `SBATCH_NETWORK=sharp` in your sbatch script assuming you are using slurm. COLLNET_DIRECT algorithm is supposed to be used for the collective kernels when all conditions for SHARP are met. However, you can add these env vars to help enabling SHARP `NCCL_ALGO=CollNetDirect` and `NCCL_SHARP_GROUP_SIZE_THRESH=2`. I use k8s. I already added these env in the container. But I consistently get the SHARP insufficient resources error, and I'm using ConnectX6. I have two other confusing points: 1 In the code, it is the first call to the ```python torch.distributed.barrier(     group=get_data_parallel_group(), device_ids=[torch.cuda.current_device()] ) ``` After calling ```python os.environ[""NCCL_SHARP_DISABLE""] = ""1"" ``` If the top one is OPEN and the bottom one is CLOSE, then first the SHARP is turned OPEN at the DP and then it's turned CLOSE globally, I understand that the OPEN won't take effect? Because it was turned OPEN and then turned CLOSE again. 2 Has megartron created too many groups? ```python     for i in range(pipeline_model_parallel_size):         start_rank = i * num_pipeline_model_parallel_groups         end_rank = (i + 1) * num_pipeline_model_parallel_groups         for j in range(tensor_model_parallel_size):             ranks = range(start_rank + j, end_rank, tensor_model_parallel_size)             all_data_parallel_group_ranks.append(list(ranks))             group = torch.distributed.new_group(ranks)             group_gloo = torch.distributed.new_group(ranks, backend=""gloo"")             if rank in ranks:                 _DATA_PARALLEL_GROUP = group                 _DATA_PARALLEL_GROUP_GLOO = group_gloo                 _DATA_PARALLEL_GLOBAL_RANKS = ranks ``` The above code calls `new_group` after a double *for* loop, does this create too many groups, resulting in insufficient SHARP resources?","> 1. If the top one is OPEN and the bottom one is CLOSE, then first the SHARP is turned OPEN at the DP and then it's turned CLOSE globally, I understand that the OPEN won't take effect? Because it was turned OPEN and then turned CLOSE again. Yes, the DP groups that we set barrier to set them as the target apply SHARP should not be affected with the env var disabled. > 2. The above code calls new_group after a double for loop, does this create too many groups, resulting in insufficient SHARP resources? Can you try with a few dataparallel groups? I have tested this on QM1 with DP=8 and QM2 with DP16, and both worked for me.", Thank you!,"> NCCL_SHARP_DISABLE  Where is this parameter used, I don't find a reference to it in the `nccl` documentation or code. One more question: why don't you turn on sharp globally in this commit, but rather on the DP.","> Where is this parameter used, I don't find a reference to it in the nccl documentation or code. Oh, I found it here.","> One more question: why don't you turn on sharp globally in this commit, but rather on the DP. I got it, it's because DPs usually communicate across machines and need to use switches, so turn on IB sharp between DPs.","Going to close this, feel free to reopen if you still have questions.","> I use k8s   I also use k8s, are there other ENV needed to set for megatron to use sharp? "
Barber0,[QUESTION] What does persistent layer norm kernel mean?,"I am reading and learning the source code of MegatronLM. I am curious about the concept `persistent layer norm kernel`. There's a comment in fused_layer_norm.py shows that  `List of hiddens sizes supported in the persistent layer norm kernel  If the hidden size is not supported, fall back to the nonpersistent  kernel.""` What does **persistent layer norm kernel** mean?",2023-08-22T05:08:32Z,question,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/475,This refers to the use of persistent threads in the apex layer norm kernel I believe. So it's a kernel that does not exit and is continually polling for work from a queue in global memory. 
Barber0,[QUESTION] How to merge the weights of sequence-parallel LayerNorm ?,"I tried to pretrain a GPTlike language model with sequenceparallel.  And I found that the LayerNorm weights in different ranks various. `LayerNorm weights in Rank0:  tensor([0.5815, 0.6235, 0.5947,  ..., 0.4775, 0.4360, 0.4680],        dtype=torch.float16)` `LayerNorm weights in Rank1:  tensor([0.6885, 0.6792, 0.6880,  ..., 0.6987, 0.6108, 0.5713],        dtype=torch.float16)` If I want to merge these splitedweights into an intact set of weights that can run in single GPU, how to merge these LayerNorm weights?",2023-08-22T01:09:37Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/474,"Where are you printing these? We do an allreduce already to ""merge"" the gradients across the different sequenceparallel replicas: https://github.com/NVIDIA/MegatronLM/blob/805a3d5af69949e0266ec7c95a177bbfcb621331/megatron/core/pipeline_parallel/distrib_grad.pyL105 You shouldn't be seeing different LayerNorm weights.","Going to close this, please reopen if you are still running into this issue."
mare5x,Escape `%` in help message for `--rotary-percent`.,Fix for https://github.com/NVIDIA/MegatronLM/issues/472.,2023-08-17T10:52:42Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/473
mare5x,[BUG] argparse % escape error for --help,"**Describe the bug** Error when calling `python tools/retro/main.py help`. **To Reproduce** Call from the repository root: `python tools/retro/main.py help` (or `python m tools.retro.main help`) **Expected behavior** Help should be shown. It isn't. **Stack trace/logs** ```bash $ python tools/retro/main.py help Traceback (most recent call last):   File ""/home/marko/dev/flretro/MegatronLM/tools/retro/main.py"", line 197, in      initialize_megatron(extra_args_provider=add_retro_args)   File ""/home/marko/dev/flretro/MegatronLM/megatron/initialize.py"", line 44, in initialize_megatron     args = parse_args(extra_args_provider, ignore_unknown_args)   File ""/home/marko/dev/flretro/MegatronLM/megatron/arguments.py"", line 50, in parse_args     args = parser.parse_args()   File ""/home/marko/miniconda3/envs/flretro/lib/python3.10/argparse.py"", line 1833, in parse_args     args, argv = self.parse_known_args(args, namespace)   File ""/home/marko/miniconda3/envs/flretro/lib/python3.10/argparse.py"", line 1866, in parse_known_args     namespace, args = self._parse_known_args(args, namespace)   File ""/home/marko/miniconda3/envs/flretro/lib/python3.10/argparse.py"", line 2079, in _parse_known_args     start_index = consume_optional(start_index)   File ""/home/marko/miniconda3/envs/flretro/lib/python3.10/argparse.py"", line 2019, in consume_optional     take_action(action, args, option_string)   File ""/home/marko/miniconda3/envs/flretro/lib/python3.10/argparse.py"", line 1943, in take_action     action(self, namespace, argument_values, option_string)   File ""/home/marko/miniconda3/envs/flretro/lib/python3.10/argparse.py"", line 1106, in __call__     parser.print_help()   File ""/home/marko/miniconda3/envs/flretro/lib/python3.10/argparse.py"", line 2567, in print_help     self._print_message(self.format_help(), file)   File ""/home/marko/miniconda3/envs/flretro/lib/python3.10/argparse.py"", line 2551, in format_help     return formatter.format_help()   File ""/home/marko/miniconda3/envs/flretro/lib/python3.10/argparse.py"", line 283, in format_help     help = self._root_section.format_help()   File ""/home/marko/miniconda3/envs/flretro/lib/python3.10/argparse.py"", line 214, in format_help     item_help = join([func(*args) for func, args in self.items])   File ""/home/marko/miniconda3/envs/flretro/lib/python3.10/argparse.py"", line 214, in      item_help = join([func(*args) for func, args in self.items])   File ""/home/marko/miniconda3/envs/flretro/lib/python3.10/argparse.py"", line 214, in format_help     item_help = join([func(*args) for func, args in self.items])   File ""/home/marko/miniconda3/envs/flretro/lib/python3.10/argparse.py"", line 214, in      item_help = join([func(*args) for func, args in self.items])   File ""/home/marko/miniconda3/envs/flretro/lib/python3.10/argparse.py"", line 540, in _format_action     help_text = self._expand_help(action)   File ""/home/marko/miniconda3/envs/flretro/lib/python3.10/argparse.py"", line 637, in _expand_help     return self._get_help_string(action) % params ValueError: incomplete format ``` **Environment (please complete the following information):**   MegatronLM commit ID: `2ea701c78ed03924069846fcfd445f3415be7b56`   PyTorch version: `2.0.1`   CUDA version: `11.8`   NCCL version `(2, 14, 3)` **Proposed fix* CC(Escape `%` in help message for `rotarypercent`.) ",2023-08-17T10:49:50Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/472,Fixed in https://github.com/NVIDIA/MegatronLM/commit/1aa7144f8946e8b5149db6cc40bfa7224df25c75.
xuwenju123,[BUG] DeadLock when using GQA and Mcore,"**Describe the bug** I run the NeMo code and get the job stuck with pipeline_model_parallel_size > 1 using 8 GPUs. I run the job using the mainline NeMo and Megatronlm code, and enable mcore_gpt: True  num_query_groups: 4 Also install the latest Transformer Engine. and turn off flash attention and fused attention by appending NVTE_FLASH_ATTN=0 NVTE_FUSED_ATTN=0 before the command (python ...py) **To Reproduce** Use the mainline **Expected behavior** A clear and concise description of what you expected to happen. **Stack trace/logs** print out the tensors to be sent and received in forward_backward_pipelining_without_interleaving rank: 0 recv_tensor_shapes: [(2048, 1, 768)] send_tensor_shapes: [(2048, 1, 768)] rank: 1 recv_tensor_shapes: [(2048, 1, 768)] send_tensor_shapes: [(2048, 1, 768)] rank: 0 input_tensor [None] rank: 0 output_tensor[0] torch.Size([128, 1, 768]) input_tensor = recv_forward(recv_tensor_shapes, config) The input_tensor is None?? **Environment (please complete the following information):**   MegatronLM commit ID: 2ea701c78ed03924069846fcfd445f3415be7b56   transformerengine: transformerengine0.12.0.dev0+630a131   PyTorch version:  torch 2.0.0a0+1767026   CUDA version:  12.1   NCCL version: (2, 17, 1) **Proposed fix** If you have a proposal for how to fix the issue state it here or link to a PR. **Additional context** Add any other context about the problem here.",2023-08-16T22:31:20Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/471,Marking as stale. No activity in 60 days.,Hi have you solve this problem?,Marking as stale. No activity in 60 days.
matthew-frank,Add NaN check asserts before scalar all-reduce operations.  This is iâ€¦,â€¦nexpensive but gives much finergrain info when training diverges.,2023-08-16T17:36:59Z,enhancement,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/470,We'll be merging this in internally today and push back out to github. Thanks!
airicairic,[BUG],"**Describe the bug** Datapath set to dataset list now causes error. **To Reproduce** Try to use a list of datasets. **Expected behavior** Trains on multiple datasets according to assigned percentages. **Stack trace/logs** megatron/data/blendable_dataset.py"", line 51, in __init__     desc += dataset.desc + ""\n\n""   File ""/opt/conda/lib/python3.8/sitepackages/torch/utils/data/dataset.py"", line 83, in __getattr__     raise AttributeError",2023-08-16T13:58:48Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/469,Can you post the full commandline that produced this error? A full stack trace would be helpful too.,"Going to close this, please reopen with more details if you are still running into this issue."
cicirori,[BUG] num_layers must be divisible by transformer_pipeline_model_parallel_size * virtual_pipeline_model_parallel_size,"**Describe the bug** num_layers must be divisible by transformer_pipeline_model_parallel_size * virtual_pipeline_model_parallel_size Megatron only did follow checks: ```     if args.num_layers_per_virtual_pipeline_stage is not None:         assert args.pipeline_model_parallel_size > 2, \             'pipelinemodelparallel size should be greater than 2 with ' \             'interleaved schedule'         assert args.num_layers % args.num_layers_per_virtual_pipeline_stage == 0, \             'number of layers is not divisible by number of layers per virtual ' \             'pipeline stage'         args.virtual_pipeline_model_parallel_size = \             (args.num_layers // args.transformer_pipeline_model_parallel_size) // \             args.num_layers_per_virtual_pipeline_stage ``` If I have a mode with 80 layers pp=16 vp=2, it will pass all checks and running successfully. But in this case it's actually running a model with 64 layers. virtual_pipeline_model_parallel_size would be set to 2 in this case. (2.5 without `//` ) After executing the following two pieces of code, there are 2 layers on each device: https://github.com/NVIDIA/MegatronLM/blob/main/megatron/model/transformer.pyL1365 https://github.com/NVIDIA/MegatronLM/blob/main/megatron/model/transformer.pyL1436 So, the actual num_layers would be `2*2*16=64`, not 80. So, additional assertion should be added.",2023-08-16T09:06:43Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/468,"Thank you for this! We have patched this internally, and it should reflect in Github soon."
xu-song,[QUESTION] what is the difference of len(sizes) and len(doc_idx),**what is the difference of `len(sizes)` and `len(doc_idx)`**  https://github.com/NVIDIA/MegatronLM/blob/d898a8991d1a08d29074f87819d1bf41517e35f5/megatron/data/indexed_dataset.pyL371L372 I find that `_doc_count = _len + 1` . What does `_doc_count` do?  The cause is  https://github.com/NVIDIA/MegatronLM/blob/d898a8991d1a08d29074f87819d1bf41517e35f5/megatron/data/indexed_dataset.pyL549,2023-08-16T08:53:44Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/467,Marking as stale. No activity in 60 days.
wuziyou199217,[ENHANCEMENT] P2P overlap for 1F1B pipeline scheduler,"**Is your feature request related to a problem? Please describe.** For now, P2P overlap only supports interleaved pipeline scheduler. However, I've find that interleaved pipeline leads to different checkpoint format, which makes it more difficult to convert the checkpoint format.  **Describe the solution you'd like** Support P2P overlap for 1F1B pipeline scheduler **Describe alternatives you've considered** None **Proposed implementation** None **Additional context** None",2023-08-16T07:48:42Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/466,Closing this since this issue overlaps with CC([QUESTION] Does the functionality of P2P overlap working for 1F1B scheduler?).
wuziyou199217,[QUESTION] Does the functionality of P2P overlap working for 1F1B scheduler?,"**Your question** I've noticed that the functionality of P2P overlap has been implemented for interleaved pipeline scheduler, is there any possibility to port it to 1F1B pipeline scheduler theoretically?",2023-08-16T07:38:30Z,stale,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/465,We don't currently have plans to support this unfortunately. What is your use case?,Marking as stale. No activity in 60 days., Did you implement the overlap?,Marking as stale. No activity in 60 days.
forpanyang,[QUESTION] Do the functionalities of flash attention and selective recomputation overlap?,"**Your question** As the paper Reducing Activation Recomputation in Large Transformer Models described, selective recomputation mainly saved the intermediate activation of attention. It seems the optimization chance vanished if one uses flashattention.",2023-08-16T06:28:49Z,,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/464,~They are orthogonal. FlashAttention reduces the memory footprint of the attention layer by not fully materializing `KQ^T`; selective recomputation also reduces the footprint of the FFN / MLP layers.~,"Closing this, feel free to reopen if you still have questions.","> They are orthogonal. FlashAttention reduces the memory footprint of the attention layer by not fully materializing `KQ^T`; selective recomputation also reduces the footprint of the FFN / MLP layers. but whether i disable selective ckpt or not, its max reserved remains same. but mfu improved a lot when recompute option is set as false. and following the paper, i only notice selective methods checkpoint qk term, its softmax and dropout activations not mlp. so i doubt it does not save anything when flash attention is used..?  ","You are right, updated my original comment. Selective recomputation should not be needed if using Flash Attention."
zhouzaida,Remove unnecessary assign,,2023-08-16T02:58:39Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/463,I don't think this assignment is unnecessary?,The usage of `all_gather` can be found at https://pytorch.org/docs/stable/distributed.htmltorch.distributed.all_gather. The `tensor_list` can be all zero tensor or empty tensor so it is not necessary to get the rank and assign `input_` to that rank.,Marking as stale. No activity in 60 days.
airicairic,[BUG],"**Describe the bug** task/eval_utils.py has an import error from previous version. **To Reproduce** Run a task, e.g. RACE **Expected behavior** Should import successfully **Stack trace/logs** tasks/eval_utils.py"", line 14, in          from megatron.schedules import get_forward_backward_funcfrom megatron.schedules import get_forward_backward_func     ModuleNotFoundErrorModuleNotFoundError    from megatron.schedules import get_forward_backward_func: : from megatron.schedules import get_forward_backward_func     No module named 'megatron.schedules'        No module named 'megatron.schedules' **Proposed fix** Change import to: from megatron.core.pipeline_parallel import get_forward_backward_func",2023-08-15T14:47:29Z,stale,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/462,"megatron/model/multiple_choice.py"", line 42, in __init__     init_method) NameError: name 'init_method' is not defined use init_method_normal or import init_method_normal as init_method",Marking as stale. No activity in 60 days.,Met the same issue when trying to finetune the model. Can be handled by `from megatron.core.pipeline_parallel import get_forward_backward_func` ,Marking as stale. No activity in 60 days.
gshennvm,fix file typo,"fixes a typo in tensor_parallel tests, this should be called `__init__.py`",2023-08-12T19:05:37Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/461,"Thanks for the MR, the tests have moved around so this file no longer exists. "
etoilestar,Star vit,"thank you for your reply, I have simplified my code.",2023-08-11T02:37:53Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/460,"> Hello, this is my first time submitting PR, and I am sorry to bother you. I am to explain why I am to add these codes. 1. In the examples package, pretrain_vit_distribute.sh and run_deepspeed_vit_zero2.sh are the script to train vit. 2. In pretrain_vit.py, I modify the way to load the dataset to fit model parallelism training, and I also modify the way to define the model to use deepspeed. 3. In vit_embedding.py and vit_model.py, I adjusted the code to accommodate pipeline parallelism and tensor parallelism. ","This appears to be for a fork of MegatronLM (looks like MegatronDeepSpeed?), instead of for this repo. "
janEbert,Add RMSNorm,This adds support for RMSNorm even without TransformerEngine by using the existing fused implementation in Apex.,2023-08-10T14:50:04Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/459,Marking as stale. No activity in 60 days.
janEbert,Add T5 SwiGLU,"The standard SwiGLU is incorrectly implemented, making the same mistake as PyTorch's GLU activation function. This keeps backward compatibility by activating the fixed SwiGLU via `t5swiglu`, while disallowing having it enabled together with `swiglu`.",2023-08-10T14:49:23Z,stale,open,0,2,https://github.com/NVIDIA/Megatron-LM/issues/458,"This also deliberately does not change `ffnhiddensize` because it's userunfriendly. If the value is specified, I want _that_ to be used, not some other value.",Marking as stale. No activity in 60 days.
xiaojunjie,[QUESTION] Is there a way to change the weights of datasets in BlendableDataset,"**Your question** Given a BlendableDataset of dataset A and B with weights 1:1 After training N iterationsï¼Œchange weights to 2:1 and continue from saved checkpoint According to BlendableDataset,  it will loss some samples of A, and repeat some samples of B Is there a good way to avoid data loss and duplicationï¼Ÿ",2023-08-10T07:26:21Z,stale,closed,1,8,https://github.com/NVIDIA/Megatron-LM/issues/457,Marking as stale. No activity in 60 days.,This feature would indeed be very useful if this was implemented.,Marking as stale. No activity in 60 days.," Hi, do you solve this issue?",This isn't currently supported in MegatronLM. This PR here adds support for this.,Marking as stale. No activity in 60 days.,Libero ,Marking as stale. No activity in 60 days.
Vic0428,[QUESTION] Is there any code example to use grad_sync_func to overlap data parallel reduce-scatter?,"Hi all, Is there any code example to use `grad_sync_func` to overlap data parallel reducescatter?  It seems that the `grad_sync_func` is not being used in `training.py`. I am trying to understand how much reducescatter can be overlapped in the backward pass :) !7c197ebffcee40f2af625098366681b6 Have a nice day! ~Vic",2023-08-09T23:31:51Z,stale,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/456,"> Hi all, >  > Is there any code example to use `grad_sync_func` to overlap data parallel reducescatter? It seems that the `grad_sync_func` is not being used in `training.py`. I am trying to understand how much reducescatter can be overlapped in the backward pass :) >  > !7c197ebffcee40f2af625098366681b6 >  > Have a nice day! ~Vic You may look into this one: https://github.com/alibaba/MegatronLLaMA/blob/main/megatron/optimizer/overlapped_dist_optimizer.py",This is now supported using the `overlapgradreduce` commandline argument: https://github.com/NVIDIA/MegatronLM/commit/e801b2bb2f24ad12b2b21efb0c07a717d82335d0.,It is called here: https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/pipeline_parallel/schedules.pyL327.,Marking as stale. No activity in 60 days.
salomonj11,Fix Spelling Errors On README.md,,2023-08-09T18:00:31Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/455,Marking as stale. No activity in 60 days.
XuehaiPan,Refactor imports for better IDE integration,"This PR changes the import statements in `megatron.core` submodule for better IDE integration. In the old implementation, the statement `import megatron.core.xxx` sets submodule object `xxx` to the partially initialized module `megatron.core`. Like: ```python setattr(megatron.core, 'xxx', ) ``` Then in the `__init__.py` file's `globals()`, it can access the `xxx` submodule via a variable name `xxx` (which is set in `megatron.core.__dict__`). The old implementation just works but not follows the Python best practice. It relies on the implementation details of the Python import statement. In this PR, we use explicitly `fromimport` statement to explicitly set the member to the global namespace. ```python from megatron.core import xxx ``` This benefits better readability and better IDE and linter integration.  Before:   After:  ",2023-08-09T17:31:15Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/454,Marking as stale. No activity in 60 days.
janEbert,[BUG] RoPE ignores position IDs,"**Describe the bug** If using `resetpositionids`, the RoPE implementation does not take this into account; it will still use the embeddings from position 0 to sequence length  1. **To Reproduce** As you can see in `language_model.py`, the generated rotary embedding tensor does not take the `enc_position_ids` into account. **Expected behavior** From the rotary embedding tensor, the indices in the sequence dimension for the given `enc_position_ids` should be taken instead. For `enc_position_ids = torch.arange(0, seq_len)`, this would return the same (correct) values as it does now. However, then it would also handle other arbitrary position IDs. It's also important that this handles `dec_position_ids` separately. **Proposed fix**  Most simply, this would be implemented in the forward pass of `RotaryEmbedding` by taking position IDs instead of a sequence length as its only argument. Again, it's also really important to create a new rotary embedding tensor for the decoder part of a encoderdecoder transformer using `dec_input_ids`.",2023-08-09T13:05:21Z,stale,open,0,5,https://github.com/NVIDIA/Megatron-LM/issues/453,Marking as stale. No activity in 60 days.,"This is still present in the current MCore version of the `GPTModel` (and probably others), it should not have been closed.",Let me check with the team and get back. Thanks for the comment. will reopen,Cheers!,Marking as stale. No activity in 60 days.
AlvL1225,Data is too large to be stored in only one file(preprocess_data.py)[QUESTION],"**Your question** Hi, the doc says 'preprocess_data.py' will generate 2 files(bin and index).But for over 1T tokens dataset, the file is too large to save. Can the 'preprocess_data.py' generate several data shards?",2023-08-09T03:34:34Z,,closed,0,8,https://github.com/NVIDIA/Megatron-LM/issues/452,"The way I've done it is to manually split the JSON file into shards, use `preprocess_data.py` to process each shard, then merge using `merge_datasets.py`.","Yes,  's workflow is the best approach.","> Yes,  's workflow is the best approach. Hi barker , I tried this but merge_dataset.py will still create a very huge file. Is it possible to only create a merged index but keep bin files sharded?","> The way I've done it is to manually split the JSON file into shards, use `preprocess_data.py` to process each shard, then merge using `merge_datasets.py`. Hi  , will this workflow still create a huge bin file? What I am worrying is that one single merged huge file(probably larger than 2TB) may cause storage problem in some storage systems.","You're right, sorry; the thing I wrote doesn't solve your issue.","> You're right, sorry; the thing I wrote doesn't solve your issue. Thank you for your advice! I still have another question. For a common file system(like NFS or XFS), this may not be a problem I guess? Did everyone use one single file(TB level) when using MegatronLM? If so, my worrying might be unnecessary.","I work on a file system on which single, very large files are highly preferred (it's a GPFS). I am not familiar enough with other file systems' intricacies to make any statements, sorry. :)","> I work on a file system on which single, very large files are highly preferred (it's a GPFS). I am not familiar enough with other file systems' intricacies to make any statements, sorry. :) It's also very helpful to know that, thanks!"
chenglong0313,[QUESTION] Can I use the pytorch_model.bin format file in huggingface as pre-training initialization weights?,Can I use the pytorch_model.bin format file in huggingface as pretraining initialization weights? like thisï¼š !11,2023-08-08T09:25:27Z,question,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/451,"Huggingface Accelerate offers some interoperability with Megatron, see here: https://huggingface.co/docs/accelerate/usage_guides/megatron_lm Linked from that page is a conversion script for MegatronLM GPT models to Huggingface. https://github.com/huggingface/transformers/blob/main/src/transformers/models/megatron_gpt2/checkpoint_reshaping_and_interoperability.py This functionality is provided by Huggingface, not NVIDIA/MegatronLM so it would be best to ask them for further support. Hope that helps."
sedrick-keh-tri,"[QUESTION] torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1164, internal error - please report this issue to the NCCL developers, NCCL version 2.18.1","**Your question** I am trying to run Megatron multinode on Docker. I have a docker set up on both nodes. Specifically, I run the following in each node:  ```bash docker run gpus all shmsize=1g ipc=host network=host env NCCL_DEBUG=INFO env NCCL_SOCKET_IFNAME=^docker0,lo it rm v /home/${USER}:/workspace nvcr.io/nvidia/pytorch:23.06py3 ``` In Node 1, I run `examples/pretrain_gpt_distributed_with_mp.sh` with the following hyperparameters:  ```bash MASTER_ADDR="""" MASTER_PORT=6000 NNODES=2 NODE_RANK=0 WORLD_SIZE=$(($GPUS_PER_NODE*$NNODES)) ``` In Node 2, I run a similar script with the following hyperprameters: (basically the same except for `NODE_RANK`) ```bash MASTER_ADDR="""" MASTER_PORT=6000 NNODES=2 NODE_RANK=1 WORLD_SIZE=$(($GPUS_PER_NODE*$NNODES)) ``` Here, """" is the IP address of the original node (before calling `docker run`)  The scripts above work fine outside of Docker, but when I run it inside Docker, I get the following error: ``` :462:635 [3] proxy.cc:1484 NCCL WARN [Service thread] Error encountered progressing operation=Connect, res=2, closing connection :464:637 [5] NCCL INFO misc/socket.cc:583 > 2 :462:635 [3] proxy.cc:1518 NCCL WARN [Proxy Service 11] Failed to execute operation Connect from rank 11, retcode 2 :464:637 [5] NCCL INFO transport/net_socket.cc:336 > 2 :464:637 [5] NCCL INFO transport/net.cc:592 > 2 :464:637 [5] NCCL INFO proxy.cc:1306 > 2 :464:637 [5] proxy.cc:1484 NCCL WARN [Service thread] Error encountered progressing operation=Connect, res=2, closing connection :464:637 [5] proxy.cc:1518 NCCL WARN [Proxy Service 13] Failed to execute operation Connect from rank 13, retcode 2 :462:633 [3] misc/socket.cc:49 NCCL WARN socketProgress: Connection closed by remote peer .cm.cluster :462:633 [3] NCCL INFO misc/socket.cc:746 > 6 :464:628 [5] misc/socket.cc:49 NCCL WARN socketProgress: Connection closed by remote peer .cm.cluster :462:633 [3] proxy.cc:1143 NCCL WARN Socket recv failed while polling for opId=0x7ffeb707dab0 :464:628 [5] NCCL INFO misc/socket.cc:746 > 6 :462:633 [3] NCCL INFO transport/net.cc:288 > 3 Traceback (most recent call last):   File ""/workspace/MegatronLM/pretrain_gpt.py"", line 119, in  Traceback (most recent call last):   File ""/workspace/MegatronLM/pretrain_gpt.py"", line 119, in      pretrain(train_valid_test_datasets_provider,   File ""/workspace/MegatronLM/megatron/training.py"", line 90, in pretrain     pretrain(train_valid_test_datasets_provider,   File ""/workspace/MegatronLM/megatron/training.py"", line 90, in pretrain     initialize_megatron(extra_args_provider=extra_args_provider,   File ""/workspace/MegatronLM/megatron/initialize.py"", line 86, in initialize_megatron     initialize_megatron(extra_args_provider=extra_args_provider,   File ""/workspace/MegatronLM/megatron/initialize.py"", line 86, in initialize_megatron     _compile_dependencies()   File ""/workspace/MegatronLM/megatron/initialize.py"", line 150, in _compile_dependencies     _compile_dependencies()   File ""/workspace/MegatronLM/megatron/initialize.py"", line 150, in _compile_dependencies     torch.distributed.barrier()   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/distributed_c10d.py"", line 145, in wrapper     return func(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/distributed_c10d.py"", line 3553, in barrier     torch.distributed.barrier()   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/distributed_c10d.py"", line 145, in wrapper     return func(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/distributed/distributed_c10d.py"", line 3553, in barrier     work = default_pg.barrier(opts=opts) torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1164, internal error  please report this issue to the NCCL developers, NCCL version 2.18.1 ncclInternalError: Internal check failed. Last error: Socket recv failed while polling for opId=0x7ffeb708d1f0     work = default_pg.barrier(opts=opts) ```",2023-08-07T22:47:13Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/450,"Looks like this is a NCCL setup issue, can I close this?"
SefaZeng,[QUESTION] Training GPT2 on V100 is very slow.,**Your question** I try to train a model with 600M parameters with 250k vocab size. So the model configuration is the same as the pure English model with about 300M parameters. I train the model with 64(8*8) 32GB V100 and the global_batch_size is 256 with seq_length 2048 which means about 0.5M tokens in a iteration. I find the iteration time is 4.3s per iteration. Is this time for per iteration good or it's slow? If the calculation is correct: `1e12 / 500000 * 4.3 / 3600 / 24  = 99.53` Does it mean I need 99 days to train a model with only 600M parameters?  The training log is as follows: ```  iteration      100/  500000  ```,2023-08-07T09:18:47Z,question stale,closed,0,6,https://github.com/NVIDIA/Megatron-LM/issues/449,"The following are the arguments: ```  arguments    accumulate_allreduce_grads_in_fp32 .............. False   adam_beta1 ...................................... 0.9   adam_beta2 ...................................... 0.95   adam_eps ........................................ 1e08   add_bias_linear ................................. True   add_position_embedding .......................... True   adlr_autoresume ................................. False   adlr_autoresume_interval ........................ 1000   apply_layernorm_1p .............................. False   apply_query_key_layer_scaling ................... True   apply_residual_connection_post_layernorm ........ False   async_tensor_model_parallel_allreduce ........... False   attention_dropout ............................... 0.1   attention_softmax_in_fp32 ....................... False   barrier_with_L1_time ............................ True   bert_binary_head ................................ True   bert_embedder_type .............................. megatron   bert_load ....................................... None   bf16 ............................................ False   bias_dropout_fusion ............................. True   bias_gelu_fusion ................................ True   biencoder_projection_dim ........................ 0   biencoder_shared_query_context_model ............ False   block_data_path ................................. None   classes_fraction ................................ 1.0   clip_grad ....................................... 1.0   consumed_train_samples .......................... 0   consumed_valid_samples .......................... 0   data_cache_path ................................. None   data_impl ....................................... mmap   data_parallel_random_init ....................... False   data_parallel_size .............................. 16   data_path ....................................... ['1', '/pile/megatron_bin/pile_00_text_document', '1', '/pile/megatron_bin/pile_01_text_document', '1', '/pile/megatron_bin/pile_02_text_document', '1', '/pile/megatron_bin/pile_03_text_document', '1', '/pile/megatron_bin/pile_04_text_document', '1', '/pile/megatron_bin/pile_05_text_document', '1', '/pile/megatron_bin/pile_07_text_document', '1']   data_per_class_fraction ......................... 1.0   data_sharding ................................... True   dataloader_type ................................. single   DDP_impl ........................................ local   decoder_num_layers .............................. None   decoder_seq_length .............................. None   dino_bottleneck_size ............................ 256   dino_freeze_last_layer .......................... 1   dino_head_hidden_size ........................... 2048   dino_local_crops_number ......................... 10   dino_local_img_size ............................. 96   dino_norm_last_layer ............................ False   dino_teacher_temp ............................... 0.07   dino_warmup_teacher_temp ........................ 0.04   dino_warmup_teacher_temp_epochs ................. 30   distribute_saved_activations .................... False   distributed_backend ............................. nccl   distributed_timeout_minutes ..................... 100   embedding_path .................................. None   embedding_weights_in_fp32 ....................... False   empty_unused_memory_level ....................... 0   encoder_num_layers .............................. 24   encoder_seq_length .............................. 2048   end_weight_decay ................................ 0.1   eod_mask_loss ................................... False   eval_interval ................................... 1000   eval_iters ...................................... 10   evidence_data_path .............................. None   exit_duration_in_mins ........................... None   exit_interval ................................... None   exit_on_missing_checkpoint ...................... False   exit_signal_handler ............................. False   ffn_hidden_size ................................. 4096   finetune ........................................ False   fp16 ............................................ True   fp16_lm_cross_entropy ........................... False   fp32_residual_connection ........................ False   fp8_amax_compute_algo ........................... most_recent   fp8_amax_history_len ............................ 1   fp8_e4m3 ........................................ False   fp8_hybrid ...................................... False   fp8_interval .................................... 1   fp8_margin ...................................... 0   fp8_wgrad ....................................... True   global_batch_size ............................... 256   gradient_accumulation_fusion .................... True   group_query_attention ........................... False   head_lr_mult .................................... 1.0   hidden_dropout .................................. 0.1   hidden_size ..................................... 1024   hysteresis ...................................... 2   ict_head_size ................................... None   ict_load ........................................ None   img_h ........................................... 224   img_w ........................................... 224   indexer_batch_size .............................. 128   indexer_log_interval ............................ 1000   inference_batch_times_seqlen_threshold .......... 512   init_method_std ................................. 0.006   init_method_xavier_uniform ...................... False   initial_loss_scale .............................. 4294967296   iter_per_epoch .................................. 1250   kv_channels ..................................... 64   layernorm_epsilon ............................... 1e05   lazy_mpu_init ................................... None   load ............................................ /MegatronLM/checkpoints/baseline   local_rank ...................................... None   log_batch_size_to_tensorboard ................... False   log_interval .................................... 100   log_learning_rate_to_tensorboard ................ True   log_loss_scale_to_tensorboard ................... True   log_memory_to_tensorboard ....................... False   log_num_zeros_in_grad ........................... False   log_params_norm ................................. False   log_timers_to_tensorboard ....................... False   log_validation_ppl_to_tensorboard ............... False   log_world_size_to_tensorboard ................... False   loss_scale ...................................... None   loss_scale_window ............................... 1000   lr .............................................. 0.0003   lr_decay_iters .................................. 320000   lr_decay_samples ................................ None   lr_decay_style .................................. cosine   lr_warmup_fraction .............................. None   lr_warmup_iters ................................. 750   lr_warmup_samples ............................... 0   make_vocab_size_divisible_by .................... 128   mask_factor ..................................... 1.0   mask_prob ....................................... 0.15   mask_type ....................................... random   masked_softmax_fusion ........................... True   master_addr ..................................... 11.214.159.213   master_port ..................................... 32307   max_position_embeddings ......................... 2048   max_tokens_to_oom ............................... 12000   merge_file ...................................... None   micro_batch_size ................................ 4   min_loss_scale .................................. 1.0   min_lr .......................................... 3e05   mmap_warmup ..................................... False   no_load_optim ................................... None   no_load_rng ..................................... None   no_persist_layer_norm ........................... False   no_save_optim ................................... None   no_save_rng ..................................... None   num_attention_heads ............................. 16   num_channels .................................... 3   num_classes ..................................... 1000   num_experts ..................................... None   num_layers ...................................... 24   num_layers_per_virtual_pipeline_stage ........... None   num_query_groups ................................ 1   num_workers ..................................... 2   onnx_safe ....................................... None   openai_gelu ..................................... False   optimizer ....................................... adam   output_bert_embeddings .......................... False   overlap_p2p_comm ................................ False   override_opt_param_scheduler .................... False   params_dtype .................................... torch.float16   patch_dim ....................................... 16   perform_initialization .......................... True   pipeline_model_parallel_size .................... 2   pipeline_model_parallel_split_rank .............. None   position_embedding_type ......................... rope   profile ......................................... False   profile_ranks ................................... [0]   profile_step_end ................................ 12   profile_step_start .............................. 10   query_in_block_prob ............................. 0.1   rampup_batch_size ............................... None   rank ............................................ 0   recompute_granularity ........................... None   recompute_method ................................ None   recompute_num_layers ............................ 1   reset_attention_mask ............................ False   reset_position_ids .............................. False   retriever_report_topk_accuracies ................ []   retriever_score_scaling ......................... False   retriever_seq_length ............................ 256   retro_add_retriever ............................. False   retro_cyclic_train_iters ........................ None   retro_encoder_attention_dropout ................. 0.1   retro_encoder_hidden_dropout .................... 0.1   retro_encoder_layers ............................ 2   retro_num_neighbors ............................. 2   retro_num_retrieved_chunks ...................... 2   retro_return_doc_ids ............................ False   retro_workdir ................................... None   rotary_percent .................................. 1.0   sample_rate ..................................... 1.0   save ............................................ /MegatronLM/checkpoints/baseline   save_interval ................................... 10000   scatter_gather_tensors_in_pipeline .............. True   seed ............................................ 1234   seq_length ...................................... 2048   sequence_parallel ............................... True   sgd_momentum .................................... 0.9   short_seq_prob .................................. 0.1   skip_train ...................................... False   split ........................................... 949,50,1   squared_relu .................................... False   standalone_embedding_stage ...................... False   start_weight_decay .............................. 0.1   swiglu .......................................... False   swin_backbone_type .............................. tiny   tensor_model_parallel_size ...................... 2   tensorboard_dir ................................. None   tensorboard_log_interval ........................ 1   tensorboard_queue_size .......................... 1000   test_data_path .................................. None   timing_log_level ................................ 0   timing_log_option ............................... minmax   titles_data_path ................................ None   tokenizer_model ................................. /MegatronLM/../mt5/spiece.model   tokenizer_type .................................. MT5Tokenizer   train_data_path ................................. None   train_iters ..................................... 500000   train_samples ................................... None   transformer_impl ................................ local   transformer_pipeline_model_parallel_size ........ 2   untie_embeddings_and_output_weights ............. False   use_checkpoint_args ............................. False   use_checkpoint_opt_param_scheduler .............. False   use_contiguous_buffers_in_local_ddp ............. True   use_cpu_initialization .......................... None   use_distributed_optimizer ....................... False   use_flash_attn .................................. False   use_one_sent_docs ............................... False   use_ring_exchange_p2p ........................... False   use_rotary_position_embeddings .................. True   valid_data_path ................................. None   variable_seq_lengths ............................ False   virtual_pipeline_model_parallel_size ............ None   vision_backbone_type ............................ vit   vision_pretraining .............................. False   vision_pretraining_type ......................... classify   vocab_extra_ids ................................. 0   vocab_file ...................................... /mt5/vocab.txt   vocab_size ...................................... None   weight_decay .................................... 0.1   weight_decay_incr_style ......................... constant   world_size ...................................... 64  end of arguments  ```","My initial reaction is that this _might_ be a reasonable per iteration time for V100 since on A100 it's typical to see per iteration times around 1s. I don't have V100 I can test on unfortunately, but I'll try to replicate the configuration with A100 and let you know what I see.","> My initial reaction is that this _might_ be a reasonable per iteration time for V100 since on A100 it's typical to see per iteration times around 1s. I don't have V100 I can test on unfortunately, but I'll try to replicate the configuration with A100 and let you know what I see. Thank you for your reply! Does this mean training a GPT2 with 350M parameters through 1 trillion tokens(which is a standard config for nowadays LLMs) on 64 32G V100 needs about 90 days? That's a  bit of a shock to me...",Another problem is why the memory usage is a bit low while each dataset is 40G and there are 30 shards of data. But the memory usage for each machine is only 16~20 G.,"I noticed you are using `tensor_model_parallel_size=2` and `pipeline_model_parallel_size=2`. You shouldn't need these for the scale of model you are training. I also used the formula in https://arxiv.org/pdf/2104.04473.pdf to estimate the throughput you are observing. It seems to be about 72 * 256 * 2048 * 24 * 1024 * 1024 / (4.3 * 64) = 3.5 Teraflop/s/GPU, which is a very small fraction of peak V100 device throughput (130 Teraflop/s). Something seems wrong here; the first thing I would try is reducing `tensor_model_parallel_size` and `pipeline_model_parallel_size` to 1.",Marking as stale. No activity in 60 days.
SefaZeng,[QUESTION] How to train the model with sharded dataset?,"**Your question** I want to train a model with the Pile dataset, but the size of this dataset is about 1TB. And the memory of my V100 machine is only 300GB. I want to shard the dataset into multiple ones like pile_00, pile_01... And I want to load a single one into the memory every time. How can I do that? Is there any examples?",2023-08-06T15:01:31Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/448
mayank31398,fix dataset preprocessor,This PR fixes the issue https://github.com/NVIDIA/MegatronLM/issues/431.,2023-08-05T05:41:05Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/447,"Merged: https://github.com/NVIDIA/MegatronLM/commit/0609f27fe8376f17ab65c001d3d8f35cd8175950, thanks!"
KlaudiaTH,Megatron lmeval server,,2023-08-03T08:40:30Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/446
robin087,[QUESTION] The implementation of tensor_parallel.cross_entropy when using tensor parallel,"**Your question** When using tensor parallel, we find that the cross_entropy loss computed by tensor_parallel.cross_entropy is different from torch.nn.functional.cross_entropy. The difference is about 1% for our data. For the implementation of tensor_parallel.cross_entropy, we find that the loss is computed on the partition_vocab_size which is 8 times smaller than vocab_size (tp_size = 8). We think maybe this implementation causes the difference above. In this case, is this implementation correct? Or can this implementation ensure the performance when using tensor parallel?",2023-08-03T07:25:08Z,stale,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/445,"Hi, we are recently facing the similar problem of numerical differences between megatron and torch,  and thus I'm wondering have you compared your input Tensor of both cross_entropy in your case, is there any difference between the inputs? or, the layers or operations much before? The way we compare two Tensors is `(TensorA  TensorB).abs().sum()`", when you see these numerical differences between `torch.nn.functional.cross_entropy` and ` tensor_parallel.cross_entropy` are you using ColumnParallelLinear as inputs for both or do you use `torch.nn.Linear` with `torch.nn.functional.cross_entropy`?,"I'm not sure how this is done in MegatronLM, so this may be irrelevant, but if TP changes the vocabulary padding then this may cause some differences.",Marking as stale. No activity in 60 days.
SefaZeng,[QUESTION] Data preprocess raise OOM error with 250G memory.,**Your question** I try to process the Pile dataset with only one part(00.jsonl from EleutherAI) which is 43G size. The memory used is 250G and 80 CPU core. The script is as follows: ``` INPUT=$WORK_PATH/00.jsonl OUTPUT=$WORK_PATH/megatron_bin/pile_00 TOKENIZER=HFTokenizer VOCAB=$WORK_PATH/mt5 ${python} $WORK_PATH/tools/preprocess_data.py \        input ${INPUT} \        outputprefix ${OUTPUT} \        tokenizermodel $VOCAB \        tokenizertype $TOKENIZER \        workers 80 \                                                                                                                                                                                                                                                           partitions 4 ``` Is there any way to reduce the memory consumption and speed it up?,2023-08-03T03:15:23Z,,closed,2,0,https://github.com/NVIDIA/Megatron-LM/issues/444
baoleai,[QUESTION] Why is additional allreduce necessary for layernorm in sequence parallelism scenarios? ,"**Your question** I noticed that in the optimizer , there is an `allreduce_layernorm_grads` function when sequence parallelism is used. However, I could not find any explanation for this in the paper or other related issues.",2023-08-02T02:11:42Z,stale,closed,0,6,https://github.com/NVIDIA/Megatron-LM/issues/443,"section 4.2.2 from Reducing Activation Recomputationin Large Transformer Models. In sequence parallellism, layernorm is split by sequence in tp groups.","According to section 4.2.2 of the paper Reducing Activation Recomputation in Large Transformer Models, when using tensor parallelism (TP) with sequence parallelism (SP), the only communication operations required are allgather and reducescatter. However, it is unclear if there are any further optimizations that can be made.","> According to section 4.2.2 of the paper Reducing Activation Recomputation in Large Transformer Models, when using tensor parallelism (TP) with sequence parallelism (SP), the only communication operations required are allgather and reducescatter. However, it is unclear if there are any further optimizations that can be made. Since it's split by sequence, I suppose you could think of it as parallelizing the ""data""(sequence)  within a tensor parallel group.","> > According to section 4.2.2 of the paper Reducing Activation Recomputation in Large Transformer Models, when using tensor parallelism (TP) with sequence parallelism (SP), the only communication operations required are allgather and reducescatter. However, it is unclear if there are any further optimizations that can be made. >  > Since it's split by sequence, I suppose you could think of it as parallelizing the ""data""(sequence) within a tensor parallel group. Can I understand it this way? Assuming world_size=2, seq_length=2048, when the sequence is passed to LayerNorm, the first 1024 tokens and the last 1024 tokens of the sequence will be calculated using different LayerNorms (different weights and bias). If I want to make convert the parallelized weights into one that can run on single GPU, how should I deal with these splitLayerNorm?","  I think the weights in LayerNorm are not split, so you can choose just one of them.",Marking as stale. No activity in 60 days.
Marks101,Bug fix: Use amax_reduction_group instead of data_parallel_group as fp8_group,"All our FP8 trainings of a 40B gpt3 model with tensor_model_parallel_size=8 have been diverging after around 1000 iterations. Comparing our code with NeMo, we figured out that the major difference is the reduction group for the amax history (see https://github.com/NVIDIA/NeMo/blob/7f4e8a50e80c5582e551ab98a60fccd4cc21fa7a/nemo/collections/nlp/modules/common/megatron/transformer.pyL1466). Fixing this did the job for us: everything stable now.",2023-08-01T06:12:21Z,,closed,3,1,https://github.com/NVIDIA/Megatron-LM/issues/442,"Fixed internally, thanks for the MR!"
liu-zichen,[QUESTION] Why does each pipeline layer initialize and read datasets separately?,"Currently each pipeline layer (i.e. tensor model parallel group) initializes and reads datasets separately. However, I think that all ranks in the model parallel group ($tp \times pp$) should view the same inputs. If my understanding above is correct, the correctness of the model training now relies on the first pipeline and the last pipeline reading the same data without any random factor. But why not read and broadcast data in model parallel group? Each rank0 in tensor model parallel group would build datasets: https://github.com/NVIDIA/MegatronLM/blob/65da5be50cdaad2205e0bc96295bcaaf2c2c2a34/megatron/training.pyL982 The read data is broadcast in tensor model parallel group: https://github.com/NVIDIA/MegatronLM/blob/65da5be50cdaad2205e0bc96295bcaaf2c2c2a34/pretrain_gpt.pyL49",2023-08-01T02:47:44Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/441,"Yup, this could certainly be done.",Marking as stale. No activity in 60 days.
janEbert,Use enum for position embedding type,"Improve maintainability and extensibility. I couldn't figure out how to make this backwardcompatible without slightly convoluted workarounds (such as `x if isinstance(x, str) else x.value`). Since you haven't tagged a release with the stringbased version, breaking BC may be fine (?).",2023-07-31T09:10:30Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/440,Marking as stale. No activity in 60 days.
1926627357,"[QUESTION] the structure of gpt is ""encoder-only"" in megatron","MegatronLM provides code file of gpt in gpt_model.py. It builds gpt model by `get_language_model` function while setting `add_encoder=True` and `add_decoder=False` as default. Actually, gpt model is decoderonly on the contrary. Therefore, I'm very confused now whether gpt model provided by megatron is correct, or some infomation is overlooked by myself.  :(",2023-07-29T01:20:06Z,,closed,4,3,https://github.com/NVIDIA/Megatron-LM/issues/439,"Similar to the PyTorch `torch.nn.TransformerDecoder` module, here, ""decoder"" implies usage of crossattention between the encoder outputs and decoder intermediary outputs. Of course, this only matters if you have both an encoder and a decoder (i.e., you are using an encoderdecoder Transformer). Now going away from encoderdecoder Transformers, whether you're considering an encoderonly or a decoderonly architecture, they are actually completely the same; only the attention mask is different: full for encoderonly, causal for decoderonly. So whether you use Megatron's `ParallelTransformerLayer([...], add_encoder=True, add_decoder=False)` or PyTorch's `torch.nn.TransformerEncoder`, these are the only ways you can obtain a decoderonly model. Again, this is due to the expectations that the ""decoder"" version of the layer/module has: it expects to receive an encoder's output to perform crossattention on.",I see! Thanks!,"I believe this is a very common confusion about the naming of these models. I really think people should come up with more descriptive names. Nevertheless, here is a summary of what the terms really mean in different context: Original transformer paper (_Attention is all you need_):  Encoder: no cross attention, no causal masks.  Decoder: with cross attention, with causal masks. ""Encoderonly"" vs ""decoderonly"" models:  ""Encoder"": no causal masks.  ""Decoder"": with causal masks.  ""Only"": no cross attention. PyTorch `TransformerEncoderLayer`, `TransformerDecoderLayer`:  Encoder: no cross attention, optional causal masks.  Decoder: with cross attention, optional causal masks. So to create a GPTlike ""decoderonly"" model using PyTorch's modules or libraries with similar naming conventions, one needs to use the encoder modules instead of the decoder modules, and set up the causal masks to make it ""decoderonly"". "
zhangsheng377,fix the getattr use,"if key is not in args, the getattr will throw expection.",2023-07-28T09:11:56Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/438
MoFHeka,[BUG] Fail to apply GQA when using pipeline parallel.,"**Describe the bug** view op in attention calculation error when enable groups_query_attention and numquerygroups=2 and pipelinemodelparallelsize=2 in one A100 machine **To Reproduce** any GPT model **Expected behavior** GQA work fine when pipeline parallel **Stack trace/logs** ```shell   File ""/usr/local/lib/python3.10/distpackages/megatron/model/transformer.py"", line 750, in forward                                                                                      â”‚     query_layer = query_layer.view(query_layer.size(0), query_layer.size(1), 1, self.hidden_size_per_attention_head)                                                                     â”‚ RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.  ``` **Environment (please complete the following information):**   MegatronLM commit ID   PyTorch version 2.1   CUDA version 12.1   NCCL version ",2023-07-28T07:48:33Z,stale,closed,0,5,https://github.com/NVIDIA/Megatron-LM/issues/437,Same issue. Just fix it with `reshape` method?,Same question.,Marking as stale. No activity in 60 days.,"This is a bug, I believe. I think `reshape()` should be a reasonable workaround for now.",Marking as stale. No activity in 60 days.
SefaZeng,[QUESTION] Unable to find address for: bond1,"**Your question** Ask a clear and concise question about MegatronLM. I try to run `pretrain_gpt_distributed_with_mp.sh` with 2 nodes and 8 GPUS_PER_NODE, but there is an error about `RuntimeError: [enforce fail at /root/pytorch/third_party/gloo/gloo/transport/tcp/device.cc:83] ifa != nullptr. Unable to find address for: bond1`. The full log: ``` WARNING:torch.distributed.run: ***************************************** Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.  ***************************************** /usr/local/python/lib/python3.8/sitepackages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension:    warn(f""Failed to load image Python extension: {e}"") /usr/local/python/lib/python3.8/sitepackages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension:    warn(f""Failed to load image Python extension: {e}"") /usr/local/python/lib/python3.8/sitepackages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension:    warn(f""Failed to load image Python extension: {e}"") /usr/local/python/lib/python3.8/sitepackages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension:    warn(f""Failed to load image Python extension: {e}"") /usr/local/python/lib/python3.8/sitepackages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension:    warn(f""Failed to load image Python extension: {e}"") /usr/local/python/lib/python3.8/sitepackages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension:    warn(f""Failed to load image Python extension: {e}"") /usr/local/python/lib/python3.8/sitepackages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension:    warn(f""Failed to load image Python extension: {e}"") /usr/local/python/lib/python3.8/sitepackages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension:    warn(f""Failed to load image Python extension: {e}"") You are using the legacy behaviour of the . This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565 You are using the legacy behaviour of the . This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565 You are using the legacy behaviour of the . This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565 You are using the legacy behaviour of the . This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565 You are using the legacy behaviour of the . This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565 You are using the legacy behaviour of the . This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565 You are using the legacy behaviour of the . This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565 You are using the legacy behaviour of the . This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565 /usr/local/python/lib/python3.8/sitepackages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.   warnings.warn( /usr/local/python/lib/python3.8/sitepackages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.   warnings.warn( /usr/local/python/lib/python3.8/sitepackages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.   warnings.warn( /usr/local/python/lib/python3.8/sitepackages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.   warnings.warn( /usr/local/python/lib/python3.8/sitepackages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.   warnings.warn( /usr/local/python/lib/python3.8/sitepackages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.   warnings.warn( /usr/local/python/lib/python3.8/sitepackages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.   warnings.warn( /usr/local/python/lib/python3.8/sitepackages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.   warnings.warn( Traceback (most recent call last):   File ""/workspace/MegatronLM/pretrain_gpt.py"", line 119, in      pretrain(train_valid_test_datasets_provider,   File ""/workspace/MegatronLM/megatron/training.py"", line 90, in pretrain     initialize_megatron(extra_args_provider=extra_args_provider,   File ""/workspace/MegatronLM/megatron/initialize.py"", line 80, in initialize_megatron     finish_mpu_init()   File ""/workspace/MegatronLM/megatron/initialize.py"", line 60, in finish_mpu_init     _initialize_distributed()   File ""/workspace/MegatronLM/megatron/initialize.py"", line 209, in _initialize_distributed     mpu.initialize_model_parallel(   File ""/workspace/MegatronLM/megatron/core/parallel_state.py"", line 169, in initialize_model_parallel     group_gloo = torch.distributed.new_group(ranks, backend=""gloo"")   File ""/usr/local/python/lib/python3.8/sitepackages/torch/distributed/distributed_c10d.py"", line 2966, in new_group     pg = _new_process_group_helper(   File ""/usr/local/python/lib/python3.8/sitepackages/torch/distributed/distributed_c10d.py"", line 703, in _new_process_group_helper     pg = ProcessGroupGloo(prefix_store, rank, world_size, timeout=timeout) RuntimeError: [enforce fail at /root/pytorch/third_party/gloo/gloo/transport/tcp/device.cc:83] ifa != nullptr. Unable to find address for: bond1 Traceback (most recent call last):   File ""/workspace/MegatronLM/pretrain_gpt.py"", line 119, in      pretrain(train_valid_test_datasets_provider,   File ""/workspace/MegatronLM/megatron/training.py"", line 90, in pretrain Traceback (most recent call last):   File ""/workspace/MegatronLM/pretrain_gpt.py"", line 119, in  Traceback (most recent call last):   File ""/workspace/MegatronLM/pretrain_gpt.py"", line 119, in      initialize_megatron(extra_args_provider=extra_args_provider,   File ""/workspace/MegatronLM/megatron/initialize.py"", line 80, in initialize_megatron     pretrain(train_valid_test_datasets_provider,   File ""/workspace/MegatronLM/megatron/training.py"", line 90, in pretrain     pretrain(train_valid_test_datasets_provider,   File ""/workspace/MegatronLM/megatron/training.py"", line 90, in pretrain     finish_mpu_init()   File ""/workspace/MegatronLM/megatron/initialize.py"", line 60, in finish_mpu_init     initialize_megatron(extra_args_provider=extra_args_provider,   File ""/workspace/MegatronLM/megatron/initialize.py"", line 80, in initialize_megatron     initialize_megatron(extra_args_provider=extra_args_provider,   File ""/workspace/MegatronLM/megatron/initialize.py"", line 80, in initialize_megatron     _initialize_distributed()   File ""/workspace/MegatronLM/megatron/initialize.py"", line 209, in _initialize_distributed     finish_mpu_init()   File ""/workspace/MegatronLM/megatron/initialize.py"", line 60, in finish_mpu_init     finish_mpu_init()   File ""/workspace/MegatronLM/megatron/initialize.py"", line 60, in finish_mpu_init     _initialize_distributed()   File ""/workspace/MegatronLM/megatron/initialize.py"", line 209, in _initialize_distributed     mpu.initialize_model_parallel(   File ""/workspace/MegatronLM/megatron/core/parallel_state.py"", line 169, in initialize_model_parallel     _initialize_distributed()   File ""/workspace/MegatronLM/megatron/initialize.py"", line 209, in _initialize_distributed     mpu.initialize_model_parallel(   File ""/workspace/MegatronLM/megatron/core/parallel_state.py"", line 169, in initialize_model_parallel     group_gloo = torch.distributed.new_group(ranks, backend=""gloo"")   File ""/usr/local/python/lib/python3.8/sitepackages/torch/distributed/distributed_c10d.py"", line 2966, in new_group     mpu.initialize_model_parallel(   File ""/workspace/MegatronLM/megatron/core/parallel_state.py"", line 169, in initialize_model_parallel     group_gloo = torch.distributed.new_group(ranks, backend=""gloo"")   File ""/usr/local/python/lib/python3.8/sitepackages/torch/distributed/distributed_c10d.py"", line 2966, in new_group     group_gloo = torch.distributed.new_group(ranks, backend=""gloo"")   File ""/usr/local/python/lib/python3.8/sitepackages/torch/distributed/distributed_c10d.py"", line 2966, in new_group     pg = _new_process_group_helper(   File ""/usr/local/python/lib/python3.8/sitepackages/torch/distributed/distributed_c10d.py"", line 703, in _new_process_group_helper     pg = ProcessGroupGloo(prefix_store, rank, world_size, timeout=timeout) RuntimeError: [enforce fail at /root/pytorch/third_party/gloo/gloo/transport/tcp/device.cc:83] ifa != nullptr. Unable to find address for: bond1     pg = _new_process_group_helper(   File ""/usr/local/python/lib/python3.8/sitepackages/torch/distributed/distributed_c10d.py"", line 703, in _new_process_group_helper     pg = _new_process_group_helper(   File ""/usr/local/python/lib/python3.8/sitepackages/torch/distributed/distributed_c10d.py"", line 703, in _new_process_group_helper     pg = ProcessGroupGloo(prefix_store, rank, world_size, timeout=timeout) RuntimeError: [enforce fail at /root/pytorch/third_party/gloo/gloo/transport/tcp/device.cc:83] ifa != nullptr. Unable to find address for: bond1     pg = ProcessGroupGloo(prefix_store, rank, world_size, timeout=timeout) RuntimeError: [enforce fail at /root/pytorch/third_party/gloo/gloo/transport/tcp/device.cc:83] ifa != nullptr. Unable to find address for: bond1 WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 93 closing signal SIGTERM WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 95 closing signal SIGTERM WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 97 closing signal SIGTERM WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 99 closing signal SIGTERM ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 92) of binary: /usr/local/python/bin/python3.8 ERROR:torch.distributed.elastic.agent.server.api:Error waiting on exit barrier. Elapsed: 315.7617883682251 seconds Traceback (most recent call last):   File ""/usr/local/python/lib/python3.8/sitepackages/torch/distributed/elastic/agent/server/api.py"", line 906, in _exit_barrier     store_util.barrier(   File ""/usr/local/python/lib/python3.8/sitepackages/torch/distributed/elastic/utils/store.py"", line 67, in barrier     synchronize(store, data, rank, world_size, key_prefix, barrier_timeout)   File ""/usr/local/python/lib/python3.8/sitepackages/torch/distributed/elastic/utils/store.py"", line 53, in synchronize     agent_data = get_all(store, key_prefix, world_size)   File ""/usr/local/python/lib/python3.8/sitepackages/torch/distributed/elastic/utils/store.py"", line 31, in get_all     data = store.get(f""{prefix}{idx}"") RuntimeError: Socket Timeout Traceback (most recent call last):   File ""/usr/local/python/bin/torchrun"", line 8, in      sys.exit(main())   File ""/usr/local/python/lib/python3.8/sitepackages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 345, in wrapper     return f(*args, **kwargs)   File ""/usr/local/python/lib/python3.8/sitepackages/torch/distributed/run.py"", line 724, in main     run(args)   File ""/usr/local/python/lib/python3.8/sitepackages/torch/distributed/run.py"", line 715, in run     elastic_launch(   File ""/usr/local/python/lib/python3.8/sitepackages/torch/distributed/launcher/api.py"", line 131, in __call__     return launch_agent(self._config, self._entrypoint, list(args))   File ""/usr/local/python/lib/python3.8/sitepackages/torch/distributed/launcher/api.py"", line 245, in launch_agent     raise ChildFailedError( torch.distributed.elastic.multiprocessing.errors.ChildFailedError:  ============================================================ /workspace/MegatronLM/pretrain_gpt.py FAILED ```",2023-07-27T15:02:18Z,,closed,0,5,https://github.com/NVIDIA/Megatron-LM/issues/436,"Can you try running this little test program (using the same number of nodes and GPUs per node as the failure above) and see if this works on your infrastructure? `import os import torch import torch.distributed as dist dist.init_process_group(backend='gloo') rank = os.getenv('RANK', 1) print(rank, 'entering barrier...') torch.distributed.barrier() print(rank, 'through barrier...')`","> Can you try running this little test program (using the same number of nodes and GPUs per node as the failure above) and see if this works on your infrastructure? >  > `import os import torch import torch.distributed as dist >  > dist.init_process_group(backend='gloo') >  > rank = os.getenv('RANK', 1) >  > print(rank, 'entering barrier...') torch.distributed.barrier() print(rank, 'through barrier...')` Thank you. I will try this script. I try to comment out the gloo related code which is `group_gloo = torch.distributed.new_group(ranks, backend=""gloo"")` and the error above disappeared. But there is another error about NCCL:  `RuntimeError: NCCL error in: /root/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1102, internal error, NCCL version 21.2.12` Is this error due to a pytorch version issue? What version of pytorch is needed for MegatronLM, will 1.10 do?","> Can you try running this little test program (using the same number of nodes and GPUs per node as the failure above) and see if this works on your infrastructure? >  > `import os import torch import torch.distributed as dist >  > dist.init_process_group(backend='gloo') >  > rank = os.getenv('RANK', 1) >  > print(rank, 'entering barrier...') torch.distributed.barrier() print(rank, 'through barrier...')` The same error occurs in this program: ``` Traceback (most recent call last):   File ""/workspace/test.py"", line 5, in      dist.init_process_group(backend='gloo')   File ""/usr/local/python/lib/python3.8/sitepackages/torch/distributed/distributed_c10d.py"", line 602, in init_process_group     default_pg = _new_process_group_helper(   File ""/usr/local/python/lib/python3.8/sitepackages/torch/distributed/distributed_c10d.py"", line 703, in _new_process_group_helper     pg = ProcessGroupGloo(prefix_store, rank, world_size, timeout=timeout) RuntimeError: [enforce fail at /root/pytorch/third_party/gloo/gloo/transport/tcp/device.cc:83] ifa != nullptr. Unable to find address for: bond1 ```","Since the small test program fails with the Gloo backend there might be something wrong with your compute infrastructure. My best guess is that there is an incorrect setting of GLOO_SOCKET_IFNAME, or Gloo cannot figure out the correct network interface name and you need to provide the correct interface name in that environment variable. See https://pytorch.org/docs/stable/distributed.htmlcommonenvironmentvariables for more information. For the internal NCCL error, please rerun with the environment variable NCCL_DEBUG set to at least INFO. See https://docs.nvidia.com/deeplearning/nccl/userguide/docs/env.htmlnccldebug for more information. We recommend using PyTorch containers from NGC (https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch). Also see https://github.com/NVIDIA/MegatronLMsetup for more information.","The problem is NCCL can not find the correct NCCL_SOCKET_IFNAME, I try to set `NCCL_SOCKET_IFNAME=eth0` but it also failed. Then I find `ifconfig` print `eth1`, I guess ifconfig will give the right IFNAME for NCCL. But for me, I try to set `export NCCL_SOCKET_IFNAME=eth,en,em,bond` and the training goes well."
janEbert,[QUESTION] Creation of Gloo groups,"After the big `core_transformers` refactor, the same `torch.distributed` groups that NCCL already creates are recreated in Gloo. Is there a specific reason for this or can I safely remove these duplicate groups? ~The creation of the Gloo groups sometimes causes issues on my supercomputing cluster so it would be desirable to stay on just the specified backend (i.e. NCCL).~ â† This issue with Gloo creation was fixed by setting `GLOO_SOCKET_IFNAME=ib0`, which is obviously independent of MegatronLM. However, I would still like to know why or whether using Gloo in addition to NCCL is actually necessary.",2023-07-27T09:02:45Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/435, I think the Gloo groups are needed for saving the distributed optimizer. Megatron gathers the distributed optimizer on CPU before saving This is different from the earlier behaviour where Megatron used to save the optimizer per process (sharded optimizer).,"Ahh, that makes sense, I was wondering why CPU communication was suddenly required. Thank you!"
anon-nlp-dev,[QUESTION] GPT Pretraining Objective,"Hi, I am quite new to the language modeling in general, and while going through your code to see the inner working of the GPT model, I came across the following two lines: https://github.com/NVIDIA/MegatronLM/blob/65da5be50cdaad2205e0bc96295bcaaf2c2c2a34/pretrain_gpt.pyL53L54 Since GPT is an autoregressive model, I was expecting the `labels` to only contain the next token, however it seems that the `labels` share entries with the input `tokens`. Can you please explain why is it done this way? My assumption is that the next token is generated iteratively by increasing the number of input tokens and masking the rest. Afterwards, we take all the predicted tokens and align them to the labels and compute the loss. Is that the right assumption?",2023-07-26T14:16:31Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/434,"You're right that Transformers are autoregressive, however, that is only if you want to generate _new_ tokens. In general, Transformers will generate one output token for each input token. For decoderonly models (like GPT), during the generation for each output token, only the input tokens ""before"" that output token are seen due to the causal attention mask. More specifically, to generate output at position `t`, the model has access to all inputs at positions `', 'I',    'have', 'a',   'dog', '.'] Labels:         ['I',     'have', 'a',    'dog', '.',   ''] ``` As you can see, the model is expected to generate the token at the next position for each input position. We additionally have to supply a `` (beginning of sentence) token so the model has an ""empty"" input to generate from. After all, we would like the model to be able to generate from scratch, with no information at all. That's what the `` token enables. When the input is finished, we would like the model to also predict this via the `` (end of sentence) token. That's how we enable the model to decide to stop generation at some point. While the `` and `` tokens are named according to ""sentences"", this is not how they are used in the Megatron codebase; here, they simply distinguish the beginning or end of the text the model receives. So now why do we care about all the labels? Because it's super efficient. We generate all training signals for the whole input text with one forward pass of the model. Due to the causal masking of the Transformer decoder, the model trains to predict `'I'` from `['']`, it learns to predict `'have'` from `['', 'I']`, it learns to predict `'a'` from `['', 'I', 'have']`, and so on. All with a single forward pass over all the input tokens."," Thank you for the detailed explanation, I appreciate you taking time to explain this. Knowing how causal masking helps in training by generating output for each of the input position in a single forward pass, helps a lot in understanding the inner working of a decoder :)"
1306825592,[QUESTION]How much performance is improved after flashattention is used?,"When I used flashattention training, there didn't seem to be a noticeable improvement in performance.",2023-07-26T02:55:49Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/433,"What configuration (model, number and type of GPUs, degrees of parallelism) are you using?",Marking as stale. No activity in 60 days.
janEbert,Allow usage of non-varlen FlashAttention,"Requires FlashAttention2 and only works for GPT pretraining. Provides a small speedup, but I understand if this is a bit too convoluted to be worth it.",2023-07-25T09:10:10Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/432,Marking as stale. No activity in 60 days.
mayank31398,Bug in data preprocessing,eos token is not included in the length as with the earlier data prerocessing script. https://github.com/NVIDIA/MegatronLM/blob/65da5be50cdaad2205e0bc96295bcaaf2c2c2a34/tools/preprocess_data.pyL91L97 The eos token is added after length calculation and earlier script didn't calculate lenght and used `add_item` method. We are now using `add_doc` which takes lengths and lengths are being calculated incorrectly since eos is added later. barker ,2023-07-25T07:05:23Z,bug,closed,0,11,https://github.com/NVIDIA/Megatron-LM/issues/431,barker  can you take a look at this?,"  barker  seems like a bug that needs to be fixed urgently left side is the old script for preprocess data and right is the new one. As mentioned earlier, an eos token is missing.", What is the commit of your old script?,"It doesn't look to me like the issue is coming from the snippet of code you shared above. That's adding an EOD token to doc_ids, not an EOS token to the sentence and the length is based on the sentence. In the old script example where were you getting the sentence lengths from?","barker I modified the following 3 methods in `MMapIndexedDatasetBuilder` class to add print statements. ```python     def add_item(self, tensor):         """"""         add a sentence to the dataset, this might be an entire         document depending on how we are splitting         """"""          np_array is a sentence         np_array = np.array(tensor.numpy(), dtype=self._dtype)         self._data_file.write(np_array.tobytes(order='C'))          length of the sentence         self._sizes.append(np_array.size)         print(""sizes"", self._sizes)     def add_doc(self, tensor, sizes):         np_array = np.array(tensor, dtype=self._dtype)         self._data_file.write(np_array.tobytes(order='C'))         self._sizes.extend(sizes)         self._doc_idx.append(len(self._sizes))         print(""sizes"", self._sizes)         print(""doc_idx"", self._doc_idx)     def end_document(self):         """"""         ends the current document and adds the starting position of the next document         """"""         self._doc_idx.append(len(self._sizes))         print(""doc_idx"", self._doc_idx) ``` For the older script, I am using the code before preprocess_data.py was replaced with preprocess_data_partitions.py. this is the hash: 7fc9611165da9111c325baf619cdbfad4ba4d5d9","For me, I am not splitting docs into sentences. I have not passed `splitsentences`", This is what I get if I detokenize midway during training. The left side is the correct sample as in the dataset. The right side is getting parts of wrong tokens from some other document. This is tested with USPTO data.,barker opened a PR which fixes this issue: https://github.com/NVIDIA/MegatronLM/pull/447 I have tested this and it works.,"Thanks   I'll get it merged in. I misunderstood the issue yesterday, I see now that the length is the length of the whole document and not an individual sentence so it should include the EOD token.","Fix merged: https://github.com/NVIDIA/MegatronLM/commit/0609f27fe8376f17ab65c001d3d8f35cd8175950, thanks",Thanks a lot for the quick response :)
mayank31398,drop duplicates for the method `set_virtual_pipeline_model_parallel_world_size`,Currently there are 2 duplicates of the same method.,2023-07-22T12:10:03Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/430,Marking as stale. No activity in 60 days.,Looks like this is now resolved in `main`. Thanks for pointing this out!
Vic0428,[BUG] TypeError: 'tuple' object does not support item assignment (sequence_parallel),"Hi all, With sequence parallel and 1f1binterleave schedule, this line triggers the following error. I created a simple fix for this (https://github.com/NVIDIA/MegatronLM/pull/426).  ``` File ""Megatron/megatron/core/pipeline_parallel/schedules.py"", line 425, in forward_backward_pipelining_with_interleaving     tensor_shape[0] = tensor_shape[0] // parallel_state.get_tensor_model_parallel_world_size() TypeError: 'tuple' object does not support item assignment ``` ~Weiqi",2023-07-21T22:49:45Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/429,Marking as stale. No activity in 60 days.
mayank31398,Are there any plans to add torch DDP with pipeline parallel?,"torch DDP is more efficient than local DDP since it overlaps comms with backward. However, in its current state, the communication in local DDP doesn't do this. Are there plans to add this?",2023-07-21T19:06:01Z,duplicate,closed,0,8,https://github.com/NVIDIA/Megatron-LM/issues/428,Yes: https://github.com/NVIDIA/MegatronLM/issues/391,Thanks a lot for the quick reply :),would love to contribute to this btw. if this has already started and you have some pointers :),"There hasn't been any work in earnest yet, although we'd like to get to it soon due to the potential training speedup. We hope/believe it can be achieved with a relatively small amount of code. So we'd welcome help/contribution but it'd be best to take small steps and start with a proposal we can iterate on as necessary. Thanks!","> There hasn't been any work in earnest yet, although we'd like to get to it soon due to the potential training speedup. We hope/believe it can be achieved with a relatively small amount of code. So we'd welcome help/contribution but it'd be best to take small steps and start with a proposal we can iterate on as necessary. Thanks! Hi barker, We implemented it in https://github.com/alibaba/MegatronLLaMA/blob/main/megatron/optimizer/overlapped_dist_optimizer.py.  In certain cases, it works really well.","We recently rewrote the DDP wrapper to support overlapping of allreduce with backprop computation: https://github.com/NVIDIA/MegatronLM/blob/main/megatron/model/distributed.py. It can be turned on using the `overlapgradreduce` commandline argument. The current version does not yet support the distributed optimizer, but this is coming soon.","> We recently rewrote the DDP wrapper to support overlapping of allreduce with backprop computation: https://github.com/NVIDIA/MegatronLM/blob/main/megatron/model/distributed.py. It can be turned on using the `overlapgradreduce` commandline argument. The current version does not yet support the distributed optimizer, but this is coming soon.  I noticed that there is an implementation of distributed adam in apex that supports overlap for communication calculations and is used in nemo.  Not quite sure the reason why megatronlm has to implement its own version, but as a user (who makes some internal changes occasionally), megatronlm's recent code quality and confusing roadmap has been very frustrating (as a very simple example, the latest ddp implementation is not compatible with distributed optimizer. But you can turn on both options without any errors). It's as if there are a thousand teams inside nvidia working on the llm training framework.","The latest DDP implementation should support all possible usecases in the next week or so (distributed optimizer, pipeline parallelism). We have an internal implementation working that is making its way through peer review. What are the issues with Megatron's code quality?"
zxgx,[QUESTION] Does the zero-out operation conduct during the model initialization and affect reproducibility?,"**Your question** As indicated in the annotation, embedding weight in the last stage would be zeroed out.  I doubt that this zeroout operation is not conducted, as the two conditions in the `if` statement in L87 are contradictory. But I assume that, finally after the model initialization, the two embedding weights in `self.language_model.embedding` and `self.word_embeddings` would be added up by the following `all_reduce` anyway, so the two weight matrices would still be the same, am I right?  If so, I assume this problem may lead to some reproducibilitiy issue since the following `all_reduce` changes the initial embedding weight imported from other models. Just close this issue if I'm wrong ðŸ˜„  https://github.com/NVIDIA/MegatronLM/blob/6ef5bdc9fbe030c45d9f7a19e6c42d39f844aaa7/megatron/model/module.pyL85L89 ```[tasklist]  Tasks ```",2023-07-21T09:17:26Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/427,"This is for the T5 model where you have an embedding layer as part of the first stage in the decoder, which isn't the first stage of the overall pipeline. In that case, and when using the embedding weights as the weights for the output layer, there are three copies of the embedding weights: at the beginning of the encoder, beginning of the decoder, and the output layer. This code isn't run for GPT or Bert models. Does that answer the question?",That's true! I must ignore some setup for encoder&decoder models because I follow scripts in `examples/sc21` to read the source code. ðŸ˜µ  Thanks for your answer!
Vic0428,Fix sequence parallel bug in 1f1b-interleave schedule,"Hi all, With sequence parallel and 1f1binterleave schedule, this line triggers the following error. I created a simple fix for this.  ``` File ""Megatron/megatron/core/pipeline_parallel/schedules.py"", line 425, in forward_backward_pipelining_with_interleaving     tensor_shape[0] = tensor_shape[0] // parallel_state.get_tensor_model_parallel_world_size() TypeError: 'tuple' object does not support item assignment ``` ~Weiqi",2023-07-20T21:47:58Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/426,Marking as stale. No activity in 60 days.
mayank31398,Revert to datatype order,"barker   The ordering of dtypes has changed here: https://github.com/bigcodeproject/MegatronLM/commit/bf5206e06608d4457bf2d0d111ac7910aa22b774 eariler, 6 corresponded to 4 byte float and 7 to 8 byte float. Does this matter?",2023-07-20T10:11:00Z,,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/425,This change was intentional. Please see the discussion here: https://github.com/NVIDIA/MegatronLM/pull/327issuecomment1545870657 There was a long standing bug in MegatronLM where np.float was incorrectly used to represent fp32 (when it actually meant fp64). Swapping the ordering was required for backwards compatibility with datasets created using the old buggy version. Has this caused a problem for you?,"Hey barker I am not using any old datasets. So, for me the reverse order should be fine right? I had fixed this myself in this. But this is different order of what is in Nvidia megatron.","I think either order will work for you. You might want to use the same as we are using just to stay in sync. I'll close the issue now, feel free to reopen if you hit problems. Thanks.","Thanks a lot. Unfortunately, we have created some datasets already with our current repo and it will be hard for us to do it all over again. Ill take a look into this. Thanks"
xutianming,Fix mismatch arguments in arguments.py,Fix mismatch arguments in arguments.py,2023-07-19T07:16:21Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/424,Merged: de72f85d1e587b9ed8fdda30e66a40a9097717f5  thanks
lvcc2018,[ENHANCEMENT] Support for LLaMA and LLaMA-2,Support for LLaMA and LLaMA2,2023-07-19T03:37:29Z,enhancement wontfix,closed,0,5,https://github.com/NVIDIA/Megatron-LM/issues/423,In theory you could pretrain LLaMA today in MegatronLM. The only known blocker to pretraining LLaMA 2 is that we don't currently support GQA but we are actively working to add that soon. We don't have the configurations and scripts available outofthebox for doing the LLaMA pretraining so you'd have to implement those yourself but all the building blocks should be available.,"> In theory you could pretrain LLaMA today in MegatronLM. The only known blocker to pretraining LLaMA 2 is that we don't currently support GQA but we are actively working to add that soon. We don't have the configurations and scripts available outofthebox for doing the LLaMA pretraining so you'd have to implement those yourself but all the building blocks should be available. Oh, Thank U~ May I ask when will all these blocks be available? (like RMS Norm, GQA...) Recently I am working on implementing llama 1 and llama 2 in MegatronLM. If these blocks will be release in several weeks, I can simply wait XD. barker   By the way, RMS Norm is not supported yet (but available in apex).","I hack the transformer.py to use apex's RMS norm, it does not work in sequence parallel...",    You may look into https://github.com/alibaba/MegatronLLaMA,or https://github.com/epfLLM/MegatronLLM  (this one supports llama and falcon)
LianxinRay,[QUESTION] cuda graph use,"we use cuda graph to wrap the transformer layer one by one, and we use pipeline without interleaving, but we got the nan grad in training process. At the same time, if we don't adopt 1F1B (only set micro_batch_size), training process has no problem. That's so Confusing. Hope to get the response.",2023-07-18T08:07:36Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/422,Marking as stale. No activity in 60 days.
cordercorder,[QUESTION] Question about the `matmul_input_buffer` variable in the `CoreAttention` module,"In MegatronLM, the `forward` function of the `CoreAttention` module contains a variable called `matmul_input_buffer`, which is utilized as an argument in the `torch.baddbmm` function with a beta value set to 0. However, according to the PyTorch documentation, if beta is set to 0, the `matmul_input_buffer` variable will be ignored. Therefore, it raises the question: Why is the `matmul_input_buffer` variable used in this context, and what is the effect of this variable? For convenience, the pertinent code snippet is presented below for reference: https://github.com/NVIDIA/MegatronLM/blob/040eac9414ccbd1301ae16369c3044c5632b7e14/megatron/model/transformer.pyL243L276",2023-07-17T07:56:25Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/421,"Hey, I think the output is stored back in the first argument of the `torch.baddbmm`. This basically acts as a buffer so PyTorch doesn't have to allocate another buffer for the output tensor and is thus more efficient.",Marking as stale. No activity in 60 days.
sneaxiy,[QUESTION] Why torch.cuda.synchronize() is added after  the computational CUDA stream waits the P2P communication,"Quite curious about the following codes. In L94, the CUDA stream used for OP computation would wait the CUDA stream used for P2P communication, and there would be no data race in different CUDA streams any more. In this way, it seems that L98 is useless, but why is this synchronization added? If we remove L98, is it possible that the training process would hang? https://github.com/NVIDIA/MegatronLM/blob/040eac9414ccbd1301ae16369c3044c5632b7e14/megatron/core/pipeline_parallel/p2p_communication.pyL91L98",2023-07-16T04:31:41Z,,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/420,Marking as stale. No activity in 60 days.,"I think this was added because of a bug in `batch_isend_irecv()`; I don't exactly remember the nature of the failure (it could have been hangs, but it could have also been wrong losses). This may no longer be a problem with the latest PyTorch containers; we just haven't checked.",Marking as stale. No activity in 60 days.,Marking as stale. No activity in 60 days.
aoyulong,Squeeze zero-size group ranges for the distributed optimizer,"**Describe the bug**  Considering the following case:      The model has two parameter which are `param0` with `10` elements and `param1` with `2` elements. There are also two parameter groups where `group0` has `param0` and `group1` has `param1`.     The used data parallel degree is `2`, so the size of the contiguous gradient buffer is `10 + 2 =12`, and each rank owns a local gradient buffer with `12 / 2 = 6` elements. So after applying the distributed optimizer, we got        Rank 0: `group0 = {'params': param0[0:6]}`, `group1 = {'params': []}`       Rank 1:  `group1 = {'params': param0[6:10]}`, ` group1 = {'params': param1[0:2]}`  The potential problem is the `group1 `of Rank 0 has nothing and the underlying optimizer implementation should take care of the parameter group of zero length.     The new Apex Adam implementation can check for empty params in group now: https://github.com/NVIDIA/apex/pull/1596.     But the Apex Adam in the NGC  23.03 docker image doesn't have been updated. So the code of Apex Adam `device = group['params'][0].device` will trigger `IndexError list index out of range` when given the empty params. **To Reproduce** Use a very large data parallel degree when enabling the distributed optimizer with NGC  23.03 docker image. **Proposed fix** The original code has the comment `Squeeze zerosize group ranges` but without no implementation now. In my opinion, this is also the upstream responsibility to deal with it. So this pr add back the zero group filtering. Besides, fix the assignment of `group_range[""orig_group_idx""]` (but this assignment is useless since no statements refer to it).",2023-07-16T03:32:58Z,stale,open,4,3,https://github.com/NVIDIA/Megatron-LM/issues/419,Marking as stale. No activity in 60 days., I hope this fix can be reviewed again since some other users had the same issue 495. ,Marking as stale. No activity in 60 days.
BFeng14,[QUESTION] finetuning with gradient accumulation doesn't currently work,"Hi, we found in tasks/finetune_utils.py that microbatches>1 is not supported in the finetune stage, and now we need to set microbatches>1 to get larger global_batch_size. Is there any hidden question behind that?  ``` def _train(model, optimizer, opt_param_scheduler, forward_step,            train_dataloader, valid_dataloader, end_of_epoch_callback):     """"""Train the model.""""""     args = get_args()     timers = get_timers()     assert get_num_microbatches(     ) == 1, ""finetuning with gradient accumulation doesn't currently work"" ```",2023-07-12T09:52:09Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/418,Marking as stale. No activity in 60 days.
yaoyu-33,[WIP] Cache input activation only if wgrad is required,,2023-07-11T21:26:57Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/417,Marking as stale. No activity in 60 days.
mrkumar83,Description: Demo code to pre train a gpt model with the code parrot dataset. ,Demo code to pre train a gpt model with the code parrot dataset. This includes torchrun based runner bash script and utilizes sequence parallelism across two A10 gpus. The README describes where and how to utilize the NVIDIA docker container for training.,2023-07-11T19:05:59Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/416
t90tank,"How to use flash attention when setting ""--reset-position-ids""?","It seems that Megatron do not pass attention mask to flash attention,  but when setting ""resetpositionids"", different inputs have different attention masks. Does Megatron support this case?  Also, can we open sequence parallel when setting ""resetpositionids""?",2023-07-11T12:09:30Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/415,"It seems resetpositionids doesn't work with flash attention. I checked flash attention's API, it supports causal attention mask only, not custom attention masks.","Yes, this won't work See this PR: https://github.com/bigcodeproject/MegatronLM/pull/53 in the BigCode fork","Thanks, I will try other ways."
aoyulong,Skip 'world_size' instead of coping from checkpint args.,"**Describe** The `world_size` of `margs` will be overwritten by `checkpoint_args` in `tools/checkpoint_saver_megatron.py`. **To Reproduce** 1. Produce a ckpt by global_batch_size =24,  micro_batch_size=1, data_parallel_size =2, tensor_model_parallel_size=4, pipeline_model_paralell_size=2 2. Convert the ckpt to target_tensor_parallel_size=1, target_pipeline_paralell_size=1 by using `tools/checkpoint_util.py ` 3. Get assertion error `assert global_batch_size % micro_batch_times_data_parallel == 0` in `megatron/microbatches.py `",2023-07-11T08:30:43Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/414
cicirori,fix cuda oom when reducing dp world size and make no_save_optim useful for dist_opt,see: https://github.com/NVIDIA/MegatronLM/issues/380,2023-07-08T05:08:28Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/413,Marking as stale. No activity in 60 days.
vrublack,Bug with Rotary embeddings in micro-batched inference ,"The code that deals with Rotary embeddings seems to assume that the first time the model is called, we inference a prefix of the sequence, and the subsequent calls inference a single token at a time. However, this assumption breaks when we use micro batches because then, subsequent calls only have a batch offset but not necessarily a sequence offset. To reproduce, one needs to activate Rotary embeddings and make sure that the condition `current_batch_x_seqlen >= self.pipelining_batch_x_seqlen` is `True` so that the model is inferenced with micro batches (and one needs to enable pipeline parallelism, obviously).",2023-07-07T15:30:11Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/412,Marking as stale. No activity in 60 days.
ladyrick,"bug fix: when eval, dropout_p turns to 0 forever","It's a bug in `FlashSelfAttention` class. Once you run `forward` in eval mode, `dropout_p` will become 0 forever, even you turn back to training mode.",2023-07-07T09:51:34Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/411
ladyrick,fix some variable is not defined bug,Fix some variables is not defined bug: 1. `rank` not defined in `read_metadata()` 2. `checkpoint_name` not defined in `load_checkpoint()` 3. `model_checkpoint_name` not defined in `load_biencoder_checkpoint()`,2023-07-07T09:48:20Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/410
AlphaNext,RuntimeError: Cannot process input tensor without concrete number of dimensions.," 1) Environment ``` torch                         1.10.1+cu111 torchaudio                    0.10.1+cu111 torchvision                   0.11.2+cu111 ``` **Note: Don't use the docker images in README** ``` docker pull nvcr.io/nvidia/pytorch:xx.xxpy3 docker run gpus all it rm v /path/to/megatron:/workspace/megatron v /path/to/dataset:/workspace/dataset v /path/to/checkpoints:/workspace/checkpoints nvcr.io/nvidia/pytorch:xx.xxpy3 ```  2) Run command: examples/pretrain_bert.sh  3) ERROR log: ``` Loading extension module scaled_softmax_cuda... >>> done with compiling and loading fused kernels. Compilation time: 6.691 seconds /home/Code/public_algos/NVMegatronLM/megatron/initialize.py:277: UserWarning: FALLBACK path is taken. This is an indication that codegenFailed for some reason. To debug try disable codege n fallback pathvia setting the env variable`export PYTORCH_CUDA_FUSER_DISABLE_FALLBACK=1` (Triggered internally at  ../torch/csrc/jit/codegen/cuda/manager.cpp:274.)   output = bias_gelu(bias, input) Traceback (most recent call last):   File ""pretrain_bert.py"", line 139, in      forward_step, args_defaults={'tokenizer_type': 'BertWordPieceLowerCase'})   File ""/home/Code/public_algos/NVMegatronLM/megatron/training.py"", line 93, in pretrain     set_jit_fusion_options()   File ""/home/Code/public_algos/NVMegatronLM/megatron/initialize.py"", line 253, in set_jit_fusion_options     _warmup_jit_function()   File ""/home/Code/public_algos/NVMegatronLM/megatron/initialize.py"", line 302, in _warmup_jit_function     output = bias_dropout_add_fused_train(input, bias, residual, dropout_rate) RuntimeError: The following operation failed in the TorchScript interpreter. Traceback of TorchScript (most recent call last): RuntimeError: Cannot process input tensor without concrete number of dimensions. ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 16017) of binary: /opt/conda/bin/python3.7 ```  4) Traced here: bias_dropout_add_fused_train function call",2023-07-07T06:32:10Z,stale,closed,0,6,https://github.com/NVIDIA/Megatron-LM/issues/409,"hi,  , do you solve this problem? I met the same issue.",I got same error as well.,I got same error as well.,Marking as stale. No activity in 60 days.,The problem is not solved.,Marking as stale. No activity in 60 days.
janEbert,Fix missing import,,2023-07-06T13:02:12Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/408,Thanks for all these PRs! Lots of things to test (and break) when pulling things out to the core library.,Thank you for the merges! Very understandable that this kind of major refactor would lead to some breakage.
parthsarthi03,preprocessing script for RETRO does not have all the arguments,"While running run preprocess_data.sh for retro, I'm getting  `main.py: error: the following arguments are required: retrogptdatapath, retrogptevalinterval, retrogptevaliters, retrogptglobalbatchsize, retroindexntrain` It seems like these arguments were added in e68e57a220d75358bd1842225c6b3776c3f143a2. but  https://github.com/NVIDIA/MegatronLM/blob/main/tools/retro/examples/preprocess_data.sh does not support these arguments. ",2023-07-05T21:12:12Z,,closed,6,1,https://github.com/NVIDIA/Megatron-LM/issues/407,"Thank you for pointing this out, example scripts sometimes get behind :). `preprocess_data.sh` and `pretrain_model.sh` have both been updated with the latest arguments, so please try using them again."
janEbert,Fix undefined variable name,Basically code ordering was wrong. Fix CC(Undefined variable `tensor_shape[0]`).,2023-07-05T16:36:48Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/406,"Since this is already the second undefined variable since the refactor, I've ran `mypy` across the code looking for undefined names. However, thankfully, it did not find further occurrences."
janEbert,Only regard decoder sequence length when using an encoder-decoder,"`decoder_seq_len` was unconditionally divided before. When using a decoderonly model, this variable may not be set and just `seq_len` may be given (`seq_len` is also what's used a few lines below the modified code in the decoderonly case).",2023-07-05T16:18:01Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/405
janEbert,Undefined variable `tensor_shape[0]`,https://github.com/NVIDIA/MegatronLM/blob/3316e811cc5335ee24c2d203416d864edcf2f7a8/megatron/core/pipeline_parallel/schedules.pyL420 Probably breakage from the refactor.,2023-07-05T16:12:47Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/404
janEbert,Fix wrong config check,"In the new `TransformerConfig`, when `recomputegranularity` is set, it is checked whether `recomputemethod` is in allowed the values and not `None`. However, when `recomputegranularity selective` is used, `recomputemethod` _must not_ be set (i.e. it must be `None`) according to `megatron/arguments.py`.",2023-07-05T15:48:24Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/403
janEbert,Fix variable access,These were apparently missed when porting over to the usage of `TransformerConfig`.,2023-07-04T13:44:02Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/402,Marking as stale. No activity in 60 days.,Fixed in another upstream refactor.
janEbert,Add ALiBi positional embedding,"This adds the ALiBi method for positional information by  porting over the implementation from BigScience/MegatronDeepSpeed and making minor adjustments to improve memory footprint and dynamicity. ~In addition, I changed the selection of position embedding:~ ~`userotarypositionembeddings` is now replaced by `positionembeddingtype rotary`. If you want, I can keep the old flag for backwardcompatibility. The current state doesn't worry about backwardcompatibility.~ ~I'd also suggest replacing `nopositionembedding` by adding it as the `learned` or `absolute` position embedding type, but I did not go that far. This would prevent mistakes by making sure absolute position embeddings are turned off when a different positional embedding is used.~ The above has by now been integrated in a less maintainable way by using strings instead of enums, but with backwardcompatibility. This PR now bases upon CC(Use enum for position embedding type) to use enums instead of strings for `positionembeddingtype`.",2023-07-04T13:40:54Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/401,Marking as stale. No activity in 60 days.
ladyrick,fix some variables is not defined bug,Fix some variables is not defined bug: 1. `rank` not defined in `read_metadata()` 2. `checkpoint_name` not defined in `load_checkpoint()` 3. `model_checkpoint_name` not defined in `load_biencoder_checkpoint()`,2023-07-04T08:14:33Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/400
torshie,GPTDataset generate sequences longer than expected.,"I applied the following patch to print sample lengths, found all samples generated in the `else` branch are of length 2049 when `seqlength 2048` command line option is specified. ```diff diff git a/megatron/data/gpt_dataset.py b/megatron/data/gpt_dataset.py index 2662b5f..e44ba41 100644  a/megatron/data/gpt_dataset.py +++ b/megatron/data/gpt_dataset.py @@ 9,6 +9,7 @@ import time  import numpy as np  import torch +import megatron  from megatron import print_rank_0  from megatron.core import mpu  from megatron.data.blendable_dataset import BlendableDataset @@ 299,6 +300,7 @@ class GPTDataset(torch.utils.data.Dataset):                  self.doc_idx[doc_index_l],                  length=offset_l + 1))              sample = np.concatenate(sample_list) +            megatron.print_rank_0(f'shape: {sample.shape}')          if self.return_doc_ids:  for retro preprocessing              return {'text': np.array(sample, dtype=np.int64), ``` Is this an expected behavior or a bug ?",2023-07-04T07:14:50Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/399,"The extra token is used as the label when creating the batch, which you can see here: https://github.com/NVIDIA/MegatronLM/blob/040eac9414ccbd1301ae16369c3044c5632b7e14/pretrain_gpt.pyL53"
jon-barker,Create stale.yml,,2023-07-03T17:30:03Z,enhancement,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/398, Creating here as it's github specific,Added in an internal MR.
jon-barker,Update issue templates,,2023-07-03T16:16:10Z,enhancement,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/397, Creating this PR here as it's github specific,Added in an internal MR.
FeixLiu,fp8 transformer engine only brings 35% speed up?,"Hi there, I've used Megatron to train 13B gpt model on a H100 machine. Before I use fp8 transformer engine, the speed of the training is about 0.34s/step. After I enabled the fp8 transformer engine with these two arguments `fp8hybrid, transformerimpl ""transformer_engine""`, the speed of the training is about 0.24s/step. From this blog, the fp8 should have 100% spped up compared with bf16. But I only got 35% speed up on Megatron.  Does the 35% speed up reasonable or I've made some mistakes on using fp8 transformer engine? Thanks a lot for the reply.",2023-07-03T03:07:17Z,,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/396,"I assume you are referencing Figure 9 from the white paper linked from that blog? If so, that figure is simply stating that fp8 is computationally 2x the throughput of bf16, when isolating arithmetic operations. The actual endtoend speedup will be less than this, since you must account for other overheads like communication, memory bandwidth, and the optimizer step. The speedup will also vary greatly depending on your model size and micro batch size.","Got it, thanks for the reply!",should it possible to use fp8 with pipeline parallelism?  My training gets hung up when I try to use both.  I can use fp8 with model parallel ok though.,"When I want to initiate the training using H100, parameter bf16 is ok, But, if I try the same parameter with fp8 parameter, the Error OOM occurs, It confuses me a lot. The added param is  ```     bf16 \     fp8format hybrid \     fp8amaxcomputealgo max \     fp8amaxhistorylen 16 \     transformerimpl transformer_engine ```"
etoilestar,the whole dataset published in hugingface,"hello, could you give me some guides about how to use the whole dataset published in https://huggingface.co/datasets/openwebtext, I find these dataset are compressed in .xz files, I don't know how to convert to .json file.",2023-07-02T08:38:21Z,stale,closed,1,3,https://github.com/NVIDIA/Megatron-LM/issues/395,"One of the files listed on that dataset page is `openwebtext.py` for processing the data. Have you tried using this script? More generally, a `xz` file can be extracted using the `tar` command. Once you are able to access the raw text within that dataset, then you can write a simple script that outputs a jsonl file that follows the format detailed here: https://github.com/NVIDIA/MegatronLMdatapreprocessing","Yes, I tried to use that project, but the network here is not easy to download. I found that those .xz files contain a lot of corpus information, which should be the files obtained after downloading, but I donâ€™t know what is the connection with the json file needed for this project. Thank you for your help",Marking as stale. No activity in 60 days.
double-vin,pretrain_gpt.py: error: unrecognized arguments: --activations-checkpoint-method uniform,I didn't notice the implementation of activationscheckpointmethod.,2023-07-01T02:39:14Z,bug stale,closed,1,3,https://github.com/NVIDIA/Megatron-LM/issues/394,Thanks for pointing this out. It looks like references to that argument should be removed from the README.md. We'll do that in the next update to `main`.,Marking as stale. No activity in 60 days.,This is now resolved!
xealml,AttributeError: 'Namespace' object has no attribute 'overlap_p2p_comm',"Traceback (most recent call last):   File ""../pretrain_gpt.py"", line 115, in      pretrain(train_valid_test_datasets_provider, model_provider,   File ""/data1/limenglin/MegatronLM/megatron/training.py"", line 155, in pretrain     iteration = train(forward_step_func,   File ""/data1/limenglin/MegatronLM/megatron/training.py"", line 705, in train     train_step(forward_step_func,   File ""/data1/limenglin/MegatronLM/megatron/training.py"", line 434, in train_step     overlap_p2p_comm=args.overlap_p2p_comm, AttributeError: 'Namespace' object has no attribute 'overlap_p2p_comm'",2023-06-29T15:52:57Z,bug stale,closed,0,7,https://github.com/NVIDIA/Megatron-LM/issues/393,Hi. What commit of MegatronLM are you using and what command line args for pretrain_gpt.py?  I don't see this issue in `main` with or without `overlap_p2p_communication`,">  commit 4f8e9ac412eff9cabae31495b9f5f3f02fe5a9d4 (HEAD > main, tag: 23.06, origin/master, origin/main, origin/HEAD) Merge: 2004f03 23329e2 Author: Jared Casper  Date:   Wed Jun 28 14:38:03 2023 0700     Merge branch 'jbarker/preprocessing_workers' into 'main'     Make workers required in preprocess_data.py     See merge request ADLR/megatronlm!647",Can you please provide the hardware configuration (number of nodes + GPUs) you are using the command line args you are using for `pretrain_gpt.py`? I cannot replicate this using that same commit on a single node or multiple nodes with or without `overlapp2pcommunication`,">  hi, with a100*8 ",Please provide the command line args you are using with `pretrain_gpt.py`. I cannot replicate this issue on 8xA100 with the details you've provided.,Marking as stale. No activity in 60 days.,"Going to close this, please reopen with more details so that we can triage approrpriately."
alexqdh,Do not reinitialize if torch.distributed.is_initialized() returns True,"Do not reinitialize if torch.distributed.is_initialized() returns True, put the init_process_group to else branch",2023-06-29T08:41:52Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/392,Marking as stale. No activity in 60 days.
li-yi-dong,Any plan for supporting overlapping collective communication for data parallel?,"When training with `DistributedOptimizer`, there would be a long collective communication before `optimizer.step`. Is there any plan to overlap this collective communication with backward computation?",2023-06-29T01:17:50Z,enhancement stale,closed,0,5,https://github.com/NVIDIA/Megatron-LM/issues/391,same problem,"Hi. Yes, this is in the development plan however pipelining and other optimizations we've added make it complicated so we don't have an estimated time for completion yet.","> Hi. Yes, this is in the development plan however pipelining and other optimizations we've added make it complicated so we don't have an estimated time for completion yet. Hope to see it soon : )",Marking as stale. No activity in 60 days.,This is now supported with the `overlapgradreduce` commandline flag: https://github.com/NVIDIA/MegatronLM/blob/main/megatron/arguments.pyL1013.
RalphMao,Minor change in tensor_parallel to align with apex implementation," Context 1. NeMo previously depended on `apex.transformer.tensor_parallel`, and now depends on `megatron.core.tensor_parallel`.  2. NeMo plans to add quantization support to megatron models, the apex version of `RowParallelLinear` and `ColumnParallelLinear` is easier to add quantization hook, while the current megatron version requires code copy of the entire forward function. This PR changes the implementation of megatron.core with apex.",2023-06-28T22:50:24Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/390
luliyucoordinate,Remove redundant `save_for_backward`,,2023-06-28T11:50:22Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/389,Marking as stale. No activity in 60 days.
thomas010,coupled data sharding across data parallel processes,"the folloinwing code for sampling in data/data_samplers.py seems to be coupled. that is to say, some samples for different dataparallel processes always cooccrurence in different epoch: `        def __iter__(self):         active_total_samples = self.total_samples  self.last_batch_size         self.epoch = self.consumed_samples // active_total_samples         current_epoch_samples = self.consumed_samples % active_total_samples         assert current_epoch_samples % self.micro_batch_times_data_parallel_size == 0         if isinstance(self.dataset, RandomSeedDataset):             self.dataset.set_epoch(self.epoch)          data sharding and random sampling         if self.data_sharding:             bucket_size = (self.total_samples // self.micro_batch_times_data_parallel_size) \                            * self.micro_batch_size             bucket_offset = current_epoch_samples // self.data_parallel_size             start_idx = self.data_parallel_rank * bucket_size             g = torch.Generator()             g.manual_seed(self.epoch)             random_idx = torch.randperm(bucket_size, generator=g).tolist()             idx_range = [start_idx + x for x in random_idx[bucket_offset:]]         else:             full_bucket_size = (self.total_samples // self.micro_batch_size) \                                 * self.micro_batch_size             full_bucket_offset = current_epoch_samples             g = torch.Generator()             g.manual_seed(self.epoch)             idx_range_total = \                 torch.randperm(full_bucket_size, generator=g).tolist()             idx_range_active = idx_range_total[full_bucket_offset:]             idx_range = idx_range_active[self.data_parallel_rank::self.data_parallel_size] `",2023-06-28T07:31:10Z,help wanted stale,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/388,"Can you explain more?  It looks OK to me. It's true that for a given data parallel rank the same subset of `random_idx` is always used each epoch, but `random_idx` comes from a `torch.randperm` so the actual indexes into the dataset used on a given data parallel rank will be shuffled each epoch I think.","> Can you explain more? It looks OK to me. It's true that for a given data parallel rank the same subset of `random_idx` is always used each epoch, but `random_idx` comes from a `torch.randperm` so the actual indexes into the dataset used on a given data parallel rank will be shuffled each epoch I think. I means that data samples on different data parallel ranks can be coupled.  assume there are 2 parallel ranks with totally 8 data samples sharded as follows: rank0: sample0, sample1, sample2, sample3 rank1: sample4, sample5, sample6, sample7 On each epoch,  samples 0~3 and samples 4~7 are permuted with the sample `random_idx`. As a result, sample0 and sample4 always cooccurence in the same batch, the sample with others.","Since `random_idx` is generated from a random permutation that is seeded by the epoch it will contain different values each epoch. So although rank0 and rank1 will sample the same positions inside `random_idx` each epoch (based on `start_idx` or `self.data_parallel_rank`) those positions will contain different values corresponding to different indexes in the actual dataset I believe.  For clarity, are you using `self.data_sharding=True`?",Marking as stale. No activity in 60 days.
KB-Ding,Does the `--dataset-impl` in Pre-training support `lazy` mode? ,"Taking the pretraining of GPT as an example, can `datasetimpl` in pretrain_gpt.sh be set to `lazy`? The GPTDataset uses `self.indexed_dataset.get()`, but the IndexedDataset does not support the get().",2023-06-27T19:25:20Z,question,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/387,"No, we actually only support `datasetimpl mmap` right now. The `lazy` and `cached` options were inherited from fairseq but are not supported in MegatronLM. There will be an update to MegatronLM that removes them as options for the argument and removes mention of them from the README.md. Is there a reason you need to use `lazy` instead of `mmap`?","barker Thanks, I wanted to try out the `lazy` mode as I saw it in README and args. Removing it will make it clear that MegatronLM does not support this configuration."
yulingao,RuntimeError: Socket Timeout when setting up NCCL communicator," when i try to train gpt on HPC with slurm, I encounter this error. pytorch 11.3, cuda 11.4, nccl 2.8(I'm not sure whether nccl has been used) > Traceback (most recent call last):   File ""../MegatronLM/pretrain_gpt.py"", line 116, in      pretrain(train_valid_test_datasets_provider,   File ""/gs/home/momo/pytorch/transformer/MegatronLM/megatron/training.py"", line 89, in pretrain Traceback (most recent call last):   File ""../MegatronLM/pretrain_gpt.py"", line 116, in      pretrain(train_valid_test_datasets_provider,   File ""/gs/home/momo/pytorch/transformer/MegatronLM/megatron/training.py"", line 89, in pretrain Traceback (most recent call last):   File ""../MegatronLM/pretrain_gpt.py"", line 116, in      pretrain(train_valid_test_datasets_provider,   File ""/gs/home/momo/pytorch/transformer/MegatronLM/megatron/training.py"", line 89, in pretrain     initialize_megatron(extra_args_provider=extra_args_provider,   File ""/gs/home/momo/pytorch/transformer/MegatronLM/megatron/initialize.py"", line 82, in initialize_megatron     initialize_megatron(extra_args_provider=extra_args_provider,   File ""/gs/home/momo/pytorch/transformer/MegatronLM/megatron/initialize.py"", line 82, in initialize_megatron     initialize_megatron(extra_args_provider=extra_args_provider,   File ""/gs/home/momo/pytorch/transformer/MegatronLM/megatron/initialize.py"", line 82, in initialize_megatron     _compile_dependencies()   File ""/gs/home/momo/pytorch/transformer/MegatronLM/megatron/initialize.py"", line 133, in _compile_dependencies     torch.distributed.barrier()   File ""/gs/home/momo/anaconda3/envs/cuda11.4/lib/python3.8/sitepackages/torch/distributed/distributed_c10d.py"", line 2776, in barrier     _compile_dependencies()   File ""/gs/home/momo/pytorch/transformer/MegatronLM/megatron/initialize.py"", line 133, in _compile_dependencies     torch.distributed.barrier()   File ""/gs/home/momo/anaconda3/envs/cuda11.4/lib/python3.8/sitepackages/torch/distributed/distributed_c10d.py"", line 2776, in barrier     _compile_dependencies()   File ""/gs/home/momo/pytorch/transformer/MegatronLM/megatron/initialize.py"", line 133, in _compile_dependencies     torch.distributed.barrier()   File ""/gs/home/momo/anaconda3/envs/cuda11.4/lib/python3.8/sitepackages/torch/distributed/distributed_c10d.py"", line 2776, in barrier     work = default_pg.barrier(opts=opts) RuntimeError: [2] is setting up NCCL communicator and retreiving ncclUniqueId from [0] via c10d keyvalue store by key '0', but store>get('0') got error: Socket Timeout Exception raised from recvBytes at ../torch/csrc/distributed/c10d/Utils.hpp:580 (most recent call first): frame CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x7f6fb68207d2 in /gs/home/momo/anaconda3/envs/cuda11.4/lib/python3.8/sitepackages/torch/lib/libc10.so) frame CC(Faster dataloader merge): c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x5f (0x7f6fb681cf3f in /gs/home/momo/anaconda3/envs/cuda11.4/lib/python3.8/sitepackages/torch/lib/libc10.so) frame CC(rename confusing arg): c10d::TCPStore::doWait(c10::ArrayRef, std::chrono::duration >) + 0x11f (0x7f6ff2e052ff in /gs/home/momo/anaconda3/envs/cuda11.4/lib/python3.8/sitepackages/torch/lib/libtorch_cpu.so) frame CC(Compatibility with pytorchtransformers for finetuning ): c10d::TCPStore::doGet(std::string const&) + 0x21 (0x7f6ff2e06281 in /gs/home/momo/anaconda3/envs/cuda11.4/lib/python3.8/sitepackages/torch/lib/libtorch_cpu.so) frame CC(Note that as of now you need to have PySOL cloned to the directory here before building the container.): c10d::TCPStore::get(std::string const&) + 0x5b (0x7f6ff2e0630b in /gs/home/momo/anaconda3/envs/cuda11.4/lib/python3.8/sitepackages/torch/lib/libtorch_cpu.so) frame CC(I do not know how to solve it.): c10d::PrefixStore::get(std::string const&) + 0x32 (0x7f6ff2dd7e52 in /gs/home/momo/anaconda3/envs/cuda11.4/lib/python3.8/sitepackages/torch/lib/libtorch_cpu.so) frame CC(perplexity too big for gpt2 wikitext evaluation): c10d::PrefixStore::get(std::string const&) + 0x32 (0x7f6ff2dd7e52 in /gs/home/momo/anaconda3/envs/cuda11.4/lib/python3.8/sitepackages/torch/lib/libtorch_cpu.so) frame CC(PyTorch 1.2 support?): c10d::PrefixStore::get(std::string const&) + 0x32 (0x7f6ff2dd7e52 in /gs/home/momo/anaconda3/envs/cuda11.4/lib/python3.8/sitepackages/torch/lib/libtorch_cpu.so) frame CC(Support latest PyTorch RNG state API.): c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, c10d::OpType, std::string const&, int) + 0xe4 (0x7f70098ba4b4 in /gs/home/momo/anaconda3/envs/cuda11.4/lib/python3.8/sitepackages/torch/lib/libtorch_cuda_cpp.so) frame CC(added missing validdata line): c10d::ProcessGroupNCCL::getNCCLComm(std::string const&, std::vector > const&, c10d::OpType, int, bool) + 0x1d9 (0x7f70098be549 in /gs/home/momo/anaconda3/envs/cuda11.4/lib/python3.8/sitepackages/torch/lib/libtorch_cuda_cpp.so) frame CC(Can we get some samples?):  + 0x26ff9e5 (0x7f70098c19e5 in /gs/home/momo/anaconda3/envs/cuda11.4/lib/python3.8/sitepackages/torch/lib/libtorch_cuda_cpp.so) frame CC(Any plans to release pretrained model?): c10d::ProcessGroupNCCL::allreduce_impl(std::vector >&, c10d::AllreduceOptions const&) + 0xf (0x7f70098c2cdf in /gs/home/momo/anaconda3/envs/cuda11.4/lib/python3.8/sitepackages/torch/lib/libtorch_cuda_cpp.so) frame CC(Collecting Wikipedia Training Data issues): c10d::ProcessGroupNCCL::allreduce(std::vector >&, c10d::AllreduceOptions const&) + 0x2d3 (0x7f70098c8df3 in /gs/home/momo/anaconda3/envs/cuda11.4/lib/python3.8/sitepackages/torch/lib/libtorch_cuda_cpp.so) frame CC(No module named 'apex'): c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x72a (0x7f70098d284a in /gs/home/momo/anaconda3/envs/cuda11.4/lib/python3.8/sitepackages/torch/lib/libtorch_cuda_cpp.so) frame CC(1:train loss decrease too faster.2:learning rates did not change after warmup iter,always kept on 1.5e4. Is it normal phenomenon? ):  + 0x7fda43 (0x7f706c135a43 in /gs/home/momo/anaconda3/envs/cuda11.4/lib/python3.8/sitepackages/torch/lib/libtorch_python.so) frame CC(Error when runinng script pretrain_gpt2_distributed.sh):  + 0x1f8132 (0x7f706bb30132 in /gs/home/momo/anaconda3/envs/cuda11.4/lib/python3.8/sitepackages/torch/lib/libtorch_python.so) frame CC(Rename): PyCFunction_Call + 0x52 (0x4f5652 in /gs/home/momo/anaconda3/envs/cuda11.4/bin/python) frame CC(Why NEGOTIATE_ALLREDUCE is much longer in TensorFlow comparing to PyTorch?): _PyObject_MakeTpCall + 0x3bb (0x4e0c8b in /gs/home/momo/anaconda3/envs/cuda11.4/bin/python) frame CC(How to train on other corpora besides web/wiki text): /gs/home/momo/anaconda3/envs/cuda11.4/bin/python() [0x4f53fd] frame CC(Why the backward function of _CopyToModelParallelRegion calls reduce fuction? can somebody share the mathematical proof): _PyEval_EvalFrameDefault + 0x1150 (0x4d9140 in /gs/home/momo/anaconda3/envs/cuda11.4/bin/python) frame CC(mpu.vocab_parallel_cross_entropy VS cross entropy): /gs/home/momo/anaconda3/envs/cuda11.4/bin/python() [0x586b70] frame CC(Possible solution for using torch.multiprocessing.spawn): _PyEval_EvalFrameDefault + 0x49a9 (0x4dc999 in /gs/home/momo/anaconda3/envs/cuda11.4/bin/python) frame CC(bert model encoding error on python3.6.8 ): _PyFunction_Vectorcall + 0x106 (0x4e7fe6 in /gs/home/momo/anaconda3/envs/cuda11.4/bin/python) frame CC(GPT2 generation samples error ): _PyEval_EvalFrameDefault + 0x399 (0x4d8389 in /gs/home/momo/anaconda3/envs/cuda11.4/bin/python) frame CC(GPT2 evaluation, wikitext2, wikitext103, PTB): _PyEval_EvalCodeWithName + 0x2f1 (0x4d6fb1 in /gs/home/momo/anaconda3/envs/cuda11.4/bin/python) frame CC(fixed case where test set is a 0 dim object to be returned as a list): _PyFunction_Vectorcall + 0x19c (0x4e807c in /gs/home/momo/anaconda3/envs/cuda11.4/bin/python) frame CC(dropout should be wrapped in `get_cuda_rng_tracker`): _PyEval_EvalFrameDefault + 0x1150 (0x4d9140 in /gs/home/momo/anaconda3/envs/cuda11.4/bin/python) frame CC(Calculate the TFLOPS performance with elapsed time per iteration): _PyEval_EvalCodeWithName + 0x2f1 (0x4d6fb1 in /gs/home/momo/anaconda3/envs/cuda11.4/bin/python) frame CC(Couldn't find BERT 3.9B related implementation): _PyFunction_Vectorcall + 0x19c (0x4e807c in /gs/home/momo/anaconda3/envs/cuda11.4/bin/python) frame CC(BERT Loss not decreasing): _PyEval_EvalFrameDefault + 0x1150 (0x4d9140 in /gs/home/momo/anaconda3/envs/cuda11.4/bin/python) frame CC(Fix typos): _PyEval_EvalCodeWithName + 0x2f1 (0x4d6fb1 in /gs/home/momo/anaconda3/envs/cuda11.4/bin/python) frame CC(Evaluating on all GLUE tasks): PyEval_EvalCodeEx + 0x39 (0x585d79 in /gs/home/momo/anaconda3/envs/cuda11.4/bin/python) frame CC(evaluation loss  MegatronLM Compatibility): /gs/home/momo/anaconda3/envs/cuda11.4/bin/python() [0x44fd9e] frame CC(FileExistsError when training with a shared filesystem): Py_BytesMain + 0x39 (0x579dd9 in /gs/home/momo/anaconda3/envs/cuda11.4/bin/python) frame CC(Is there a pretrained 8.3B parameter model?): __libc_start_main + 0xf5 (0x7f70741f7a85 in /lib64/libc.so.6) frame CC(Scaling up to GPT3 size (175B)): /gs/home/momo/anaconda3/envs/cuda11.4/bin/python() [0x579c8d]",2023-06-27T16:04:02Z,bug,closed,0,14,https://github.com/NVIDIA/Megatron-LM/issues/386,"Are you using Docker on slurm? If so, do you have a Dockerfile or container I could use to reproduce the error?","> Are you using Docker on slurm? If so, do you have a Dockerfile or container I could use to reproduce the error? I want to use docker, but no docker or container in this HPC that i'm using, only provide anaconda3, and have no access to the internet. ","OK, can you share the configuration of the training job then? i.e. the arguments to `pretrain_gpt.py`. Thanks","of course, here is my configuration file, thank you for your time > !/bin/bash SBATCH jobname=train_gpt2 SBATCH partition=gpu01 SBATCH nodes=1 SBATCH gres=gpu:4 SBATCH time=12:00:00 SBATCH o out.std SBATCH e out.err conda activate cuda11.4 cd $SLURM_SUBMIT_DIR echo ""SLURM_JOBID= ""$SLURM_JOBID echo ""SLURM_JOB_NODELIST= ""$SLURM_JOB_NODELIST GPUS_PER_NODE=4 MASTER_ADDR=localhost MASTER_PORT=6001 NNODES=1 NODE_RANK=0 WORLD_SIZE=$(($GPUS_PER_NODE*$NNODES)) DATA_PATH=data/meggpt2_text_document CHECKPOINT_PATH=./ck export CUDA_DEVICE_MAX_CONNECTIONS=""1"" DISTRIBUTED_ARGS=""nproc_per_node $GPUS_PER_NODE nnodes $NNODES node_rank $NODE_RANK master_addr $MASTER_ADDR master_port $MASTER_PORT"" python m torch.distributed.launch $DISTRIBUTED_ARGS \        ../MegatronLM/pretrain_gpt.py \        tensormodelparallelsize 1 \        pipelinemodelparallelsize 1 \        numlayers 12 \        hiddensize 768 \        numattentionheads 12 \        microbatchsize 4 \        globalbatchsize 16 \        seqlength 1024 \        maxpositionembeddings 1024 \        trainiters 5000 \        lrdecayiters 320000 \        save $CHECKPOINT_PATH \        load $CHECKPOINT_PATH \        datapath $DATA_PATH \        vocabfile data/vocab.json \        mergefile data/merges.txt \        dataimpl mmap \        split 949,50,1 \        distributedbackend nccl \        lr 0.00015 \        lrdecaystyle cosine \        minlr 1.0e5 \        weightdecay 1e2 \        clipgrad 1.0 \        lrwarmupfraction .01 \        loginterval 10 \        saveinterval 500 \        evalinterval 100 \        evaliters 10 \        fp16 echo ""Job end""","I haven't been able to replicate this result yet, but will try again with a container tomorrow with those exact versions of torch, cuda and nccl. One thing you might try is upgrading your nccl as it's quite old and the source of the error appears to be nccl.","Can you please try rerunning it with the line ""export NCCL_DEBUG=INFO"" somewhere above the line that starts with ""python m torch.distributed.launch ...""? (see https://docs.nvidia.com/deeplearning/nccl/userguide/docs/env.htmlnccldebug for more information about what this does)","> I haven't been able to replicate this result yet, but will try again with a container tomorrow with those exact versions of torch, cuda and nccl. One thing you might try is upgrading your nccl as it's quite old and the source of the error appears to be nccl. Sad thing. I can't upgrade nccl as I don't have administrative rights for the HPC.","> Can you please try rerunning it with the line ""export NCCL_DEBUG=INFO"" somewhere above the line that starts with ""python m torch.distributed.launch ...""? >  > (see https://docs.nvidia.com/deeplearning/nccl/userguide/docs/env.htmlnccldebug for more information about what this does) I have tried to add the following to slurm sbatch scripts. But still no nccl related output in out.err or out.std. Do you think it could be that i didn't call nccl correctly? export NCCL_SOCKET_IFNAME=ens4f0 export NCCL_DEBUG=INFO export NCCL_DEBUG_SUBSYS=ALL","Unless you're absolutely certain about the correct interface I would recommend **_not_** setting NCCL_SOCKET_IFNAME. Instead of trying to debug this with MegatronLM can you try running this simple test program: `import os import torch import torch.distributed as dist dist.init_process_group(backend='nccl') rank = os.getenv('RANK', 1) print(rank, 'entering barrier...') torch.distributed.barrier() print(rank, 'through barrier...')`","> Unless you're absolutely certain about the correct interface I would recommend **_not_** setting NCCL_SOCKET_IFNAME. Instead of trying to debug this with MegatronLM can you try running this simple test program: >  > `import os import torch import torch.distributed as dist >  > dist.init_process_group(backend='nccl') >  > rank = os.getenv('RANK', 1) >  > print(rank, 'entering barrier...') torch.distributed.barrier() print(rank, 'through barrier...')` I save this test program into test.py, run it with ""python m torch.distributed.launch test.py"", and get the following result. It seems like that NCCL works well. > 0 entering barrier... 0 through barrier... FutureWarning: The module torch.distributed.launch is deprecated and will be removed in future. Use torchrun. Note that use_env is set by default in torchrun. If your script expects `local_rank` argument to be set, please change it to read from `os.environ['LOCAL_RANK']` instead. See  https://pytorch.org/docs/stable/distributed.htmllaunchutility for  further instructions","Also, this error occurs when ""compiling and loading fused kernels"", that is, lines  133 of megatron/initialize.py.  I wonder if the socket timeout is caused by fused_kernels.load(args) being stuck or not loading within the default 10 minutes?      Always build on rank zero first.     if torch.distributed.get_rank() == 0:         start_time = time.time()         print('> compiling and loading fused kernels ...', flush=True)         fused_kernels.load(args)         torch.distributed.barrier()     else:         torch.distributed.barrier()    this is line 133         fused_kernels.load(args)      Simple barrier to make sure all ranks have passed the      compilation phase successfully before moving on to the      rest of the program. We think this might ensure that      the lock is released.     torch.distributed.barrier()","I try to add  the load time for ""fused_kernel.load(args)"" by specifying  the "" distributedtimeoutminutes 30 "" in the training args. It's still reporting the same ""socket timeout"" error, so I'm basically sure it's this ""fused_kernel.load(args)"" that's stuck.","barker   Hi, I find the problem. The first time I run MegatronLM, the ""torch.utils.cpp_extension.load"" found the wrong cuda (10.1) because the CUDA_HOME environment was not configured, so the fused_kernel did not compile successfully(that is, scaled_masked_softmax_cuda.so, scaled_softmax_cuda.so, scaled_upper_triang_masked_softmax_cuda.so). ""torch.utils.cpp_extension.load"" left a lock file in the /path/to/MegatronLM/megatron/fused_kernels/build directory, which prevented subsequent compilation of the program at runtime, resulting in a socket timeout.  I have now removed this lock file, and specified the CUDA_HOME in the slurm submit file, I was able to start training properly. Thank you so much for your help!","Glad you figured it out. An update to Megatron coming very soon will remove the need for the fused_kernel compilation, since we'll be switching to using the Apex kernels. Closing this issue."
velconia,How to apply different optimizers and lr_scheduler to different part of models?,"Hi, I want use SGD for the last layer of GPT model and AdamW for the rest of the GPT model but I don't figure out how to do it It seems that MegatronLM only support one optimizer for one model right?",2023-06-26T12:31:13Z,enhancement stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/385,"That's correct, currently it's a single optimizer that applies to the whole model.",Marking as stale. No activity in 60 days.
janEbert,Allow setting initial warmup LR,"Allows starting linear warmup from a value other than 0. Backwardcompatible due to the default of 0.0, except for the API change. If you'd like, I can also add the new argument in a backwardcompatible way; however, I thought the positioning made sense this way. (The order initial LR, max LR, min LR follows the temporal evolution.) :)",2023-06-26T08:16:25Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/384
forceshorty,Why do we only need to use the layernorm parameters of tp_rank 0 when converting a checkpoint?,"https://github.com/NVIDIA/MegatronLM/blob/060415572f4365a2e895f8036c4e37dad0efbdf5/tools/checkpoint_loader_megatron.pyL236 Is it because all layernorms on tp_rank are the same, so only tp_rank 0 can be used? If using sequence parallelism, will different tp_ranks contain different layernorms? In this case, do we still only need to use parameters from tp_rank 0? Please advise.",2023-06-26T05:17:01Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/383,"Sequence parallelism doesn't split layernorm weight&&bias, it spilts input tensor in sequence dimension. So all ranks have same layernorm parameters.","  Thank you for your answer. Because all tp_ranks have save layernorm, Does it mean that we can use other rank's layernorm weight, not just tp_rank0 in checkpoint convert script?","Yes, but tp_rank0 always exists, so we load weight from tp_rank0."
GuanhuaWang,add 2 node dist train for internal cluster,,2023-06-23T18:45:38Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/382
aoyulong,How to split the num_layers unevenly when using pipeline parallelism?,"For now, MegatronLM requires the num_layers must be divisible by pipeline_model_parallel_size: `assert args.num_layers % args.transformer_pipeline_model_parallel_size == 0, \                 'num_layers must be divisible by transformer_pipeline_model_parallel_size'` How to split the num_layers unevenly? For example, given num_layers=7 and pipeline_model_parallel_size=2, the results will be 3 num_layers for stage 0 and 4 num_layers for stage 1.",2023-06-21T09:45:21Z,enhancement stale,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/381,"As you note, this isn't currently supported. Are you looking for guidance on how it should be implemented?",barker I will try to implement it and some guidances will be very helpful.  I have found the direct place that needs to be modified. But I am not sure whether there are some other places that need to be modified.,There is a proposed approach to this here: https://github.com/NVIDIA/MegatronLM/pull/274 This is still in review and may not be the solution we adopt though.,Marking as stale. No activity in 60 days.
cicirori,Distributed optimizer DP-independent checkpoint would use redundant gpu memory.,"When reducing the training size, use dpindependent checkpoint would cause gpu oom sometimes. ``` Traceback (most recent call last):   File ""megatronlm/pretrain_gpt.py"", line 116, in      pretrain(train_valid_test_datasets_provider,   File ""megatronlm/megatron/training.py"", line 113, in pretrain     model, optimizer, opt_param_scheduler = setup_model_and_optimizer(   File ""megatronlm/megatron/training.py"", line 527, in setup_model_and_optimizer     args.iteration = load_checkpoint(model, optimizer, opt_param_scheduler)   File ""megatronlm/megatron/checkpointing.py"", line 572, in load_checkpoint     optimizer.load_state_dict(state_dict['optimizer'])   File ""megatronlm/megatron/optimizer/distrib_optimizer.py"", line 563, in load_state_dict     self.optimizer.load_state_dict({   File ""/opt/conda/lib/python3.10/sitepackages/torch/optim/optimizer.py"", line 379, in load_state_dict     state_dict = deepcopy(state_dict)   File ""/opt/conda/lib/python3.10/copy.py"", line 146, in deepcopy     y = copier(x, memo)   File ""/opt/conda/lib/python3.10/copy.py"", line 231, in _deepcopy_dict     y[deepcopy(key, memo)] = deepcopy(value, memo)   File ""/opt/conda/lib/python3.10/copy.py"", line 146, in deepcopy     y = copier(x, memo)   File ""/opt/conda/lib/python3.10/copy.py"", line 231, in _deepcopy_dict     y[deepcopy(key, memo)] = deepcopy(value, memo)   File ""/opt/conda/lib/python3.10/copy.py"", line 146, in deepcopy     y = copier(x, memo)   File ""/opt/conda/lib/python3.10/copy.py"", line 231, in _deepcopy_dict     y[deepcopy(key, memo)] = deepcopy(value, memo)   File ""/opt/conda/lib/python3.10/copy.py"", line 153, in deepcopy     y = copier(memo)   File ""/opt/conda/lib/python3.10/sitepackages/torch/_tensor.py"", line 118, in __deepcopy__     new_storage = self._typed_storage()._deepcopy(memo)   File ""/opt/conda/lib/python3.10/sitepackages/torch/storage.py"", line 661, in _deepcopy     return self._new_wrapped_storage(copy.deepcopy(self._untyped_storage, memo))   File ""/opt/conda/lib/python3.10/copy.py"", line 153, in deepcopy     y = copier(memo)   File ""/opt/conda/lib/python3.10/sitepackages/torch/storage.py"", line 98, in __deepcopy__     new_storage = self.clone()   File ""/opt/conda/lib/python3.10/sitepackages/torch/storage.py"", line 112, in clone     return type(self)(self.nbytes(), device=self.device).copy_(self) torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 170.00 MiB (GPU 1; 79.35 GiB total capacity; 77.59 GiB already allocated; 7.19 MiB free; 77.99 GiB reserved in total by PyTorch) If reserved memory is >> al located memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF ```",2023-06-20T08:18:46Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/380,"https://github.com/NVIDIA/MegatronLM/blob/main/megatron/optimizer/distrib_optimizer.pyL540 the optimizer's `load_state_dict` of torch will deep copy it's inputs. just tried change `init_shard` from cuda device to cpu, this problem seems fixed. nvidia ",Marking as stale. No activity in 60 days.
xxr3376,Fix pytorch version check constriant,Refine the compare method to make sure PyTorch 2.x could pass the check,2023-06-20T04:49:03Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/379,Merged: de72f85d1e587b9ed8fdda30e66a40a9097717f5  thanks
yaroslavvb,"Using `--data-impl cached` breaks with ""AttributeError: 'IndexedCachedDataset' object has no attribute 'get_doc_idx' ","generation/reading of Megatron ""cached"" format appears broken in tags/23.05, tags/23.04, f7056ab How to reproduce:  Follow basic example under Data preprocessing  Generate `.idx` and `.bin` files, ie ``` cd ~/MegatronLM export MAX_JOBS=20 VOCAB_FILE=~/data/.../bertlargeuncasedvocab.txt JSON_PATH=~/data/.../nvidia.json OUTPUT_PREFIX=~/data/.../nvidia python tools/preprocess_data.py \        input $JSON_PATH \        outputprefix $OUTPUT_PREFIX \        vocabfile $VOCAB_FILE \        datasetimpl cached \        tokenizertype BertWordPieceLowerCase \        splitsentences \         workers 20 chunksize 25 ``` run pretraining ``` cd ~/MegatronLM export CUDA_DEVICE_MAX_CONNECTIONS=1 export MAX_JOBS=20 CHECKPOINT_PATH=~/data/.../release/mp_rank_00/model_optim_rng.pt VOCAB_FILE=~/data/.../bertlargeuncasedvocab.txt DATA_PATH=~/data/.../nvidia_text_sentence export CUDA_DEVICE_MAX_CONNECTIONS=1 BERT_ARGS=""     numlayers 24 \     hiddensize 1024 \     numattentionheads 16 \     seqlength 512 \     maxpositionembeddings 512 \     microbatchsize 4 \     globalbatchsize 8 \     lr 0.0001 \     trainiters 2000000 \     lrdecayiters 990000 \     lrdecaystyle linear \     minlr 0.00001 \     weightdecay 1e2 \     lrwarmupfraction .01 \     clipgrad 1.0 \     fp16 "" DATA_ARGS=""     datapath $DATA_PATH \     vocabfile $VOCAB_FILE \     dataimpl cached \     split 949,50,1 "" OUTPUT_ARGS=""     loginterval 100 \     saveinterval 10000 \     evalinterval 1000 \     evaliters 10 "" torchrun pretrain_bert.py \     $BERT_ARGS \     $DATA_ARGS \     $OUTPUT_ARGS \     save $CHECKPOINT_PATH \     load $CHECKPOINT_PATH ``` See the following ``` Emitting ninja build file /home/yaroslav_nvkf_ai/MegatronLM/megatron/fused_kernels/build/build.ninja... Building extension module scaled_softmax_cuda... Using envvar MAX_JOBS (20) as the number of workers... ninja: no work to do. Loading extension module scaled_softmax_cuda... >>> done with compiling and loading fused kernels. Compilation time: 2.190 seconds time to initialize megatron (seconds): 4.230 [after megatron is initialized] datetime: 20230616 21:43:40  building BERT model ...  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 336297858 > learning rate decay style: linear WARNING: could not find the metadata file /home/yaroslav_nvkf_ai/data/jklp/release/mp_rank_00/model_optim_rng.pt/latest_checkpointed_iteration.txt      will not load any checkpoints and will start from random /home/yaroslav_nvkf_ai/anaconda3/envs/megatron/lib/python3.10/sitepackages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.   warnings.warn( (min, max) time across ranks (ms):     loadcheckpoint ................................: (1.74, 1.74) [after model, optimizer, and learning rate scheduler are built] datetime: 20230616 21:43:40  > building train, validation, and test datasets ...  > datasets target sizes (minimum size):     train:      16000000     validation: 160080     test:       80 > building train, validation, and test datasets for BERT ...  > building dataset index ...  > finished creating indexed dataset in 0.002279 seconds  > indexed dataset stats:     number of documents: 2     number of sentences: 2  > dataset split:     train:      document indices in [0, 2) total of 2 documents      sentence indices in [0, 2) total of 2 sentences     validation:      document indices in [2, 2) total of 0 documents      sentence indices in [2, 2) total of 0 sentences     test:      document indices in [2, 2) total of 0 documents      sentence indices in [2, 2) total of 0 sentences Traceback (most recent call last):   File ""/home/yaroslav_nvkf_ai/MegatronLM/pretrain_bert.py"", line 134, in      pretrain(train_valid_test_datasets_provider, model_provider,   File ""/home/yaroslav_nvkf_ai/MegatronLM/megatron/training.py"", line 134, in pretrain     = build_train_valid_test_data_iterators(   File ""/home/yaroslav_nvkf_ai/MegatronLM/megatron/training.py"", line 979, in build_train_valid_test_data_iterators     build_train_valid_test_data_loaders(   File ""/home/yaroslav_nvkf_ai/MegatronLM/megatron/training.py"", line 940, in build_train_valid_test_data_loaders     train_ds, valid_ds, test_ds = build_train_valid_test_datasets(   File ""/home/yaroslav_nvkf_ai/MegatronLM/megatron/training.py"", line 913, in build_train_valid_test_datasets     return build_train_valid_test_datasets_provider(train_val_test_num_samples)   File ""/home/yaroslav_nvkf_ai/MegatronLM/pretrain_bert.py"", line 116, in train_valid_test_datasets_provider     train_ds, valid_ds, test_ds = build_train_valid_test_datasets(   File ""/home/yaroslav_nvkf_ai/MegatronLM/megatron/data/dataset_utils.py"", line 431, in build_train_valid_test_datasets     return _build_train_valid_test_datasets(data_prefix[0],   File ""/home/yaroslav_nvkf_ai/MegatronLM/megatron/data/dataset_utils.py"", line 589, in _build_train_valid_test_datasets     train_dataset = build_dataset(0, 'train')   File ""/home/yaroslav_nvkf_ai/MegatronLM/megatron/data/dataset_utils.py"", line 535, in build_dataset     doc_idx_ptr = indexed_dataset.get_doc_idx() AttributeError: 'IndexedCachedDataset' object has no attribute 'get_doc_idx' ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 43659) of binary: /home/yaroslav_nvkf_ai/anaconda3/envs/megatron/bin/python ```",2023-06-16T21:46:38Z,bug,closed,0,5,https://github.com/NVIDIA/Megatron-LM/issues/378,I've replicated this. Debugging today.," Looking into this more it appears we've never supported `dataimpl cached`, it was carried over from fairseq but needs updating with document indexing. Can you share why you're looking to use it rather than mmap?","barker we ran out of memory processing Common Crawl with 10GB chunks using `mmap` option, switching to `cached` allowed us to process it without crashing. If `cached` is not supported, would make sense to close this issue, perhaps a with a doc update indicating it's not supported", We have a fix incoming today for the OOM error you encountered with mmap. We'll also remove the `cached` option and references.," Sorry for the delay getting this in, but the OOM issue you encountered with mmap option for 10GB chunks should be solved by the update to `preprocess_data.py` in https://github.com/NVIDIA/MegatronLM/commit/4f8e9ac412eff9cabae31495b9f5f3f02fe5a9d4 References to `lazy` and `cached` have also been removed from the docs. Closing the issue, let me know if you hit any issues with this change."
msalhab96,Updating incorrect commands and missing arguments,"Updated the below in the README.md file:  ```vocab``` replaced by ```vocabfile```.  ```workers``` and ```chunksize``` arguments are missing from the preprocessing command, hence I added them. ",2023-06-16T15:13:50Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/377,Marking as stale. No activity in 60 days.
chensinit,Where is megatrone.enums ?,Hello. I got llm model and try to run it. I install many library. But i can't install megateon.enums. Where is megatron.enums? What should i install for it? Thank you.,2023-06-15T09:43:34Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/376,I'm sorry. That was custom class.
dongguanting,"How to solve ""NaN or Inf found in input tensor.""","When I pretrained the GPT model, I found that there will be such a ""NaN or Inf found in input tensor."" print out during the training process. I suspect that the loss is not calculated successfully. How to fix this bug? !image",2023-06-14T06:04:51Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/375,"I solved it , because i used unmatched vocab.json and merge.bpe"
dongguanting,"Where can I see the mode parameter amount of the current pretraining model (GPT), will it be printed out?","I add the param:  `logparamsnorm \` but I can not find the print out.  I found it, but I do not think it represent the amount of mode parameters (maybe it is a norm param). Because I align the setting with GPT 3B, but it only print 599.041.   So where can i find the whole amount of mode parameters?",2023-06-14T06:00:06Z,enhancement question stale,closed,0,5,https://github.com/NVIDIA/Megatron-LM/issues/374,"`logparamsnorm` turns on logging of the L2 norm of all the model parameters, i.e. a scalar value. I don't understand what value you're looking for  do you mean the number of model parameters?",yes > I don't understand what value you're looking for  do you mean the number of model parameters?,"In the output log you should see something like this: `building GPT model ...                                                                                                                                                                                              > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 1277857792                                                                                                                                > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 1277857792                                                                                                                                > number of parameters on (tensor, pipeline) model parallel rank (4, 0): 1277857792                                                                                                                                > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1277857792                                                                                                                                > number of parameters on (tensor, pipeline) model parallel rank (6, 0): 1277857792                                                                                                                                > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 1277857792                                                                                                                                > number of parameters on (tensor, pipeline) model parallel rank (7, 0): 1277857792                                                                                                                                > number of parameters on (tensor, pipeline) model parallel rank (5, 0): 1277857792    ` We don't currently output the total number of parameters, but I'll record that as a feature request.","helloï¼Œ I find the number of parameters is smaller than the public amountï¼Œ for instanceï¼Œ when I use pretrain_gpt_distribute.shï¼Œ the GPT2 model, I can only get the number of parameters 254M, can you tell me why?  Part of my logs are shown belowï¼š **[after megatron is initialized] datetime: 20230822 03:42:58  building GPT model ... [20230822 03:42:58,537] [INFO] [utils.py:785:see_memory_usage] Before Building Model [20230822 03:42:58,538] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB  [20230822 03:42:58,538] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 39.97 GB, percent = 4.0% [20230822 03:42:58,550] [INFO] [utils.py:785:see_memory_usage] Before Building Model [20230822 03:42:58,551] [INFO] [utils.py:785:see_memory_usage] Before Building Model [20230822 03:42:58,551] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB  [20230822 03:42:58,551] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB  [20230822 03:42:58,551] [INFO] [utils.py:785:see_memory_usage] Before Building Model [20230822 03:42:58,551] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 39.97 GB, percent = 4.0% [20230822 03:42:58,551] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 39.97 GB, percent = 4.0% [20230822 03:42:58,552] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB  [20230822 03:42:58,552] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 39.97 GB, percent = 4.0% [20230822 03:42:58,562] [INFO] [utils.py:785:see_memory_usage] Before Building Model [20230822 03:42:58,562] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB  [20230822 03:42:58,563] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 39.97 GB, percent = 4.0% [20230822 03:42:58,584] [INFO] [utils.py:785:see_memory_usage] Before Building Model [20230822 03:42:58,585] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB  [20230822 03:42:58,585] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 39.98 GB, percent = 4.0% [20230822 03:42:58,635] [INFO] [utils.py:785:see_memory_usage] Before Building Model [20230822 03:42:58,636] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB  [20230822 03:42:58,637] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 40.0 GB, percent = 4.0% [20230822 03:42:58,652] [INFO] [utils.py:785:see_memory_usage] After Building Model [20230822 03:42:58,653] [INFO] [utils.py:786:see_memory_usage] MA 0.47 GB         Max_MA 0.47 GB         CA 0.57 GB         Max_CA 1 GB  [20230822 03:42:58,653] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 40.01 GB, percent = 4.0%  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 254109696**",Marking as stale. No activity in 60 days.
BoxiangW,Update gpt_dataset.py,Simplified _get_epoch function to avoid useless computation. Previous code waste time when tokens_per_epoch is small. ,2023-06-14T02:11:18Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/373,Marking as stale. No activity in 60 days.
cicirori,[Question] Why bf16 mode must use fp32 for accumulation and communication,I would like to know why bf16 must be used at the same time as accumulate_allreduce_grads_in_fp32? Is it for accuracy reasons or is it a historical problem due to older versions of nccl not supporting the bf16 format?,2023-06-13T05:39:42Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/372,I'm curious about that too.,"We found this to lead to more stable training before, but you could also try to perform the allreduce in `bf16` (it might hurt convergence but will be faster)."
haozhouamzn,Throughput of new release is slower than previous release on GPT2 pre-training,I'm running MegatronLM on GPT2 model. I found the current version has lower throughput than previous version in January 2023. I ran the same config over two versions and the **elapsed time per iteration (ms)** of current version is significantly (~60%) longer. Is there any hint what causes this model regression?  ```  Previous version  commit c92f10bdd19496ad67f802a5802cc3995640a66f  iteration 5/200 & tee $EXP_PATH/run_log & wait %1 ```,2023-06-09T15:56:48Z,regression,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/371,Just wanted to confirm that I have been able to replicate this issue. I can't think of an obvious cause but will profile and get back to you.,Got to the bottom of this. Sadly this is not actually a regression but instead the faster timing at the earlier commit was incorrect. You are using `recompute_granularity full` and there was a bug where each layer in the checkpointed transformer would get the same input. That bug was fixed here: https://github.com/NVIDIA/MegatronLM/commit/fc2c81d3a64c587590b9c3c5f0470e51c4ff4b26 So I believe the later commit is correct in both function and timing. Closing the issue.
bugggggggg,shingles error,,2023-06-09T15:37:38Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/370,Marking as stale. No activity in 60 days.
pengshuang,"Does the Megatron-LM support automatic selection of different types of network cards, such as Ethernet and high-speed IB network cards?","Our scenario involves two heterogeneous GPU clusters, Cluster A and Cluster B, each consisting of 20 GPU machines (A10080G). Cluster A is internally equipped with both highspeed IB cards and regular Ethernet cards, while Cluster B is internally equipped with highspeed RoCE cards and regular Ethernet cards. Due to the inability to establish a highspeed IB network between Cluster A and Cluster B, communication between them can only be done via Ethernet using TCP/IP (Socket). Our objective is to have machines within Cluster A and Cluster B connected through highspeed cards, while machines between Cluster A and Cluster B are connected through regular Ethernet networking. I would like to inquire whether Megatron supports automatic card detection and configuration to enable distributed training between the heterogeneous clusters. If it doesn't support it, is it possible to achieve this by modifying the PyTorch code (NCCL part) in MegatronLM? I am unsure of the feasibility and would greatly appreciate your response. Thank you very much!",2023-06-09T14:15:18Z,enhancement wontfix,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/369,"This isn't an interconnect configuration we currently support, and we don't have plans to support it in the foreseeable future. If you want to try implementing something like this I would recommend watching this presentation: https://www.nvidia.com/enus/ondemand/session/gtcfall22a41193/",Closing. I think the question is answered.
imhmhm,Should the length of batch sampler be the count of batches?,"https://github.com/NVIDIA/MegatronLM/blob/992da75a1fd90989eb1a97be8d9ff3eca993aa83/megatron/data/data_samplers.pyLL74C34L74C34 The ""MegatronPretrainingSampler"" and ""MegatronPretrainingRandomSampler"" are batch samplers, and according to the pytorch batch sampler definition in https://github.com/pytorch/pytorch/blob/664058fa83f1d8eede5d66418abff6e20bd76ca8/torch/utils/data/sampler.pyLL257C5L257C5, the length seems be the count of batches, however we have the length here to be the count of datasets as follows:  def __len__(self):         return self.total_samples Please correct me if this is a misunderstanding",2023-06-08T14:13:42Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/368,Marking as stale. No activity in 60 days.
xshaun,fix a bug on missing error message when loading checkpoints,"To support to print error message for each rank, not only rank0, when loading checkpoints.   The errors on nonzero ranks can cause a hang since the torch.distributed.barrier() is invoked and there is no any information on terminal.",2023-06-08T13:34:16Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/367,Marking as stale. No activity in 60 days.
TheGravityZero,ImportError: .../megatron/fused_kernels/build/scaled_upper_triang_masked_softmax_cuda.so: undefined symbol: _ZN3c106detail19maybe_wrap_dim_slowEllb,"Hello, I have this issue: ```   File """", line 583, in module_from_spec   File """", line 1043, in create_module   File """", line 219, in _call_with_frames_removed ImportError: .../megatron/fused_kernels/build/scaled_upper_triang_masked_softmax_cuda.so: undefined symbol: _ZN3c106detail19maybe_wrap_dim_slowEllb ``` My configuration: ``` apex                     0.1 torch                    1.11.0+cu113 torchaudio               0.11.0+cu113 torchfile                0.1.0 torchvision              0.12.0+cu113 ``` I will be glad for any help!",2023-06-08T09:19:15Z,help wanted,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/366,Did you move or update environments that you were running the code in? If so the problem may be that the fused_kernels need rebuilding for the new environment. Try deleting `.../megatron/fused_kernels/build` and rerunning. The fused_kernels will automatically recompile for the current environment.,Please try again with the latest `main`. This commit removes the need for compiling custom kernels and instead relies on the kernels from Apex.
etoilestar,PP/TP training about vit,"helloï¼Œit seems that you did not finish vit model with PP/TP in this project, I recently tried to write this code, can you give me some advice?",2023-06-08T07:54:29Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/365,Marking as stale. No activity in 60 days.,"Hi, this PR is too large for us to reasonably review unfortunately. We are working on adding TP/PP support for ViT models so we would welcome design suggestions. If you can create a targeted PR that just contains the changes for this functionality I'd be glad to take a look."
ghost,Simplified implementation of _build_doc_idx,"The original implementation of `_build_doc_idx` is recursive, but this recursion can be simplified to make the code clearer and easier to read.",2023-06-07T15:57:25Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/364,Marking as stale. No activity in 60 days.
kylematoba,AttributeError: module 'megatron.core.parallel_state' has no attribute 'broadcast_data',https://github.com/NVIDIA/MegatronLM/blob/e6d7e09845590d0a36bc7f29eb28db974fb8da4e/examples/detxoify_lm/finetune_gpt.pyL53 should be this https://github.com/NVIDIA/MegatronLM/blob/e6d7e09845590d0a36bc7f29eb28db974fb8da4e/megatron/core/tensor_parallel/data.pyL65.,2023-06-05T14:57:31Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/363,Marking as stale. No activity in 60 days.
miguelusque,Issue with Bert + documents with one sentece when generating train/validation/test indexes,"Hi, I have recently experienced an error when train to train Bert model with documents with a single sentence only. If I do not specify `bertnobinaryhead`, the script that generates the train/validation/test indexes from the dataset produces an error with NCCL in a multinode environment or it just does never finishes (endless loop?) when training in a single GPU. Could you please have a look at it? Thanks! I have copied the log details at NVIDIAs internal slack channel. Please DM me (Miguel MartÃ­nez) if you have further questions. Thanks! P.S.: It completely makes sense that we need to specify `bertnobinaryhead` when using documents with a single sentence, but it took me a while to notice it, and the log was not explicative at all. Thanks!!!!",2023-06-05T11:59:46Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/362,Marking as stale. No activity in 60 days.
Haijunlv,how to scale up zero1 data parallel world size when resume training ?,"When we use megatron to train llm, we would like to scale up nodes in training progress, such as from tp8 pp4 dp2(64 cards) to tp8 pp4 dp4(128 cards). But if we use zero1 at 64 nodes, optimizer state is splitted between dp=2. And we need to resplitted dp2 optimizer state between dp4 at 128 nodes. This demand is general in llm training. So Is there any solution or similar solution to handle this problem?",2023-06-03T08:24:53Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/361,"Please try again using the latest code base. The distributed optimizer now saves to a unified checkpoint, where all DP shards are concatenated into a single tensor."
17889982022,Swiglu activation may be wrong when TP > 1,"Firstly, appreciate for this amazing projectï¼ I want to use swiglu activation in the trianing of GPTï¼Œbut I found there may be a possible error in the calculation of swiglu when the size of TP comes to > 1. Compared to the traditional MLP(FFN) part in Transformersï¼Œthe swiglu can be seen having 3 linear projections, in which the whole calculation is `self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))`. In MegatronLM,  the ""gate_proj"" and ""up_proj""  are concatenated to a single linear named ""dense_h_to_4h"", and the output of ""dense_h_to_4h"" is then chunked into two parts to execute the above process using `F.silu(x[0]) * x[1])`. This works fine except the tensormodelparallelsize (TP) > 1. Take TP=2 as an example,  the weight of â€œdense_h_to_4h"" will be split into 2 partitions along column axis,  every gpu (or tp_rank) will hold a single partition to calculate its own output, in which the partition is then chunked into two parts again. That's the problem, the original meaning of 2 partitions in ""dense_h_to_4h"" should have referred to the ""gate_proj"" and ""up_proj"", but after the TP operationï¼Œ""gate_proj"" and ""up_proj"" have been separated into different tp_ranks. This leads  that it is the the output of ""gate_proj""  will be chunked into two part in tp_rank0 (or ""up_proj"" in tprank1) rather than ""dense_h_to_4h"" itself ,  which is not conform to the original calculation of swish. Hope to check if there is really indeed an error, thanks!",2023-06-02T13:01:14Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/360,"TP splits both gate_proj and up_proj up, so they become interleaved if you were to do a simple concatenation of the combined tensor. So tp_rank0 has [gate_proj_0, up_proj_0], and tp_rank_1 has [gate_proj_1, up_proj_1]. (This needs to be taken into consideration when changing the TP size). Or am I misunderstanding your concern?","Thanks for replying! TP should have gone to split â€œgate_proj"" itself (or ""up_proj "") into 2 ranks, but in MegatronLM TP split ""dense_h_to_4h"" (the concatenation of gate_proj and up_proj as I understand) into 2 ranks, results in that tp_rank0 holds ""gate_proj"" and tp_rank1 holds ""up_proj""ã€‚ It seems that â€œgate_proj"" (or ""up_proj "") is not splited along the TP dimension. More clearly, as you say, the correct calculation is tp_rank0 has [gate_proj_0, up_proj_0], and tp_rank_1 has [gate_proj_1, up_proj_1]. But it seems that MegatronLM doesn't work that way, I did an experimental comparison of the dense_h_to_4hâ€˜s outputs in both TP=1 and TP=2, and I found that when TP=2, tp_rank0 has [gate_proj_0, gate_proj_1] and tp_rank_1 has [up_proj_0, up_proj_1] That is what I was confused about before. Of course my understanding may also be wrong.  Appreciate again for your focus.","Sorry, I found the TP conversion in `tools/checkpoint_utils.py` had noticed the handling of this case, so there is nothing wrong with it.  In my experiment I use my own code to conduct the TP conversion, where I split the weight of `dense_h_to_4h` directly into the target TP size.  I overlooked the special treatment here, which is why the results are inconsistent."
MoFHeka,[Feat] Rebase main branch from NVIDIA/Megatron-LM including commit #37563bc and before.,Rebase main branch from NVIDIA/MegatronLM commit https://github.com/NVIDIA/MegatronLM/commit/37563bc1a6f4aa3aa552ae27a3e96f83cf842487,2023-05-31T19:34:22Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/359
zach-m,Update indexed_dataset.py,np.float deprecated in NumPy 1.20,2023-05-31T11:22:02Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/358,See CC(Update indexed_dataset.py).,This issue should be solved by this commit Thanks to  and  for the proposed solutions.
yingtongxiong,some questions about sequence parallel,"Hi, I have some questions about sequence parallel.  I am very confused about the last sequence parallel in transformer when I read your code. As we can see in this https://github.com/NVIDIA/MegatronLM/blob/main/megatron/model/transformer.pyL152 the last part in MLP, which is a rowParallel. If sequence parallel set to be True, there is a reduce and scatter operation in RowParallel https://github.com/NVIDIA/MegatronLM/blob/main/megatron/core/tensor_parallel/layers.pyL707. I consider this operation is prepared for the sequence parallel in the dropout after MLP as the above picture shows. However, I don't find any allgather operation after the dropout and the transformer layer output is subsequence as you can see in this https://github.com/NVIDIA/MegatronLM/blob/main/megatron/model/transformer.pyL896. Therefore, I am very confused about how can I get the entire transformer output(the output shape is (s, b, h)) and where is the code doing the allgather operation. Could you help me with thisï¼Ÿ",2023-05-30T07:14:09Z,,closed,0,5,https://github.com/NVIDIA/Megatron-LM/issues/357,It may be do the all reduce operations before dropout?,Thank you. I have got it. The allgather operation will be performed before or after calculating logits if sequence_parallel is True.,Closing  question appears to have been answered.,"You can find do allgather in forward pass and reducescatter in backward pass in ColumnParallelLinear, and do reducescatter in forward pass and allgather in backward pass in RowParallelLinear.  ColumnParallelLinear is exactly the entrance layer of Attention and MLP. RowParallelLinearis exactly the exit layer of Attention and MLP.","More specifically the input will be gathered in `linear_with_frozen_weight()`: ```     if sequence_parallel:         input = gather_from_sequence_parallel_region(input, tensor_parallel_output_grad=True)     else:         input = input ```"
learning-chip,Code fixes for local-storage-only environment,"In certain virtualized environment there is no shared storage. Both source code and data are stored (replicated) in each worker node's local storage. The code sections below only load data or build/load binaries on local directory. Other nodes cannot see master's file path. https://github.com/NVIDIA/MegatronLM/blob/cd2537d444792b487b1ab5a6fa685e09c9957409/megatron/data/gpt_dataset.pyL310L317 (same for dataset builders other than GPT) https://github.com/NVIDIA/MegatronLM/blob/cd2537d444792b487b1ab5a6fa685e09c9957409/megatron/initialize.pyL92L100 https://github.com/NVIDIA/MegatronLM/blob/cd2537d444792b487b1ab5a6fa685e09c9957409/megatron/initialize.pyL126L130 https://github.com/NVIDIA/MegatronLM/blob/cd2537d444792b487b1ab5a6fa685e09c9957409/megatron/initialize.pyL140L143 Changing `torch.distributed.get_rank()` to `torch.distributed.get_rank() % torch.cuda.device_count()` fixes the problem, by having one process on each node accessing its local disk. Of course here assumes each node has the same number of devices. Otherwise need to pass `os.environ['LOCAL_RANK']` from the main script to identify the local rank.",2023-05-30T04:36:06Z,,closed,4,4,https://github.com/NVIDIA/Megatron-LM/issues/356,Marking as stale. No activity in 60 days.,"I have a similar problem. My cluster has a relatively slow shared storage system, so I want to copy dataset to compute node temporary storage system. However, I found that, Megatron will first build a data index cache only on rank=0 gpu, so other node can not access this data cache file, which will return FileNotFoundError.",Marking as stale. No activity in 60 days.,"> I have a similar problem. My cluster has a relatively slow shared storage system, so I want to copy dataset to compute node temporary storage system. However, I found that, Megatron will first build a data index cache only on rank=0 gpu, so other node can not access this data cache file, which will return FileNotFoundError.  The same issue. Could you please advise on how to resolve it?"
wkcn,Bug Fixed when using activation checkpoints and rotary position embedding,"Hi there, In `megatron/model/transformer.py`, the signature of the function `ParallelAttention.forward` is ```python  In class ParallelAttention     def forward(self, hidden_states, attention_mask,                 encoder_output=None, enc_dec_attn_mask=None,                 inference_params=None, rotary_pos_emb=None): ``` When using activation checkpoints and rotary position embedding, this function is called as the following https://github.com/NVIDIA/MegatronLM/blob/main/megatron/model/transformer.pyL1174L1178 ```python                     hidden_states = tensor_parallel.checkpoint(                         custom(l, l + self.recompute_num_layers),                         self.distribute_saved_activations,                         hidden_states, attention_mask, encoder_output,                         enc_dec_attn_mask, rotary_pos_emb) ``` The variable `rotary_pos_emb` is passed as `inference_params` by mistake. In this PR, I correct the order of arguments.",2023-05-29T09:28:09Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/355,"Thanks for catching this, we don't run with full recompute much anymore so hasn't shown up for us. We've since added more args to the Layer's forward method that make this even worse. We are going to fix by making rotary_pos_emb passed view keyword (we should have from the start) instead of keeping it a positional arg and trying to get order correct. Fix should be coming soon.",Thanks  ! I close this PR.
mzamini92,Update ensemble_classifier.py,"Some improvements: In the `get_threshold` function, you check if one_threshold is True and then overwrite the `all_predictions` and `all_labels` dictionaries with combined values. Instead of modifying the dictionaries, you can handle the combined case separately to simplify the code. Since you are using `all_predictions`, `all_labels`, and `all_uid` as dictionaries to accumulate values, you can use `defaultdict` from the `collections` module to simplify the code. This will eliminate the need for multiple `if` conditions and make the code more concise. In the `calc_threshold` function, you calculate the `trials` list on each iteration, which is unnecessary. Move the calculation outside the loop to improve performance. you don't use the `preds.sum(1)` value in any subsequent calculations. Since it's not being used, you can remove this assertion to improve performance.",2023-05-29T00:24:51Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/354,Marking as stale. No activity in 60 days.
mzamini92,Update run_text_generation_server.py,"The `tryexcept` blocks in the while True loop have almost identical code.  Use argparse for argument parsing: Instead of defining a custom `add_text_generate_args` function, you can use the `argparse` module for argument parsing. It provides a more standard and flexible way to handle commandline arguments.",2023-05-29T00:08:55Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/353,Marking as stale. No activity in 60 days.
dumpmemory,try to add missing _copy_model_params_to_main_params function into distrib_optimizer.py,try to add _copy_model_params_to_main_params in distrib_optimizer,2023-05-22T16:49:17Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/352,"Thanks for the MR, we had made these changes ourselves and they should now be in the github repo.",Thanks
TUDelftHao,How does the flash attention work in megatron?,"Hi, I am a little bit confused about the usage of the flash attention module.  For a sequence input of length N, we usually concat several pieces of sentences to the length of N instead of masking 0 for more efficient use of memory. In normal cases, we calculate an attention mask beforehand to ensure only the correlation between desired texts, like what the llama model does. To my knowledge, we have to manually set the cumulative length (cu_seqlens_k, cu_seqlens_q) to record the start and end index of each text chunk of input if applying flash attention. However, in MegatronLM codebase, the param of cu_seqlens_k is set based on the seqlen_k by default: `cu_seqlens_k = torch.arange(0, (batch_size + 1) * seqlen_k, step=seqlen_k, dtype=torch.int32, device=q.device)` I have compared the losses generated between normal selfattention and flash attention from identical inputs, which are exactly the same. I am therefore confused by the function of cu_seqlens_k in flash attention module. Anyone who knows why this can work correctly? Or I have misunderstood the meaning of parameter cu_seqlens_k?",2023-05-22T13:21:50Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/351,Marking as stale. No activity in 60 days.
xshaun,A typing error on variable name in distrib_optimizer.py (param->modelâ€¦,"A typing error on variable name in distrib_optimizer.py (param>model_param), causing undefined variable error.",2023-05-21T07:24:43Z,stale,open,0,1,https://github.com/NVIDIA/Megatron-LM/issues/350,Marking as stale. No activity in 60 days.
liuslnlp,Fix two bug of finetuning with bf16 and finetuning with DistributedOptimizer,,2023-05-19T12:13:51Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/349,Thank you for the PR. I believe both of these changes are reflected in the current main branch.
miguelusque,Replace np.float by float (cont.),"np.float was deprecated in numpy 1.20 (https://numpy.org/devdocs/release/1.20.0notes.htmldeprecations), and MegatronLM fails when enabling logvalidationppltotensorboard) and using the latest version of numpy. This commit fixes that runtime error. Related to https://github.com/NVIDIA/MegatronLM/pull/347",2023-05-18T22:02:57Z,,closed,2,3,https://github.com/NVIDIA/Megatron-LM/issues/348,"numpy 1.24 removed `np.float`, now this PR becomes must. Otherwise will get `AttributeError: module 'numpy' has no attribute 'float'.`",See CC(Update indexed_dataset.py).,This issue should be solved by this commit Thanks to  and  for the proposed solutions.
miguelusque,Replace np.float by float,"np.float was deprecated in numpy 1.20 (https://numpy.org/devdocs/release/1.20.0notes.htmldeprecations), and MegatronLM fails when enabling logvalidationppltotensorboard) and using the latest version of numpy. This commit fixes that runtime error.",2023-05-18T18:58:41Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/347,See CC(Update indexed_dataset.py).,This issue should be solved by this commit Thanks to  and  for the proposed solutions.
ZeldaHuang,[Fix] mlp-bias concatenation in checkpoint_util when using swiglu,"When use swiglu, checkpoint_util.py only handle the mlp weight concatenation and forget to handle the mlp bias. It will cause incorrect inference result after merging checkpoints.",2023-05-17T13:39:28Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/346,Oops! Good catch! We hadn't used swiglu with bias. Thanks for the PR!
lvcc2018,How to freeze some params when training a GPT model?,"I tried to set params.required_grad to False when initializing the model, but some error in the optimizer occurred. So how to freeze some params correctly?",2023-05-15T12:28:11Z,enhancement stale,closed,0,11,https://github.com/NVIDIA/Megatron-LM/issues/345,+1,+1,"I made changes in three places: 1. Add `torch.no_grad()` in `TransformerLanguageModel.forward` when calling the encoder: ```python  Run encoder. args = get_args() if args.freeze_weight:   add `freezeweight` option in arguments.py     with torch.no_grad():         if enc_hidden_states is None:             if self.encoder is not None:                 encoder_output = self.encoder(                     encoder_input,                     enc_attn_mask,                     inference_params=inference_params)             else:                 encoder_output = self.encoder_hidden_state         else:             encoder_output = enc_hidden_states.to(encoder_input.dtype) else:      remain unchanged from the original code     if enc_hidden_states is None:         if self.encoder is not None:             encoder_output = self.encoder(                 encoder_input,                 enc_attn_mask,                 inference_params=inference_params)         else:             encoder_output = self.encoder_hidden_state     else:         encoder_output = enc_hidden_states.to(encoder_input.dtype) ``` 2. Prevent layernorm and rmsnorm from entering train mode by overriding `train` method of `MixedFusedLayerRMSNorm` and `MixedFusedLayerNorm`: ```python def train(self, mode):     args = get_args()     if args.freeze_weight:         super().train(False)         if mode:             print_rank_0(                 f""Attempt to set RMSNorm to train mode, but we set freeze_weight == True, so RMSNorm is still in eval mode"")         assert not self.training     else:         super().train(mode) ``` 3. Recursively set `required_grad = False` when loading state dict: ```python  in `TransformerLanguageModel.load_state_dict`  Encoder. if self.add_encoder:     ...     self.encoder.load_state_dict(state_dict_, strict=strict)     args = get_args()     if args.freeze_weight:         self.no_requires_grad(self.encoder) ``` ```python def no_requires_grad(self, module):     if len(module._modules.keys()) != 0:        for submodule_name, submodule in module._modules.items():             print(f""Enter {submodule_name}"")             self.no_requires_grad(submodule)     else:         print(f""Setting {module._get_name()}.requires_grad = False"")         module.requires_grad = False         print(f""Leave"") ``` I freeze all the parameters in `no_requires_grad`, except embedding and lm_head (output_dense), we can also selectively freeze the specified weight by judging the `submodule_name`. I think wrapping `torch.no_grad` and setting `require_grads = True` are the same, but I haven't tried enabling only one of them. If we only need to freeze partial parameters in one layer, we can manually clear gradients in a tensor after `loss.backward` and before `optimizer.step`. In the code below I extended tokenizer vocab_size, I want to freeze old embeddings&lm_head and only train new embeddings&lm_head, so I set their grad to 0 by `xxx.main_grad[] = 0`. ```python  in `train_step` losses_reduced = forward_backward_func(     forward_step_func, data_iterator, model,     optimizer, fwd_bwd_timers, forward_only=False) timers('forwardbackward').stop()  Empty unused memory. if args.empty_unused_memory_level >= 1:     torch.cuda.empty_cache()  Reduce gradients. optimizer.reduce_model_grads(args, timers)  Vision gradients. ...  Clean gradients of old embeddings if we are extending vocab if args.freeze_weight:     assert args.extend_vocab and args.old_vocab_size != 1     unwrapped_model = unwrap_model(model[0],                                    (torchDDP, LocalDDP, Float16Module))     embeddings = unwrapped_model._modules[""language_model""]._modules[""embedding""]._modules[""word_embeddings""]     lm_head = unwrapped_model._modules[""language_model""]._modules[""output_dense""]     new_vocab_start_index = embeddings.vocab_start_index     old_vocab_end_index = args.old_vocab_size  1     if old_vocab_end_index >= new_vocab_start_index:         embeddings._parameters['weight'].main_grad[:old_vocab_end_index  new_vocab_start_index + 1] = 0         lm_head._parameters['weight'].main_grad[:old_vocab_end_index  new_vocab_start_index + 1] = 0      Update parameters.     timers('optimizer', log_level=1).start(barrier=args.barrier_with_L1_time)     update_successful, grad_norm, num_zeros_in_grad = optimizer.step(args, timers)     timers('optimizer').stop() ``` If `tensormodelparallelsize` > 1, each layer of the loaded model is already split, so we need to pay special attention to the range to clear gradient.",Marking as stale. No activity in 60 days.,"I want to warmup train only the word embedding and lm head. I use pipeline parallel size == 4, and freeze encoder. That means there are no weights to train in two nodes, that causes the optimizer bug on these two nodes.  Do you have any ideas about this problem?"," Can you show the error message? In addition, I think tp=4 will distribute embedding and lmhead equally to each gpu. For example, when vocab size is 32000, theoretically the embedding size on each gpu is 32000/4=8000. Since alignment is 128 (controlled by `make_vocab_size_divisible_by`), The actual embedding size will be 8064. That means each gpu should have trainable parameters.",">  Can you show the error message? >  > In addition, I think tp=4 will distribute embedding and lmhead equally to each gpu. For example, when vocab size is 32000, theoretically the embedding size on each gpu is 32000/4=8000. Since alignment is 128 (controlled by `make_vocab_size_divisible_by`), The actual embedding size will be 8064. That means each gpu should have trainable parameters. thank you for your reply. tp=4 is ok for me. Here the problem is that i use pp=4 and tp=8 to train 70B model. pp=4 causes the problem. On node0 and node3, the initial is ok because there are word embedding and lm_head to train on each GPU. But on node1 and node2, there are only encoder params which are all freezed. log: ` [default3]:  File ""/root/code/M6BoYan/finetune_online.py"", line 117, in  [default3]:    pretrain( [default3]:  File ""/root/code/M6BoYan/megatron/training.py"", line 110, in pretrain [default3]:    model, optimizer, opt_param_scheduler = setup_model_and_optimizer( [default3]:  File ""/root/code/M6BoYan/megatron/training.py"", line 428, in setup_model_and_optimizer [default3]:    optimizer = get_megatron_optimizer(model, no_wd_decay_cond, [default3]:  File ""/root/code/M6BoYan/megatron/optimizer/__init__.py"", line 78, in get_megatron_optimizer [default3]:    optimizer = Adam(param_groups, [default3]:  File ""/usr/lib/python3.8/sitepackages/apex/optimizers/fused_adam.py"", line 81, in __init__ [default3]:    super(FusedAdam, self).__init__(params, defaults) [default6]:  File ""/usr/lib/python3.8/sitepackages/apex/optimizers/fused_adam.py"", line 81, in __init__ [default6]:    super(FusedAdam, self).__init__(params, defaults) [default6]:  File ""/usr/lib/python3.8/sitepackages/torch/optim/optimizer.py"", line 61, in __init__ [default6]:    raise ValueError(""optimizer got an empty parameter list"") [default3]:  File ""/usr/lib/python3.8/sitepackages/torch/optim/optimizer.py"", line 61, in __init__ [default3]:    raise ValueError(""optimizer got an empty parameter list"") `",">  Can you show the error message? >  > In addition, I think tp=4 will distribute embedding and lmhead equally to each gpu. For example, when vocab size is 32000, theoretically the embedding size on each gpu is 32000/4=8000. Since alignment is 128 (controlled by `make_vocab_size_divisible_by`), The actual embedding size will be 8064. That means each gpu should have trainable parameters. Andï¼Œi have a new question.  If I want to warm up train only some special token in the SFT stage(I found the initialization of special token which have not been pretrained influence a lot to the SFT training). Which means maybe only 2 word embedding weights and 2 lm_head weights need to train. In this case, there maybe also optimizer problem when using tp=8. ","I've only tested the code with pp=1. This problem does occur when pp>1, but I haven't thought of a good solution yet.  However, considering that you have frozen most of the model weights, you may try lowering the pp level to pp=1. If you use bf16 or fp16 for training, since most parameters are not involved in gradient calculation and optimizer state, I guess using tensor parallelism alone is enough for training the 70B model.","> I've only tested the code with pp=1. This problem does occur when pp>1, but I haven't thought of a good solution yet. >  > However, considering that you have frozen most of the model weights, you may try lowering the pp level to pp=1. If you use bf16 or fp16 for training, since most parameters are not involved in gradient calculation and optimizer state, I guess using tensor parallelism alone is enough for training the 70B model. thank you. pp=1 is ok for me, but the sequence length could not be set to 8192 due to the GPT memory",Marking as stale. No activity in 60 days.
jacob-crux,Modify LayerNorm to support other dtype inputs,"The Megatron code was recently updated to use apex's layernorm code. However, in the case of fp32, the Layer Nom code of the previous Megatron was considered, but the Apex code did not consider the Layer Nom, so I made a PR. In the case of code using `fp32residualconnection` in Megatron, fp32 dtype is used for layernorm input, but when learning with bf16 or fp16, there is a problem that layernorm of apex cannot be used. By using `FusedLayerNormAffineFunction` instead of `FusedLayerNormAffineFunction` within Apex, you can keep the same layernorm functionality previously used in Megatron.",2023-05-12T08:46:47Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/344,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 21 days.,No activity on stale PR in 21 days.,Marking as stale. No activity in 60 days.
jackychancjcjcj,RuntimeError: Address already in use,"I am running distributed pertaining script. And got problem as shown. Any suggestions? thanks. My config: MASTER_ADDR=localhost MASTER_PORT=8514 NNODES=1 NODE_RANK=0 WORLD_SIZE=$(($GPUS_PER_NODE*$NNODES)) DISTRIBUTED_ARGS=""nproc_per_node $GPUS_PER_NODE nnodes $NNODES node_rank $NODE_RANK master_addr $MASTER_ADDR master_port $MASTER_PORT"" python m torch.distributed.launch $DISTRIBUTED_ARGS ...... File ""/megatron/initialize.py"", line 181, in _initialize_distributed torch.distributed.init_process_group( File ""/root/miniconda3/lib/python3.8/sitepackages/torch/distributed/distributed_c10d.py"", line 520, in init_process_group store, rank, world_size = next(rendezvous_iterator) File ""/root/miniconda3/lib/python3.8/sitepackages/torch/distributed/rendezvous.py"", line 142, in _tcp_rendezvous_handler store = TCPStore(result.hostname, result.port, world_size, start_daemon, timeout) RuntimeError: Address already in use",2023-05-11T23:53:14Z,help wanted stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/343,Can you trying using a different MASTER_PORT? This error usually means that that port is already in use.,Marking as stale. No activity in 60 days.,"Hopefully using a different master port worked for you. I'm going to close this, but feel free to reopen if you are still running into issues."
Aidyn-A,[Parallel State] Manipulate NCCL_NET for hybrid IB/Socket setups,Landing an APEX commit https://github.com/NVIDIA/apex/pull/1620 Also related https://github.com/NVIDIA/NeMo/issues/6625issuecomment1542882983,2023-05-11T17:03:22Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/341,", I believe Roce is classified as the IB, and if I am right you can use it. Please double check the NCCL documentation.",Marking as stale. No activity in 60 days.
li-yi-dong,Fix calling torch.TypedStorage._untyped,"I ran the main branch with PyTorch1.13.1, and get this exception: ``` Traceback (most recent call last):   File ""/checkpoint/binary/train_package/pretrain_gpt.py"", line 117, in      pretrain(train_valid_test_datasets_provider, model_provider,   File ""/checkpoint/binary/train_package/megatron/training.py"", line 111, in pretrain     model, optimizer, opt_param_scheduler = setup_model_and_optimizer(   File ""/checkpoint/binary/train_package/megatron/training.py"", line 375, in setup_model_and_optimizer     optimizer = get_megatron_optimizer(model, no_wd_decay_cond,   File ""/checkpoint/binary/train_package/megatron/optimizer/__init__.py"", line 128, in get_megatron_optimizer     return opt_ty(optimizer,   File ""/checkpoint/binary/train_package/megatron/optimizer/distrib_optimizer.py"", line 391, in __init__     param_buffer = torch.tensor(grad_buffer.data.storage()._untyped(), AttributeError: 'torch.storage.TypedStorage' object has no attribute '_untyped' ``` So, I turn to PyTorch docs. It seems that this interface has become a public interface since 1.13 https://pytorch.org/docs/stable/storage.htmltorch.TypedStorage.untyped",2023-05-11T01:50:17Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/340,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,No activity on stale PR in 21 days.
etoilestar,"hi, I have a question about vit","Hello, is the vit pipeline parallel implemented in megatron? thank you.",2023-05-10T15:05:36Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/339,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,"okay, thank you",ViT PP support is not yet implemented. We are working on it internally though. Hopefully there'll be support in O(weeks).
xyang2013,Import error,"Hi, I am having the following error: from tools.retro.utils import get_args_path as get_retro_args_path ModuleNotFoundError: No module named 'tools.retro' It is fine in the IDE but gives this error during execution. Any suggestion? Thanks",2023-05-10T12:59:50Z,bug stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/338,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days., Can you explain what command you were running from the command line that ended up calling `from tools.retro.utils import get_args_path as get_retro_args_path`? Thanks,Marking as stale. No activity in 60 days.
LydiaXiaohongLi,support llama pretraining,,2023-05-09T15:13:56Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/337
drcege,Fix duplicate init_process_group,The behavior seems to be improperly changed in f3e57f6fee62909eed55f43a321031ad0664e600   ,2023-05-09T07:43:13Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/336,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,No activity on stale PR in 21 days.,Marking as stale. No activity in 60 days.
jacksonchen1998,[UPDATE] README.md,Add related paper called Evaluating Parameter Efficient Learning for Generation.,2023-05-08T15:50:43Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/335,Merged: de72f85d1e587b9ed8fdda30e66a40a9097717f5  thanks
jacksonchen1998,[UPDATE] README.md,Add related paper called Evaluating Parameter Efficient Learning for Generation.,2023-05-08T15:45:30Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/334
nullnonenilNULL,AttributeError: 'DistributedOptimizer' object has no attribute '_copy_model_params_to_main_params',when i finetune model from checkpoint with usedistributedoptimizerï¼Œi got the error as the title. ,2023-05-05T09:42:03Z,,closed,0,6,https://github.com/NVIDIA/Megatron-LM/issues/333,"same problem, seems like only `Float16OptimizerWithFloat16Params` implements `_copy_model_params_to_main_params`",same problem,same problem,would u mind testing this pr https://github.com/NVIDIA/MegatronLM/pull/352 ?,This should be fixed in the latest main branch.,> This should be fixed in the latest main branch. Thanks bro
2catycm,"Which data loading strategy is the fastest? cached , mmap, or lazy? ","1. Which data loading strategy is the fastest? cached , mmap, or lazy? Or, what aspect it depends on machines? There isn't any documentation on this, and I cannot understand how they work respectively.  On the Internet I found that mmap is memory map, a system call to pin the data content from disk to memory, but what is ""cached"" and what is ""lazy""? 2. Besides, why do we need to specify the way of loading strategy in preprocess_data.pyï¼Ÿ I thought it was just about how to load data, but not about how to preprocess it.",2023-05-02T20:27:25Z,,closed,2,1,https://github.com/NVIDIA/Megatron-LM/issues/332,We actually only support `mmap` during training. The presence of the other options is an unsupported carryover from fairseq. There is an incoming update that will remove the other options from preprocessing and training and update the README.md accordingly. 
timmoon10,Optimize communication in interleaved pipeline parallelism,"This is work toward restoring NeMoMegatron support for interleaved pipeline parallelism after https://github.com/NVIDIA/NeMo/pull/6393. In particular, it ports some changes from the Apex fork of Megatron:  Adds support for custom communication (e.g. for the Apex distributed optimizer). This is for the nonpipeline, noninterleaved pipeline, and interleaved pipeline cases.  Optimizes performance for interleaved pipeline parallelism by aligning communication over the pipeline parallel group to avoid GPU idling. See https://github.com/NVIDIA/apex/pull/1611. This is intended to have minimal effect on existing downstream code. Pinging .",2023-05-02T00:49:59Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/331
szhengac,[Bug] Transformer engine layer is very slow,"Hi, it seems the transformer engine layer is buggy and I obtained very poor performance when benchmarking a 22B GPT throughput on a single 8xH100 SXM5 box. The 22B parameter model and training configurations are borrowed from paper https://arxiv.org/abs/2205.05198. The numbers are shown as bellow local implementation with bf16: `311 TFLOPS` transformer engine implementation by adding `transformerimpl 'transformer_engine'` to the argument: `87 TFLOPS` transformer engine implementation with FP8 with hybrid format: `358 TFLOPS`",2023-05-01T22:15:29Z,stale,closed,0,12,https://github.com/NVIDIA/Megatron-LM/issues/330,"Hey, could you answer the following regarding your setup?  What are the settings you're using for parallelism?    Data parallel size?    Tensor parallel size? Is sequence parallel enabled?    Pipeline parallel size?   What are you e2e per step timings for the 3 settings (FP8 vs local BF16 vs TE bf16)?","  I use the same configuration as in the paper. Specifically: DP=1, TP=8, PP=1. Sequence parallel is enabled. The TFLOPS I show above reflect the step time, since I calculated TFLOPS based on Megatron log output.", the example launch script is here: https://github.com/szhengac/MegatronLM/blob/main/examples/benchmark_gpt3.sh,Could you try with the environment variable `NVTE_BIAS_GELU_NVFUSION` set to 0?  `export NVTE_BIAS_GELU_NVFUSION=0`," I lost the access to H100 and I will see if I can get it back. At the same time, do you have any machines for testing?", I can confirm that setting `NVTE_BIAS_GELU_NVFUSION=0` fixes the performance. So bias gelu fusion is buggy.," I also try to use the fp8 in my 5b model training using 8 h100sxm4 card.But the memroy consumption is much larger than fp16, do you have any ideas about the reason?","Larger memory consumption, and slower speed in MegatronLM training. Any advice to check the transformerengine working properly?",Same problem.,Marking as stale. No activity in 60 days.,same large memory consumption problem observed with fp8,Marking as stale. No activity in 60 days.
loadams,deleted megatron-lm packages on pypi,"It appears that the megatronlm packages that used to exist on pypi were removed.  Now they're been replaced with two packages, 2.2.2 that seems to be a replacement for 1.1.5 which is useful, but the package doesn't appear to contain anything, so we cannot use it as a replacement for 1.1.5",2023-05-01T21:11:23Z,question,closed,0,11,https://github.com/NVIDIA/Megatron-LM/issues/329,"This also causes a build issue for us. Our project uses Nvidia NeMo v1.3.0, which depends on megatronlm 2.2.0. The obvious solution in this case would be for us to update the NeMo library to a more recent version that seems to remove the megatronlm dependency, but doing so is nontrivial for us due to a mishmash of version requirements. ","  depending on your version, you may be able to use the git tag 2.2.0 in your requirements.txt, but I'm still not sure why they pulled these packages.",Marking as stale. No activity in 60 days.,It might be stale but this is still a problem for existing megatron users.,Looking into it.,The megatronlm pypi packages were for versions of megatronlm over two years old. NeMO has since switched to using the megatroncore pypi package. The old packages were removed from pypi to prevent confusion between megtronlm and megatroncore packages.  If you need to use an old version of NeMO and/or megatronlm you can pull an old megatronlm commit and build from source. Let me know if you need help doing that.,"Hi barker  thanks so much for your reply and that makes sense for why the packages were pulled.  I can do that, but I'm not sure what commit the 1.1.5 package was built from.  If you know, could you share that here?  ", Thanks for understanding. I'm trying to track down that git commit ID for you..., I believe this is the commit you want for pip 1.1.5: https://github.com/NVIDIA/MegatronLM/commit/d80433e1a0dd78a3bfd70e7e72d51d90169139fc,"Thanks, I'll give that a try","Hi barker, I am curious what was the reason for change in packaging strategy for megatronlm. It seems 4 years ago all of `megatron` directory was a package but then in Sep 2022, some of the code was moves to subdirectory `core` and only `core` was made a package. Also, NeMO docker doesn't do `pip install` but modifies `PYTHONPATH`. Any idea what were the reasons?"
MubarakHAlketbi,Update clip_grads.py,"The torch._six module was an internal PyTorch module used to provide compatibility between Python 2 and Python 3. However, since Python 2 reached its endoflife on January 1, 2020, and PyTorch dropped support for Python 2, the torch._six module was removed.",2023-04-29T14:28:26Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/328,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,No activity on stale PR in 21 days.,Fixed here: https://github.com/NVIDIA/MegatronLM/commit/f7056ab0da255378be37a8427414bd65dc9637f6  thanks
MubarakHAlketbi,Update indexed_dataset.py,`np.float` was a deprecated alias for the builtin `float`.,2023-04-29T14:20:10Z,,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/327,Any updates on this PR? Numpy 1.24 has already removed the support for `np.float` leading to import error for MegatronLM.,"Actually MegatronLM always had a bug here. Indeed `np.float` is the same as `np.float64`; it is also the same as `np.double`, which can be seen in the directly following line, but specifying a different byte size. According to the byte size, what was previously `np.float` should actually be `np.float32` or (for the name analogue of `np.double`) `np.single`. In Python >=3.7 and maybe even earlier versions, a later entry in a dictionary overrides an earlier entry if it has the same key (as was the case for `np.float` and `np.double` here), so the resulting code was still working correctly (returning a byte size of 8Â â€“ the correct valueÂ â€“ for `np.float`).","However, changing `np.float` to `np.single` will lead to errors on existing datasets because of the `dtypes` dictionary and the integer value that is saved in previously created indexed datasets. If you make the change from `np.float64` (what was previously `np.float`) to `np.single` (or `np.float32`), I would also propose the following change to this PR: To handle backwardcompatibility while also fixing the previously described bytesize bug, change the line `7: np.double` (in the `dtypes` dictionary) to `7: np.single` and the line before that to `6: np.double`. That way, you achieve full backwardcompatibility while also making indexed datasets actually handle singleprecision floats.",This issue should be solved by this commit Thanks to  and  for the proposed solutions.
ktaebum,Safe assertion for interleaved pipeline,"Simple edge case of the current implementation: When training GPT13B model that has 40 Transformer layers with  `pipeline_model_parallel_size`: 8  `num_layers_per_virtual_pipeline_stage`: 2 It does not fail although `40 % (8 * 2) != 0`. Consequently, each model chunk has two layers (missing 1 layer per each device).",2023-04-26T06:40:56Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/326,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,No activity on stale PR in 21 days.,Marking as stale. No activity in 60 days.
liuzhenhai93,polish 1f1b interleaved schedule logic,polish some code about 1f1b interleaved schedule that are hard to sink in ,2023-04-25T16:11:00Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/325,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,No activity on stale PR in 21 days.,Marking as stale. No activity in 60 days.
etoilestar,how to get parameter amount?,"hello, can you tell me how to get parameter amount and model size when I want to train a specific model? thank you.",2023-04-24T11:56:35Z,duplicate,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/324,"Closing as duplicate of CC(Where can I see the mode parameter amount of the current pretraining model (GPT), will it be printed out?) "
Dylancer1998,sequence parallel backward mode,"The paper states that in the sequence parallel mode, during the backward pass of ColumnParallelLinear, only one reduce_scatter communication is performed. However, when looking at the code, there is an additional all_gather operation on the inputs. Is this necessary? Paper: https://arxiv.org/abs/2205.05198 Code: https://github.com/NVIDIA/MegatronLM/blob/3db2063b1ff992a971ba18f7101eecc9c4e90f03/megatron/core/tensor_parallel/layers.pyL261",2023-04-24T00:44:35Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/323,Marking as stale. No activity in 60 days.
digger-yu,Update README.md,Spelling errors change eliminted to eliminated,2023-04-23T12:12:40Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/322
query1988,Update pretrain_gpt.py,,2023-04-23T07:23:06Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/321,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,No activity on stale PR in 21 days.,This PR appears to add nonEnglish comments which we won't merge into `main`
yejunguo,transformer.py: pass the missed scale param to FlashAttention,"for the case that self.apply_query_key_layer_scaling is enabled, the scale param is missed to FlashAttention",2023-04-23T01:59:44Z,,closed,0,10,https://github.com/NVIDIA/Megatron-LM/issues/320,"once https://github.com/NVIDIA/MegatronLM/pull/317 is merged, we can refine the code to calculate self.norm_factor at init function for both CoreAttention and FlashAttention.","Hi  , could you help to review this PR, it is relative to flash attention, thanks.","I don't think this is right, and there's no need to change the scale passed to FlashAttention. The existing code (without FlashAttention) always computes softmax(QK^T / sqrt(headdim)). If `self.apply_query_key_layer_scaling`, it first computes QK^T / (sqrt(headdim) * layer_number). Then it calls FusedScaleMaskSoftmax with `coeff=layer_number`, which means that it scales the input by `layer_number` then performs softmax. Overall result is equivalent to softmax(QK^T / sqrt(headdim)). You can print out the output to verify.","thanks  for the quick reply. I agree the outputs are basically same (except the dtype conversion) when self.apply_query_key_layer_scaling is true, but, how do you think about the existing code in the CoreAttention?   It first computes QK^T / (sqrt(headdim) * layer_number), and then multiple layer_number back in FusedScaleMaskSoftmax.  It actually does nothing with self.apply_query_key_layer_scaling except the dtype conversion.  IMHO, we first need https://github.com/NVIDIA/MegatronLM/pull/317 to fix it.","Mathematically it's scaling by 1/layer_number, then multiply by layer_number. However, for numerical stability, this might be preferred, so that the product QK^T doesn't overflow for fp16. I believe that's the original motivation of Megatron's implementation. You can check with the MegatronLM folks.","thanks  , good point. apply_query_key_layer_scaling is introduced with 691747b1329880193665493536136f0f839a2674  ``` commit 691747b1329880193665493536136f0f839a2674 Author: Mohammad Shoeybi  Date:   Mon Jan 13 20:10:08 2020 0800     added querykey layer scaling and softmax fp32 option ```  could you help to confirm the motivation of this option? Is it used to avoid the possible overflow issue by fp16? thanks.","looks that shoeybi at github and Mohammad Shoeybi at nvidia might not the same person. Just adding the recent committers for comment,    nv     do you happen to know the motivation of https://github.com/NVIDIA/MegatronLM/commit/691747b1329880193665493536136f0f839a2674, is it used to avoid the possible overflow issue by fp16? thanks","hi,    nv   do you happen to know the motivation of https://github.com/NVIDIA/MegatronLM/commit/691747b1329880193665493536136f0f839a2674, is it used to avoid the possible overflow issue by fp16? thanks","did not get any more comment, and I totally agree with , it is used to avoid the possible overflow issue for fp16, will close the two PRs.",closing
2catycm,â€œnvc++ is the only NVHPC compiler that is supported.â€ when compiling fused kernels. ," Step to reproduce 1. git clone this  2. Fix Issue CC(Megtaron code incompatibility with pytorch 2.0) and a similar problem(np.float should be np.float64 or np.float32 here) 3. Copy examples/`pretrain_gpt_distributed_with_mp.sh` out to the project root directory 4. set `pretrain_gpt_distributed_with_mp.sh`  properly, fill in the paths. 5. run `pretrain_gpt_distributed_with_mp.sh`  Then you can see when compiling the c++ files in megatron/fused kernels, the build.ninja fails to use nvcc to build `scaled_upper_triang_masked_softmax_cuda.cuda.o` and `scaled_upper_triang_masked_softmax_cuda.cuda.o`.   My Environment  Software Python version: 3.11.0 Pytorch: '2.1.0.dev20230421+cu118' or '2.0.0+cu118' CXX: g++ 8.5.0 NVIDIA LIBRARIES: we used nvhpc/22.11, which contains cuda, nccl etc.  nccl version: (2, 17, 1) integrated in nvhpc  cuda version: 11.8 integrated  in nvhpc  mpi4: openmpi4.0.5 in nvhpc  Hardware 8 A100 GPUs linked with NVLink, on 2 NUMA  nodes.  ```log 	[4mGPU0	GPU1	GPU2	GPU3	GPU4	GPU5	GPU6	GPU7	mlx5_0	CPU Affinity	NUMA Affinity[0m GPU0	 X 	NV12	NV12	NV12	NV12	NV12	NV12	NV12	NODE	063	0 GPU1	NV12	 X 	NV12	NV12	NV12	NV12	NV12	NV12	NODE	063	0 GPU2	NV12	NV12	 X 	NV12	NV12	NV12	NV12	NV12	NODE	063	0 GPU3	NV12	NV12	NV12	 X 	NV12	NV12	NV12	NV12	NODE	063	0 GPU4	NV12	NV12	NV12	NV12	 X 	NV12	NV12	NV12	SYS	64127	1 GPU5	NV12	NV12	NV12	NV12	NV12	 X 	NV12	NV12	SYS	64127	1 GPU6	NV12	NV12	NV12	NV12	NV12	NV12	 X 	NV12	SYS	64127	1 GPU7	NV12	NV12	NV12	NV12	NV12	NV12	NV12	 X 	SYS	64127	1 mlx5_0	NODE	NODE	NODE	NODE	SYS	SYS	SYS	SYS	 X 		 ```  Problem  ```log [1/3] /share/nvhpc/Linux_x86_64/22.11/cuda/11.8/bin/nvcc  ccbin /share/nvhpc/Linux_x86_64/22.11/compilers/bin/nvc DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda DTORCH_API_INCLUDE_EXTENSION_H DPYBIND11_COMPILER_TYPE=\""_gcc\"" DPYBIND11_STDLIB=\""_libstdcpp\"" DPYBIND11_BUILD_ABI=\""_cxxabi1011\"" isystem /work/asc22/miniconda3/envs/yllm/lib/python3.11/sitepackages/torch/include isystem /work/asc22/miniconda3/envs/yllm/lib/python3.11/sitepackages/torch/include/torch/csrc/api/include isystem /work/asc22/miniconda3/envs/yllm/lib/python3.11/sitepackages/torch/include/TH isystem /work/asc22/miniconda3/envs/yllm/lib/python3.11/sitepackages/torch/include/THC isystem /share/nvhpc/Linux_x86_64/22.11/cuda/11.8/include isystem /work/asc22/miniconda3/envs/yllm/include/python3.11 D_GLIBCXX_USE_CXX11_ABI=0 D__CUDA_NO_HALF_OPERATORS__ D__CUDA_NO_HALF_CONVERSIONS__ D__CUDA_NO_BFLOAT16_CONVERSIONS__ D__CUDA_NO_HALF2_OPERATORS__ exptrelaxedconstexpr gencode=arch=compute_80,code=compute_80 gencode=arch=compute_80,code=sm_80 compileroptions 'fPIC' O3 gencode arch=compute_70,code=sm_70 use_fast_math U__CUDA_NO_HALF_OPERATORS__ U__CUDA_NO_HALF_CONVERSIONS__ exptrelaxedconstexpr exptextendedlambda gencode arch=compute_80,code=sm_80 gencode arch=compute_90,code=sm_90 std=c++17 c /work/asc22/yecanming/repos/yllm/MegatronLM/megatron/fused_kernels/scaled_upper_triang_masked_softmax_cuda.cu o scaled_upper_triang_masked_softmax_cuda.cuda.o  FAILED: scaled_upper_triang_masked_softmax_cuda.cuda.o  nvcc fatal   : Unsupported NVHPC compiler found. nvc++ is the only NVHPC compiler that is supported. ```",2023-04-22T13:23:01Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/319,"From my perspective, when executing"" /share/nvhpc/Linux_x86_64/22.11/cuda/11.8/bin/nvcc  ccbin /share/nvhpc/Linux_x86_64/22.11/compilers/bin/nvc ....."" The problem is that the file being compiled is c++ file, but ccbin  is set to be ""nvc"". It should be set to ""nvc++"" instead. ","The problem origins to `fused_kernels/__init__.py` , but I don't know where in `fused_kernels/__init__.py`  should we specify `ccbin`. I tried to change  ""extra_cuda_cflags=['O3',                                'gencode', 'arch=compute_70,code=sm_70',                                'use_fast_math'] + extra_cuda_flags + cc_flag,"" into ""extra_cuda_cflags=['O3',                                'gencode', 'arch=compute_70,code=sm_70',                                'use_fast_math', 'ccbin', 'nvc++'] + extra_cuda_flags + cc_flag,"""" But it results in double appearance of ""ccbin""","Solved.  The solution is not to use nvhpc.  Instead, use CUDA.x pytorch built with 11.x and nccl built on the same cuda. make sure export $CC=g++, but not nvcc, nvc++, nvc, icx  Otherwise the extension can never be compiled. torch2.0 is not supported too. "
ktaebum,pass the correct dropout prob when using FlashAttention,A recent fix at cdd2afdf22a8d3490b5c9091c957449765519fdc performs stateful modification of the dropout probability of the Attention layer. This could lead to incorrect behavior after the first evaluation performed during training.,2023-04-20T07:15:15Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/318,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,No activity on stale PR in 21 days.,Marking as stale. No activity in 60 days.
yejunguo,transformer.py: do not handle query-key-layer-scaling again in FusedSâ€¦,"â€¦caleMaskSoftmax In CoreAttention.__init__         self.norm_factor = math.sqrt(self.hidden_size_per_attention_head)         if self.apply_query_key_layer_scaling:             coeff = self.layer_number             self.norm_factor *= coeff In CoreAttention.forward         matmul_result = torch.baddbmm(             matmul_input_buffer,             query_layer.transpose(0, 1),    [b * np, sq, hn]             key_layer.transpose(0, 1).transpose(1, 2),   [b * np, hn, sk]             beta=0.0, alpha=(1.0/self.norm_factor)) query_key_layer_scaling is already handled in torch.baddbmm(alpha), and so don't need to handle again in FusedScaleMaskSoftmax which actually multiples the self.layer_number back, see detail in below code In CoreAttention.__init__         self.scale_mask_softmax = FusedScaleMaskSoftmax(             self.fp16, self.bf16,             self.attn_mask_type,             args.masked_softmax_fusion,             attention_mask_func,             self.attention_softmax_in_fp32,             coeff) In FusedScaleMaskSoftmax.__init__     def __init__(         self,         input_in_fp16,         input_in_bf16,         attn_mask_type,         scaled_masked_softmax_fusion,         mask_func,         softmax_in_fp32,         scale,     ):         super(FusedScaleMaskSoftmax, self).__init__()         ...         self.scale = scale In FusedScaleMaskSoftmax.forward_torch_softmax         if self.scale is not None:             input = input * self.scale So, roughly in math, it is qk * 1 /(sqrt(s_per_h) * l_num) * l_num equals qk * 1 / sqrt(s_per_h) besides the dtype change, it means that querykeylayerscaling contributes little with current implementation.",2023-04-20T05:07:59Z,,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/317,"to make the change clear, this PR is very simple, once it is approved/merged, I'll create another PR to clean the code.",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,No activity on stale PR in 21 days.,closing as discussed at https://github.com/NVIDIA/MegatronLM/pull/320issuecomment1617076514
etoilestar,Question about distribute training ,"hello, I just trained using pretrained_gpt_distribute.sh, but I meet Bus error: nonexistent physical address with signal 7,could you help me? hardware: rtx2080ti, docker:pytorch:22.03py3, dataset:webtext",2023-04-20T01:42:22Z,,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/316," > elasped time to build and save shuffleidx mapping (seconds): 0.064410 ceceb0c7f2e6:454:838 [0] NCCL INFO Using network Socket ceceb0c7f2e6:455:839 [1] NCCL INFO Using network Socket ceceb0c7f2e6:454:838 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1. ceceb0c7f2e6:454:838 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1. ceceb0c7f2e6:454:838 [0] NCCL INFO Could not enable P2P between dev 1(=1c000) and dev 0(=1b000) ceceb0c7f2e6:454:838 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1. ceceb0c7f2e6:454:838 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1. ceceb0c7f2e6:454:838 [0] NCCL INFO Could not enable P2P between dev 0(=1b000) and dev 1(=1c000) ceceb0c7f2e6:454:838 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1. ceceb0c7f2e6:454:838 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1. ceceb0c7f2e6:454:838 [0] NCCL INFO Could not enable P2P between dev 1(=1c000) and dev 0(=1b000) ceceb0c7f2e6:454:838 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1. ceceb0c7f2e6:454:838 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1. ceceb0c7f2e6:454:838 [0] NCCL INFO Could not enable P2P between dev 0(=1b000) and dev 1(=1c000) ceceb0c7f2e6:454:838 [0] NCCL INFO Setting affinity for GPU 0 to 0fffff00,000fffff ceceb0c7f2e6:455:839 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1. ceceb0c7f2e6:455:839 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1. ceceb0c7f2e6:455:839 [1] NCCL INFO Could not enable P2P between dev 1(=1c000) and dev 0(=1b000) ceceb0c7f2e6:455:839 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1. ceceb0c7f2e6:455:839 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1. ceceb0c7f2e6:455:839 [1] NCCL INFO Could not enable P2P between dev 0(=1b000) and dev 1(=1c000) ceceb0c7f2e6:455:839 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1. ceceb0c7f2e6:455:839 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1. ceceb0c7f2e6:455:839 [1] NCCL INFO Could not enable P2P between dev 1(=1c000) and dev 0(=1b000) ceceb0c7f2e6:455:839 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1. ceceb0c7f2e6:455:839 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1. ceceb0c7f2e6:455:839 [1] NCCL INFO Could not enable P2P between dev 0(=1b000) and dev 1(=1c000) ceceb0c7f2e6:455:839 [1] NCCL INFO Setting affinity for GPU 1 to 0fffff00,000fffff ceceb0c7f2e6:454:838 [0] NCCL INFO Channel 00/02 :    0   1 ceceb0c7f2e6:455:839 [1] NCCL INFO Trees [0] 1/1/1>1>0 [1] 1/1/1>1>0 ceceb0c7f2e6:454:838 [0] NCCL INFO Channel 01/02 :    0   1 ceceb0c7f2e6:454:838 [0] NCCL INFO Trees [0] 1/1/1>0>1 [1] 1/1/1>0>1 ceceb0c7f2e6:455:839 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1. ceceb0c7f2e6:455:839 [1] NCCL INFO Could not enable P2P between dev 1(=1c000) and dev 0(=1b000) ceceb0c7f2e6:454:838 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1. ceceb0c7f2e6:454:838 [0] NCCL INFO Could not enable P2P between dev 0(=1b000) and dev 1(=1c000) ceceb0c7f2e6:454:838 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1. ceceb0c7f2e6:454:838 [0] NCCL INFO Could not enable P2P between dev 0(=1b000) and dev 1(=1c000) [ceceb0c7f2e6:454  :0:838] Caught signal 7 (Bus error: nonexistent physical address) ceceb0c7f2e6:455:839 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1. ceceb0c7f2e6:455:839 [1] NCCL INFO Could not enable P2P between dev 1(=1c000) and dev 0(=1b000) [ceceb0c7f2e6:455  :0:839] Caught signal 7 (Bus error: nonexistent physical address) ==== backtrace (tid:    839) ====  0 0x0000000000043090 killpg()  ???:0  1 0x000000000018bb41 __nss_database_lookup()  ???:0  2 0x000000000007587d ncclGroupEnd()  ???:0  3 0x000000000007b0ef ncclGroupEnd()  ???:0  4 0x0000000000059e97 ncclGetUniqueId()  ???:0  5 0x00000000000489b1 ???()  /usr/lib/x86_64linuxgnu/libnccl.so.2:0  6 0x000000000004a655 ???()  /usr/lib/x86_64linuxgnu/libnccl.so.2:0  7 0x0000000000063dcc ncclRedOpDestroy()  ???:0  8 0x0000000000008609 start_thread()  ???:0  9 0x000000000011f133 clone()  ???:0 ================================= ==== backtrace (tid:    838) ====  0 0x0000000000043090 killpg()  ???:0  1 0x000000000018bb41 __nss_database_lookup()  ???:0  2 0x000000000007587d ncclGroupEnd()  ???:0  3 0x000000000007b0ef ncclGroupEnd()  ???:0  4 0x0000000000059e97 ncclGetUniqueId()  ???:0  5 0x00000000000489b1 ???()  /usr/lib/x86_64linuxgnu/libnccl.so.2:0  6 0x000000000004a655 ???()  /usr/lib/x86_64linuxgnu/libnccl.so.2:0  7 0x0000000000063dcc ncclRedOpDestroy()  ???:0  8 0x0000000000008609 start_thread()  ???:0  9 0x000000000011f133 clone()  ???:0 ================================= ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 7) local_rank: 0 (pid: 454) of binary: /usr/bin/python Traceback (most recent call last):   File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main     return _run_code(code, main_globals, None,   File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code     exec(code, run_globals)   File ""/usr/local/lib/python3.8/distpackages/torch/distributed/launch.py"", line 195, in      main()   File ""/usr/local/lib/python3.8/distpackages/torch/distributed/launch.py"", line 191, in main     launch(args)   File ""/usr/local/lib/python3.8/distpackages/torch/distributed/launch.py"", line 176, in launch     run(args)   File ""/usr/local/lib/python3.8/distpackages/torch/distributed/run.py"", line 753, in run     elastic_launch(   File ""/usr/local/lib/python3.8/distpackages/torch/distributed/launcher/api.py"", line 132, in __call__     return launch_agent(self._config, self._entrypoint, list(args))   File ""/usr/local/lib/python3.8/distpackages/torch/distributed/launcher/api.py"", line 246, in launch_agent     raise ChildFailedError( torch.distributed.elastic.multiprocessing.errors.ChildFailedError:  =================================================== pretrain_gpt.py FAILED  Failures: [1]:   time      : 20230421_07:42:43   host      : ceceb0c7f2e6   rank      : 1 (local_rank: 1)   exitcode  : 7 (pid: 455)   error_file:    traceback : Signal 7 (SIGBUS) received by PID 455  Root Cause (first observed failure): [0]:   time      : 20230421_07:42:43   host      : ceceb0c7f2e6   rank      : 0 (local_rank: 0)   exitcode  : 7 (pid: 454)   error_file:    traceback : Signal 7 (SIGBUS) received by PID 454 ===================================================",I resolved this issue in my Docker environment by increasing the shmsize.,"thank youï¼Œ this issue has been solved in this way, so I can train now.","Change of shmsize solved mine as well, but why it works like this?"
Sohone-Guo,Saved checkpoint: unexpected pos 4996040960 vs 4996040856,"Hi, megatronlm I use the example of pretrain_gpt_distributed.sh to train a model. But there is a problem during saving. The parameters: ```bash tensormodelparallelsize 2 \ pipelinemodelparallelsize 2 \ ``` The problem is:  ```bash saving checkpoint at iteration     400 to customer_scripts/checkpoints/3b   successfully saved checkpoint at iteration     400 to customer_scripts/checkpoints/3b (min, max) time across ranks (ms):     savecheckpoint ................................: (383764.98, 383765.08)  iteration      500/  500000  saving checkpoint at iteration     600 to customer_scripts/checkpoints/3b Traceback (most recent call last):   File ""/usr/local/lib/python3.8/distpackages/torch/serialization.py"", line 428, in save     _save(obj, opened_zipfile, pickle_module, pickle_protocol)   File ""/usr/local/lib/python3.8/distpackages/torch/serialization.py"", line 655, in _save     zip_file.write_record(name, storage.data_ptr(), num_bytes) RuntimeError: [enforce fail at inline_container.cc:459] . PytorchStreamWriter failed writing file data/406: file write failed During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""pretrain_gpt.py"", line 117, in      pretrain(train_valid_test_datasets_provider, model_provider,   File ""/workspace/megatron/training.py"", line 152, in pretrain     iteration = train(forward_step_func,   File ""/workspace/megatron/training.py"", line 747, in train     save_checkpoint_and_time(iteration, model, optimizer,   File ""/workspace/megatron/training.py"", line 667, in save_checkpoint_and_time     save_checkpoint(iteration, model, optimizer, opt_param_scheduler)   File ""/workspace/megatron/checkpointing.py"", line 274, in save_checkpoint     torch.save(state_dict, model_checkpoint_name)   File ""/usr/local/lib/python3.8/distpackages/torch/serialization.py"", line 429, in save     return   File ""/usr/local/lib/python3.8/distpackages/torch/serialization.py"", line 290, in __exit__     self.file_like.write_end_of_file() RuntimeError: [enforce fail at inline_container.cc:325] . unexpected pos 4996040960 vs 4996040856 WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 17810 closing signal SIGTERM WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 17811 closing signal SIGTERM WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 17812 closing signal SIGTERM ```",2023-04-18T06:27:31Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/314
jzhang38,Any plans to release the weight of the reproduced RETRO?,,2023-04-17T12:56:47Z,stale,closed,2,2,https://github.com/NVIDIA/Megatron-LM/issues/313,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
Zhang-kg,[Question] How to generate a merge file and a vocab file,"I want to use the Megatron framework for Chinese NLP pretraining tasks. Currently, I have Chinese corpus resources and a vocab.txt file. However, for most frameworks, it seems that vocab.json and merge.txt are needed. Can I generate the above two files from Chinese corpus resources? If so, how can I generate them? Sorry, I haven't found a particularly suitable tutorial on Google.",2023-04-17T10:50:13Z,stale,closed,1,9,https://github.com/NVIDIA/Megatron-LM/issues/312,I also encountered this problemï¼Œhow to get chinese vocab.json and  merge.txt?,æ‚¨å¥½ï¼Œæ‚¨çš„é‚®ä»¶æˆ‘å·²ç»æ”¶åˆ°ï¼,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,æ‚¨å¥½ï¼Œæ‚¨çš„é‚®ä»¶æˆ‘å·²ç»æ”¶åˆ°ï¼,Marking as stale. No activity in 60 days.,æ‚¨å¥½ï¼Œæ‚¨çš„é‚®ä»¶æˆ‘å·²ç»æ”¶åˆ°ï¼,Marking as stale. No activity in 60 days.,"Hi, I am also wondering what is merge_file and vocab_file. Can you provide some clues? ",Please read about it online. Its related to tokenizers. https://github.com/huggingface/transformers/issues/1083issuecomment524303077 That blog might help
rufaelfekadu,Updating the docker image,"Hi, Can you please update the docker image provided. I don't think the image is up to date with the code and some parts of the code like the 'gradient_accumulation_fusion' parameter for column parallel class is throwing an error.",2023-04-16T21:40:27Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/311,Which docker container are you referring to?,Marking as stale. No activity in 60 days.
xu-song,Add log index map files,"Add log about index map files  old log ``` > WARNING: could not find index map files, building the indices on rank 0 ...  > elasped time to build and save docidx mapping (seconds): 0.008020     using:      number of documents:       2662      number of epochs:          1      tokens_per_epoch:          109490076893      sequence length:           2048      total number of samples:   53461951  > elapsed time to build and save sampleidx mapping (seconds): 1.838097  > elapsed time to build and save shuffleidx mapping (seconds): 4.836150 ```  new log ``` > WARNING: could not find index map files for data/wiki_text_document_train_0_indexmap_5145600ns_2048sl_1234s, building the indices on rank 0 ...  > elasped time to build and save docidx mapping (seconds): 0.008020     using:      number of documents:       2662      number of epochs:          1      tokens_per_epoch:          109490076893      sequence length:           2048      total number of samples:   53461951  > elapsed time to build and save sampleidx mapping (seconds): 1.838097  > elapsed time to build and save shuffleidx mapping (seconds): 4.836150 ```",2023-04-13T08:57:12Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/310,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,No activity on stale PR in 21 days.,Marking as stale. No activity in 60 days.
xu-song,Print tokens_per_epoch,print `tokens_per_epoch` It helps to get `num_epochs` `num_epochs = seq_length * num_samples / tokens_per_epoch `,2023-04-13T01:30:10Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/309,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,No activity on stale PR in 21 days.,Marking as stale. No activity in 60 days.
shcho1118,fix compute check for hopper arch,"""compute_90"" (Hopper arch) is supported since CUDA 11.8, not 11.7, so I fixed the code very slightly. Please check https://docs.nvidia.com/cuda/hoppercompatibilityguide/buildingapplicationsusingcudatoolkit117orearlier",2023-04-12T13:12:28Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/308,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,No activity on stale PR in 21 days.,Merged: de72f85d1e587b9ed8fdda30e66a40a9097717f5  thanks
drcege,Very imbalanced GPU memory usage with `selective` recompute granularity / activation checkpointing,"Hi, I was trying to benchmark different `activation checkpointing` / `recompute granularity` on a multinode cluster. It was stated in README that `Selective recomputation is the default and recommended in almost all cases`. However, I found that this scheme can lead to fairly imbalanced GPU memory usage on different nodes. I want to pretrain a GPT13B model on 7 nodes, each equipped with 8 A100 (totally 56 GPUs). With 3D parallelism, I set  TP=2  PP=2  micro_batch_size=4  global_batch_size=1792 The screenshots below compare `recomputeactivations` vs  `checkpointactivations` (or equivalently `recomputegranularity full` and `recomputemethod uniform`) With selective recompution, the GPU memory usage varies greatly on different nodes, ranging from 50% to 70%. It is a waste of GPU memory, as the final batch size will be constrained by the node with the highest memory usage.  The GPU memory usage is normal with full checkpointing.  Is this a bug or an expected behavior? If this is expected, is there any way to optimize it?",2023-04-11T11:35:36Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/307,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
ktaebum,fix 'untyped' attribute,"When using ZeRO optimizer, the current implementation initializes param buffers by sharing underlying storage with grad buffer. However, `torch.Storage` does not always have `_untyped` attribute. Torch whose version is greater than 1.12 has exported that attribute as a public: `torch.Storage.untyped`",2023-04-11T06:36:43Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/306,I met the same problem. This should be fixed as soon as possible.
yoosan,how to merge many ckpts to one file,"I have trained a GPT with 4way tensor parallelism, also used the distributed optimizer. Now I get trained ckpts as shown below. **Could anynone tell me how can I merge these ckpts to one?** ",2023-04-10T11:48:25Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/305,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,"i have the same question, can you resolve it ?",Marking as stale. No activity in 60 days.
Jiaxin-Wen,fix typo,attention maske > attention mask,2023-04-02T08:07:35Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/304,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,No activity on stale PR in 21 days.,Marking as stale. No activity in 60 days.
kmswin1,fix mpu to parallel_state in megatron/dataset_utils.py,,2023-03-31T11:53:32Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/303,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,No activity on stale PR in 21 days.,Marking as stale. No activity in 60 days.
kmswin1,fix mpu to parallel_state in dataset_utils.py,,2023-03-31T11:48:13Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/302
FarmerLiuAng,why errno: 97 - Address family not supported by protocol,"When I use run_text_generation_server_345M.sh, this error happens. Could you help me? [W socket.cpp:424] [c10d] The server socket cannot be initialized on [::]:6000 (errno: 97  Address family not supported by protocol). [W socket.cpp:599] [c10d] The client socket cannot be initialized to connect to [localhost]:6000 (errno: 97  Address family not supported by protocol). [W socket.cpp:599] [c10d] The client socket cannot be initialized to connect to [localhost]:6000 (errno: 97  Address family not supported by protocol). Traceback (most recent call last):   File ""/opt/conda/lib/python3.8/runpy.py"", line 194, in _run_module_as_main     return _run_code(code, main_globals, None,   File ""/opt/conda/lib/python3.8/runpy.py"", line 87, in _run_code     exec(code, run_globals)   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/run.py"", line 765, in      main()   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 345, in wrapper     return f(*args, **kwargs)   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/run.py"", line 761, in main     run(args)   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/run.py"", line 752, in run     elastic_launch(   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/launcher/api.py"", line 131, in __call__     return launch_agent(self._config, self._entrypoint, list(args))   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/launcher/api.py"", line 236, in launch_agent     result = agent.run()   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/elastic/metrics/api.py"", line 125, in wrapper     result = f(*args, **kwargs)   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/elastic/agent/server/api.py"", line 711, in run     result = self._invoke_run(role)   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/elastic/agent/server/api.py"", line 846, in _invoke_run     self._initialize_workers(self._worker_group)   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/elastic/metrics/api.py"", line 125, in wrapper     result = f(*args, **kwargs)   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/elastic/agent/server/api.py"", line 680, in _initialize_workers     self._rendezvous(worker_group)   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/elastic/metrics/api.py"", line 125, in wrapper     result = f(*args, **kwargs)   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/elastic/agent/server/api.py"", line 551, in _rendezvous     master_addr, master_port = self._get_master_addr_port(store)   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/elastic/agent/server/api.py"", line 524, in _get_master_addr_port     master_addr = os.environ['MASTER_ADDR']   File ""/opt/conda/lib/python3.8/os.py"", line 675, in __getitem__     raise KeyError(key) from None KeyError: 'MASTER_ADDR'",2023-03-30T09:30:16Z,help wanted stale,closed,0,5,https://github.com/NVIDIA/Megatron-LM/issues/301,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,"Hi. I'm not able to replicate this issue. Did you modify `run_text_generation_server_345M.sh`? Also, are you running inside a container? If so, which one?","Thanks for your reply. I didn't modify the script. And I ran in "" nvcr.io/nvidia/pytorch:21.09py3"".  ",Could you please try running with a more recent container? I'd recommend 23.04 right now. Thanks ,Marking as stale. No activity in 60 days.
liuchanglab,why use VocabParallelEmbedding?,"Words embedding takes 1~2G GPU memeory (num_embeddings=50000, embedding_dim=3700).  One GPU is enough for words embedding. Why use a distributed vocab embedding? The position embedding only use one GPU. !image",2023-03-28T13:36:42Z,stale,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/300,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.,same question. This brings difficulty if you want to freeze part of the vocabulary embedding.,Marking as stale. No activity in 60 days.
starkhu,[Question]: Anyone help explain what it's for _EMBEDDING_GROUP?,"In my opinion, only the first stage  is required to embed the input , other stages are not required to implement the embedding operation. therefore, the weights corresponding to the embedding are not required in other stages. Why is the weight for Embeding initialized at each stage in the code?",2023-03-27T12:01:13Z,stale,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/299,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,I would like to understand this as well. From what I gathered via comments in  https://github.com/NVIDIA/MegatronLM/blob/main/megatron/model/module.pyL50 https://github.com/NVIDIA/MegatronLM/blob/main/megatron/optimizer/optimizer.pyL202 This is some type of optimizations to make the pipeline parallelism more efficient. But obviously would like to hear more.,"Embeddings are initialized on at most two stages: the first and last stage. We need embeddings on the last stage in order to transform token representations from ""feature space"" back to ""vocabulary space"". If we want to keep these embeddings ""tied"" (i.e., when `untieembeddingsandoutputweights` is not specified), we need to allreduce the gradients to ensure the two embedding copies continue to remain in sync. Note that when not using pipeline parallelism, embeddings are tied by default.",Marking as stale. No activity in 60 days.
wkcn,[Fix] compile dependencies in local rank 0,"Hi there, when using MegatronLM on multiple nodes, the dependencies are built in only rank 0 of Node 0. Other nodes do not build the dependencies, e.g. `megatron.data.helpers`. It causes that other nodes raise the following error: ```     File ""./megatron/data/blendable_dataset.py"", line 52, in __init__       from megatron.data import helpers   ImportError: cannot import name 'helpers' from 'megatron.data' (./megatron/data/__init__.py) ``` To address the issue, I replaced `torch.distributed.get_rank()` with `args.local_rank` in the function `megatron.initialize._compile_dependencies`. The dependencies will be compiled if local_rank is 0.",2023-03-25T12:00:15Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/298,Close it since it does not trigger the issue if the code and data are on the shared file system.
hujunchao,How to finetune llama-65b?,,2023-03-22T23:56:19Z,stale,closed,5,11,https://github.com/NVIDIA/Megatron-LM/issues/297,same question.,+1,same question,+1 same question,+1,+1,+1,same question here,+1,+1,Marking as stale. No activity in 60 days.
HaoboGu,How to change tensor-model-parallel-size of a model,"Hello, I want to finetune an existing GPT model using megatron, but I got this error:  ```   File ""/mnt/workspace/alibeeshopmegatron/megatron/checkpointing.py"", line 58, in _compare     assert checkpoint_value == args_value, error_message AssertionError: tensor_model_parallel_size value from checkpoint (1) is not equal to the input argument value (2). ``` I guess this is because the existing model is trained with `tensormodelparallelsize 1` and I set this parameter to 2 due to my GPU memory limitation. I am wondering is there an approach to change checkpoints `tensormodelparallelsize` param?",2023-03-21T05:08:09Z,stale,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/296,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,I got same problem.,"We do not have a utility right now to convert checkpoints (i.e., create a TP=1 checkpoint from a TP=2 checkpoint). But we welcome contributions!",Marking as stale. No activity in 60 days.
andy-yang-1,Fix text_generation_cli ,"The old version of the CLI tool used urllib2, which cannot be used on Python3. This commit fixed this issue.",2023-03-17T08:58:32Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/295,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,No activity on stale PR in 21 days.,Marking as stale. No activity in 60 days.
ajindal1,Fix torch six import error for Torch 2.0,Fixes torch six import error for Torch 2.0 as mentioned in this issue: https://github.com/NVIDIA/MegatronLM/issues/293,2023-03-14T22:49:10Z,stale,open,0,6,https://github.com/NVIDIA/Megatron-LM/issues/294, any updates? wanna use megatron on PT2.0., PTAL.,It would be great if this PR can be merged. I am using PyTorch 2.0 now but cannot load MegatronLM properly. Thx,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,No activity on stale PR in 21 days.,Marking as stale. No activity in 60 days.
savitamittal1,Megtaron code incompatibility with pytorch 2.0,"It looks like torch._six module is deprecated in pytorch 2.0. To have megatron compatibility with pytorch 2.0, it willr equire to remove the reference of this module from Megatron optimizer code. Here is the traceback: Traceback (most recent call last): File ""pretrain_gpt.py"", line 28, in from megatron.training import pretrain File ""/mnt/azureml/cr/j/d81be8a5d3ae4306b72803cebea562af/exe/wd/megatron/training.py"", line 43, in from megatron.optimizer import get_megatron_optimizer File ""/mnt/azureml/cr/j/d81be8a5d3ae4306b72803cebea562af/exe/wd/megatron/optimizer/init.py"", line 23, in from .optimizer import Float16OptimizerWithFloat16Params, FP32Optimizer File ""/mnt/azureml/cr/j/d81be8a5d3ae4306b72803cebea562af/exe/wd/megatron/optimizer/optimizer.py"", line 30, in from .clip_grads import clip_grad_norm_fp32, count_zeros_fp32 File ""/mnt/azureml/cr/j/d81be8a5d3ae4306b72803cebea562af/exe/wd/megatron/optimizer/clip_grads.py"", line 19, in from torch._six import inf ModuleNotFoundError: No module named 'torch._six'",2023-03-09T22:37:29Z,bug,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/293,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Closing. Fixed here: https://github.com/NVIDIA/MegatronLM/commit/f7056ab0da255378be37a8427414bd65dc9637f6
hujunchao,How can I finetune bert on SquAD?,"Thank you for your great work! I have a question that how can I finetune bert on SquAD? Anyone can help me, please!",2023-03-08T12:33:32Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/292,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
janEbert,Fix two small problems,"Problems:  BERT/T5 dataset handles already corrupted indices incorrectly.  GPT tokenizer vocab size does not include special tokens. This PR fixes these issues. Since the changes are so small, I didn't bother creating separate PRs but please tell me if you need it separate.",2023-02-28T16:41:45Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/291,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,No activity on stale PR in 21 days.,Marking as stale. No activity in 60 days.
ksivaman,fix bug in uniform activation recompute,"When running with uniform activation checkpointing and `recomputenumlayers` > 1, the original hidden states are passed to all layers from `start` to `end` instead of the updated activations.",2023-02-27T19:34:45Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/290,Merged this but accidentally squashed the commit so github doesn't recognize it as merged.
ayush-raj7,Nemo Megatron T5 3B Inference,"Hi, I am able to deploy nemomegatront55b on triton inference server using the bignlp inference container following the steps described here: https://developer.nvidia.com/blog/deployinga13bgpt3modelwithnvidianemomegatron/ However while running inference it is throwing an error. ``` W0223 07:15:17.424793 1 libfastertransformer.cc:1674] model t5_3b, instance t5_3b_0, executing 1 requests W0223 07:15:17.424844 1 libfastertransformer.cc:831] TRITONBACKEND_ModelExecute: Running t5_3b_0 with 1 requests W0223 07:15:17.424857 1 libfastertransformer.cc:905] get total batch_size = 1 W0223 07:15:17.424874 1 libfastertransformer.cc:1321] get input count = 6 W0223 07:15:17.424923 1 libfastertransformer.cc:1393] collect name: input_ids size: 1256 bytes W0223 07:15:17.424936 1 libfastertransformer.cc:1393] collect name: input_lengths size: 4 bytes W0223 07:15:17.424945 1 libfastertransformer.cc:1393] collect name: request_output_len size: 4 bytes W0223 07:15:17.424955 1 libfastertransformer.cc:1393] collect name: runtime_top_k size: 4 bytes W0223 07:15:17.424961 1 libfastertransformer.cc:1393] collect name: runtime_top_p size: 4 bytes W0223 07:15:17.424970 1 libfastertransformer.cc:1393] collect name: temperature size: 4 bytes W0223 07:15:17.424975 1 libfastertransformer.cc:1404] the data is in CPU W0223 07:15:17.424991 1 libfastertransformer.cc:1411] the data is in CPU W0223 07:15:17.425026 1 libfastertransformer.cc:1268] before ThreadForward 0 W0223 07:15:17.425172 1 libfastertransformer.cc:1276] after ThreadForward 0 I0223 07:15:17.425278 1 libfastertransformer.cc:1109] Start to forward terminate called after throwing an instance of 'std::out_of_range'   what():  _Map_base::at Signal (6) received.  0 0x000055E60D4E5C19 in tritonserver  1 0x00007F470CB4A090 in /usr/lib/x86_64linuxgnu/libc.so.6  2 gsignal in /usr/lib/x86_64linuxgnu/libc.so.6  3 abort in /usr/lib/x86_64linuxgnu/libc.so.6  4 0x00007F470CF03911 in /usr/lib/x86_64linuxgnu/libstdc++.so.6  5 0x00007F470CF0F38C in /usr/lib/x86_64linuxgnu/libstdc++.so.6  6 0x00007F470CF0F3F7 in /usr/lib/x86_64linuxgnu/libstdc++.so.6  7 0x00007F470CF0F6A9 in /usr/lib/x86_64linuxgnu/libstdc++.so.6  8 std::__throw_out_of_range(char const*) in /usr/lib/x86_64linuxgnu/libstdc++.so.6  9 std::__detail::_Map_base, std::allocator >, std::pair, std::allocator > const, triton::Tensor>, std::allocator, std::allocator > const, triton::Tensor> >, std::__detail::_Select1st, std::equal_to, std::allocator > >, std::hash, std::allocator > >, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits, true>::at(std::__cxx11::basic_string, std::allocator > const&) in /opt/tritonserver/backends/fastertransformer/libtransformershared.so 10 T5TritonModelInstance::forward(std::shared_ptr, std::allocator >, triton::Tensor, std::hash, std::allocator > >, std::equal_to, std::allocator > >, std::allocator, std::allocator > const, triton::Tensor> > > >) in /opt/tritonserver/backends/fastertransformer/libtransformershared.so 11 0x00007F470005C08A in /opt/tritonserver/backends/fastertransformer/libtriton_fastertransformer.so 12 0x00007F470CF3BDE4 in /usr/lib/x86_64linuxgnu/libstdc++.so.6 13 0x00007F470E150609 in /usr/lib/x86_64linuxgnu/libpthread.so.0 14 clone in /usr/lib/x86_64linuxgnu/libc.so.6 Signal (11) received.  0 0x000055E60D4E5C19 in tritonserver  1 0x00007F470CB4A090 in /usr/lib/x86_64linuxgnu/libc.so.6  2 abort in /usr/lib/x86_64linuxgnu/libc.so.6  3 0x00007F470CF03911 in /usr/lib/x86_64linuxgnu/libstdc++.so.6  4 0x00007F470CF0F38C in /usr/lib/x86_64linuxgnu/libstdc++.so.6  5 0x00007F470CF0F3F7 in /usr/lib/x86_64linuxgnu/libstdc++.so.6  6 0x00007F470CF0F6A9 in /usr/lib/x86_64linuxgnu/libstdc++.so.6  7 std::__throw_out_of_range(char const*) in /usr/lib/x86_64linuxgnu/libstdc++.so.6  8 std::__detail::_Map_base, std::allocator >, std::pair, std::allocator > const, triton::Tensor>, std::allocator, std::allocator > const, triton::Tensor> >, std::__detail::_Select1st, std::equal_to, std::allocator > >, std::hash, std::allocator > >, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits, true>::at(std::__cxx11::basic_string, std::allocator > const&) in /opt/tritonserver/backends/fastertransformer/libtransformershared.so  9 T5TritonModelInstance::forward(std::shared_ptr, std::allocator >, triton::Tensor, std::hash, std::allocator > >, std::equal_to, std::allocator > >, std::allocator, std::allocator > const, triton::Tensor> > > >) in /opt/tritonserver/backends/fastertransformer/libtransformershared.so 10 0x00007F470005C08A in /opt/tritonserver/backends/fastertransformer/libtriton_fastertransformer.so 11 0x00007F470CF3BDE4 in /usr/lib/x86_64linuxgnu/libstdc++.so.6 12 0x00007F470E150609 in /usr/lib/x86_64linuxgnu/libpthread.so.0 13 clone in /usr/lib/x86_64linuxgnu/libc.so.6 ``` I am using the following checkpoint:  nemo_megatron_t5_3b_bf16_tp1.nemo P.S. I am able to perform inference using nemo_gpt5B_fp16_tp1.nemo",2023-02-23T07:25:49Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/289,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
tgale96,Add support for MegaBlocks MoEs,"These changes add support for using MegaBlocks dMoE and MoE layers in Megatron. MegaBlocks is exposed through an adapter which isolates the `megablocks` package dependency so that it does not need to be installed if users are not training MoEs.  Changes Description:  Add wrappers for MegaBlocks layers in megatron/model/transformer.py  Add load balancing loss support in pretrain_gpt.py  Add MoE arguments in megatron/arguments.py  Document MoE support in README.md Note that this pull request does not include the changes to Megatron to support expert model parallelism, pipeline parallelism and tensor model parallelism for MoEs.",2023-02-22T02:17:04Z,stale,open,2,12,https://github.com/NVIDIA/Megatron-LM/issues/288,LGTM.   can you please take a final look?,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Commenting so that this doesn't automatically get closed :),Marking as stale. No activity in 60 days.,commenting,Marking as stale. No activity in 60 days.,commenting,Marking as stale. No activity in 60 days.,commenting,Marking as stale. No activity in 60 days.,"if dmoe is merged, team megatron will win the nobel prize i guess",Marking as stale. No activity in 60 days.
tgale96,Add support for MegaBlocks MoEs,"These changes add support for using MegaBlocks dMoE and MoE layers in Megatron. MegaBlocks is exposed through an adapter which isolates the megablocks package dependency so that it does not need to be installed if users are not training MoEs. The changes can be grouped by features: Basic dMoE/MoE Support:  Add wrappers for MegaBlocks layers in megatron/model/transformer.py  Add load balancing loss support in pretrain_gpt.py  Add MoE arguments in megatron/arguments.py  Document MoE support in README.md Expert Model Parallelism Support:  Exclude expert parameters from data parallel gradient reduction in megatron/model/distributed.py  Handle expert parameters in megatron/optimizer/optimizer.py  Handle sharded expert parameter gradient clipping in megatron/optimizer/clip_grads.py  Handle sharded checkpointing in megatron/checkpointing.py Pipeline Parallelism Support:  Add new ModelType to identify when perlayer load balancing losses are present.  Handle load balancing loss for pipeline parallelism in megatron/schedules.py.  Handle load balancing loss for pipeline parallelism in megatron/training.py This PR adds MegaBlocks MoE support to GPT models. Extending support to other pretraining scripts requires limited changes. Expert model parallelism in this PR is supported across the data parallel group. We're interested in decoupling this, and also extending MegaBlocks to support tensor model parallelism in future PRs.",2023-02-20T18:05:18Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/287,"Closing in favor of https://github.com/NVIDIA/MegatronLM/pull/288, which starts by integrating the changes listed under ""Basic dMoE/MoE Support""."
tgale96,Megablocks migration,,2023-02-20T16:00:52Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/286
oliverYoung2001,Possible error in the paper of megatron 3, The formula in yellow should be 4asbh ?,2023-02-18T16:47:07Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/285,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
wduo,FileNotFoundError: [Errno 2] No such file or directory: '/opt/conda/lib/python3.7/site-packages/megatron_util/fused_kernels/build/lock',"When i train 'damo/nlp_gpt3_textgeneration_2.7B', the error as title.  Could you help me?  ",2023-02-17T09:49:37Z,stale,closed,0,5,https://github.com/NVIDIA/Megatron-LM/issues/284,"When i train 'damo/nlp_gpt3_textgeneration_chinesebase', the error is: AssertionError: intra_layer_model parallel group is not initialized I use 2 A100, and train.py is running in docker container","Traceback (most recent call last):   File ""./finetune_poetry_2.7B_pai.py"", line 82, in      trainer.train()   File ""/opt/conda/lib/python3.7/sitepackages/modelscope/trainers/trainer.py"", line 495, in train     self.train_loop(self.train_dataloader)   File ""/opt/conda/lib/python3.7/sitepackages/modelscope/trainers/trainer.py"", line 885, in train_loop     self.train_step(self.model, data_batch, **kwargs)   File ""/opt/conda/lib/python3.7/sitepackages/modelscope/trainers/nlp/gpt3_trainer.py"", line 29, in train_step     inputs = mpu.broadcast_data(keys, inputs, datatype)   File ""/opt/conda/lib/python3.7/sitepackages/megatron_util/mpu/data.py"", line 89, in broadcast_data     data)   File ""/opt/conda/lib/python3.7/sitepackages/megatron_util/mpu/data.py"", line 39, in _build_key_size_numel_dictionaries     if get_tensor_model_parallel_rank() == 0:   File ""/opt/conda/lib/python3.7/sitepackages/megatron_util/mpu/initialize.py"", line 318, in get_tensor_model_parallel_rank     return torch.distributed.get_rank(group=get_tensor_model_parallel_group())   File ""/opt/conda/lib/python3.7/sitepackages/megatron_util/mpu/initialize.py"", line 241, in get_tensor_model_parallel_group     'intra_layer_model parallel group is not initialized' AssertionError: intra_layer_model parallel group is not initialized","Traceback (most recent call last):   File ""/opt/conda/lib/python3.7/sitepackages/modelscope/utils/registry.py"", line 210, in build_from_cfg     return obj_cls._instantiate(**args)   File ""/opt/conda/lib/python3.7/sitepackages/modelscope/models/base/base_model.py"", line 65, in _instantiate     return cls(**kwargs)   File ""/opt/conda/lib/python3.7/sitepackages/modelscope/models/nlp/gpt3/text_generation.py"", line 32, in __init__     self.model = DistributedGPT3(model_dir, **kwargs)   File ""/opt/conda/lib/python3.7/sitepackages/modelscope/models/nlp/gpt3/distributed_gpt3.py"", line 976, in __init__     init_megatron_util(model_dir=model_dir, rank=rank)   File ""/opt/conda/lib/python3.7/sitepackages/modelscope/utils/megatron_utils.py"", line 46, in init_megatron_util     initialize_megatron(megatron_cfg)   File ""/opt/conda/lib/python3.7/sitepackages/megatron_util/initialize.py"", line 73, in initialize_megatron     _compile_dependencies()   File ""/opt/conda/lib/python3.7/sitepackages/megatron_util/initialize.py"", line 91, in _compile_dependencies     fused_kernels.load(args)   File ""/opt/conda/lib/python3.7/sitepackages/megatron_util/fused_kernels/__init__.py"", line 73, in load     sources, extra_cuda_flags)   File ""/opt/conda/lib/python3.7/sitepackages/megatron_util/fused_kernels/__init__.py"", line 55, in _cpp_extention_load_helper     verbose=(args.rank == 0)   File ""/opt/conda/lib/python3.7/sitepackages/torch/utils/cpp_extension.py"", line 1156, in load     keep_intermediates=keep_intermediates)   File ""/opt/conda/lib/python3.7/sitepackages/torch/utils/cpp_extension.py"", line 1344, in _jit_compile     if baton.try_acquire():   File ""/opt/conda/lib/python3.7/sitepackages/torch/utils/file_baton.py"", line 29, in try_acquire     self.fd = os.open(self.lock_file_path, os.O_CREAT | os.O_EXCL) FileNotFoundError: [Errno 2] No such file or directory: '/opt/conda/lib/python3.7/sitepackages/megatron_util/fused_kernels/build/lock'",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
wduo,FileNotFoundError: [Errno 2] No such file or directory: '/opt/conda/lib/python3.7/site-packages/megatron_util/fused_kernels/build/lock',"When i train 'damo/nlp_gpt3_textgeneration_2.7B', the error as title.  Could you help me?  ",2023-02-17T09:49:37Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/283
Soroushmehr1,Customize the GPT model,Are there any settings in the GPT model to customize it with e.g. 6.7B parameters?,2023-02-13T21:15:08Z,question,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/282,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,"There are numerous arguments that will change the number of parameters of the model, e.g. `numlayers`, `hiddensize`, `numattentionheads`."
lvcc2018,"Are there any other layer norm functions, such as RMSNorm or DeepNorm","Are there any other layer norm functions, such as RMSNorm or DeepNorm",2023-02-13T13:46:03Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/281,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
tnlin,support continuous pre-training/fine-tuning for OpenAI's Whisper model,"Hi, I am working on continuous pretraining/finetuning with OpenAI's Whisper model  https://github.com/openai/whisper 1. Is there any roadmap for this project to support more models like Whisper? 2. Are there any suggestions or examples for me to port the Whisper model to this project? Thanks!",2023-02-13T06:13:22Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/280,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.,"> Hi, I am working on continuous pretraining/finetuning with OpenAI's Whisper model https://github.com/openai/whisper >  > 1. Is there any roadmap for this project to support more models like Whisper? > 2. Are there any suggestions or examples for me to port the Whisper model to this project? >  > Thanks! Hi  , I am trying to finetuning whisperlargev3 recently. Did you solve the problem of adapting whisper to MegatronLM? Thanks."
felix-schneider,`--no-query-key-layer-scaling` makes no difference,"The factor is applied twice: Once in transformers.py:258 and once in fused_softmax.py:197. In transformers.py, you divide by the factor and in fused_softmax.py you multiply. So the factors cancel each other out. Is this only an option for numerical stability? I.e. this is intended behaviour?",2023-02-08T17:43:27Z,stale,closed,1,2,https://github.com/NVIDIA/Megatron-LM/issues/279,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
roywei,add check for h100,"the current main branch fails to compile on A100 GPUs environment AWS EC2 P4d.24xlarge instances, with A100 GPU and CUDA 11.7 Error: ```0: nvcc fatal   : Unsupported gpu architecture 'compute_90'``` fix: add a query for compute cap, assuming there is nvidiasmi installed test: works on A100 instances, but I don't have a H100 machine to test.",2023-02-06T23:32:58Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/278,"I'm not able to reproduce that error, what were your steps? Are you using an NGC PyTorch container?","I think we plan to eventually use Apex kernels instead of compiling here, in which case we won't need to check hardware or CUDA.","sorry for the late response, it looks like the latest code is working fine on A100 without any changes. I was not using NGC and I'm directly running this in a conda environment. Closing for now."
roywei,Check head dim for flash attention,"add a check for head dim when flash attention is used, only supports head dim <=128 Reference: https://github.com/HazyResearch/flashattention/issues/108 https://github.com/NVIDIA/MegatronLM/commit/9200e43aec830ead3950180f1e7a0f82df82d019 Also added fallback logic when flash attention failed    could you help take a look? Thanks!",2023-02-06T22:56:49Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/277,For MegatronLM we explicitly want an error and training to fail if useflashattn is specified and flash attention isn't available. This is to avoid accidentally running slower than expected. If you don't have flash attention or your case doesn't work for it you just need to remove the flag to disable it.
karlie38,Loading pretrained checkpoints in pretraining with model parallel,There were bugs when loading pretrained checkpoints in with model parallel trianing. There was no bug when only with data parallel training. Is is possible to train the model with model parallel initialized from pretrained checkpoints? !image,2023-02-02T10:11:06Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/276,Solved,"same issue, how do you solved it?  "
tjdgh0715,Questions about profiling all-reduce while training BERT,"Dear MegatronLM team, I'm trying to train MegatronLM's BERTbase via Distributed Data Parallel (DDP). And also for the environment, I'm using a machine with 2 x A5000 GPUs, connected via PCIe network. I found that there were some interesting parts while profiling the allreduce via ```NCCL_DEBUG=INFO```. Here is the snippet of my log. ```  iteration        6/      10  batchgenerator: 1.13 jungfrau:11104:11104 [1] NCCL INFO Broadcast: opCount 0 sendbuff 0x7f69019ffe00 recvbuff 0x7f69019ffe00 count 240 datatype 0 op 0 root 0 comm 0x7f6a38002f70 [nranks=1] stream 0x55fe4921ac30 jungfrau:11103:11103 [0] NCCL INFO Broadcast: opCount 0 sendbuff 0x7fb6c79ffe00 recvbuff 0x7fb6c79ffe00 count 240 datatype 0 op 0 root 0 comm 0x7fb800002f70 [nranks=1] stream 0x563cb9e1a0d0 jungfrau:11103:11103 [0] NCCL INFO Broadcast: opCount 0 sendbuff 0x7fb6c7853200 recvbuff 0x7fb6c7853200 count 656384 datatype 0 op 0 root 0 comm 0x7fb800002f70 [nranks=1] stream 0x563cb9e1a0d0 jungfrau:11104:11104 [1] NCCL INFO Broadcast: opCount 0 sendbuff 0x7f6901853200 recvbuff 0x7f6901853200 count 656384 datatype 0 op 0 root 0 comm 0x7f6a38002f70 [nranks=1] stream 0x55fe4921ac30 jungfrau:11103:11103 [0] NCCL INFO AllReduce: opCount 15 sendbuff 0x7fb6c79ffe00 recvbuff 0x7fb6c79ffe00 count 1 datatype 4 op 0 root 0 comm 0x7fb814002f70 [nranks=2] stream 0x563cb9e19e50 jungfrau:11104:11104 [1] NCCL INFO AllReduce: opCount 15 sendbuff 0x7f69019ffe00 recvbuff 0x7f69019ffe00 count 1 datatype 4 op 0 root 0 comm 0x7f6a99dab510 [nranks=2] stream 0x55fe4921a9b0 jungfrau:11104:11104 [1] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f6a37d40000 recvbuff 0x7f6a37d40000 count 16384 datatype 7 op 2 root 0 comm 0x7f6a38002f70 [nranks=1] stream 0x55fe4921ac30 jungfrau:11103:11103 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7fb7ffd40000 recvbuff 0x7fb7ffd40000 count 16384 datatype 7 op 2 root 0 comm 0x7fb800002f70 [nranks=1] stream 0x563cb9e1a0d0 jungfrau:11104:11104 [1] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f6733820000 recvbuff 0x7f6733820000 count 16384 datatype 7 op 0 root 0 comm 0x7f6a38002f70 [nranks=1] stream 0x55fe4921ac30 jungfrau:11104:11104 [1] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f6a37da0000 recvbuff 0x7f6a37da0000 count 16384 datatype 7 op 0 root 0 comm 0x7f6a38002f70 [nranks=1] stream 0x55fe4921ac30 jungfrau:11104:11104 [1] NCCL INFO AllReduce: opCount 16 sendbuff 0x7f6acd55a600 recvbuff 0x7f6acd55a600 count 2 datatype 7 op 0 root 0 comm 0x7f6a99dab510 [nranks=2] stream 0x55fe4921a9b0 jungfrau:11103:11103 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7fb4fd820000 recvbuff 0x7fb4fd820000 count 16384 datatype 7 op 0 root 0 comm 0x7fb800002f70 [nranks=1] stream 0x563cb9e1a0d0 jungfrau:11103:11103 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7fb7ffda0000 recvbuff 0x7fb7ffda0000 count 16384 datatype 7 op 0 root 0 comm 0x7fb800002f70 [nranks=1] stream 0x563cb9e1a0d0 jungfrau:11103:11103 [0] NCCL INFO AllReduce: opCount 16 sendbuff 0x7fb86875a600 recvbuff 0x7fb86875a600 count 2 datatype 7 op 0 root 0 comm 0x7fb814002f70 [nranks=2] stream 0x563cb9e19e50 jungfrau:11103:11103 [0] NCCL INFO AllReduce: opCount 17 sendbuff 0x7fb58c000000 recvbuff 0x7fb58c000000 count 110160258 datatype 6 op 0 root 0 comm 0x7fb814002f70 [nranks=2] stream 0x563cb9e19e50 jungfrau:11104:11104 [1] NCCL INFO AllReduce: opCount 17 sendbuff 0x7f67c6000000 recvbuff 0x7f67c6000000 count 110160258 datatype 6 op 0 root 0 comm 0x7f6a99dab510 [nranks=2] stream 0x55fe4921a9b0 jungfrau:11104:11104 [1] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f6acd44d600 recvbuff 0x7f6acd44d600 count 1 datatype 7 op 2 root 0 comm 0x7f673c002f70 [nranks=1] stream 0x55fe4921ad70 jungfrau:11103:11103 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7fb86864d600 recvbuff 0x7fb86864d600 count 1 datatype 7 op 2 root 0 comm 0x7fb508002f70 [nranks=1] stream 0x563cb9e1a210 ``` Because the BERTbase model has 12 layers, I thought that AllReduce's function call would be launched ```12``` times in one iteration when the layerbylayer synchronization happened. However, I'm quite confused when I saw that the function call was launched ```14``` times. Can I get a device for reasoning this data? (Additionally, I couldn't get some answer for why the Broadcast was launched by 4 times, too.) Thank you!",2023-01-12T06:54:22Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/275,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
cyanguwa,add support for customized pipeline stages,"This is a prototype for supporting customized pipeline stages. For example, users can define a pipeline as [6 layers, 7 layers, 7 layers, 4 layers], instead of [6 6 6 6], by passing in 'custompipelinestages 6 7 7 4' as an argument. This feature should work orthogonally with the virtual pipeline in Megatron, but in this prototype, it's an 'either/or' case, rather than 'and'. ",2023-01-11T01:04:03Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/274,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,No activity on stale PR in 21 days.,Marking as stale. No activity in 60 days.
chrisby,Module 'megatron.core.parallel_state' has no attribute 'parallel_state',"Hi there, I am trying to merge a GPT26.7B trained with  ``` TENSOR_MP_SIZE=8 PIPELINE_MP_SIZE=1 ``` using the `tools/checkpoint_util.py` script. However, I am getting the following error message: ``` AttributeError: module 'megatron.core.parallel_state' has no attribute 'parallel_state' ``` This can be fixed by going through `tools/checkpoint_loader_megatron.py` and substituting `mpu.parallel_state` with `mpu` but then I get another error: ``` File ""MegatronLM/megatron/core/parallel_state.py"", line 227, in get_tensor_model_parallel_group     assert _TENSOR_MODEL_PARALLEL_GROUP is not None, \ AssertionError: intra_layer_model parallel group is not initialized ``` Is the first fix causing this? ",2023-01-04T13:52:59Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/273,Fixed with 8ed3887a1607e1e0718e36653386e86a809327af 
thomasw21,Add support for HF tokenizer,,2022-12-21T17:57:23Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/272,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,No activity on stale PR in 21 days.,Marking as stale. No activity in 60 days.
mrwyattii,Fix deprecated numpy types,"Ignore, opened PR to wrong repo, oops :(",2022-12-20T22:34:00Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/271
GxjGit,Are there more features to be released?,"It has been a long time since last release. Are there more features to be released?  such as, supporting more high perfamance opsï¼Œ  adapting to Pytorch 2.0 and so on.",2022-12-15T12:30:33Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/269,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,"Yes, MegatronLM development continues. Pytorch 2.0 is no supported and performance improvements have been made since this issue was opened. If you have other specific enhancement requests please open new issues for them. Thanks."
janEbert,Add UL2 data sampling and pretraining,"This adds pretraining using UL2 for both encoderdecoder, noncausal decoderonly, and causal decoderonly models. I have not yet run largescale tests to see if it yields the desired training improvements, but I wanted to give others the option to take a look at the code already. I'm also not super sure about the noncausal GPT model, but I can disable (or even remove) that part if desired.",2022-12-13T19:42:42Z,stale,open,0,24,https://github.com/NVIDIA/Megatron-LM/issues/268,"Previously, I truncated sequences so the maximum amount of duplicated `extra_id` tokens would fit in and still be accepted by the model, losing a bit of data most of the time. I now changed it so the program just errors out and asks the user to put in a longer sequence length for the model. This is probably a worse/undesired solution, so I kept the other code in for now (but commented). Note that erroring out is also how the `T5Dataset` does it.","There were several issues still remaining in the UL2 implementation, most notably that I only tested for micro batch sizes of 1, which when increased made the decoderonly models fail. Also most notably in terms of the UL2 sampling, there was an issue regarding the Sdenoisers, in which the mean was not correctly positioned, leading to shorter masks than desired. The implementation also more closely follows the `seqio` implementation in the UL2 paper now, which omits the single `extra_id` token for the PrefixLM task, which we previously added.","Thank you so much  for the detailed review! I agree with most of what you said here. Some of the issues addressed are sadly part of the original `T5Dataset` implementation and seemed to not have been a problem in the past. However, I need to correctly handle e.g. unbounded probability distributions better.","I can finally report results... Comparing standard T5 training vs training with UL2 or UL2R, results in lmevalharness were almost always better with UL2/UL2R. Which should mean this code does improve evaluation results. :)","I wonder if you have compared the standard GPT vs training with UL2 or UL2R. If so, could you show more about the experiment settings and results? If not, perhaps I can help to carry out experiments.","Hey , I have sadly not gotten to that yet. Since this month is abnormally busy, I assume that it will take some time for me to create and compile results for standard GPT training.  T5 Experiments For the encoderdecoder experiments, I strongly leaned on the T5 paper. I used xPos embeddings (to have something comparable to the T5 relative attention), SwiGLU layers without bias, Adam, and the same inverse sqrt learning rate schedule as in the T5 paper. Other parameters were all the same as for the `T5base` model. xPos embeddings, SwiGLU, and the LR schedule aren't implemented here.  GPT Experiments If you want to try your own experiments, I would suggest taking one of the GPT papers and just comparing a run with UL2 vs. without. UL2 always used noncausal decoders, so the GPT should probably also be noncausal for fairness. However, while you can create a noncausal GPT model, the `GPTDataset` doesn't take this into account; you would have to implement noncausal attention masks for the `GPTDataset` as well. Focusing on the part that is already implemented, for the appropriate UL2 training you would keep all parameters the same, except: ```shell python pretrain_ul2.py \     [...] \     packsamples \     ul2likeul2r \     ul2modeltype ND ```  `ul2modeltype ND` selects a >ndcd<ecoder, use `CD`),  `ul2likeul2r` means to use more sensible random sampling (uniform instead of normal, as specified in the UL2R paper), and  `packsamples` packs different sequences into one sample so the amount of tokens seen is similar to the GPT model (because the GPT dataset always packs samples).","I rebased the code on the current master. This entailed some backwardincompatible changes to `T5Dataset` and `BertDataset` so that I could support the new caching of `splits_string`, as done in the updated `GPTDataset`.",One thing that could be changed is to include the `prefix_lm` argument I added to `GPTModel` in the `TransformerConfig`. But I wasn't sure if that's the place for it.,"> Hey , I have sadly not gotten to that yet. Since this month is abnormally busy, I assume that it will take some time for me to create and compile results for standard GPT training. >  >  T5 Experiments > For the encoderdecoder experiments, I strongly leaned on the T5 paper. I used xPos embeddings (to have something comparable to the T5 relative attention), SwiGLU layers without bias, Adam, and the same inverse sqrt learning rate schedule as in the T5 paper. Other parameters were all the same as for the `T5base` model. >  > xPos embeddings, SwiGLU, and the LR schedule aren't implemented here. >  >  GPT Experiments > If you want to try your own experiments, I would suggest taking one of the GPT papers and just comparing a run with UL2 vs. without. UL2 always used noncausal decoders, so the GPT should probably also be noncausal for fairness. However, while you can create a noncausal GPT model, the `GPTDataset` doesn't take this into account; you would have to implement noncausal attention masks for the `GPTDataset` as well. >  > Focusing on the part that is already implemented, for the appropriate UL2 training you would keep all parameters the same, except: >  > ```shell > python pretrain_ul2.py \ >     [...] \ >     packsamples \ >     ul2packany \ >     ul2likeul2r \ >     ul2modeltype ND > ``` >  > * `ul2modeltype ND` selects a >ndcd * `ul2likeul2r` means to use more sensible random sampling (uniform instead of normal, as specified in the UL2R paper), and > * `packsamples` packs different sequences into one sample so the amount of tokens seen is similar to the GPT model (because the GPT dataset always packs samples). Hi , I would like to benchmark on causal GPT and noncausal GPT(prefixlm) using this UL2 branch. The model is already under training (1.5B parameters, 50B training tokens, global_bsz=1024, max_length=2048).  Could you please help me with the evaluation procedure? Specifically, including:  1. Convert megatron checkpoint to huggingface transformers (swiglu, rotary embeddings, support PrefixLMstyle attention);  2. When evaluating PrefixLM with lmevaluationharness, does the format of the input model need to be changed? Because many extra_id are added during training, do I need to add bos, extra_id, sep, etc. special tokens in the appropriate positions during inference, or do I need to do any special processing? Thank you!","For 1., you can use the existing MegatronLMGPT2 â†” HF conversion script and add missing features like SwiGLU, RoPE, etc. to it. For 2., I adapted an eval harness adaptor so that you can supply a mode switching token. It would make way more sense to implement this into the eval harness itself, though. Other than the mode switching token, I don't think you need to add anything else."," Let me confirm, do you mean that when using lmevalharness for inference, we don't need to use PrefixLM's bidirectional attention, but only CausalLM's unidirectional attention anywhere in the input?","Ah you're absolutely right, sorry! I had also changed the same adapter to supply a bidirectional mask for the context; similarly, this would have to also be implemented for the harness."," Hi, I have trained a baseline causal decoder model without UL2, but when training a causal/noncausal decoder that uses UL2, I encountered an outofbound error when creating blendable dataset from ul2 datasets:  This problem might change with different datasets used. For example, using certain subsets of the whole datasets (Actually I have 20 datasets) does not observe this error. However, `pretrain_gpt.py` can train on the same datasets without any error. Setting `ul2_pack_any=False` and `ul2modeltype ND/CD` can not eliminate this error. The detailed training arguments are given below: ```bash torchrun \ nproc_per_node 1 \ nnodes 1 \ node_rank 0 \ master_addr localhost \ master_port 9999 pretrain_ul2.py \ numlayers 24 \ hiddensize 2048 \ numattentionheads 16 \ attentiondropout 0.0 \ hiddendropout 0.0 \ seqlength 2048 \ nopositionembedding \ userotarypositionembeddings \ rotarypercent 1.0 \ swiglu \ maxpositionembeddings 2048 \ disablebiaslinear \ transformerimpl local \ trainiters 25000 \ lr 3e4 \ lrwarmupiters 2000 \ lrdecayiters 25000 \ lrdecaystyle cosine \ minlr 3.00e05 \ weightdecay 0.1 \ clipgrad 1.0 \ optimizer adam \ adambeta1 0.9 \ adambeta2 0.95 \ adameps 1e08 \ microbatchsize 8 \ globalbatchsize 128 \ tensormodelparallelsize 1 \ pipelinemodelparallelsize 1 \ sequenceparallel \ usedistributedoptimizer \ bf16 \ attentionsoftmaxinfp32 \ useflashattn \ datapath 0.09160467 /path/to/pile_train \  the other datasets are now shown here for simplicity tokenizertype SentencePieceTokenizer \ vocabfile custom_tokenizer.model \ tokenizermodel custom_tokenizer.model \ dataimpl mmap \ logtimerstotensorboard \ logmemorytotensorboard \ logworldsizetotensorboard \ logvalidationppltotensorboard \ tensorboarddir checkpoint/tensorboard \ loginterval 1 \ timingloglevel 0 \ ul2modeltype ND \ ul2denoisers R R S X X X X \ ul2meanspanlengths 3 8 0.25 3 8 64 64 \ ul2maskratios 0.15 0.15 0.25 0.5 0.5 0.15 0.5 \ ul2rdenoisertoken '[R]' \ ul2sdenoisertoken '[S]' \ ul2xdenoisertoken '[X]' \ ul2packany \ packsamples \ ul2likeul2r \ vocabextraids 100 \ distributedbackend nccl \ usecheckpointopt_paramscheduler \ saveinterval 1000 \ evalinterval 1000 \ save checkpoint  ```",Thanks for the notification about `BlendableDataset`! I'll have to see why it doesn't work.," Hi, I have managed to start the training jobs by making a temp fix in `BlendableDataset.__getitem__`, but I'm not sure if this could bring sideeffect (Possibily duplicate samples in dataparallel group?). Looking forward to your fix anyway. !image","Just a showerthought but it could be the following: `UL2Dataset` takes `max_num_samples` literally because I expected from the variable name that I could return a dataset with fewer samples than this specifies. `BlendableDataset` uses the values passed to `max_num_samples` for each dataset and sums them up for its datasets' sizes, but doesn't check whether the datasets actually have that size. Accordingly, when datasets have fewer entries, it can index into nonexistent indices. I guess your fix is fine for now. Ideally, the lines of code I referenced would be executed after the subdataset have been created and sum up `len({train,valid,test}_dataset)` instead. I'll fix that next week, thank you so much for posting about the error. :)","Ok, then I will use this fix for the time being to continue the followup experiment, looking forward to your good news!","Hey, sorry I took longer than I suggested. I hope this fixes the issue you're facing! I also rebased the branch on top of `main`.","I think there are issues after all; with a decoderonly, I get much worse evaluation results compared to a GPTstyle pretraining. One issue that already came to my mind is that spans could possibly be next to each other, which means that spans can become much longer than desired. Other than that I think I really need to implement some tests to make sure the code does what it should in all cases.","While I didn't address the neighboring spans issue, it seems at some point I broke the causal targets. These are now fixed.",Marking as stale. No activity in 60 days.,Are there any plans for additional commits or merges? I am interested in the UL2 decoderonly part,"I haven't had time to continue my UL2 experiments, so I cannot verify the correctness of the current code. There are no specific feature plans, but if there are bugs I'll definitely fix them. I don't think this will be merged, since NVIDIA have their own UL2 implementation in NeMo (albeit with differences from the paper) and this PR is a quite major change to the MegatronLM code base.",Marking as stale. No activity in 60 days.
tridao,Integrate FlashAttention into Megatron-LM,We add an option to use FlashAttention in MegatronLM. Cc ,2022-12-09T02:07:21Z,,closed,8,4,https://github.com/NVIDIA/Megatron-LM/issues/267,ðŸŽ‰ Thank you  for sending this in!,Can't wait :),"Thanks for the PR! Sorry for the delay, I went on an extended holiday break. :) Reviewing this now.",ðŸŽ‰
HueCheng1021,T5 model run on a single gpu,"sh examples/pretrain_t5.sh setting number of microbatches to constant 1 > building BertWordPieceLowerCase tokenizer ...  > padded vocab (size: 21230) with 18 dummy tokens (new size: 21248) > initializing torch distributed ... Traceback (most recent call last):   File ""pretrain_t5.py"", line 181, in      forward_step, args_defaults={'tokenizer_type': 'BertWordPieceLowerCase'})   File ""/workspace/MegatronLM3.0/megatron/training.py"", line 103, in pretrain     args_defaults=args_defaults)   File ""/workspace/MegatronLM3.0/megatron/initialize.py"", line 81, in initialize_megatron     finish_mpu_init()   File ""/workspace/MegatronLM3.0/megatron/initialize.py"", line 62, in finish_mpu_init     _initialize_distributed()   File ""/workspace/MegatronLM3.0/megatron/initialize.py"", line 182, in _initialize_distributed     timeout=timedelta(minutes=10))   File ""/opt/conda/lib/python3.7/sitepackages/torch/distributed/distributed_c10d.py"", line 595, in init_process_group     store, rank, world_size = next(rendezvous_iterator)   File ""/opt/conda/lib/python3.7/sitepackages/torch/distributed/rendezvous.py"", line 229, in _env_rendezvous_handler     master_addr = _get_env_or_raise(""MASTER_ADDR"")   File ""/opt/conda/lib/python3.7/sitepackages/torch/distributed/rendezvous.py"", line 206, in _get_env_or_raise     raise _env_error(env_var) ValueError: Error initializing torch.distributed using env:// rendezvous: environment variable MASTER_ADDR expected, but not set",2022-12-07T03:49:26Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/266,"You can workaround this by replacing ""python"" in the ""python pretrain_t5.py"" with a torchrun launch: torchrun nnodes=1 nproc_per_node=1 rdzv_id=101 rdzv_endpoint=""localhost:5969 pretrain_t5.py. This will resolve the error you are hitting.   Pretty clear these scripts were not tested for the single gpu case though.   This same error is present in the gpt script as well. ",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
Averylamp,Adds black and isort for formatting,"We were wondering if MegatronLM would be interested in using formatting in the codebase or adopt a formatting style.  We realize that it may have been an intentional change to not adopt any formatting, but in our own usage of MegatronLM we've been trying to adopt formatting which would make it harder to sync with upstream and vice versa.   Wanted to create a PR with formatting which we thought was the closes to what was already used in the repo to show some of the diffs that it'd make in the codebase Config in a new `pyproject.toml`",2022-12-05T19:13:53Z,,closed,1,0,https://github.com/NVIDIA/Megatron-LM/issues/265
ranggihwang,Please delete this issue.,"Sorry, it's uploaded to the wrong repository.",2022-11-28T07:37:40Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/264
vlomshakov,add script to split up GPT model tensors into multiple chunks,"Maybe this script helps somebody else. It is not universal unfortunatly: only supports GPT model and doesn't support pipeline paralellism. But it has solved my case: finetuning GPT model with using tensor parallelism, if you have only merged checkpoint.",2022-11-27T21:23:28Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/263,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,No activity on stale PR in 21 days.,Marking as stale. No activity in 60 days.
pangsg,version3.0 checkpoint can't be used by old version megatron,"Hello, I have trained a ckpt by megatron latest verisonï¼Œwhen I use old verison Megatron to load this checkpoint and run inferenceï¼Œthe output of the model is very strangerï¼Œcan u help us fix this issue? Thanks a lot!",2022-11-26T09:27:07Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/262,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
pangsg,version3.0 ckpt,,2022-11-26T09:23:32Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/261
zijwang,Max seq len of 4096 support in scaled_upper_triang_masked_softmax?,"It seems `scaled_upper_triang_masked_softmax` does not support max sequence length of 4096, see https://github.com/NVIDIA/MegatronLM/blob/3860e995269df61d234ed910d4756e104e1ab844/megatron/fused_kernels/scaled_upper_triang_masked_softmax.hL414L417 However, `scaled_masked_softmax` does support 4096,  https://github.com/NVIDIA/MegatronLM/blob/e156d2fea7fc5c98e645f7742eb86b643956d840/megatron/fused_kernels/scaled_masked_softmax.hL616L619 Is there any reason why 4096 isn't supported in `scaled_upper_triang_masked_softmax`? Thanks.",2022-11-22T16:30:36Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/260,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Closing. Issue was fixed here: https://github.com/NVIDIA/MegatronLM/commit/6753e8bf9eaf1191e69f8e6d5015c4f861569dd2,"I met the same problem, and is seems the latest megatron has no `scaled_masked_softmax.h`?"
jomayeri,Jomayeri/bf16 training support,Checking for the `cur_scale` attribute on the optimizer allows for use of the BF16 optimizer with Megatron.,2022-11-03T17:25:45Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/259
kisseternity,Speed comparison between tensor parallel and pipeline parallel,"Hello, I have compared the training speed between tensor parallel and pipeline parallel in Megatron with a DGX A100 node.  I find that when the microbatchsize and gradient accumulation steps are big enough, pure pipeline parallism runs faster than  pure tensor parallelism, with over 30% speed up. But in the paper _Efficient LargeScale Language Model Training on GPU Clusters Using MegatronLM_, the conclusion is that inside a server node, it's best practice to use only tensor parallel to achieve highest TFLOPS.  !image What I'm curious is that does the TFLOPS equal to training speed? As tensor parallel spends some computation on all_reduce or other communication steps, it seems to have much more to compute than pipeline parallelism naturally. Here is my test environment: DGX A100 node with 8 GPUs. 10 billion, 8 billion and 1 billion params Bert in Megatron by adjusting the hyperparameters. The comparison is done using 8 tensor parallelism size and 8 pipeline parallelism. Any suggestion and idea is helpful, thanks.",2022-11-03T12:31:30Z,stale,closed,2,2,https://github.com/NVIDIA/Megatron-LM/issues/258,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
miguelusque,Fix argument-help typo,This PR fixes a typo in saver argument help.,2022-11-02T17:49:16Z,stale,open,0,5,https://github.com/NVIDIA/Megatron-LM/issues/257,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,No activity on stale PR in 21 days.,Hi! This issue is yet relevant.,Reopening,Marking as stale. No activity in 60 days.
pangsg,"why the shape of token2use must be [:,prev_context_length:context_length]","why the shape of tokens2use must be [:,prev_context_length:context_length]? In this way, the token length is always one.",2022-11-01T09:45:21Z,stale,closed,0,10,https://github.com/NVIDIA/Megatron-LM/issues/256,"During text generation, only one token is generated at a time. ","> During text generation, only one token is generated at a time. Thank you for your reply. But I'm still confused. At time t, it's true that only one token is generated, but at time t+1, we should concatenate the newly generated token into the context and treat [context:text] as the input of the GPT model at time t+1. As I understand it, tokens2use is supposed to represent the input of GPTmodel, but in your code, tokens2use always takes the length of 1, does it mean that there is only 1 token as the input at time t+1? Looking forward to your reply, thank you! ```             tokens2use = tokens[:, prev_context_length:context_length]             positions2use = position_ids[:, prev_context_length:context_length]             attention_mask2use = attention_mask[                 ..., prev_context_length:context_length, :context_length]              logits will be meanigful only in the last pipeline stage.             logits = forward_step(tokens2use, positions2use, attention_mask2use) ```","I have already figured out this issue, thanks for your help!","> I have already figured out this issue, thanks for your help! Hi, i am also confused about this problem, could you please share your explanation? Thanks!","During decoding, we can cache the previous calculation results of keys and values to save computation time. The cached keys and values will be fetched from the cache and concatenated with the new key and value. As a result, we only need to pass one token at a time.","> During decoding, we can cache the previous calculation results of keys and values to save computation time. The cached keys and values will be fetched from the cache and concatenated with the new key and value. As a result, we only need to pass one token at a time. But i don't find any parameter about the cache, also, i got size error when using the original code",You can find detailed information about the implementation of the keys and values cache here. I hope this helps.,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.,Closing this since I think the original question was resolved.
fighterhit,Megatron-LM pretrain_bert_distributed.sh example get stuck,https://github.com/NVIDIA/nccl/issues/743 Any idea would be appreciated!,2022-10-31T08:26:18Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/255
roywei,[Question]Megatron Performance with NGC PyTorch,"Hi I'm not sure if this is the right repo to ask this question please help redirect me. I'm training MegatronLM with NGC container, but I need a custom change on PyTorch. https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch I found out that NGC stock PyTorch in the container is constantly faster than other alternatives, in forward/backward compute time.  If I make custom changes and compile from source, or install from conda/pip it's always slower. Any ideas why or how can I match the performance? I'm already using the same PyTorch commit from NGC release notes https://docs.nvidia.com/deeplearning/frameworks/pytorchreleasenotes/rel_2203.htmlrel_2203 I'd like to make some custom change and match the perf of NGC PyTorch. Any insights will be helpful, thanks! ```  NGC PyTorch iteration      600/     800  batchgenerator: 1.44 ```",2022-10-28T16:59:55Z,,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/254,"Hi roywei, unfortunately I'm not sure we can be much help with this one. There is a whole team devoted to maximizing performance of those NGC containers so there is a decent amount of magic in there getting everything to play nicely. The nvfuser version/usage could be different, the NCCL version could be different from the upstream pytorch code, assuming you don't set USE_SYSTEM_NCCL in recent containers. The cuDNN version could be an additional difference?","Hi  thanks for the reply! I was able to narrow it down to the PyTorch source code inside the container /opt/pytorch, looks like there is some custom changes by the ncg team. If I compile from there I get better speed. Iâ€™ve made everything else the same, using nccl/cuda/cudnn provided by system(inside the ngc container) Is there a support channel I could reach out for NGC container? Want to understand the implications to make changes on those custom PyTorch source code. Thanks so much for your help!","I'm not sure of the best way to reach out to NGC container support. I think if you ping Piotr Bialecki or Masaki Kozuki they might be able to help or direct you further. Ping them on GitHub, PyTorch Slack, Twitter, etc.",Thank you so much! Closing now
Amazing-J,preprocess,"!image When I reach this stage of preprocessing, there will be no response.",2022-10-18T01:58:52Z,stale,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/253,Is it because there is not enough memory?,"Hello, has this issue been resolved?",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
pacman100,Create setup.py, What does this PR do? 1. Adds `setup.py`,2022-10-17T12:07:47Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/252
chhzh123,View size is not compatible with input tensor's size and stride,"I'm using Megatron's module and experiencing this error. It seems Megatron supposes the memory is contiguous, but sometimes it is not the case. ```   File ""/home/ubuntu/tools/anaconda3/envs/pydev/lib/python3.9/sitepackages/torch/_tensor.py"", line 482, in backward     torch.autograd.backward(   File ""/home/ubuntu/tools/anaconda3/envs/pydev/lib/python3.9/sitepackages/torch/autograd/__init__.py"", line 197, in backward     Variable._execution_engine.run_backward(   Calls into the C++ engine to run the backward pass   File ""/home/ubuntu/tools/anaconda3/envs/pydev/lib/python3.9/sitepackages/torch/autograd/function.py"", line 267, in apply     return user_fn(self, *args)   File ""/home/ubuntu/MegatronLM/megatron/mpu/layers.py"", line 269, in backward     grad_output = grad_output.view(grad_output.shape[0] * grad_output.shape[1], RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead. ``` The fix is simple and does not affect the original functionality. Only these few lines need to be changed. https://github.com/NVIDIA/MegatronLM/blob/main/megatron/mpu/layers.pyL269L272 ```python grad_output = grad_output.contiguous().view(grad_output.shape[0] * grad_output.shape[1], grad_output.shape[2]) total_input = total_input.contiguous().view(total_input.shape[0] * total_input.shape[1], total_input.shape[2]) ```",2022-10-10T23:23:13Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/251,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
rohitdwivedula,[BUG] Finetuning a GPT2 checkpoint does not seem to be working,"I trained my own GPT2355M param model using a script that looks something like this:  ``` MASTER_ADDR=localhost MASTER_PORT=6000 NNODES=1 NODE_RANK=0 DISTRIBUTED_ARGS=""nproc_per_node $GPUS_PER_NODE nnodes $NNODES node_rank $NODE_RANK master_addr $MASTER_ADDR master_port $MASTER_PORT"" python m torch.distributed.launch $DISTRIBUTED_ARGS \        pretrain_gpt.py \        numlayers 24 hiddensize 1024 numattentionheads 16 \        microbatchsize $MICROBATCH_SIZE globalbatchsize $BATCH_SIZE seqlength 1024 \        maxpositionembeddings 1024 \        distributedbackend nccl \        save $CHECKPOINT_SAVE_PATH \        load $LOAD_PATH \        datapath $DATA_PATH dataimpl mmap split 1000,0,0 \        vocabfile $DATA_PATH/gpt2vocab.json \        mergefile $DATA_PATH/gpt2merges.txt \        trainiters $TRAIN_ITERS lrdecayiters $LR_DECAY_ITERS \        lr 5.0e5 lrdecaystyle cosine minlr 1.0e5 \        weightdecay 1e2 clipgrad 1.0 lrwarmupfraction .01 \        activationscheckpointmethod uniform \        fp16 ``` I trained this model for a total of 300k iterations with a general multidomain dataset. The last checkpoint is saved in the directory `$CHECKPOINT_SAVE_PATH/iter_0300000`. This is what the final checkpoint directory looks like: ``` . â””â”€â”€ mp_rank_00     â””â”€â”€ model_optim_rng.pt ``` The `model_optim_rng.pt` file looks like this: ```python3 dict_keys(['args', 'checkpoint_version', 'iteration', 'model', 'optimizer', 'lr_scheduler', 'random_rng_state', 'np_rng_state', 'torch_rng_state', 'cuda_rng_state', 'rng_tracker_states']) ``` Now, I have a domainspecific dataset that I want to finetune this model with. So, I attempted to add the `finetune` argument, change the `data` argument to the new (smaller) dataset and then tried to run the training script with a lower LR of `5e5`. ``` Traceback (most recent call last):   File ""pretrain_gpt.py"", line 124, in      pretrain(train_valid_test_datasets_provider, model_provider, forward_step,   File ""/home//MegatronLM/megatron/training.py"", line 129, in pretrain     = build_train_valid_test_data_iterators(   File ""/home//MegatronLM/megatron/training.py"", line 829, in build_train_valid_test_data_iterators     train_dataloader = build_pretraining_data_loader(   File ""/home//MegatronLM/megatron/data/data_samplers.py"", line 34, in build_pretraining_data_loader     batch_sampler = MegatronPretrainingSampler(   File ""/home//MegatronLM/megatron/data/data_samplers.py"", line 73, in __init__     assert self.consumed_samples < self.total_samples, \ AssertionError: no samples left to consume: 153600000, 3012780 ``` It looks like the train script is picking up the samples consumed so far (153600000) from the pretraining checkpoint itself. For context, `153600000` is the number of samples I did pretraining for. `3012780` is the size of my new, smaller dataset that I want to finetune with. However, the `finetune` argument says: ```   finetune            Load model for finetuning. Do not load optimizer or rng state from checkpoint and set iteration to 0. Assumed when loading a release checkpoint. ``` So, `finetune` is not working as intended? Or is there some extra steps I need to do before finetuning for this to work? ",2022-10-10T06:11:34Z,stale,closed,0,5,https://github.com/NVIDIA/Megatron-LM/issues/250,"The next thing I tried was to see what `release` checkpoints look like. So, I downloaded the GPT2345M param model checkpoint provided on this repo by doing: ``` wget contentdisposition https://api.ngc.nvidia.com/v2/models/nvidia/megatron_lm_345m/versions/v0.0/zip O megatron_lm_345m_v0.0.zip ``` The first difference that I noticed between the last training checkpoint that I had (`iter_0300000`) and the official release checkpoint (`release`) is that:  The `model_optim_rng.pt` file from the `release` checkpoint contains only two items  `dict_keys(['iteration', 'model'])`. My checkpoint had a lot of other extra keys, such as `'args', 'checkpoint_version', 'optimizer', 'lr_scheduler', 'random_rng_state', 'np_rng_state', 'torch_rng_state', 'cuda_rng_state', 'rng_tracker_states'`.  In my checkpoint, the value of `iteration` was `300000` but in the release checkpoint it was just `1`.  I thought I would try to convert my `iter_0300000` checkpoint to match the format of your release checkpoint by making these two changes and saving them to a new directory called `release`. I also changed `latest_checkpoint_iteration.txt` to be `release` so that the checkpointloading function would pick up this edited checkpoint. Doing this, however, also does not seem to work. Firstly, the checkpointloading code itself produces a warning: ```  loading checkpoint from /path/to/gpt/ckpt/ at iteration 0 could not find arguments in the checkpoint ...  checkpoint version 0  succesfully fixed querykeyvalues ordering for checkpoint version 0   successfully loaded checkpoint from /path/to/gpt/ckpt/ at iteration 0 ``` and secondly, the initial train losses (in bold) seem way too high: > [Rank 0] (after 100 iterations) memory (MB)   Also, to put into context why I think the model is not being correctly loaded:  The final perplexity in the pretrained model was around `~18.3`  The initial train loss during finetuning is like `~9.6`, corresponding to a perplexity of `~14764`  which seems almost close to the perplexity a randomly initialized model would have.","So, in general, it looks like there are more differences between `release` checkpoints and general training checkpoints. In the Python scripts below, the variable `training` refers to my pretraining checkpoint, and `release` refers to the GPT355M param checkpoint released in this repo:  ```python3 > training['model']['language_model'].keys() dict_keys(['embedding', 'encoder']) > release['model']['language_model'].keys() dict_keys(['embedding', 'transformer']) ``` (there is a difference in naming  `transformer` vs `encoder`).  In addition to this, inside the `transformer`/`encoder` blocks:  in training checkpoints, self attentions are stored with keys that look like this: `layers.0.self_attention.query_key_value.weight`  in release checkpoints, the same layers look like this: `layers.0.attention.query_key_value.weight:torch`  the difference: The names contain `self_attention` vs `attention`. I tried to make these changes as well to my train checkpoint `.pt` file and run the finetune script again, but even with this, it still seems to be loading a random model instead of picking up weights from the checkpoint.","I have met this BUG, and have no idea to fix it. Do you figure out this problem now?",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
XiaoqingNLP,"The model can converge normally when mp=1, but does not converge when mp=2. Is there any good analysis method?","If the model can converge normally when mp=1, but cannot converge normally when mp=2, do you have any good suggestions for analyzing the problem in this case?",2022-09-18T08:21:13Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/249
yf225,Numerical difference between ColumnParallelLinear and torch.nn.Linear,"We have a simple script to test that `ColumnParallelLinear` and `torch.nn.Linear` always have the same weights and produce the same output if we feed in the same training data. Full version of the script is at https://gist.github.com/yf225/fb17ea158443a530ea2d5a37d2d2a066 and the key part is: ```python class ColumnParallelLinear(torch.nn.Module):     def __init__(self, input_size, output_size, world_size, bias=True,                  stride=1):         super(ColumnParallelLinear, self).__init__()         self.input_size = input_size         self.output_size = output_size          Divide the weight matrix along the last dimension.         self.output_size_per_partition = mpu.divide(output_size, world_size)          Note: torch.nn.functional.linear performs XA^T + b and as a result          we allocate the transpose.         self.weight = Parameter(torch.empty(             self.output_size_per_partition, self.input_size,             device=torch.cuda.current_device()))         set_tensor_model_parallel_attributes(tensor=self.weight,                                              is_parallel=True,                                              dim=0,                                              stride=stride)         if bias:             self.bias = Parameter(torch.empty(                 self.output_size_per_partition,                 device=torch.cuda.current_device()))             set_tensor_model_parallel_attributes(tensor=self.bias,                                                 is_parallel=True,                                                 dim=0,                                                 stride=stride)         else:             self.register_parameter('bias', None)     def forward(self, input_):         input_parallel = mpu.copy_to_tensor_model_parallel_region(input_)         bias = self.bias         output_parallel = F.linear(input_parallel, self.weight, bias)         output = mpu.gather_from_tensor_model_parallel_region(output_parallel)         return output ... model_tp = ColumnParallelLinear(input_size=8, output_size=8, world_size=tp_size, bias=False) model_no_tp = torch.nn.Linear(8, 8, bias=False) ... for epoch in range(num_epochs):     ...     for data in train_loader:         ...         inputs, labels = data[0].to(device), data[1].to(device)         optimizer_tp.zero_grad()         optimizer_no_tp.zero_grad()         outputs_tp = model_tp(inputs)         outputs_no_tp = model_no_tp(inputs)         loss_tp = criterion(outputs_tp, labels)         loss_no_tp = criterion(outputs_no_tp, labels)         if torch.distributed.get_rank() == 0:              check outputs and loss have similar values using torch.allclose(rtol=...)             ...         loss_tp.backward()         loss_no_tp.backward()         optimizer_tp.step()         optimizer_no_tp.step()         if torch.distributed.get_rank() == 0:              check parameters and buffers have similar values             check_param_buf_match(model_tp, model_no_tp, tp_size, iter)         iter += 1 ``` However, we observed that when input is random floatingpoint numbers and input and weight data types are `fp32`, the parameters of `ColumnParallelLinear` and `torch.nn.Linear` will have different values and fail the `torch.allclose(rtol=1e5)` test (i.e. divergence) after the **first** forwardbackwardstep iteration: ``` iter: 0: DP rank: 0, TP rank: 0: train: outputs values match between model_tp and model_no_tp! iter: 0: DP rank: 0, TP rank: 0: train: loss values match between model_tp and model_no_tp! iter: 0: forwardbackwardstep is done! Traceback (most recent call last):   File ""/fsx/users/willfeng/pyclsscale/tests/ColumnParallelLinear_vs_torch_nn_Linear_divergence.py"", line 327, in      main()   File ""/fsx/users/willfeng/pyclsscale/tests/ColumnParallelLinear_vs_torch_nn_Linear_divergence.py"", line 322, in main     check_param_buf_match(model_tp, model_no_tp, tp_size, rtol, iter)   File ""/fsx/users/willfeng/pyclsscale/tests/ColumnParallelLinear_vs_torch_nn_Linear_divergence.py"", line 207, in check_param_buf_match     assert torch.allclose(param_tp, param_no_tp, rtol=rtol), f""iter: {iter}, DP rank: {mpu.get_data_parallel_rank()}, TP rank: {mpu.get_tensor_model_parallel_rank()}: param_tp: {param_tp}, param_no_tp: {param_no_tp}, max abs diff: {torch.max(torch.abs(param_tp  param_no_tp))}"" AssertionError: iter: 0, DP rank: 0, TP rank: 0: param_tp: Parameter containing: tensor([[ 0.0657,  0.0117, 0.0960,  0.0514,  0.0330,  0.0709, 0.0534,  0.1271]],        device='cuda:0', requires_grad=True), param_no_tp: tensor([[ 0.0657,  0.0117, 0.0960,  0.0514,  0.0330,  0.0709, 0.0534,  0.1272]],        device='cuda:0', grad_fn=), max abs diff: 2.0176172256469727e05 ``` Another interesting observation is that if input and weight are in fp16, they diverge after 10 iterations. If input and weight are in bf16, there is no divergence at all. Our understanding is that tensor sharding in ColumnParallelLinear should not affect the matmul numerics and hence it should always match what we get from torch.nn.Linear regardless of dtype. But the experiment above seems to suggest otherwise. Would really appreciate any help debugging this discrepancy between `ColumnParallelLinear` and `torch.nn.Linear`. Thanks!",2022-09-10T20:17:04Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/248,"Hi, we are recently facing the similar problem of numerical differences between ColumnParallelLinear and torch.nn.Linear, and could you please kindly sharing the cause to this problem you found with us? thanks a lot !"
Clicksense,Does Megatron support or will support unpad strategy for variable sequence length? Just like MLPerf Bert training did. ,"We see that ""Unpad"" strategy help a lot on Bert training perforamnce, and it's implemented in MLPerf training Bert repro.  On Bert training, the perf in MLPerf is much faster than Megatron.  Just like the title: Does Megatron support or will support unpad strategy for variable sequence length?",2022-08-30T00:18:13Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/247,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
ktaebum,consider when not using distributed file system,"When running a training job with multinodes, The current version assumes that all nodes can access a single file system. In other words, when each node uses an isolated file system, the nodes except for master node cannot access compiled fused kernels, data index mapper, etc... This PR supports running such cluster configuration",2022-08-22T06:49:05Z,stale,open,2,3,https://github.com/NVIDIA/Megatron-LM/issues/246,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,No activity on stale PR in 21 days.,Marking as stale. No activity in 60 days.
siddharth9820,Moe pipelining,WIP PR for pipeline parallelism Has convergence issues,2022-08-18T18:05:05Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/245
Baibaifan,Variable usage problem,There is a variable usage problem in 335 lines megatron/optimizer/distrib_optimizer.py. For `param.type()` `param` is not defined.,2022-08-17T10:14:36Z,bug,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/244,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Fixed: https://github.com/NVIDIA/MegatronLM/commit/37b5490e3ed107b30f5e75d40226a29aa3d851cb  thanks!
eagle705,modify max sequence length from 2048 to 8192,This is related to CC(How to use a sequence length longer than 2048)  I fixed few lines about max_seq_len condition in fused kernel. I confirmed that this works well by doing positional embedding experiments that increases the sequence length to 8192.,2022-08-16T09:04:12Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/243,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,This PR was made redundant by https://github.com/NVIDIA/MegatronLM/commit/cd126362bbe8a08d1813485f6d8307f605a4eedd which switches from using custom kernels to Apex kernels which support the longer sequence length.
rigorosyangffff,is there a pretrained GPT model of tensor parallel? ,"Is there a pretrained GPT model of tensor parallel?  I trained one GPT model of tensor parallelã€‚but when predicting after load the tensor parallel model, decoding always occur error. I mean when decodingï¼Œthe index 0 of vocab (""!"") is always decoded. So I want to check the predicting codes if there is one pretrained GPT model of tensor parallel. Thanks, Tao",2022-08-15T03:49:02Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/242,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
ktaebum,Trigger memory log based on skipped_iter,"reporting memory status is commented as ` Report memory after optimization state has been initialized.` However, I think optimizer states might not be initialized even though the learning rate is not zero when `grad_scaler` is used.",2022-08-14T07:52:29Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/241,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,No activity on stale PR in 21 days.,Marking as stale. No activity in 60 days.
deepakn94,Reload optimizer model parameters if --no-load-optim is used,,2022-08-09T22:35:26Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/240,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,No activity on stale PR in 21 days.,Marking as stale. No activity in 60 days.
eagle705,How to use a sequence length longer than 2048,"Hi,  I am trying to train and inference a model that uses a sequence length longer than 2048,  but the fused kernel seems to be blocking it, and if I modify this part and this part too, then ""init.cc:1021 NCCL WARN Cuda failure 'an illegal memory access was encountered'"" error occurs. Is there other way to use longer sequence length than 2048?",2022-08-04T01:10:28Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/239,I realized the `init.cc:1021 NCCL WARN Cuda failure 'an illegal memory access was encountered'`error related to GPU OOM. I fixed few lines in fused kernel and found out it worked.,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.
Averylamp,Typing Support,I was just curious if there was any plans to add typing support to the Megatron library? I believe that typing would help adoption and make it easier to use without errors.  It's currently very difficult to find the output of certain functions,2022-08-03T07:21:19Z,enhancement wontfix,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/238,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,There's no immediate plan to add typing throughout the repo due to the level of effort involved.
ktaebum,Update recomputation argument in examples,Change `activationscheckpointmethod` to `recomputemethod`,2022-07-24T05:10:07Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/237,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,No activity on stale PR in 21 days.,Marking as stale. No activity in 60 days.
bzantium,[Fix] operational error in RowParallelLinear when using sequence parallel,"When using sequence parallel, it reducescatters the output of the matmul between input and weight in RowParallelLinear before adding bias. Since the output is scattered across the tensor parallel region, bias also needs to be broadcasted for proper backpropagation.  ",2022-07-16T08:34:22Z,,closed,1,2,https://github.com/NVIDIA/Megatron-LM/issues/236,https://github.com/NVIDIA/MegatronLM/blob/main/megatron/mpu/layers.pyL492 would take care of it. We do an allreduce on bias gradients., Thanks for clarification! I didn't catch there's allreduce function for sequence parallel at optimizer.py. I close it.
donghyeonk,[FIX] tools/merge_mp_partitions.py before LM evaluation,"Hi,  To evaluate a MegatronLM model trained in parallel, we tried to merge model parallel (MP) partitions by running the script in the following link.  https://github.com/NVIDIA/MegatronLM/blob/v3.0.2/README.mdevaluationandtasks ``` TENSOR_MODEL_PARALLEL_SIZE=2 TARGET_PIPELINE_MODEL_PARALLEL_SIZE=2 VOCAB_FILE=bertvocab.txt CHECKPOINT_PATH=checkpoints/bert_345m WORLD_SIZE=$TENSOR_MODEL_PARALLEL_SIZE python tools/merge_mp_partitions.py \         modeltype BERT \         tensormodelparallelsize $TENSOR_MODEL_PARALLEL_SIZE \         pipelinemodelparallelsize 1 \         targetpipelinemodelparallelsize $TARGET_PIPELINE_MODEL_PARALLEL_SIZE \         tokenizertype BertWordPieceLowerCase \         vocabfile $VOCAB_FILE \         numlayers 24 \         hiddensize 1024 \         numattentionheads 16 \         seqlength 512 \         maxpositionembeddings 512 \         load $CHECKPOINT_PATH         save $CHECKPOINT_PATH/merged ``` However, we found fused kernel loading missing, DDP process/data parallel group initializing missing, and data type errors, so fixed the problems and create this PR.",2022-07-13T08:04:27Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/235,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,No activity on stale PR in 21 days.,Marking as stale. No activity in 60 days.
mayank31398,OPT-175B fp16 or bfloat16?,Is the 175B model trained using fp16 or bfloat16?,2022-07-10T00:12:09Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/234
bzantium,Add scripts for merging and splitting checkpoints; fix minor issues when running scripts,"This PR is related to CC(Suggestion for merge (and split into) model parallel partitions). I added scripts for merging (and splitting into) model parallelism including tensor and pipeline parallel. For running the scripts, I also revised `megatron/checkpointing.py` and `megatron/model/fused_layer_norm.py` not compromising generality.  following shell script is an example with gpt345m. ``` !/bin/bash WORLD_SIZE=$((TENSOR_MODEL_PARALLEL_SIZE * PIPELINE_MODEL_PARALLEL_SIZE)) \                                 python tools/split_into_mp_partitions.py \                                 modeltype GPT \                                 tensormodelparallelsize $TENSOR_MODEL_PARALLEL_SIZE \                                 pipelinemodelparallelsize $PIPELINE_MODEL_PARALLEL_SIZE \                                 tokenizertype GPT2BPETokenizer \                                 vocabfile $VOCAB_FILE \                                 mergefile $MERGE_FILE \                                 numlayers 24 \                                 hiddensize 1024 \                                 numattentionheads 16 \                                 seqlength 1024 \                                 maxpositionembeddings 1024 \                                 load $LOAD_PATH \                                 save $SAVE_PATH ```",2022-07-08T05:12:46Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/233,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,No activity on stale PR in 21 days.,Marking as stale. No activity in 60 days.
bzantium,Suggestion for merge (and split into) model parallel partitions,"Currently, the script of `tools/merge_mp_partitions.py` only provides merging tensor model parallelism and splitting into given pipeline model parallelism which is quite constrained for use. I suggest to enlarge the script for merging both tensor and pipeline parallelism and also provide a script for splitting checkpoint into partitions separately.",2022-07-08T01:50:36Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/232,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.
lanse-sir,How to pre-train model with multiple nodes and multiple gpus?,,2022-07-05T09:53:07Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/231,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.
bingfeiz,"about the ""Multi-Stage Prompting for Knowledgeable Dialogue Generation""",First of all thank you very much for your work. When I tried to reproduce the model code of MSDP I found that the checkpoint mentioned in the paper was missing. I found that only checkpoints for Bert and GPT are provided in the readme. So I would like to ask if you can provide me with the checkpoints used in the model.,2022-07-05T08:09:05Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/230,"I have the same problem. I tried to reproduce this work, but couldn't find the model or code here. It would be appreciated if you could provide it",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.
kisseternity,[Fix] Only warm up jit fusions without specific arguments,"As the arguments _nobiasgelufusion_ and _nobiasdropoutfusion_ disable the two fusions, I think it's better to add  determine statements for the arguments before warming up them.",2022-07-05T03:28:24Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/229,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,No activity on stale PR in 21 days.,Marking as stale. No activity in 60 days.
quxinqq,"When running GPT trainning,  the program quit due to torch.distributed.elastic.agent.server.api:Received 1 death signal, shutting down workers","1.When running GPT trainning with megatron, the program quit due to torch.distributed.elastic.agent.server.api:Received 1 death signal, shutting down workers 2.code MegatronLM github branch master and I changed /MegatronLM/megatron/tokenizer/bert_tokenization.py and /MegatronLM/megatron/tokenizer/tokenizer.py for berttokenizer data preprocess needs. tokenizer.zip bert_tokenization.zip 3.training data ~103MB vocab_processed.txt mygpt2_test_0704_text_document.zip mygpt2_test_0704_text_document.bin is ~103MB which exceed size limit, if you need , i can send it. 4.bash 0704_gpt_train.sh 0704_gpt_train.zip 5.env:  linux:Linux version 4.15.0167generic (builddamd64045) (gcc version 7.5.0 (Ubuntu 7.5.03ubuntu1~18.04)) python env.txt 6.error log: 0704.log",2022-07-03T22:53:46Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/228,"Collecting environment information... PyTorch version: 1.11.0+cu102 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.5 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.27 Python version: 3.8.13 (default, Mar 28 2022, 11:38:47)  [GCC 7.5.0] (64bit runtime) Python platform: Linux4.15.0167genericx86_64withglibc2.17 Is CUDA available: True CUDA runtime version: 10.2.89 GPU models and configuration:  GPU 0: Tesla V100SPCIE32GB GPU 1: Tesla V100SPCIE32GB GPU 2: Tesla V100SPCIE32GB GPU 3: Tesla V100SPCIE32GB GPU 4: Tesla V100SPCIE32GB GPU 5: Tesla V100SPCIE32GB GPU 6: Tesla V100SPCIE32GB GPU 7: Tesla V100SPCIE32GB Nvidia driver version: 470.103.01 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.2.4 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.2.4 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.2.4 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.2.4 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.2.4 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.2.4 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.2.4 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.22.3 [pip3] torch==1.11.0 [pip3] torchaudio==0.11.0 [pip3] torchvision==0.12.0 [conda] blas                      1.0                         mkl    defaults [conda] cudatoolkit               10.2.89             h713d32c_10    condaforge [conda] ffmpeg                    4.3                  hf484d3e_0    pytorch [conda] mkl                       2021.4.0           h06a4308_640    defaults [conda] mklservice               2.4.0            py38h95df7f1_0    condaforge [conda] mkl_fft                   1.3.1            py38h8666266_1    condaforge [conda] mkl_random                1.2.2            py38h1abd341_0    condaforge [conda] numpy                     1.22.3           py38he7a7128_0    defaults [conda] numpybase                1.22.3           py38hf524024_0    defaults [conda] pytorchmutex             1.0                        cuda    pytorch [conda] torch                     1.11.0                   pypi_0    pypi [conda] torchaudio                0.11.0               py38_cu102    pytorch [conda] torchvision               0.12.0               py38_cu102    pytorch",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.
mayank31398,Can we get the scripts for merging pipeline parallel model partitions?,This would be really helpful,2022-06-30T17:32:43Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/227,"You can check the scripts in PR CC(Add scripts for merging and splitting checkpoints; fix minor issues when running scripts). If you have problem, please let me know. Thanks!",Thanks
yayaQAQ,"When complied helper.cpp failed, you can install ninja-build","https://github.com/ninjabuild/ninja/wiki/PrebuiltNinjapackages When I use the remote python interpreter, my compilation of helper.cpp fails (but it works directly in ubuntu environment), I looked for many ways and finally installed ninjabuild and it compiled successfully. up link is the way to install.",2022-06-30T13:37:30Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/226
mayank31398,Is this really necesarry? (might improve performance),I noticed this line has comments above it to create dataloaders only on first rank of model parallel group but the code doesn't do that. The code creates dataloaders on each tensor parallel group instead. Shouldn't the following line https://github.com/NVIDIA/MegatronLM/blob/3f4e71df3c81ae289e434c863466b658eaab37de/megatron/training.pyL893 be replaced by  `if mpu.is_pipeline_first_stage() and mpu.get_tensor_model_parallel_rank() == 0:`  ,2022-06-28T15:04:29Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/225,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,"nvm, it is necessary it seems"
anmolgulati,nf,,2022-06-25T23:45:25Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/224
Wild-Dog,Can I debate it like it debated itself at the Oxford Union?,Is that possible with what's here?,2022-06-22T00:14:50Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/223,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,I had to look up what this was referring to. The functionality to have MegatronLM debate itself is not part of this repo. I believe the Megatron11b model from FAIR was used. I would recommend reaching out to the folks at Oxford that put on the debate to find out what software they used.
LangQingjie,save and load checkpoint ,"Hello, I would like to ask whether Megatron LM can save the calculation results after calculating n interatinos, so that the calculation can be continued from this intermediate point later.",2022-06-21T12:29:50Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/222,"Of course, take pretrain_bert,.sh as example, you can define cheakpoint saving path through args.save argument, the checkpoint file will be saved every args.save_interval iterations.",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.
estuday,Error:size mismatch for weight,"i want use tools/run_text_generation_server.py put pangu in web,but one error occurred `copying a param with shape torch.Size([40064, 2560]) from checkpoint, the shape in current model is torch.Size([50304, 2560])` how can i solve this error",2022-06-15T02:46:50Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/221,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.
seungrokjung,unintelligible token generation in GPT text generation,"Hi,  Running gpt2 model (345M) gives weird tokens in Megatron.  ./example/run_text_generation_server_345M_8_tensor_parallel.sh  gives weird token_generation, such as,  curl 'http://localhost:5000/api' X 'PUT' H 'ContentType: application/json; charset=UTF8'  d '{""prompts"":[""the weather""], ""tokens_to_generate"":10}' {""logprobs"":null,""segments"":[[""the"","" weather"","" pitching"","" Damian"",""umed"","" M\u00e9"","" protagonist"","" receive"",""boarding"","" surreal"","" nostalgia"",""116""]],""text"":[""the weather pitching Damianumed M\u00e9 protagonist receiveboarding surreal nostalgia116""]} Does anyone have an idea why it gives such return values?",2022-06-14T01:21:27Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/220,Solved. Just model was not properly uploaded to 8 gpus. 
ShrimpLau,Confused about the comment of adding Encoder,"The comment says:  ```py          Transformer.          Encoder (usually set to True, False if part of an encoderdecoder          architecture and in encoderonly stage).         if self.add_encoder:             self.encoder = ParallelTransformer(                 self.init_method,                 output_layer_init_method,                 self_attn_mask_type=self.encoder_attn_mask_type,                 pre_process=self.pre_process,                 post_process=self.post_process             )             self._encoder_key = 'encoder'         else:             self.encoder = None ``` Why is `add_encoder` False if in encoderonly stage? Is it a typo(encoderonly > decoderonly) or some other explanation?  Many thanks!",2022-06-08T08:15:30Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/219,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.
ryanrussell,Improve Docs Readability,Signedoffby: Ryan Russell ,2022-06-01T21:30:08Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/218,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,No activity on stale PR in 21 days.,Merged: https://github.com/NVIDIA/MegatronLM/commit/ce7231f6bf5ca288f41bfc20feeab0ec438807ca  thanks for the work
Waterpine,Finetune MNLI,"When I try to finetune MNLI on slurm, I am confronted with the following issues: Detected CUDA files, patching ldflags Emitting ninja build file /home/hongyiwa/MegatronLM/megatron/fused_kernels/build/build.ninja... Building extension module scaled_upper_triang_masked_softmax_cuda... Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N) [1/2] /usr/bin/nvcc  DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda DTORCH_API_INCLUDE_EXTENSION_H DPYBIND11_COMPILER_TYPE=\""_gcc\"" DPYBIND11_STDLIB=\""_libstdcpp\"" DPYBIND11_BUILD_ABI=\""_cxxabi1011\"" isystem /home/hongyiwa/.conda/envs/song/lib/python3.9/sitepackages/torch/include isystem /home/hongyiwa/.conda/envs/song/lib/python3.9/sitepackages/torch/include/torch/csrc/api/include isystem /home/hongyiwa/.conda/envs/song/lib/python3.9/sitepackages/torch/include/TH isystem /home/hongyiwa/.conda/envs/song/lib/python3.9/sitepackages/torch/include/THC isystem /home/hongyiwa/.conda/envs/song/include/python3.9 D_GLIBCXX_USE_CXX11_ABI=0 D__CUDA_NO_HALF_OPERATORS__ D__CUDA_NO_HALF_CONVERSIONS__ D__CUDA_NO_BFLOAT16_CONVERSIONS__ D__CUDA_NO_HALF2_OPERATORS__ exptrelaxedconstexpr gencode=arch=compute_70,code=compute_70 gencode=arch=compute_70,code=sm_70 compileroptions 'fPIC' O3 gencode arch=compute_70,code=sm_70 use_fast_math U__CUDA_NO_HALF_OPERATORS__ U__CUDA_NO_HALF_CONVERSIONS__ exptrelaxedconstexpr exptextendedlambda gencode arch=compute_80,code=sm_80 std=c++14 c /home/hongyiwa/MegatronLM/megatron/fused_kernels/scaled_upper_triang_masked_softmax_cuda.cu o scaled_upper_triang_masked_softmax_cuda.cuda.o  FAILED: scaled_upper_triang_masked_softmax_cuda.cuda.o  /usr/bin/nvcc  DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda DTORCH_API_INCLUDE_EXTENSION_H DPYBIND11_COMPILER_TYPE=\""_gcc\"" DPYBIND11_STDLIB=\""_libstdcpp\"" DPYBIND11_BUILD_ABI=\""_cxxabi1011\"" isystem /home/hongyiwa/.conda/envs/song/lib/python3.9/sitepackages/torch/include isystem /home/hongyiwa/.conda/envs/song/lib/python3.9/sitepackages/torch/include/torch/csrc/api/include isystem /home/hongyiwa/.conda/envs/song/lib/python3.9/sitepackages/torch/include/TH isystem /home/hongyiwa/.conda/envs/song/lib/python3.9/sitepackages/torch/include/THC isystem /home/hongyiwa/.conda/envs/song/include/python3.9 D_GLIBCXX_USE_CXX11_ABI=0 D__CUDA_NO_HALF_OPERATORS__ D__CUDA_NO_HALF_CONVERSIONS__ D__CUDA_NO_BFLOAT16_CONVERSIONS__ D__CUDA_NO_HALF2_OPERATORS__ exptrelaxedconstexpr gencode=arch=compute_70,code=compute_70 gencode=arch=compute_70,code=sm_70 compileroptions 'fPIC' O3 gencode arch=compute_70,code=sm_70 use_fast_math U__CUDA_NO_HALF_OPERATORS__ U__CUDA_NO_HALF_CONVERSIONS__ exptrelaxedconstexpr exptextendedlambda gencode arch=compute_80,code=sm_80 std=c++14 c /home/hongyiwa/MegatronLM/megatron/fused_kernels/scaled_upper_triang_masked_softmax_cuda.cu o scaled_upper_triang_masked_softmax_cuda.cuda.o  ERROR: No supported gcc/g++ host compiler found.        Use 'nvcc ccbin ' to specify a host compiler. ninja: build stopped: subcommand failed. Traceback (most recent call last):   File ""/home/hongyiwa/.conda/envs/song/lib/python3.9/sitepackages/torch/utils/cpp_extension.py"", line 1740, in _run_ninja_build     subprocess.run(   File ""/home/hongyiwa/.conda/envs/song/lib/python3.9/subprocess.py"", line 528, in run     raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command '['ninja', 'v']' returned nonzero exit status 1. The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/home/hongyiwa/MegatronLM/examples/../tasks/main.py"", line 93, in      initialize_megatron(extra_args_provider=get_tasks_args)   File ""/home/hongyiwa/MegatronLM/megatron/initialize.py"", line 87, in initialize_megatron     _compile_dependencies()   File ""/home/hongyiwa/MegatronLM/megatron/initialize.py"", line 135, in _compile_dependencies     fused_kernels.load(args)   File ""/home/hongyiwa/MegatronLM/megatron/fused_kernels/__init__.py"", line 71, in load     scaled_upper_triang_masked_softmax_cuda = _cpp_extention_load_helper(   File ""/home/hongyiwa/MegatronLM/megatron/fused_kernels/__init__.py"", line 47, in _cpp_extention_load_helper     return cpp_extension.load(   File ""/home/hongyiwa/.conda/envs/song/lib/python3.9/sitepackages/torch/utils/cpp_extension.py"", line 1144, in load     return _jit_compile(   File ""/home/hongyiwa/.conda/envs/song/lib/python3.9/sitepackages/torch/utils/cpp_extension.py"", line 1357, in _jit_compile     _write_ninja_file_and_build_library(   File ""/home/hongyiwa/.conda/envs/song/lib/python3.9/sitepackages/torch/utils/cpp_extension.py"", line 1469, in _write_ninja_file_and_build_library     _run_ninja_build(   File ""/home/hongyiwa/.conda/envs/song/lib/python3.9/sitepackages/torch/utils/cpp_extension.py"", line 1756, in _run_ninja_build     raise RuntimeError(message) from e RuntimeError: Error building extension 'scaled_upper_triang_masked_softmax_cuda' WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 632720 closing signal SIGTERM WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 632721 closing signal SIGTERM WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 632722 closing signal SIGTERM ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 632719) of binary: /home/hongyiwa/.conda/envs/song/bin/python3 Traceback (most recent call last):   File ""/home/hongyiwa/.conda/envs/song/lib/python3.9/runpy.py"", line 197, in _run_module_as_main     return _run_code(code, main_globals, None,   File ""/home/hongyiwa/.conda/envs/song/lib/python3.9/runpy.py"", line 87, in _run_code     exec(code, run_globals)   File ""/home/hongyiwa/.conda/envs/song/lib/python3.9/sitepackages/torch/distributed/launch.py"", line 193, in      main()   File ""/home/hongyiwa/.conda/envs/song/lib/python3.9/sitepackages/torch/distributed/launch.py"", line 189, in main     launch(args)   File ""/home/hongyiwa/.conda/envs/song/lib/python3.9/sitepackages/torch/distributed/launch.py"", line 174, in launch     run(args)   File ""/home/hongyiwa/.conda/envs/song/lib/python3.9/sitepackages/torch/distributed/run.py"", line 715, in run     elastic_launch(   File ""/home/hongyiwa/.conda/envs/song/lib/python3.9/sitepackages/torch/distributed/launcher/api.py"", line 131, in __call__     return launch_agent(self._config, self._entrypoint, list(args))   File ""/home/hongyiwa/.conda/envs/song/lib/python3.9/sitepackages/torch/distributed/launcher/api.py"", line 245, in launch_agent     raise ChildFailedError( torch.distributed.elastic.multiprocessing.errors.ChildFailedError:  Could you give a hand to solve it? Thanks!",2022-06-01T04:44:50Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/217," Hi, Do you solved this problem ? I have solved in one node, but it occured again when i training with 2 node.",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.
520jefferson,Finetune T5 on translation task,"I want to use T5  in translation task, but i only find a pretrained task. How should i to use T5 in  a translation task ?",2022-05-28T14:24:32Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/216,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.
milad1378yz,Inference on multi GPU,"Hi, I have a sizeable pretrained model and I want to get inference on multiple GPU from it(I don't want to train it).so is there any way for that? In summary, I want modelparallelism. and if there is a way, how is it done?",2022-05-23T14:25:56Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/215,"Hi there.  I'm assuming this is a model that was trained with the MegatronLM code?  If so, here's an example script for launching a server with an 8way tensor parallel model: https://github.com/NVIDIA/MegatronLM/blob/main/examples/run_text_generation_server_345M_8_tensor_parallel.sh You should be able to modify that to match your Megatron model and launch a server for your model.  You can query it with curl, or a website, or use the commandline script here: https://github.com/NVIDIA/MegatronLM/blob/main/tools/text_generation_cli.py",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.
DanielHesslow,Delays and async_op in sequence parallel ,"ðŸ‘‹ I have some questions regarding the sequence parallel implementation. In particular, there's a few places with manual delays added: https://github.com/NVIDIA/MegatronLM/blob/32132c6e84152bbbe03549ac86957ebe29540f53/megatron/mpu/layers.pyL262 If I understand it correctly, this is an attempt to allow the `all_gather` to be allocated gpu resources before the matmul starts so we have a larger probability of overlapping communication and computation.  First question: Am I correct in assuming that if this doesn't happen, the only potential problem is that it's communication/computation will not overlap so there will be a performance degratation. There's no possibility that if we omit the delay the could be some sort of nccl deadlock? Second question: Why do we use `async_op=True`? If I understand the nccl semantics in pytorch correctly, `aync_op` only specifies if we should wait for the operation to be scheduled onto the cuda stream or not.  See here: https://github.com/NVIDIA/MegatronLM/blob/32132c6e84152bbbe03549ac86957ebe29540f53/megatron/mpu/layers.pyL258 Could we omit the delay if we set `async_op=False`?  I assume there will not be any performance degradation in setting `async_op=False` since this is just waiting for scheduling on the stream. Third question: Not completely related to only the sequence parallel, but to TP in general. In the pytorch documentation for process groups: https://pytorch.org/docs/stable/distributed.htmlgroups, there's the following warning: > Using multiple process groups with the NCCL backend concurrently is not safe and the user should perform explicit synchronization in their application to ensure only one process group is used at a time. This means collectives from one process group should have completed execution on the device (not just enqueued since CUDA execution is async) before collectives from another process group are enqueued. See Using multiple NCCL communicators concurrently for more details. When we're doing data parallelism in combination with TP / SP, it seems like according to this warning that we need to synchronize the communication between the different process groups so that they don't overlap, or else we risk deadlocking. If I understand the pytorch+nccl semantics correctly, each process group has their own separate cuda stream, where its communication happens, these communication streams are synchronized against the default stream with events so that computation/communication is appropriately ordered, but the different communications streams on different process groups (eg DP vs TP) does not have any such guarantees. How does megatron ensure that DP and TP does not interfere and cause deadlocking? Thanks a lot, and sorry for the wall of text.  I really enjoyed to seq_parallel paper so I want to properly understand this  Daniel",2022-05-23T12:04:24Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/214,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.
mayank31398,How to print number of unique model parameters?,,2022-05-19T09:36:34Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/213,Use the formula given in the Megatron paper.
basicv8vc,fix activations-checkpoint-method typo,,2022-05-18T01:02:10Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/212
bzantium,fix typo in docstrings,typo: storre > store,2022-05-17T01:57:44Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/211,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,No activity on stale PR in 21 days.,Merged: https://github.com/NVIDIA/MegatronLM/commit/fd2e86d4cce9506980eb03ffe7fe32207483db13  thanks!
Codle,Bug for load_checkpoint while merging checkpoint partitions.,"In `megatron/checkpointing.py`'s line 379  385, it directly use `len` function. ```python  Model. if len(model) == 1:     model[0].load_state_dict(state_dict['model'], strict=strict) else:     for i in range(len(model)):         mpu.set_virtual_pipeline_model_parallel_rank(i)         model[i].load_state_dict(state_dict['model%d' % i], strict=strict) ``` The `model` is got from `utils.unwrap_model` function in line 307. This function returns a list object or a DDP model object. When it returns a DDP object, the `len(model) == 1` will case an error. This problem will be produced in merge checkpoint partitions. ```python def unwrap_model(model, module_instances=(torchDDP)):     return_list = True     if not isinstance(model, list):         model = [model]         return_list = False     unwrapped_model = []     for model_module in model:         while isinstance(model_module, module_instances):             model_module = model_module.module         unwrapped_model.append(model_module)     if not return_list:         return unwrapped_model[0]     return unwrapped_model ``` A better solution is add a type check before use `len()` function, like: ```python if not isinstance(model, list):     model = [model]  Model. if len(model) == 1:     model[0].load_state_dict(state_dict['model'], strict=strict) else:     for i in range(len(model)):         mpu.set_virtual_pipeline_model_parallel_rank(i)         model[i].load_state_dict(state_dict['model%d' % i], strict=strict) ```",2022-05-12T05:29:33Z,stale,closed,1,1,https://github.com/NVIDIA/Megatron-LM/issues/210,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.
insujang,Pytorch distributed runtime check failure when using pipeline parallelism,"Hope this issue post would be helpful to others who suffer from similar problem. I am trying to run `examples/pretrain_gpt_distributed_with_mp.sh`, but when pipeline model parallelism is enabled, the following error occurs on every nodes: ``` Traceback (most recent call last):   File ""pretrain_gpt.py"", line 127, in      pretrain(train_valid_test_datasets_provider, model_provider,   File ""/data/insujang/MegatronLM/megatron/training.py"", line 147, in pretrain     iteration = train(forward_step_func,   File ""/data/insujang/MegatronLM/megatron/training.py"", line 695, in train     train_step(forward_step_func,   File ""/data/insujang/MegatronLM/megatron/training.py"", line 398, in train_step     losses_reduced = forward_backward_func(   File ""/data/insujang/MegatronLM/megatron/schedules.py"", line 381, in forward_backward_pipelining_with_interleaving     p2p_communication.send_forward_recv_forward(   File ""/data/insujang/MegatronLM/megatron/p2p_communication.py"", line 270, in send_forward_recv_forward     input_tensor, _ = _communicate(   File ""/data/insujang/MegatronLM/megatron/p2p_communication.py"", line 124, in _communicate     send_next_op = torch.distributed.P2POp(   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/distributed_c10d.py"", line 827, in __new__     _check_op(op)   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/distributed_c10d.py"", line 263, in _check_op     raise RuntimeError(""Invalid ``op``. Expected ``op`` "" RuntimeError: Invalid ``op``. Expected ``op`` to be of type ``torch.distributed.isend`` or ``torch.distributed.irecv``. ``` (for workers error occurs from the receive part but by the same error) Glancing at the code, I first did not understand why the code does not work since all arguments looked good: ``` def _check_op(op):     """"""     Helper to check that the ``op`` is either isend or irecv.     """"""     if op not in [isend, irecv]:         raise RuntimeError(""Invalid ``op``. Expected ``op`` ""                            ""to be of type ``torch.distributed.isend`` or ""                            ""``torch.distributed.irecv``."") send_next_op = torch.distributed.P2POp(                 torch.distributed.isend, tensor_send_next,                 mpu.get_pipeline_model_parallel_next_rank()) ``` Strangely `torch.distributed.P2POp` does work if I manually test it with the following example: ``` import torch tensor = torch.rand(2) op = torch.distributed.P2POp(torch.distributed.isend, tensor, 1) ``` which means, it is not about a Pytorch problem. When I print `op` in `_check_op` function (adding `print(op)`), the output is different when I run Megatron. Manual test: ```  ``` Megatron: ``` .wrapper_func at [address]> ``` which makes the `_check_op` fails and makes a runtime error. To prevent it, change P2POp creation function like this: ``` from torch.distributed import isend, irecv ... if tensor_send_next is not None:   send_next_op = torch.distributed.P2POp(isend, tensor_send_next, mpu.get_pipeline_model_parallel_next_rank())   ops.append(send_next_op) ``` I am not familiar with such Pythonrelated errors, so any suggestion of a better solution would be appreciated.",2022-05-12T02:44:46Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/209,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.
nickums,txt_generation_cli.py imports urllib2,(in megatron/tools/) urllib2 is not supported by Python 3,2022-05-09T13:18:07Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/208,you can replace it with `import urllib.request as urllib2`,"thankyou,  I have updated my  txt_generation_cli.py accordingly."
XqFeng-Josie,msdp gpt pretrained model max support 1024? ,"I see the msdp prompt_knwl_gen.sh and prompt_resp_gen.sh both use the model is 375gpt, that means sep_length can be set 2048. Then I use the 345gpt, which just can be set as 1024, which make the response prompts be truncatted. ",2022-05-04T02:51:33Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/207,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.
thomasw21,Allow random state variables to be synced across TP.,"Upstream change from https://github.com/bigscienceworkshop/MegatronDeepSpeed/pull/276. Essentially in BigScience, we noticed states going out of sync, in particular `torch.rng_state`. It was mainly due to the data loader using rng_state to generate seeds for workers. This causes the main process to consume a RNG state,  while other TP ranks didn't. This cause a change in RNG state between `tp_rank = 0` and `tp_rank > 0`, which some layers relie on (for example, `nn.Dropout`). More details in https://github.com/bigscienceworkshop/MegatronDeepSpeed/pull/276",2022-04-17T11:09:15Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/206,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,No activity on stale PR in 21 days.,Marking as stale. No activity in 60 days.
BinhangYuan,Why would one use --accumulate-allreduce-grads-in-fp32?,"In what situation, one should use the argument accumulateallreducegradsinfp32?  Based on https://github.com/NVIDIA/MegatronLM/blob/main/megatron/model/distributed.py, It seems that if this flag is set, the data parallel gradient buffer would be forced to use float32? Why would one use this if the parameter and its gradients are fp16 or bfp16? Thanks for your help!",2022-04-04T11:36:57Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/205,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.
irlyngaas,Merging F1STAT Branch to main ,,2022-04-01T13:26:58Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/204,Mistaken pull request meant to do in my forked repo.
AQA6666,How to adjust training args to adapt our server without many gpus?,"We are now trying to train a financial t5large on our  8*A100(40G) server, and we note that the T5 training parameters in the example are as followsï¼š T5_ARGS=""numlayers 24 \          hiddensize 1024 \          numattentionheads 16 \          kvchannels 64 \          ffnhiddensize 3072 \          encoderseqlength 512 \          decoderseqlength 128 \          maxpositionembeddings 512 \          lr 0.0001 \          lrdecayiters 990000 \          trainiters 2000000 \          minlr 0.00001 \          lrwarmupfraction 0.01 \          microbatchsize 16 \          globalbatchsize 2048 \          vocabfile $VOCAB_FILE \          vocabextraids 100 \          split 949,50,1 \          fp16"" Note that globalbatchsize(2048) / microbatchsize(16) = 128, I think that these args are fit to gpu nums = 128, but we are now trying to train it on a 8 gpus server, How should we adjust args?  Another question is that the lr=0.001 with batch size = 2048 , seq length = 512, but in t5's paper , lr is 0.01 with batch size = 1024, seq length = 1024, which args are better choice?",2022-03-27T13:04:20Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/203,"Third question, we will try to pretrain a t5XL(3B) model after training t5large, could this args fit to 3B model, or some of them should be adjusted?", ,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.
qysnn,Training recipe/example for vit(visual transformer),"Hello, It seems Megatron added support for visual transformer from the code in the repo such as MegatronLM/tree/main/megatron/model/vision and MegatronLM/blob/main/pretrain_ict.py. But I couldn't find any document about it.  I wonder if there is any training recipe/example available for the visual transformer? Thanks in advance.",2022-03-17T18:14:07Z,enhancement,closed,1,4,https://github.com/NVIDIA/Megatron-LM/issues/202,Any updates for the ViT example?,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,These will be added soon,"You can now find example training scripts for the vision classification, inpainting and Dino models in the examples folder, .e.g `examples/pretrain_vision_classifiy.sh`"
jlamypoirier,Fix a crash with multiple datasets,"A call to `_build_train_valid_test_datasets` is missing the `max_seq_length_dec` argument, so training crashes if there is more than one dataset. Fix is straightforward.",2022-03-10T22:16:55Z,stale,open,1,3,https://github.com/NVIDIA/Megatron-LM/issues/201,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,No activity on stale PR in 21 days.,Marking as stale. No activity in 60 days.
lucasjinreal,is that possible running on macOS M1?,is that possible running on macOS M1?,2022-03-08T06:31:01Z,question,closed,1,2,https://github.com/NVIDIA/Megatron-LM/issues/200,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,"It may be possible with some work to get parts of MegatronLM running on M1 via the recent PyTorch support for Apple silicon. However, we don't intend to actively support that use case and our priority is large scaling training over many GPUs."
stas00,fix explanation wording,This PR just fixes an explanation which is otherwise is unparseable. Confirmed with Deepak. Thank you!,2022-03-06T18:27:07Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/199,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,No activity on stale PR in 21 days.,Marking as stale. No activity in 60 days.
marscrazy,how to load state checkpoints from different MP settingsï¼Ÿ,"Thanks for your great works. _[My server is been setup as a 2 GPU node.]_ I have problems to load a GPT2 with MP=2 as follows, ```         torch.distributed.init_process_group(xx,xx,xx,xx)         model = GPT2(args,...)         mpu.initialize_model_parallel(2)         weights = torch.load('/path/to/checkpoint')['module']         model.load_state_dict(weights) ``` error says the weights is not fit the parameter sizes.  However, I can load the GPT2 model with MP=1 as follows, ```         torch.distributed.init_process_group(xx,xx,xx,xx)         model = GPT2(args,...)         mpu.initialize_model_parallel(1)         weights = torch.load('/path/to/checkpoint')['module']         model.load_state_dict(weights) ``` How to load a checkpoint with MP=1 for a model initialized with MP=2? Thanks for your attention.",2022-03-05T12:49:28Z,stale,closed,1,2,https://github.com/NVIDIA/Megatron-LM/issues/198,+1,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.
marigoold,Fix bug in create_masked_lm_predictions in data/dataset_utils.py,"There is a bug in https://github.com/NVIDIA/MegatronLM/blob/e156d2fea7fc5c98e645f7742eb86b643956d840/megatron/data/dataset_utils.pyL259L262 and https://github.com/NVIDIA/MegatronLM/blob/e156d2fea7fc5c98e645f7742eb86b643956d840/megatron/data/dataset_utils.pyL333L336 Actually, the conditional judgement and the continue keyword do nothing, and do not skip processing `index` which is in `covered_indexes` and `select_indexes`.",2022-03-03T04:43:13Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/197
marigoold,Fix bug in create_masked_lm_predictions in data/dataset_utils.py,"There is a bug in https://github.com/NVIDIA/MegatronLM/blob/e156d2fea7fc5c98e645f7742eb86b643956d840/megatron/data/dataset_utils.pyL259L262 and https://github.com/NVIDIA/MegatronLM/blob/e156d2fea7fc5c98e645f7742eb86b643956d840/megatron/data/dataset_utils.pyL333L336 Actually, the conditional judgement and the continue keyword do nothing, and do not skip processing `index` which is in `covered_indexes` and `select_indexes`.",2022-03-03T04:30:33Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/196
Muennighoff,Update learning_rates.py,,2022-03-01T11:27:27Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/195
LangQingjie,Transformer layers Division,"In this code, the Transfomer layer is evenly distributed, which leads to the unbalanced load of the four nodes. Because there are some calculations before and after the Transfomer layers, the load of rank 0 and rank 3 nodes is higher than that of rank 1 and rank 2. Which part of the code can I modify to change the division of transformer layers.",2022-02-26T02:45:13Z,stale,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/194,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.
nicemyeong,SImple question on forward_step in pretrain_gpt.py,Solved,2022-02-25T15:39:02Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/193
haolin-nju,ModuleNotFoundError: No module named 'fused_mix_prec_layer_norm_cuda',"I was trying to pretrain BERT from scratch. However, I encountered a module not found error when running merge_mp_bert.sh. Here is how the error looks like. ```shell $ sh merge_mp_bert.sh ... > building the full model ... building BERT model ... Traceback (most recent call last):   File ""tools/merge_mp_partitions.py"", line 352, in      main()   File ""tools/merge_mp_partitions.py"", line 236, in main     merged_model = get_model(model_type)   File ""tools/merge_mp_partitions.py"", line 128, in get_model     model = model_provider()   File ""/home/linh/opensource/MegatronLM2.5/pretrain_bert.py"", line 40, in model_provider     model = BertModel(   File ""/home/linh/opensource/MegatronLM2.5/megatron/model/bert_model.py"", line 146, in __init__     self.language_model, self._language_model_key = get_language_model(   File ""/home/linh/opensource/MegatronLM2.5/megatron/model/language_model.py"", line 62, in get_language_model     language_model = TransformerLanguageModel(   File ""/home/linh/opensource/MegatronLM2.5/megatron/model/language_model.py"", line 305, in __init__     self.encoder = ParallelTransformer(   File ""/home/linh/opensource/MegatronLM2.5/megatron/model/transformer.py"", line 586, in __init__     [build_layer(i + 1 + offset) for i in range(self.num_layers)])   File ""/home/linh/opensource/MegatronLM2.5/megatron/model/transformer.py"", line 586, in      [build_layer(i + 1 + offset) for i in range(self.num_layers)])   File ""/home/linh/opensource/MegatronLM2.5/megatron/model/transformer.py"", line 557, in build_layer     return ParallelTransformerLayer(   File ""/home/linh/opensource/MegatronLM2.5/megatron/model/transformer.py"", line 405, in __init__     self.input_layernorm = LayerNorm(   File ""/home/linh/opensource/MegatronLM2.5/megatron/model/fused_layer_norm.py"", line 68, in __init__     fused_mix_prec_layer_norm_cuda = importlib.import_module(   File ""/home/linh/anaconda3/envs/torch1.8/lib/python3.8/importlib/__init__.py"", line 127, in import_module     return _bootstrap._gcd_import(name[level:], package, level)   File """", line 1014, in _gcd_import   File """", line 991, in _find_and_load   File """", line 973, in _find_and_load_unlocked ModuleNotFoundError: No module named 'fused_mix_prec_layer_norm_cuda' ``` My nvcc version is ```shell nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 20052020 NVIDIA Corporation Built on Mon_Oct_12_20:09:46_PDT_2020 Cuda compilation tools, release 11.1, V11.1.105 Build cuda_11.1.TC455_06.29190527_0 ``` , and my `pytorch.version.cuda` is 11.1. I have tried to reinstall my apex according to these two issues NVIDIA/apex CC(How to do punctuation  standardzation?) and  [NVIDIA/apex CC([QUESTION] Why is `reset_attention_mask=False` by default?)](https://github.com/NVIDIA/apex/issues/954), but the error still exists.",2022-02-25T09:08:09Z,stale,closed,0,6,https://github.com/NVIDIA/Megatron-LM/issues/192,"I have the same problem. I directly comment out this codes. ```python global fused_mix_prec_layer_norm_cuda  fused_mix_prec_layer_norm_cuda = importlib.import_module(    ""fused_mix_prec_layer_norm_cuda"") fused_mix_prec_layer_norm_cuda = None ```",Can someone follow up on this? This is happening to me as well  ,fused_mix_prec_layer_norm_cuda is present in fused_kernels folder. Relaunching the job again fixed the issue for me for some reason.,"Hi I am facing this issue while building on Windows  `DLL load failed while importing fused_mix_prec_layer_norm_cuda: The specified module could not be found` nvcc version Cuda compilation tools, release 11.1, V11.1.74 Build cuda_11.1.relgpu_drvr455TC455_06.29069683_0 NVIDIASMI 452.39 Driver Version: 472.50        CUDA Version: 11.4",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
hyunwoongko,Discussion about softmax operating range," As a result of experiment, the operating range of fused softmax is too tight. Fused softmax can be used outside the current range of use. Any thoughts on changing this? ",2022-02-23T23:42:37Z,stale,closed,0,12,https://github.com/NVIDIA/Megatron-LM/issues/191,see this: https://github.com/tunibai/oslo/blob/v.1.1.2/oslo/fused_kernels_utils.pyL461,"The current version in Megatron supports up to 4096, I believe you can add more powers of two by extending the switch statement here: https://github.com/NVIDIA/MegatronLM/blob/e156d2fea7fc5c98e645f7742eb86b643956d840/megatron/fused_kernels/scaled_masked_softmax.hL567","I mean scalability at shorter lengths, not longer lengths.",Can you be more specific? Do you mean sk < 16?," OSLO side I conducted some experiements, and I found many more cases where they could be fused inside the max length. ```python     def is_available(dtype, bsz, np, sq, sk, use_triang_mask):         if dtype != torch.half or sk > 2048 or sk  1 and sq % bsz_per_block == 0:                 return True         return False ```  this is megatron side. ```python     def is_kernel_available(self, mask, b, np, sq, sk):         attn_batches = b * np         if (             self.scaled_masked_softmax_fusion   user want to fuse             and self.input_in_float16   input must be fp16             and 16 < sk <= 4096   sk must be 16 ~ 2048             and sq % 4 == 0   sq must be divisor of 4             and attn_batches % 4 == 0   np * b must be divisor of 4         ):             if 0 <= sk <= 4096:                 batch_per_block = self.get_batch_per_block(sq, sk, b, np)                 if self.attn_mask_type == AttnMaskType.causal:                     if attn_batches % batch_per_block == 0:                         return True                 else:                     if sq % batch_per_block == 0:                         return True         return False ```",I don't want to try to understand the differences between those code blocks. :) Can you be more specific about what cases you are asking about?, shorter than sk < 16.  when attn_batches is not diviser by 4.  sq length has nothing to do with 4 when we used scale mask softmax (not upper triang)  ...,"I wrote that code two months ago, so I can't remember all the details, but it worked well under the conditions I made.","I don't think fusion is going to be very beneficial in that small regime, so we haven't really thought about what would work in those cases.","If I share the results of my experiments on this and code, are you willing to reflect code changes? I respect your decision as it's on the Megatron side. Megatron has a fixed length when training, so it won't do much,  but I've found this to be helpful in inference (especially text generation) situations. Because, when inference, the length changes each time.","If it helps with inference it'd be great if you could submit a PR with whatever experiment results you have. We'll try to take a look and merge it if it makes sense, but can't promise it'll be anytime soon since we're all kind of hosed with time more timesensitive projects at the moment and will need to spend some time understanding the implications and what not before we can merge it. We do really appreciate you taking time to give back to the project, just may not have time to look at it for a while! ",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.
LangQingjie,metrics,How can we get the metrics as the paper memtioned such as : throughput and floatingpoint operation? Does this software provide relevant interfaces for that ,2022-02-19T02:05:35Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/190
minjiaz,Support of Mixture-of-Students,This commit includes support for MixtureofStudents (MoS) that reduces the model size of MixtureofExperts.,2022-02-15T04:58:26Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/189
LangQingjie,Memory ,"Hello, I have a question to ask. When I use Megatron for training, I train the 2.35b parameter. When using fp16, the maximum value of the calculated video memory should be 16byte * 2.35b = 35g memory, but the video memory in our training is 37.9 + 37.9g. Why?",2022-02-13T03:11:45Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/188,"That's just the memory for the parameters. Activations, gradients, fp32 copies of the parameters, optimizer state, etc. also needs to be stored on the GPU.  Also, there will be some fragmentation that will increase the memory usage."
thinksoso,Why don't use exp in crossentropy.py,"When i debug the program, i find a special code in cross_entropy.py https://github.com/NVIDIA/MegatronLM/blob/03d09af09be0f759ad6c9dd60b36e22da67b7c44/megatron/mpu/cross_entropy.pyL76 !image and the variables is !ä¼ä¸šå¾®ä¿¡æˆªå›¾_16431700537122 So, why the predicted_logits is not the **exp** logits but the original logits? https://colab.research.google.com/drive/1VGnOCd3mMCJh8a4mOfenWSZNDZ1rMopF?usp=sharing I made a test. Compared with pytorch's crossentropy loss implementation, the operation that don't use exp looks like just scale up the loss and grad. I think it **doesn't hurt** the training process.  What's the motivation to scale up it?  Thank you! ( I pulled a same issue before in MicroSoft/DeepSpeed https://github.com/microsoft/DeepSpeedExamples/issues/158",2022-02-01T08:53:22Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/186,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
ShivamSharma2705,Unable to load checkpoint,I am unable to load the checkpoints in generate_samples_gpt.py script. I am getting the following error. [Errno 2] No such file or directory: 'checkpoints/gpt2_345m/iter_0006000/mp_rank_00/model_optim_rng.pt' The following files are in the checkpoints folder created after training completed: global_step1000  global_step3500  global_step5500 global_step1500  global_step4000  global_step6000 global_step2000  global_step4500  latest global_step2500  global_step500   latest_checkpointed_iteration.txt global_step3000  global_step5000  zero_to_fp32.py Is there some error in checkpoint naming/loading?,2022-02-01T06:36:06Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/185,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
cryoco,T5 pretraining grows slower if micro batch size > 1,"* megatron version: v2.6 * pytorch version: 1.10.0+cu111 * cluster: 2 nodes, A100*8 for each node * script: pretrain_t5_distributed_with_mp.sh * args: ``` using world size: 16, dataparallelsize: 4, tensormodelparallel size: 4, pipelinemodelparallel size: 1 using torch.float16 for parameters ... Persistent fused layer norm kernel is supported from pytorch v1.11 (nvidia pytorch container paired with v1.11). Defaulting to no_persist_layer_norm=True  arguments    accumulate_allreduce_grads_in_fp32 .............. False   activations_checkpoint_method ................... None   activations_checkpoint_num_layers ............... 1   adam_beta1 ...................................... 0.9   adam_beta2 ...................................... 0.999   adam_eps ........................................ 1e08   adlr_autoresume ................................. False   adlr_autoresume_interval ........................ 1000   apply_query_key_layer_scaling ................... True   apply_residual_connection_post_layernorm ........ False   attention_dropout ............................... 0.1   attention_softmax_in_fp32 ....................... False   bert_load ....................................... None   bf16 ............................................ False   bias_dropout_fusion ............................. True   bias_gelu_fusion ................................ True   biencoder_projection_dim ........................ 0   biencoder_shared_query_context_model ............ False   block_data_path ................................. None   clip_grad ....................................... 1.0   consumed_train_samples .......................... 0   consumed_valid_samples .......................... 0   data_impl ....................................... mmap   data_parallel_size .............................. 4   data_path ....................................... ['./processed_data_sample/t5data_text_sentence']   dataloader_type ................................. single   DDP_impl ........................................ local   deallocate_pipeline_outputs ..................... False   decoder_seq_length .............................. 128   distribute_checkpointed_activations ............. False   distributed_backend ............................. nccl   embedding_path .................................. None   empty_unused_memory_level ....................... 0   encoder_seq_length .............................. 512   eod_mask_loss ................................... False   eval_interval ................................... 1000   eval_iters ...................................... 100   evidence_data_path .............................. None   exit_duration_in_mins ........................... None   exit_interval ................................... None   exit_signal_handler ............................. False   ffn_hidden_size ................................. 10240   finetune ........................................ False   fp16 ............................................ True   fp16_lm_cross_entropy ........................... False   fp32_residual_connection ........................ False   global_batch_size ............................... 8   hidden_dropout .................................. 0.1   hidden_size ..................................... 2048   hysteresis ...................................... 2   ict_head_size ................................... None   ict_load ........................................ None   indexer_batch_size .............................. 128   indexer_log_interval ............................ 1000   init_method_std ................................. 0.02   init_method_xavier_uniform ...................... False   initial_loss_scale .............................. 4294967296   kv_channels ..................................... 64   layernorm_epsilon ............................... 1e05   lazy_mpu_init ................................... None   load ............................................ None   local_rank ...................................... None   log_batch_size_to_tensorboard ................... False   log_interval .................................... 10   log_learning_rate_to_tensorboard ................ True   log_loss_scale_to_tensorboard ................... True   log_memory_to_tensorboard ....................... False   log_num_zeros_in_grad ........................... False   log_params_norm ................................. False   log_timers_to_tensorboard ....................... False   log_validation_ppl_to_tensorboard ............... False   log_world_size_to_tensorboard ................... False   loss_scale ...................................... None   loss_scale_window ............................... 1000   lr .............................................. 0.0001   lr_decay_iters .................................. 1000000   lr_decay_samples ................................ None   lr_decay_style .................................. linear   lr_warmup_fraction .............................. None   lr_warmup_iters ................................. 0   lr_warmup_samples ............................... 0   make_vocab_size_divisible_by .................... 128   mask_prob ....................................... 0.15   masked_softmax_fusion ........................... True   max_position_embeddings ......................... 4096   merge_file ...................................... None   micro_batch_size ................................ 1   min_loss_scale .................................. 1.0   min_lr .......................................... 1e05   mmap_warmup ..................................... False   no_async_tensor_model_parallel_allreduce ........ False   no_load_optim ................................... None   no_load_rng ..................................... None   no_persist_layer_norm ........................... True   no_save_optim ................................... None   no_save_rng ..................................... None   num_attention_heads ............................. 64   num_layers ...................................... 24   num_layers_per_virtual_pipeline_stage ........... None   num_workers ..................................... 2   onnx_safe ....................................... None   openai_gelu ..................................... False   optimizer ....................................... adam   override_lr_scheduler ........................... False   params_dtype .................................... torch.float16   pipeline_model_parallel_size .................... 1   pipeline_model_parallel_split_rank .............. None   query_in_block_prob ............................. 0.1   rampup_batch_size ............................... None   rank ............................................ 0   reset_attention_mask ............................ False   reset_position_ids .............................. False   retriever_score_scaling ......................... False   retriever_seq_length ............................ 256   sample_rate ..................................... 1.0   save ............................................ None   save_interval ................................... None   scatter_gather_tensors_in_pipeline .............. True   seed ............................................ 1234   seq_length ...................................... 512   sgd_momentum .................................... 0.9   short_seq_prob .................................. 0.1   split ........................................... 1000,0,0   tensor_model_parallel_size ...................... 4   tensorboard_dir ................................. ./log_dir   tensorboard_log_interval ........................ 1   tensorboard_queue_size .......................... 1000   titles_data_path ................................ None   tokenizer_type .................................. BertWordPieceLowerCase   train_iters ..................................... 1000   train_samples ................................... None   use_checkpoint_lr_scheduler ..................... False   use_contiguous_buffers_in_local_ddp ............. True   use_cpu_initialization .......................... None   use_one_sent_docs ............................... False   virtual_pipeline_model_parallel_size ............ None   vocab_extra_ids ................................. 100   vocab_file ...................................... ./t5_vocab.txt   weight_decay .................................... 0.01   world_size ...................................... 16  end of arguments  ``` * with `micro_batch_size`=1, `global_batch_size`=8, the throughput seems stable: ```  iteration       10/    1000  optimizer: 831.14 ``` It seems that forward and backward computing grows slower. Please take a look. Thanks!",2022-01-29T10:01:13Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/184,Also having this issue  were you able to fix this problem?,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
stas00,[README] 2 report table clarifications,"This PR adds the following clarifications that are essential to be able to understand the report tables in README. 1. Specify explicitly which gpu and node sizes were used. The paper has this info, so proposing to copy it next to the table. Otherwise it's hard to guess whether you used 40GB A100s or 80GB ones (and secondary, n_gpus per node). 2. Explain the model parallel size column, and give an example. Since the current column name is super confusing as it looks like PP, but it's PP*8 most of the time. Thank you!",2022-01-27T03:57:45Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/183
genesith,Further pretraining using BERT-base weights,"Hello, I would like to use my own corpus to further pretrain a BERTlike model that was already pretrained on a more general domain. Unfortunately, given my resources, the default size of Megatron might be too large to pursue pretraining with the given checkpoints (345M parameters). I was wondering if there was a way to leverage the public bertbaseuncased model as a checkpoint for a Megatron model of the same shape. Is this simply not feasible due to structural differences? Alternatively, could there be any checkpoints of smaller sized models available? Thanks!",2022-01-25T05:46:14Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/182,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
singleheart,fix an argument,fix the argument name at https://github.com/NVIDIA/MegatronLM/blob/9a8b89acd8f6ba096860170d0e30ddc0bc2bacd4/examples/msdp/prep_resp_gen.shL17 `knowledge_gen_file` > `knwl_gen_file` which is defined at https://github.com/NVIDIA/MegatronLM/blob/9a8b89acd8f6ba096860170d0e30ddc0bc2bacd4/tasks/msdp/preprocessing.pyL38,2022-01-22T09:20:02Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/181
singleheart,What is the difference between F1 and KF1 evaluation in eval_resp_generation.sh?,"Actually, I know that F1 and KF1 are different. But `eval_resp_generation.sh` provides the same codes for both. F1: https://github.com/NVIDIA/MegatronLM/blob/9a8b89acd8f6ba096860170d0e30ddc0bc2bacd4/examples/msdp/eval_resp_generation.shL14L28 KF1: https://github.com/NVIDIA/MegatronLM/blob/9a8b89acd8f6ba096860170d0e30ddc0bc2bacd4/examples/msdp/eval_resp_generation.shL35L49 It seems that both codes are the same. Is there any script missing?",2022-01-22T09:05:14Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/180,The `GROUND_TRUTH_PATH` is different. F1 measures the overlap between the modelâ€™s response and the human response from the dataset. KF1 instead measures the overlap between the modelâ€™s response and the knowledge on which the human grounded during dataset collection. ,"Oh, I've missed the `GROUND_TRUTH_PATH`. Thank you. "
singleheart,How to load finetuned DPR model for MSDP preprocessing?,"Hi, I want to run MSDP task, and it requires a finetuned DPR model. https://github.com/NVIDIA/MegatronLM/blob/9a8b89acd8f6ba096860170d0e30ddc0bc2bacd4/examples/msdp/data_processing.shL49 ParlAI does provides several DPR models that are finetuned with Wizard Of Wikipedia dataset.  (https://parl.ai/docs/zoo.htmlwizardofwikipediamodels) I've downloaded some models from there but it seems that `preprocessing.py` could not load them. Perhaps parlai does not provide config file for their models, but just checkpoint files. For example, https://parl.ai/docs/zoo.htmlmultisetdprmodel contains a cp file only. I wonder how to load it from MegatronLM. https://github.com/NVIDIA/MegatronLM/blob/9a8b89acd8f6ba096860170d0e30ddc0bc2bacd4/tasks/msdp/preprocessing.pyL391  This code just invokes some error messages. Maybe a model class should be defined here.",2022-01-20T11:50:42Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/179,"> Hi, I want to run MSDP task, and it requires a finetuned DPR model. >  > https://github.com/NVIDIA/MegatronLM/blob/9a8b89acd8f6ba096860170d0e30ddc0bc2bacd4/examples/msdp/data_processing.shL49 >  > ParlAI does provides several DPR models that are finetuned with Wizard Of Wikipedia dataset. (https://parl.ai/docs/zoo.htmlwizardofwikipediamodels) >  > I've downloaded some models from there but it seems that `preprocessing.py` could not load them. Perhaps parlai does not provide config file for their models, but just checkpoint files. For example, https://parl.ai/docs/zoo.htmlmultisetdprmodel contains a cp file only. I wonder how to load it from MegatronLM. >  > https://github.com/NVIDIA/MegatronLM/blob/9a8b89acd8f6ba096860170d0e30ddc0bc2bacd4/tasks/msdp/preprocessing.pyL391 >  >  > This code just invokes some error messages. Maybe a model class should be defined here. i konw how to do it. `from transformers import AutoTokenizer, AutoModel tokenizer = AutoTokenizer.from_pretrained(""facebook/dprquestion_encodersinglenqbase"") model = AutoModel.from_pretrained(""facebook/dprquestion_encodersinglenqbase"")`",Thank you very much. It works! 
ZikaiGuo,How to convert megatron T5 model to huggingface T5 model?,NVIDIA have provided official script to convert megatronBERT and megatronGPT to huggingface model. Could you release script for megatronT5 in the future?,2022-01-20T11:42:33Z,stale,closed,16,2,https://github.com/NVIDIA/Megatron-LM/issues/178,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
ZikaiGuo,How to finetune T5 with Megatron?,Could you add an example that finetune t5 on downstream tasks?,2022-01-20T11:39:23Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/177,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
Henry-Avery,Is there any problem in my pretrain_gpt.sh? The AssertionError: last epoch number of samples exceeded max value.,"My configuration is only one single server with single P6000 GPUï¼Œand use the recommend docker container in Github. And I use 1k text sample, correctly process my data in json file , maybe the problem is caused by the lack of data? my train script is as follow:   ``` export MASTER_ADDR=localhost export MASTER_PORT=5678 CHECKPOINT_PATH=checkpoints/gpt2_34m_maybe VOCAB_FILE=""/workspace/megtron/MegatronLMmain/gpt2vocab.json"" MERGE_FILE=""/workspace/megtron/MegatronLMmain/gpt2merges.txt"" DATA_PATH=mygpt2_text_document GPT_ARGS=""numlayers 4 \           hiddensize 1024 \           numattentionheads 1 \           seqlength 1024 \           maxpositionembeddings 1024 \           microbatchsize 4 \           globalbatchsize 8 \           lr 0.00015 \           trainiters 500000 \           lrdecayiters 320000 \           lrdecaystyle cosine \           vocabfile $VOCAB_FILE \           mergefile $MERGE_FILE \           lrwarmupfraction .01 \           fp16"" OUTPUT_ARGS=""loginterval 10 \              saveinterval 500 \              evalinterval 100 \              evaliters 10 \              activationscheckpointmethod uniform"" python /workspace/megtron/MegatronLMmain/pretrain_gpt.py \        $GPT_ARGS \        $OUTPUT_ARGS \        save $CHECKPOINT_PATH \        load $CHECKPOINT_PATH \        datapath $DATA_PATH \ ``` and the log is very long, `  openai_gelu ..................................... False   optimizer ....................................... adambin  pretrain_ict.py       tasks                  yyt_preprocess_data_gpt2.sh   override_lr_scheduler ........................... Falsedx  pretrain_t5.py        testMegatronData.json  yyt_pretrain_gpt.sh   train_iters ..................................... 1000   train_samples ................................... None   use_checkpoint_lr_scheduler ..................... False   use_contiguous_buffers_in_local_ddp ............. True   use_cpu_initialization .......................... None   use_one_sent_docs ............................... False   merge_file ...................................... gpt2merges.txt   micro_batch_size ................................ 4   min_loss_scale .................................. 1.0ervocab.json   min_lr .......................................... 0.01   mmap_warmup ..................................... Falsegpt2_text_document']   no_async_tensor_model_parallel_allreduce ........ Falsee   num_layers_per_virtual_pipeline_stage ........... Nonel   num_workers ..................................... 2onee   onnx_safe ....................................... Nonesize: 50304)   openai_gelu ..................................... False   optimizer ....................................... adam   override_lr_scheduler ........................... False   params_dtype .................................... torch.float16   patch_dim ....................................... 16neining.py"", line 96, in pretrain   pipeline_model_parallel_size .................... 1one 30, 1r,   pipeline_model_parallel_split_rank .............. Noneeialize.py"", line 82, in initialize_megatron   query_in_block_prob ............................. 0.1e   rampup_batch_size ............................... Noneeialize.py"", line 60, in finish_mpu_init   rank ............................................ 0ruee   reset_attention_mask ............................ Falseialize.py"", line 180, in _initialize_distributed   reset_position_ids .............................. FalsePETokenizer   retriever_report_topk_accuracies ................ []00h.float161 (nvidia pytorch container paired with v1.11). Defaulting to no_persist_layer_norm=True   retriever_score_scaling ......................... False   retriever_seq_length ............................ 256seributed/rendezvous.py"", line 182, in _env_rendezvous_handler   sample_rate ..................................... 1.0torch v1.11 (nvidia pytorch container paired with v1.11). Defaulting to no_persist_layer_norm=True   vocab_file ...................................... gpt2vocab.json   weight_decay .................................... 0.01  world_size ...................................... 1  end of arguments setting number of microbatches to constant 2 > building GPT2BPETokenizer tokenizer ...  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304) > initializing torch distributed ...   vocab_file ...................................... gpt2vocab.json   weight_decay .................................... 0.01   world_size ...................................... 1 end of arguments setting number of microbatches to constant 2> building GPT2BPETokenizer tokenizer ... > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)> initializing torch distributed ...> initializing tensor model parallel with size 1 > initializing pipeline model parallel with size 1  weight_decay .................................... 0.01  world_size ...................................... 1 end of arguments setting number of microbatches to constant 2> building GPT2BPETokenizer tokenizer ... > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)> initializing torch distributed ...> initializing tensor model parallel with size 1> initializing torch distributed ...> setting random seeds to 1234 ... > setting random seeds to 1234 ...> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234> compiling dataset index builder ...make: Entering directory '/workspace/megtron/MegatronLMmain/megatron/data'  vocab_file ...................................... gpt2vocab.json  weight_decay .................................... 0.01  world_size ...................................... 1 end of arguments setting number of microbatches to constant 2> building GPT2BPETokenizer tokenizer ... > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)> initializing torch distributed ...> initializing tensor model parallel with size 1> initializing pipeline model parallel with size 1> setting random seeds to 1234 ...> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234> compiling dataset index builder ...make: Entering directory '/workspace/megtron/MegatronLMmain/megatron/data'g++ O3 Wall shared std=c++11 fPIC fdiagnosticscolor I/opt/conda/include/python3.8 I/opt/conda/lib/python3.8/sitepackages/pybind11/include helpers.cpp o helpernuxgnu.somake: Leaving directory '/workspace/megtron/MegatronLMmain/megatron/data' >>> done with dataset index builder. Compilation time: 7.327 seconds> compiling and loading fused kernels ...Detected CUDA files, patching ldflagsEmitting ninja build file /workspace/megtron/MegatronLMmain/megatron/fused_kernels/build/build.ninja...Building extension module scaled_upper_triang_masked_softmax_cuda...Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)[1/3] c++ MMD MF scaled_upper_triang_masked_softmax.o.d DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda DTORCH_API_INCLUDE_EXTENSION_H DPYBIND11_COMPIBIND11_STDLIB=\""_libstdcpp\"" DPYBIND11_BUILD_ABI=\""_cxxabi1013\"" isystem /opt/conda/lib/python3.8/sitepackages/torch/include isystem /opt/conda/lib/python3.8/sitepaorch/csrc/api/include isystem /opt/conda/lib/python3.8/sitepackages/torch/include/TH isystem /opt/conda/lib/python3.8/sitepackages/torch/include/THC isystem /usr/lotem /opt/conda/include/python3.8 D_GLIBCXX_USE_CXX11_ABI=1 fPIC std=c++14 O3 c /workspace/megtron/MegatronLMmain/megatron/fused_kernels/scaled_upper_triang_masked_upper_triang_masked_softmax.o  [2/3] /usr/local/cuda/bin/nvcc  DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda DTORCH_API_INCLUDE_EXTENSION_H DPYBIND11_COMPILER_TYPE=\""_gcc\"" DPYBIND\"" DPYBIND11_BUILD_ABI=\""_cxxabi1013\"" isystem /opt/conda/lib/python3.8/sitepackages/torch/include isystem /opt/conda/lib/python3.8/sitepackages/torch/include/torchstem /opt/conda/lib/python3.8/sitepackages/torch/include/TH isystem /opt/conda/lib/python3.8/sitepackages/torch/include/THC isystem /usr/local/cuda/include isystem hon3.8 D_GLIBCXX_USE_CXX11_ABI=1 D__CUDA_NO_HALF_OPERATORS__ D__CUDA_NO_HALF_CONVERSIONS__ D__CUDA_NO_BFLOAT16_CONVERSIONS__ D__CUDA_NO_HALF2_OPERATORS__ exptrele=arch=compute_61,code=compute_61 gencode=arch=compute_61,code=sm_61 compileroptions 'fPIC' O3 gencode arch=compute_70,code=sm_70 use_fast_math U__CUDA_NO_HALFNO_HALF_CONVERSIONS__ exptrelaxedconstexpr exptextendedlambda gencode arch=compute_80,code=sm_80 std=c++14 c /workspace/megtron/MegatronLMmain/megatron/fuse_triang_masked_softmax_cuda.cu o scaled_upper_triang_masked_softmax_cuda.cuda.o [3/3] c++ scaled_upper_triang_masked_softmax.o scaled_upper_triang_masked_softmax_cuda.cuda.o shared L/opt/conda/lib/python3.8/sitepackages/torch/lib lc10 lc10_cudauda ltorch ltorch_python L/usr/local/cuda/lib64 lcudart o scaled_upper_triang_masked_softmax_cuda.so Loading extension module scaled_upper_triang_masked_softmax_cuda... Detected CUDA files, patching ldflags [2/3] /usr/local/cuda/bin/nvcc  DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda DTORCH_API_INCLUDE_EXTENSION_H DPYBIND11_COMPILER_TYPE=\""_gcc\"" DPYBIND11_STDLIB=\""__BUILD_ABI=\""_cxxabi1013\"" isystem /opt/conda/lib/python3.8/sitepackages/torch/include isystem /opt/conda/lib/python3.8/sitepackages/torch/include/torch/csrc/api/incda/lib/python3.8/sitepackages/torch/include/TH isystem /opt/conda/lib/python3.8/sitepackages/torch/include/THC isystem /usr/local/cuda/include isystem /opt/conda/inBCXX_USE_CXX11_ABI=1 D__CUDA_NO_HALF_OPERATORS__ D__CUDA_NO_HALF_CONVERSIONS__ D__CUDA_NO_BFLOAT16_CONVERSIONS__ D__CUDA_NO_HALF2_OPERATORS__ exptrelaxedconstexpe_61,code=compute_61 gencode=arch=compute_61,code=sm_61 compileroptions 'fPIC' O3 gencode arch=compute_70,code=sm_70 use_fast_math U__CUDA_NO_HALF_OPERATORS__ [3/3] c++ scaled_masked_softmax.o scaled_masked_softmax_cuda.cuda.o shared L/opt/conda/lib/python3.8/sitepackages/torch/lib lc10 lc10_cuda ltorch_cpu ltorch_cuda [2/3] /usr/local/cuda/bin/nvcc  DTORCH_EXTENSION_NAME=fused_mix_prec_layer_norm_cuda DTORCH_API_INCLUDE_EXTENSION_H DPYBIND11_COMPILER_TYPE=\""_gcc\"" DPYBIND11_STDLIBND11_BUILD_ABI=\""_cxxabi1013\"" isystem /opt/conda/lib/python3.8/sitepackages/torch/include isystem /opt/conda/lib/python3.8/sitepackages/torch/include/torch/csrc/api  > elasped time to build and save shuffleidx mapping (seconds): 0.000984   no_persist_layer_norm ........................... Truelen_indexmap_8000ns_128sl_1234s_doc_idx.npy   no_save_optim ................................... Noneltrain_indexmap_8000ns_128sl_1234s_sample_idx.npy   no_save_rng ..................................... Nonee_train_indexmap_8000ns_128sl_1234s_shuffle_idx.npy   num_attention_heads ............................. 1one   num_channels .................................... 3alse   num_classes ..................................... 1000   num_layers ...................................... 4one indices on rank 0 ...   num_layers_per_virtual_pipeline_stage ........... None of number of samples per epoch (56), setting separate_last_epoch to True   num_workers ..................................... 2024ds): 0.000463   onnx_safe ....................................... Nonee   openai_gelu ..................................... False   optimizer ....................................... adam   override_lr_scheduler ........................... False   params_dtype .................................... torch.float16   patch_dim ....................................... 16neconds): 0.000296   pipeline_model_parallel_size .................... 1alse04) ...   pipeline_model_parallel_split_rank .............. Noneeconds): 0.000271   query_in_block_prob ............................. 0.1seid_indexmap_880ns_128sl_1234s_doc_idx.npy   rampup_batch_size ............................... None_valid_indexmap_880ns_128sl_1234s_sample_idx.npy   rank ............................................ 0alse_valid_indexmap_880ns_128sl_1234s_shuffle_idx.npy   reset_attention_mask ............................ False   reset_position_ids .............................. False   retriever_report_topk_accuracies ................ []1   retriever_score_scaling ......................... Falseindices on rank 0 ...   retriever_seq_length ............................ 256se   sample_rate ..................................... 1.0eorm   save ............................................ checkpoints/gpt2_345m   save_interval ................................... 500aining.py"", line 134, in pretrain   scatter_gather_tensors_in_pipeline .............. True9   seed ............................................ 12348ning.py"", line 884, in build_train_valid_test_data_iterators   seq_length ...................................... 1024edatasets_provider(   sgd_momentum .................................... 0.92datasets_provider   short_seq_prob .................................. 0.1sedatasets(   split ........................................... 969, 30, 1dataset.py"", line 38, in build_train_valid_test_datasets   tensor_model_parallel_size ...................... 112[0],   tensorboard_dir ................................. None5/gpt_dataset.py"", line 118, in _build_train_valid_test_datasets   tensorboard_log_interval ........................ 1one   tensorboard_queue_size .......................... 1000kpoints/gpt2_345m, line 110, in build_dataset   titles_data_path ................................ Nonee   tokenizer_type .................................. GPT2BPETokenizert.py"", line 152, in __init__   train_itera_ids ................................. 0000uild_index_mappings(   vocab_file ...................................... gpt2vocab.jsonet.py"", line 242, in _build_index_mappings   weight_decay .................................... 0.01eh + 1), \   world_size ...................................... 1rueeparallel size: 1, pipelinemodelparallel size: 1   end of arguments  setting number of microbatches to constant 2...... Falserch v1.11 (nvidia pytorch container paired with v1.11). Defaulting to no_persist_layer_norm=True > building GPT2BPETokenizer tokenizer ............. Noneeretrain_gpt_yyt2.sh > initializing tensor model parallel with size 1ns (new size: 50304) > initializing pipeline model parallel with size 1 > setting random seeds to 1234 ... > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234 make: Nothing to be done for 'default'. make: Leaving directory '/workspace/megtron/MegatronLMmain/megatron/data'' >>> done with dataset index builder. Compilation time: 0.114 seconds > compiling and loading fused kernels ... Detected CUDA files, patching ldflags Emitting ninja build file /workspace/megtron/MegatronLMmain/megatron/fused_kernels/build/build.ninja... Building extension module scaled_upper_triang_masked_softmax_cuda... Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N) ninja: no work to do. Loading extension module scaled_upper_triang_masked_softmax_cuda... Detected CUDA files, patching ldflags Emitting ninja build file /workspace/megtron/MegatronLMmain/megatron/fused_kernels/build/build.ninja... Building extension module scaled_masked_softmax_cuda... Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N) Building extension module scaled_upper_triang_masked_softmax_cuda... Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N) ninja: no work to do. Loading extension module scaled_upper_triang_masked_softmax_cuda... Detected CUDA files, patching ldflags Emitting ninja build file /workspace/megtron/MegatronLMmain/megatron/fused_kernels/build/build.ninja... Building extension module scaled_masked_softmax_cuda...Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)  > building shuffle index with split [0, 8030) and [8030, 8030) ...  > elasped time to build and save shuffleidx mapping (seconds): 0.000436  > loading docidx mapping from mygpt2_text_document_train_indexmap_8000ns_1024sl_1234s_doc_idx.npy  > loading sampleidx mapping from mygpt2_text_document_train_indexmap_8000ns_1024sl_1234s_sample_idx.npy  > loading shuffleidx mapping from mygpt2_text_document_train_indexmap_8000ns_1024sl_1234s_shuffle_idx.npy     loaded indexed file in 0.001 seconds     total number of samples: 8031    total number of samples: 8031     total number of epochs: 36     total number of epochs: 36  > WARNING: could not find index map files, building the indices on rank 0 ...  > last epoch number of samples (5) is larger than 80% of number of samples per epoch (7), setting separate_last_epoch to False  > elasped time to build and save docidx mapping (seconds): 0.000500     using:      number of documents:       30      number of epochs:          125      sequence length:           1024      total number of samples:   883  > elasped time to build and save sampleidx mapping (seconds): 0.000285  > building shuffle index with split [0, 883) and [883, 883) ...  > elasped time to build and save shuffleidx mapping (seconds): 0.000232  > loading docidx mapping from mygpt2_text_document_valid_indexmap_880ns_1024sl_1234s_doc_idx.npy  > loading sampleidx mapping from mygpt2_text_document_valid_indexmap_880ns_1024sl_1234s_sample_idx.npy  > loading shuffleidx mapping from mygpt2_text_document_valid_indexmap_880ns_1024sl_1234s_shuffle_idx.npy     loaded indexed file in 0.001 seconds     total number of samples: 884     total number of epochs: 125  > WARNING: could not find index map files, building the indices on rank 0 ... Traceback (most recent call last):   File ""pretrain_gpt.py"", line 124, in      pretrain(train_valid_test_datasets_provider, model_provider,   File ""/workspace/megtron/MegatronLMmain/megatron/training.py"", line 134, in pretrain     = build_train_valid_test_data_iterators(   File ""/workspace/megtron/MegatronLMmain/megatron/training.py"", line 884, in build_train_valid_test_data_iterators     train_ds, valid_ds, test_ds = build_train_valid_test_datasets_provider(   File ""pretrain_gpt.py"", line 109, in train_valid_test_datasets_provider     train_ds, valid_ds, test_ds = build_train_valid_test_datasets(   File ""/workspace/megtron/MegatronLMmain/megatron/data/gpt_dataset.py"", line 38, in build_train_valid_test_datasets     return _build_train_valid_test_datasets(data_prefix[0],   File ""/workspace/megtron/MegatronLMmain/megatron/data/gpt_dataset.py"", line 118, in _build_train_valid_test_datasets     test_dataset = build_dataset(2, 'test')  File ""/workspace/megtron/MegatronLMmain/megatron/data/gpt_dataset.py"", line 110, in build_dataset     dataset = GPTDataset(name, data_prefix,   File ""/workspace/megtron/MegatronLMmain/megatron/data/gpt_dataset.py"", line 152, in __init__     self.doc_idx, self.sample_idx, self.shuffle_idx = _build_index_mappings(   File ""/workspace/megtron/MegatronLMmain/megatron/data/gpt_dataset.py"", line 242, in _build_index_mappings     assert last_epoch_num_samples < (num_samples_per_epoch + 1), \ AssertionError: last epoch number of samples exceeded max value. `",2022-01-18T01:33:22Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/176,"Hello, How was the problem solved?","Maybeï¼ŒI donâ€™t follow this issue ä»Ž Windows ç‰ˆé‚®ä»¶å‘é€ å‘ä»¶äºº: ***@***.***> å‘é€æ—¶é—´: 2023å¹´3æœˆ1æ—¥ 14:38 æ”¶ä»¶äºº: ***@***.***> æŠ„é€: ***@***.***>; State ***@***.***> ä¸»é¢˜: Re: [NVIDIA/MegatronLM] Is there any problem in my pretrain_gpt.sh? The AssertionError: last epoch number of samples exceeded max value. (Issue CC(Is there any problem in my pretrain_gpt.sh? The AssertionError: last epoch number of samples exceeded max value.)) Hello, How was the problem solved? â€• Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you modified the open/close state.Message ID: ***@***.***>",are you using personal hardware? try reduce            seqlength 1024 \           maxpositionembeddings 1024 \  start with 200  800  i think it is related to how much vram you have
marigoold,Does scaled softmax of qkv perform scaling operations duplicately?,"I implemented the t5 model and compared it with the implementation in Megatron using the same weights, with the following results ``` max embedding diff:  0.0 max encoder layers0 diff:  0.00013916194 max encoder layers1 diff:  0.0049294047 max encoder layers2 diff:  0.0081669465 max encoder layers3 diff:  0.011228945 max encoder layers4 diff:  0.014814734 max encoder layers5 diff:  0.015128866 max encoder layers6 diff:  0.016686544 max encoder layers7 diff:  0.019047052 max encoder layers8 diff:  0.022120267 max encoder layers9 diff:  0.023271054 max encoder layers10 diff:  0.023713648 max encoder layers11 diff:  0.025910359 ``` When I set the `layer_index` of all encoder layers in my implementation to 0, the comparison results are as follows:  ``` max embedding diff:  0.0 max encoder layers0 diff:  0.00013916194 max encoder layers1 diff:  0.00024357438 max encoder layers2 diff:  0.0003227368 max encoder layers3 diff:  0.00041988492 max encoder layers4 diff:  0.00054493546 max encoder layers5 diff:  0.0006237328 max encoder layers6 diff:  0.0006262362 max encoder layers7 diff:  0.0006904714 max encoder layers8 diff:  0.0007108152 max encoder layers9 diff:  0.0008518249 max encoder layers10 diff:  0.0008100569 max encoder layers11 diff:  0.0008664727 ``` I tracked and debugged the Megatron t5 model and found that both the numerator and denominator of attention_score were scaled by layer_index before calculating the softmax. numerator scaling:  https://github.com/NVIDIA/MegatronLM/blob/9a8b89acd8f6ba096860170d0e30ddc0bc2bacd4/megatron/model/fused_softmax.pyL171L176 denominator scaling: https://github.com/NVIDIA/MegatronLM/blob/9a8b89acd8f6ba096860170d0e30ddc0bc2bacd4/megatron/model/transformer.pyL156L159 The two scaling operations cancel each other out and the result is equivalent to that with no scaling, therefore the scaling factor of each layer of encoder is equal.",2022-01-12T02:44:34Z,stale,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/175,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.,"I believe this is because the qk scale here mostly deals with numerical stability of fp16, where, if divided, logits are easier to represent and scaled back before softmax. This is semantically different than the heuristics that the attention scores should be scaled based on layer indices.",Marking as stale. No activity in 60 days.
ZHAOTING,Optimizer will hold unloaded model weights when model weights but not optimizer states are loaded from a checkpoint.,"Consider the scenario of finetuning a model on a new corpus, for example, we want to load `model` from a checkpoint while keeping `optimizer`'s  states untouched.  However, in the `setup_model_and_optimizer()` function, `optimizer` is built before loading checkpoint and holds the parameters of a randomly initialized model.  If `args.finetune` is `True`, the model state dict will be loaded from the checkpoint, and `optimizer` will be untouched and still have the parameters of the randomly initialized model in its `optimizer.param_groups` instead of the newly loaded `model`. In this case, after the first parameter update, the parameters of the newly loaded `model` will revert to the ones of the randomly initialized model unexpectedly. https://github.com/NVIDIA/MegatronLM/blob/9d86ca677c48b4e006156b1edce631e6c4bd5c12/megatron/training.pyL339L362",2022-01-03T05:38:52Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/174,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
rajeshkppt,Update README.md,typo,2021-12-13T09:01:03Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/173
singleheart,Error: 'DistributedDataParallel' object has no attribute 'set_input_tensor' on text generation. need to unwrap model,"During text generation, I encountered the error below: ```   File ""/home/singleheart/MegatronLM/megatron/text_generation/api.py"", line 114, in generate     return generate_tokens_probs_and_return_on_first_stage(           File ""/home/singleheart/MegatronLM/megatron/text_generation/generation.py"", line 183, in generate_tokens_probs_and_return_on_first_stage     logits = forward_step(tokens2use, positions2use, attention_mask2use)   File ""/home/singleheart/MegatronLM/megatron/text_generation/forward_step.py"", line 87, in __call__     return _no_pipelining_forward_step(self.model,   File ""/home/singleheart/MegatronLM/megatron/text_generation/forward_step.py"", line 143, in _no_pipelining_forward_step output_tensor = _forward_step_helper(model, tokens, position_ids,   File ""/home/singleheart/MegatronLM/megatron/text_generation/forward_step.py"", line 128, in _forward_step_helper     model.set_input_tensor(recv_buffer)   File ""/home/singleheart/.pyenv/versions/megatron/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 1130, in __getattr__     raise AttributeError(""'{}' object has no attribute '{}'"".format( AttributeError: 'DistributedDataParallel' object has no attribute 'set_input_tensor' ``` I think the `model` should be unwrapped to call `set_input_tensor()` using `megatron.utils.unwrap_model` as this: https://github.com/NVIDIA/MegatronLM/blob/d41696840ed0a7edb7e0499eb82a48ae112d9bb3/megatron/schedules.pyL57L58 After unwrapping, I have successfully generated text.",2021-12-09T07:25:57Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/171,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
Yijia-Xiao,Add support of lazy dataloader?,"Hi, I have been using this great framework for a while. However, it seems that the latest version no longer supports lazydataloader. Would you have any plan to add this feature? Because the pretraining corpus could be giant and too big to be stored as a single file. Thank you!",2021-12-04T04:26:06Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/170,"There is no plan to bring back a lazy loader. You can list multiple datasets in the datapath argument, see the format in the help string here: https://github.com/NVIDIA/MegatronLM/blob/d41696840ed0a7edb7e0499eb82a48ae112d9bb3/megatron/arguments.pyL703L707 If they all have the same weight it is functionally the same as concatenating all the input data paths.",Got it. Thank you :)
unlimblue,There is a difference in the calculation of num_warmup_microbatches ,In interleaved1F1Bï¼š https://github.com/NVIDIA/MegatronLM/blob/b31e1296354e979722627a6c4dedafe19b51fa97/megatron/schedules.pyL222L223 but in 1F1B: https://github.com/NVIDIA/MegatronLM/blob/b31e1296354e979722627a6c4dedafe19b51fa97/megatron/schedules.pyL531L533 what is the purpose of this diff?,2021-11-30T10:07:04Z,,closed,0,12,https://github.com/NVIDIA/Megatron-LM/issues/169, Do you remember why interleaved has the `* 2`?,"We are executing the schedules shown in the figure below (top figure is 1F1B, bottom figure is 1F1B + interleaved). !image In the regular 1F1B schedule, the number of warmup microbatches is 3 on device 1 (rank 0), 2 on device 2 (rank 1), 1 on device 3 (rank 2), and 0 on device 4 (rank 3). In the interleaved 1F1B schedule, the number of warmup microbatches is 10 on device 1 (rank 0), 8 on device 2 (rank 1), 6 on device 3 (rank 2), and 4 on device 3 (rank 3). We need 4 warmup microbatches in the interleaved case on the last rank since we need to finish forward passes for all the first chunks on each rank before we can enter steady state. Note that the number of warmup microbatches would be different if we had three model chunks on each rank instead of two. The purpose of the warmup microbatches is to admit enough work into the pipeline so that every worker is busy in steady state."," So a better understanding is that in interleaved mode, the formula of `num_warmup_microbatches` is expressed as: ```python num_warmup_microbatches = \       (pipeline_parallel_size  pipeline_parallel_rank  1) * num_model_chunks ```  .","Not quite. The number of warmup microbatches in interleaved mode is this (which is what the implementation does as well): ``` num_warmup_microbatches = ((pipeline_parallel_size  pipeline_parallel_rank  1) * 2) +     ((num_model_chunks  1) * pipeline_parallel_size) ``` For the example timeline above, `pipeline_parallel_size` is 4 and `num_model_chunks` is 2. Which is how we get 10 warmup microbatches on rank 0, 8 on rank 1, 6 on rank 2, and 4 on rank 3.","In summary, if I want to expand to arbitrary model chunks, I need modify the formula of `num_warmup_microbatches` to: ```python num_warmup_microbatches = ((pipeline_parallel_size  pipeline_parallel_rank  1) * num_model_chunks) +     ((num_model_chunks  1) * pipeline_parallel_size) ``` . In addition, I also noticed that the estimation formula of pipeline bubble time fraction mentioned in the paper is: ï¼Œ where m is the number of microbatches in a batch, p is the number of pipeline stages and v is `num_model_chunks`.  Increasing v (`num_model_chunks`) will reduce bubble time fraction, but will introduce more interstage communication.  So,   setting `num_model_chunks` to 2 is the optimal empirical value obtained by your actual experiments?   is this value recommended for different models (our customized models)? Or we need to analyze the specific situation.","No, the formula I posted above already takes into account `num_model_chunks`. The `*2` comes from the fact that we need 2 more warmup microbatches in the first rank compared to the second, and second compared to the third, and so on (which is why rank 0 needs 10 warmup microbatches, rank 1 needs 8, etc.). For an arbitrary `num_model_chunks`, the number of warmup microbatches is: ``` num_warmup_microbatches = ((pipeline_parallel_size  pipeline_parallel_rank  1) * 2) +     ((num_model_chunks  1) * pipeline_parallel_size) ``` You are right that increasing the number of model chunks decreases the size of the pipeline bubble, but increases the amount of inpipeline communication. The optimal value is model and hardwaredependent and is not 2 for all deployments. You should play around with this a bit for your specific situation.","Thanks for your clarification! What I understand is that the purpose of this `*2` is, when the pipeline parallel executes to the steady state, compared to without `*2`, the timeline of all devices is shifted to the right by one forward calculation time, so that you can use `send_forward_backward_recv_forward_backward` to make full use of the communication bandwidth as much as possible. And this operation will not change the bubble ratio. + without `*2`    !image + with `*2`    !image In addition, by analyzing the timeline in the 1F1B mode, when the timeline is shifted to the right by one forward calculation time, which can also be optimized for communication bandwidth, and the bubble ratio has not changed. + without `*2`    !image + with `*2`    !image Is the above approach reasonable?","Yup, that's correct! Good analysis. And yes, you could use the same approach with the default 1F1B approach as well. Note that there is added memory footprint from doing this though (as was the case with the interleaved schedule as well).","I forgot to add one thing: without the `*2` in the interleaved schedule, it was unclear how to schedule communication when the model wraps around. For example, consider the forward pass of the second chunk of microbatch 5. When should rank 3 send the intermediate forward tensor to rank 0 if we use the `without *2` schedule?","This question gave me a clearer understanding of the reason why the schedule of interleaved1F1B and 1F1B is so different.  1. use 1F1B schedule to get the timeline of interleaved1F1B: !image 2. in order to solve the second chunk of microbatch 5 problem, we need to shift all forwards to the left to start the recv operation of chunk 2 as soon as possible: !image 3. due to the limitation of inpipeline communication of microbatch 2 forward, middle part of timeline need to be shifted to the right: !image 4. after all other ranks have done the same processing, our timeline is the same as interleaved schedule: !image","Yup, exactly!","Hi, after checked all content above, I still have a little question: will the microbatch 5 problem in `without *2` schedule still be a problem if we use pytorch's `isend/irecv` interface or just do p2p communication in another CUDA stream?"
kvtoraman,Update README.md,Remove duplicated bulletpoint,2021-11-30T06:39:38Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/168
jasperzhong,torch.cuda.synchronize() might be unnecessary in p2p_communication.py,"Hi. I notice that there is an explicit cuda device synchronization to avoid race condition in p2p_communication.py. ```py         if len(ops) > 0:             reqs = torch.distributed.batch_isend_irecv(ops)             for req in reqs:                 req.wait()      To protect against race condition when using batch_isend_irecv().     torch.cuda.synchronize() ``` However, I think the synchronization here is not needed. This is because `req.wait()` will block the default stream (i.e., the compute stream) until the communication operations on the NCCL stream finishes. Refer to this upstream issue https://github.com/pytorch/pytorch/issues/68112 and the related code for details. ",2021-11-24T07:27:20Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/167,I found the loss becomes nan without the synchronization. I will study this problem. ,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
xiongjun19,<Signals.SIGSEGV: 11> occured in multiple nodes pretraing,"Hi, Dear!    I'm trying to pretraing bert in two nodes! I used the docker image pulled from nvcr.io/nvidia/pytorch:20.12py3 as my evirnoment, I have managed to run pretraining in a node with single T4 card. but when I trying to train the same model with two nodes both with single T4 card, I always getting the following Error: !image but if I setting nnodes as one all things goes well? can anyone give me some guide? thanks very much",2021-11-22T07:53:21Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/166,"In my case, it was due to firewall setting. Setting your NIC into 'trusted' zone in firewall may help you from this error.",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
tanmoyio,LM for long sequence (e.g. - BigBird) support into Megatron-LM,I am exploring possible ways to add BigBird support to current MegatronLM. I would like to work on it. Let me know if this sound good or not. ,2021-11-21T19:03:56Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/165
unlimblue,Why is it 3us?,"https://github.com/NVIDIA/MegatronLM/blob/b31e1296354e979722627a6c4dedafe19b51fa97/megatron/mpu/layers.pyL226 Mentioned in the comment above:  > Delay the start of weight gradient computation shortly (3us) to have allreduce scheduled first and have GPU resources allocated , but I am confused about some details: 1. Why should we wait for allreduce scheduled and GPU resources allocated? 2. Why is it 3us but nothing else? 3. What is the purpose of +1 after torch.empty?",2021-11-19T08:44:21Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/164,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
whulxl,AttributeError: 'IndexedDataset' object has no attribute 'get_doc_idxâ€™,"I am preparing the data for GPT training according with the MegatronLM project'sï¼ˆtag 1.1ï¼‰ README.md at path ""MegatronLM/tools/openwebtext/tools"", I run the preprocess_dataset.py, the code in it: python3 tools/preprocess_data.py input data/corpus/test.json outputprefix mybert vocab data/tokenizer/bert/bertlargeuncasedvocab.txt tokenizertype BertWordPieceLowerCase splitsentences datasetimpl cached i use â€œcachedâ€ type of datasetimpl but when i run  python3 pretrain_bert.py \        numlayers 12 \        hiddensize 1024 \        numattentionheads 16 \        batchsize 4 \        seqlength 512 \        maxpositionembeddings 512 \        trainiters 2000000 \        save $CHECKPOINT_PATH \        load $CHECKPOINT_PATH \        datapath $DATA_PATH \        vocabfile data/tokenizer/bert/bertlargeuncasedvocab.txt \        dataimpl cached\        split 949,50,1 \        distributedbackend nccl \        lr 0.0001 \        minlr 0.00001 \        lrdecaystyle linear \        lrdecayiters 990000 \        weightdecay 1e2 \        clipgrad 1.0 \        warmup .01 \        loginterval 100 \        saveinterval 10000 \        evalinterval 1000 \        evaliters 10 \        fp16 I encountered the below error: AttributeError: 'IndexedDataset' object has no attribute 'get_doc_idx' I want to know how can I fix the error, someone can give me some tips, thx!",2021-11-17T13:31:34Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/163,"I met the same problem,did you fix this?",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
Neleon,ImportError: cannot import name 'Tokenizer' from 'tokenizer',"I am preparing the data for GPT training according with the MegatronLM project's README.md at path ""MegatronLM/tools/openwebtext/tools"", I run the cleanup_dataset.py, the code in it: import sys from tokenizer import Tokenizer MIN_DOCUMENT_LENGHT = 128 I encountered the below error: ImportError: cannot import name 'Tokenizer' from 'tokenizer' I want to know where can I find the tokenizer lib, someone can give me some tips, thx!",2021-11-17T07:02:07Z,duplicate stale,closed,0,5,https://github.com/NVIDIA/Megatron-LM/issues/162,"Hi, I'm having the same problem. Did you solve the problem? Please, give me a hint! Thanks!",æˆ‘ä¹Ÿç¢°åˆ°äº†åŒæ ·çš„é—®é¢˜ï¼Œè¯·é—®æœ‰äººå·²ç»è§£å†³äº†å—ï¼Ÿ,The 'tokenizer' in 'from tokenizer import Tokenizer' is not a lib. And fixes are Issue33 and Issue89. > æˆ‘ä¹Ÿç¢°åˆ°äº†åŒæ ·çš„é—®é¢˜ï¼Œè¯·é—®æœ‰äººå·²ç»è§£å†³äº†å—ï¼Ÿ,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Closing. Duplicate of: https://github.com/NVIDIA/MegatronLM/issues/33
xiongjun19,How to do punctuation  standardzation?,I'm wondering how to do the standardzation of the extracted corpus from wikipedia ? ,2021-11-17T06:36:06Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/161,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
skye95git,Can't get attribute 'ModelType' on <module 'megatron.model.enums',"I use code corpus to pretrain the GPT model. When I load the pretrained model to provide interactive services ``` export MASTER_ADDR=localhost export MASTER_PORT=5677 singularity exec nv /home/linjiayi/pytorch_20.12py3.sif examples/generate_code.sh ``` generate_code.sh: ``` !/bin/bash CHECKPOINT_PATH=/home/linjiayi/MegatronLMold/checkpoint_code VOCAB_FILE=/home/linjiayi/MegatronLMold/data/gpt2vocab.json MERGE_FILE=/home/linjiayi/MegatronLMold/data/gpt2merges.txt python tools/generate_samples_gpt.py \        tensormodelparallelsize 1 \        numlayers 24 \        hiddensize 1024 \        load $CHECKPOINT_PATH \        numattentionheads 16 \        maxpositionembeddings 1024 \        tokenizertype GPT2BPETokenizer \        fp16 \        microbatchsize 2 \        seqlength 1024 \        outseqlength 1024 \        temperature 1.0 \        vocabfile $VOCAB_FILE \        mergefile $MERGE_FILE \        genfile unconditional_samples.json \        numsamples 0 \        top_p 0.9 \        recompute ``` When I run the code generation service, **there is an error**: ``` loading checkpoint from /home/linjiayi/MegatronLMold/checkpoint_code at iteration 450000 could not load the checkpoint Can't get attribute 'ModelType' on  ``` But it was fine when I ran text generation using the pretraining model you provided:  Is there something wrong with my pretrained model? What should I do?",2021-11-17T02:30:08Z,stale,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/160,Did you solve itï¼Ÿ,Same problemï¼Œdid anyone solver this problemÂ·ï¼Ÿ,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
ZHAOTING,Call of `_build_train_valid_test_datasets` misses one positional argument `max_seq_length_dec`.,"Hi, I noticed this bug when trying to train BERT with multiple input data paths. `max_seq_length_dec` is a required positional argument of function `_build_train_valid_test_datasets` but is not provided in the case of being given multiple data prefixes. https://github.com/NVIDIA/MegatronLM/blob/b31e1296354e979722627a6c4dedafe19b51fa97/megatron/data/dataset_utils.pyL445",2021-11-12T04:24:39Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/159,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
cklsoft,How to use Megatron-like bert to inference online?,"Hi,      After pretraining a Megatronlike bert model, how can we use it online for inference ? By converting it to an original BERT model, or using Megatron inference code. Note that, we can not use `nvcr.io/nvidia/pytorch:20.12py3` in our servers for inference.   ",2021-11-11T03:46:09Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/158,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
ParamsRaman,Usage of --use-cpu-initialization flag,"Hi, I have a few questions regarding the `usecpuinitialization` flag in Megatron. 1. Can you explain the use of this flag? I see the arguments description for it, but it is not clear to me under which situation would one use this flag? ``` group.add_argument('lazympuinit', type=bool, required=False,                        help='If set to True, initialize_megatron() '                        'skips DDP initialization and returns function to '                        'complete it instead.Also turns on '                        'usecpuinitialization flag. This is for '                        'external DDP manager.' ) group.add_argument('usecpuinitialization', action='store_true',                    default=None, help='If set, affine parallel weights '                    'initialization uses CPU' ) ``` 2. This flag seems to be used in the constructor for several model parallel classes such as VocabParallelEmbedding, ParallelEmbedding, RowParallelLinear and ColumnParallelLinear defined in https://github.com/NVIDIA/MegatronLM/blob/main/megatron/mpu/layers.py . Based on whether this flag is set, two sets of weights are initialized  self.weight and self.master_weight as shown here: https://github.com/NVIDIA/MegatronLM/blob/main/megatron/mpu/layers.pyL391,L404 .  Could you explain why additional master_weight is used? 3. In the unit tests for layers.py I see that master_weight is accessed  https://github.com/NVIDIA/MegatronLM/blob/b31e1296354e979722627a6c4dedafe19b51fa97/megatron/mpu/tests/test_layers.pyL284  Does this mean the use cpu initialization flag has to bet set while running the unit tests?  Appreciate any clarification on this. Thanks!",2021-11-03T20:50:09Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/157,"The `usecpuinitialization` is primarily used for testing, but it can also be used in the case that you want to initialize things on a node with no GPU (some tests can be run without GPU, various tools, etc.).  The master_weight returned is only used for testing."
jamesr66a,Strike note that PP is not supported for T5 in README.md,"PP seems to have been added in https://github.com/NVIDIA/MegatronLM/commit/46c74b4ca06a7794db1e2615544095535cdf12c2, so I think this clause is not accurate anymore",2021-11-02T17:47:58Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/156
satpalsr,Typo corrections,"Updated utils.py Fixed an unnecessary ""\"". Fixed typos in README.md",2021-10-31T04:35:34Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/155
Yijia-Xiao,About building *.bin and *.idx,"Hi, thank you for your great work! I've been using MegatronLM for some time, and I've encountered some problems in building a large dataset. I used preprocess_data.py to build a `jsonl` (about 1TB) to *.bin and *.idx file; the server comes with 504GB memory. But unfortunately, when the *.bin grows to about 600GB, the process seems to be dead. I wonder if there are some solution for big corpus, or will the lazy loader works? Thank you:)",2021-10-29T02:56:46Z,bug stale,closed,0,6,https://github.com/NVIDIA/Megatron-LM/issues/154,"Hello, has this issue been resolved?",This issue will be addressed in the next few days by an update to `preprocess_data.py` that allows processing a large dataset in multiple partitions and thereby avoiding OOM errors. I'll update this issue when the update hits.,Marking as stale. No activity in 60 days.,Qual porcentagens jÃ¡ estÃ¡ para finalizar o projeto? ,Pode concluir tÃ¡ autorizado ,Marking as stale. No activity in 60 days.
puririshi98,making megatron large language models rely on apex.transformer instead of mpu,"This change should carry over to BERT or GPT This PR is not ready/tested yet, creating it solely for the diffs/discussion",2021-10-12T19:05:47Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/153,"Hello, can this method speed up the execution of Megatron"
hyunwoongko,question about softmax kernel,I think pad mask is not applied when we use scale upper triangular mask softmax.  Are there any plans to add pad masking to the scale upper triangular mask softmax kernel?,2021-10-12T12:22:01Z,enhancement,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/152,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,"There's no immediate plan to add this, but we'd welcome a PR if you want to do it. Thanks."
hyunwoongko,bug of fused softmax kernel.,I think `scale` must be input other way from now.,2021-10-12T12:16:07Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/151,It was my mistake.
hyunwoongko,is fused layernorm really better?,https://github.com/pytorch/pytorch/commit/8b87f9a5107e8b3c4f87d5297af698bb55838d81difff12c726e3e8cd2b4768f8984fef27059 I think we don't need to use apex fused layernorm anymore.  torch layernorm is better. what do you think about this? cc.   ,2021-10-11T14:17:12Z,enhancement stale,closed,1,3,https://github.com/NVIDIA/Megatron-LM/issues/150,In this pull request `apex` maintainer pretty much says that `apex` is deprecated. MegatronLM should remove its dependency on `apex` completely.,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
Lavenderjiang,'BertModel' has no len(),"Hello, I'm trying to use tools/merge_mp_partitions to combine BERT checkpoints trained with model parallelism.  However, it seems like BERT model does not have __len__(), and the script gives a type error. The checkpoints were generated by Megatron, and I did not change anything. Any help would be appreciated! Here's the error message: ``` merging model parallel partitions ...  > number of partitions: 2  > checkpoint path: ./finetune_dir  > model parameters:     number of tokens ................ 30002      number of layers ................ 48     hidden size ..................... 256     number of attention heads ....... 256     maximum position embeddings ..... 512 initializing torch.distributed... torch distributed initialized! > building the full model ... building BERT model ... > building BertWordPieceCase tokenizer ...  > padded vocab (size: 30002) with 206 dummy tokens (new size: 30208) building BERT model ... > loading ./finetune_dir/iter_0850000/mp_rank_00/model_optim_rng.pt ...  loading checkpoint from ./finetune_dir at iteration 850000 Traceback (most recent call last):   File ""./merge_mp_partitions.py"", line 364, in      main()   File ""./merge_mp_partitions.py"", line 266, in main     load_checkpoint(model_, None, None)   File ""~/pretraining/megatron/checkpointing.py"", line 359, in load_checkpoint  if len(model) == 1: TypeError: object of type 'BertModel' has no len() ```",2021-09-27T18:17:06Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/149,"Resolved by putting the model in a list. Changed this line to  `   load_checkpoint([model_], None, None)`",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
alxiom,logical discrepancy of use_checkpoint_lr_scheduler,"We suppose NOT to use checkpoint, but why checking `cls_value == sd_value`? https://github.com/NVIDIA/MegatronLM/blob/3860e995269df61d234ed910d4756e104e1ab844/megatron/learning_rates.pyL121L122",2021-09-24T02:34:27Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/148,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
skye95git,No module named 'amp_C',"I have downloaded the pretrained GPT345M checkpoint. I want to try GPT Text Generation task. I run the command: ```shell CHECKPOINT_PATH=checkpoints/gpt2_345m VOCAB_FILE=gpt2vocab.json MERGE_FILE=gpt2merges.txt GPT_ARGS=""numlayers 24 \           hiddensize 1024 \           numattentionheads 16 \           seqlength 1024 \           maxpositionembeddings 1024 \           microbatchsize 4 \           globalbatchsize 8 \           lr 0.00015 \           trainiters 500000 \           lrdecayiters 320000 \           lrdecaystyle cosine \           vocabfile $VOCAB_FILE \           mergefile $MERGE_FILE \           lrwarmupfraction .01 \           fp16"" MAX_OUTPUT_SEQUENCE_LENGTH=1024 TEMPERATURE=1.0 TOP_P=0.9 NUMBER_OF_SAMPLES=2 OUTPUT_FILE=samples.json python tools/generate_samples_gpt.py \        $GPT_ARGS \        load $CHECKPOINT_PATH \        outseqlength $MAX_OUTPUT_SEQUENCE_LENGTH \        temperature $TEMPERATURE \        genfile $OUTPUT_FILE \        numsamples $NUMBER_OF_SAMPLES \        top_p $TOP_P \        recompute ``` There is an error:  `No module named 'amp_C'` I install apex: ```shell git clone https://www.github.com/nvidia/apex   cd apex   python3 setup.py install ``` There is still an error: `No module named 'amp_C'` What should I do?",2021-09-17T09:38:53Z,stale,closed,0,14,https://github.com/NVIDIA/Megatron-LM/issues/147,"Hi, I run text generation with docker `nvcr.io/nvidia/pytorch:20.12py3`.: `bash examples/generate_text.sh` There is an error:  I can't find `generate_and_write_samples_unconditional` in `text_generation_utils.py`. What should I do?","> I have downloaded the pretrained GPT345M checkpoint. I want to try GPT Text Generation task. I run the command: >  > ```shell > CHECKPOINT_PATH=checkpoints/gpt2_345m > VOCAB_FILE=gpt2vocab.json > MERGE_FILE=gpt2merges.txt > GPT_ARGS=""numlayers 24 \ >           hiddensize 1024 \ >           numattentionheads 16 \ >           seqlength 1024 \ >           maxpositionembeddings 1024 \ >           microbatchsize 4 \ >           globalbatchsize 8 \ >           lr 0.00015 \ >           trainiters 500000 \ >           lrdecayiters 320000 \ >           lrdecaystyle cosine \ >           vocabfile $VOCAB_FILE \ >           mergefile $MERGE_FILE \ >           lrwarmupfraction .01 \ >           fp16"" >  > MAX_OUTPUT_SEQUENCE_LENGTH=1024 > TEMPERATURE=1.0 > TOP_P=0.9 > NUMBER_OF_SAMPLES=2 > OUTPUT_FILE=samples.json >  > python tools/generate_samples_gpt.py \ >        $GPT_ARGS \ >        load $CHECKPOINT_PATH \ >        outseqlength $MAX_OUTPUT_SEQUENCE_LENGTH \ >        temperature $TEMPERATURE \ >        genfile $OUTPUT_FILE \ >        numsamples $NUMBER_OF_SAMPLES \ >        top_p $TOP_P \ >        recompute > ``` >  > There is an error: `No module named 'amp_C'` >  > I install apex: >  > ```shell > git clone https://www.github.com/nvidia/apex   > cd apex   > python3 setup.py install > ``` >  > There is still an error: `No module named 'amp_C'` >  > What should I do? Hi, I met the same error. Did you fix it?",Same error,Same error,Same error ,Marking as stale. No activity in 60 days.,"You should install `apex` using the following command instead of using `pip` or `python setup.py`: ```bash pip install v disablepipversioncheck nocachedir nobuildisolation globaloption=""cpp_ext"" globaloption=""cuda_ext"" ./ ``` For more detailed information, please refer to the following document: https://github.com/NVIDIA/apex/blob/master/README.mdlinux","> You should install `apex` using the following command instead of using `pip` or `python setup.py`: >  > ```shell > pip install v disablepipversioncheck nocachedir nobuildisolation globaloption=""cpp_ext"" globaloption=""cuda_ext"" ./ > ``` >  > For more detailed information, please refer to the following document: https://github.com/NVIDIA/apex/blob/master/README.mdlinux I did this, but I still get the same error unfortunately. Anyone able to fix this?",Marking as stale. No activity in 60 days.,Same problem. Any solution?,same problem,Same problem on PPC64LE and it is a complete and utter showstopper of course. Does noone care?,This worked for me when installing the apex package. https://github.com/NVIDIA/apex/issues/1763issuecomment1900584359,Marking as stale. No activity in 60 days.
abdul756,"RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`, CUDA error: device-side assert triggered, ","When i am running the pretrain_bert.py script  NK=0 WORLD_SIZE=1 DATA_PATH='my_bert_sentence1/my_bert_sentence_text_sentence' CHECKPOINT_PATH='checkpoints' !python m torch.distributed.launch MegatronLM/pretrain_bert.py \        numlayers 2 \        hiddensize 16 \        numattentionheads 16 \        microbatchsize 8 \        globalbatchsize 16 \        seqlength 512 \        maxpositionembeddings 512 \        trainiters 10 \        lrdecayiters 100 \        save $CHECKPOINT_PATH \        load $CHECKPOINT_PATH \        datapath $DATA_PATH \        vocabfile vocab.txt \        nomaskedsoftmaxfusion \        dataimpl mmap \        split 10 \        lr 0.0001 \        minlr 0.00001 \        lrdecaystyle linear \        lrwarmupfraction .01 \        weightdecay 1e2 \        clipgrad 1.0 \        loginterval 100 \        saveinterval 10000 \        evalinterval 1000 \        evaliters 10 \        fp16 I am facing the error RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`, CUDA error: deviceside assert triggered,  Please help me",2021-09-15T08:31:17Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/146,kill your all existed python processes about megatron. ,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
abdul756,AttributeError: 'Namespace' object has no attribute 'model_parallel_size',"When i am running the preprocess.py  file its showing error Namespace' object has no attribute 'model_parallel_size' !python MegatronLM/tools/preprocess_data.py \        input 'manifest_file.json'\        outputprefix 'my_t5' \        vocab 'vocab/vocab.txt' \        datasetimpl mmap \        tokenizertype BertWordPieceLowerCase \        workers 1 \        splitsentences Error: Opening manifest_file.json > building BertWordPieceLowerCase tokenizer ... Traceback (most recent call last):   File ""MegatronLM/tools/preprocess_data.py"", line 203, in      main()   File ""MegatronLM/tools/preprocess_data.py"", line 155, in main     tokenizer = build_tokenizer(args)   File ""/opt/conda/lib/python3.6/sitepackages/megatron/tokenizer/tokenizer.py"", line 48, in build_tokenizer     args)   File ""/opt/conda/lib/python3.6/sitepackages/megatron/tokenizer/tokenizer.py"", line 59, in _vocab_size_with_padding     args.model_parallel_size AttributeError: 'Namespace' object has no attribute 'model_parallel_size'",2021-09-14T18:14:56Z,,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/145,This happens due to mixing the installed megatron package with the repo checkout. If you uninstall the megatron package it should pick up the tokenizer in the repo and work as expected.,Thanks for the help . Can you guide me how to do this is in a jupyter notebook,"Sorry, I'm not familiar with jupyter","> Sorry, I'm not familiar with jupyter Fine , thank you so much for the help"
weiliw-amz,BERT distributed pretrain siginificantly slower for multinode than single-node,"I followed README and https://github.com/NVIDIA/MegatronLMdistributedpretraining to run the following distributed pretrain, and found the `backwardcompute` step cost far more time for multinode, than on singlenode.  Machine  AWS `p3.2xlarge` instances with NVIDIA Tesla V100 GPU, 16GB GPU memory, up to 10 Gbps networking, no GPU P2P.  Data A small proportion of Wikipedia data downloaded from README https://github.com/NVIDIA/MegatronLMcollectingwikipediatrainingdata Concatenated first 100 loose JSON files in text/AA folder Preprocessed for BERT according to README https://github.com/NVIDIA/MegatronLMdatapreprocessing  Packages PyTorch: 1.9.0 + CUDA 11.1 + NCCL 2708 CUDA: 11.1 Apex: Compiled at commit 9ce0a10fb6c2537ef6a59f27b7875e32a9e9b8b8 Megatron: commit 3860e995269df61d234ed910d4756e104e1ab844  Model Pretrain Pretrain a BERT model with following run script containing all parameters, using `DDPimpl torch` ``` !/bin/bash export CXX=/usr/bin/g++ DISTRIBUTED_ARGS=""nproc_per_node 1 \                   nnodes $WORLD_SIZE \                   node_rank $RANK \                   master_addr $MASTER_ADDR \                   master_port $MASTER_PORT"" CHECKPOINT_PATH=checkpoints/bert_run CHECKPOINT_ARGS=""save $CHECKPOINT_PATH \                  load $CHECKPOINT_PATH \                  saveinterval 100"" VOCAB_FILE=/fsx/dataset/wikitext/json_small/bert_vocab_AA.txt DATA_PATH=/fsx/dataset/wikitext/json_small/wikiAAbert_text_sentence BERT_ARGS=""numlayers 24 \            hiddensize 1024 \            numattentionheads 16 \            seqlength 512 \            maxpositionembeddings 512 \            lr 0.0001 \            lrdecayiters 9900 \            trainiters 20000 \            minlr 0.00001 \            lrwarmupfraction 0.01 \            microbatchsize 4 \            globalbatchsize 8 \            vocabfile $VOCAB_FILE \            split 949,50,1 \            fp16"" OUTPUT_ARGS=""loginterval 10 \              saveinterval 500 \              evalinterval 100 \              evaliters 10 \              activationscheckpointmethod uniform"" python3 m torch.distributed.launch $DISTRIBUTED_ARGS \        ./MegatronLM/pretrain_bert.py \        $BERT_ARGS \        $OUTPUT_ARGS \        $CHECKPOINT_ARGS \        datapath $DATA_PATH \        DDPimpl torch            ```  Single Node Run the following: ``` WORLD_SIZE=1 RANK=0 MASTER_ADDR=localhost MASTER_PORT=1234 ./bert_run.sh ```  2 Nodes Run on main node: ``` WORLD_SIZE=2 RANK=0 MASTER_ADDR=$MASTER_ADDR MASTER_PORT=1234 ./bert_run.sh ``` Run on worker node: ``` WORLD_SIZE=2 RANK=1 MASTER_ADDR=$MASTER_ADDR MASTER_PORT=1234 ./bert_run.sh ```  Time Cost  Single Node The overall pretrain costs ~2h. Step time cost info (e.g. iteration 4110, others are similar): ``` iteration     4110/   20000 1>0>1/1/1 ip17231437:9789:10167 [0] NCCL INFO Channel 00 : 1[1e0] > 0[1e0] [receive] via NET/Socket/0 ip17231437:9789:10167 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread ip17231437:9789:10167 [0] NCCL INFO Channel 00 : 0[1e0] > 1[1e0] [send] via NET/Socket/0 ip17231437:9789:10167 [0] NCCL INFO Channel 01 : 1[1e0] > 0[1e0] [receive] via NET/Socket/0 ip17231437:9789:10167 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread ip17231437:9789:10167 [0] NCCL INFO Channel 01 : 0[1e0] > 1[1e0] [send] via NET/Socket/0 ip17231437:9789:10167 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer ip17231437:9789:10167 [0] NCCL INFO comm 0x7f965c002dd0 rank 0 nranks 2 cudaDev 0 busId 1e0  Init COMPLETE ip17231437:9789:9789 [0] NCCL INFO Launch mode Parallel ``` Thanks!",2021-09-14T02:28:21Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/144,"hi, I face the similar situation. Did you solve that?",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
hadyelsahar,"Cannot import C++ compiled ""helpers"" ","I am having errors importing the compiled C++ `helpers.cpp` into python in gpt_dataset.py ```              Use C++ implementation for speed.              First compile and then import.             from megatron.data import helpers             assert doc_idx.dtype == np.int32             assert sizes.dtype == np.int32             sample_idx = helpers.build_sample_idx(sizes, doc_idx, seq_length,                                                   num_epochs, tokens_per_epoch)              sample_idx = _build_sample_idx(sizes, doc_idx, seq_length,                                            num_epochs, tokens_per_epoch) ``` Leading to the following error ``` ImportError: cannot import name 'helpers' from 'megatron.data' (XXXX/MegatronDeepSpeed/megatron/data/__init__.py) Killing subprocess 28861 ``` I managed to isolate the problem by compiling the helpers.cpp separately using `gcc  v9.3.1`. Compilation works but loading `import helpers` doesn't work.   ``` (venvmegatron) bash4.2$ pwd XXX/MegatronDeepSpeed/megatron/data (venvmegatron) bash4.2$ gcc version | head 1  gcc (GCC) 9.3.1 20200408 (Red Hat 9.3.12) (venvmegatron) bash4.2$ make make: python3config: Command not found make: python3config: Command not found g++ O3 Wall shared std=c++11 fPIC fdiagnosticscolor I/nfs/core/python/3.9/include/python3.9 I/xxxx/bigscience/venvmegatron/lib/python3.9/sitepackages/pybind11/include helpers.cpp o helpers (venvmegatron) bash4.2$ ipython Python 3.9.4 (default, Apr  7 2021, 12:46:00) Type 'copyright', 'credits' or 'license' for more information IPython 7.27.0  An enhanced Interactive Python. Type '?' for help. In [1]: import helpers  ModuleNotFoundError                       Traceback (most recent call last)  in  > 1 import helpers ModuleNotFoundError: No module named 'helpers' In [2]: ```",2021-09-14T00:19:15Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/143,"My issue was that `python3config` in the make file is not exported as a variable. ``` CXXFLAGS += O3 Wall shared std=c++11 fPIC fdiagnosticscolor CPPFLAGS += $(shell python3 m pybind11 includes) LIBNAME = helpers LIBEXT = $(shell pythonconfig extensionsuffix) default: $(LIBNAME)$(LIBEXT) %$(LIBEXT): %.cpp 	$(CXX) $(CXXFLAGS) $(CPPFLAGS) $< o $@ ```  Fix   `pip install pythonconfig`  change `python3config` to `pythonconfig` in the make file  if the make works it should output `helper.so` rather than `helper` which is the case here: ``` (venvmegatron) bash4.2$ pwd /xxx/MegatronDeepSpeed/megatron/data (venvmegatron) bash4.2$ make g++ O3 Wall shared std=c++11 fPIC fdiagnosticscolor I/xxxx/python/3.9/include/python3.9 I/xxxxx/venvmegatron/lib/python3.9/sitepackages/pybind11/include helpers.cpp o helpers.so (venvmegatron) bash4.2$ ls lrth help* rwrr. 1 user domain users  27K Sep 14 03:59 helpers.cpp rwxrxrx. 1 user domain users 287K Sep 14 04:26 helpers.so (venvmegatron) bash4.2$ python c ""import helpers"" (venvmegatron) bash4.2$ ```"," posted the workaround but the issue is still there and future users will be running into it just the same. This is at least 2nd time that this is happening on my watch. The previous one occurred some weeks back. The proper flagging of the problem was missing, and was fixed here: https://github.com/bigscienceworkshop/MegatronDeepSpeed/pull/98 I propose https://github.com/NVIDIA/MegatronLM merges this fix. Thank you. ,  "
jasperzhong,the figure in the paper seems wrong,"https://arxiv.org/abs/2104.04473 !image In Figure 4 (Top), for example, forward pass 3 in device 3 cannot be executed until device 3 receives the gradient w.r.t the first microbatch from device 4. The right position of forward pass 3 in device 3 should be right after the backward pass 1 in device 3.  https://github.com/NVIDIA/MegatronLM/blob/3860e995269df61d234ed910d4756e104e1ab844/megatron/schedules.pyL431",2021-09-05T04:08:15Z,stale,closed,3,3,https://github.com/NVIDIA/Megatron-LM/issues/142,"Yes indeed, according to the code: The F3, F4 of Device3 should be postponed to the idle time behind B1, B2 respectively. The F4 of Device2 should be postponed to the idle time behind B1. But this don't affect the overall latency.",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
jasperzhong,What is ring_exchange? ,I am reading the code and get confused with this API.  https://github.com/NVIDIA/MegatronLM/blob/3860e995269df61d234ed910d4756e104e1ab844/megatron/p2p_communication.pyL98L102 I cannot find this function in PyTorch.  Is this a function used internally?,2021-09-04T09:50:46Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/141,This is an internal prototype. Please ignore it.
hyunwoongko,Fix README.md,`GPTPretraining` is duplicated.,2021-09-03T09:20:35Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/140,"Included in another merge, thanks for pointing it out!"
yixuan-qiao,load T5 pretrained model from huggingface or google,"We test the pretraining phase of T5 using model parallel and data parallel. It's really a easy framework to use with high GPU utilization. But there are some points that take us a long time when load pretrained model checkpoints from huggingface or google official a) model structure, such as sentencepiece tokenizer, LN w/o bias, attention w/o bias, etc. b) manually split pretrained checkpoints for 4 model parallel is so painful, is there some better way to do this? we just maintain a list where some of layers are cut by row or column",2021-09-01T00:36:22Z,stale,closed,11,8,https://github.com/NVIDIA/Megatron-LM/issues/139,"I have the same problem and I would like to know how did you solve the problem about position embedding? For T5 models, Huggingface uses relative position but Megatron uses classic embedding method.",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,"I would also like to know how to convert a huggingface checkpoint to a Megatron Checkpoint. Probably we need something opposite to this, https://github.com/huggingface/transformers/blob/main/src/transformers/models/bloom/convert_bloom_original_checkpoint_to_pytorch.py If someone from MegatronLM can give me some hints, I can go ahead and try to write the converter scripts.  the minimal things to do imo is,   1. Write a converter from huggingface ckpt > megatron ckpt 2. Write some test to make sure at least the forward pass is same for both of the model. ",Marking as stale. No activity in 60 days.,Are there any updates about this question?,And the lm_head layer is also different from huggingface ,I have the same question but haven't found any helpful information.,Marking as stale. No activity in 60 days.
jasperzhong,fix a typo,,2021-08-31T13:35:53Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/138
stas00,importing new features from the BigScience Megatron-LM fork,"At https://github.com/bigscienceworkshop/MegatronDeepSpeed new and improved data preprocessing scripts have been created. Most are documented here: https://github.com/bigscienceworkshop/MegatronDeepSpeed/issues/10  and his team are definitely interested in these new features, but it might be difficult for the Megatron team to easily import those complete new features, as the code base is starting to diverse, so perhaps the original creators would be kind enough and have extra resources to import those into Megatron via PR. At the moment this call is primarily for  and  who created the lion shared of those contributions. Guys, I wonder if it'd be easier to make one large PR that includes both of your contributions? I am thinking that as there are several shared files between both of your works. But I trust your judgment should you be moved to do this extra work. If you do that please tag:  and  on your PR. I think  will be the main contact. Thank you all!",2021-08-24T02:06:07Z,stale,closed,3,3,https://github.com/NVIDIA/Megatron-LM/issues/137,"Thanks everyone for the new features. We prefer smaller PRs as it is easier to review, comment, and test them.",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
cassandrayoung,BERT preprocessing stalls when system memory is full,BERT preprocessing stalls when system memory is full. A nonspecific output is seen when the memory limit is reached. Had to check system memory usage to see that it was full.  This is the link to the Wiki dump dataset used: https://dumps.wikimedia.org/other/cirrussearch/20210621/enwiki20210621cirrussearchgeneral.json.gz,2021-08-20T22:58:15Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/136,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
mushan09,Why use the same rng state for weight initialization?,"Q1:I found that in the random.py,  _set_cuda_rng_state is used to ensure that the generated weights are all the same. I want to know why it is necessary to initialize this way? Q2:Why does cross attention need to initialize Q and KV separately, so that the weight after initialization, Q=Kâ‰ V.         if attention_type == AttnType.self_attn:             self.query_key_value = mpu.ColumnParallelLinear(                 args.hidden_size,                 3 * projection_size,                 gather_output=False,                 init_method=init_method)         else:             assert attention_type == AttnType.cross_attn             self.query = mpu.ColumnParallelLinear(                 args.hidden_size,                 projection_size,                 gather_output=False,                 init_method=init_method)             self.key_value = mpu.ColumnParallelLinear(                 args.hidden_size,                 2 * projection_size,                 gather_output=False,                 init_method=init_method)",2021-08-20T08:43:12Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/135,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
eqy,Destroy more groups in `destroy_model_parallel`,Some tests expect a clean model parallel slate and complain if a previous test left something behind; this change clears more variables that the tests complain about.,2021-08-16T22:30:55Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/134
hyunwoongko,Improve and fix bugs about fused softmax layer,1. Fix bugs about `ELEMENTS_PER_LDG_STG` (reported in https://github.com/NVIDIA/MegatronLM/issues/132) 2. Add test codes for all fused cuda kernel using huggingface transformers 3. Add constraint about `0 <= length_key <= 2048` (originally it was in the header file as `TORCH_INTERNAL_ASSERT`) 4. Add constraint about `batch_per_block` (originally it was in the header file as `TORCH_INTERNAL_ASSERT`) 5. Refactor python fused sacle mask softmax layer codes,2021-08-12T21:39:11Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/133,!á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 20210813 á„‹á…©á„Œá…¥á†« 6 45 18 Everything works well., I fixed codes.:),These changes should all be merged in now. Thanks again for the PR!
hyunwoongko,Error in fused softmax kernel result," Problem ? !á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 20210812 á„‹á…©á„Œá…¥á†« 11 28 52 The result of the fused softmax layer is different from the result of the original torch softmax layer.  How to reproduce ? ```python import math import torch from torch.nn import Softmax from transformers import BertTokenizer from transformers.models.bert.modeling_bert import BertModel from fused import FusedScaleMaskSoftmax from fused import AttnMaskType def load_fused_kernels():     try:         import fused_mix_prec_layer_norm_cuda         import scaled_masked_softmax_cuda         import scaled_upper_triang_masked_softmax_cuda         import torch         print(""[Success] load_fused_kernels"")     except ImportError as e:         print(""[Fail] load_fused_kernels"")         raise e def attention_mask_func(attention_scores, attention_mask):     attention_scores.masked_fill_(attention_mask, 10000.0)     return attention_scores def test_softmax():     bert = BertModel.from_pretrained(""bertbasecased"").cuda().half()     tokenizer = BertTokenizer.from_pretrained(""bertbasecased"")      len_query=24, batch_per_block=8 (in my setting)     tokens = tokenizer(         [             ""Hello. How are you? I am fine thank you and you? yes Good. hi hello hello hello hello""         ]         * 4,         return_tensors=""pt"",     )     embedding_output = bert.embeddings(         input_ids=tokens[""input_ids""].cuda(),         position_ids=None,         token_type_ids=tokens[""token_type_ids""].cuda(),         inputs_embeds=None,         past_key_values_length=0,     )      (bsz, 1, 1, seq_len), all values are 0.     mask = bert.get_extended_attention_mask(         attention_mask=tokens[""attention_mask""].cuda(),         input_shape=tokens[""input_ids""].shape,         device=bert.device,     )      (bsz, 1, seq_len, seq_len)     mask = mask.repeat(1, 1, mask.size()[1], 1)     attention = bert.encoder.layer[0].attention.self     query_proj = attention.query     key_proj = attention.key     value_proj = attention.value     key_layer = attention.transpose_for_scores(key_proj(embedding_output))     value_layer = attention.transpose_for_scores(value_proj(embedding_output))     query_layer = attention.transpose_for_scores(query_proj(embedding_output))     attention_scores = torch.matmul(query_layer, key_layer.transpose(1, 2))     attention_scores /= math.sqrt(key_layer.size()[1])     fused_softmax = FusedScaleMaskSoftmax(         mask_func=attention_mask_func,         attn_mask_type=AttnMaskType.padding,         input_in_fp16=True,         input_in_bf16=False,         scale=None,         softmax_in_fp32=False,         scaled_masked_softmax_fusion=True,     )     fused_softmax_output = fused_softmax(         attention_scores,         (mask != 0),     )     torch_softmax = FusedScaleMaskSoftmax(         mask_func=attention_mask_func,         attn_mask_type=AttnMaskType.padding,         input_in_fp16=True,         input_in_bf16=False,         scale=None,         softmax_in_fp32=False,         scaled_masked_softmax_fusion=False,     )     torch_softmax_output = torch_softmax(         attention_scores,         (mask != 0),     )     print(""fused (turn on fusion):"", fused_softmax_output[0][0][0])     print(""\n"")     print(""fused (turn off fusion):"", torch_softmax_output[0][0][0])     torch_softmax = torch.nn.Softmax(dim=1)     torch_softmax_output = torch_softmax(attention_scores)     print(""\n"")     print(""torch softmax"", torch_softmax_output[0][0][0]) if __name__ == ""__main__"":     load_fused_kernels()     test_softmax() ```",2021-08-12T01:29:46Z,,closed,2,22,https://github.com/NVIDIA/Megatron-LM/issues/132,"here is GPT2 test. It has similar problem. ```python def test_upper_mask_softmax():     gpt = GPT2Model.from_pretrained(""gpt2"").cuda().half()     tokenizer = GPT2Tokenizer.from_pretrained(""gpt2"")     tokens = tokenizer(         [             ""Hello. How are you? I am fine thank you and you? yes Good. hi hello hello hello hello hello hello""         ]         * 4,         return_tensors=""pt"",     )     attention_mask = tokens[""attention_mask""].cuda()     attention_mask = attention_mask.view(attention_mask.size(0), 1)     attention_mask = attention_mask[:, None, None, :]     attention_mask = (1.0  attention_mask) * 10000.0     attention_mask = attention_mask.repeat(1, 1, attention_mask.size()[1], 1)     embedding = gpt.wte     c_attn = gpt.h[0].attn.c_attn     c_bias = gpt.h[0].attn.bias     _split_heads = gpt.h[0].attn._split_heads     num_heads = gpt.h[0].attn.num_heads     head_dim = gpt.h[0].attn.head_dim     hidden_states = embedding(tokens[""input_ids""].cuda())     q, k, v = c_attn(hidden_states).split(768, dim=1)     q = _split_heads(q, num_heads, head_dim)     k = _split_heads(k, num_heads, head_dim)     v = _split_heads(v, num_heads, head_dim)     attn_weights = torch.matmul(q, k.transpose(1, 2))     q_length, k_length = q.size(2), k.size(2)     causal_mask = c_bias[:, :, k_length  q_length : k_length, :k_length].bool()     total_mask = ~(causal_mask & (attention_mask == 0))     """"""     tensor([[[[False,  True,  True,  ...,  True,  True,  True],               [False, False,  True,  ...,  True,  True,  True],               [False, False, False,  ...,  True,  True,  True],               ...,               [False, False, False,  ..., False,  True,  True],               [False, False, False,  ..., False, False,  True],               [False, False, False,  ..., False, False, False]]]     """"""     fused_softmax = FusedScaleMaskSoftmax(         mask_func=attention_mask_func,         attn_mask_type=AttnMaskType.causal,         input_in_fp16=True,         input_in_bf16=False,         scale=None,         softmax_in_fp32=False,         scaled_masked_softmax_fusion=True,     )     fused_softmax_output = fused_softmax(         attn_weights,         total_mask,     )     torch_softmax = FusedScaleMaskSoftmax(         mask_func=attention_mask_func,         attn_mask_type=AttnMaskType.causal,         input_in_fp16=True,         input_in_bf16=False,         scale=None,         softmax_in_fp32=False,         scaled_masked_softmax_fusion=False,     )     torch_softmax_output = torch_softmax(         attn_weights,         total_mask,     )     test_result = (         (fused_softmax_output[0][0][1]  torch_softmax_output[0][0][1]).abs().max()     )     if test_result <= 1e6:         print(""[Success] test_upper_mask_softmax"")     else:         print(""[Fail] test_upper_mask_softmax"") ```","layer norm kernel is works well. ```python def test_layer_norm():     bert = BertModel.from_pretrained(""bertbasecased"").cuda().half()     tokenizer = BertTokenizer.from_pretrained(""bertbasecased"")     tokens = tokenizer(         [             ""Hello. How are you? I am fine thank you and you? yes Good. hi hello hello hello hello""         ]         * 4,         return_tensors=""pt"",     )      [bsz, seq_len, d_model]     embedding_output = (         bert.embeddings(             input_ids=tokens[""input_ids""].cuda(),             position_ids=None,             token_type_ids=tokens[""token_type_ids""].cuda(),             inputs_embeds=None,             past_key_values_length=0,         )         .cuda()         .half()     )     fused_layernorm_layer = (         FusedLayerNorm(             normalized_shape=768,             eps=1e5,         )         .cuda()         .half()     )     torch_layernorm_layer = (         LayerNorm(             normalized_shape=768,             eps=1e5,         )         .cuda()         .half()     )     fused_output = fused_layernorm_layer(embedding_output)     torch_output = torch_layernorm_layer(embedding_output)     test_result = (fused_output  torch_output).abs()     while test_result.dim() != 1:         test_result = test_result.mean(dim=1)     diff = test_result.mean(dim=1)     if diff  mean_difference={diff}""             f""\n > fused_values={fused_output[1][1][:5].tolist()}""             f""\n > torch_values={torch_output[1][1][:5].tolist()}""         )     else:         print(             f""\n[Fail] test_layer_norm""             f""\n > mean_difference={diff}, ""             f""\n > fused_values={fused_output[1][1][:5].tolist()}, ""             f""\n > torch_values={torch_output[1][1][:5].tolist()}""         ) ```",!image,I think I'm missing something here... you `import scaled_masked_softmax_cuda` in `load_fused_kernels` but then don't use it. You seem to actually be testing the functions imported here: ``` from fused import FusedScaleMaskSoftmax from fused import AttnMaskType ``` What is this `fused` module?,they are same with megatron's modules. (no code change) I moved related codes to `fused` directory. https://github.com/NVIDIA/MegatronLM/blob/main/megatron/model/fused_softmax.py https://github.com/NVIDIA/MegatronLM/blob/main/megatron/model/enums.py,`import scaled_masked_softmax_cuda` in `load_fused_kernels` is just import test. they do not affect softmax test.,"I think there is something wrong with the kernel or something with my test code. I experimented with compute capability 70 (V100), 75 (T4). but I don't think it's a problem with the development environment because the experimental results were the same. I also changed the batch size to 64, but the results were the same.","Thanks for reporting this. It looks like it is triggered when the sequence length is < 128, which is not something we had tested. Will you test with the following changes? ``` diff git a/megatron/fused_kernels/scaled_masked_softmax.h b/megatron/fused_kernels/scaled_masked_softmax.h index 78e97e4ec60d545a93bf628e6a0135aca707baf5..e80bfe647872dadf912a572560f84ad201315242 100644  a/megatron/fused_kernels/scaled_masked_softmax.h +++ b/megatron/fused_kernels/scaled_masked_softmax.h @@ 111,7 +111,7 @@ __global__ void scaled_masked_softmax_warp_forward(      constexpr int WARP_SIZE = (next_power_of_two < C10_WARP_SIZE) ? next_power_of_two : C10_WARP_SIZE;      constexpr int WARP_ITERATIONS = next_power_of_two / WARP_SIZE;      constexpr int WARP_BATCH = (next_power_of_two <= 128) ? 2 : 1;     constexpr int ELEMENTS_PER_LDG_STG = 4; +    constexpr int ELEMENTS_PER_LDG_STG = (WARP_ITERATIONS < 4) ? 1 : 4;      // blockDim/threadIdx = (WARP_SIZE, WARPS_PER_BLOCK, )      // gridDim/blockIdx = (seq_len, attn_heads, batches)  @@ 230,7 +230,7 @@ __global__ void scaled_masked_softmax_warp_backward(      constexpr int WARP_SIZE = (next_power_of_two < C10_WARP_SIZE) ? next_power_of_two : C10_WARP_SIZE;      constexpr int WARP_ITERATIONS = next_power_of_two / WARP_SIZE;      constexpr int WARP_BATCH = (next_power_of_two <= 128) ? 2 : 1;     constexpr int ELEMENTS_PER_LDG_STG = 4; +    constexpr int ELEMENTS_PER_LDG_STG = (WARP_ITERATIONS < 4) ? 1 : 4;      // blockDim/threadIdx = (WARP_SIZE, WARPS_PER_BLOCK, )      // gridDim/blockIdx = (seq_len, attn_heads, batches)  diff git a/megatron/fused_kernels/scaled_upper_triang_masked_softmax.h b/megatron/fused_kernels/scaled_upper_triang_masked_softmax.h index addca0a0a3bbe5322c4e4522471f17a3c00e2ee1..ca722cbbc626929fe0b3d54c0fcdc74b594dc7b0 100644  a/megatron/fused_kernels/scaled_upper_triang_masked_softmax.h +++ b/megatron/fused_kernels/scaled_upper_triang_masked_softmax.h @@ 125,7 +125,7 @@ __global__ void scaled_upper_triang_masked_softmax_warp_forward(      constexpr int WARP_SIZE = (next_power_of_two < C10_WARP_SIZE) ? next_power_of_two : C10_WARP_SIZE;      constexpr int WARP_ITERATIONS = next_power_of_two / WARP_SIZE;      constexpr int WARP_BATCH = (next_power_of_two <= 128) ? 2 : 1;     constexpr int ELEMENTS_PER_LDG_STG = 4; +    constexpr int ELEMENTS_PER_LDG_STG = (WARP_ITERATIONS < 4) ? 1 : 4;      int first_batch = (blockDim.y * blockIdx.y + threadIdx.y) * gridDim.x * WARP_BATCH + blockIdx.x;      int local_seq = blockIdx.x + 1;  @@ 245,7 +245,7 @@ __global__ void scaled_upper_triang_masked_softmax_warp_backward(      constexpr int WARP_SIZE = (next_power_of_two < C10_WARP_SIZE) ? next_power_of_two : C10_WARP_SIZE;      constexpr int WARP_ITERATIONS = next_power_of_two / WARP_SIZE;      constexpr int WARP_BATCH = (next_power_of_two <= 128) ? 2 : 1;     constexpr int ELEMENTS_PER_LDG_STG = 4; +    constexpr int ELEMENTS_PER_LDG_STG = (WARP_ITERATIONS < 4) ? 1 : 4;      int first_batch = (blockDim.y * blockIdx.y + threadIdx.y) * gridDim.x * WARP_BATCH + blockIdx.x;      int local_seq = blockIdx.x + 1;  ```",  !image It works well! Thank you.,"I have one more suggestion. The constraint currently exist in Python code does not match all conditions for kernel execution. May I PR some improvements, including current fixes and test codes? (I already fixed some codes)", That'd be great!,https://github.com/NVIDIA/MegatronLM/pull/133 I uploaded PR. please review this!,PR is in ... closing,"I see the PR hasn't been merged. Closed too soon? I see this commit, which seems to be part of 's PR: https://github.com/NVIDIA/MegatronLM/commit/2387ce01c91e5ddcb91d86a335d993e0a664b9dd Is that the PR you were referring to? But where is the PR, I can only see that commit",See CC(Improve and fix bugs about fused softmax layer) ,"Exactly! The PR status is OPEN and not merged. Unless you meant something else by ""PR is in""","I was puzzled too. I thought the Megatron team had something they didn't want to merge into main branch. My PR has many improvements like code refactoring, test cases, etc. But they are not important parts. The most important part of PR (resolving kernel errors) has been reflected to main branch. If my findings can help someone, that's enough for me :) I think It's better to close my PR if Megatron team have something they didn't want to merge.","The reason I'm asking, is that in order to sync the downstream we need a SHA commit which currently isn't there, since PR hasn't been merged. Normally an issue gets closed only once the corresponding PR is merged. Therefore please don't rush to close your PR and let's give a chance to the MegatronLM team to clarify on what's happening.",I see :),The PR is currently being tested. We will merge is in soon., PR has been merged !,"Yes, thank you! I will ask the Deepspeed folks to sync and then will sync to our fork from there."
jayparks,Question on loss scaling,"Hello, thanks for the great library. I have a question on the ORQA finetuning code. After calculating the loss, there is an additional step for scaling the supervised retrieval loss. link I don't see this step in the original DPR and other similar code. Could anyone explain the motivation for this scaling? Thanks!",2021-08-09T22:37:04Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/131,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
dzabraev,Unitialized data usage,"Here unitialized tensor is allocated, then in the next line this tensor used in baddbmm operation. `torch.zeros` have to be used in this place. Also comment in the first line confuses me. You preallocate tensor for output, but do not pass it to `out` argument. Instead use it as additive component. It is unsafe to use unitialized data as numbers (even you multiply it by zero), because not all sequence of bytes is valid float number. Just a small example how to get Nan  ```python unitialized_data = b'\xff\xff\xff\xff' np.frombuffer(unitialized_data, dtype=np.float32) * np.float32(0)  == array([nan], dtype=float32) ``` https://github.com/NVIDIA/MegatronLM/blob/90e0a0dd08159e1c95f4f9d99bb8687f327d36c3/megatron/model/transformer.pyL262L275",2021-08-05T13:50:18Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/130,"Hey dzabrev,  Please refer to baddbmm documentation: https://pytorch.org/docs/stable/generated/torch.baddbmm.html It clearly states that ```If beta is 0, then input will be ignored, and nan and inf in it will not be propagated.``` Moreover, we do not want the operation to happen inplace in order to enable the backprop. "
VictorSanh,Tensorboard logging `last_rank` vs `torch.distributed.get_rank() == 0`,"My understanding of this line is that it ensures that only the last rank process has a nonNone `_GLOBAL_TENSORBOARD_WRITER`. However, this line expect the first rank process to log some speedrelated metrics, so it is actually never logged because the first rank process doesn't have a TB writer... Could you confirm, the correct if condition should be `if writer and is_last_rank():` which would be consistent with the previous TB logs in the same `training.py`?",2021-08-04T17:02:09Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/129,"Thanks for reporting this issue. This is definitely a bug, it is fixed, and will be pushed in the next release."
unlimblue,Differences in T5 implementation,https://github.com/NVIDIA/MegatronLM/blob/90e0a0dd08159e1c95f4f9d99bb8687f327d36c3/megatron/model/language_model.pyL140 It seems that there is no implementation of `relative_attention_bias` like in huggingface and official. ,2021-08-03T07:07:26Z,stale,closed,0,10,https://github.com/NVIDIA/Megatron-LM/issues/128,"Some of the other components are also different, such as layer norm, FFN",What is the difference in FFN?,"the official T5 use relu without bias, the transformation of q,k,v in attention also without bias",Thanks for the clarification.,"Hi, is there any updates about the difference?","So relative positional embedding is not implemented within Megatron, as opposed to the huggingface/official implementation. I wonder what is the reason behind this?",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.,Is there any updates about the difference?,Marking as stale. No activity in 60 days.
hyunwoongko,Fix typo,,2021-08-01T11:59:26Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/127
DevavratSinghBisht,KeyError: 'query_model' during ICT Pretraining ,"I tried to use the BERT345Muncased model for ICT pretraining but an error is occurring, the complete log is given below. I am thinking that this model is not compatible for the task, I wasn't able to find any other suitable model in the repo, if this is the case then please direct me towards a compatible model. examples/pretrain_ict.sh: line 8: wiki/models/megatron_bert_345m_v0.1_uncased: Is a directory examples/pretrain_ict.sh: line 9: wiki/processed_data/corpus_indexed_title_sentence: No such file or directory examples/pretrain_ict.sh: line 10: wiki/processed_data/corpus_indexed_text_sentence: No such file or directory examples/pretrain_ict.sh: line 11: wiki/models/megatron_bert_345m_v0.1_uncased: Is a directory using world size: 1, dataparallelsize: 1, tensormodelparallel size: 1, pipelinemodelparallel size: 1  WARNING: overriding default arguments for tokenizer_type:BertWordPieceLowerCase                        with tokenizer_type:BertWordPieceLowerCase setting global batch size to 32 using torch.float16 for parameters ...  arguments    accumulate_allreduce_grads_in_fp32 .............. False   adam_beta1 ...................................... 0.9   adam_beta2 ...................................... 0.999   adam_eps ........................................ 1e08   adlr_autoresume ................................. False   adlr_autoresume_interval ........................ 1000   apply_query_key_layer_scaling ................... True   apply_residual_connection_post_layernorm ........ False   attention_dropout ............................... 0.1   attention_softmax_in_fp32 ....................... False   bert_binary_head ................................ True   bert_load ....................................... wiki/models/megatron_bert_345m_v0.1_uncased   bf16 ............................................ False   bias_dropout_fusion ............................. True   bias_gelu_fusion ................................ True   biencoder_projection_dim ........................ 0   biencoder_shared_query_context_model ............ False   block_data_path ................................. None   checkpoint_activations .......................... False   checkpoint_num_layers ........................... 1   clip_grad ....................................... 1.0   consumed_train_samples .......................... 0   consumed_valid_samples .......................... 0   data_impl ....................................... infer   data_parallel_size .............................. 1   data_path ....................................... ['wiki/processed_data/corpus_indexed_title_sentence']   dataloader_type ................................. single   DDP_impl ........................................ torch   decoder_seq_length .............................. None   distribute_checkpointed_activations ............. False   distributed_backend ............................. nccl   embedding_path .................................. None   encoder_seq_length .............................. 256   eod_mask_loss ................................... False   eval_interval ................................... 1000   eval_iters ...................................... 10   evidence_data_path .............................. None   exit_duration_in_mins ........................... None   exit_interval ................................... 8000   ffn_hidden_size ................................. 3072   finetune ........................................ False   fp16 ............................................ True   fp16_lm_cross_entropy ........................... False   fp32_residual_connection ........................ False   global_batch_size ............................... 32   hidden_dropout .................................. 0.1   hidden_size ..................................... 768   hysteresis ...................................... 2   ict_head_size ................................... None   ict_load ........................................ None   img_dim ......................................... 224   indexer_batch_size .............................. 128   indexer_log_interval ............................ 1000   init_method_std ................................. 0.02   init_method_xavier_uniform ...................... False   initial_loss_scale .............................. 4294967296   kv_channels ..................................... 64   layernorm_epsilon ............................... 1e05   lazy_mpu_init ................................... None   load ............................................ wiki/models/megatron_bert_345m_v0.1_uncased   local_rank ...................................... None   log_batch_size_to_tensorboard ................... False   log_interval .................................... 100   log_learning_rate_to_tensorboard ................ True   log_loss_scale_to_tensorboard ................... True   log_num_zeros_in_grad ........................... False   log_params_norm ................................. False   log_timers_to_tensorboard ....................... False   log_validation_ppl_to_tensorboard ............... False   loss_scale ...................................... None   loss_scale_window ............................... 1000   lr .............................................. 0.0001   lr_decay_iters .................................. None   lr_decay_samples ................................ None   lr_decay_style .................................. linear   lr_warmup_fraction .............................. 0.01   lr_warmup_iters ................................. 0   lr_warmup_samples ............................... 0   make_vocab_size_divisible_by .................... 128   mask_prob ....................................... 0.15   masked_softmax_fusion ........................... True   max_position_embeddings ......................... 512   merge_file ...................................... None   micro_batch_size ................................ 32   min_loss_scale .................................. 1.0   min_lr .......................................... 0.0   mmap_warmup ..................................... False   no_load_optim ................................... None   no_load_rng ..................................... None   no_save_optim ................................... None   no_save_rng ..................................... None   num_attention_heads ............................. 12   num_channels .................................... 3   num_classes ..................................... 1000   num_layers ...................................... 12   num_layers_per_virtual_pipeline_stage ........... None   num_workers ..................................... 2   onnx_safe ....................................... None   openai_gelu ..................................... False   optimizer ....................................... adam   override_lr_scheduler ........................... False   params_dtype .................................... torch.float16   patch_dim ....................................... 16   pipeline_model_parallel_size .................... 1   query_in_block_prob ............................. 0.1   rampup_batch_size ............................... None   rank ............................................ 0   reset_attention_mask ............................ False   reset_position_ids .............................. False   retriever_report_topk_accuracies ................ [1, 5, 10, 20, 100]   retriever_score_scaling ......................... True   retriever_seq_length ............................ 256   sample_rate ..................................... 1.0   save ............................................ wiki/models/megatron_bert_345m_v0.1_uncased   save_interval ................................... 4000   scatter_gather_tensors_in_pipeline .............. True   seed ............................................ 1234   seq_length ...................................... 256   sgd_momentum .................................... 0.9   short_seq_prob .................................. 0.1   split ........................................... 969, 30, 1   tensor_model_parallel_size ...................... 1   tensorboard_dir ................................. None   tensorboard_log_interval ........................ 1   tensorboard_queue_size .......................... 1000   titles_data_path ................................ wiki/processed_data/corpus_indexed_title_sentence   tokenizer_type .................................. BertWordPieceLowerCase   train_iters ..................................... 100000   train_samples ................................... None   use_checkpoint_lr_scheduler ..................... False   use_contiguous_buffers_in_ddp ................... False   use_cpu_initialization .......................... None   use_one_sent_docs ............................... False   virtual_pipeline_model_parallel_size ............ None   vocab_extra_ids ................................. 0   vocab_file ...................................... wiki/models/megatron_bert_345m_v0.1_uncased/bertlargeuncasedvocab.txt   weight_decay .................................... 0.01   world_size ...................................... 1  end of arguments  setting number of microbatches to constant 1 > building BertWordPieceLowerCase tokenizer ...  > padded vocab (size: 30524) with 68 dummy tokens (new size: 30592) > initializing torch distributed ... > initializing tensor model parallel with size 1 > initializing pipeline model parallel with size 1 > setting random seeds to 1234 ... > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234 > compiling dataset index builder ... make: Entering directory '/raid/ashish/nvirqa/MegatronLM/megatron/data' make: Nothing to be done for 'default'. make: Leaving directory '/raid/ashish/nvirqa/MegatronLM/megatron/data' >>> done with dataset index builder. Compilation time: 0.097 seconds > compiling and loading fused kernels ... Detected CUDA files, patching ldflags Emitting ninja build file /raid/ashish/nvirqa/MegatronLM/megatron/fused_kernels/build/build.ninja... Building extension module scaled_upper_triang_masked_softmax_cuda... Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N) ninja: no work to do. Loading extension module scaled_upper_triang_masked_softmax_cuda... Detected CUDA files, patching ldflags Emitting ninja build file /raid/ashish/nvirqa/MegatronLM/megatron/fused_kernels/build/build.ninja... Building extension module scaled_masked_softmax_cuda... Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N) ninja: no work to do. Loading extension module scaled_masked_softmax_cuda... Detected CUDA files, patching ldflags Emitting ninja build file /raid/ashish/nvirqa/MegatronLM/megatron/fused_kernels/build/build.ninja... Building extension module fused_mix_prec_layer_norm_cuda... Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N) ninja: no work to do. Loading extension module fused_mix_prec_layer_norm_cuda... >>> done with compiling and loading fused kernels. Compilation time: 0.402 seconds time to initialize megatron (seconds): 25.511 [after megatron is initialized] datetime: 20210728 04:24:57  building BiEncoderModel...  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 217890816 > learning rate decay style: linear  loading checkpoint from wiki/models/megatron_bert_345m_v0.1_uncased at iteration 0 could not find arguments in the checkpoint ... Loading query model Traceback (most recent call last):   File ""pretrain_ict.py"", line 175, in      pretrain(train_valid_test_datasets_provider,   File ""/raid/ashish/nvirqa/MegatronLM/megatron/training.py"", line 114, in pretrain     model, optimizer, lr_scheduler = setup_model_and_optimizer(model_provider)   File ""/raid/ashish/nvirqa/MegatronLM/megatron/training.py"", line 327, in setup_model_and_optimizer     args.iteration = load_checkpoint(model, optimizer, lr_scheduler)   File ""/raid/ashish/nvirqa/MegatronLM/megatron/checkpointing.py"", line 336, in load_checkpoint     model[0].load_state_dict(state_dict['model'], strict=strict)   File ""/raid/ashish/nvirqa/MegatronLM/megatron/model/module.py"", line 189, in load_state_dict     self.module.load_state_dict(state_dict, strict=strict)   File ""/raid/ashish/nvirqa/MegatronLM/megatron/model/biencoder_model.py"", line 174, in load_state_dict     state_dict[self._query_key], strict=strict) KeyError: 'query_model'",2021-07-28T04:40:33Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/126,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
Lavenderjiang,torch.distributed initialization address already in use,"When running examples/pretrain_bert_distributed.sh, I had the ""address already in use"" error from this line, even after I made sure the address host:port is open. This issue is fixed by removing the world_size and init_method in the init_process group: ```  torch.distributed.init_process_group(             backend=args.distributed_backend,             rank=args.rank) ```",2021-07-27T19:00:47Z,,closed,1,2,https://github.com/NVIDIA/Megatron-LM/issues/125,Same issue here. Thanks for the information! ,"I remove this two options, and then get stuck. any suggestion?"
DevavratSinghBisht,Squad format Dataset for Supervised Finetuning and Evaluation of ORQA task.,The readme of ORQA task mentions about  Supervised Finetuning and evaluation of retriever models for the Google Natural Questions Open dataset. Is it possible to do the same for a SQuAD format dataset? If yes then please explain how can we do it ?,2021-07-21T07:50:43Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/124,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
ktaebum,fix typo in mappings.py,,2021-07-20T07:28:53Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/123
Lavenderjiang,Document must have 2+ sentences,"I found that documents must have 2+ sentences to be successfully index mapped (see code here). This results in issues like empty index map or a hang in creating the mapping (see issue CC(Error running bert pretraining example), CC(pretrain_bert.py stuck at ""building index mapping for train ..."")). Is it possible to adapt the code to work with singlesentence documents as well?",2021-07-19T18:10:40Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/122,"If no binary head is provided, the code should work with a single sentence per document","Hi  I have compiled a large dataset with more than 100 million sentences but I still get this issue, what could be the reason!?"
roclark,Allow EVs to be set while calling examples,"Currently, in order to use the provided example scripts, any environment variables listed at the top of the files need to be manually edited and saved before the script can be run. This limits the ability to automate and streamline the process of running these scripts and can be tedious to create copies of files while running from a shared filesystem, such as an NFS, on multiple nodes as the values will be overwritten on different nodes. By using default values and allowing the variables to be read into the script, it is much easier to call scripts with different parameters on multiple nodes, such as: ``` MASTER_ADDR=127.0.0.1 examples/pretrain_bert_distributed.sh ``` SignedOffBy: Robert Clark ",2021-07-19T16:44:44Z,stale,open,0,3,https://github.com/NVIDIA/Megatron-LM/issues/121,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,No activity on stale PR in 21 days.,Marking as stale. No activity in 60 days.
eric-haibin-lin,fix a typo,,2021-07-16T17:31:29Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/120
BlueCloudDev,Dataset does not exist,"`Dataset does not exist: /data/openwebtext/subset01 Path should be a basename that both .idx and .bin can be appended to get full filenames.  > finished creating indexed dataset in 0.000021 seconds Traceback (most recent call last):   File ""/mega/MegatronLM/pretrain_gpt.py"", line 124, in      pretrain(train_valid_test_datasets_provider, model_provider, forward_step,   File ""/mega/MegatronLM/megatron/training.py"", line 131, in pretrain     = build_train_valid_test_data_iterators(   File ""/mega/MegatronLM/megatron/training.py"", line 815, in build_train_valid_test_data_iterators     train_ds, valid_ds, test_ds = build_train_valid_test_datasets_provider(   File ""/mega/MegatronLM/pretrain_gpt.py"", line 109, in train_valid_test_datasets_provider     train_ds, valid_ds, test_ds = build_train_valid_test_datasets(   File ""/mega/MegatronLM/megatron/data/gpt_dataset.py"", line 54, in build_train_valid_test_datasets     train_ds, valid_ds, test_ds = _build_train_valid_test_datasets(   File ""/mega/MegatronLM/megatron/data/gpt_dataset.py"", line 86, in _build_train_valid_test_datasets     indexed_dataset = get_indexed_dataset_(data_prefix,   File ""/mega/MegatronLM/megatron/data/gpt_dataset.py"", line 134, in get_indexed_dataset_     indexed_dataset.sizes.shape[0])) AttributeError: 'NoneType' object has no attribute 'sizes'` This folder exists, is filled with data, and is accessible from the container through containermounts.  What does this error mean? How do I provide my data in the format it expects?",2021-07-15T02:21:39Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/119,I have the same problem. How can I solve it? ,"> I have the same problem. How can I solve it? Hi, I just had this problem and solved it. My data directory:  ```bash root:/workdir/ds/data/megatron/books1_text_document tree ./ ./ â”œâ”€â”€ books1_text_document.bin â”œâ”€â”€ books1_text_document.idx â”œâ”€â”€ xxxxx ``` ""Path should be a basename that both .idx and .bin can be appended to get full filenames"" reminded me that `DATA_PATH` should be set to `/workdir/ds/data/megatron/books1_text_document/books1_text_document` instead of `/workdir/ds /data/megatron/books1_text_document`. In my opinion, this should not be called `Path`. In my opinion, this should not be called `Path`. ðŸ˜‚"
bugface,Continue Pre-training from a checkpoint with different config,"I have a model pretrained on 10 nodes for 500k steps.  Then, I need to reduce the number of nodes to 5. But after modified the node numbers, start training yields an error: ``` Traceback (most recent call last):   File ""/red/gatortronphi/workspace/scripts/MegatronLM/pretrain_bert.py"", line 154, in      pretrain(train_valid_test_datasets_provider, model_provider, forward_step,   File ""/red/gatortronphi/workspace/scripts/MegatronLM/megatron/training.py"", line 107, in pretrain     model, optimizer, lr_scheduler = setup_model_and_optimizer(model_provider)   File ""/red/gatortronphi/workspace/scripts/MegatronLM/megatron/training.py"", line 282, in setup_model_and_optimizer     args.iteration = load_checkpoint(model, optimizer, lr_scheduler)   File ""/red/gatortronphi/workspace/scripts/MegatronLM/megatron/checkpointing.py"", line 263, in load_checkpoint     lr_scheduler.load_state_dict(state_dict['lr_scheduler'])   File ""/red/gatortronphi/workspace/scripts/MegatronLM/megatron/learning_rates.py"", line 146, in load_state_dict     self.warmup_steps = self._check_and_set(self.warmup_steps,   File ""/red/gatortronphi/workspace/scripts/MegatronLM/megatron/learning_rates.py"", line 122, in _check_and_set     assert cls_value == sd_value, \ AssertionError: AnnealingLR: class input value 5600000.0 and checkpointvalue 11040000.0 for warmup iterations do not match ``` How should I deal with this issue? More generic question: if I want to modify more configs like total training steps, dataset, batch size, learning rate but want to continue from an existing checkpoint, how should I do? Thanks",2021-07-13T16:13:17Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/118,"Given a situation:  I originally set the training step to 1M. After training, I found the model was not converged and I needed to train another 1M step. How should I setup my config?","For this problem, just use one of these two command line arguments: `overridelrscheduler` or `usecheckpointlrscheduler`"
armundle,In BERT pretraining how to specify DATA_PATH to take multiple files,"I am trying to train MegatronLM/BERT with wiki dataset. I followed the instructions to download and preprocess the latest wiki dump and extract the txt with WikiExtractor.py as described here For `wikiextractor`, the command looks like: `python m wikiextractor.WikiExtractor data/enwikilatestpagesarticles.xml.bz2 o data/wiki json` This creates json files with folders as  ``` . â”œâ”€â”€ AA â”œâ”€â”€ AB â”œâ”€â”€ AC â”œâ”€â”€ AD â”œâ”€â”€ AE â”œâ”€â”€ ... â”œâ”€â”€ GD â””â”€â”€ GE ``` where each folder has `wiki_ `json files ``` â”œâ”€â”€ wiki_00 â”œâ”€â”€ wiki_01 â”œâ”€â”€ ... â”œâ”€â”€ wiki_98 â””â”€â”€ wiki_99 ``` Each json file has multiple objects, one JSON per line, e.g. ```json {... ""id"": ""620"", ""revid"": ""15996738"", ""url"": ""https://en.wikipedia.org/wiki?curid=620"", ""title"": ""Animal Farm"", ""text"": ""Animal Farm is an allegorical novella by George Orwell, first pub    lished in ...  but ran in Brazilian and Burmese newspapers."" ... } ``` This generates a total of 16046 such JSON files. In order to preprocess them with the preprocess_data.py script, I had to create bash script which would process all of these files and create ~ 16k pairs of .idx and .bin files ```bash SCRIPT=($1) VOCAB=($2) WIKI_DIR=($3) OUTDIR=($4) find ""$WIKI_DIR"" type f  print0  rev)                 prefix=""${subfilename}_${filename}""                 echo ""Procesing $prefix""                 python $SCRIPT input $line outputprefix ${OUTDIR}/megatronbert${prefix}  vocab $VOCAB datasetimpl mmap  tokenizertype BertWordPieceLowerCase splitsentences         done ``` This creates ~16k .idx and .bin pairs e.g.  ``` megatronbertAA_wiki_00_text_sentence.bin, megatronbertAA_wiki_00_text_sentence.idx, ... megatronbertGE_wiki_47_text_sentence.bin, megatronbertGE_wiki_47_text_sentence.idx ``` With all this preprocessing done, now I want to train the BERT with pretrain_bert.py as outlined here. But it looks like the `DATA_PATH` variable only takes a single .idx/.bin pair suffix.  **How do I use all of the 16k+ pairs of .idx/.bin for BERT pretraining?**",2021-07-07T15:35:50Z,stale,closed,5,6,https://github.com/NVIDIA/Megatron-LM/issues/117,"Same question here, should we merge all jsons into one file? The script I used to merge `wiki_xx` into one file ```bash SCRIPT=($1) VOCAB=($2) WIKI_DIR=($3) OUTDIR=($4) mkdir p $OUTDIR rm $OUTDIR/wiki_all.json touch $OUTDIR/wiki_all.json find ""$WIKI_DIR"" type f  print0  rev)             prefix=""${subfilename}_${filename}""             new_name=$(echo ""$line"")             echo ""Procesing $prefix, $filename, $new_name""             cat $new_name >> $OUTDIR/wiki_all.json     done python tools/preprocess_data.py \        input $OUTDIR/wiki_all.json \        outputprefix $OUTDIR/mybert \        vocab bertvocab.txt \        datasetimpl mmap \        tokenizertype BertWordPieceLowerCase \        splitsentences ```",Yeah  it turns out that you can/should merge into a single file and process that.,barad Have u ever played with singlefilegenerated bin for large scale training? I am concerned about whether single file will lead to IO bottleneck. ,"We have not had any issues with large single files using lustre or nfs file systems. If you want to use multiple files, you can break your input data into multiple files and use dataset blending (example here).",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
roclark,Add seq-length argument to distribued BERT example,"The `seqlength` argument was missing from the example distributed BERT pretraining script which throws an error while attempting to parse the arguments as the code expects all arguments to be specified and have a non`None` value. By updating the example variables at the top of the script and running it, the application will quickly error out with the following stack unless the `seqlength` argument is specified: ``` AssertionError Traceback (most recent call last):   File ""pretrain_bert.py"", line 146, in      pretrain(train_valid_test_datasets_provider, model_provider, forward_step,   File ""/workspace/megatronlm/megatron/training.py"", line 94, in pretrain     initialize_megatron(extra_args_provider=extra_args_provider,   File ""/workspace/megatronlm/megatron/initialize.py"", line 51, in initialize_megatron     set_global_variables(extra_args_provider=extra_args_provider,   File ""/workspace/megatronlm/megatron/global_vars.py"", line 82, in set_global_variables     args = _parse_args(extra_args_provider=extra_args_provider,   File ""/workspace/megatronlm/megatron/global_vars.py"", line 98, in _parse_args     _GLOBAL_ARGS = parse_args(extra_args_provider=extra_args_provider,   File ""/workspace/megatronlm/megatron/arguments.py"", line 213, in parse_args     assert args.encoder_seq_length is not None ``` SignedOffBy: Robert Clark ",2021-07-06T16:08:00Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/116
zhuhong,The training of T5 using FP16 is unstable,"We have meet a problem to train T5 using FP16, as shown in the figure below: !image after check the loss vs loss scale, we find that the loss scale is going down to 1 rapidly.  !å¾®ä¿¡å›¾ç‰‡_20210705191533 I want to know if there are some strategy to stablize the training process by tuning the parameters about loss scale like its window or minlossscale ?",2021-07-06T01:50:28Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/115,"We also found this problem in our testing, and sometimes it was even hard to get started",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
hwijeen,How about supporting alternatives to fine-tuning?,"Hi, thank you for the great library. Recently, many algorithms are proposed to replace finetuning as it incurs too many burdens, especially with huge models like GPT3. Examples include Ptuning and LoRA. I personally implemented both on top of MegatronLM and was able to achieve SOTA accuracy on a number of Korean benchmark datasets (I used model size ranging from 300M to 82B). How about supporting the algorithms like the above? I think it's an extension of the current `finetune` option, and is a big plus in terms of the practicality of huge models.",2021-07-06T01:49:20Z,stale,closed,7,10,https://github.com/NVIDIA/Megatron-LM/issues/114,can you share your implementation? thanks!, +1, +1,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,"Hi, sorry for the delayed response. I won't be able to make a PR as I no longer have access to the code, it was from my previous job. I'd be happy to work together as a side project if people are still interested.",ht   Could you share what are your use cases? Are you trying to pefttune GPT models for conditional generation tasks? And may I ask what made you reopen this issue barker? Are you working on this?,"hi   > Are you trying to pefttune GPT models for conditional generation tasks? yes. I found this tutorial, and it seems that NeMo can inject LoRA adapters into megatronlm model. https://github.com/NVIDIA/NeMo/blob/main/tutorials/nlp/lora.ipynb",Marking as stale. No activity in 60 days.,"It would be great if MegatronLM could support PEFT methods, e.g. QLoRA. We're sorely lacking a PEFT trainer with Tensor Parallelism.",Marking as stale. No activity in 60 days.
hwijeen,fix misleading typo,* Edit docstring in ParallelMLP as it does not have dropout * Tiny typos,2021-07-02T07:10:04Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/113
xyltt,AttributeError: 'Parameter' object has no attribute 'main_grad',"When I train the model, if some modules (parameters) in the model are not involved in the current forward propagation calculation, then the parameters in these modules are not gradient calculated during the back propagation.At this point, the error in the title appears.This error occurs in the optimizer.py file.",2021-06-27T11:32:58Z,stale,closed,0,10,https://github.com/NVIDIA/Megatron-LM/issues/112,I got this problem too. Did you solve it ?," TL I encountered this error too. Below is the trace. Did any of you find out the cause and fix for this? Appreciate any pointers. `   File ""/workspace/prraman/megatron/optimizer.py"", line 384, in step     self._copy_model_grads_to_main_grads()   File ""/workspace/prraman/megatron/optimizer.py"", line 311, in _copy_model_grads_to_main_grads     main_param.grad = model_param.main_grad.float() AttributeError: 'Parameter' object has no attribute 'main_grad' Traceback (most recent call last): ` Essentially, error occurs when trying to access `main_grad` attribute inside `model_param` object in this line: https://github.com/NVIDIA/MegatronLM/blob/main/megatron/optimizer/optimizer.pyL316  Just bumping this up to your attention in case you have any recommendations to fix it. Thanks","Following up with some debugging on this, I found that the flag `params_have_main_grad` which causes the above code to look for `main_grad` attribute is set here (https://github.com/NVIDIA/MegatronLM/blob/main/megatron/optimizer/__init__.pyL71,L73) in the __init__.py of the optimizer module. Basically, the flag is set when `args.DDP_impl == 'local'`. However, I find that in in megatron/model/distributed.py where the `main_grad` attribute is actually initialized, I find that it is created only when `self.use_contiguous_buffers flag` is also set (https://github.com/NVIDIA/MegatronLM/blob/main/megatron/model/distributed.pyL124). Could that be a possible cause for this error? Do both flags need to be set for `main_grad` to be allocated and used?  In my case `args.DDP_impl` is set to ""local"" but `args.use_contiguous_buffers_in_ddp` flag is ""false"". P.S. above error occurs even when running the code serially (i.e. with tensor_model_parallel_size and pipeline_model_parallel_size set to 1)","I want to train a model with an unified encoder and two seperate decoders, then I got this problem.  TL   Did you solve it ? Appreciate any pointers, thanks.",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.," Sorry if this is too late. Add `nogradientaccumulationfusion` to the arguments, and it will probably solve your problem. ",">  Sorry if this is too late. Add `nogradientaccumulationfusion` to the arguments, and it will probably solve your problem. we use nogradientaccumulationfusion. but it does not work . Do you have any idea ?",Marking as stale. No activity in 60 days.,"You need to make sure that every network layer you define is used, otherwise this error will be reported. You just need to check if there are some network layers that are not used and delete those that are not used.",Marking as stale. No activity in 60 days.
Shaw95,Problems about model parallel,"Hi, thank you for providing such an excellent framework I met a problem when using it  to implement my own network, I found that if I train on 8 GPUs with only data parallel, the loss  converges normally.  While if I trained with 2way MP and 4way DP, the loss of the first epoch is OK, but afterwards the loss is going up.  I'm just wondering what possibly causes this problem and if there are any tips for using model parallism. Thanks a lot! Shaw ",2021-06-11T08:29:35Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/111,Had the same problem.  Try to increase the batch size and/or lower the learning rate. ,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
Hanlard,Preocessing data about T5,1. Which Tokenizer type? 2. Is  'append_eod'  need?,2021-06-11T06:43:57Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/110,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
Hanlard,PreTrain-T5-[get_samples_mapping]-[ RuntimeError: Socket Timeout ],"In the megatron\data\dataset_utils.py ""get_samples_mapping"" function,  the following line had run for 12 hours.          samples_mapping = helpers.build_mapping(..) And this line only runs for rank=0, so the next line torch.distributed.all_reduce(counts, ... ) reports [ RuntimeError: Socket Timeout ]",2021-06-09T01:50:20Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/109,"I had a similar issue when using DDP and model parallel. I solved the issue as set timeout to a longer time (default is 30min) in the torch.distributed.init_process_group (megatron/initialize.py at line 183) ``` torch.distributed.init_process_group(    backend=args.distributed_backend,    timeout=datetime.timedelta(0, 3600),   notice here I change the timeout to 2 hours    world_size=args.world_size,    rank=args.rank,    init_method=init_method  ) ```",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
hangzhang-nlp,Unclear description of ICT pretraining,How to get ICT pretrain data? I didn't find any information about TITLE_DATA_PATH and TEXT_DATA_PATH in pretrain_ict.sh,2021-06-07T12:46:04Z,stale,closed,0,6,https://github.com/NVIDIA/Megatron-LM/issues/108, Looking forward to your early reply.,This is an ongoing work. We plan to share the latest soon.,"Thanks for your share. but there are still questions to reproduce the results in ""EndtoEnd Training of Neural Retrievers for OpenDomain Question Answering"".  Could you release the bertbase(Megatron) or checkpoints in different settings like DPR, Rocketqa. "," , do you have any plan to release the unsupervised ICT model checkpoint that is pretrained on the wikipedia corpus?",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
zhiqi-0,Distributed training all-reduce order,"Hi, I'm just wondering if there is a potential issue for allreduce order when both data parallelism and tensor model parallelism are enabled during training. With **torch DDP**, both tensor model parallelism and data parallelism use allreduce, and are launched on different streams. While the execution order is determined by hardware, will it cause hanging in some cases like:  GPU1: [MP] allreduce > [DP] allreduce  GPU2: [DP] allreduce > [MP] allreduce From issues discussed here, I think it may be unsafe for undetermined order.",2021-05-31T04:49:03Z,stale,closed,1,5,https://github.com/NVIDIA/Megatron-LM/issues/107,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.,I have encountered the same problem. Can anyone provide some opinions?," are you seeing a hang? Can you describe the setting, perhaps provide an example command line, and also paste the last couple of lines in the logs?",Marking as stale. No activity in 60 days.
rainmaker712,Add new attention features on megatron to optimize the performance.,"Hi, I've added several attention features on Megatron LM to optimize the memory usage. One of the options I have is to implement selfattention part to improve it. I've successfully implemented the random attention part from synthesizer, but when I added the dense layers, I have the message returns  ```   File ""/app/megatron/optimizer/optimizer.py"", line 321, in step grad_norm, params_norm, clip_flag = self.clip_grad_norm(self.clip_grad)grad_norm, params_norm, clip_flag = self.clip_grad_norm(self.clip_grad)   File ""/app/megatron/optimizer/optimizer.py"", line 72, in clip_grad_norm   File ""/app/megatron/optimizer/optimizer.py"", line 72, in clip_grad_norm         grad_norm, params_norm, clip_flag = clip_grad_norm_fp32(params, clip_grad)grad_norm, params_norm, clip_flag = clip_grad_norm_fp32(params, clip_grad)   File ""/app/megatron/optimizer/clip_grads.py"", line 64, in clip_grad_norm_fp32   File ""/app/megatron/optimizer/clip_grads.py"", line 64, in clip_grad_norm_fp32         grad = param.grad.detach()grad = param.grad.detach()  AttributeError: : 'NoneType' object has no attribute 'detach'  ``` Any ideas on how to fix it?",2021-05-22T23:48:44Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/106,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,"For some reason the gradient of a supposed parameter is not calculated, it seems. Make sure that all the tensors you want to use as parameters require gradients, while nonparameter tensors don't require gradients. It could also be that the gradient calculation is disrupted by a nondifferentiable operation.",Marking as stale. No activity in 60 days.
stas00,[README] t5 fixes,this PR:  adds instructions to t5 preprocessing  fixes the env var cmd line args  adds the required `vocabextraids 100`  ,2021-05-18T19:11:31Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/105
devrimcavusoglu,Typo fix in readme,Quick typo fix in README.md.,2021-05-18T00:28:32Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/104
stas00,fix copy-n-paste error,remove erroneous arg.  ,2021-05-17T22:03:34Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/103
codecaution,Final accuracy of GPT2 (1.5 B) using openWebText on Megatron-LM ,"Hi. I try to reproduce the experiment on GPT2(1.5B) with MegatronLM recently, but I don't get the expected accuracy performance. I conduct the experiments by following the steps below. 1. Following the instruction, I collect GPT Webtext Data using the default argument in download.py and process the data as here. 2. Pretrain GPT2 with 4xDGXA100 (32GPU) and configs as follow: `python m torch.distributed.launch $DISTRIBUTED_ARGS \        pretrain_gpt.py \        numlayers 48 \        hiddensize 1600 \        numattentionheads 25 \        microbatchsize 8 \        globalbatchsize 512 \        seqlength 1024 \        maxpositionembeddings 1024 \        trainiters 1000000 \        lrdecayiters 50000 \        save $CHECKPOINT_PATH \        load $CHECKPOINT_PATH \        datapath $DATA_PATH \        vocabfile $VOCAB_PATH \        mergefile $MERGE_PATH \        dataimpl mmap \        split 949,50,1 \        distributedbackend nccl \        lr 0.00025 \        lrdecaystyle cosine \        minlr 1.0e5 \        weightdecay 1e2 \        clipgrad 1.0 \        lrwarmupiters 2000\        checkpointactivations \        loginterval 100 \        saveinterval 10000 \        evalinterval 1000 \        evaliters 10 \        fp16  avg accuracy: 4.5275E01 I wonder if I have any wrong stepsï¼Ÿ After comparing the GPT2 paper and MegatronLM paper, I have a question that **Why MegatronLM trained on bigger dataset(collection of Wikipedia, CCStories, RealNews, and OpenWebtext, ~174GB) but get lower accuracy(45.18% on LAMBADA) than openAI GPT2 (55.48% on LAMBADA) trained only on OpenWebtext(~40GB) with similar parameter size.**   Could you give some help? Thank you very much.",2021-05-15T01:35:47Z,stale,closed,1,3,https://github.com/NVIDIA/Megatron-LM/issues/102,"hi, could you share what the results of download.py  look like? After downloading process, I found nothing in the data folder. Thank you so much :)",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
lazykyama,Failed to load NGC's GPT-2 checkpoint file from tools/generate_samples_gpt.py,"When I tried to run `tools/generate_samples_gpt.py` with NGC's pretrained model (https://ngc.nvidia.com/catalog/models/nvidia:megatron_lm_345m), the following error happened. ``` Traceback (most recent call last):   File ""tools/generate_samples_gpt.py"", line 112, in      main()   File ""tools/generate_samples_gpt.py"", line 94, in main     _ = load_checkpoint(model, None, None)   File ""/path/to/MegatronLM/megatron/checkpointing.py"", line 341, in load_checkpoint     fix_query_key_value_ordering(model, checkpoint_version)   File ""/path/to/MegatronLM/megatron/checkpointing.py"", line 214, in fix_query_key_value_ordering     for name, param in model.named_parameters(): AttributeError: 'list' object has no attribute 'named_parameters' ``` As far as I checked, this issue looks like caused due to a step to verify checkpoint version and fix the model architecture. After modifying like below, the error disappeared. ```diff diff git a/megatron/checkpointing.py b/megatron/checkpointing.py index 43dfa16..5f7f05f 100644  a/megatron/checkpointing.py +++ b/megatron/checkpointing.py @@ 211,6 +211,9 @@ def fix_query_key_value_ordering(model, checkpoint_version):      version is smaller than 2.0      """"""      if checkpoint_version < 2.0: +        if isinstance(model, list): +            assert len(model)==1 +            model = model[0]          for name, param in model.named_parameters():              if name.endswith(('.query_key_value.weight', '.query_key_value.bias')):                  if checkpoint_version == 0: ``` Is such behavior expected and is the change above ok? Environment information is below: * GPU driver: 450.102.04  * Runtime: NGC container image nvcr.io/nvidia/pytorch:20.12py3 * Runtime options: like below ```     tensormodelparallelsize 1 \     pipelinemodelparallelsize 1 \     numlayers 24 \     hiddensize 1024 \     numattentionheads 16 \     seqlength 1024 \     maxpositionembeddings 1024 \     microbatchsize 4 \     globalbatchsize 8 \     lr 0.00015 \     trainiters 500000 \     lrdecayiters 320000 \     lrdecaystyle cosine \     vocabfile ../resources/gpt2vocab.json \     mergefile ../resources/gpt2merges.txt \     lrwarmupfraction .01 \     fp16 \     load ${MODELPATH} \     outseqlength 1024 \     temperature 1.0 \     genfile samples_from_file.json \     sampleinputfile ../data/seed.txt \     sampleoutputfile ${OUTFILE_PATH} \     numsamples 0 \     top_p 0.9 \     noloadoptim noloadrng \     recompute ```",2021-05-12T11:11:52Z,stale,closed,1,2,https://github.com/NVIDIA/Megatron-LM/issues/101,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,It looks like this issue has been already resolved by 26b49aab4fd18f8ae1defd22eb8f813a7b35704f.
PS-AI,Import Error for MegatronBertForMaskedLM,"Hi, I am new to Transformers. I am trying to run run Megatron BERT using Hugging Face Transformers  Model  nvidia/megatronbertcased345m  in Google Colab. I have done the steps given in the page: https://huggingface.co/nvidia/megatronbertcased345m in Google Colab and am trying to run the first example on Masked LM I get the following error when running the code: ImportError: cannot import name 'MegatronBertForMaskedLM' from 'transformers' (unknown location) Could you guide me on this?",2021-05-11T12:01:40Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/100,!pip install $MYDIR/transformers instead of  !pip install transformers solved the problem
HuuY,how to export the gpt2 to onnx model? ,"Hi, could you please tell me how I can export gpt2 to onnx model? Is there a protobuf limitition of 2GB? I have already run the inference code of gpt2 based on megatron with nvidia V100, and I try to export a onnx file.  But it is said that: ''RuntimeError: Exporting model exceed maximum protobuf size of 2GB. Please call torch.onnx.export with use_external_data_format=True.'' When I did so, the onnx model was exported as lots of weight files, and it can not be used with onnxruntime to inference. Can anyone help me? Thanks a lot.",2021-05-07T09:24:57Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/99," for large models, like bigger than 2 GB, you should make weights standalone. Can you share how do you exported GPT2 to onnx?",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
stas00,checkpoint wget download doesn't work,"FYI, the instructions at https://github.com/NVIDIA/MegatronLMdownloadingcheckpoints lead to 0sized files. e.g., ``` wget contentdisposition https://api.ngc.nvidia.com/v2/models/nvidia/megatron_lm_345m/versions/v0.0/zip O megatron_lm_345m_v0.0.zip 20210505 11:42:01  https://api.ngc.nvidia.com/v2/models/nvidia/megatron_lm_345m/versions/v0.0/zip Resolving api.ngc.nvidia.com (api.ngc.nvidia.com)... 13.57.84.77, 13.52.19.24 Connecting to api.ngc.nvidia.com (api.ngc.nvidia.com):443... connected. HTTP request sent, awaiting response... 200  Length: unspecified Saving to: â€˜megatron_lm_345m_v0.0.zipâ€™ megatron_lm_345m_v0.0.zip                    [                                                                             ]       0  .KB/s    in 0s       20210505 11:42:01 (0.00 B/s)  â€˜megatron_lm_345m_v0.0.zipâ€™ saved [0] ``` I was able to download the files manually via https://ngc.nvidia.com/catalog/models/nvidia:megatron_bert_345m/version",2021-05-05T18:53:44Z,,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/98, FWIW the commands in the corresponding model cards on https://huggingface.co/nvidia (written by ) work for me.,``` wget contentdisposition https://api.ngc.nvidia.com/v2/models/nvidia/megatron_bert_345m/versions/v0.1_cased/zip O $MYDIR/nvidia/megatronbertcased345m/checkpoint.zip wget contentdisposition https://api.ngc.nvidia.com/v2/models/nvidia/megatron_bert_345m/versions/v0.1_uncased/zip O $MYDIR/nvidia/megatronbertuncased345m/checkpoint.zip wget contentdisposition https://api.ngc.nvidia.com/v2/models/nvidia/megatron_lm_345m/versions/v0.0/zip O $MYDIR/nvidia/megatrongpt2345m/checkpoint.zip ```,"Thank you, c  These are the same as in the main README.md, I tried the same as you pasted to no avail and also with `curl` and it doesn't work either. ``` wget version GNU Wget 1.20.3 built on linuxgnu. ``` Should it specify Length? I'm getting ``` Length: unspecified ``` FWIW, besides my home setup,  I tried from one of our HF machines  totally different network, still getting 0 sized file.",Figured it out  nvidia's ngc server didn't like my UserAgent. Unsetting it solved the problem and now I can download the files successfully. 
pengwa,add mark_dirty for in-place operation,"Hi Team, According to the PyTorch code explanation here:   https://github.com/pytorch/pytorch/blob/75024e228ca441290b6a1c2e564300ad507d7af6/torch/autograd/function.pyL35,  torch.distributed.all_reduce operates inplace for its inputs according to https://pytorch.org/docs/stable/_modules/torch/distributed/distributed_c10d.htmlall_reduce So I propose this change as PR. Hope this makes sense.",2021-04-30T08:43:17Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/97
leenldk,Error in README file,"I believe there is an error in README. In subsection ""Downloading Checkpoints"", the command for downloading BERT345Muncased model should be wget from site ""https://api.ngc.nvidia.com/v2/models/nvidia/megatron_bert_345m/versions/v0.1_uncased/zip"" instead of ""https://api.ngc.nvidia.com/v2/models/nvidia/megatron_bert_345m/versions/v0.1_cased/zip"". You mistyped ""uncased"" to ""cased"".",2021-04-21T14:19:37Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/96,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Fixed here: https://github.com/NVIDIA/MegatronLM/commit/26b49aab4fd18f8ae1defd22eb8f813a7b35704f
Benan-Akca,The hidden layer size doesnt match with megatron.,The Megatron model has a minimum of 1024 hidden size to my knowledge. But the example sell script has 768 hidden layer as original bert model. How should I rearrange the specs in shell script? `RANK=0 WORLD_SIZE=1  Wikipedia data can be downloaded from the following link:  https://github.com/facebookresearch/DPR/blob/master/data/download_data.py EVIDENCE_DATA_DIR= EMBEDDING_PATH= CHECKPOINT_PATH= python tools/create_doc_index.py \     numlayers 12 \     hiddensize 768 \     numattentionheads 12 \     tensormodelparallelsize 1 \     microbatchsize 128 \     checkpointactivations \     seqlength 512 \     retrieverseqlength 256 \     maxpositionembeddings 512 \     load ${CHECKPOINT_PATH} \     evidencedatapath ${EVIDENCE_DATA_DIR} \     embeddingpath ${EMBEDDING_PATH} \     indexerloginterval 1000 \     indexerbatchsize 128 \     vocabfile bertvocab.txt \     numworkers 2 \     fp16`,2021-04-20T09:53:19Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/95,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
Benan-Akca,Do we need to use preprocess.py before loading dataset?,"Hi , In order to use the  ""/create_doc_index.py"" do we need to use the preprocess.py script before? Or we directly use the dataset?",2021-04-20T09:48:12Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/94,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
Benan-Akca,Do we need to use preprocess.py before loading dataset?,,2021-04-20T09:44:58Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/93
SerenaQuinn,How to solve the OOM problem when my vocab_size is very big,"My vocab_size is about 30w, and a ""cuda out of memory"" error occured. I think I should put the word_embeddings into cpu, then fetch the data and compute in gpu, then update the word_embeddings.  >  arguments    adam_beta1 ...................................... 0.9   adam_beta2 ...................................... 0.999   adam_eps ........................................ 1e08   adlr_autoresume ................................. False   adlr_autoresume_interval ........................ 1000   apply_query_key_layer_scaling ................... True   apply_residual_connection_post_layernorm ........ False   attention_dropout ............................... 0.1   attention_softmax_in_fp32 ....................... False   bert_load ....................................... None   bias_dropout_fusion ............................. True   bias_gelu_fusion ................................ True   block_data_path ................................. None   checkpoint_activations .......................... True   checkpoint_num_layers ........................... 1   clip_grad ....................................... 1.0   consumed_train_samples .......................... 0   consumed_valid_samples .......................... 0   data_impl ....................................... infer   data_parallel_size .............................. 1   data_path ....................................... ['/data/quxiaoru/Dataset/rev_gen/elec5/nei/graph_text_document']   DDP_impl ........................................ local   distribute_checkpointed_activations ............. False   distributed_backend ............................. nccl   eod_mask_loss ................................... False   eval_interval ................................... 1000   eval_iters ...................................... 10   exit_duration_in_mins ........................... None   exit_interval ................................... None   faiss_use_gpu ................................... False   finetune ........................................ False   fp16 ............................................ True   fp16_lm_cross_entropy ........................... False   fp32_allreduce .................................. False   fp32_residual_connection ........................ False   global_batch_size ............................... 8   graph_file ...................................... /data/quxiaoru/Dataset/rev_gen/elec5/nei/graph_file.txt   hidden_dropout .................................. 0.1   hidden_size ..................................... 256   hysteresis ...................................... 2   ict_head_size ................................... None   ict_load ........................................ None   indexer_batch_size .............................. 128   indexer_log_interval ............................ 1000   init_method_std ................................. 0.02   initial_loss_scale .............................. 4294967296   layernorm_epsilon ............................... 1e05   lazy_mpu_init ................................... None   load ............................................ /data/quxiaoru/Model/checkpoints/rev_gen/elec5/nei   local_rank ...................................... None   log_interval .................................... 100   loss_scale ...................................... None   loss_scale_window ............................... 1000   lr .............................................. 0.00015   lr_decay_iters .................................. None   lr_decay_samples ................................ None   lr_decay_style .................................. constant   lr_warmup_fraction .............................. 0.01   lr_warmup_iters ................................. 0   lr_warmup_samples ............................... 0   make_vocab_size_divisible_by .................... 128   mask_prob ....................................... 0.15   max_graph_capacity .............................. 20   max_position_embeddings ......................... 1024   merge_file ...................................... ../gpt2merges.txt   micro_batch_size ................................ 8   min_loss_scale .................................. 1.0   min_lr .......................................... 0.0   mmap_warmup ..................................... False   no_load_optim ................................... False   no_load_rng ..................................... False   no_resample ..................................... False   no_save_optim ................................... False   no_save_rng ..................................... False   num_attention_heads ............................. 8   num_layers ...................................... 6   num_workers ..................................... 2   onnx_safe ....................................... None   openai_gelu ..................................... False   override_lr_scheduler ........................... False   params_dtype .................................... torch.float16   pipeline_model_parallel_size .................... 1   query_in_block_prob ............................. 0.1   rampup_batch_size ............................... None   rank ............................................ 0   report_topk_accuracies .......................... []   reset_attention_mask ............................ True   reset_position_ids .............................. True   save ............................................ /data/quxiaoru/Model/checkpoints/rev_gen/elec5/nei   save_interval ................................... 5000   scaled_masked_softmax_fusion .................... True   scaled_upper_triang_masked_softmax_fusion ....... True   seed ............................................ 1234   seq_length ...................................... 1024   short_seq_prob .................................. 0.1   split ........................................... 80, 10, 10   tensor_model_parallel_size ...................... 1   tensorboard_dir ................................. None   titles_data_path ................................ None   tokenizer_type .................................. GraphBPETokenizer   train_iters ..................................... 5000000   train_samples ................................... None   use_checkpoint_lr_scheduler ..................... False   use_cpu_initialization .......................... False   use_one_sent_docs ............................... False   vocab_file ...................................... ../gpt2vocab.json   weight_decay .................................... 0.01   world_size ...................................... 1  end of arguments  setting number of microbatches to constant 1 > building GraphBPETokenizer tokenizer ...  > padded vocab (size: 305662) with 2 dummy tokens (new size: 305664) > initializing torch distributed ... > initializing tensor model parallel with size 1 > initializing pipeline model parallel with size 1 > setting random seeds to 1234 ... > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234 time to initialize megatron (seconds): 33.262 [after megatron is initialized] datetime: 20210417 05:58:57 building GPT model ...  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 83251200 > learning rate decay style: constant WARNING: could not find the metadata file /data/quxiaoru/Model/checkpoints/rev_gen/elec5/nei/latest_checkpointed_iteration.txt     will not load any checkpoints and will start from random time (ms)  train/valid/test data iterators: 405.21 training ... [before the start of training step] datetime: 20210417 05:58:57 Traceback (most recent call last):   File ""../pretrain_graph.py"", line 98, in      'scaled_upper_triang_masked_softmax_fusion': True})   File ""/data/quxiaoru/Code/rev_gen/megatron/training.py"", line 129, in pretrain     train_data_iterator, valid_data_iterator)   File ""/data/quxiaoru/Code/rev_gen/megatron/training.py"", line 808, in train     lr_scheduler)   File ""/data/quxiaoru/Code/rev_gen/megatron/training.py"", line 594, in train_step     forward_step_func, data_iterator, model, optimizer, timers)   File ""/data/quxiaoru/Code/rev_gen/megatron/training.py"", line 507, in forward_backward_no_pipelining     loss, loss_reduced = forward_step_func(data_iterator, model, input_tensor=None)   File ""../pretrain_graph.py"", line 72, in forward_step     labels=labels)   File ""/data/quxiaoru/anaconda3/envs/torch18/lib/python3.7/sitepackages/torch/nn/modules/module.py"", line 889, in _call_impl     result = self.forward(*input, **kwargs)   File ""/data/quxiaoru/Code/rev_gen/megatron/model/distributed.py"", line 76, in forward     return self.module(*inputs, **kwargs)   File ""/data/quxiaoru/anaconda3/envs/torch18/lib/python3.7/sitepackages/torch/nn/modules/module.py"", line 889, in _call_impl     result = self.forward(*input, **kwargs)   File ""/data/quxiaoru/Code/rev_gen/megatron/model/module.py"", line 140, in forward     outputs = self.module(*inputs, **kwargs)   File ""/data/quxiaoru/anaconda3/envs/torch18/lib/python3.7/sitepackages/torch/nn/modules/module.py"", line 889, in _call_impl     result = self.forward(*input, **kwargs)   File ""/data/quxiaoru/Code/rev_gen/megatron/model/gpt_model.py"", line 150, in forward     forward_method_parallel_output=forward_method_parallel_output)   File ""/data/quxiaoru/Code/rev_gen/megatron/model/gpt_model.py"", line 104, in forward     self.fp16_lm_cross_entropy)   File ""/data/quxiaoru/Code/rev_gen/megatron/model/gpt_model.py"", line 60, in post_language_model_processing     loss = mpu.vocab_parallel_cross_entropy(output.float(), labels) RuntimeError: CUDA out of memory. Tried to allocate 9.33 GiB (GPU 0; 10.76 GiB total capacity; 5.17 GiB already allocated; 4.48 GiB free; 5.28 GiB reserved in total by PyTorch)",2021-04-18T02:16:07Z,stale,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/92,Does anyone know how to sovle this problem?," CC(OOM when training the same size of gpt2(2.6B) with mp=2 dp=8 with 64*V100(32GB)) may help, though not quite clear what makes sense for different DDP_impl",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
Benan-Akca,Can not create embeddings from Megatron,"I tried to get embeddings with using the example create_embeddings.sh I used wikipedia dataset as mentioned, another time I used a self created json file that has only one data in it. I tried them as both outputs of preprocess.py and without using preprocess.py.  It gives me this error  `using world size: 1, dataparallelsize: 1, tensormodelparallel size: 1, pipelinemodelparallel size: 1  setting global batch size to 128 using torch.float16 for parameters ...  arguments    accumulate_allreduce_grads_in_fp32 .............. False   adam_beta1 ...................................... 0.9   adam_beta2 ...................................... 0.999   adam_eps ........................................ 1e08   adlr_autoresume ................................. False   adlr_autoresume_interval ........................ 1000   apply_query_key_layer_scaling ................... True   apply_residual_connection_post_layernorm ........ False   attention_dropout ............................... 0.1   attention_softmax_in_fp32 ....................... False   bert_binary_head ................................ True   bert_load ....................................... None   bf16 ............................................ False   bias_dropout_fusion ............................. True   bias_gelu_fusion ................................ True   biencoder_projection_dim ........................ 0   biencoder_shared_query_context_model ............ False   block_data_path ................................. None   checkpoint_activations .......................... True   checkpoint_num_layers ........................... 1   clip_grad ....................................... 1.0   consumed_train_samples .......................... 0   consumed_valid_samples .......................... 0   data_impl ....................................... infer   data_parallel_size .............................. 1   data_path ....................................... None   dataloader_type ................................. single   DDP_impl ........................................ local   decoder_seq_length .............................. None   distribute_checkpointed_activations ............. False   distributed_backend ............................. nccl   embedding_path .................................. /content/apex/MegatronLM/   encoder_seq_length .............................. 512   eod_mask_loss ................................... False   eval_interval ................................... 1000   eval_iters ...................................... 100   evidence_data_path .............................. /content/apex/MegatronLM/data.json   exit_duration_in_mins ........................... None   exit_interval ................................... None   ffn_hidden_size ................................. 3072   finetune ........................................ False   fp16 ............................................ True   fp16_lm_cross_entropy ........................... False   fp32_residual_connection ........................ False   global_batch_size ............................... 128   hidden_dropout .................................. 0.1   hidden_size ..................................... 768   hysteresis ...................................... 2   ict_head_size ................................... None   ict_load ........................................ None   img_dim ......................................... 224   indexer_batch_size .............................. 128   indexer_log_interval ............................ 1000   init_method_std ................................. 0.02   init_method_xavier_uniform ...................... False   initial_loss_scale .............................. 4294967296   kv_channels ..................................... 64   layernorm_epsilon ............................... 1e05   lazy_mpu_init ................................... None   load ............................................ /content/apex/MegatronLM//checkpoints/release/mp_rank_00/model_optim_rng.pt   local_rank ...................................... None   log_batch_size_to_tensorboard ................... False   log_interval .................................... 100   log_learning_rate_to_tensorboard ................ True   log_loss_scale_to_tensorboard ................... True   log_num_zeros_in_grad ........................... False   log_params_norm ................................. False   log_timers_to_tensorboard ....................... False   log_validation_ppl_to_tensorboard ............... False   loss_scale ...................................... None   loss_scale_window ............................... 1000   lr .............................................. None   lr_decay_iters .................................. None   lr_decay_samples ................................ None   lr_decay_style .................................. linear   lr_warmup_fraction .............................. None   lr_warmup_iters ................................. 0   lr_warmup_samples ............................... 0   make_vocab_size_divisible_by .................... 128   mask_prob ....................................... 0.15   masked_softmax_fusion ........................... True   max_position_embeddings ......................... 512   merge_file ...................................... None   micro_batch_size ................................ 128   min_loss_scale .................................. 1.0   min_lr .......................................... 0.0   mmap_warmup ..................................... False   no_load_optim ................................... None   no_load_rng ..................................... None   no_save_optim ................................... None   no_save_rng ..................................... None   num_attention_heads ............................. 12   num_channels .................................... 3   num_classes ..................................... 1000   num_layers ...................................... 12   num_layers_per_virtual_pipeline_stage ........... None   num_workers ..................................... 2   onnx_safe ....................................... None   openai_gelu ..................................... False   optimizer ....................................... adam   override_lr_scheduler ........................... False   params_dtype .................................... torch.float16   patch_dim ....................................... 16   pipeline_model_parallel_size .................... 1   query_in_block_prob ............................. 0.1   rampup_batch_size ............................... None   rank ............................................ 0   reset_attention_mask ............................ False   reset_position_ids .............................. False   retriever_report_topk_accuracies ................ []   retriever_score_scaling ......................... False   retriever_seq_length ............................ 256   sample_rate ..................................... 1.0   save ............................................ None   save_interval ................................... None   scatter_gather_tensors_in_pipeline .............. True   seed ............................................ 1234   seq_length ...................................... 512   sgd_momentum .................................... 0.9   short_seq_prob .................................. 0.1   split ........................................... 969, 30, 1   tensor_model_parallel_size ...................... 1   tensorboard_dir ................................. None   tensorboard_log_interval ........................ 1   tensorboard_queue_size .......................... 1000   titles_data_path ................................ None   tokenizer_type .................................. BertWordPieceLowerCase   train_iters ..................................... None   train_samples ................................... None   use_checkpoint_lr_scheduler ..................... False   use_contiguous_buffers_in_ddp ................... False   use_cpu_initialization .......................... None   use_one_sent_docs ............................... False   virtual_pipeline_model_parallel_size ............ None   vocab_file ...................................... bertvocab.txt   weight_decay .................................... 0.01   world_size ...................................... 1  end of arguments  setting number of microbatches to constant 1 > building BertWordPieceLowerCase tokenizer ...  > padded vocab (size: 30522) with 70 dummy tokens (new size: 30592) > initializing torch distributed ... > initializing tensor model parallel with size 1 > initializing pipeline model parallel with size 1 > setting random seeds to 1234 ... > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234 > compiling dataset index builder ... make: Entering directory '/usr/local/lib/python3.7/distpackages/megatron/data' make: Nothing to be done for 'default'. make: Leaving directory '/usr/local/lib/python3.7/distpackages/megatron/data' >>> done with dataset index builder. Compilation time: 0.057 seconds > compiling and loading fused kernels ... Detected CUDA files, patching ldflags Emitting ninja build file /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/build/build.ninja... Building extension module scaled_upper_triang_masked_softmax_cuda... Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N) ninja: warning: bad deps log signature or version; starting over [1/3] c++ MMD MF scaled_upper_triang_masked_softmax.o.d DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda DTORCH_API_INCLUDE_EXTENSION_H DPYBIND11_COMPILER_TYPE=\""_gcc\"" DPYBIND11_STDLIB=\""_libstdcpp\"" DPYBIND11_BUILD_ABI=\""_cxxabi1011\"" isystem /usr/local/lib/python3.7/distpackages/torch/include isystem /usr/local/lib/python3.7/distpackages/torch/include/torch/csrc/api/include isystem /usr/local/lib/python3.7/distpackages/torch/include/TH isystem /usr/local/lib/python3.7/distpackages/torch/include/THC isystem /usr/local/cuda/include isystem /usr/include/python3.7m D_GLIBCXX_USE_CXX11_ABI=0 fPIC std=c++14 O3 c /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/scaled_upper_triang_masked_softmax.cpp o scaled_upper_triang_masked_softmax.o  [2/3] /usr/local/cuda/bin/nvcc  DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda DTORCH_API_INCLUDE_EXTENSION_H DPYBIND11_COMPILER_TYPE=\""_gcc\"" DPYBIND11_STDLIB=\""_libstdcpp\"" DPYBIND11_BUILD_ABI=\""_cxxabi1011\"" isystem /usr/local/lib/python3.7/distpackages/torch/include isystem /usr/local/lib/python3.7/distpackages/torch/include/torch/csrc/api/include isystem /usr/local/lib/python3.7/distpackages/torch/include/TH isystem /usr/local/lib/python3.7/distpackages/torch/include/THC isystem /usr/local/cuda/include isystem /usr/include/python3.7m D_GLIBCXX_USE_CXX11_ABI=0 D__CUDA_NO_HALF_OPERATORS__ D__CUDA_NO_HALF_CONVERSIONS__ D__CUDA_NO_BFLOAT16_CONVERSIONS__ D__CUDA_NO_HALF2_OPERATORS__ exptrelaxedconstexpr gencode=arch=compute_70,code=compute_70 gencode=arch=compute_70,code=sm_70 compileroptions 'fPIC' O3 gencode arch=compute_70,code=sm_70 use_fast_math U__CUDA_NO_HALF_OPERATORS__ U__CUDA_NO_HALF_CONVERSIONS__ exptrelaxedconstexpr exptextendedlambda gencode arch=compute_80,code=sm_80 std=c++14 c /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/scaled_upper_triang_masked_softmax_cuda.cu o scaled_upper_triang_masked_softmax_cuda.cuda.o  [3/3] c++ scaled_upper_triang_masked_softmax.o scaled_upper_triang_masked_softmax_cuda.cuda.o shared L/usr/local/lib/python3.7/distpackages/torch/lib lc10 lc10_cuda ltorch_cpu ltorch_cuda ltorch ltorch_python L/usr/local/cuda/lib64 lcudart o scaled_upper_triang_masked_softmax_cuda.so Loading extension module scaled_upper_triang_masked_softmax_cuda... Detected CUDA files, patching ldflags Emitting ninja build file /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/build/build.ninja... Building extension module scaled_masked_softmax_cuda... Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N) [1/3] c++ MMD MF scaled_masked_softmax.o.d DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda DTORCH_API_INCLUDE_EXTENSION_H DPYBIND11_COMPILER_TYPE=\""_gcc\"" DPYBIND11_STDLIB=\""_libstdcpp\"" DPYBIND11_BUILD_ABI=\""_cxxabi1011\"" isystem /usr/local/lib/python3.7/distpackages/torch/include isystem /usr/local/lib/python3.7/distpackages/torch/include/torch/csrc/api/include isystem /usr/local/lib/python3.7/distpackages/torch/include/TH isystem /usr/local/lib/python3.7/distpackages/torch/include/THC isystem /usr/local/cuda/include isystem /usr/include/python3.7m D_GLIBCXX_USE_CXX11_ABI=0 fPIC std=c++14 O3 c /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/scaled_masked_softmax.cpp o scaled_masked_softmax.o  [2/3] /usr/local/cuda/bin/nvcc  DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda DTORCH_API_INCLUDE_EXTENSION_H DPYBIND11_COMPILER_TYPE=\""_gcc\"" DPYBIND11_STDLIB=\""_libstdcpp\"" DPYBIND11_BUILD_ABI=\""_cxxabi1011\"" isystem /usr/local/lib/python3.7/distpackages/torch/include isystem /usr/local/lib/python3.7/distpackages/torch/include/torch/csrc/api/include isystem /usr/local/lib/python3.7/distpackages/torch/include/TH isystem /usr/local/lib/python3.7/distpackages/torch/include/THC isystem /usr/local/cuda/include isystem /usr/include/python3.7m D_GLIBCXX_USE_CXX11_ABI=0 D__CUDA_NO_HALF_OPERATORS__ D__CUDA_NO_HALF_CONVERSIONS__ D__CUDA_NO_BFLOAT16_CONVERSIONS__ D__CUDA_NO_HALF2_OPERATORS__ exptrelaxedconstexpr gencode=arch=compute_70,code=compute_70 gencode=arch=compute_70,code=sm_70 compileroptions 'fPIC' O3 gencode arch=compute_70,code=sm_70 use_fast_math U__CUDA_NO_HALF_OPERATORS__ U__CUDA_NO_HALF_CONVERSIONS__ exptrelaxedconstexpr exptextendedlambda gencode arch=compute_80,code=sm_80 std=c++14 c /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/scaled_masked_softmax_cuda.cu o scaled_masked_softmax_cuda.cuda.o  [3/3] c++ scaled_masked_softmax.o scaled_masked_softmax_cuda.cuda.o shared L/usr/local/lib/python3.7/distpackages/torch/lib lc10 lc10_cuda ltorch_cpu ltorch_cuda ltorch ltorch_python L/usr/local/cuda/lib64 lcudart o scaled_masked_softmax_cuda.so Loading extension module scaled_masked_softmax_cuda... Detected CUDA files, patching ldflags Emitting ninja build file /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/build/build.ninja... Building extension module fused_mix_prec_layer_norm_cuda... Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N) [1/3] c++ MMD MF layer_norm_cuda.o.d DTORCH_EXTENSION_NAME=fused_mix_prec_layer_norm_cuda DTORCH_API_INCLUDE_EXTENSION_H DPYBIND11_COMPILER_TYPE=\""_gcc\"" DPYBIND11_STDLIB=\""_libstdcpp\"" DPYBIND11_BUILD_ABI=\""_cxxabi1011\"" isystem /usr/local/lib/python3.7/distpackages/torch/include isystem /usr/local/lib/python3.7/distpackages/torch/include/torch/csrc/api/include isystem /usr/local/lib/python3.7/distpackages/torch/include/TH isystem /usr/local/lib/python3.7/distpackages/torch/include/THC isystem /usr/local/cuda/include isystem /usr/include/python3.7m D_GLIBCXX_USE_CXX11_ABI=0 fPIC std=c++14 O3 c /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda.cpp o layer_norm_cuda.o  [2/3] /usr/local/cuda/bin/nvcc  DTORCH_EXTENSION_NAME=fused_mix_prec_layer_norm_cuda DTORCH_API_INCLUDE_EXTENSION_H DPYBIND11_COMPILER_TYPE=\""_gcc\"" DPYBIND11_STDLIB=\""_libstdcpp\"" DPYBIND11_BUILD_ABI=\""_cxxabi1011\"" isystem /usr/local/lib/python3.7/distpackages/torch/include isystem /usr/local/lib/python3.7/distpackages/torch/include/torch/csrc/api/include isystem /usr/local/lib/python3.7/distpackages/torch/include/TH isystem /usr/local/lib/python3.7/distpackages/torch/include/THC isystem /usr/local/cuda/include isystem /usr/include/python3.7m D_GLIBCXX_USE_CXX11_ABI=0 D__CUDA_NO_HALF_OPERATORS__ D__CUDA_NO_HALF_CONVERSIONS__ D__CUDA_NO_BFLOAT16_CONVERSIONS__ D__CUDA_NO_HALF2_OPERATORS__ exptrelaxedconstexpr gencode=arch=compute_70,code=compute_70 gencode=arch=compute_70,code=sm_70 compileroptions 'fPIC' O3 gencode arch=compute_70,code=sm_70 use_fast_math maxrregcount=50 gencode arch=compute_80,code=sm_80 std=c++14 c /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu o layer_norm_cuda_kernel.cuda.o  /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu: In function â€˜void cuda_layer_norm(at::Tensor*, at::Tensor*, at::Tensor*, at::Tensor*, int, int, c10::IntList, at::Tensor*, at::Tensor*, double)â€™: /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:703:224: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                 ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:703:247: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                        ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:703:272: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                                 ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:703:296: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                                                         ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:703:359: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                                                                                                                        ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:703:414: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                                                                                                                                                                               ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:703:119: warning: â€˜T* at::Tensor::data() const [with T = c10::Half]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                        ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:703:142: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                               ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:703:167: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                        ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:703:191: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:703:258: warning: â€˜T* at::Tensor::data() const [with T = c10::Half]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                   ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:703:317: warning: â€˜T* at::Tensor::data() const [with T = c10::Half]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                                                                              ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:703:131: warning: â€˜T* at::Tensor::data() const [with T = c10::BFloat16]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                    ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:703:154: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                           ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:703:179: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                    ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:703:203: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                            ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:703:274: warning: â€˜T* at::Tensor::data() const [with T = c10::BFloat16]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                                   ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:703:337: warning: â€˜T* at::Tensor::data() const [with T = c10::BFloat16]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                                                                                                  ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:703:151: warning: â€˜T* at::Tensor::data() const [with T = c10::Half]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                        ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:703:174: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                               ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:703:199: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                        ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:703:227: warning: â€˜T* at::Tensor::data() const [with T = c10::Half]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                    ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:703:294: warning: â€˜T* at::Tensor::data() const [with T = c10::Half]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                                                       ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:703:353: warning: â€˜T* at::Tensor::data() const [with T = c10::Half]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                                                                                                                  ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:703:166: warning: â€˜T* at::Tensor::data() const [with T = c10::BFloat16]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                       ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:703:189: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                              ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:703:214: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                       ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:703:246: warning: â€˜T* at::Tensor::data() const [with T = c10::BFloat16]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                       ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:703:317: warning: â€˜T* at::Tensor::data() const [with T = c10::BFloat16]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                                                                              ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:703:380: warning: â€˜T* at::Tensor::data() const [with T = c10::BFloat16]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                                                                                                                                             ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu: In function â€˜void cuda_layer_norm_gradient(at::Tensor*, at::Tensor*, at::Tensor*, at::Tensor*, int, int, c10::IntList, at::Tensor*, at::Tensor*, double, at::Tensor*, at::Tensor*, at::Tensor*)â€™: /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:224: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                 ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:247: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                        ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:272: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                                 ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:333: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                                                                                              ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:389: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                                                                                                                                                      ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:438: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                                                                                                                                                                                                       ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:489: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:550: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:120: warning: â€˜T* at::Tensor::data() const [with T = c10::Half]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                         ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:143: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:168: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                         ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:233: warning: â€˜T* at::Tensor::data() const [with T = c10::Half]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                          ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:293: warning: â€˜T* at::Tensor::data() const [with T = c10::Half]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                                                      ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:342: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                                                                                                       ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:397: warning: â€˜T* at::Tensor::data() const [with T = c10::Half]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                                                                                                                                                              ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:462: warning: â€˜T* at::Tensor::data() const [with T = c10::Half]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:132: warning: â€˜T* at::Tensor::data() const [with T = c10::BFloat16]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                     ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:155: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                            ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:180: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                     ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:249: warning: â€˜T* at::Tensor::data() const [with T = c10::BFloat16]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                          ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:313: warning: â€˜T* at::Tensor::data() const [with T = c10::BFloat16]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                                                                          ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:362: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                                                                                                                           ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:421: warning: â€˜T* at::Tensor::data() const [with T = c10::BFloat16]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                                                                                                                                                                                      ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:490: warning: â€˜T* at::Tensor::data() const [with T = c10::BFloat16]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:152: warning: â€˜T* at::Tensor::data() const [with T = c10::Half]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                         ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:175: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:200: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                         ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:265: warning: â€˜T* at::Tensor::data() const [with T = c10::Half]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                          ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:325: warning: â€˜T* at::Tensor::data() const [with T = c10::Half]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                                                                                      ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:378: warning: â€˜T* at::Tensor::data() const [with T = c10::Half]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                                                                                                                                           ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:433: warning: â€˜T* at::Tensor::data() const [with T = c10::Half]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                                                                                                                                                                                                  ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:498: warning: â€˜T* at::Tensor::data() const [with T = c10::Half]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:167: warning: â€˜T* at::Tensor::data() const [with T = c10::BFloat16]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                        ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:190: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                               ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:215: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                        ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:284: warning: â€˜T* at::Tensor::data() const [with T = c10::BFloat16]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                                             ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:348: warning: â€˜T* at::Tensor::data() const [with T = c10::BFloat16]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                                                                                                             ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:405: warning: â€˜T* at::Tensor::data() const [with T = c10::BFloat16]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                                                                                                                                                                      ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:464: warning: â€˜T* at::Tensor::data() const [with T = c10::BFloat16]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:533: warning: â€˜T* at::Tensor::data() const [with T = c10::BFloat16]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ^ /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu: In instantiation of â€˜void HostLayerNormGradient(const V*, const U*, const U*, at::Tensor*, int, int, const V*, const V*, double, T*, V*, V*) [with T = float; U = float; V = float]â€™: /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:562:   required from here /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:748:106: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]        cuComputePartGradGammaBeta>>(                                                                                                           ^                                                                                                                                                      /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:748:106: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]        cuComputePartGradGammaBeta>>(                                                                                                           ^                                                                                                                                                      /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:748:106: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]        cuComputePartGradGammaBeta>>(                                                                                                           ^                                                                                                                                                      /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:761:102: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]        cuComputeGradGammaBeta>>(                                                                                                       ^                                                                                                                         /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:761:102: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]        cuComputeGradGammaBeta>>(                                                                                                       ^                                                                                                                         /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:779:97: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      cuComputeGradInput>>(                                                                                                  ^                                                                                          /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu: In instantiation of â€˜void HostLayerNormGradient(const V*, const U*, const U*, at::Tensor*, int, int, const V*, const V*, double, T*, V*, V*) [with T = float; U = float; V = c10::Half]â€™: /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:474:   required from here /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:748:106: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]        cuComputePartGradGammaBeta>>(                                                                                                           ^                                                                                                                                                      /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:748:106: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]        cuComputePartGradGammaBeta>>(                                                                                                           ^                                                                                                                                                      /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:748:106: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]        cuComputePartGradGammaBeta>>(                                                                                                           ^                                                                                                                                                      /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:761:102: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]        cuComputeGradGammaBeta>>(                                                                                                       ^                                                                                                                         /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:761:102: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]        cuComputeGradGammaBeta>>(                                                                                                       ^                                                                                                                         /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:779:97: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      cuComputeGradInput>>(                                                                                                  ^                                                                                          /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu: In instantiation of â€˜void HostLayerNormGradient(const V*, const U*, const U*, at::Tensor*, int, int, const V*, const V*, double, T*, V*, V*) [with T = float; U = float; V = c10::BFloat16]â€™: /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:502:   required from here /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:748:106: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]        cuComputePartGradGammaBeta>>(                                                                                                           ^                                                                                                                                                      /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:748:106: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]        cuComputePartGradGammaBeta>>(                                                                                                           ^                                                                                                                                                      /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:748:106: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]        cuComputePartGradGammaBeta>>(                                                                                                           ^                                                                                                                                                      /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:761:102: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]        cuComputeGradGammaBeta>>(                                                                                                       ^                                                                                                                         /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:761:102: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]        cuComputeGradGammaBeta>>(                                                                                                       ^                                                                                                                         /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:779:97: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      cuComputeGradInput>>(                                                                                                  ^                                                                                          /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu: In instantiation of â€˜void HostLayerNormGradient(const V*, const U*, const U*, at::Tensor*, int, int, const V*, const V*, double, T*, V*, V*) [with T = c10::Half; U = float; V = c10::Half]â€™: /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:510:   required from here /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:748:106: warning: â€˜T* at::Tensor::data() const [with T = c10::Half]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]        cuComputePartGradGammaBeta>>(                                                                                                           ^                                                                                                                                                      /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:748:106: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]        cuComputePartGradGammaBeta>>(                                                                                                           ^                                                                                                                                                      /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:748:106: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]        cuComputePartGradGammaBeta>>(                                                                                                           ^                                                                                                                                                      /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:761:102: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]        cuComputeGradGammaBeta>>(                                                                                                       ^                                                                                                                         /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:761:102: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]        cuComputeGradGammaBeta>>(                                                                                                       ^                                                                                                                         /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:779:97: warning: â€˜T* at::Tensor::data() const [with T = c10::Half]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      cuComputeGradInput>>(                                                                                                  ^                                                                                          /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu: In instantiation of â€˜void HostLayerNormGradient(const V*, const U*, const U*, at::Tensor*, int, int, const V*, const V*, double, T*, V*, V*) [with T = c10::BFloat16; U = float; V = c10::BFloat16]â€™: /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:811:545:   required from here /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:748:106: warning: â€˜T* at::Tensor::data() const [with T = c10::BFloat16]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]        cuComputePartGradGammaBeta>>(                                                                                                           ^                                                                                                                                                      /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:748:106: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]        cuComputePartGradGammaBeta>>(                                                                                                           ^                                                                                                                                                      /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:748:106: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]        cuComputePartGradGammaBeta>>(                                                                                                           ^                                                                                                                                                      /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:761:102: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]        cuComputeGradGammaBeta>>(                                                                                                       ^                                                                                                                         /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:761:102: warning: â€˜T* at::Tensor::data() const [with T = float]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]        cuComputeGradGammaBeta>>(                                                                                                       ^                                                                                                                         /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ /usr/local/lib/python3.7/distpackages/megatron/fused_kernels/layer_norm_cuda_kernel.cu:779:97: warning: â€˜T* at::Tensor::data() const [with T = c10::BFloat16]â€™ is deprecated: Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [Wdeprecateddeclarations]      cuComputeGradInput>>(                                                                                                  ^                                                                                          /usr/local/lib/python3.7/distpackages/torch/include/ATen/core/TensorBody.h:398:1: note: declared here    T * data() const {  ^ ~~ [3/3] c++ layer_norm_cuda.o layer_norm_cuda_kernel.cuda.o shared L/usr/local/lib/python3.7/distpackages/torch/lib lc10 lc10_cuda ltorch_cpu ltorch_cuda ltorch ltorch_python L/usr/local/cuda/lib64 lcudart o fused_mix_prec_layer_norm_cuda.so Loading extension module fused_mix_prec_layer_norm_cuda... >>> done with compiling and loading fused kernels. Compilation time: 166.480 seconds Traceback (most recent call last):   File ""tools/create_doc_index.py"", line 32, in      main()   File ""tools/create_doc_index.py"", line 27, in main     index_builder = IndexBuilder()   File ""/usr/local/lib/python3.7/distpackages/megatron/indexer.py"", line 37, in __init__     self.load_attributes()   File ""/usr/local/lib/python3.7/distpackages/megatron/indexer.py"", line 50, in load_attributes     model = get_model(lambda: biencoder_model_provider(only_context_model \   File ""/usr/local/lib/python3.7/distpackages/megatron/training.py"", line 217, in get_model     post_process=post_process TypeError: () got an unexpected keyword argument 'pre_process'` Can you help me pls",2021-04-17T13:00:16Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/91,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
Carolingliang,AttributeError: module 'torch' has no attribute '_amp_foreach_non_finite_check_and_unscale_',"I use cuda 10.1, when I run both v2.0 and v2.1 release of MegatronLMï¼Œ the code is :  bash examples/pretrain_gpt.sh  I get a bug as follow: !image",2021-04-08T06:41:17Z,stale,closed,2,4,https://github.com/NVIDIA/Megatron-LM/issues/90,I also have this problem now. may i ask how did you deal with it?,Upgradding pytorch to >1.8.0 will resolve the problem.,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
hyoo,fix issue #33 missing modules,`tools/openwebtext/cleanup_dataset.py` script requires these two modules in place. `Tokenizer` class was found in CC(tokenizer.py is missing in openwebtext)  and `megatron.tokenizer.gpt2_tokenization.py` needs `file_utils.py` in line 117 https://github.com/NVIDIA/MegatronLM/blob/main/megatron/tokenizer/gpt2_tokenization.pyL117,2021-04-05T02:47:30Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/89,May I ask how this does not get merged?," There are a large number of open issues and PRs so the project maintainers can't address them all. I'll take a look at issue CC(tokenizer.py is missing in openwebtext) and see if it still needs addressing. However, this PR is cannot be reopened as the source repo has been deleted."
bugface,bert_dataset.py - ValueError: Seed must be between 0 and 2**32 - 1,"Try to train a MegatronBert based on a 100GB text dataset. The training was ok for the first 550k steps (minibatch=8; globalbatch=7808, distributed on 976 GPUs) Then it raised a error: ```ValueError : Caught ValueError in DataLoader worker process 0. Original Traceback (most recent call last):   File ""/opt/conda/lib/python3.8/sitepackages/torch/utils/data/_utils/worker.py"", line 202, in _worker_loop     data = fetcher.fetch(index)   File ""/opt/conda/lib/python3.8/sitepackages/torch/utils/data/_utils/fetch.py"", line 44, in fetch     data = [self.dataset[idx] for idx in possibly_batched_index]   File ""/opt/conda/lib/python3.8/sitepackages/torch/utils/data/_utils/fetch.py"", line 44, in      data = [self.dataset[idx] for idx in possibly_batched_index]   File ""/red/gatortronphi/workspace/scripts/MegatronLM/megatron/data/bert_dataset.py"", line 77, in __getitem__     np_rng = np.random.RandomState(seed=(self.seed + idx))   File ""mtrand.pyx"", line 183, in numpy.random.mtrand.RandomState.__init__ return self._process_data(data)   File ""/opt/conda/lib/python3.8/sitepackages/torch/utils/data/dataloader.py"", line 1225, in _process_data   File ""_mt19937.pyx"", line 166, in numpy.random._mt19937.MT19937._legacy_seeding   File ""_mt19937.pyx"", line 180, in numpy.random._mt19937.MT19937._legacy_seeding ValueError: Seed must be between 0 and 2**32  1 ``` where I think the problem is located at line 77  ```np_rng = np.random.RandomState(seed=(self.seed + idx))```  where the idx is too big for int due to the large dataset.",2021-03-23T23:40:48Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/88,a quick fix: ``` np_rng = np.random.RandomState(seed=(self.seed + idx)%2**31) ```,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
bugface,Support LAMB optimizer," LAMB optimizer (https://nvidia.github.io/apex/optimizers.html)  this optimizer is implemented in Apex https://nvidia.github.io/apex/optimizers.html  Current MegatronLM only support Adam  megatron/optimizer/__init__.py ``` def get_megatron_optimizer(model):     args = get_args()      Base optimizer.     param_groups = _get_params_for_weight_decay_optimization(model)     optimizer = Adam(param_groups,                      lr=args.lr,                      weight_decay=args.weight_decay,                      betas=(args.adam_beta1, args.adam_beta2),                      eps=args.adam_eps) ```  Why  using LAMB can support larger batch sizes and shorter training time.    make training on large datasets (hundreds of GB text) possible  what to change  arguments.py ``` def _add_training_args(parser):     group = parser.add_argument_group(title='training')     group.add_argument('optimizer', type=str, default='adam',                        help='can choose between adam and lamb as optimizer.') ```  optimizer/__init__.py ``` from apex.optimizers import FusedLAMB as Lamb def get_megatron_optimizer(model, optim='adam'):     args = get_args()      Base optimizer.     param_groups = _get_params_for_weight_decay_optimization(model)     if optim == ""adam"":         optimizer = Adam(param_groups,                          lr=args.lr,                          weight_decay=args.weight_decay,                          betas=(args.adam_beta1, args.adam_beta2),                          eps=args.adam_eps)     elif optim == 'lamb':         optimizer = Lamb(param_groups,                          lr=args.lr,                          weight_decay=args.weight_decay,                          betas=(args.adam_beta1, args.adam_beta2),                          eps=args.adam_eps)     else:         raise NotImplementedError(""We only implemented Adam and LAMB so far."") ```  training.py ``` def setup_model_and_optimizer(model_provider_func): ...     optimizer = get_megatron_optimizer(unwrapped_model, optim=args.optimizer) ``` Any thoughts?",2021-03-23T16:07:22Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/87,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
zjujh1995,loss curve in pretraining BERT is very strange,"We can't reproduce the result of pretraining BERT in paper ""MegatronLM"", and we've observed a very strange loss curve. The loss converged slowly in the beginning. After several k steps, the loss began to speed up to converge. As we know, loss usually converges fast in the beginning and slows down gradually during training procedure. Therefore, we're very confused to this special loss curve. May somebody finds the reason? All the parameters are kept the same as in the shell script ""pretrain_bert.sh"". Or will the curve be more normal if we use the parameters in the original paper instead of the shell script? Many thanks!",2021-03-16T07:30:41Z,stale,closed,0,8,https://github.com/NVIDIA/Megatron-LM/issues/86,"BTW, we've also trained GPT model based on Megatron, and the loss is normal as we expect.",The model learns initially with a small learning rate which gradually grows to the specified rate. This initial warmup phase is to prevent the model from overfitting to early samples and empirically useful when you scale to a very large batch size and learning rate. You can change warmup params to determine how long the phase is ,"> The model learns initially with a small learning rate which gradually grows to the specified rate. This initial warmup phase is to prevent the model from overfitting to early samples and empirically useful when you scale to a very large batch size and learning rate. You can change warmup params to determine how long the phase is Thanks for your reply. However, we've also done an experiment without LR schedualer, and the loss curve was still strange as I described above. On the other hand, the loss curve of GPT model is smooth and normal all the time (w or w/o LR schedualer). Only the one of BERT is confusing. We've doublechecked all our experimental settings, and there're also other researchers except for us facing the same confusing question.","> > The model learns initially with a small learning rate which gradually grows to the specified rate. This initial warmup phase is to prevent the model from overfitting to early samples and empirically useful when you scale to a very large batch size and learning rate. You can change warmup params to determine how long the phase is >  > Thanks for your reply. However, we've also done an experiment without LR schedualer, and the loss curve was still strange as I described above. On the other hand, the loss curve of GPT model is smooth and normal all the time (w or w/o LR schedualer). Only the one of BERT is confusing. >  > We've doublechecked all our experimental settings, and there're also other researchers except for us facing the same confusing question. Would you provide more context? Like how many steps have you trained and what is the loss value, how does the curve look like etc. ",I also meet the problem that the loss shows two sharp decrease. Have you understood the reason? ,"Exact same scenario to me. My first reaction was learning rate too large, then I decrease initial lr to 2.6e5. It seems interesting that no matter I set it 1e4 or 2.6e5 it always plateaus around the same loss value. Looking forward to reasonable explanations. ",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
rhythmswing,Vocabulary size of released pretrained BERT does not match the provided vocabulary file ,"Hi, It seems that the vocabulary size in provided checkpoints (https://ngc.nvidia.com/catalog/models/nvidia:megatron_bert_345m) for BERT 345M is 29056, different from the vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bertlargeuncasedvocab.txt which is 30592. Is it because that huggingface updated their vocab file and is there any matching vocab file for the provided pretrained model? ",2021-03-16T04:27:22Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/85,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
bugface,Issue#81 fix,The pull request aims to fix the issue mentioned in Issue CC(merge_file_ in MMapIndexedDatasetBuilder does not work because of _doc_idx ).  The fix mainly works for mmap dataset.,2021-03-15T18:06:13Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/84
nonatofabio,Fix issue with preprocessing empty first document,"This is a bug fix for issue CC(Data Preprocess fails if first document in merged json is empty)  When running the preprocess_data.py script as described on the documentation, if the first processed entry has an empty text this line fails with an `IndexError: list index out of range` because `len(doc_ids)==0` The bug fix implements a check for the length of `doc_ids`.",2021-03-15T17:19:26Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/83,Thanks for the PR. This has already been addressed. 
rhythmswing,Fused kernel compilation could get stuck ,"Hi,  I've noticed that the program could get stuck at ""using torch.float16 for parameters ..."". I found that the problem was stuck at compilating fused_kernels and deleting megatron/fused_kernel/build seems to fix the problem. I'm not sure what causes this.  I'm posting this in hope it could be helpful. ",2021-03-14T05:01:47Z,bug stale,closed,10,17,https://github.com/NVIDIA/Megatron-LM/issues/82,"same problem, stuck at here. ``` using world size: 1, dataparallelsize: 1, tensormodelparallel size: 1, pipelinemodelparallel size: 1  using torch.float16 for parameters ... ^CTraceback (most recent call last):   File ""pretrain_gpt.py"", line 149, in      pretrain(train_valid_test_datasets_provider, model_provider, forward_step,   File ""/home/superbencher/MegatronLM/megatron/training.py"", line 87, in pretrain     initialize_megatron(extra_args_provider=extra_args_provider,   File ""/home/superbencher/MegatronLM/megatron/initialize.py"", line 49, in initialize_megatron     set_global_variables(extra_args_provider=extra_args_provider,   File ""/home/superbencher/MegatronLM/megatron/global_vars.py"", line 82, in set_global_variables     args = _parse_args(extra_args_provider=extra_args_provider,   File ""/home/superbencher/MegatronLM/megatron/global_vars.py"", line 97, in _parse_args     _GLOBAL_ARGS = parse_args(extra_args_provider=extra_args_provider,   File ""/home/superbencher/MegatronLM/megatron/arguments.py"", line 188, in parse_args     fused_kernels.load_scaled_upper_triang_masked_softmax_fusion_kernel()   File ""/home/superbencher/MegatronLM/megatron/fused_kernels/__init__.py"", line 60, in load_scaled_upper_triang_masked_softmax_fusion_kernel     scaled_upper_triang_masked_softmax_cuda = cpp_extension.load(   File ""/home/superbencher/.local/lib/python3.8/sitepackages/torch/utils/cpp_extension.py"", line 1079, in load     return _jit_compile(   File ""/home/superbencher/.local/lib/python3.8/sitepackages/torch/utils/cpp_extension.py"", line 1306, in _jit_compile     baton.wait()   File ""/home/superbencher/.local/lib/python3.8/sitepackages/torch/utils/file_baton.py"", line 42, in wait     time.sleep(self.wait_seconds) KeyboardInterrupt ```",skip it by setting        noscaledmaskedsoftmaxfusion. do not how much it could affect end2end performance.,delete ```MegatronLM/megatron/fused_kernels/build/``` and restart works for me.,"Same issue. Actually, I got it run by removing the `megatron/fused_kernels/build` as suggested by ,  but I am wondering if it is the right way to get it fixed?","Experiencing the same issue here, even if the observed behaviour was different on different nodes of the cluster (not sure if it was caused by different software stacks or different gpus). Deleting `megatron/fused_kernels/build` did not work for me, and I only managed to solve the issue by completely dropping fused kernels as suggested by . To update this solution, the arguments that do this in the current version are `nomaskedsoftmaxfusion` and `nobiasdropoutfusion`.","Deleting `/megatron/fused_kernels/build` is recommended if you have upgraded CUDA versions or moved to different hardware.  Those changes will not automatically be detected causing a rebuild of the kernels, which may be required. We will be addressing this issue soon by moving to using the same prebuilt kernels from Apex and not requiring this custom kernel build step. I'll close this issue when that happens.",Marking as stale. No activity in 60 days.,Got stuck when compiling the fused_kernels when training on multiple nodes. But it works well in a single node. Why?,Marking as stale. No activity in 60 days.,> Got stuck when compiling the fused_kernels when training on multiple nodes. But it works well in a single node. Why? Same here. Did you solved this problem?,"> Got stuck when compiling the fused_kernels when training on multiple nodes. But it works well in a single node. Why?   Same problem, have you fixed this problem? ",> > Got stuck when compiling the fused_kernels when training on multiple nodes. But it works well in a single node. Why? >  > Same here. Did you solved this problem? +1 same issue here,> Got stuck when compiling the fused_kernels when training on multiple nodes. But it works well in a single node. Why? same here,"> Got stuck when compiling the fused_kernels when training on multiple nodes. But it works well in a single node. Why? Actually, it seems to be a problem with pytorch barrier, and simply setting NCCL_P2P_DISABLE=1 worked for me.  credit: https://discuss.pytorch.org/t/torchdistributedbarrierdoesntworkwithpytorch20andbackendnccl/190232 ","awesome to hear, will try this, thanks!","> > Got stuck when compiling the fused_kernels when training on multiple nodes. But it works well in a single node. Why? >  > Actually, it seems to be a problem with pytorch barrier, and simply setting NCCL_P2P_DISABLE=1 worked for me. credit: https://discuss.pytorch.org/t/torchdistributedbarrierdoesntworkwithpytorch20andbackendnccl/190232 I met this problem on one of my nodes. Working on that node along (NNODE=1), it does not work. To solve, I applied NCCL_P2P_DISABLE=1 to that node. This seems a hardwarerelated / BIOS setting issue. But distributed training by excluding that node worked for me.",Marking as stale. No activity in 60 days.
bugface,merge_file_ in MMapIndexedDatasetBuilder does not work because of _doc_idx ," task try to merge two .bin files into one   code ```python data_path_prefix = [""test1"", ""test2""] class A:     def __init__(self):         self.tokenizer_type = 'BertWordPieceCase'         self.rank = 0         self.vocab_file = '/blue/yonghui.wu/alexgre/data/vocabs/bert/vocab.txt'         self.merge_file = None         self.make_vocab_size_divisible_by = 128         self.tensor_model_parallel_size = 1 args = A() tokenizer = build_tokenizer(args) builders = indexed_dataset.make_builder(output_bin_files,  impl='mmap', vocab_size=30592) for each in data_path_prefix:     builders.merge_file_(each) builders.finalize(output_idx_files) ```   issue after merging, when ran training using the merged data, raise an error: ``` > building train, validation, and test datasets ...  > datasets target sizes (minimum size):     train:      1024000000     validation: 107520     test:       5120 > building train, validation, and test datasets for BERT ...  > building dataset index ...     reading sizes...     reading pointers...     reading document index...     creating numpy buffer of mmap...     creating memory view of numpy buffer... Traceback (most recent call last):   File ""../MegatronLM/pretrain_bert.py"", line 154, in      pretrain(train_valid_test_datasets_provider, model_provider, forward_step,   File ""/blue/yonghui.wu/alexgre/MegatronLM/megatron/training.py"", line 115, in pretrain     = build_train_valid_test_data_iterators(   File ""/blue/yonghui.wu/alexgre/MegatronLM/megatron/training.py"", line 995, in build_train_valid_test_data_iterators     train_ds, valid_ds, test_ds = build_train_valid_test_datasets_provider(   File ""../MegatronLM/pretrain_bert.py"", line 137, in train_valid_test_datasets_provider     train_ds, valid_ds, test_ds = build_train_valid_test_datasets(   File ""/blue/yonghui.wu/alexgre/MegatronLM/megatron/data/dataset_utils.py"", line 398, in build_train_valid_test_datasets     return _build_train_valid_test_datasets(data_prefix[0],   File ""/blue/yonghui.wu/alexgre/MegatronLM/megatron/data/dataset_utils.py"", line 453, in _build_train_valid_test_datasets     indexed_dataset = get_indexed_dataset_(data_prefix,   File ""/blue/yonghui.wu/alexgre/MegatronLM/megatron/data/dataset_utils.py"", line 549, in get_indexed_dataset_     assert indexed_dataset.sizes.shape[0] == indexed_dataset.doc_idx[1] AssertionError ```  solution I modify the ```merge_file_``` function of ```MMapIndexedDatasetBuilder``` class in ```indexed_dataset.py``` and now it workes ```python class MMapIndexedDatasetBuilder(object):     def __init__(self, out_file, dtype=np.int64):         self._data_file = open(out_file, 'wb')         self._dtype = dtype         self._sizes = []         self._doc_idx = [0]         self._merge_idx = 0     def add_item(self, tensor):         np_array = np.array(tensor.numpy(), dtype=self._dtype)         self._data_file.write(np_array.tobytes(order='C'))         self._sizes.append(np_array.size)     def end_document(self):         self._doc_idx.append(len(self._sizes))     def merge_file_(self, another_file):          Concatenate index         index = MMapIndexedDataset.Index(index_file_path(another_file))         assert index.dtype == self._dtype         for s in index.sizes:             self._sizes.append(s)         if self._merge_idx == 0:             self._doc_idx = []             self._doc_idx.extend(index.doc_idx)         else:             start_pt = self._doc_idx[1]             for each in index.doc_idx:                 new_doc_idx = start_pt + each                 self._doc_idx.append(new_doc_idx)          Concatenate data         with open(data_file_path(another_file), 'rb') as f:             shutil.copyfileobj(f, self._data_file)         self._merge_idx += 1     def finalize(self, index_file):         self._data_file.close()         self._sizes = np.array(self._sizes)         self._doc_idx = np.array(self._doc_idx)         print(self._sizes.shape)         print(self._doc_idx.shape)         print(self._sizes.shape[0], self._doc_idx[1])         with MMapIndexedDataset.Index.writer(index_file, self._dtype) as index:             index.write(self._sizes, self._doc_idx) ```  follow up do you want me to create a pull request on this?",2021-03-14T03:29:32Z,,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/81,"But I had this error assert  > indexed_dataset.sizes.shape[0] == indexed_dataset.doc_idx[1]  too I tried in this way,then the training just went done in 1 second.According to the output,this train/valid/test set all is 0,seems that nothing had been done. Can you fix my problem?ball ball you ! Both Chinese never cheat Chinese!","> But I had this error assert >  > > indexed_dataset.sizes.shape[0] == indexed_dataset.doc_idx[1] >  > too I tried in this way,then the training just went done in 1 second.According to the output,this train/valid/test set all is 0,seems that nothing had been done. Can you fix my problem?ball ball you ! Both Chinese never cheat Chinese! The solution posted works for my case.  Without your code, data, and ENV, it is hard to say exactly why it does not work for you.  I am not a developer for MegatronLM, you might want to contact the repo maintainer for a reliable solution.","OK ,thank you the same way.",This was fixed with this commit: https://github.com/NVIDIA/MegatronLM/commit/a2c5e6cd58728190e1183fd02fdabbbc57e35f0f
Co1lin,Can't find scaled_masked_softmax.cpp,"I want to run `bash examples/generate_text.sh`, but an error occurs: ``` Traceback (most recent call last):   File ""tools/generate_samples_gpt.py"", line 116, in      main()   File ""tools/generate_samples_gpt.py"", line 94, in main     initialize_megatron(extra_args_provider=add_text_generate_args,   File ""/opt/conda/envs/as/lib/python3.8/sitepackages/megatron_lm1.1.5py3.8.egg/megatron/initialize.py"", line 48, in initialize_megatron     set_global_variables(extra_args_provider=extra_args_provider,   File ""/opt/conda/envs/as/lib/python3.8/sitepackages/megatron_lm1.1.5py3.8.egg/megatron/global_vars.py"", line 82, in set_global_variables     args = _parse_args(extra_args_provider=extra_args_provider,   File ""/opt/conda/envs/as/lib/python3.8/sitepackages/megatron_lm1.1.5py3.8.egg/megatron/global_vars.py"", line 97, in _parse_args     _GLOBAL_ARGS = parse_args(extra_args_provider=extra_args_provider,   File ""/opt/conda/envs/as/lib/python3.8/sitepackages/megatron_lm1.1.5py3.8.egg/megatron/arguments.py"", line 190, in parse_args     fused_kernels.load_scaled_masked_softmax_fusion_kernel()   File ""/opt/conda/envs/as/lib/python3.8/sitepackages/megatron_lm1.1.5py3.8.egg/megatron/fused_kernels/__init__.py"", line 88, in load_scaled_masked_softmax_fusion_kernel     scaled_upper_triang_masked_softmax_cuda = cpp_extension.load(   File ""/opt/conda/envs/as/lib/python3.8/sitepackages/torch/utils/cpp_extension.py"", line 1079, in load     return _jit_compile(   File ""/opt/conda/envs/as/lib/python3.8/sitepackages/torch/utils/cpp_extension.py"", line 1262, in _jit_compile     version = JIT_EXTENSION_VERSIONER.bump_version_if_changed(   File ""/opt/conda/envs/as/lib/python3.8/sitepackages/torch/utils/_cpp_extension_versioner.py"", line 45, in bump_version_if_changed     hash_value = hash_source_files(hash_value, source_files)   File ""/opt/conda/envs/as/lib/python3.8/sitepackages/torch/utils/_cpp_extension_versioner.py"", line 15, in hash_source_files     with open(filename) as file: FileNotFoundError: [Errno 2] No such file or directory: '/opt/conda/envs/as/lib/python3.8/sitepackages/megatron_lm1.1.5py3.8.egg/megatron/fused_kernels/scaled_masked_softmax.cpp' ``` It seems like that the program can't find scaled_masked_softmax.cpp. I have the directory `/opt/conda/envs/as/lib/python3.8/sitepackages/megatron_lm1.1.5py3.8.egg/megatron/fused_kernels/`, but there are only `__init__.py  __pycache__  build`. I am not sure that I've setup Megatron well. I have run `python setup install` in the cloned repo folder. When checking with `conda list`, it shows `megatronlm               1.1.5                    pypi_0    pypi`.",2021-03-11T01:54:47Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/80,I face the same problem and wonder how you solved this problem,"> I face the same problem and wonder how you solved this problem Just as what I said in the original issue, I have ""installed"" the Megatron by `python setup install`. However, it actually doesn't work in this way. I shouldn't have install it. The right way is to run Megatron directly without installing. So at last I `pip uninstall megatronlm` and now it works well.",Thank you very much. It also works for me.
rhythmswing,How to Train on Multiple AWS Machines?,"Hi, Iâ€™m trying to apply this library for distributed training across multiple AWS instances, but I am not sure how to setup. Currently I can run Pretraining perfectly with the example script (â€œpretrain_bert_distributed.shâ€). I wonder how to train on more than one instances though? Iâ€™m trying to naively just changing the environment variables in the script. In the main instance I keep everything unchanged except that NNODES=2, and in the second instance I changed MASTER_ADDR to the local IP address of the main instance, NNODES=2, nad NODE_RANK=1. I tried to run both scripts on both instances, but they get stuck at â€œinitializing torch distributed ...â€.  Thanks in advance for any help! Rui ",2021-03-10T23:30:54Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/79,Fixed with correct network settings. ,"> Fixed with correct network settings. Hi, I have the same problem, could you post your script?"
huangjundashuaige,why rank 0 comsumes more gpu memory than other ranks within single machine,Where does the extra memory consumption come from? Or I just simply use it wrong?,2021-03-10T01:53:14Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/78,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
huangjundashuaige,why rank 0 comsumes more gpu memory than other ranks within single machine,Where does the extra memory consumption come from? Or I just simply use it wrong?,2021-03-10T01:51:25Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/77
ShivanshuPurohit,How to calculate FLOPS?,"I am trying to determine what is the total GPU utilization in training a GPTlike LM. As per this table !image A 1B model on a single A100 achieves ~150TFLOPs, while 8 achieve 109TFLOPs/GPU on an 8B model. What is the formula used to calculate FLOPs for LM?",2021-03-09T14:46:10Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/76,"In this paper : ""Efficient LargeScale Language Model Training on GPU Clusters"" has offered a formula, but I am not calculated it by hand, so I don't know whether this would fit your table.",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
lazykyama,Fix wrong function and module names,"This is a trivial fix. Some function/module names were not changed and several errors happen like `NameError: name 'get_model_parallel_rank' is not defined`. The change in `megatron/mpu/layers.py` is related to a runtime option, `usecpuinitialization`. Also, to run GPT evaluation (https://github.com/NVIDIA/MegatronLMwikitextperplexityevaluation), the changes in `tasks/zeroshot_gpt2/evaluate.py` are needed.",2021-03-09T08:48:10Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/75,Thanks for the PR. The changes have already been addressed.
casually-PYlearner,GLUE tasks for BERT,The GLUE tasks only  contain MNLI and QQP. Will the evaluation of other tasks be added later ï¼Ÿ,2021-03-08T02:33:52Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/74,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
ywd-pku,the issue of GPU utilize,"I meet a question, and hope could get the review.  While testing the BERTlarge's performance of Megatron in single  A100, it has lower gpu utilization rate, the average rate is about 65%. But the gpu utilization rate close to 98% if I test the model in single V100.",2021-03-02T09:14:08Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/73,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
jinze1994,Why use trainable position embedding instead of absolute sinusoid encoding?,"As the standard transformer in 'Attention is all your need' and Bert, the position embedding is the absolute sinusoid encoding for the language model. But the Megatron embedding uses trainable embedding. Is it right and may I ask why is that? https://github.com/NVIDIA/MegatronLM/blob/1b3dfa2ff9fe1643e15ddd1cf775abcdb2146f13/megatron/model/language_model.pyL141L146",2021-02-26T03:40:34Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/72
JinliangLu96,"RuntimeError: NCCL error in: ../torch/lib/c10d/ProcessGroupNCCL.cpp:839, unhandled system error, NCCL version 2.8.3","I am trying to pretrained gpt on two gpus, but I got the following error. ``` ***************************************** Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.  ***************************************** using world size: 2, dataparallelsize: 2, tensormodelparallel size: 1, pipelinemodelparallel size: 1  using torch.float16 for parameters ...  arguments    adam_beta1 ...................................... 0.9   adam_beta2 ...................................... 0.999   adam_eps ........................................ 1e08   adlr_autoresume ................................. False   adlr_autoresume_interval ........................ 1000   apply_query_key_layer_scaling ................... True   apply_residual_connection_post_layernorm ........ False   attention_dropout ............................... 0.1   attention_softmax_in_fp32 ....................... False   bert_load ....................................... None   bias_dropout_fusion ............................. True   bias_gelu_fusion ................................ True   block_data_path ................................. None   checkpoint_activations .......................... True   checkpoint_num_layers ........................... 1   clip_grad ....................................... 1.0   consumed_train_samples .......................... 0   consumed_valid_samples .......................... 0   data_impl ....................................... mmap   data_parallel_size .............................. 2   data_path ....................................... ['gptmegatron/chinesegpt_text_document']   DDP_impl ........................................ local   distribute_checkpointed_activations ............. False   distributed_backend ............................. nccl   eod_mask_loss ................................... False   eval_interval ................................... 1000   eval_iters ...................................... 10   exit_duration_in_mins ........................... None   exit_interval ................................... None   faiss_use_gpu ................................... False   finetune ........................................ False   fp16 ............................................ True   fp16_lm_cross_entropy ........................... False   fp32_allreduce .................................. False   fp32_residual_connection ........................ False   global_batch_size ............................... 64   hidden_dropout .................................. 0.1   hidden_size ..................................... 1024   hysteresis ...................................... 2   ict_head_size ................................... None   ict_load ........................................ None   indexer_batch_size .............................. 128   indexer_log_interval ............................ 1000   init_method_std ................................. 0.02   initial_loss_scale .............................. 4294967296   layernorm_epsilon ............................... 1e05   lazy_mpu_init ................................... None   load ............................................ gptmodels/   local_rank ...................................... 0   log_interval .................................... 100   loss_scale ...................................... None   loss_scale_window ............................... 1000   lr .............................................. 0.00015   lr_decay_iters .................................. 320000   lr_decay_samples ................................ None   lr_decay_style .................................. cosine   lr_warmup_fraction .............................. 0.01   lr_warmup_iters ................................. 0   lr_warmup_samples ............................... 0   make_vocab_size_divisible_by .................... 128   mask_prob ....................................... 0.15   max_position_embeddings ......................... 1024   merge_file ...................................... gpt2merges.txt   micro_batch_size ................................ 8   min_loss_scale .................................. 1.0   min_lr .......................................... 1e05   mmap_warmup ..................................... False   no_load_optim ................................... False   no_load_rng ..................................... False   no_save_optim ................................... False   no_save_rng ..................................... False   num_attention_heads ............................. 16   num_layers ...................................... 12   num_workers ..................................... 2   onnx_safe ....................................... None   openai_gelu ..................................... False   override_lr_scheduler ........................... False   params_dtype .................................... torch.float16   pipeline_model_parallel_size .................... 1   query_in_block_prob ............................. 0.1   rampup_batch_size ............................... None   rank ............................................ 0   report_topk_accuracies .......................... []   reset_attention_mask ............................ False   reset_position_ids .............................. False   save ............................................ gptmodels/   save_interval ................................... 10000   scaled_masked_softmax_fusion .................... True   scaled_upper_triang_masked_softmax_fusion ....... True   seed ............................................ 1234   seq_length ...................................... 1024   short_seq_prob .................................. 0.1   split ........................................... 949,50,1   tensor_model_parallel_size ...................... 1   tensorboard_dir ................................. None   titles_data_path ................................ None   tokenizer_type .................................. GPT2BPETokenizer   train_iters ..................................... 500000   train_samples ................................... None   use_checkpoint_lr_scheduler ..................... False   use_cpu_initialization .......................... False   use_one_sent_docs ............................... False   vocab_file ...................................... gpt2vocab.json   weight_decay .................................... 0.01   world_size ...................................... 2  end of arguments  setting number of microbatches to constant 4 > building GPT2BPETokenizer tokenizer ...  > padded vocab (size: 30000) with 80 dummy tokens (new size: 30080) > initializing torch distributed ... > initializing tensor model parallel with size 1 > initializing pipeline model parallel with size 1 > setting random seeds to 1234 ... > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234 time to initialize megatron (seconds): 44.130 [after megatron is initialized] datetime: 20210224 07:41:32  building GPT model ...  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 183007232 > learning rate decay style: cosine WARNING: could not find the metadata file gptmodels/latest_checkpointed_iteration.txt      will not load any checkpoints and will start from random time (ms) | load checkpoint: 0.21 [after model, optimizer, and learning rate scheduler are built] datetime: 20210224 07:41:32  > building train, validation, and test datasets ...  > datasets target sizes (minimum size):     train:      32000000     validation: 320640     test:       640 > building train, validation, and test datasets for GPT ...  > building dataset index ...     reading sizes...     reading pointers...     reading document index...     creating numpy buffer of mmap...     creating memory view of numpy buffer...  > finished creating indexed dataset in 0.000936 seconds     number of documents: 100000  > dataset split:     train:      document indices in [0, 94900) total of 94900 documents     validation:      document indices in [94900, 99900) total of 5000 documents     test:      document indices in [99900, 100000) total of 100 documents Traceback (most recent call last):   File ""pretrain_gpt.py"", line 146, in      pretrain(train_valid_test_datasets_provider, model_provider, forward_step,   File ""/workspace/MegatronLM/megatron/training.py"", line 115, in pretrain     = build_train_valid_test_data_iterators(   File ""/workspace/MegatronLM/megatron/training.py"", line 993, in build_train_valid_test_data_iterators     train_ds, valid_ds, test_ds = build_train_valid_test_datasets_provider(   File ""pretrain_gpt.py"", line 131, in train_valid_test_datasets_provider     train_ds, valid_ds, test_ds = build_train_valid_test_datasets(   File ""/workspace/MegatronLM/megatron/data/gpt_dataset.py"", line 38, in build_train_valid_test_datasets     return _build_train_valid_test_datasets(data_prefix[0],   File ""/workspace/MegatronLM/megatron/data/gpt_dataset.py"", line 116, in _build_train_valid_test_datasets     train_dataset = build_dataset(0, 'train')   File ""/workspace/MegatronLM/megatron/data/gpt_dataset.py"", line 110, in build_dataset     dataset = GPTDataset(name, data_prefix,   File ""/workspace/MegatronLM/megatron/data/gpt_dataset.py"", line 152, in __init__     self.doc_idx, self.sample_idx, self.shuffle_idx = _build_index_mappings(   File ""/workspace/MegatronLM/megatron/data/gpt_dataset.py"", line 300, in _build_index_mappings     torch.distributed.all_reduce(counts, group=mpu.get_data_parallel_group())   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/distributed_c10d.py"", line 1107, in all_reduce     work = group.allreduce([tensor], opts) RuntimeError: NCCL error in: ../torch/lib/c10d/ProcessGroupNCCL.cpp:839, unhandled system error, NCCL version 2.8.3 ncclSystemError: System call (socket, malloc, munmap, etc) failed. Traceback (most recent call last):   File ""pretrain_gpt.py"", line 146, in      pretrain(train_valid_test_datasets_provider, model_provider, forward_step,   File ""/workspace/MegatronLM/megatron/training.py"", line 115, in pretrain     = build_train_valid_test_data_iterators(   File ""/workspace/MegatronLM/megatron/training.py"", line 993, in build_train_valid_test_data_iterators     train_ds, valid_ds, test_ds = build_train_valid_test_datasets_provider(   File ""pretrain_gpt.py"", line 131, in train_valid_test_datasets_provider     train_ds, valid_ds, test_ds = build_train_valid_test_datasets(   File ""/workspace/MegatronLM/megatron/data/gpt_dataset.py"", line 38, in build_train_valid_test_datasets     return _build_train_valid_test_datasets(data_prefix[0],   File ""/workspace/MegatronLM/megatron/data/gpt_dataset.py"", line 116, in _build_train_valid_test_datasets     train_dataset = build_dataset(0, 'train')   File ""/workspace/MegatronLM/megatron/data/gpt_dataset.py"", line 110, in build_dataset     dataset = GPTDataset(name, data_prefix,   File ""/workspace/MegatronLM/megatron/data/gpt_dataset.py"", line 152, in __init__     self.doc_idx, self.sample_idx, self.shuffle_idx = _build_index_mappings(   File ""/workspace/MegatronLM/megatron/data/gpt_dataset.py"", line 300, in _build_index_mappings     torch.distributed.all_reduce(counts, group=mpu.get_data_parallel_group())   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/distributed_c10d.py"", line 1107, in all_reduce     work = group.allreduce([tensor], opts) RuntimeError: NCCL error in: ../torch/lib/c10d/ProcessGroupNCCL.cpp:839, unhandled system error, NCCL version 2.8.3 ncclSystemError: System call (socket, malloc, munmap, etc) failed. Traceback (most recent call last):   File ""/opt/conda/lib/python3.8/runpy.py"", line 194, in _run_module_as_main     return _run_code(code, main_globals, None,   File ""/opt/conda/lib/python3.8/runpy.py"", line 87, in _run_code     exec(code, run_globals)   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/launch.py"", line 303, in      main()   File ""/opt/conda/lib/python3.8/sitepackages/torch/distributed/launch.py"", line 294, in main     raise subprocess.CalledProcessError(returncode=process.returncode, subprocess.CalledProcessError: Command '['/opt/conda/bin/python', 'u', 'pretrain_gpt.py', 'local_rank=1', 'numlayers', '12', 'hiddensize', '1024', 'numattentionheads', '16', 'microbatchsize', '8', 'globalbatchsize', '64', 'seqlength', '1024', 'maxpositionembeddings', '1024', 'trainiters', '500000', 'lrdecayiters', '320000', 'save', 'gptmodels/', 'load', 'gptmodels/', 'datapath', 'gptmegatron/chinesegpt_text_document', 'vocabfile', 'gpt2vocab.json', 'mergefile', 'gpt2merges.txt', 'dataimpl', 'mmap', 'split', '949,50,1', 'distributedbackend', 'nccl', 'lr', '0.00015', 'lrdecaystyle', 'cosine', 'minlr', '1.0e5', 'weightdecay', '1e2', 'clipgrad', '1.0', 'lrwarmupfraction', '.01', 'checkpointactivations', 'loginterval', '100', 'saveinterval', '10000', 'evalinterval', '1000', 'evaliters', '10', 'fp16']' returned nonzero exit status 1. ``` I use the NGC's recent PyTorch containers version 20.12, and the script was running in the docker container. So, I don't know why the nccl error occurs. When I used 1 gpu, it works well. But running multigpus will fails. I don't know how to solve it. Can someone help me? Thank you.",2021-02-24T07:51:53Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/71,I have fixed the problem. It caused by the size of 'dev/shm' in the docker container. I used the following command to increase the size. And it works well. ``` docker run it gpus all shmsize=1024m xxxx /bin/bash ```
abestern,"pretrain_bert.py stuck at ""building index mapping for train ...""","I'm trying to pretrain BERT on a small text corpus. I've put 239487 sentences into loose json: ``` {""text"": ""BBC News also known as the BBC News Channel is a British freetoair television news channel""} {""text"": ""It was launched as BBC News  on  November  at pm as part of the BBCs foray into digital domestic television channels becoming the first competitor to Sky News which had been running since ""} {""text"": ""For a time looped news sport and weather bulletins were available to view via BBC Red Button""} {""text"": ""On  February  the channel was named News Channel of the Year at the Royal Television Society Television Journalism Awards for the first time in its history""} ``` I've then preprocessed the json following the template in README.md: ``` python /MegatronLM/tools/preprocess_data.py \ 	input tmp.json \ 	outputprefix mybert \ 	vocab bertlargecasedvocab.txt \ 	datasetimpl mmap \ 	tokenizertype BertWordPieceLowerCase \ 	splitsentences ``` I then kickoff the pretraining: ``` !/bin/bash RANK=0  WORLD_SIZE=1  python /MegatronLM/pretrain_bert.py \ 	numlayers 12 \ 	hiddensize 1024 \ 	numattentionheads 8 \ 	microbatchsize 4 \ 	globalbatchsize 8 \ 	seqlength 512 \ 	maxpositionembeddings 512 \ 	trainiters 1000 \ 	datapath /data/languagedata/enwikifebdocsplit/mybert_text_sentence0001 \ 	vocabfile /data/languagedata/bertlargecasedvocab.txt \ 	dataimpl mmap \ 	split 949,50,1 \ 	lr 0.0001 \ 	minlr 0.00001 \ 	lrdecaystyle linear \ 	lrwarmupfraction .01 \ 	weightdecay 1e2 \ 	clipgrad 1.0 \ 	loginterval 1 \ 	saveinterval 1 \ 	evalinterval 1 \ 	evaliters 1 \ 	fp16 ``` Execution appears to get stuck here: ```  > building sapmles index mapping for train ...     using uint32 for data mapping...     using:      number of documents:            227274      sentences range:                0, 225091)      total number of sentences:      225091      number of epochs:               2147483646      maximum number of samples:      8000      maximum sequence length:        509      short sequence probability:     0.1      short sequence ration (1/prob): 10      seed:                           1234 ``` Given the small dataset, I'm suspicious that there is a problem. I believe that number of epochs is being set to  `np.iinfo(np.int32).max  1` [here which may be leading to unexpected behavior.  ",2021-02-13T16:06:01Z,stale,closed,0,7,https://github.com/NVIDIA/Megatron-LM/issues/70,"hi, did you solve this problem? I met this problem too",Met this problem too,Did you solve this problem? I met this problem too.,Hi  did you solve this problem?,Anyone has solved this problem? This case is same to mine!,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
JinliangLu96,Can Megatron-LM support training over â€œshardedâ€ datasets?,"I want to ask whether MegatronLM supports training on multiple small datasets by order? I have a big dataset, however, preprocessing costs a lot of time. So, I split it into pieces and preprocess them separately. But I don't know how to train the model on the small datasets by order. Can someone know how to solve this? thanks!",2021-02-04T09:16:04Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/69
mosheber,fix sapmles typo,,2021-02-02T23:00:34Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/68,Thanks for the PR. Have already addressed this and will be included in the next release. 
pritamdamania87,Error running bert pretraining example,"I'm using the following data for mycorpus.json (as demonstrated in the README): ``` {""src"": ""www.nvidia.com"", ""text"": ""The quick brown fox"", ""type"": ""Eng"", ""id"": ""0"", ""title"": ""First Part""} {""src"": ""The Internet"", ""text"": ""jumps over the lazy dog"", ""type"": ""Eng"", ""id"": ""42"", ""title"": ""Second Part""} ``` preprocessing the data as follows: ``` python tools/preprocess_data.py \        input mycorpus.json \        outputprefix mybert \        vocab bertvocab.txt \        datasetimpl mmap \        tokenizertype BertWordPieceLowerCase \        splitsentences ``` And running the training script as follows (again from README): ``` CHECKPOINT_PATH=checkpoints/bert_345m VOCAB_FILE=bertvocab.txt DATA_PATH=mybert_text_sentence BERT_ARGS=""numlayers 24 \            hiddensize 1024 \            numattentionheads 16 \            seqlength 512 \            maxpositionembeddings 512 \            lr 0.0001 \            lrdecayiters 990000 \            trainiters 2000000 \            minlr 0.00001 \            lrwarmupfraction 0.01 \ 	   microbatchsize 4 \	               globalbatchsize 8 \            vocabfile $VOCAB_FILE \            split 949,50,1 \            fp16"" OUTPUT_ARGS=""loginterval 10 \              saveinterval 500 \              evalinterval 100 \              evaliters 10 \              checkpointactivations"" python pretrain_bert.py \        $BERT_ARGS \        $OUTPUT_ARGS \        save $CHECKPOINT_PATH \        load $CHECKPOINT_PATH \        datapath $DATA_PATH ``` Although, I run into the following error during training: ``` Traceback (most recent call last):   File ""pretrain_bert.py"", line 155, in      args_defaults={'tokenizer_type': 'BertWordPieceLowerCase'})   File ""/data/users/pritam/MegatronLM/megatron/training.py"", line 116, in pretrain     train_valid_test_dataset_provider)   File ""/data/users/pritam/MegatronLM/megatron/training.py"", line 1000, in build_train_valid_test_data_iterators     valid_ds, args.consumed_valid_samples)   File ""/data/users/pritam/MegatronLM/megatron/data/data_loaders.py"", line 38, in build_pretraining_data_loader     data_parallel_size=mpu.get_data_parallel_world_size())   File ""/data/users/pritam/MegatronLM/megatron/data/data_loaders.py"", line 62, in __init__     'no sample to consume: {}'.format(self.total_samples) AssertionError: no sample to consume: 0 ``` Full log: https://gist.github.com/pritamdamania87/7141eadd162ba672b465a7920e62508e",2021-02-02T07:10:09Z,stale,closed,1,4,https://github.com/NVIDIA/Megatron-LM/issues/67,"With only two sentences in your data set there isn't enough for a train/valid/test set, so you need to add more data or change the split argument to something like `99,1,0` or something? You'll want to look at the "" > dataset split:"" part of the output, make sure there is data in the training and eval set.","> With only two sentences in your data set there isn't enough for a train/valid/test set, so you need to add more data or change the split argument to something like `99,1,0` or something? You'll want to look at the "" > dataset split:"" part of the output, make sure there is data in the training and eval set. I tried with a dataset with ~900 samples, but still encountered the error. Here's my output ``` dataset split:     train:      document indices in [0, 726) total of 726 documents      sentence indices in [0, 726) total of 726 sentences     validation:      document indices in [726, 817) total of 91 documents      sentence indices in [726, 817) total of 91 sentences     test:      document indices in [817, 908) total of 91 documents      sentence indices in [817, 908) total of 91 sentences  loading indexed mapping from mybert_text_sentence_train_indexmap_160mns_509msl_0.10ssp_1234s.npy     loaded indexed file in 0.001 seconds     total number of samples: 0   loading indexed mapping from mybert_text_sentence_valid_indexmap_240mns_509msl_0.10ssp_1234s.npy     loaded indexed file in 0.001 seconds     total number of samples: 0   loading indexed mapping from mybert_text_sentence_test_indexmap_80mns_509msl_0.10ssp_1234s.npy     loaded indexed file in 0.000 seconds     total number of samples: 0 finished creating BERT datasets ... ```",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
dnola,KeyError: running GPT text generation sample,"Hello, Running in the __20.12 PyTorch NGC container on V100__, when I try to run examples/generate_text.sh I get a series of errors... 1. It can't find tools/generate_samples_gpt2.py If I change the name of tools/generate_samples_gpt.py to tools/generate_samples_gpt2.py it proceeds for a bit until... 2. It doesn't like the batch size parameter: `AssertionError: batchsize argument is no longer valid, use microbatchsize instead ` If I change the argument name in generate_text.sh I can get it to proceed more until... 3. I get a KeyError: ` > building GPT2BPETokenizer tokenizer ...  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304) > initializing torch distributed ... > initializing tensor model parallel with size 1 > initializing pipeline model parallel with size 1 > setting random seeds to 1234 ... > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234 building GPT model ...  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 354871296 WARNING: could not find the metadata file checkpoints/gpt2_345m/latest_checkpointed_iteration.txt      will not load any checkpoints and will start from random Avg s/batch: 47.92085814476013 Traceback (most recent call last):   File ""tools/generate_samples_gpt2.py"", line 116, in      main()   File ""tools/generate_samples_gpt2.py"", line 111, in main     generate_and_write_samples_unconditional(model)   File ""/workspace/megatron/text_generation_utils.py"", line 339, in generate_and_write_samples_unconditional     for datum in generate_samples_unconditional(model):   File ""/workspace/megatron/text_generation_utils.py"", line 317, in generate_samples_unconditional     text = tokenizer.detokenize(tokens)   File ""/workspace/megatron/tokenizer/tokenizer.py"", line 216, in detokenize     return self.tokenizer.decode(token_ids)   File ""/workspace/megatron/tokenizer/gpt2_tokenization.py"", line 284, in decode     text = ''.join([self.decoder[token] for token in tokens])   File ""/workspace/megatron/tokenizer/gpt2_tokenization.py"", line 284, in      text = ''.join([self.decoder[token] for token in tokens]) KeyError: 50280 ` Any advice on how to proceed? Were the changes I made appropriate? Thank you!",2021-02-02T03:55:15Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/66,"Yes, that change to microbatchsize is correct, we need to update the example script. The problem you ran into is you need to give it a valid checkpoint. Without a checkpoint to load it'll generate random tokens, one of which is not a valid token.",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
seungickjang,A loaded model seems not inference properly,"Hello, I finetuned a pretrained BERT with RACE data and tried to inference.  While finetuning the BERT, the accuracy rate was about 60~70% with the valid data. But during the inference, the accuracy rate on the valid data was only about 25% which is almost random. When I saved/loaded a model, I used 'save_checkpoint' and 'load_checkpoint' functions in the megatron/checkingpoint.py, and called model.eval() for evaluation as well. Also, I referenced ""tasks/zeroshot_gpt2/evaluate.py"" and modified for my own purpose. Is it a bug? Or it would be good if there's any example code or documentation regarding this issue. Regards,",2021-01-29T00:40:03Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/65,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
bugface,Apex dependency,"In the requirements.txt, there is no info about apex. But when we run the Megatron, it required Apex compiled with cpp extension to be installed.  Could you update the readme to include apex installation/requirement information?",2021-01-28T15:59:41Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/64, CC(No module named 'apex') CC(GPT2 generation samples error )
wade3han,Unintended error caused by compiling fused_kernels,"https://github.com/NVIDIA/MegatronLM/blob/main/megatron/arguments.pyL186L198 https://github.com/NVIDIA/MegatronLM/blob/main/megatron/fused_kernels/__init__.pyL46L72 When I tried to train GPT3 on multinode using `torch.distributed.launch`, sometimes the training process was stuck while compiling the fused_kernels. This bug can be occurred by timing issue when multiple processes compile concurrently. The simplest way is to remove `./fused_kernels/build` and run script again, but I thought it is not solving the fundamental problem. In my case, I resolved this issue can be solved by using `torch.distributed.barrier`, letting the process compile the `fused_kernels` only on master rank (rank == 0). If the authors think resolving this issue is necessary for the codes, then I will leave PR :)",2021-01-26T10:01:02Z,,closed,1,4,https://github.com/NVIDIA/Megatron-LM/issues/63,"We've had reports of this happening internally as well. The problem with only compiling on the master rank is we need to support the use case that the code lives locally on several nodes (i.e. a multinode environment and the code isn't on a shared disk). So the kernels need to be compiled on several ranks.  `torch.utils.cpp_extension.load` is supposed to lock the directory so that only one rank runs the build at a time (and only the first run actually does anything, subsequent runs just see that it is already built and returns). Apparently that lock isn't properly deleted/released in some instances.",Thanks for your response! I'm glad that your group already had reports of this happening. Is your group internally planning to resolve this issue?,"It's on the list but pretty low priority since we seldom run into it. If you can figure out why the lock mechanism in `torch.utils.cpp_extension.load` doesn't always work as I think it should I'm all ears. :) Might be we are expecting more from it than it is meant to handle, might be we are using it wrong, might be a bug in torch, etc.","I will close this issue, and if there is an additional issue then I will reopen it. Thank you :)"
nonatofabio,Data Preprocess fails if first document in merged json is empty,"When running the preprocess_data.py script as described on the documentation, if the first processed entry has an empty text this line fails with an `IndexError: list index out of range` because `len(doc_ids)==0` Adding a check for `doc_ids` length on this statement can correct the problem:  ```python if self.args.append_eod and len(doc_ids) > 0:    doc_ids[1].append(Encoder.tokenizer.eod) ```",2021-01-12T01:28:30Z,stale,closed,1,3,https://github.com/NVIDIA/Megatron-LM/issues/62,Just encountered the same error.   Could you create a pull request?,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
RezaYazdaniAminabadi,RuntimeError when running Megatorn-LM with recompute flag turned off,"Hi Megatron team, I am trying to evaluate the MegatronLM new implementation for the languagemodeling tasks and I was running the example script you provided here. I also want to use the GPT2 modeling for running the inference on a different model for which I need to turn off the recompute flag.  However, after removing this flag frim the example script, I ran into a runtime error, due to wrong matrix size.  Here is the tracelog of my test: Traceback (most recent call last):   File ""tools/generate_samples_gpt2.py"", line 127, in      main()   File ""tools/generate_samples_gpt2.py"", line 122, in main     generate_and_write_samples_unconditional(model)   File ""/home/reyazda/.local/lib/python3.6/sitepackages/megatron/text_generation_utils.py"", line 275, in generate_and_write_samples_unconditional     for datum in generate_samples_unconditional(model):   File ""/home/reyazda/.local/lib/python3.6/sitepackages/megatron/text_generation_utils.py"", line 248, in generate_samples_unconditional     copy.deepcopy(context_tokens)):   File ""/home/reyazda/.local/lib/python3.6/sitepackages/megatron/text_generation_utils.py"", line 314, in get_token_stream     for tokens, lengths in batch_token_iterator:   File ""/home/reyazda/.local/lib/python3.6/sitepackages/megatron/text_generation_utils.py"", line 381, in sample_sequence_batch     forward_method_parallel_output=False)   File ""/home/reyazda/.local/lib/python3.6/sitepackages/torch/nn/modules/module.py"", line 722, in _call_impl     result = self.forward(*input, **kwargs)   File ""/home/reyazda/.local/lib/python3.6/sitepackages/megatron/model/distributed.py"", line 76, in forward     return self.module(*inputs, **kwargs)   File ""/home/reyazda/.local/lib/python3.6/sitepackages/torch/nn/modules/module.py"", line 722, in _call_impl     result = self.forward(*input, **kwargs)   File ""/home/reyazda/.local/lib/python3.6/sitepackages/megatron/fp16/fp16.py"", line 74, in forward     return fp16_to_fp32(self.module(*(fp32_to_fp16(inputs)), **kwargs))   File ""/home/reyazda/.local/lib/python3.6/sitepackages/torch/nn/modules/module.py"", line 722, in _call_impl     result = self.forward(*input, **kwargs)   File ""/home/reyazda/.local/lib/python3.6/sitepackages/megatron/model/gpt2_model.py"", line 63, in forward     get_key_value=get_key_value)   File ""/home/reyazda/.local/lib/python3.6/sitepackages/torch/nn/modules/module.py"", line 722, in _call_impl     result = self.forward(*input, **kwargs)   File ""/home/reyazda/.local/lib/python3.6/sitepackages/megatron/model/language_model.py"", line 309, in forward     get_key_value=get_key_value)   File ""/home/reyazda/.local/lib/python3.6/sitepackages/torch/nn/modules/module.py"", line 722, in _call_impl     result = self.forward(*input, **kwargs)   File ""/home/reyazda/.local/lib/python3.6/sitepackages/megatron/model/transformer.py"", line 584, in forward     get_key_value=get_key_value)   File ""/home/reyazda/.local/lib/python3.6/sitepackages/torch/nn/modules/module.py"", line 722, in _call_impl     result = self.forward(*input, **kwargs)   File ""/home/reyazda/.local/lib/python3.6/sitepackages/megatron/model/transformer.py"", line 422, in forward     get_key_value=get_key_value)   File ""/home/reyazda/.local/lib/python3.6/sitepackages/torch/nn/modules/module.py"", line 722, in _call_impl     result = self.forward(*input, **kwargs)   File ""/home/reyazda/.local/lib/python3.6/sitepackages/megatron/model/transformer.py"", line 322, in forward     context_layer = torch.bmm(attention_probs, value_layer.transpose(0,1)) RuntimeError: invalid argument 6: wrong matrix size at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:84 Can someone please tell me the reason for this behaviour and whether this is expected? Thanks. Reza",2021-01-07T22:24:27Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/61,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
Hanlard,OOM when training the same size of gpt2(2.6B) with mp=2 dp=8 with 64*V100(32GB),"**We train gpt2(2.6B) with following parameters ï¼Œbut OOM** using world size: 16 and modelparallel size: 2  using torch.float16 for parameters ...  arguments    adam_beta1 ...................... 0.9   adam_beta2 ...................... 0.999   adam_eps ........................ 1e08   adlr_autoresume ................. False   adlr_autoresume_interval ........ 1000   apply_query_key_layer_scaling ... False   apply_residual_connection_post_layernorm  False   attention_dropout ............... 0.1   attention_softmax_in_fp32 ....... False   batch_size ...................... 8   bert_load ....................... None   bias_dropout_fusion ............. False   bias_gelu_fusion ................ False   block_data_path ................. None   checkpoint_activations .......... True   checkpoint_num_layers ........... 1   clip_grad ....................... 1.0   data_impl ....................... mmap   data_path ....................... /raid/gpt3traindata/filterBy256100Gnotag_text_document   DDP_impl ........................ local   distribute_checkpointed_activations  True   distributed_backend ............. nccl   dynamic_loss_scale .............. True   eod_mask_loss ................... False   eval_interval ................... 10000   eval_iters ...................... 10   exit_interval ................... None   faiss_use_gpu ................... False   finetune ........................ False   fp16 ............................ True   fp16_lm_cross_entropy ........... True   fp32_allreduce .................. False   hidden_dropout .................. 0.1   hidden_size ..................... 1920   hysteresis ...................... 2   ict_head_size ................... None   ict_load ........................ None   indexer_batch_size .............. 128   indexer_log_interval ............ 1000   init_method_std ................. 0.02   layernorm_epsilon ............... 1e05   lazy_mpu_init ................... None   load ............................ checkpoints/gpt2_64_xxxM   local_rank ...................... 0   log_interval .................... 5   loss_scale ...................... None   loss_scale_window ............... 1000   lr .............................. 0.00015   lr_decay_iters .................. 70000   lr_decay_style .................. cosine   make_vocab_size_divisible_by .... 128   mask_prob ....................... 0.15   max_position_embeddings ......... 1024   merge_file ...................... bpe_3w_new/merges.txt   min_lr .......................... 1e05   min_scale ....................... 1   mmap_warmup ..................... False   model_parallel_size ............. 2   no_load_optim ................... False   no_load_rng ..................... False   no_save_optim ................... False   no_save_rng ..................... False   num_attention_heads ............. 20   num_layers ...................... 54   num_unique_layers ............... None   num_workers ..................... 2   onnx_safe ....................... None   openai_gelu ..................... False   override_lr_scheduler ........... False   param_sharing_style ............. grouped   params_dtype .................... torch.float16   query_in_block_prob ............. 0.1   rank ............................ 0   report_topk_accuracies .......... []   reset_attention_mask ............ False   reset_position_ids .............. False   save ............................ checkpoints/gpt2_64_xxxM   save_interval ................... 10000   scaled_masked_softmax_fusion .... False   scaled_upper_triang_masked_softmax_fusion  False   seed ............................ 1234   seq_length ...................... 1024   short_seq_prob .................. 0.1   split ........................... 950,49,1   tensorboard_dir ................. logs/gpt2_64_xxxxxxxxx   titles_data_path ................ None   tokenizer_type .................. GPT2BPETokenizer   train_iters ..................... 120000   use_checkpoint_lr_scheduler ..... False   use_cpu_initialization .......... True   use_one_sent_docs ............... False   vocab_file ...................... bpe_3w_new/vocab.json   warmup .......................... 0.01   weight_decay .................... 0.01   world_size ...................... 16  end of arguments  > building GPT2BPETokenizer tokenizer ...  > padded vocab (size: 30001) with 207 dummy tokens (new size: 30208) > setting tensorboard ... > initializing torch distributed ... > initializing model parallel with size 2 > setting random seeds to 1234 ... > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234 > building the checkpointed activations memory buffer with 424673280 num elements and torch.float16 dtype (810.0 MB)... building GPT2 model ...  > number of parameters on model parallel rank 1: 1226348160  > number of parameters on model parallel rank 0: 1226348160 > learning rate decay style: cosine WARNING: could not find the metadata file checkpoints/gpt2_64_xxxM/latest_checkpointed_iteration.txt      will not load any checkpoints and will start from random > building train, validation, and test datasets ...  > datasets target sizes (minimum size):     train:      7680000     validation: 8320     test:       640 > building train, validation, and test datasets for GPT2 ...  > building dataset index ...     reading sizes...     reading pointers...     reading document index...     creating numpy buffer of mmap...     creating memory view of numpy buffer...  > finished creating indexed dataset in 0.021106 seconds     number of documents: 48889798  > dataset split:     train:      document indices in [0, 46445308) total of 46445308 documents     validation:      document indices in [46445308, 48840908) total of 2395600 documents     test:      document indices in [48840908, 48889798) total of 48890 documents _________________________________________________________________________________ ... _________________________________________________________________________________ setting training data start iteration to 0 setting validation data start iteration to 0 done with setups ... time (ms)  batch generator: 1.42 Traceback (most recent call last):   File ""pretrain_gpt2.py"", line 115, in      args_defaults={'tokenizer_type': 'GPT2BPETokenizer'})   File ""/userhome/megatron/megatron/training.py"", line 109, in pretrain     train_data_iterator, valid_data_iterator)   File ""/userhome/megatron/megatron/training.py"", line 438, in train     lr_scheduler)   File ""/userhome/megatron/megatron/training.py"", line 300, in train_step     backward_step(optimizer, model, loss)   File ""/userhome/megatron/megatron/training.py"", line 265, in backward_step     fp32_allreduce=args.fp32_allreduce)   File ""/userhome/megatron/megatron/model/distributed.py"", line 53, in allreduce_params     coalesced = _flatten_dense_tensors(grads)   File ""/opt/conda/lib/python3.6/sitepackages/torch/_utils.py"", line 229, in _flatten_dense_tensors     flat = torch.cat([t.contiguous().view(1) for t in tensors], dim=0) RuntimeError: CUDA out of memory. Tried to allocate 2.29 GiB (GPU 0; 31.72 GiB total capacity; 19.22 GiB already allocated; 1.88 GiB free; 28.32 GiB reserved in total by PyTorch) Traceback (most recent call last):   File ""/opt/conda/lib/python3.6/runpy.py"", line 193, in _run_module_as_main     ""__main__"", mod_spec)   File ""/opt/conda/lib/python3.6/runpy.py"", line 85, in _run_code     exec(code, run_globals)   File ""/opt/conda/lib/python3.6/sitepackages/torch/distributed/launch.py"", line 263, in      main()   File ""/opt/conda/lib/python3.6/sitepackages/torch/distributed/launch.py"", line 259, in main     cmd=cmd) subprocess.CalledProcessError: Command '['/opt/conda/bin/python', 'u', 'pretrain_gpt2.py', 'local_rank=15', 'modelparallelsize', '2', 'numlayers', '54', 'hiddensize', '1920', 'numattentionheads', '20', 'batchsize', '8', 'seqlength', '1024', 'maxpositionembeddings', '1024', 'trainiters', '120000', 'lrdecayiters', '70000', 'save', 'checkpoints/gpt2_64_xxxM', 'load', 'checkpoints/gpt2_64_xxxM', 'datapath', '/raid/gpt3traindata/filterBy256100Gnotag_text_document', 'vocabfile', 'bpe_3w_new/vocab.json', 'mergefile', 'bpe_3w_new/merges.txt', 'dataimpl', 'mmap', 'split', '950,49,1', 'distributedbackend', 'nccl', 'lr', '0.00015', 'lrdecaystyle', 'cosine', 'minlr', '1.0e5', 'weightdecay', '1e2', 'clipgrad', '1.0', 'warmup', '.01', 'checkpointactivations', 'loginterval', '5', 'tensorboarddir', 'logs/gpt2_64_xxxxxxxxx', 'saveinterval', '10000', 'evalinterval', '10000', 'evaliters', '10', 'checkpointnumlayers', '1', 'fp16', 'checkpointactivations', 'distributecheckpointedactivations', 'fp16lmcrossentropy', 'usecpuinitialization']' returned nonzero exit status 1. ***************************************** Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.  *****************************************",2021-01-05T03:53:57Z,stale,closed,0,10,https://github.com/NVIDIA/Megatron-LM/issues/60,This is 16 * V100(32G) for debug...,"I find that if add ""torch.cuda.empty_cahce()"" at every train_step then paragram will not OOM ... ",I have used following techniches to save memory...        fp16  \        checkpointactivations  \        distributecheckpointedactivations  \        fp16lmcrossentropy  \        usecpuinitialization  ,"Did you intentionally choose MP=2? Would increasing MP help? (e.g. MP=4, DP=4)","> Did you intentionally choose MP=2? Would increasing MP help? (e.g. MP=4, DP=4) increasing MP will help...","> Did you intentionally choose MP=2? Would increasing MP help? (e.g. MP=4, DP=4) I solve the problem by changing DDP_impl = â€œlocalâ€  to â€torchâ€ ... ","In the readme it says that ""2.5 billion parameters using 2way model parallel and 1.2 billion parameters with no model parallel"".  It's confusing that splitting the model into 2 shards takes more GPU memory. Assuming that the original model size as M, after 2way model parallel each GPU should hold M/2.  Does Anyone know what other GPU footprint comes from with setting DDP_impl â€œlocalâ€ and â€torchâ€ respectively? Dose the communication library pin M or more GPU memory? Thanks.","I am getting out of memory errors with mp>1, which all disappear if I just use `mp=1`, with the deepspeed megatron enabled.",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
yselivonchyk,Data preprocessing Readme instructions fail,"I put example data into a ./data/data.json file: ```{""src"": ""www.nvidia.com"", ""text"": ""The quick brown fox"", ""type"": ""Eng"", ""id"": ""0"", ""title"": ""First Part""} {""src"": ""The Internet"", ""text"": ""jumps over the lazy dog"", ""type"": ""Eng"", ""id"": ""42"", ""title"": ""Second Part""} ``` And run suggested command: ``` ```python tools/preprocess_data.py \        input ./data/data.json \        outputprefix wtever \        vocab bertvocab.txt \        datasetimpl mmap \        tokenizertype BertWordPieceLowerCase \        splitsentences``` ``` Which first results in: ```NameError: name 'nltk' is not defined``` After installing nltk, running the same script results in: ``` FileNotFoundError: [Errno 2] No such file or directory: 'bertvocab.txt' ``` Is there a description of vocab.txt format and content? Can you provide an example vocab file for the example data?",2020-12-19T17:58:01Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/59,The necessary BERT vocab file should look like:  ```[CLS] [SEP] [PAD] [MASK] [UNK] The quick brown fox jumps over lazy dog ```,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,The vocab files used for bert and GPT training are linked here: https://github.com/NVIDIA/MegatronLMdownloadingcheckpoints
Line290,Save checkpoint error with model parallel size > 1,"It seems a bug in the save_checkpoint function,. Assuming a data parallel rank with two model parallel ranks noted as M1 and M2. Both M1 and M2 finished checking there is no folder, and then M1 create a new one, but M2 also want to create this folder, it will raise FileExistsError.",2020-11-27T06:26:23Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/58,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
gcooper-isi,merge_mp_partitions.py fails with an exception,"When I run tools/merge_mp_partitions.py, it fails with an exception: ``` Traceback (most recent call last):   File ""merge_mp_partitions.py"", line 286, in      main()   File ""merge_mp_partitions.py"", line 212, in main     merged_model = get_model(model_type)   File ""merge_mp_partitions.py"", line 125, in get_model     model = model_provider()   File ""/data/gcooper/nlgevaluation/MegatronLM/pretrain_gpt2.py"", line 35, in model_provider     model = GPT2Model(num_tokentypes=0, parallel_output=True)   File ""/data/gcooper/nlgevaluation/MegatronLM/megatron/model/gpt2_model.py"", line 51, in __init__     args.num_layers))   File ""/data/gcooper/nlgevaluation/MegatronLM/megatron/model/language_model.py"", line 62, in get_language_model     add_pooler=add_pooler)   File ""/data/gcooper/nlgevaluation/MegatronLM/megatron/model/language_model.py"", line 283, in __init__     self.num_tokentypes)   File ""/data/gcooper/nlgevaluation/MegatronLM/megatron/model/language_model.py"", line 123, in __init__     vocab_size, self.hidden_size, init_method=self.init_method)   File ""/data/gcooper/nlgevaluation/MegatronLM/megatron/mpu/layers.py"", line 145, in __init__     partition_dim=0, stride=1)   File ""/data/gcooper/nlgevaluation/MegatronLM/megatron/mpu/layers.py"", line 58, in _initialize_affine_weight_gpu     with get_cuda_rng_tracker().fork():   File ""/opt/conda/lib/python3.6/contextlib.py"", line 81, in __enter__     return next(self.gen)   File ""/data/gcooper/nlgevaluation/MegatronLM/megatron/mpu/random.py"", line 183, in fork     raise Exception('cuda rng state {} is not added'.format(name)) Exception: cuda rng state modelparallelrng is not added ``` When training, the RNG state gets set in initialize_megatron(), but that is not called in this case.",2020-11-20T22:36:00Z,stale,closed,1,10,https://github.com/NVIDIA/Megatron-LM/issues/57,"Hello, has anybody solved this problem? Is there a workaround? Thanks.",Same problem here,"I have a similar problems. > Traceback (most recent call last):   File ""/data/liuguang/Sailing/tests/test_trainer_deepspeed.py"", line 193, in      print(model(**batch))   File ""/opt/conda/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 889, in _call_impl     result = self.forward(*input, **kwargs)   File ""/opt/conda/lib/python3.8/sitepackages/deepspeed/runtime/engine.py"", line 1589, in forward     loss = self.module(*inputs, **kwargs)   File ""/opt/conda/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 889, in _call_impl     result = self.forward(*input, **kwargs)   File ""/data/liuguang/Sailing/easybigmodel/model/glm_model.py"", line 305, in forward     model_out= self.model(input_ids, position_ids, attention_mask)   File ""/opt/conda/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 889, in _call_impl     result = self.forward(*input, **kwargs)   File ""/data/liuguang/Sailing/easybigmodel/model/glm_model_mpu.py"", line 122, in forward     transformer_output = self.transformer(embeddings, position_ids, attention_mask, mems,   File ""/opt/conda/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 889, in _call_impl     result = self.forward(*input, **kwargs)   File ""/data/liuguang/Sailing/easybigmodel/model/blocks/transformer_mpu.py"", line 655, in forward     hidden_states = layer(*args, mem=mem_i)   File ""/opt/conda/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 889, in _call_impl     result = self.forward(*input, **kwargs)   File ""/data/liuguang/Sailing/easybigmodel/model/blocks/transformer_mpu.py"", line 402, in forward     attention_output = self.attention(layernorm_output, ltor_mask, position_embeddings, r_w_bias, r_r_bias, mem)   File ""/opt/conda/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 889, in _call_impl     result = self.forward(*input, **kwargs)   File ""/data/liuguang/Sailing/easybigmodel/model/layers/attentions_mpu.py"", line 394, in forward     with get_cuda_rng_tracker().fork():   File ""/opt/conda/lib/python3.8/contextlib.py"", line 113, in __enter__     return next(self.gen)   File ""/opt/conda/lib/python3.8/sitepackages/deepspeed/runtime/activation_checkpointing/checkpointing.py"", line 174, in fork     raise Exception('cuda rng state {} is not added'.format(name)) Exception: cuda rng state modelparallelrng is not added Besides, whatâ€˜s `with get_cuda_rng_tracker().fork():` doingï¼Ÿ",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.,I got the same problem. Have you solved it ?,Marking as stale. No activity in 60 days.,"It is caused by not initializing rng state. The code belows would work ```python import torch import torch.distributed as dist from megatron.core import mpu, tensor_parallel dist.init_process_group() torch.cuda.set_device(dist.get_rank()) mpu.initialize_model_parallel(xxxx) tensor_parallel.random.model_parallel_cuda_manual_seed(xxx) ```",hÃ¡ uma sugestÃ£o relacionada para expandir o script para mesclar tanto o paralelismo de tensor quanto de pipeline e tambÃ©m fornecer um script para dividir o checkpoint em partiÃ§Ãµes separadamente2. Isso pode valer a pena investigar tambÃ©m.,Marking as stale. No activity in 60 days.
gcooper-isi,Save hyperparameters file,"There are a variety of command line parameters, particularly those specifying the structure of the neural network, that must remain the same between when a model is originally trained and when the model is later used for finetuning or text generation. In many cases, if these parameters are not set to the same value both times, the second job will simply fail, and it's not always clear why. This pull request saves the values for these unchangeable parameters to a file in the model directory. Any subsequent job that loads from that model directory will always use the parameter values saved to that file.",2020-11-12T17:05:36Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/56
gcooper-isi,Save vocabulary and merge files in model dir,"In most cases, when fine tuning or generating text from an existing model, you will want to use exactly the same vocabulary and merge files that were used to train the original model. Therefore, it is helpful for these files to default to the same ones used for training the original model in these cases. To facilitate this, this pull request automatically copies these files into the model directory when they are specified on the command line. If they are not specified on the command line, it will look for the files in the model directory and use them if they are there.",2020-11-12T16:58:43Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/55
RainFrost1,Can you share your vocab file when traing bert uncased?,"When I use the pretraind model Bert345m, I find that the size of the vocab file is 29056, but the google's vocab file is 30592. Can you share your vocab file or give some tips to extract vocab from google's?",2020-11-10T06:57:57Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/54
gcooper-isi,Ignore padding tokens when generating text,"The current version of `text_generation_utils.py` considers all tokens when generating text. However, some of these tokens may be padding tokens. If they somehow get selected for generation, this causes an exception. This pull request excludes padding tokens from consideration when generating text.",2020-11-05T22:35:02Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/53
gcooper-isi,Ignore padding tokens when generating text,"To ensure that the vocabulary can be evenly divided across all the GPUs that the model is split across, and also apparently for some efficiency reasons I don't fully understand, MegatronLM adds padding tokens to the model. But these padding tokens do not correspond to any real tokens in the vocabulary, and thus can't even be decoded. The current version of the text generation code allows these tokens to be selected during generation, which causes an exception to be thrown. This corresponds to one of the two issues originally fixed in https://github.com/gcooperisi/DeepSpeedExamples/pull/19/files NVIDIA has already fixed the other issue addressed in that other pull request (that generation was only using the tokens for the first GPU within each data parallel group when generating text).",2020-11-04T16:40:14Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/52
jiezhangGt,Is model mp_rank_00  correctï¼Ÿ,"Following the steps, I downloaded the gpt modelï¼ˆ`wget contentdisposition https://api.ngc.nvidia.com/v2/models/nvidia/megatron_lm_345m/versions/v0.0/zip O megatron_lm_345m_v0.0.zip`ï¼‰ and dictionary & merge files, but the wikitext eval result is not so good, is there something wrong ï¼Ÿ   validation results on WIKITEXT103   done :) ```",2020-11-04T08:11:43Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/51,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
gcooper-isi,Fix typo in GPT2BPETokenizer.EncodeAsTokens(),"Corresponds to https://github.com/gcooperisi/DeepSpeedExamples/pull/27 Note that this file is now deprecated so it's probably not really relevant in the new version of MegatronLM. But the typo is there, so we might as well submit this pull request.",2020-11-02T18:08:38Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/50
lazykyama,Change distributed argument group title to appropriate one.,This is a very trivial change. A help message for distributed args is wrong. `title` parameter is changed from `mixed precision` to `distributed`.,2020-10-20T06:15:46Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/49
eric-haibin-lin,Reproduce 71.9 TFlops throughput,"Hi, I wonder what batch size per GPU was used for the benchmark below, if I want to reproduce it?  !scalingmp",2020-10-09T17:29:35Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/48, any comment?,"This table is specifically looking at model parallelism so there is no data parallelism involved. The batch size given is the full and only batch size. With model parallelism ""batch size per GPU"" isn't really meaningful, as in this case all of the GPUs are working on the same data. Does that help?", that makes sense! Thanks for answering me question!
sublee,Escape % in --split help message,"The `split` help message contains literal `%` which is in conflict with `%`formatting. Due to this reason, `help` on a MegatronLM application fails. The literal `%` should be escaped with `%%`. ``` $ python some_megatron_application.py help ... TypeError: %o format: an integer is required, not dict ```",2020-10-08T17:17:56Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/47
abhinavs95,Malicious domain name in openwebtext URL list,"I was following the instructions for preparing openwebtext dataset using instructions here: https://github.com/NVIDIA/MegatronLM/blob/main/tools/openwebtext/README.md In the URL list downloaded from the link in Step 1 of Download the dataset, one of the domain names (""horsefucker.org"") is associated with a known C&C server. This caused security vulnerability on my system. The blacklist_urls list must be updated with this domain name so that its filtered before data download begins.",2020-10-02T17:46:05Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/46,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Will update blacklist,Fixed: https://github.com/NVIDIA/MegatronLM/commit/780978175455afadc78eceea2c8f63ee4fceec7b  thanks for highlighting this
eric-haibin-lin,Fully half precision optimizer,"Hi, Has anyone implemented/tried fully halfprecision optimizer with Megatron? I see from the GPT3 paper: > ... implemented an early version of the codebase, and developed the memory optimizations for fully halfprecision training. It looks like Megatron FP16_Optimizer is still using mixed precision. Has anyone looked into this before? This would allow training these big models with much less memory ... Thanks!",2020-09-26T06:31:59Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/45
msmolyak,Incorporating Megatron-ML with DeepSpeed,"Microsoft incorporated MegatronLM with their DeepSpeed project in DeepSpeedExamples project. The combination of the two projects features increased speed and lower memory requirements compared to standalone MegatronLM model. However, the version of MegatronLM used in DeepSpeedExamples dates to February of 2020 and as such lacks the latest updates of your product. Have you considered supporting an uptodate version of MegatronLM integrated with DeepSpeed project? ",2020-09-25T18:19:51Z,stale,closed,14,2,https://github.com/NVIDIA/Megatron-LM/issues/44,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Closing. This issue should be addressed to the DeepSpeed developers at Microsoft: https://github.com/microsoft/DeepSpeed/
king-menin,Pretrain and generate,"if i run with `python m torch.distributed.launch nproc_per_node 16 pretrain_gpt2.py model_parallel_size==16` and after run generate: `python generate_samples.py`  i have error while initialization: size mismatch for transformer.layers.15.attention.dense.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([1024, 1024]). can i load model on one GPU and train distributed on 16 GPU with model_parallel_size==16? thank you!",2020-09-21T12:22:06Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/43,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
nakosung,Fix typo,redcue > reduce,2020-08-07T23:48:02Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/42
nakosung,Fix typo,Tiny typo fix : sedd > seed,2020-08-07T23:33:40Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/41
nakosung,Scaling up to GPT-3 size (175B),Will it be straightforward to scale Megatron LM up to 175B?,2020-08-06T06:48:24Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/40,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
shamoons,Is there a pretrained 8.3B parameter model?,I believe this repo links to the 345M parameter model. Is there any way to get the 8.3B parameter one?,2020-08-02T13:06:06Z,stale,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/39,Same. I need the 8.3B one as well.,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.,This is marked as completed but I can't find the 8.3B model on the NGC catalog. Was it never released or removed again?
g-karthik,FileExistsError when training with a shared file-system,"When training on a multinode cluster with a shared filesystem, I observe the following: ``` initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234 configuring data Traceback (most recent call last): File ""/fsx/DeepSpeedExamples/MegatronLM/pretrain_gpt2.py"", line 707, in      main() File ""/fsx/DeepSpeedExamples/MegatronLM/pretrain_gpt2.py"", line 652, in main     args.eod_token = get_train_val_test_data(args)   File ""/fsx/DeepSpeedExamples/MegatronLM/pretrain_gpt2.py"", line 598, in get_train_val_test_data     args)   File ""/fsx/DeepSpeedExamples/MegatronLM/configure_data.py"", line 34, in apply     return make_loaders(args)   File ""/fsx/DeepSpeedExamples/MegatronLM/configure_data.py"", line 170, in make_loaders     train, tokenizer = data_utils.make_dataset(**data_set_args)   File ""/fsx/DeepSpeedExamples/MegatronLM/data_utils/__init__.py"", line 93, in make_dataset     datasets = [get_dataset_from_path(p) for p in path]   File ""/fsx/DeepSpeedExamples/MegatronLM/data_utils/__init__.py"", line 93, in      datasets = [get_dataset_from_path(p) for p in path]   File ""/fsx/DeepSpeedExamples/MegatronLM/data_utils/__init__.py"", line 83, in get_dataset_from_path     make_lazy(path_, text.X, data_type='data')   File ""/fsx/DeepSpeedExamples/MegatronLM/data_utils/lazy_loader.py"", line 51, in make_lazy     os.makedirs(lazypath)   File ""/usr/lib/python3.6/os.py"", line 220, in makedirs     mkdir(name, mode) FileExistsError: [Errno 17] File exists: '/fsx/datasets/openwebtext/openwebtext.lazy' ``` I don't see the same error when training on a single independent node (with its own filesystem) though. Do you have a clean fix for this?",2020-07-07T22:08:57Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/38,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
usuyama,Huggingface <-> Megatron-LM Compatibility,"Looking for a way to convert model weights between huggingface and MegatronLM. (1): Continual pretraining from pretrained weights from huggingface (2): Convert MegatronLM model weights to huggingface It shouldn't be too difficult to adjust layer names/weights, but I'm hoping someone has already done this. Related CC(Compatibility with pytorchtransformers for finetuning ) (already closed but couldn't find the solution)",2020-07-06T07:07:04Z,stale,closed,13,27,https://github.com/NVIDIA/Megatron-LM/issues/37,"hmm it seems not so straight forward to convert to huggingface format.  At least, I think LayerNorms locations don't match. MegatronLM model structure: ``` BertModel(   (language_model): TransformerLanguageModel(     (embedding): Embedding(       (word_embeddings): VocabParallelEmbedding()       (position_embeddings): Embedding(512, 768)       (tokentype_embeddings): Embedding(2, 768)       (embedding_dropout): Dropout(p=0.1, inplace=False)     )     (transformer): ParallelTransformer(       (layers): ModuleList(         (0): ParallelTransformerLayer(           (input_layernorm): FusedLayerNorm(torch.Size([768]), eps=1e05, elementwise_affine=True)           (attention): ParallelSelfAttention(             (query_key_value): ColumnParallelLinear()             (attention_dropout): Dropout(p=0.1, inplace=False)             (dense): RowParallelLinear()             (output_dropout): Dropout(p=0.1, inplace=False)           )           (post_attention_layernorm): FusedLayerNorm(torch.Size([768]), eps=1e05, elementwise_affine=True)           (mlp): ParallelMLP(             (dense_h_to_4h): ColumnParallelLinear()             (dense_4h_to_h): RowParallelLinear()             (dropout): Dropout(p=0.1, inplace=False)           )         )         (1): ParallelTransformerLayer(           (input_layernorm): FusedLayerNorm(torch.Size([768]), eps=1e05, elementwise_affine=True)           (attention): ParallelSelfAttention(             (query_key_value): ColumnParallelLinear()             (attention_dropout): Dropout(p=0.1, inplace=False)             (dense): RowParallelLinear()             (output_dropout): Dropout(p=0.1, inplace=False)           )           (post_attention_layernorm): FusedLayerNorm(torch.Size([768]), eps=1e05, elementwise_affine=True)           (mlp): ParallelMLP(             (dense_h_to_4h): ColumnParallelLinear()             (dense_4h_to_h): RowParallelLinear()             (dropout): Dropout(p=0.1, inplace=False)           )         )         (2): ParallelTransformerLayer(           (input_layernorm): FusedLayerNorm(torch.Size([768]), eps=1e05, elementwise_affine=True)           (attention): ParallelSelfAttention(             (query_key_value): ColumnParallelLinear()             (attention_dropout): Dropout(p=0.1, inplace=False)             (dense): RowParallelLinear()             (output_dropout): Dropout(p=0.1, inplace=False)           )           (post_attention_layernorm): FusedLayerNorm(torch.Size([768]), eps=1e05, elementwise_affine=True)           (mlp): ParallelMLP(             (dense_h_to_4h): ColumnParallelLinear()             (dense_4h_to_h): RowParallelLinear()             (dropout): Dropout(p=0.1, inplace=False)           )         )         (3): ParallelTransformerLayer(           (input_layernorm): FusedLayerNorm(torch.Size([768]), eps=1e05, elementwise_affine=True)           (attention): ParallelSelfAttention(             (query_key_value): ColumnParallelLinear()             (attention_dropout): Dropout(p=0.1, inplace=False)             (dense): RowParallelLinear()             (output_dropout): Dropout(p=0.1, inplace=False)           )           (post_attention_layernorm): FusedLayerNorm(torch.Size([768]), eps=1e05, elementwise_affine=True)           (mlp): ParallelMLP(             (dense_h_to_4h): ColumnParallelLinear()             (dense_4h_to_h): RowParallelLinear()             (dropout): Dropout(p=0.1, inplace=False)           )         )         (4): ParallelTransformerLayer(           (input_layernorm): FusedLayerNorm(torch.Size([768]), eps=1e05, elementwise_affine=True)           (attention): ParallelSelfAttention(             (query_key_value): ColumnParallelLinear()             (attention_dropout): Dropout(p=0.1, inplace=False)             (dense): RowParallelLinear()             (output_dropout): Dropout(p=0.1, inplace=False)           )           (post_attention_layernorm): FusedLayerNorm(torch.Size([768]), eps=1e05, elementwise_affine=True)           (mlp): ParallelMLP(             (dense_h_to_4h): ColumnParallelLinear()             (dense_4h_to_h): RowParallelLinear()             (dropout): Dropout(p=0.1, inplace=False)           )         )         (5): ParallelTransformerLayer(           (input_layernorm): FusedLayerNorm(torch.Size([768]), eps=1e05, elementwise_affine=True)           (attention): ParallelSelfAttention(             (query_key_value): ColumnParallelLinear()             (attention_dropout): Dropout(p=0.1, inplace=False)             (dense): RowParallelLinear()             (output_dropout): Dropout(p=0.1, inplace=False)           )           (post_attention_layernorm): FusedLayerNorm(torch.Size([768]), eps=1e05, elementwise_affine=True)           (mlp): ParallelMLP(             (dense_h_to_4h): ColumnParallelLinear()             (dense_4h_to_h): RowParallelLinear()             (dropout): Dropout(p=0.1, inplace=False)           )         )         (6): ParallelTransformerLayer(           (input_layernorm): FusedLayerNorm(torch.Size([768]), eps=1e05, elementwise_affine=True)           (attention): ParallelSelfAttention(             (query_key_value): ColumnParallelLinear()             (attention_dropout): Dropout(p=0.1, inplace=False)             (dense): RowParallelLinear()             (output_dropout): Dropout(p=0.1, inplace=False)           )           (post_attention_layernorm): FusedLayerNorm(torch.Size([768]), eps=1e05, elementwise_affine=True)           (mlp): ParallelMLP(             (dense_h_to_4h): ColumnParallelLinear()             (dense_4h_to_h): RowParallelLinear()             (dropout): Dropout(p=0.1, inplace=False)           )         )         (7): ParallelTransformerLayer(           (input_layernorm): FusedLayerNorm(torch.Size([768]), eps=1e05, elementwise_affine=True)           (attention): ParallelSelfAttention(             (query_key_value): ColumnParallelLinear()             (attention_dropout): Dropout(p=0.1, inplace=False)             (dense): RowParallelLinear()             (output_dropout): Dropout(p=0.1, inplace=False)           )           (post_attention_layernorm): FusedLayerNorm(torch.Size([768]), eps=1e05, elementwise_affine=True)           (mlp): ParallelMLP(             (dense_h_to_4h): ColumnParallelLinear()             (dense_4h_to_h): RowParallelLinear()             (dropout): Dropout(p=0.1, inplace=False)           )         )         (8): ParallelTransformerLayer(           (input_layernorm): FusedLayerNorm(torch.Size([768]), eps=1e05, elementwise_affine=True)           (attention): ParallelSelfAttention(             (query_key_value): ColumnParallelLinear()             (attention_dropout): Dropout(p=0.1, inplace=False)             (dense): RowParallelLinear()             (output_dropout): Dropout(p=0.1, inplace=False)           )           (post_attention_layernorm): FusedLayerNorm(torch.Size([768]), eps=1e05, elementwise_affine=True)           (mlp): ParallelMLP(             (dense_h_to_4h): ColumnParallelLinear()             (dense_4h_to_h): RowParallelLinear()             (dropout): Dropout(p=0.1, inplace=False)           )         )         (9): ParallelTransformerLayer(           (input_layernorm): FusedLayerNorm(torch.Size([768]), eps=1e05, elementwise_affine=True)           (attention): ParallelSelfAttention(             (query_key_value): ColumnParallelLinear()             (attention_dropout): Dropout(p=0.1, inplace=False)             (dense): RowParallelLinear()             (output_dropout): Dropout(p=0.1, inplace=False)           )           (post_attention_layernorm): FusedLayerNorm(torch.Size([768]), eps=1e05, elementwise_affine=True)           (mlp): ParallelMLP(             (dense_h_to_4h): ColumnParallelLinear()             (dense_4h_to_h): RowParallelLinear()             (dropout): Dropout(p=0.1, inplace=False)           )         )         (10): ParallelTransformerLayer(           (input_layernorm): FusedLayerNorm(torch.Size([768]), eps=1e05, elementwise_affine=True)           (attention): ParallelSelfAttention(             (query_key_value): ColumnParallelLinear()             (attention_dropout): Dropout(p=0.1, inplace=False)             (dense): RowParallelLinear()             (output_dropout): Dropout(p=0.1, inplace=False)           )           (post_attention_layernorm): FusedLayerNorm(torch.Size([768]), eps=1e05, elementwise_affine=True)           (mlp): ParallelMLP(             (dense_h_to_4h): ColumnParallelLinear()             (dense_4h_to_h): RowParallelLinear()             (dropout): Dropout(p=0.1, inplace=False)           )         )         (11): ParallelTransformerLayer(           (input_layernorm): FusedLayerNorm(torch.Size([768]), eps=1e05, elementwise_affine=True)           (attention): ParallelSelfAttention(             (query_key_value): ColumnParallelLinear()             (attention_dropout): Dropout(p=0.1, inplace=False)             (dense): RowParallelLinear()             (output_dropout): Dropout(p=0.1, inplace=False)           )           (post_attention_layernorm): FusedLayerNorm(torch.Size([768]), eps=1e05, elementwise_affine=True)           (mlp): ParallelMLP(             (dense_h_to_4h): ColumnParallelLinear()             (dense_4h_to_h): RowParallelLinear()             (dropout): Dropout(p=0.1, inplace=False)           )         )       )       (final_layernorm): FusedLayerNorm(torch.Size([768]), eps=1e05, elementwise_affine=True)     )     (pooler): Pooler(       (dense): Linear(in_features=768, out_features=768, bias=True)     )   )   (lm_head): BertLMHead(     (dense): Linear(in_features=768, out_features=768, bias=True)     (layernorm): FusedLayerNorm(torch.Size([768]), eps=1e05, elementwise_affine=True)   )   (binary_head): Linear(in_features=768, out_features=2, bias=True) ) ```","for reference, huggingface BertModel ``` BertModel(   (embeddings): BertEmbeddings(     (word_embeddings): Embedding(30522, 768, padding_idx=0)     (position_embeddings): Embedding(512, 768)     (token_type_embeddings): Embedding(2, 768)     (LayerNorm): LayerNorm((768,), eps=1e12, elementwise_affine=True)     (dropout): Dropout(p=0.1, inplace=False)   )   (encoder): BertEncoder(     (layer): ModuleList(       (0): BertLayer(         (attention): BertAttention(           (self): BertSelfAttention(             (query): Linear(in_features=768, out_features=768, bias=True)             (key): Linear(in_features=768, out_features=768, bias=True)             (value): Linear(in_features=768, out_features=768, bias=True)             (dropout): Dropout(p=0.1, inplace=False)           )           (output): BertSelfOutput(             (dense): Linear(in_features=768, out_features=768, bias=True)             (LayerNorm): LayerNorm((768,), eps=1e12, elementwise_affine=True)             (dropout): Dropout(p=0.1, inplace=False)           )         )         (intermediate): BertIntermediate(           (dense): Linear(in_features=768, out_features=3072, bias=True)         )         (output): BertOutput(           (dense): Linear(in_features=3072, out_features=768, bias=True)           (LayerNorm): LayerNorm((768,), eps=1e12, elementwise_affine=True)           (dropout): Dropout(p=0.1, inplace=False)         )       )       (1): BertLayer(         (attention): BertAttention(           (self): BertSelfAttention(             (query): Linear(in_features=768, out_features=768, bias=True)             (key): Linear(in_features=768, out_features=768, bias=True)             (value): Linear(in_features=768, out_features=768, bias=True)             (dropout): Dropout(p=0.1, inplace=False)           )           (output): BertSelfOutput(             (dense): Linear(in_features=768, out_features=768, bias=True)             (LayerNorm): LayerNorm((768,), eps=1e12, elementwise_affine=True)             (dropout): Dropout(p=0.1, inplace=False)           )         )         (intermediate): BertIntermediate(           (dense): Linear(in_features=768, out_features=3072, bias=True)         )         (output): BertOutput(           (dense): Linear(in_features=3072, out_features=768, bias=True)           (LayerNorm): LayerNorm((768,), eps=1e12, elementwise_affine=True)           (dropout): Dropout(p=0.1, inplace=False)         )       )       (2): BertLayer(         (attention): BertAttention(           (self): BertSelfAttention(             (query): Linear(in_features=768, out_features=768, bias=True)             (key): Linear(in_features=768, out_features=768, bias=True)             (value): Linear(in_features=768, out_features=768, bias=True)             (dropout): Dropout(p=0.1, inplace=False)           )           (output): BertSelfOutput(             (dense): Linear(in_features=768, out_features=768, bias=True)             (LayerNorm): LayerNorm((768,), eps=1e12, elementwise_affine=True)             (dropout): Dropout(p=0.1, inplace=False)           )         )         (intermediate): BertIntermediate(           (dense): Linear(in_features=768, out_features=3072, bias=True)         )         (output): BertOutput(           (dense): Linear(in_features=3072, out_features=768, bias=True)           (LayerNorm): LayerNorm((768,), eps=1e12, elementwise_affine=True)           (dropout): Dropout(p=0.1, inplace=False)         )       )       (3): BertLayer(         (attention): BertAttention(           (self): BertSelfAttention(             (query): Linear(in_features=768, out_features=768, bias=True)             (key): Linear(in_features=768, out_features=768, bias=True)             (value): Linear(in_features=768, out_features=768, bias=True)             (dropout): Dropout(p=0.1, inplace=False)           )           (output): BertSelfOutput(             (dense): Linear(in_features=768, out_features=768, bias=True)             (LayerNorm): LayerNorm((768,), eps=1e12, elementwise_affine=True)             (dropout): Dropout(p=0.1, inplace=False)           )         )         (intermediate): BertIntermediate(           (dense): Linear(in_features=768, out_features=3072, bias=True)         )         (output): BertOutput(           (dense): Linear(in_features=3072, out_features=768, bias=True)           (LayerNorm): LayerNorm((768,), eps=1e12, elementwise_affine=True)           (dropout): Dropout(p=0.1, inplace=False)         )       )       (4): BertLayer(         (attention): BertAttention(           (self): BertSelfAttention(             (query): Linear(in_features=768, out_features=768, bias=True)             (key): Linear(in_features=768, out_features=768, bias=True)             (value): Linear(in_features=768, out_features=768, bias=True)             (dropout): Dropout(p=0.1, inplace=False)           )           (output): BertSelfOutput(             (dense): Linear(in_features=768, out_features=768, bias=True)             (LayerNorm): LayerNorm((768,), eps=1e12, elementwise_affine=True)             (dropout): Dropout(p=0.1, inplace=False)           )         )         (intermediate): BertIntermediate(           (dense): Linear(in_features=768, out_features=3072, bias=True)         )         (output): BertOutput(           (dense): Linear(in_features=3072, out_features=768, bias=True)           (LayerNorm): LayerNorm((768,), eps=1e12, elementwise_affine=True)           (dropout): Dropout(p=0.1, inplace=False)         )       )       (5): BertLayer(         (attention): BertAttention(           (self): BertSelfAttention(             (query): Linear(in_features=768, out_features=768, bias=True)             (key): Linear(in_features=768, out_features=768, bias=True)             (value): Linear(in_features=768, out_features=768, bias=True)             (dropout): Dropout(p=0.1, inplace=False)           )           (output): BertSelfOutput(             (dense): Linear(in_features=768, out_features=768, bias=True)             (LayerNorm): LayerNorm((768,), eps=1e12, elementwise_affine=True)             (dropout): Dropout(p=0.1, inplace=False)           )         )         (intermediate): BertIntermediate(           (dense): Linear(in_features=768, out_features=3072, bias=True)         )         (output): BertOutput(           (dense): Linear(in_features=3072, out_features=768, bias=True)           (LayerNorm): LayerNorm((768,), eps=1e12, elementwise_affine=True)           (dropout): Dropout(p=0.1, inplace=False)         )       )       (6): BertLayer(         (attention): BertAttention(           (self): BertSelfAttention(             (query): Linear(in_features=768, out_features=768, bias=True)             (key): Linear(in_features=768, out_features=768, bias=True)             (value): Linear(in_features=768, out_features=768, bias=True)             (dropout): Dropout(p=0.1, inplace=False)           )           (output): BertSelfOutput(             (dense): Linear(in_features=768, out_features=768, bias=True)             (LayerNorm): LayerNorm((768,), eps=1e12, elementwise_affine=True)             (dropout): Dropout(p=0.1, inplace=False)           )         )         (intermediate): BertIntermediate(           (dense): Linear(in_features=768, out_features=3072, bias=True)         )         (output): BertOutput(           (dense): Linear(in_features=3072, out_features=768, bias=True)           (LayerNorm): LayerNorm((768,), eps=1e12, elementwise_affine=True)           (dropout): Dropout(p=0.1, inplace=False)         )       )       (7): BertLayer(         (attention): BertAttention(           (self): BertSelfAttention(             (query): Linear(in_features=768, out_features=768, bias=True)             (key): Linear(in_features=768, out_features=768, bias=True)             (value): Linear(in_features=768, out_features=768, bias=True)             (dropout): Dropout(p=0.1, inplace=False)           )           (output): BertSelfOutput(             (dense): Linear(in_features=768, out_features=768, bias=True)             (LayerNorm): LayerNorm((768,), eps=1e12, elementwise_affine=True)             (dropout): Dropout(p=0.1, inplace=False)           )         )         (intermediate): BertIntermediate(           (dense): Linear(in_features=768, out_features=3072, bias=True)         )         (output): BertOutput(           (dense): Linear(in_features=3072, out_features=768, bias=True)           (LayerNorm): LayerNorm((768,), eps=1e12, elementwise_affine=True)           (dropout): Dropout(p=0.1, inplace=False)         )       )       (8): BertLayer(         (attention): BertAttention(           (self): BertSelfAttention(             (query): Linear(in_features=768, out_features=768, bias=True)             (key): Linear(in_features=768, out_features=768, bias=True)             (value): Linear(in_features=768, out_features=768, bias=True)             (dropout): Dropout(p=0.1, inplace=False)           )           (output): BertSelfOutput(             (dense): Linear(in_features=768, out_features=768, bias=True)             (LayerNorm): LayerNorm((768,), eps=1e12, elementwise_affine=True)             (dropout): Dropout(p=0.1, inplace=False)           )         )         (intermediate): BertIntermediate(           (dense): Linear(in_features=768, out_features=3072, bias=True)         )         (output): BertOutput(           (dense): Linear(in_features=3072, out_features=768, bias=True)           (LayerNorm): LayerNorm((768,), eps=1e12, elementwise_affine=True)           (dropout): Dropout(p=0.1, inplace=False)         )       )       (9): BertLayer(         (attention): BertAttention(           (self): BertSelfAttention(             (query): Linear(in_features=768, out_features=768, bias=True)             (key): Linear(in_features=768, out_features=768, bias=True)             (value): Linear(in_features=768, out_features=768, bias=True)             (dropout): Dropout(p=0.1, inplace=False)           )           (output): BertSelfOutput(             (dense): Linear(in_features=768, out_features=768, bias=True)             (LayerNorm): LayerNorm((768,), eps=1e12, elementwise_affine=True)             (dropout): Dropout(p=0.1, inplace=False)           )         )         (intermediate): BertIntermediate(           (dense): Linear(in_features=768, out_features=3072, bias=True)         )         (output): BertOutput(           (dense): Linear(in_features=3072, out_features=768, bias=True)           (LayerNorm): LayerNorm((768,), eps=1e12, elementwise_affine=True)           (dropout): Dropout(p=0.1, inplace=False)         )       )       (10): BertLayer(         (attention): BertAttention(           (self): BertSelfAttention(             (query): Linear(in_features=768, out_features=768, bias=True)             (key): Linear(in_features=768, out_features=768, bias=True)             (value): Linear(in_features=768, out_features=768, bias=True)             (dropout): Dropout(p=0.1, inplace=False)           )           (output): BertSelfOutput(             (dense): Linear(in_features=768, out_features=768, bias=True)             (LayerNorm): LayerNorm((768,), eps=1e12, elementwise_affine=True)             (dropout): Dropout(p=0.1, inplace=False)           )         )         (intermediate): BertIntermediate(           (dense): Linear(in_features=768, out_features=3072, bias=True)         )         (output): BertOutput(           (dense): Linear(in_features=3072, out_features=768, bias=True)           (LayerNorm): LayerNorm((768,), eps=1e12, elementwise_affine=True)           (dropout): Dropout(p=0.1, inplace=False)         )       )       (11): BertLayer(         (attention): BertAttention(           (self): BertSelfAttention(             (query): Linear(in_features=768, out_features=768, bias=True)             (key): Linear(in_features=768, out_features=768, bias=True)             (value): Linear(in_features=768, out_features=768, bias=True)             (dropout): Dropout(p=0.1, inplace=False)           )           (output): BertSelfOutput(             (dense): Linear(in_features=768, out_features=768, bias=True)             (LayerNorm): LayerNorm((768,), eps=1e12, elementwise_affine=True)             (dropout): Dropout(p=0.1, inplace=False)           )         )         (intermediate): BertIntermediate(           (dense): Linear(in_features=768, out_features=3072, bias=True)         )         (output): BertOutput(           (dense): Linear(in_features=3072, out_features=768, bias=True)           (LayerNorm): LayerNorm((768,), eps=1e12, elementwise_affine=True)           (dropout): Dropout(p=0.1, inplace=False)         )       )     )   )   (pooler): BertPooler(     (dense): Linear(in_features=768, out_features=768, bias=True)     (activation): Tanh()   ) ) ```",Any thoughts/advice?     ,Any updates?,"Was interested in the same questions, . See excerpt from Megatron paper. Does look like MegatronHF will require some updates on HF side. !image","Thanks,   Need to check the forward function for details, but the order of weights looks different as you pointed out. MegatronLM ```         (11): ParallelTransformerLayer(           (input_layernorm): FusedLayerNorm(torch.Size([768]), eps=1e05, elementwise_affine=True)           (attention): ParallelSelfAttention(             (query_key_value): ColumnParallelLinear()             (attention_dropout): Dropout(p=0.1, inplace=False)             (dense): RowParallelLinear()             (output_dropout): Dropout(p=0.1, inplace=False)           )           (post_attention_layernorm): FusedLayerNorm(torch.Size([768]), eps=1e05, elementwise_affine=True)           (mlp): ParallelMLP(             (dense_h_to_4h): ColumnParallelLinear()             (dense_4h_to_h): RowParallelLinear()             (dropout): Dropout(p=0.1, inplace=False)           ) ``` HuggingFace ```       (11): BertLayer(         (attention): BertAttention(           (self): BertSelfAttention(             (query): Linear(in_features=768, out_features=768, bias=True)             (key): Linear(in_features=768, out_features=768, bias=True)             (value): Linear(in_features=768, out_features=768, bias=True)             (dropout): Dropout(p=0.1, inplace=False)           )           (output): BertSelfOutput(             (dense): Linear(in_features=768, out_features=768, bias=True)             (LayerNorm): LayerNorm((768,), eps=1e12, elementwise_affine=True)             (dropout): Dropout(p=0.1, inplace=False)           )         )         (intermediate): BertIntermediate(           (dense): Linear(in_features=768, out_features=3072, bias=True)         )         (output): BertOutput(           (dense): Linear(in_features=3072, out_features=768, bias=True)           (LayerNorm): LayerNorm((768,), eps=1e12, elementwise_affine=True)           (dropout): Dropout(p=0.1, inplace=False)         )       ) ```",I have the same question. Any new update?,Curious about this too â€“Â I have a GPT2 model trained with Megatron and would love to get it imported into HF.,"In order to convert the Megatron GPT2 model to HF(huggingface transformers) GPT2, a layer level parameter conversion was performed and verification was conducted, but the conversion was not performed properly. The following is the core concept of transformation. Megatron  GPT2 transformer layer and shape ``` layers.0.input_layernorm.weight, shape: torch.Size([1920]) layers.0.input_layernorm.bias, shape: torch.Size([1920]) layers.0.attention.query_key_value.weight, shape: torch.Size([5760, 1920])   need transpose layers.0.attention.query_key_value.bias, shape: torch.Size([5760]) layers.0.attention.dense.weight, shape: torch.Size([1920, 1920]) layers.0.attention.dense.bias, shape: torch.Size([1920]) layers.0.post_attention_layernorm.weight, shape: torch.Size([1920]) layers.0.post_attention_layernorm.bias, shape: torch.Size([1920]) layers.0.mlp.dense_h_to_4h.weight, shape: torch.Size([7680, 1920])    need transpose layers.0.mlp.dense_h_to_4h.bias, shape: torch.Size([7680]) layers.0.mlp.dense_4h_to_h.weight, shape: torch.Size([1920, 7680])   need transpose layers.0.mlp.dense_4h_to_h.bias, shape: torch.Size([1920]) ``` HF GPT2 transformer layer and shape ``` transformer.h.0.ln_1.weight, shape: torch.Size([1920]) transformer.h.0.ln_1.bias, shape: torch.Size([1920]) transformer.h.0.attn.bias, shape: torch.Size([1, 1, 1920, 1920]) transformer.h.0.attn.masked_bias, shape: torch.Size([]) transformer.h.0.attn.c_attn.weight, shape: torch.Size([1920, 5760]) transformer.h.0.attn.c_attn.bias, shape: torch.Size([5760]) transformer.h.0.attn.c_proj.weight, shape: torch.Size([1920, 1920]) transformer.h.0.attn.c_proj.bias, shape: torch.Size([1920]) transformer.h.0.ln_2.weight, shape: torch.Size([1920]) transformer.h.0.ln_2.bias, shape: torch.Size([1920]) transformer.h.0.mlp.c_fc.weight, shape: torch.Size([1920, 7680]) transformer.h.0.mlp.c_fc.bias, shape: torch.Size([7680]) transformer.h.0.mlp.c_proj.weight, shape: torch.Size([7680, 1920]) transformer.h.0.mlp.c_proj.bias, shape: torch.Size([1920]) ``` In the case of `attn.bias` and `masked_bias`, they were the same as the values â€‹â€‹implemented in Megatron  GPT2, so they were ignored during conversion and all parameters were converted, but the generated results of HF GPT2 were different from those of Megatron  GPT2. I guess HF GPT2 and Megatron GPT2 have some different layer level implementation. If you have any ideas on this part, please let me know.","As  pointed out, Megatron rearranged LayerNorm and residual connection in the transformer block. Maybe that's one difference you observed, jeon ?",",  thanks for reminding me. I thought it was a part related to BERT in the paper, but looking at the MegatronLM code, it seems to be the code shared with GPT2. https://github.com/NVIDIA/MegatronLM/blob/1b3dfa2ff9fe1643e15ddd1cf775abcdb2146f13/megatron/model/transformer.pyL445  This part looks different from the HF transformers. ðŸ¤”   ",Any news on this issue?,Any news on this?,Have not tried it but this exists: https://github.com/huggingface/transformers/tree/main/src/transformers/models/megatron_gpt2,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.," 1. Convert llama2 from HuggingFace to MegatronLM: ``` PYTHONPATH=$(pwd) tools/checkpoint/util.py modeltype=GPT loader=llama2_hf loaddir= savedir= tokenizermodel= ```  2. Convert llama2 from MegatronLM to HuggingFace: Step 1. Download this python script and save into `MegatronLM/tools/checkpoint/saver_llama2_hf.py` Step 2. Do the conversion ```py PYTHONPATH=$(pwd) tools/checkpoint/util.py modeltype=GPT saver=llama2_hf loaddir= savedir= ``` But before converting LLaMA2 from MGT to HF, you need to ensure that following parameters in MGT are set to the same default values as in HF during your trainning process: 1. Set `normepsilon=1e6`, 2. Do not enable `applyquerykeylayerscaling` (or enable `noquerykeylayerscaling` in older versions), 3. Neither custom attention_mask nor position_ids takes effect in MGT's GPT models in trainning, 4. Enable `disablebiaslinear`.","Hi, are there any updates? I'm mostly interested in converting GPT2/Bloom checkpoints.  ",>  Convert llama2 from HuggingFace to MegatronLM: > ``` > PYTHONPATH=$(pwd) tools/checkpoint/util.py modeltype=GPT loader=llama2_hf loaddir= savedir= tokenizermodel= > ``` >  >  Save llama2 checkpoint as HuggingFace to MegatronLM: > Step 1. Download this file to MegatronLM/tools/checkpoint/saver_llama2_hf.py >  > Step 2. Do the conversion >  > ```python > PYTHONPATH=$(pwd) tools/checkpoint/util.py modeltype=GPT loader=llama2_hf loaddir= savedir= > ``` >  > Step 3. Test >  > ```python > from transformers import AutoModelForCausalLM > model = AutoModelForCausalLM.from_pretrain() > ``` Works perfectly for me. Just changed loader=llama2hf to loader=megatron since we want to convert Megatron checkpoint to hf,"> Hi, are there any updates? I'm mostly interested in converting GPT2/Bloom checkpoints. They have script for converting GPT2 somewhere in hf's repo transformers/models/megatrongpt2 https://huggingface.co/docs/transformers/model_doc/megatron_gpt2 Otherwise it should be in somewhere in Megatron's repo.",Marking as stale. No activity in 60 days.,"Could you provide guidance on how to consolidate the weights of a moduleâ€”specifically, ParallelMLP and Parallel Attentionâ€”into a PyTorchcompatible format? I am utilizing a tensorparallel size greater than 1, which results in the module's parameters being distributed across different ranks. How can I aggregate these to obtain the complete set of model weights?",>  1. ãƒ©ãƒž2 ã‚’ HuggingFace ã‹ã‚‰ MegatronLM ã«å¤‰æ›ã—ã¾ã™ã€‚ > ``` > PYTHONPATH=$(pwd) tools/checkpoint/util.py modeltype=GPT loader=llama2_hf loaddir= savedir= tokenizermodel= > ``` >  >  2. ãƒ©ãƒž2 ã‚’ MegatronLM ã‹ã‚‰ HuggingFace ã«å¤‰æ›ã—ã¾ã™ã€‚ > ã‚¹ãƒ†ãƒƒãƒ— 1. ã“ã®Python ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€æ¬¡ã®å ´æ‰€ã«ä¿å­˜ã—ã¾ã™ã€‚`MegatronLM/tools/checkpoint/saver_llama2_hf.py` >  > ã‚¹ãƒ†ãƒƒãƒ— 2. å¤‰æ›ã‚’å®Ÿè¡Œã™ã‚‹ >  > ```python > PYTHONPATH=$(pwd) tools/checkpoint/util.py modeltype=GPT saver=llama2_hf loaddir= savedir= > ``` >  > ãŸã ã—ã€LLaMA2 ã‚’ MGT ã‹ã‚‰ HF ã«å¤‰æ›ã™ã‚‹å‰ã«ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒ—ãƒ­ã‚»ã‚¹ä¸­ã« MGT ã®æ¬¡ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒ HF ã¨åŒã˜ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã«è¨­å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ >  > 1. ã‚»ãƒƒãƒˆ`normepsilon=1e6`ã€ > 2. æœ‰åŠ¹ã«ã—ãªã„ã§ãã ã•ã„`applyquerykeylayerscaling`(ã¾ãŸã¯`noquerykeylayerscaling`å¤ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã¯æœ‰åŠ¹ã«ã—ã¾ã™)ã€‚ > 3. ã‚«ã‚¹ã‚¿ãƒ ã®attention_maskã‚‚position_idsã‚‚ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã®MGTã®GPTãƒ¢ãƒ‡ãƒ«ã§ã¯åŠ¹æžœãŒã‚ã‚Šã¾ã›ã‚“ã€‚ > 4. æœ‰åŠ¹ã«ã™ã‚‹`disablebiaslinear`ã€‚ Does this conversion script support GQA?,Marking as stale. No activity in 60 days.,I found a script in transformers https://github.com/huggingface/transformers/blob/main/src/transformers/models/megatron_gpt2/convert_megatron_gpt2_checkpoint.py anyone tried this before? It seems to convert a gpt2 from Megatron format to huggingface format,">  1. Convert llama2 from HuggingFace to MegatronLM: > ``` > PYTHONPATH=$(pwd) tools/checkpoint/util.py modeltype=GPT loader=llama2_hf loaddir= savedir= tokenizermodel= > ``` >  >  2. Convert llama2 from MegatronLM to HuggingFace: > Step 1. Download this python script and save into `MegatronLM/tools/checkpoint/saver_llama2_hf.py` >  > Step 2. Do the conversion >  > ```python > PYTHONPATH=$(pwd) tools/checkpoint/util.py modeltype=GPT saver=llama2_hf loaddir= savedir= > ``` >  > But before converting LLaMA2 from MGT to HF, you need to ensure that following parameters in MGT are set to the same default values as in HF during your trainning process: >  > 1. Set `normepsilon=1e6`, > 2. Do not enable `applyquerykeylayerscaling` (or enable `noquerykeylayerscaling` in older versions), > 3. Neither custom attention_mask nor position_ids takes effect in MGT's GPT models in trainning, > 4. Enable `disablebiaslinear`. Is this support GQA",Marking as stale. No activity in 60 days.
zarzen,"training in fp32 results error ""TypeError: zero_grad() got an unexpected keyword argument 'set_grads_to_None'""","Hi there, I would like to enable float32 training, thus, I commented out the `fp16` option in `example/pretrain_bert_distributed.sh` file. During the runtime, I got following errors: ``` TypeError: zero_grad() got an unexpected keyword argument 'set_grads_to_None'   File ""/home/ubuntu/MegatronLM/megatron/training.py"", line 231, in backward_step     lr_scheduler)   File ""/home/ubuntu/MegatronLM/megatron/training.py"", line 269, in train_step     optimizer.zero_grad(set_grads_to_None=True) TypeError: zero_grad() got an unexpected keyword argument 'set_grads_to_None'     backward_step(optimizer, model, loss)   File ""/home/ubuntu/MegatronLM/megatron/training.py"", line 231, in backward_step     optimizer.zero_grad(set_grads_to_None=True) TypeError: zero_grad() got an unexpected keyword argument 'set_grads_to_None' Traceback (most recent call last):   File ""/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/runpy.py"", line 193, in _run_module_as_main     ""__main__"", mod_spec)   File ""/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/runpy.py"", line 85, in _run_code     exec(code, run_globals)   File ""/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/sitepackages/torch/distributed/launch.py"", line 263, in      main()   File ""/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/sitepackages/torch/distributed/launch.py"", line 259, in main     cmd=cmd) ``` I think specifically this  line of code resulting the error.  Can I safely make a condition branch to do zero_grads without input option? e.g. ``` python      Backward pass.     if args.fp16:         optimizer.zero_grad(set_grads_to_None=True)     else:         optimizer.zero_grad() ```",2020-06-26T19:26:36Z,stale,closed,2,2,https://github.com/NVIDIA/Megatron-LM/issues/36,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
zarzen,FileNotFoundError Issues while running on 2 nodes,"Hi there, I want to run distributed training on two servers, each has 4 GPUs.  I have modified the `examples/pretrain_bert_distributed.sh` file accordingly, as following: ```  this is config on node1 GPUS_PER_NODE=4  Change for multinode config MASTER_ADDR= MASTER_PORT=6000 NNODES=2 NODE_RANK=1 WORLD_SIZE=$(($GPUS_PER_NODE*$NNODES)) ``` I can successfully run master process on node0. While when I launch `bash examples/pretrain_bert_distributed.sh` on node1. I will got an error as following: ``` Traceback (most recent call last):   File ""pretrain_bert.py"", line 122, in  Traceback (most recent call last):   File ""pretrain_bert.py"", line 122, in      args_defaults={'tokenizer_type': 'BertWordPieceLowerCase'})   File ""/home/ubuntu/MegatronLM/megatron/training.py"", line 85, in pretrain     args_defaults={'tokenizer_type': 'BertWordPieceLowerCase'})   File ""/home/ubuntu/MegatronLM/megatron/training.py"", line 85, in pretrain     train_valid_test_dataset_provider)   File ""/home/ubuntu/MegatronLM/megatron/training.py"", line 496, in build_train_valid_test_data_iterators     train_valid_test_dataset_provider)   File ""/home/ubuntu/MegatronLM/megatron/training.py"", line 496, in build_train_valid_test_data_iterators     train_val_test_num_samples)   File ""pretrain_bert.py"", line 113, in train_valid_test_datasets_provider     train_val_test_num_samples)   File ""pretrain_bert.py"", line 113, in train_valid_test_datasets_provider     skip_warmup=(not args.mmap_warmup))   File ""/home/ubuntu/MegatronLM/megatron/data/bert_dataset.py"", line 95, in build_train_valid_test_datasets     skip_warmup=(not args.mmap_warmup))   File ""/home/ubuntu/MegatronLM/megatron/data/bert_dataset.py"", line 95, in build_train_valid_test_datasets     train_dataset = build_dataset(0, 'train')   File ""/home/ubuntu/MegatronLM/megatron/data/bert_dataset.py"", line 86, in build_dataset     train_dataset = build_dataset(0, 'train')   File ""/home/ubuntu/MegatronLM/megatron/data/bert_dataset.py"", line 86, in build_dataset     seed=seed)   File ""/home/ubuntu/MegatronLM/megatron/data/bert_dataset.py"", line 125, in __init__     seed=seed)   File ""/home/ubuntu/MegatronLM/megatron/data/bert_dataset.py"", line 125, in __init__     self.name)   File ""/home/ubuntu/MegatronLM/megatron/data/bert_dataset.py"", line 282, in get_samples_mapping_     self.name)   File ""/home/ubuntu/MegatronLM/megatron/data/bert_dataset.py"", line 282, in get_samples_mapping_     samples_mapping = np.load(indexmap_filename, allow_pickle=True)   File ""/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/sitepackages/numpy/lib/npyio.py"", line 428, in load     samples_mapping = np.load(indexmap_filename, allow_pickle=True)   File ""/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/sitepackages/numpy/lib/npyio.py"", line 428, in load     fid = open(os_fspath(file), ""rb"")     fid = open(os_fspath(file), ""rb"") FileNotFoundError: [Errno 2] No such file or directory: 'mybert_text_sentence_train_indexmap_32000000mns_512msl_0.10ssp_1234s.npy' FileNotFoundError: [Errno 2] No such file or directory: 'mybert_text_sentence_train_indexmap_32000000mns_512msl_0.10ssp_1234s.npy' ``` Does program assume different servers share data over the network? because I see the missing file actually generated at master node. If I copy `*.npy` files from `node0` to `node1`, training runs.",2020-06-11T01:20:22Z,,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/35,"Hi there, I have met the exactly same question, do you find a way to solve it? Now I have to copy the *.npy files manually twice from node 0 to node 1 when I run a new model. ",I think the system assumes the cluster is running with a shared file system like lustre. ,"> Hi there, I have met the exactly same question, do you find a way to solve it? Now I have to copy the *.npy files manually twice from node 0 to node 1 when I run a new model. Did you ever find answer to this problem? Thanks","> > Hi there, I have met the exactly same question, do you find a way to solve it? Now I have to copy the *.npy files manually twice from node 0 to node 1 when I run a new model. >  > Did you ever find answer to this problem? Thanks If you don't plan to use a file sharing system like lustre, you'd have to manually copy and paste the dataset cache to all your nodes and comment out the self.unique_description_hash in megatron_dataset.py, otherwise it will try to load a new set of .npy files every time. Hope it helps!"
edwardjhu,Unnecessary tensorboard logging at every iteration,"The following code bypasses ""log_interval"" and writes learning rate to tensorboard at every update. This seems unnecessary and affects throughput significantly when using an external storage shared by many jobs.",2020-06-01T17:48:06Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/34,This has been addressed by introducing tensorboardloginterval
usuyama,tokenizer.py is missing in openwebtext,"`openwebtext/clean_dataset.py` fails with this error: ``` Traceback (most recent call last):   File ""openwebtext/cleanup_dataset.py"", line 25, in      from tokenizer import Tokenizer ImportError: cannot import name 'Tokenizer' ``` It seems `tokenizer.py` was deleted accidentally. https://github.com/NVIDIA/MegatronLM/blob/20764e123467893c7132ad89df5e5f5bba8355ae/openwebtext/tokenizer.py",2020-05-28T23:54:54Z,bug stale,closed,5,2,https://github.com/NVIDIA/Megatron-LM/issues/33,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
safooray,evaluation loss < training loss,"I have pretrained Megatron's BERT large and base with different batch sizes, and it always seems to be the case that the training loss is about .2 higher than the validation loss. Is this the behavior you observed and if yes, what is causing it?",2020-05-07T00:15:35Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/32,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
safooray,Evaluating on all GLUE tasks,How would you suggest to go about evaluating Megatron's BERT on the full set of GLUE tasks? ,2020-05-07T00:12:19Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/31,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
dweekly,Fix typos,"Fixed some small typos in the README ""theoritical"" [sic] ""models...reaches"" [sic]",2020-05-05T21:31:08Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/30
JF-D,BERT Loss not decreasing,"I train the BERT_BASE model on 16 V100 GPUs using En Wikipedia dataset.  I discover that the NSP loss can decrease to 0.3 from 0.7 and the MLM loss decrease to 6.8 from 10.0. During training, I use the `create_pretrainig_data.py` from googleresearch/bert to precreate bert pretraining examples, since when I use the default setting that creates training samples in dataset the speed is quite slow and sometimes it takes hours to train 100 iterations. With precreated pretraining data and lazydataloader, the training speed is quite normal. But the loss is a big problem for me. I pretrain the BERT_BASE model using the scheme in BERT paper(batch size 256, each GPU batch size 16, train 900000 steps with seqlen128, 100000 steps with seqlen 512), then I run test on SQuAD, the F1 score is quite low. Could you please offer some help. Thanks!",2020-04-18T10:27:12Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/29,"Hi,    could you please offer some help? when I pretrain bert_base with MegatronLM v0.2 using latest enwikipedia dump, I got low NSP loss, but the MLM loss not decreasing.  !image","D, could I check what was the issue for you here? My loss is also not converging and I am trying to figure out possible reasons."
LiweiPeng,Couldn't find BERT 3.9B related implementation,"Hi,  I have a question about your BERT xlarge modeling implementation. From your megatron paper Figure 7, by rearranging the order of the layer normalization and the residual connections, you successfully trained BERT xlarge up to 3.9B parameters.  !Screen Shot 20200413 at 11 02 24 AM When I read the related BERT metatron code, for all the LayerNormrelated implementation, I couldn't see the change mentioned in the paper Figure 7. Can you clarify what I am missing? Thanks.",2020-04-13T18:15:23Z,,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/28,I found latest code has code change mentioned in the paper. so close it.,I'm confused. The code seems to have 345M parameter models?,"> I found latest code has code change mentioned in the paper. so close it. Hi, I meet the same problems. Can you share where the relevant code snippet isï¼Ÿ"
fengche86,Calculate the TFLOPS performance with elapsed time per iteration,"Hi! I'm trying to study the Megatron modelparallel, and was able to run the model on V100 GPU using Pytorch docker container, with GPT2. However, the benchmark log would only print out the elapsed time spent for every 100 iterations. My question is, how can I calculate the TFLOPS performance from the elapsed time per iteration? It would be helpful if you can shed some light on the formula to calculate the TFLOPS/PFLOPS from the model configurations (hidden size, attention heads and number of layers).  I'm now getting 528.9ms per iteration for single GPU, I'm hoping that would match the single GPU baseline performance mentioned in the paper (39 TeraFLOPS). Following is the model I've been using: Config  1 Thanks!",2020-03-25T03:45:48Z,stale,closed,0,5,https://github.com/NVIDIA/Megatron-LM/issues/27,"Hi   , would you please help? Thanks!",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Have the same problem,Marking as stale. No activity in 60 days.,Have the same problem
ngoyal2707,dropout should be wrapped in `get_cuda_rng_tracker`,I think there are many places where `dropout` call is not under scope of `get_cuda_rng_tracker`. Just curious is that intention or got left out by mistake? Cause if I understand correctly every dropout call should be wrapped under that scope. Examples: https://github.com/NVIDIA/MegatronLM/blob/master/mpu/transformer.pyL155 https://github.com/NVIDIA/MegatronLM/blob/master/mpu/transformer.pyL217 https://github.com/NVIDIA/MegatronLM/blob/master/mpu/transformer.pyL560 etc,2020-02-03T21:15:51Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/26,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
PyxAI,fixed case where test set is a 0 dim object to be returned as a list,As seen in issue https://github.com/NVIDIA/MegatronLM/issues/15,2020-01-21T10:01:10Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/25,This pull request **introduces 21 alerts** and **fixes 25** when merging ca8dd4ac58ae441feef40e24213b8dcca0f0dc97 into 4947002db7eaea6d3c4aabf42adc9e48605ac52a  view on LGTM.com **new alerts:** * 9 for Module imports itself * 7 for Unused import * 2 for & CC(Is there a pretrained 8.3B parameter model?);import \*& CC(Is there a pretrained 8.3B parameter model?); may pollute namespace * 1 for Except block handles & CC(Is there a pretrained 8.3B parameter model?);BaseException& CC(Is there a pretrained 8.3B parameter model?); * 1 for Unreachable code * 1 for Wrong number of arguments in a call **fixed alerts:** * 9 for Module imports itself * 8 for Unused import * 3 for Unused local variable * 3 for Variable defined multiple times * 2 for & CC(Is there a pretrained 8.3B parameter model?);import \*& CC(Is there a pretrained 8.3B parameter model?); may pollute namespace
nlpBeginner,"GPT2 evaluation, wikitext-2, wikitext-103, PTB","how can I  set the parameters so that I can evaluate the GPT2  on the three task to match the results which are the GPT2 papers reported. such as the parameter ""overlapping_eval"".",2020-01-09T10:34:29Z,stale,closed,1,3,https://github.com/NVIDIA/Megatron-LM/issues/24,Have you found the solution for this problem? I meet the same problem with you.,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
vlarine,GPT-2 generation samples error ,"There are 5 parameters in the get_masks_and_position_ids definition: https://github.com/NVIDIA/MegatronLM/blob/master/pretrain_gpt2.pyL162 ``` def get_masks_and_position_ids(data,                                eod_token,                                reset_position_ids,                                reset_attention_mask,                                eod_mask_loss): ``` But there are only 4 in the function call: https://github.com/NVIDIA/MegatronLM/blob/master/generate_samples.pyL95 ```        attention_mask, loss_mask, position_ids = get_masks_and_position_ids(         tokens,         args.eod_token,         args.reset_position_ids,         args.reset_attention_mask) ```",2019-12-09T10:55:28Z,stale,closed,1,3,https://github.com/NVIDIA/Megatron-LM/issues/23,I ran into this as well.,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
gentelyang,bert model encoding error on python3.6.8 ,"data_config: {'world_size': 1, 'rank': 1, 'persist_state': 0, 'lazy': False, 'transpose': False, 'data_set_type': 'supervised', 'seq_length': 256, 'eval_seq_length': 256, 'samples_per_shard': 100} configuring data Traceback (most recent call last): File ""pretrain_bert.py"", line 490, in main() File ""pretrain_bert.py"", line 417, in main (train_data, val_data, test_data), tokenizer = data_config.apply(args) File ""/ssd2/bert1/MegatronLM/configure_data.py"", line 33, in apply return make_loaders(args) File ""/ssd2/bert1/MegatronLM/configure_data.py"", line 166, in make_loaders train, tokenizer = data_utils.make_dataset(**data_set_args) File ""/ssd2/bert1/MegatronLM/data_utils/init.py"", line 93, in make_dataset datasets = [get_dataset_from_path(p) for p in path] File ""/ssd2//bert1/MegatronLM/data_utils/init.py"", line 93, in datasets = [get_dataset_from_path(p) for p in path] File ""/ssd2/bert1/MegatronLM/data_utils/init.py"", line 82, in get_dataset_from_path delim=delim, drop_unlabeled=drop_unlabeled, loose_json=loose) File ""/ssd2/bert1/MegatronLM/data_utils/init.py"", line 50, in get_dataset text = json_dataset(path, **kwargs) File ""/ssd2/bert1/MegatronLM/data_utils/datasets.py"", line 327, in init for j in self.load_json_stream(self.path): File ""/ssd2/bert1/MegatronLM/data_utils/datasets.py"", line 436, in load_json_stream for j in generator: File ""/ssd2/bert1/MegatronLM/data_utils/datasets.py"", line 432, in gen_helper for row in f: File ""/ssd2/bert1/MegatronLM/bert_env/lib/python3.6/encodings/ascii.py"", line 26, in decode return codecs.ascii_decode(input, self.errors)[0] UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 5507: ordinal not in range(128)",2019-12-08T03:49:54Z,stale,closed,0,3,https://github.com/NVIDIA/Megatron-LM/issues/22,"The input json must contain ASCII compatible characters parsed as either an iterator or slices. Here's a quick fix you can use for utf8 encoding in ``datasets.py:440`` in ``def load_json_stream()`` ```python with open(load_path, 'r', encoding='utf8') as f:         json_file = json.loads(f.read())         for row in json_file:              yield row ``` Tested with ``pretrain_gpt2.py`` which has a dependency on the same data loader.",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
dzk9528,Possible solution for using torch.multiprocessing.spawn,"I am using `torch.multiprocessing.spawn` for another large classification problem and apply the mpu functions to realize 8 world size, 4 data parallel and 2 model parallel training. However, when I try to apply the data loader part, I find out you only load data when `mpu.get_model_parallel_rank() == 0`, how about the other gpu where `mpu.get_model_parallel_rank() != 0`, how will they behave at training? The iteration part in the code is same for all the GPU, this is what confuse me.",2019-12-04T02:03:19Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/21
dzk9528,mpu.vocab_parallel_cross_entropy VS cross entropy,"After reading your code, I have a question: Is `mpu.vocab_parallel_cross_entropy` same as the cross entropy in the general classification problem?",2019-12-03T20:20:49Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/20
lxl910915,Why the backward function of _CopyToModelParallelRegion calls reduce fuction? can somebody share the mathematical proof,"``` class _CopyToModelParallelRegion(torch.autograd.Function):     """"""Pass the input to the model parallel region.""""""          def forward(ctx, input_):         return input_          def backward(ctx, grad_output):         return _reduce(grad_output) ```",2019-11-06T04:00:45Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/19,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
akhileshgotmare,How to train on other corpora besides web/wiki text,"For webtext, you mention the following step: `Merge the contents into one loose json file with 1 json per newline of the format {'text': text, 'url': unique_url}. It is important for the url to be unique.` What would be the best way for training on a different text corpus that exists as a single file without topic/page separators (unlike wikipedia or webtext)? I am splitting such a file manually into N (say = 1000) parts and creating a loose json file with 1000 json objects, 1 json per newline. This approach works for the distributed training too.  However if I feed the new corpus as a single json object `{'text': , 'url': }`, it fails with the following message  ``` > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234 configuring data Traceback (most recent call last):   File ""pretrain_gpt2.py"", line 681, in      main()   File ""pretrain_gpt2.py"", line 620, in main     args.eod_token = get_train_val_test_data(args)   File ""pretrain_gpt2.py"", line 550, in get_train_val_test_data     args)   File ""MegatronLM/configure_data.py"", line 34, in apply     return make_loaders(args)   File ""MegatronLM/configure_data.py"", line 171, in make_loaders     train, tokenizer = data_utils.make_dataset(**data_set_args)   File ""MegatronLM/data_utils/__init__.py"", line 126, in make_dataset     ds = [GPT2Dataset(d, max_seq_len=seq_length) if d is not None else None for d in ds]   File ""MegatronLM/data_utils/__init__.py"", line 126, in      ds = [GPT2Dataset(d, max_seq_len=seq_length) if d is not None else None for d in ds]   File ""MegatronLM/data_utils/datasets.py"", line 479, in __init__     self.init_weighting()   File ""MegatronLM/data_utils/datasets.py"", line 490, in init_weighting     self.weighting = list(accumulate(lens)) TypeError: iteration over a 0d array ``` I'm concerned if this manual splitting impacts the sampling/batching in any way. I'm changing the PATH variable in `MegatronLM/datautils/corpora.py` inside the wikipedia/webtext class to use this new corpus.",2019-11-05T07:38:48Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/18,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Marking as stale. No activity in 60 days.
zhaiyuyong,Why NEGOTIATE_ALLREDUCE is much longer in TensorFlow comparing to PyTorch?,We try to translate this model into tensorflow. but find the same issue  https://github.com/horovod/horovod/issues/1454,2019-10-27T08:22:46Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/17,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Closing due to age.
akhileshgotmare,Rename,for consistency with commands written in the readme file,2019-10-22T06:37:05Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/16
Lepiloff,Error when runinng script pretrain_gpt2_distributed.sh,"When I run  `OMP_NUM_THREADS=10  bash scripts/pretrain_gpt2_distributed.sh` I got an error ``` > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234 configuring data Traceback (most recent call last):   File ""pretrain_gpt2.py"", line 625, in      main()   File ""pretrain_gpt2.py"", line 569, in main     args.eod_token = get_train_val_test_data(args)   File ""pretrain_gpt2.py"", line 515, in get_train_val_test_data     args)   File ""/home/ubuntu/MegatronLM/configure_data.py"", line 34, in apply     return make_loaders(args)   File ""/home/ubuntu/MegatronLM/configure_data.py"", line 170, in make_loaders     train, tokenizer = data_utils.make_dataset(**data_set_args)   File ""/home/ubuntu/MegatronLM/data_utils/__init__.py"", line 114, in make_dataset     ds = [GPT2Dataset(d, max_seq_len=seq_length) if d is not None else None for d in ds]   File ""/home/ubuntu/MegatronLM/data_utils/__init__.py"", line 114, in      ds = [GPT2Dataset(d, max_seq_len=seq_length) if d is not None else None for d in ds]   File ""/home/ubuntu/MegatronLM/data_utils/datasets.py"", line 477, in __init__     self.init_weighting()   File ""/home/ubuntu/MegatronLM/data_utils/datasets.py"", line 487, in init_weighting     self.weighting = list(accumulate(lens)) TypeError: iteration over a 0d array Traceback (most recent call last):   File ""pretrain_gpt2.py"", line 625, in      main()   File ""pretrain_gpt2.py"", line 569, in main     args.eod_token = get_train_val_test_data(args)   File ""pretrain_gpt2.py"", line 515, in get_train_val_test_data     args)   File ""/home/ubuntu/MegatronLM/configure_data.py"", line 34, in apply     return make_loaders(args)   File ""/home/ubuntu/MegatronLM/configure_data.py"", line 170, in make_loaders     train, tokenizer = data_utils.make_dataset(**data_set_args)   File ""/home/ubuntu/MegatronLM/data_utils/__init__.py"", line 114, in make_dataset     ds = [GPT2Dataset(d, max_seq_len=seq_length) if d is not None else None for d in ds]   File ""/home/ubuntu/MegatronLM/data_utils/__init__.py"", line 114, in      ds = [GPT2Dataset(d, max_seq_len=seq_length) if d is not None else None for d in ds]   File ""/home/ubuntu/MegatronLM/data_utils/datasets.py"", line 477, in __init__     self.init_weighting()   File ""/home/ubuntu/MegatronLM/data_utils/datasets.py"", line 487, in init_weighting     self.weighting = list(accumulate(lens)) TypeError: iteration over a 0d array Traceback (most recent call last):   File ""pretrain_gpt2.py"", line 625, in      main()   File ""pretrain_gpt2.py"", line 569, in main     args.eod_token = get_train_val_test_data(args)   File ""pretrain_gpt2.py"", line 515, in get_train_val_test_data     args)   File ""/home/ubuntu/MegatronLM/configure_data.py"", line 34, in apply     return make_loaders(args)   File ""/home/ubuntu/MegatronLM/configure_data.py"", line 170, in make_loaders     train, tokenizer = data_utils.make_dataset(**data_set_args)   File ""/home/ubuntu/MegatronLM/data_utils/__init__.py"", line 114, in make_dataset     ds = [GPT2Dataset(d, max_seq_len=seq_length) if d is not None else None for d in ds]   File ""/home/ubuntu/MegatronLM/data_utils/__init__.py"", line 114, in      ds = [GPT2Dataset(d, max_seq_len=seq_length) if d is not None else None for d in ds]   File ""/home/ubuntu/MegatronLM/data_utils/datasets.py"", line 477, in __init__     self.init_weighting()   File ""/home/ubuntu/MegatronLM/data_utils/datasets.py"", line 487, in init_weighting     self.weighting = list(accumulate(lens)) TypeError: iteration over a 0d array Traceback (most recent call last): Traceback (most recent call last):   File ""pretrain_gpt2.py"", line 625, in    File ""pretrain_gpt2.py"", line 625, in      main()   File ""pretrain_gpt2.py"", line 569, in main Traceback (most recent call last):     main()   File ""pretrain_gpt2.py"", line 569, in main     args.eod_token = get_train_val_test_data(args)     args.eod_token = get_train_val_test_data(args)   File ""pretrain_gpt2.py"", line 515, in get_train_val_test_data   File ""pretrain_gpt2.py"", line 515, in get_train_val_test_data     args)   File ""/home/ubuntu/MegatronLM/configure_data.py"", line 34, in apply     return make_loaders(args)   File ""/home/ubuntu/MegatronLM/configure_data.py"", line 170, in make_loaders     train, tokenizer = data_utils.make_dataset(**data_set_args)   File ""/home/ubuntu/MegatronLM/data_utils/__init__.py"", line 114, in make_dataset     ds = [GPT2Dataset(d, max_seq_len=seq_length) if d is not None else None for d in ds]   File ""/home/ubuntu/MegatronLM/data_utils/__init__.py"", line 114, in      ds = [GPT2Dataset(d, max_seq_len=seq_length) if d is not None else None for d in ds]   File ""/home/ubuntu/MegatronLM/data_utils/datasets.py"", line 477, in __init__     self.init_weighting()   File ""/home/ubuntu/MegatronLM/data_utils/datasets.py"", line 487, in init_weighting     self.weighting = list(accumulate(lens)) TypeError: iteration over a 0d array   File ""pretrain_gpt2.py"", line 625, in      args)   File ""/home/ubuntu/MegatronLM/configure_data.py"", line 34, in apply     return make_loaders(args)   File ""/home/ubuntu/MegatronLM/configure_data.py"", line 170, in make_loaders     main()   File ""pretrain_gpt2.py"", line 569, in main     train, tokenizer = data_utils.make_dataset(**data_set_args)   File ""/home/ubuntu/MegatronLM/data_utils/__init__.py"", line 114, in make_dataset     args.eod_token = get_train_val_test_data(args)   File ""pretrain_gpt2.py"", line 515, in get_train_val_test_data     ds = [GPT2Dataset(d, max_seq_len=seq_length) if d is not None else None for d in ds]   File ""/home/ubuntu/MegatronLM/data_utils/__init__.py"", line 114, in      args)   File ""/home/ubuntu/MegatronLM/configure_data.py"", line 34, in apply     ds = [GPT2Dataset(d, max_seq_len=seq_length) if d is not None else None for d in ds]   File ""/home/ubuntu/MegatronLM/data_utils/datasets.py"", line 477, in __init__     return make_loaders(args)   File ""/home/ubuntu/MegatronLM/configure_data.py"", line 170, in make_loaders     train, tokenizer = data_utils.make_dataset(**data_set_args)   File ""/home/ubuntu/MegatronLM/data_utils/__init__.py"", line 114, in make_dataset     ds = [GPT2Dataset(d, max_seq_len=seq_length) if d is not None else None for d in ds]   File ""/home/ubuntu/MegatronLM/data_utils/__init__.py"", line 114, in      self.init_weighting()     ds = [GPT2Dataset(d, max_seq_len=seq_length) if d is not None else None for d in ds]   File ""/home/ubuntu/MegatronLM/data_utils/datasets.py"", line 477, in __init__   File ""/home/ubuntu/MegatronLM/data_utils/datasets.py"", line 487, in init_weighting     self.init_weighting()   File ""/home/ubuntu/MegatronLM/data_utils/datasets.py"", line 487, in init_weighting     self.weighting = list(accumulate(lens)) TypeError: iteration over a 0d array     self.weighting = list(accumulate(lens)) TypeError: iteration over a 0d array Traceback (most recent call last):   File ""pretrain_gpt2.py"", line 625, in      main()   File ""pretrain_gpt2.py"", line 569, in main     args.eod_token = get_train_val_test_data(args)   File ""pretrain_gpt2.py"", line 515, in get_train_val_test_data     args)   File ""/home/ubuntu/MegatronLM/configure_data.py"", line 34, in apply     return make_loaders(args)   File ""/home/ubuntu/MegatronLM/configure_data.py"", line 170, in make_loaders     train, tokenizer = data_utils.make_dataset(**data_set_args)   File ""/home/ubuntu/MegatronLM/data_utils/__init__.py"", line 114, in make_dataset     ds = [GPT2Dataset(d, max_seq_len=seq_length) if d is not None else None for d in ds]   File ""/home/ubuntu/MegatronLM/data_utils/__init__.py"", line 114, in      ds = [GPT2Dataset(d, max_seq_len=seq_length) if d is not None else None for d in ds]   File ""/home/ubuntu/MegatronLM/data_utils/datasets.py"", line 477, in __init__     self.init_weighting()   File ""/home/ubuntu/MegatronLM/data_utils/datasets.py"", line 487, in init_weighting     self.weighting = list(accumulate(lens)) TypeError: iteration over a 0d array Traceback (most recent call last):   File ""pretrain_gpt2.py"", line 625, in      main()   File ""pretrain_gpt2.py"", line 569, in main     args.eod_token = get_train_val_test_data(args)   File ""pretrain_gpt2.py"", line 515, in get_train_val_test_data     args)   File ""/home/ubuntu/MegatronLM/configure_data.py"", line 34, in apply     return make_loaders(args)   File ""/home/ubuntu/MegatronLM/configure_data.py"", line 170, in make_loaders     train, tokenizer = data_utils.make_dataset(**data_set_args)   File ""/home/ubuntu/MegatronLM/data_utils/__init__.py"", line 114, in make_dataset     ds = [GPT2Dataset(d, max_seq_len=seq_length) if d is not None else None for d in ds]   File ""/home/ubuntu/MegatronLM/data_utils/__init__.py"", line 114, in      ds = [GPT2Dataset(d, max_seq_len=seq_length) if d is not None else None for d in ds]   File ""/home/ubuntu/MegatronLM/data_utils/datasets.py"", line 477, in __init__     self.init_weighting()   File ""/home/ubuntu/MegatronLM/data_utils/datasets.py"", line 487, in init_weighting     self.weighting = list(accumulate(lens)) TypeError: iteration over a 0d array Traceback (most recent call last):   File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main     ""__main__"", mod_spec)   File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code     exec(code, run_globals)   File ""/home/ubuntu/Env/ml/lib/python3.6/sitepackages/torch/distributed/launch.py"", line 246, in      main()   File ""/home/ubuntu/Env/ml/lib/python3.6/sitepackages/torch/distributed/launch.py"", line 242, in main     cmd=cmd) subprocess.CalledProcessError: Command '['/home/ubuntu/Env/ml/bin/python', 'u', 'pretrain_gpt2.py', 'local_rank=7', 'numlayers', '24', 'hiddensize', '1024', 'numattentionheads', '16', 'batchsize', '8', 'seqlength', '1024', 'maxpositionembeddings', '1024', 'trainiters', '320000', 'save', 'checkpoints/gpt2_345m', 'load', 'checkpoints/gpt2_345m', 'resumedataloader', 'traindata', 'wikipedia', 'lazyloader', 'tokenizertype', 'GPT2BPETokenizer', 'cachedir', 'cache', 'split', '949,50,1', 'distributedbackend', 'nccl', 'lr', '0.00015', 'lrdecaystyle', 'cosine', 'weightdecay', '1e2', 'clipgrad', '1.0', 'warmup', '.01', 'checkpointactivations', 'fp16']' returned nonzero exit status 1. ``` Why does this error occur and how to fix it?",2019-10-17T13:48:05Z,stale,closed,0,5,https://github.com/NVIDIA/Megatron-LM/issues/15,Also I have similare error when I run **pretrain_gpt2_model_parallel.sh** Both script using `python m torch.distributed.launch`,It seems your datasets are empty and the actual error is: ```python TypeError: iteration over a 0d array ``` Could you check the paths and make sure you can create a dataset instance?,"I also got this error and found that it was caused when using the default parameters of train/val/test split of 949,50,1 (in the bash script). As the test set only contains a single example when using the default wikipedia dataset, the np array is returned as a non iterable instead of an iterable of a single item. The accumulate function in line 489 of datasets.py is returning the mentioned error.  changing to 939,40,11 solved this issue",Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Closing  error explained by  
kangzhonghua,"1:train loss decrease too faster.2:learning rates did not change after warmup iter,always kept on 1.5e-4. Is it normal phenomenon? ","Thank your very much! I was encounter some question.      1:train loss decrease too faster.      2:learning rates did not change after warmup iter,always kept on 1.5e4.  Is it normal phenomenon? total 10GB chinese corpus ,about 3000000 sample. python3 m torch.distributed.launch \     nnodes 1 \     nproc_per_node 2 \     pretrain_gpt2.py \         numlayers 24 \         hiddensize 1024 \         numattentionheads 16 \         maxpositionembeddings 1024 \         seqlength 1024 \         batchsize 8 \         trainiters 1000000 \         saveinterval 1000 \         save checkpoints/gpt2_345m_hm10g \         load checkpoints/gpt2_345m_hm10g \         tensorboarddir logs/gpt2_345m_hm10g \         resumedataloader \         traindata corpus_data \         lazyloader \         tokenizertype SentencePieceTokenizer \         tokenizerpath data/spm/corpus_bpe_32k.model \         cachedir cache \         split 949,50,1 \         distributedbackend nccl \         lr 0.00015 \         lrdecaystyle cosine \         weightdecay 1e2 \         clipgrad 1.0 \         warmup .01 \         checkpointactivations \         fp16 \ !a_t !b_g_t !b_t !d_l_t !f_t !i_t !l_s !lr !o_t !t_l !v_l !v_ppl",2019-10-12T03:37:59Z,stale,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/14,Marking as stale. No activity in 60 days. Remove stale label or comment or this will be closed in 7 days.,Closing issue due to age
GrahamboJangles,No module named 'apex',"I get this when running `generate_text.sh` ``` Traceback (most recent call last):   File ""generate_samples.py"", line 28, in      from utils import Timers   File ""/content/MegatronLM/utils.py"", line 25, in      from fp16 import FP16_Optimizer   File ""/content/MegatronLM/fp16/__init__.py"", line 15, in      from .fp16util import (   File ""/content/MegatronLM/fp16/fp16util.py"", line 21, in      import mpu   File ""/content/MegatronLM/mpu/__init__.py"", line 35, in      from .layers import ColumnParallelLinear   File ""/content/MegatronLM/mpu/layers.py"", line 28, in      from apex.normalization.fused_layer_norm import FusedLayerNorm as LayerNorm ModuleNotFoundError: No module named 'apex' ```",2019-10-10T04:34:11Z,,closed,0,4,https://github.com/NVIDIA/Megatron-LM/issues/13,It's included in our nvidia docker containers but you can also get it from here,"  Okay, I have installed apex and all of the other dependencies but now I get this: ``` Generate Samples WARNING: No training data specified using world size: 1 and modelparallel size: 1   > using dynamic loss scaling > initializing model parallel with size 1 > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234 100% 1042301/1042301 [00:00 number of parameters on model parallel rank 0: 124475904 WARNING: could not find the metadata file checkpoints/gpt2_345m/latest_checkpointed_iteration.txt      will not load any checkpoints and will start from random Traceback (most recent call last):   File ""generate_samples.py"", line 496, in      main()   File ""generate_samples.py"", line 492, in main     write_and_generate_samples_unconditional(model, tokenizer, args)   File ""generate_samples.py"", line 319, in write_and_generate_samples_unconditional     for datum in generate_samples_unconditional(model, tokenizer, args):   File ""generate_samples.py"", line 295, in generate_samples_unconditional     for token_stream in get_token_stream(model, copy.deepcopy(context_tokens), tokenizer, args):   File ""generate_samples.py"", line 347, in get_token_stream     tokens, attention_mask, position_ids=get_batch(context_tokens_tensor, args)   File ""generate_samples.py"", line 99, in get_batch     args.reset_attention_mask) TypeError: get_masks_and_position_ids() missing 1 required positional argument: 'eod_mask_loss' ``` Do I need to train first? I'm just playing around with this but I'm not sure exactly what it's doing even after reading the documentation. Is this for training GPT2 and BERT or is it style control? Does it somehow blend the two? I'm running this in a Colab doc I made by the way.",Looks like you resolved the apex dependency but are now running into the same issue as: https://github.com/NVIDIA/MegatronLM/issues/23 Iâ€™d close this in favor of the above.  Not sure if this project is actively maintained but the examples do not work due to Issue 23.,Closing as the remaining question is not the original topic and a duplicate of CC(GPT2 generation samples error ) 
great-attraction,Collecting Wikipedia Training Data issues,"Hi! I'm dealing with the work of your library and I have a misunderstanding of the point Collecting Wikipedia Training Data. I run command python WikiExtractor.py json enwikilatestpagesarticles.xml.bz2 and expected get one json file as output ( do I understand correctly that I need an available .json file to start learning the model ?), but got a  folder with many inner folders like AA, AB, AC .. Could you explain what I'm doing wrong and how I can start training a model?",2019-10-07T08:07:34Z,,closed,4,1,https://github.com/NVIDIA/Megatron-LM/issues/12,You need to convert the extracted .json files into a single .json file to pass to `preprocess_data.py` as described here: https://github.com/NVIDIA/MegatronLM/tree/maindatapreprocessing This will process the data into mmap files that can then be used for training.
f-dx,Any plans to release pretrained model?,"Hello, Thank you for your project, are you planning to release pretrained models?",2019-09-20T09:51:36Z,,closed,21,5,https://github.com/NVIDIA/Megatron-LM/issues/11,It is impossible for researchers in Turkey to get even 1% of such hardware to train such models So Please release the trained model so we can experiment on it,Is it possibile to release a pretrained model?,Just to finally put a cap on this... https://github.com/pytorch/fairseq/blob/master/examples/megatron_11b/README.md," I wonder if it fits in 11GB GPU, or maybe a new 24GB RTX one?","As mentioned in the readme, there are some pretrained models available through the NGC catalog in addition to the fairseq provided model above. Closing."
bladedsupernova,Can we get some samples?,"Hi! Out of interest in GPT2 and the Megatron LM, can we get a idea what the code outputs? I.e some output samples of what the tool actually does, instead of having to run it just to see what it can do.",2019-09-17T02:34:50Z,,closed,1,3,https://github.com/NVIDIA/Megatron-LM/issues/10,"Do you mean text output of the trained model?  If so, you can see some sample output in the appendix of a paper we just put up on arXiv: https://arxiv.org/abs/1909.08053.","Thanks, I see them. So these are promptcompletions, correct? So it's GPT2 on steroids haha? Btw how well do you understand Transformers? I've spoke with many freelancers and collaborators and they seemed to not fully grasp it, none could recode the Paper from scratch or explain every atom of the complete thing.","Yes, for the samples in that paper GPT2 is the underlying model (the code in this repo can also do BERT models). After writing this code and training these models from scratch I think those of us working on this project can all say we have a pretty good understanding of transformers. :)  All the information is there in the various papers, but it can take some determination to really understand it all. Now that there are several open source implementations studying the code is a good way to get to the nuts and bolts. Hopefully this code can help a bit!"
raulpuric,added missing valid-data line,,2019-09-12T21:59:24Z,,closed,0,0,https://github.com/NVIDIA/Megatron-LM/issues/9
jaredcasper,Support latest PyTorch RNG state API.,Fixes CC(PyTorch 1.2 support?).,2019-09-11T04:46:38Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/8,LGTM.
jeffra,PyTorch 1.2 support?,"I am seeing an error with PyTorch 1.2, has this been tested with Megatron? Traceback (most recent call last):   File ""pretrain_gpt2.py"", line 752, in      main()   File ""pretrain_gpt2.py"", line 690, in main     set_random_seed(args.seed)   File ""pretrain_gpt2.py"", line 621, in set_random_seed     mpu.model_parallel_cuda_manual_seed(seed)   File ""/data/users/jerasley/MegatronLM/mpu/random.py"", line 166, in model_parallel_cuda_manual_seed     model_parallel_seed)   File ""/data/users/jerasley/MegatronLM/mpu/random.py"", line 99, in add     _set_cuda_rng_state(orig_rng_state)   File ""/data/users/jerasley/MegatronLM/mpu/random.py"", line 49, in _set_cuda_rng_state     _lazy_call(cb)   File ""/usr/local/lib/python3.6/distpackages/torch/cuda/__init__.py"", line 139, in _lazy_call     callable()   File ""/data/users/jerasley/MegatronLM/mpu/random.py"", line 47, in cb     _C._cuda_setRNGState(new_state) AttributeError: module 'torch._C' has no attribute '_cuda_setRNGState' Is it safe in this case to just replace the _C._cuda_setRNGState call with torch.cuda.set_rng_state?",2019-09-10T17:26:29Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/7,"The api for the RNG state has changed in PyTorch 1.2. We will have support for the version in a later release. In the meantime, we suggest using 1.1 version."," Should be fixed now, let us know if the current master doesn't work."
mschrimpf,perplexity too big for gpt2 wikitext evaluation,"When running the wikitext evaluation of gpt2 ``` python evaluate_gpt2.py      validdata wikitext103v1/wiki.test.tokens      loadopenai      hiddensize 768      vocabsize 50257      tokenizertype GPT2BPETokenizer      maxpositionembeddings 1024 ``` the resulting perplexity is `2.9290E+02`  why is the value so extremely high? Here is the console output with logging level DEBUG: ``` Evaluate GPT2 model WARNING: No training data specified using world size: 1 and modelparallel size: 1   > using dynamic loss scaling > initializing model parallel with size 1 > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234 DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443 DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 ""HEAD /models.huggingface.co/bert/gpt2vocab.json HTTP/1.1"" 200 0 DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443 DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 ""HEAD /models.huggingface.co/bert/gpt2merges.txt HTTP/1.1"" 200 0 INFO:data_utils.tokenization_gpt2:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2vocab.json from cache at /braintree/home/msch/.pytorch_pretrained_bert/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71 INFO:data_utils.tokenization_gpt2:loading merges file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2merges.txt from cache at /braintree/home/msch/.pytorch_pretrained_bert/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda wikitext Original Tokens: 270330, Detokenized tokens: 245566 > padded vocab (size: 50257) with 0 dummy tokens (new size: 50257) global rank: 0   ```",2019-09-10T13:45:57Z,,closed,0,5,https://github.com/NVIDIA/Megatron-LM/issues/6,"Hmmm that doesn't seem right.  I just ran this and got the correct result: ```python3 evaluate_gpt2.py modelparallelsize 1        numlayers 12        hiddensize 768        loginterval 100        load anything        evalbatchsize 16        numattentionheads 12        seqlength 1024        maxpositionembeddings 1024        tokenizertype GPT2BPETokenizer        textkey text        distributedbackend nccl        hiddendropout 0.1        attentiondropout 0.1        fp16        overlappingeval 32        cachedir cache  loadopenai  validdata ../gpt2_staging/eval_datasets/wikitext103/wiki.test.tokens``` This gives me ```   validation results on wiki  ``` Also for the future I think you meant to `run scripts/run_gpt2_eval.py`. We just realized there's a line missing from this, but it should work after I patch it in a sec.",I think the arguments you're missing are `numlayers 12 numattentionheads 12`,"Indeed, thank you for clarifying. I was copying the command that `run_gpt2_eval.py` was executing which was missing those arguments as you said.",fixed the script in case you're interested,"Whew I evaluated the GPT2 ``` python evaluate_gpt2.py modelparallelsize=1 numlayers=12 hiddensize=768 vocabsize=50257 loginterval=1000 load=anything evalbatchsize=16 numattentionheads=12 seqlength=1024 maxpositionembeddings=1024 tokenizertype=GPT2BPETokenizer textkey=text distributedbackend=nccl hiddendropout=0.1 attentiondropout=0.1 fp16 overlappingeval=32 cachedir=cache loadopenai validdata=/data2/z00487393/Documents/Datasets/Wikipedia/wikitext2v1/wikitext2/wiki.test.tokens ``` The result is as follows ``` Evaluate GPT2 model WARNING: No training data specified using world size: 1 and modelparallel size: 1   > using dynamic loss scaling > initializing model parallel with size 1 > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234 wikitext Original Tokens: 528983, Detokenized tokens: 245566 > padded vocab (size: 50257) with 0 dummy tokens (new size: 50257) global rank: 0  num_tokenized_tokens: 528983 building GPT2 model ...  > number of parameters: 124439808 loading openai weights model.cpu() Traceback (most recent call last):   File ""/data2/z00487393/Applications/Anaconda/envs/ML_pytorch/lib/python3.6/sitepackages/IPython/core/interactiveshell.py"", line 3326, in run_code     exec(code_obj, self.user_global_ns, self.user_ns)   File """", line 4, in      runfile('/data2/z00487393/Documents/Scripts/PyTorch/Megatron/MegatronLMmaster/evaluate_gpt2.py', args=['modelparallelsize=1', 'numlayers=12', 'hiddensize=768', 'vocabsize', '50257', 'loginterval=1000', 'load=anything', 'evalbatchsize=16', 'numattentionheads=12', 'seqlength=1024', 'maxpositionembeddings=1024', 'tokenizertype=GPT2BPETokenizer', 'textkey=text', 'distributedbackend=nccl', 'hiddendropout=0.1', 'attentiondropout=0.1', 'fp16', 'overlappingeval=32', 'cachedir=cache', 'loadopenai', 'validdata=/data2/z00487393/Documents/Datasets/Wikipedia/wikitext2v1/wikitext2/wiki.test.tokens'], wdir='/data2/z00487393/Documents/Scripts/PyTorch/Megatron/MegatronLMmaster')   File ""/data2/z00487393/.pycharm_helpers/pydev/_pydev_bundle/pydev_umd.py"", line 197, in runfile     pydev_imports.execfile(filename, global_vars, local_vars)   execute the script   File ""/data2/z00487393/.pycharm_helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile     exec(compile(contents+""\n"", file, 'exec'), glob, loc)   File ""/data2/z00487393/Documents/Scripts/PyTorch/Megatron/MegatronLMmaster/evaluate_gpt2.py"", line 574, in      main()   File ""/data2/z00487393/Documents/Scripts/PyTorch/Megatron/MegatronLMmaster/evaluate_gpt2.py"", line 557, in main     gpt2model = GPT2LMHeadModel.from_pretrained(model_path, cache_dir='gpt2_weights')   File ""/data2/z00487393/Applications/Anaconda/envs/ML_pytorch/lib/python3.6/sitepackages/pytorch_pretrained_bert/modeling_gpt2.py"", line 423, in from_pretrained     state_dict = torch.load(resolved_archive_file, map_location='cpu')   File ""/data2/z00487393/Applications/Anaconda/envs/ML_pytorch/lib/python3.6/sitepackages/torch/serialization.py"", line 386, in load     return _load(f, map_location, pickle_module, **pickle_load_args)   File ""/data2/z00487393/Applications/Anaconda/envs/ML_pytorch/lib/python3.6/sitepackages/torch/serialization.py"", line 573, in _load     result = unpickler.load() _pickle.UnpicklingError: invalid load key, '5'. ``` Thank you very much for your guidance."
CatFootPrint,I do not know how to solve it.,"When I run bash scripts/pretrain_bert.sh. I encounter the error as follows: `Traceback (most recent call last):   File ""pretrain_bert.py"", line 581, in      main()   File ""pretrain_bert.py"", line 528, in main     args.tokenizer_num_type_tokens = get_train_val_test_data(args)   File ""pretrain_bert.py"", line 475, in get_train_val_test_data     (train_data, val_data, test_data), tokenizer = data_config.apply(args)   File ""/home/z00487393/Documents/Scripts/TensorFlow/Megatron/MegatronLMmaster/configure_data.py"", line 34, in apply     return make_loaders(args)   File ""/home/z00487393/Documents/Scripts/TensorFlow/Megatron/MegatronLMmaster/configure_data.py"", line 170, in make_loaders     train, tokenizer = data_utils.make_dataset(**data_set_args)   File ""/home/z00487393/Documents/Scripts/TensorFlow/Megatron/MegatronLMmaster/data_utils/__init__.py"", line 101, in make_dataset     pad_token, character_converage, **kwargs)   File ""/home/z00487393/Documents/Scripts/TensorFlow/Megatron/MegatronLMmaster/data_utils/tokenization.py"", line 39, in make_tokenizer     return BertWordPieceTokenizer(model_type, **kwargs)   File ""/home/z00487393/Documents/Scripts/TensorFlow/Megatron/MegatronLMmaster/data_utils/tokenization.py"", line 703, in __init__     self.text_tokenizer.max_len = int(1e12) AttributeError: 'NoneType' object has no attribute 'max_len'` I cannot find the solution. If someone can help me, thanks a lot.",2019-09-04T12:02:46Z,,closed,0,10,https://github.com/NVIDIA/Megatron-LM/issues/5,so that indicates that self.text_tokenizer is None. Which means somehow that downloading the pretrained bert tokenizer failed. There should be some other output from pytorchtransformers in your logs. Could you please share that. I would double check that you have write permissions to the `cachedir` and internet connection to download the models,"You are right. I found that I cannot connect to the Internet. Thus, I downloaded the file, bertbaseuncasedvocab.txt and uploaded it to my server. Thanks very much and I appreciate your suggestion. However, the iteration will be terminated when I run pretrain.py. The errors are given as follows: ``` Traceback (most recent call last):   File """", line 1, in    File ""/home/z00487393/.pycharm_helpers/pydev/_pydev_bundle/pydev_umd.py"", line 198, in runfile     pydev_imports.execfile(filename, global_vars, local_vars)   execute the script   File ""/home/z00487393/.pycharm_helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile     exec(compile(contents+""\n"", file, 'exec'), glob, loc)   File ""/home/z00487393/Documents/Scripts/TensorFlow/Megatron/MegatronLMmaster/pretrain_bert.py"", line 586, in      main()   File ""/home/z00487393/Documents/Scripts/TensorFlow/Megatron/MegatronLMmaster/pretrain_bert.py"", line 564, in main     timers, args)   File ""/home/z00487393/Documents/Scripts/TensorFlow/Megatron/MegatronLMmaster/pretrain_bert.py"", line 370, in train     prefix, val_data_iterator, model, args, timers, False)   File ""/home/z00487393/Documents/Scripts/TensorFlow/Megatron/MegatronLMmaster/pretrain_bert.py"", line 424, in evaluate_and_print_results     args, timers, verbose)   File ""/home/z00487393/Documents/Scripts/TensorFlow/Megatron/MegatronLMmaster/pretrain_bert.py"", line 400, in evaluate     args, timers)   File ""/home/z00487393/Documents/Scripts/TensorFlow/Megatron/MegatronLMmaster/pretrain_bert.py"", line 216, in forward_step     padding_mask = get_batch(data_iterator, timers)   File ""/home/z00487393/Documents/Scripts/TensorFlow/Megatron/MegatronLMmaster/pretrain_bert.py"", line 193, in get_batch     data = next(data_iterator)   File ""/home/z00487393/Applications/Anaconda/anaconda3/envs/ML_for_Pytorch/lib/python3.6/sitepackages/torch/utils/data/dataloader.py"", line 572, in __next__     raise StopIteration StopIteration ``` Unfortunately, I cannot find the reason why the iteration is terminated.",that stack trace isn't very helpful. Maybe try and disable the multithreaded dataloaders by setting `numworkers` to 0. Hopefully that will give us a better idea,"Thanks a lot! I set numworkers=0, but encountered the same problem. Would you please help me find the reason why the program is terminated after 1000 iterations, no matter trainiters=10000 or trainiters=2000. ``` runfile('/home/z00487393/Documents/Scripts/TensorFlow/Megatron/MegatronLMmaster/pretrain_bert.py', args='numlayers=24 hiddensize=1024 numattentionheads=16 batchsize=1 seqlength=512 maxpredsperseq=80 maxpositionembeddings=512 trainiters=20000 save=checkpoints/bert_345m load=checkpoints/bert_345m traindata=wikipedia tokenizertype=BertWordPieceTokenizer tokenizermodeltype=bertlargeuncased cachedir=cache split=949,50,1 distributedbackend=nccl lr=0.0001 lrdecaystyle=linear lrdecayiters=990000 weightdecay=1e2 clipgrad=1.0 warmup=.01 numworkers=0', wdir='/home/z00487393/Documents/Scripts/TensorFlow/Megatron/MegatronLMmaster') using world size: 1 and modelparallel size: 1   > using dynamic loss scaling > initializing model parallel with size 1 Pretrain BERT model arguments:   pretrained_bert .............. False   attention_dropout ............ 0.1   num_attention_heads .......... 16   hidden_size .................. 1024   intermediate_size ............ None   num_layers ................... 24   layernorm_epsilon ............ 1e05   hidden_dropout ............... 0.1   max_position_embeddings ...... 512   vocab_size ................... 30522   deep_init .................... False   make_vocab_size_divisible_by . 128   fp16 ......................... False   fp32_embedding ............... False   fp32_layernorm ............... False   fp32_tokentypes .............. False   fp32_allreduce ............... False   hysteresis ................... 2   loss_scale ................... None   loss_scale_window ............ 1000   min_scale .................... 1   batch_size ................... 1   weight_decay ................. 0.01   checkpoint_activations ....... False   checkpoint_num_layers ........ 1   clip_grad .................... 1.0   train_iters .................. 20000   log_interval ................. 100   exit_interval ................ None   seed ......................... 1234   reset_position_ids ........... False   reset_attention_mask ......... False   lr_decay_iters ............... 990000   lr_decay_style ............... linear   lr ........................... 0.0001   warmup ....................... 0.01   save ......................... checkpoints/bert_345m   save_interval ................ 5000   no_save_optim ................ False   no_save_rng .................. False   load ......................... checkpoints/bert_345m   no_load_optim ................ False   no_load_rng .................. False   finetune ..................... False   resume_dataloader ............ False   distributed_backend .......... nccl   local_rank ................... None   eval_batch_size .............. None   eval_iters ................... 100   eval_interval ................ 1000   eval_seq_length .............. None   eval_max_preds_per_seq ....... None   overlapping_eval ............. 32   cloze_eval ................... False   eval_hf ...................... False   load_openai .................. False   temperature .................. 1.0   top_p ........................ 0.0   top_k ........................ 0   out_seq_length ............... 256   model_parallel_size .......... 1   shuffle ...................... False   train_data ................... ['wikipedia']   use_npy_data_loader .......... False   train_data_path ..............    val_data_path ................    test_data_path ...............    input_data_sizes_file ........ sizes.txt   delim ........................ ,   text_key ..................... sentence   eval_text_key ................ None   valid_data ................... None   split ........................ 949,50,1   test_data .................... None   lazy_loader .................. False   loose_json ................... False   presplit_sentences ........... False   num_workers .................. 0   tokenizer_model_type ......... bertlargeuncased   tokenizer_path ............... tokenizer.model   tokenizer_type ............... BertWordPieceTokenizer   cache_dir .................... cache   use_tfrecords ................ False   seq_length ................... 512   max_preds_per_seq ............ 80   cuda ......................... True   rank ......................... 0   world_size ................... 1   dynamic_loss_scale ........... True > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234 configuring data loading BertWordPieceTokenizer ( bertlargeuncased ) from cache_dir  cache loaded bertlargeuncased [nltk_data] Error loading punkt:  ================================================== [10439 71383 30850 43517 72275 48371 45416  3195  6762  1959 41995 55461   6631 36412 76058 17788 50721 87317 20968 61507  6800 12890 29741 33757    987  6022 47041 16967 22031  1830   102   794    77 39569  4554] ================================================== [nltk_data] Error loading punkt:  ================================================== [2058] ================================================== [nltk_data] Error loading punkt:  ================================================== [8584] ================================================== > padded vocab (size: 30522) with 70 dummy tokens (new size: 30592) building BERT model ...  > number of parameters on model parallel rank 0: 336297858 learning rate decaying linear WARNING: could not find the metadata file checkpoints/bert_345m/latest_checkpointed_iteration.txt      will not load any checkpoints and will start from random  iteration      100/   20000  data loader: 40.99 Traceback (most recent call last):   File """", line 1, in    File ""/home/z00487393/.pycharm_helpers/pydev/_pydev_bundle/pydev_umd.py"", line 198, in runfile     pydev_imports.execfile(filename, global_vars, local_vars)   execute the script   File ""/home/z00487393/.pycharm_helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile     exec(compile(contents+""\n"", file, 'exec'), glob, loc)   File ""/home/z00487393/Documents/Scripts/TensorFlow/Megatron/MegatronLMmaster/pretrain_bert.py"", line 581, in      main()   File ""/home/z00487393/Documents/Scripts/TensorFlow/Megatron/MegatronLMmaster/pretrain_bert.py"", line 559, in main     timers, args)   File ""/home/z00487393/Documents/Scripts/TensorFlow/Megatron/MegatronLMmaster/pretrain_bert.py"", line 365, in train     prefix, val_data_iterator, model, args, timers, False)   File ""/home/z00487393/Documents/Scripts/TensorFlow/Megatron/MegatronLMmaster/pretrain_bert.py"", line 419, in evaluate_and_print_results     args, timers, verbose)   File ""/home/z00487393/Documents/Scripts/TensorFlow/Megatron/MegatronLMmaster/pretrain_bert.py"", line 395, in evaluate     args, timers)   File ""/home/z00487393/Documents/Scripts/TensorFlow/Megatron/MegatronLMmaster/pretrain_bert.py"", line 211, in forward_step     padding_mask = get_batch(data_iterator, timers)   File ""/home/z00487393/Documents/Scripts/TensorFlow/Megatron/MegatronLMmaster/pretrain_bert.py"", line 188, in get_batch     data = next(data_iterator)   File ""/home/z00487393/Applications/Anaconda/anaconda3/envs/ML_for_Pytorch/lib/python3.6/sitepackages/torch/utils/data/dataloader.py"", line 559, in __next__     indices = next(self.sample_iter)   may raise StopIteration StopIteration ``` I appreciate your suggestion and thanks a lot!","> that stack trace isn't very helpful. Maybe try and disable the multithreaded dataloaders by setting `numworkers` to 0. Hopefully that will give us a better idea Unfortunately, setting num_workers==0 does not work. Would you please help me find the reason why iterations are terminated for 1000 iterations. Maybe the functions for loading data causes this problem, I think. Thank you very much for your guidance. Best wishes for you!","Dear Raul: Thanks a lot for your guidance! However, I set numworkers=0, but encountered the same problem. Would you please help me find the reason why the program is terminated after 1000 iterations, no matter trainiters=10000 or trainiters=2000. runfile('/home/z00487393/Documents/Scripts/TensorFlow/Megatron/MegatronLMmaster/pretrain_bert.py', args='numlayers=24 hiddensize=1024 numattentionheads=16 batchsize=1 seqlength=512 maxpredsperseq=80 maxpositionembeddings=512 trainiters=20000 save=checkpoints/bert_345m load=checkpoints/bert_345m traindata=wikipedia tokenizertype=BertWordPieceTokenizer tokenizermodeltype=bertlargeuncased cachedir=cache split=949,50,1 distributedbackend=nccl lr=0.0001 lrdecaystyle=linear lrdecayiters=990000 weightdecay=1e2 clipgrad=1.0 warmup=.01 numworkers=0', wdir='/home/z00487393/Documents/Scripts/TensorFlow/Megatron/MegatronLMmaster') using world size: 1 and modelparallel size: 1   > using dynamic loss scaling > initializing model parallel with size 1 Pretrain BERT model arguments:   pretrained_bert .............. False   attention_dropout ............ 0.1   num_attention_heads .......... 16   hidden_size .................. 1024   intermediate_size ............ None   num_layers ................... 24   layernorm_epsilon ............ 1e05   hidden_dropout ............... 0.1   max_position_embeddings ...... 512   vocab_size ................... 30522   deep_init .................... False   make_vocab_size_divisible_by . 128   fp16 ......................... False   fp32_embedding ............... False   fp32_layernorm ............... False   fp32_tokentypes .............. False   fp32_allreduce ............... False   hysteresis ................... 2   loss_scale ................... None   loss_scale_window ............ 1000   min_scale .................... 1   batch_size ................... 1   weight_decay ................. 0.01   checkpoint_activations ....... False   checkpoint_num_layers ........ 1   clip_grad .................... 1.0   train_iters .................. 20000   log_interval ................. 100   exit_interval ................ None   seed ......................... 1234   reset_position_ids ........... False   reset_attention_mask ......... False   lr_decay_iters ............... 990000   lr_decay_style ............... linear   lr ........................... 0.0001   warmup ....................... 0.01   save ......................... checkpoints/bert_345m   save_interval ................ 5000   no_save_optim ................ False   no_save_rng .................. False   load ......................... checkpoints/bert_345m   no_load_optim ................ False   no_load_rng .................. False   finetune ..................... False   resume_dataloader ............ False   distributed_backend .......... nccl   local_rank ................... None   eval_batch_size .............. None   eval_iters ................... 100   eval_interval ................ 1000   eval_seq_length .............. None   eval_max_preds_per_seq ....... None   overlapping_eval ............. 32   cloze_eval ................... False   eval_hf ...................... False   load_openai .................. False   temperature .................. 1.0   top_p ........................ 0.0   top_k ........................ 0   out_seq_length ............... 256   model_parallel_size .......... 1   shuffle ...................... False   train_data ................... ['wikipedia']   use_npy_data_loader .......... False   train_data_path ..............    val_data_path ................    test_data_path ...............    input_data_sizes_file ........ sizes.txt   delim ........................ ,   text_key ..................... sentence   eval_text_key ................ None   valid_data ................... None   split ........................ 949,50,1   test_data .................... None   lazy_loader .................. False   loose_json ................... False   presplit_sentences ........... False   num_workers .................. 0   tokenizer_model_type ......... bertlargeuncased   tokenizer_path ............... tokenizer.model   tokenizer_type ............... BertWordPieceTokenizer   cache_dir .................... cache   use_tfrecords ................ False   seq_length ................... 512   max_preds_per_seq ............ 80   cuda ......................... True   rank ......................... 0   world_size ................... 1   dynamic_loss_scale ........... True > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234 configuring data loading BertWordPieceTokenizer ( bertlargeuncased ) from cache_dir  cache loaded bertlargeuncased [nltk_data] Error loading punkt:  ================================================== [10439 71383 30850 43517 72275 48371 45416  3195  6762  1959 41995 55461   6631 36412 76058 17788 50721 87317 20968 61507  6800 12890 29741 33757    987  6022 47041 16967 22031  1830   102   794    77 39569  4554] ================================================== [nltk_data] Error loading punkt:  ================================================== [2058] ================================================== [nltk_data] Error loading punkt:  ================================================== [8584] ================================================== > padded vocab (size: 30522) with 70 dummy tokens (new size: 30592) building BERT model ...  > number of parameters on model parallel rank 0: 336297858 learning rate decaying linear WARNING: could not find the metadata file checkpoints/bert_345m/latest_checkpointed_iteration.txt      will not load any checkpoints and will start from random  iteration      100/   20000  data loader: 40.99 Traceback (most recent call last):   File """", line 1, in    File ""/home/z00487393/.pycharm_helpers/pydev/_pydev_bundle/pydev_umd.py"", line 198, in runfile     pydev_imports.execfile(filename, global_vars, local_vars)   execute the script   File ""/home/z00487393/.pycharm_helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile     exec(compile(contents+""\n"", file, 'exec'), glob, loc)   File ""/home/z00487393/Documents/Scripts/TensorFlow/Megatron/MegatronLMmaster/pretrain_bert.py"", line 581, in      main()   File ""/home/z00487393/Documents/Scripts/TensorFlow/Megatron/MegatronLMmaster/pretrain_bert.py"", line 559, in main     timers, args)   File ""/home/z00487393/Documents/Scripts/TensorFlow/Megatron/MegatronLMmaster/pretrain_bert.py"", line 365, in train     prefix, val_data_iterator, model, args, timers, False)   File ""/home/z00487393/Documents/Scripts/TensorFlow/Megatron/MegatronLMmaster/pretrain_bert.py"", line 419, in evaluate_and_print_results     args, timers, verbose)   File ""/home/z00487393/Documents/Scripts/TensorFlow/Megatron/MegatronLMmaster/pretrain_bert.py"", line 395, in evaluate     args, timers)   File ""/home/z00487393/Documents/Scripts/TensorFlow/Megatron/MegatronLMmaster/pretrain_bert.py"", line 211, in forward_step     padding_mask = get_batch(data_iterator, timers)   File ""/home/z00487393/Documents/Scripts/TensorFlow/Megatron/MegatronLMmaster/pretrain_bert.py"", line 188, in get_batch     data = next(data_iterator)   File ""/home/z00487393/Applications/Anaconda/anaconda3/envs/ML_for_Pytorch/lib/python3.6/sitepackages/torch/utils/data/dataloader.py"", line 559, in __next__     indices = next(self.sample_iter)   may raise StopIteration StopIteration Would you please help me find the reason why iterations are terminated for 1000 iterations. Maybe the functions for loading data causes this problem, I think. Best wishes for you! I appreciate it very much for your kindly guidance.  At 20190906 04:18:25, ""Raul Puri""  wrote: that stack trace isn't very helpful. Maybe try and disable the multithreaded dataloaders by setting numworkers to 0. Hopefully that will give us a better idea â€” You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub, or mute the thread.","> that stack trace isn't very helpful. Maybe try and disable the multithreaded dataloaders by setting `numworkers` to 0. Hopefully that will give us a better idea Fortunately, when I choose a larger training set, the the number of iteration of pretrain_bert.sh is much larger than 1000. However, I find that when the program finishes loading training data, it will stop iteration, which is shown as follow. I am not sure whether the program executes correctly or not. Best wishes for you. Thanks a lot for your kindly guidance. ```   successfully saved checkpoints/bert_345m/iter_0050000/mp_rank_00/model_optim_rng.pt Traceback (most recent call last):   File ""pretrain_bert.py"", line 584, in      main()   File ""pretrain_bert.py"", line 562, in main     timers, args)   File ""pretrain_bert.py"", line 365, in train     prefix, val_data_iterator, model, args, timers, False)   File ""pretrain_bert.py"", line 419, in evaluate_and_print_results     args, timers, verbose)   File ""pretrain_bert.py"", line 395, in evaluate     args, timers)   File ""pretrain_bert.py"", line 212, in forward_step     padding_mask = get_batch(data_iterator, timers)   File ""pretrain_bert.py"", line 189, in get_batch     data = next(data_iterator)   File ""/data2/z00487393/Applications/Anaconda/envs/ML_pytorch/lib/python3.6/sitepackages/torch/utils/data/dataloader.py"", line 794, in __next__     raise StopIteration StopIteration ```",Are you training with the full dataset? Based on your results it appears to me that you just don't have enough data? We typically do not run into this problem even with 1 million iterations @ batch 256.  Can you share the size of your wikipedia dataset in terms of raw GB and number of lines `wc l `,Sorry for late.  You are right. I adopted the whole Wikipedia data set and the programs works without warning. Thank you very much! Best Wishes for you.,"This appears to be solved, reopen if not."
manjunaths,Note that as of now you need to have PySOL cloned to the directory here before building the container.,When you say PySOL do you mean https://github.com/shlomif/PySolFC ? A cursory Google search of PySOL didn't yield anything meaningful.,2019-08-23T05:03:09Z,,closed,0,2,https://github.com/NVIDIA/Megatron-LM/issues/4,Sorry PySOL actually refers to our renamed profiling utility for pytorch called PyProf. This should be automatically installed in the docker container with the apex installation. Just ignore it. Let us know if you still can't install the docker container.,closing this
harkous,Compatibility with pytorch-transformers for fine-tuning ,"Hi, Thanks for the great package! I wanted to check about the compatibility of the trained GPT2 model/tokenizer with the pytorchtransformers package. Is it possible that, with a few changes, the trained model can be imported using that package, in order to perform additional finetuning there with different heads for example? I understand that there are some config files expected by that package, so I'm assuming these can be added. But I'm interested in knowing about the compatibility of the model/tokenizer mainly. Thanks!",2019-08-21T15:13:54Z,,closed,0,6,https://github.com/NVIDIA/Megatron-LM/issues/3,"Hi the config files are the same, however we've had to change some layers' structure and names to account for model parallelism. We wrote a helper function to allow us to load huggingface/openai weights into our model for debugging purposes. It may work in the reverse direction, but we haven't tested either direction in a while. Please shout if it doesn't work for you. Note this will not work for model parallel weights, we're still working on our serialization strategies to ship/port model parallel weights. Raul",Thanks a lot Raul. I will try the function you mentioned and get back on that.,Closing this since it has been a while; I hope that worked out for you! If you have more questions please reopen or start a new issue., Any chance on news on the parallel loading part? I want to import gpt2 pretrained weights and use Megatron to train in parallel ," Still in the plans, but not a priority so won't hit the repo for another month or two. Note that we plan to only support loading weights, not optimizer state or anything.",  I found that the helper function is deleted from the master. I'm also interested in loading huggingface model weights and continue pretraining using MegatronLM. Do you have current suggestions? Found the utils.py from the history https://github.com/NVIDIA/MegatronLM/blob/c882ac61182d423a89d21b453251a20fb7271a67/megatron/utils.py
yaroslavvb,rename confusing arg,WORLD_SIZE usually refers to total number of gpus while nproc_per_node should be set to number of gpus per node,2019-04-24T17:03:14Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/2,These variables should be named correctly in our latest version so this has been applied manually.  Thanks for your help!
raulpuric,Faster dataloader merge,,2019-04-23T20:57:51Z,,closed,0,1,https://github.com/NVIDIA/Megatron-LM/issues/1,LGTM
