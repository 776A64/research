yi,Return arrays from `ArrayImpl._check_and_rearrange`.,"Return arrays from `ArrayImpl._check_and_rearrange`. This is in preparation for a larger change, so that input buffers can be checked before Array creation in XLA and the user gets more helpful JAX error messages instead of XLA errors. Reverts 3b2410f77cdb0acc6951e1770c1229e6689b7409",2025-02-04T21:43:45Z,,open,0,0,https://github.com/jax-ml/jax/issues/26316
yi,jax.distributed with slurm/mpi not working on rocm,"on the rocmplatfrom, `jax.distributed` with the clusterautodetection mechanisms is not working as it is with cuda. Whereas in `jax._src.distributed.initialize()` the visible_devices are set properly for both 'cuda' and 'rocm', in the xla_bridge, only the cuda_visible_devices are queried upon client creation: https://github.com/jaxml/jax/blob/124e123946d09661b6b28dcd82a618be93cb132c/jax/_src/xla_bridge.pyL643 This results in nodelocal processes seeing all local devices which can lead to OOM errors in case of oversubscription or hangs or errors in rccl communication, depending on the specific setup. Thus atm only settings with a single process per node are possible with rocm, whereas with cuda also one process per device works. To my understanding and tests this could be easily fixed by querying also the 'jax_rocm_visible_devices' in xla_bridge and I would be happy to provide a PR in case the current behavior is not the intended one? According to a small set of simple tests, also the whole gpumock setup that is enclosed in the relevant context linked above works with rocm, so the changes might really be minimal.",2025-02-04T08:48:00Z,AMD GPU,open,0,2,https://github.com/jax-ml/jax/issues/26298,"Looking over the block of code there that is setting up the mock devices settings, yeah, I don't think this would be too big of change to make. I think it comes down to changing the block to also run if the plugin is rocm, and checking _ROCM_VISIBLE_DEVICES instead of CUDA_VISIBLE_DEVICES and then running through the same mock settings block. I am not sure if anyone has tried running one process per device yet with jax+rocm, but I don't really see any reason it wouldn't work. I don't really have a slurm/mpi cluster on hand to easily test this, so if you wanted to build the change and verify it on your environment and push it up as a PR that would be helpful. Thanks!",Thanks for looking into this issue! Please find the changes in the PR mentioned above. I tested with A100 and MI300 with slurm and rocm is now consistent with cuda wrt devices being visible to processes.
yi,[JAX] Add a test verifying the behavior of module-level state accessed by colocated Python,"[JAX] Add a test verifying the behavior of modulelevel state accessed by colocated Python A new test verifies that * Python modulelevel variables can be created/set and read from a colocated Python function * Python modulelevel variables are not pickled on the controller (JAX) or sent to executors via pickling An API for defining userdefined state and accessing it from multiple colocated Python functions (i.e., object support) will be added later. That will be a recommended way to express userdefined state. The capability of accessing Python module variables is still crucial because a lot of Python code (including JAX) requires this behavior to implement caching.",2025-02-04T03:56:29Z,,open,0,0,https://github.com/jax-ml/jax/issues/26293
yi,"[JAX][DOC] memory_kind, with_memory_kind and out_shardings","Add documentation for the `memory_kind` parameter in sharding, the `with_memory_kind` method, and the `out_shardings` parameter for specifying output sharding in the {func}`jax.jit` function.",2025-02-03T23:22:35Z,,open,0,0,https://github.com/jax-ml/jax/issues/26285
yi,TSAN race in //tests:export_back_compat_test_cpu under free-threading," Description This 313t freethreading CI run: https://github.com/jaxml/jax/actions/runs/13111654401/job/36576699494?pr=26261 contains this race in MLIR code: ``` WARNING: ThreadSanitizer: data race (pid=57866)   Read of size 8 at 0x729000596b98 by thread T64 (mutexes: read M0): CC(未找到相关数据) (anonymous namespace)::ParametricStorageUniquer::StorageKeyInfo::isEqual((anonymous namespace)::ParametricStorageUniquer::HashedStorage const&, (anonymous namespace)::ParametricStorageUniquer::HashedStorage const&) /proc/self/cwd/external/llvmproject/mlir/lib/Support/StorageUniquer.cpp:67:18 (libjaxlib_mlir_capi.so+0x34173cb) (BuildId: 4a2d544bd6db4a8210062f683bf6bb298c5bbcf8) CC(Python 3 compatibility issues) (anonymous namespace)::ParametricStorageUniquer::StorageKeyInfo::isEqual((anonymous namespace)::ParametricStorageUniquer::LookupKey const&, (anonymous namespace)::ParametricStorageUniquer::HashedStorage const&) /proc/self/cwd/external/llvmproject/mlir/lib/Support/StorageUniquer.cpp:70:11 (libjaxlib_mlir_capi.so+0x34173cb) CC(Explicit tuples are not valid function parameters in Python 3) bool llvm::DenseMapBase>, (anonymous namespace)::ParametricStorageUniquer::HashedStorage, llvm::detail::DenseSetEmpty, (anonymous namespace)::ParametricStorageUniquer::StorageKeyInfo, llvm::detail::DenseSetPair>::LookupBucketFor((anonymous namespace)::ParametricStorageUniquer::LookupKey const&, llvm::detail::DenseSetPair*&) /proc/self/cwd/external/llvmproject/llvm/include/llvm/ADT/DenseMap.h:654:11 (libjaxlib_mlir_capi.so+0x34173cb) CC(Undefined name: from ..core import JaxTuple) std::pair, false>, bool> llvm::DenseMapBase>, (anonymous namespace)::ParametricStorageUniquer::HashedStorage, llvm::detail::DenseSetEmpty, (anonymous namespace)::ParametricStorageUniquer::StorageKeyInfo, llvm::detail::DenseSetPair>::insert_as(std::pair&&, (anonymous namespace)::ParametricStorageUniquer::LookupKey const&) /proc/self/cwd/external/llvmproject/llvm/include/llvm/ADT/DenseMap.h:280:9 (libjaxlib_mlir_capi.so+0x34173cb) CC(Undefined name: from six.moves import xrange) std::pair>, (anonymous namespace)::ParametricStorageUniquer::StorageKeyInfo>::Iterator, bool> llvm::detail::DenseSetImpl>, (anonymous namespace)::ParametricStorageUniquer::StorageKeyInfo>::insert_as((anonymous namespace)::ParametricStorageUniquer::HashedStorage&&, (anonymous namespace)::ParametricStorageUniquer::LookupKey const&) /proc/self/cwd/external/llvmproject/llvm/include/llvm/ADT/DenseSet.h:232:19 (libjaxlib_mlir_capi.so+0x34173cb) CC(Building on OSX with CUDA) (anonymous namespace)::ParametricStorageUniquer::getOrCreateUnsafe((anonymous namespace)::ParametricStorageUniquer::Shard&, (anonymous namespace)::ParametricStorageUniquer::LookupKey&, llvm::function_ref) /proc/self/cwd/external/llvmproject/mlir/lib/Support/StorageUniquer.cpp:95:37 (libjaxlib_mlir_capi.so+0x3414f71) (BuildId: 4a2d544bd6db4a8210062f683bf6bb298c5bbcf8) CC(Made a shim to handle configuration without having absl parse flags) (anonymous namespace)::ParametricStorageUniquer::getOrCreate(bool, unsigned int, llvm::function_ref, llvm::function_ref) /proc/self/cwd/external/llvmproject/mlir/lib/Support/StorageUniquer.cpp:140:14 (libjaxlib_mlir_capi.so+0x3414f71) CC(Quickish check) mlir::detail::StorageUniquerImpl::getOrCreate(mlir::TypeID, unsigned int, llvm::function_ref, llvm::function_ref) /proc/self/cwd/external/llvmproject/mlir/lib/Support/StorageUniquer.cpp:270:27 (libjaxlib_mlir_capi.so+0x3414f71) CC(Quickish check) mlir::StorageUniquer::getParametricStorageTypeImpl(mlir::TypeID, unsigned int, llvm::function_ref, llvm::function_ref) /proc/self/cwd/external/llvmproject/mlir/lib/Support/StorageUniquer.cpp:375:16 (libjaxlib_mlir_capi.so+0x3414b4f) (BuildId: 4a2d544bd6db4a8210062f683bf6bb298c5bbcf8) CC(Adding quickstart notebook, and corresponding gitignore rules) mlir::detail::StringAttrStorage* mlir::StorageUniquer::get(llvm::function_ref, mlir::TypeID, llvm::StringRef&&, mlir::NoneType&&) /proc/self/cwd/external/llvmproject/mlir/include/mlir/Support/StorageUniquer.h:218:9 (libjaxlib_mlir_capi.so+0x33bd378) (BuildId: 4a2d544bd6db4a8210062f683bf6bb298c5bbcf8) CC([JAX] Change semantics of dtype promotion to just call numpy.result_type.) std::enable_if::value, mlir::StringAttr>::type mlir::detail::AttributeUniquer::getWithTypeID(mlir::MLIRContext*, mlir::TypeID, llvm::StringRef&&, mlir::NoneType&&) /proc/self/cwd/external/llvmproject/mlir/include/mlir/IR/AttributeSupport.h:239:39 (libjaxlib_mlir_capi.so+0x33bd378) CC(Split out `jax` and `jaxlib` packages) mlir::StringAttr mlir::detail::AttributeUniquer::get(mlir::MLIRContext*, llvm::StringRef&&, mlir::NoneType&&) /proc/self/cwd/external/llvmproject/mlir/include/mlir/IR/AttributeSupport.h:220:12 (libjaxlib_mlir_capi.so+0x33bd378) CC(Update the quickstart notebook.) mlir::StringAttr mlir::detail::StorageUserBase::get(mlir::MLIRContext*, llvm::StringRef&&, mlir::NoneType&&) /proc/self/cwd/external/llvmproject/mlir/include/mlir/IR/StorageUniquerSupport.h:181:12 (libjaxlib_mlir_capi.so+0x33bd378) CC(Fixing logo size so resize is not required) mlir::StringAttr::get(mlir::MLIRContext*, llvm::Twine const&) /proc/self/cwd/external/llvmproject/mlir/lib/IR/BuiltinAttributes.cpp:275:10 (libjaxlib_mlir_capi.so+0x33bd378) CC(Add copyright notice to quickstart notebook.) mlir::Operation::setAttr(llvm::StringRef, mlir::Attribute) /proc/self/cwd/external/llvmproject/mlir/include/mlir/IR/Operation.h:594:13 (libjaxlib_mlir_capi.so+0x25d0e57) (BuildId: 4a2d544bd6db4a8210062f683bf6bb298c5bbcf8) CC(rename in_bdims, out_bdims > in_axes, out_axes) mlirOperationSetAttributeByName /proc/self/cwd/external/llvmproject/mlir/lib/CAPI/IR/IR.cpp:683:15 (libjaxlib_mlir_capi.so+0x25d0e57) CC(Add wheelbuilding scripts) (anonymous namespace)::PyOpAttributeMap::dunderSetItem(std::__cxx11::basic_string, std::allocator> const&, mlir::python::PyAttribute const&) /proc/self/cwd/external/llvmproject/mlir/lib/Bindings/Python/IRCore.cpp:2634:5 (_mlir.so+0xc6b97) (BuildId: 15beaf78f4e0ba2c1cb26690729f5d8e660c2f73) CC(Implement np.repeat for scalar repeats.) void nanobind::cpp_function_def, std::allocator> const&, mlir::python::PyAttribute const&, nanobind::scope, nanobind::name, nanobind::is_method>(void ((anonymous namespace)::PyOpAttributeMap::*)(std::__cxx11::basic_string, std::allocator> const&, mlir::python::PyAttribute const&), nanobind::scope const&, nanobind::name const&, nanobind::is_method const&)::'lambda'((anonymous namespace)::PyOpAttributeMap*, std::__cxx11::basic_string, std::allocator> const&, mlir::python::PyAttribute const&)::operator()((anonymous namespace)::PyOpAttributeMap*, std::__cxx11::basic_string, std::allocator> const&, mlir::python::PyAttribute const&) const /proc/self/cwd/external/nanobind/include/nanobind/nb_func.h:376:20 (_mlir.so+0xc7685) (BuildId: 15beaf78f4e0ba2c1cb26690729f5d8e660c2f73) CC(Populate readme) _object* nanobind::detail::func_create, std::allocator> const&, mlir::python::PyAttribute const&, nanobind::scope, nanobind::name, nanobind::is_method>(void ((anonymous namespace)::PyOpAttributeMap::*)(std::__cxx11::basic_string, std::allocator> const&, mlir::python::PyAttribute const&), nanobind::scope const&, nanobind::name const&, nanobind::is_method const&)::'lambda'((anonymous namespace)::PyOpAttributeMap*, std::__cxx11::basic_string, std::allocator> const&, mlir::python::PyAttribute const&), void, (anonymous namespace)::PyOpAttributeMap*, std::__cxx11::basic_string, std::allocator> const&, mlir::python::PyAttribute const&, 0ul, 1ul, 2ul, nanobind::scope, nanobind::name, nanobind::is_method>((anonymous namespace)::PyOpAttributeMap&&, void (*)(nanobind::scope, nanobind::name, nanobind::is_method), std::integer_sequence, nanobind::scope const&, nanobind::name const&, nanobind::is_method const&)::'lambda'(void*, _object**, unsigned char*, nanobind::rv_policy, nanobind::detail::cleanup_list*)::operator()(void*, _object**, unsigned char*, nanobind::rv_policy, nanobind::detail::cleanup_list*) const /proc/self/cwd/external/nanobind/include/nanobind/nb_func.h:264:13 (_mlir.so+0xc7685) CC(Notebook showing how to write gufuncs with vmap) _object* nanobind::detail::func_create, std::allocator> const&, mlir::python::PyAttribute const&, nanobind::scope, nanobind::name, nanobind::is_method>(void ((anonymous namespace)::PyOpAttributeMap::*)(std::__cxx11::basic_string, std::allocator> const&, mlir::python::PyAttribute const&), nanobind::scope const&, nanobind::name const&, nanobind::is_method const&)::'lambda'((anonymous namespace)::PyOpAttributeMap*, std::__cxx11::basic_string, std::allocator> const&, mlir::python::PyAttribute const&), void, (anonymous namespace)::PyOpAttributeMap*, std::__cxx11::basic_string, std::allocator> const&, mlir::python::PyAttribute const&, 0ul, 1ul, 2ul, nanobind::scope, nanobind::name, nanobind::is_method>((anonymous namespace)::PyOpAttributeMap&&, void (*)(nanobind::scope, nanobind::name, nanobind::is_method), std::integer_sequence, nanobind::scope const&, nanobind::name const&, nanobind::is_method const&)::'lambda'(void*, _object**, unsigned char*, nanobind::rv_policy, nanobind::detail::cleanup_list*)::__invoke(void*, _object**, unsigned char*, nanobind::rv_policy, nanobind::detail::cleanup_list*) /proc/self/cwd/external/nanobind/include/nanobind/nb_func.h:219:14 (_mlir.so+0xc7685) CC(Fix link in gufuncs notebook) nanobind::detail::nb_func_vectorcall_simple(_object*, _object* const*, unsigned long, _object*) /proc/self/cwd/external/nanobind/src/nb_func.cpp:915:26 (_mlir.so+0x16e312) (BuildId: 15beaf78f4e0ba2c1cb26690729f5d8e660c2f73) CC(Typo) _PyObject_VectorcallTstate /__w/jax/jax/cpython/./Include/internal/pycore_call.h:168:11 (python3.13+0x2f4627) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(differention > differentiation) vectorcall_unbound /__w/jax/jax/cpython/Objects/typeobject.c:2572:12 (python3.13+0x2f4627) CC(Typo, Python parens) vectorcall_method /__w/jax/jax/cpython/Objects/typeobject.c:2603:24 (python3.13+0x2f4627) CC(attempt to centerjustify the jax logo in readme) slot_mp_ass_subscript /__w/jax/jax/cpython/Objects/typeobject.c (python3.13+0x2ff750) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(Barebones neural network and data loading example notebook) PyObject_SetItem /__w/jax/jax/cpython/Objects/abstract.c:232:19 (python3.13+0x1b9488) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(fix symbolic zero handling in concat transpose) _PyEval_EvalFrameDefault /__w/jax/jax/cpython/Python/generated_cases.c.h:5777:27 (python3.13+0x3f5ddb) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(Cloud TPU Support) _PyEval_EvalFrame /__w/jax/jax/cpython/./Include/internal/pycore_ceval.h:119:16 (python3.13+0x3de84a) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(examples/datasets.py doesn’t work in python3) _PyEval_Vector /__w/jax/jax/cpython/Python/ceval.c:1812:12 (python3.13+0x3de84a) CC(Add support for `np.trace` ) _PyFunction_Vectorcall /__w/jax/jax/cpython/Objects/call.c (python3.13+0x1eb3bf) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(Error on NaN?) _PyObject_VectorcallTstate /__w/jax/jax/cpython/./Include/internal/pycore_call.h:168:11 (python3.13+0x1ef440) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(Bug in examples?) method_vectorcall /__w/jax/jax/cpython/Objects/classobject.c:92:18 (python3.13+0x1ef440) CC(Fix the bug in classifier example, batching_test and README) _PyVectorcall_Call /__w/jax/jax/cpython/Objects/call.c:273:16 (python3.13+0x1eb033) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(Broadcasting of size0 dimensions not implemented) _PyObject_Call /__w/jax/jax/cpython/Objects/call.c:348:16 (python3.13+0x1eb033) CC(minor spelling tweaks) PyObject_Call /__w/jax/jax/cpython/Objects/call.c:373:12 (python3.13+0x1eb0b5) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(CUDA90 and py3 ) _PyEval_EvalFrameDefault /__w/jax/jax/cpython/Python/generated_cases.c.h:1355:26 (python3.13+0x3e4902) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(add dot_general batching rule) _PyEval_EvalFrame /__w/jax/jax/cpython/./Include/internal/pycore_ceval.h:119:16 (python3.13+0x3de84a) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(np.einsum support) _PyEval_Vector /__w/jax/jax/cpython/Python/ceval.c:1812:12 (python3.13+0x3de84a) CC(Require protobuf 3.6.0 or later) _PyFunction_Vectorcall /__w/jax/jax/cpython/Objects/call.c (python3.13+0x1eb3bf) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(Hard crash when no compatible cuda devices found) _PyObject_VectorcallDictTstate /__w/jax/jax/cpython/Objects/call.c:135:15 (python3.13+0x1e9f3d) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(Invalid proto descriptor for file ""tensorflow/compiler/xla/xla_data.proto"") _PyObject_Call_Prepend /__w/jax/jax/cpython/Objects/call.c:504:24 (python3.13+0x1eba37) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(Fix support for arrays with size0 dimensions.) slot_tp_call /__w/jax/jax/cpython/Objects/typeobject.c:9539:15 (python3.13+0x2f8928) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(Set distinct_host_configuration=false in the bazel options.) _PyObject_MakeTpCall /__w/jax/jax/cpython/Objects/call.c:242:18 (python3.13+0x1ea1ac) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(Open Source Contributions) _PyObject_VectorcallTstate /__w/jax/jax/cpython/./Include/internal/pycore_call.h:166:16 (python3.13+0x1eae08) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(np.linalg.inv support) PyObject_Vectorcall /__w/jax/jax/cpython/Objects/call.c:327:12 (python3.13+0x1eae08) CC(Feature request: export TF ops) _PyEval_EvalFrameDefault /__w/jax/jax/cpython/Python/generated_cases.c.h:813:23 (python3.13+0x3e271b) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(Update XLA and reenable numpy tests that failed on Mac) _PyEval_EvalFrame /__w/jax/jax/cpython/./Include/internal/pycore_ceval.h:119:16 (python3.13+0x3de84a) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(jacrev and jacfwd usage example) _PyEval_Vector /__w/jax/jax/cpython/Python/ceval.c:1812:12 (python3.13+0x3de84a) CC(Unimplemented: binary integer op 'power') _PyFunction_Vectorcall /__w/jax/jax/cpython/Objects/call.c (python3.13+0x1eb3bf) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(Update neural_network_and_data_loading.ipynb) _PyObject_VectorcallTstate /__w/jax/jax/cpython/./Include/internal/pycore_call.h:168:11 (python3.13+0x1ef38f) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(Update README.md) method_vectorcall /__w/jax/jax/cpython/Objects/classobject.c:70:20 (python3.13+0x1ef38f) CC(add docstrings for major public functions) _PyVectorcall_Call /__w/jax/jax/cpython/Objects/call.c:273:16 (python3.13+0x1eb033) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(Scenarios to prefer over cupy) _PyObject_Call /__w/jax/jax/cpython/Objects/call.c:348:16 (python3.13+0x1eb033) CC(More informative error on trying to concatenate zerodimensional arrays) PyObject_Call /__w/jax/jax/cpython/Objects/call.c:373:12 (python3.13+0x1eb0b5) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(Batching rules for pad and concatenate primitives not implemented) thread_run /__w/jax/jax/cpython/./Modules/_threadmodule.c:337:21 (python3.13+0x564a72) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(np.rot90 support) pythread_wrapper /__w/jax/jax/cpython/Python/thread_pthread.h:243:5 (python3.13+0x4bdd77) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80)   Previous write of size 8 at 0x729000596b98 by thread T67 (mutexes: read M0): CC(未找到相关数据) (anonymous namespace)::ParametricStorageUniquer::getOrCreateUnsafe((anonymous namespace)::ParametricStorageUniquer::Shard&, (anonymous namespace)::ParametricStorageUniquer::LookupKey&, llvm::function_ref) /proc/self/cwd/external/llvmproject/mlir/lib/Support/StorageUniquer.cpp:98:15 (libjaxlib_mlir_capi.so+0x3414fbe) (BuildId: 4a2d544bd6db4a8210062f683bf6bb298c5bbcf8) CC(Python 3 compatibility issues) (anonymous namespace)::ParametricStorageUniquer::getOrCreate(bool, unsigned int, llvm::function_ref, llvm::function_ref) /proc/self/cwd/external/llvmproject/mlir/lib/Support/StorageUniquer.cpp:140:14 (libjaxlib_mlir_capi.so+0x3414fbe) CC(Explicit tuples are not valid function parameters in Python 3) mlir::detail::StorageUniquerImpl::getOrCreate(mlir::TypeID, unsigned int, llvm::function_ref, llvm::function_ref) /proc/self/cwd/external/llvmproject/mlir/lib/Support/StorageUniquer.cpp:270:27 (libjaxlib_mlir_capi.so+0x3414fbe) CC(Undefined name: from ..core import JaxTuple) mlir::StorageUniquer::getParametricStorageTypeImpl(mlir::TypeID, unsigned int, llvm::function_ref, llvm::function_ref) /proc/self/cwd/external/llvmproject/mlir/lib/Support/StorageUniquer.cpp:375:16 (libjaxlib_mlir_capi.so+0x3414b4f) (BuildId: 4a2d544bd6db4a8210062f683bf6bb298c5bbcf8) CC(Undefined name: from six.moves import xrange) mlir::detail::StringAttrStorage* mlir::StorageUniquer::get(llvm::function_ref, mlir::TypeID, llvm::StringRef&&, mlir::NoneType&&) /proc/self/cwd/external/llvmproject/mlir/include/mlir/Support/StorageUniquer.h:218:9 (libjaxlib_mlir_capi.so+0x33bd378) (BuildId: 4a2d544bd6db4a8210062f683bf6bb298c5bbcf8) CC(Building on OSX with CUDA) std::enable_if::value, mlir::StringAttr>::type mlir::detail::AttributeUniquer::getWithTypeID(mlir::MLIRContext*, mlir::TypeID, llvm::StringRef&&, mlir::NoneType&&) /proc/self/cwd/external/llvmproject/mlir/include/mlir/IR/AttributeSupport.h:239:39 (libjaxlib_mlir_capi.so+0x33bd378) CC(Made a shim to handle configuration without having absl parse flags) mlir::StringAttr mlir::detail::AttributeUniquer::get(mlir::MLIRContext*, llvm::StringRef&&, mlir::NoneType&&) /proc/self/cwd/external/llvmproject/mlir/include/mlir/IR/AttributeSupport.h:220:12 (libjaxlib_mlir_capi.so+0x33bd378) CC(Quickish check) mlir::StringAttr mlir::detail::StorageUserBase::get(mlir::MLIRContext*, llvm::StringRef&&, mlir::NoneType&&) /proc/self/cwd/external/llvmproject/mlir/include/mlir/IR/StorageUniquerSupport.h:181:12 (libjaxlib_mlir_capi.so+0x33bd378) CC(Quickish check) mlir::StringAttr::get(mlir::MLIRContext*, llvm::Twine const&) /proc/self/cwd/external/llvmproject/mlir/lib/IR/BuiltinAttributes.cpp:275:10 (libjaxlib_mlir_capi.so+0x33bd378) CC(Adding quickstart notebook, and corresponding gitignore rules) mlir::Operation::setAttr(llvm::StringRef, mlir::Attribute) /proc/self/cwd/external/llvmproject/mlir/include/mlir/IR/Operation.h:594:13 (libjaxlib_mlir_capi.so+0x25d0e57) (BuildId: 4a2d544bd6db4a8210062f683bf6bb298c5bbcf8) CC([JAX] Change semantics of dtype promotion to just call numpy.result_type.) mlirOperationSetAttributeByName /proc/self/cwd/external/llvmproject/mlir/lib/CAPI/IR/IR.cpp:683:15 (libjaxlib_mlir_capi.so+0x25d0e57) CC(Split out `jax` and `jaxlib` packages) (anonymous namespace)::PyOpAttributeMap::dunderSetItem(std::__cxx11::basic_string, std::allocator> const&, mlir::python::PyAttribute const&) /proc/self/cwd/external/llvmproject/mlir/lib/Bindings/Python/IRCore.cpp:2634:5 (_mlir.so+0xc6b97) (BuildId: 15beaf78f4e0ba2c1cb26690729f5d8e660c2f73) CC(Update the quickstart notebook.) void nanobind::cpp_function_def, std::allocator> const&, mlir::python::PyAttribute const&, nanobind::scope, nanobind::name, nanobind::is_method>(void ((anonymous namespace)::PyOpAttributeMap::*)(std::__cxx11::basic_string, std::allocator> const&, mlir::python::PyAttribute const&), nanobind::scope const&, nanobind::name const&, nanobind::is_method const&)::'lambda'((anonymous namespace)::PyOpAttributeMap*, std::__cxx11::basic_string, std::allocator> const&, mlir::python::PyAttribute const&)::operator()((anonymous namespace)::PyOpAttributeMap*, std::__cxx11::basic_string, std::allocator> const&, mlir::python::PyAttribute const&) const /proc/self/cwd/external/nanobind/include/nanobind/nb_func.h:376:20 (_mlir.so+0xc7685) (BuildId: 15beaf78f4e0ba2c1cb26690729f5d8e660c2f73) CC(Fixing logo size so resize is not required) _object* nanobind::detail::func_create, std::allocator> const&, mlir::python::PyAttribute const&, nanobind::scope, nanobind::name, nanobind::is_method>(void ((anonymous namespace)::PyOpAttributeMap::*)(std::__cxx11::basic_string, std::allocator> const&, mlir::python::PyAttribute const&), nanobind::scope const&, nanobind::name const&, nanobind::is_method const&)::'lambda'((anonymous namespace)::PyOpAttributeMap*, std::__cxx11::basic_string, std::allocator> const&, mlir::python::PyAttribute const&), void, (anonymous namespace)::PyOpAttributeMap*, std::__cxx11::basic_string, std::allocator> const&, mlir::python::PyAttribute const&, 0ul, 1ul, 2ul, nanobind::scope, nanobind::name, nanobind::is_method>((anonymous namespace)::PyOpAttributeMap&&, void (*)(nanobind::scope, nanobind::name, nanobind::is_method), std::integer_sequence, nanobind::scope const&, nanobind::name const&, nanobind::is_method const&)::'lambda'(void*, _object**, unsigned char*, nanobind::rv_policy, nanobind::detail::cleanup_list*)::operator()(void*, _object**, unsigned char*, nanobind::rv_policy, nanobind::detail::cleanup_list*) const /proc/self/cwd/external/nanobind/include/nanobind/nb_func.h:264:13 (_mlir.so+0xc7685) CC(Add copyright notice to quickstart notebook.) _object* nanobind::detail::func_create, std::allocator> const&, mlir::python::PyAttribute const&, nanobind::scope, nanobind::name, nanobind::is_method>(void ((anonymous namespace)::PyOpAttributeMap::*)(std::__cxx11::basic_string, std::allocator> const&, mlir::python::PyAttribute const&), nanobind::scope const&, nanobind::name const&, nanobind::is_method const&)::'lambda'((anonymous namespace)::PyOpAttributeMap*, std::__cxx11::basic_string, std::allocator> const&, mlir::python::PyAttribute const&), void, (anonymous namespace)::PyOpAttributeMap*, std::__cxx11::basic_string, std::allocator> const&, mlir::python::PyAttribute const&, 0ul, 1ul, 2ul, nanobind::scope, nanobind::name, nanobind::is_method>((anonymous namespace)::PyOpAttributeMap&&, void (*)(nanobind::scope, nanobind::name, nanobind::is_method), std::integer_sequence, nanobind::scope const&, nanobind::name const&, nanobind::is_method const&)::'lambda'(void*, _object**, unsigned char*, nanobind::rv_policy, nanobind::detail::cleanup_list*)::__invoke(void*, _object**, unsigned char*, nanobind::rv_policy, nanobind::detail::cleanup_list*) /proc/self/cwd/external/nanobind/include/nanobind/nb_func.h:219:14 (_mlir.so+0xc7685) CC(rename in_bdims, out_bdims > in_axes, out_axes) nanobind::detail::nb_func_vectorcall_simple(_object*, _object* const*, unsigned long, _object*) /proc/self/cwd/external/nanobind/src/nb_func.cpp:915:26 (_mlir.so+0x16e312) (BuildId: 15beaf78f4e0ba2c1cb26690729f5d8e660c2f73) CC(Add wheelbuilding scripts) _PyObject_VectorcallTstate /__w/jax/jax/cpython/./Include/internal/pycore_call.h:168:11 (python3.13+0x2f4627) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(Implement np.repeat for scalar repeats.) vectorcall_unbound /__w/jax/jax/cpython/Objects/typeobject.c:2572:12 (python3.13+0x2f4627) CC(Populate readme) vectorcall_method /__w/jax/jax/cpython/Objects/typeobject.c:2603:24 (python3.13+0x2f4627) CC(Notebook showing how to write gufuncs with vmap) slot_mp_ass_subscript /__w/jax/jax/cpython/Objects/typeobject.c (python3.13+0x2ff750) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(Fix link in gufuncs notebook) PyObject_SetItem /__w/jax/jax/cpython/Objects/abstract.c:232:19 (python3.13+0x1b9488) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(Typo) _PyEval_EvalFrameDefault /__w/jax/jax/cpython/Python/generated_cases.c.h:5777:27 (python3.13+0x3f5ddb) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(differention > differentiation) _PyEval_EvalFrame /__w/jax/jax/cpython/./Include/internal/pycore_ceval.h:119:16 (python3.13+0x3de84a) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(Typo, Python parens) _PyEval_Vector /__w/jax/jax/cpython/Python/ceval.c:1812:12 (python3.13+0x3de84a) CC(attempt to centerjustify the jax logo in readme) _PyFunction_Vectorcall /__w/jax/jax/cpython/Objects/call.c (python3.13+0x1eb3bf) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(Barebones neural network and data loading example notebook) _PyObject_VectorcallTstate /__w/jax/jax/cpython/./Include/internal/pycore_call.h:168:11 (python3.13+0x1ef440) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(fix symbolic zero handling in concat transpose) method_vectorcall /__w/jax/jax/cpython/Objects/classobject.c:92:18 (python3.13+0x1ef440) CC(Cloud TPU Support) _PyVectorcall_Call /__w/jax/jax/cpython/Objects/call.c:273:16 (python3.13+0x1eb033) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(examples/datasets.py doesn’t work in python3) _PyObject_Call /__w/jax/jax/cpython/Objects/call.c:348:16 (python3.13+0x1eb033) CC(Add support for `np.trace` ) PyObject_Call /__w/jax/jax/cpython/Objects/call.c:373:12 (python3.13+0x1eb0b5) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(Error on NaN?) _PyEval_EvalFrameDefault /__w/jax/jax/cpython/Python/generated_cases.c.h:1355:26 (python3.13+0x3e4902) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(Bug in examples?) _PyEval_EvalFrame /__w/jax/jax/cpython/./Include/internal/pycore_ceval.h:119:16 (python3.13+0x3de84a) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(Fix the bug in classifier example, batching_test and README) _PyEval_Vector /__w/jax/jax/cpython/Python/ceval.c:1812:12 (python3.13+0x3de84a) CC(Broadcasting of size0 dimensions not implemented) _PyFunction_Vectorcall /__w/jax/jax/cpython/Objects/call.c (python3.13+0x1eb3bf) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(minor spelling tweaks) _PyObject_VectorcallDictTstate /__w/jax/jax/cpython/Objects/call.c:135:15 (python3.13+0x1e9f3d) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(CUDA90 and py3 ) _PyObject_Call_Prepend /__w/jax/jax/cpython/Objects/call.c:504:24 (python3.13+0x1eba37) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(add dot_general batching rule) slot_tp_call /__w/jax/jax/cpython/Objects/typeobject.c:9539:15 (python3.13+0x2f8928) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(np.einsum support) _PyObject_MakeTpCall /__w/jax/jax/cpython/Objects/call.c:242:18 (python3.13+0x1ea1ac) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(Require protobuf 3.6.0 or later) _PyObject_VectorcallTstate /__w/jax/jax/cpython/./Include/internal/pycore_call.h:166:16 (python3.13+0x1eae08) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(Hard crash when no compatible cuda devices found) PyObject_Vectorcall /__w/jax/jax/cpython/Objects/call.c:327:12 (python3.13+0x1eae08) CC(Invalid proto descriptor for file ""tensorflow/compiler/xla/xla_data.proto"") _PyEval_EvalFrameDefault /__w/jax/jax/cpython/Python/generated_cases.c.h:813:23 (python3.13+0x3e271b) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(Fix support for arrays with size0 dimensions.) _PyEval_EvalFrame /__w/jax/jax/cpython/./Include/internal/pycore_ceval.h:119:16 (python3.13+0x3de84a) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(Set distinct_host_configuration=false in the bazel options.) _PyEval_Vector /__w/jax/jax/cpython/Python/ceval.c:1812:12 (python3.13+0x3de84a) CC(Open Source Contributions) _PyFunction_Vectorcall /__w/jax/jax/cpython/Objects/call.c (python3.13+0x1eb3bf) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(np.linalg.inv support) _PyObject_VectorcallTstate /__w/jax/jax/cpython/./Include/internal/pycore_call.h:168:11 (python3.13+0x1ef38f) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(Feature request: export TF ops) method_vectorcall /__w/jax/jax/cpython/Objects/classobject.c:70:20 (python3.13+0x1ef38f) CC(Update XLA and reenable numpy tests that failed on Mac) _PyVectorcall_Call /__w/jax/jax/cpython/Objects/call.c:273:16 (python3.13+0x1eb033) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(jacrev and jacfwd usage example) _PyObject_Call /__w/jax/jax/cpython/Objects/call.c:348:16 (python3.13+0x1eb033) CC(Unimplemented: binary integer op 'power') PyObject_Call /__w/jax/jax/cpython/Objects/call.c:373:12 (python3.13+0x1eb0b5) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(Update neural_network_and_data_loading.ipynb) thread_run /__w/jax/jax/cpython/./Modules/_threadmodule.c:337:21 (python3.13+0x564a72) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) CC(Update README.md) pythread_wrapper /__w/jax/jax/cpython/Python/thread_pthread.h:243:5 (python3.13+0x4bdd77) (BuildId: c9937216e103905f871b62bf50b66fc5a8e96f80) ``` PTAL?  System info (python version, jaxlib version, accelerator, etc.) Python 3.13t",2025-02-03T15:27:53Z,bug free threading,open,0,2,https://github.com/jax-ml/jax/issues/26272,"Another similar race report   Click  ``` WARNING: ThreadSanitizer: data race (pid=468047)   Read of size 8 at 0x7290000c2168 by thread T72 (mutexes: read M0): CC(未找到相关数据) (anonymous namespace)::ParametricStorageUniquer::StorageKeyInfo::isEqual((anonymous namespace)::ParametricStorageUniquer::HashedStorage const&, (anonymous namespace)::ParametricStorageUniquer::HashedStorage const&) /proc/self/cwd/external/llvmproject/mlir/lib/Support/StorageUniquer.cpp:67:18 (libjaxlib_mlir_capi.so+0x341a3eb) (BuildId: c1220b9d9c837a0ef2acf3164a01c78d11fc72c5) CC(Python 3 compatibility issues) (anonymous namespace)::ParametricStorageUniquer::StorageKeyInfo::isEqual((anonymous namespace)::ParametricStorageUniquer::LookupKey const&, (anonymous namespace)::ParametricStorageUniquer::HashedStorage const&) /proc/self/cwd/external/llvmproject/mlir/lib/Support/StorageUniquer.cpp:70:11 (libjaxlib_mlir_capi.so+0x341a3eb) CC(Explicit tuples are not valid function parameters in Python 3) bool llvm::DenseMapBase>, (anonymous namespace)::ParametricStorageUniquer::HashedStorage, llvm::detail::DenseSetEmpty, (anonymous namespace)::ParametricStorageUniquer::StorageKeyInfo, llvm::detail::DenseSetPair>::LookupBucketFor((anonymous namespace)::ParametricStorageUniquer::LookupKey const&, llvm::detail::DenseSetPair*&) /proc/self/cwd/external/llvmproject/llvm/include/llvm/ADT/DenseMap.h:654:11 (libjaxlib_mlir_capi.so+0x341a3eb) CC(Undefined name: from ..core import JaxTuple) std::pair, false>, bool> llvm::DenseMapBase>, (anonymous namespace)::ParametricStorageUniquer::HashedStorage, llvm::detail::DenseSetEmpty, (anonymous namespace)::ParametricStorageUniquer::StorageKeyInfo, llvm::detail::DenseSetPair>::insert_as(std::pair&&, (anonymous namespace)::ParametricStorageUniquer::LookupKey const&) /proc/self/cwd/external/llvmproject/llvm/include/llvm/ADT/DenseMap.h:280:9 (libjaxlib_mlir_capi.so+0x341a3eb) CC(Undefined name: from six.moves import xrange) std::pair>, (anonymous namespace)::ParametricStorageUniquer::StorageKeyInfo>::Iterator, bool> llvm::detail::DenseSetImpl>, (anonymous namespace)::ParametricStorageUniquer::StorageKeyInfo>::insert_as((anonymous namespace)::ParametricStorageUniquer::HashedStorage&&, (anonymous namespace)::ParametricStorageUniquer::LookupKey const&) /proc/self/cwd/external/llvmproject/llvm/include/llvm/ADT/DenseSet.h:232:19 (libjaxlib_mlir_capi.so+0x341a3eb) CC(Building on OSX with CUDA) (anonymous namespace)::ParametricStorageUniquer::getOrCreateUnsafe((anonymous namespace)::ParametricStorageUniquer::Shard&, (anonymous namespace)::ParametricStorageUniquer::LookupKey&, llvm::function_ref) /proc/self/cwd/external/llvmproject/mlir/lib/Support/StorageUniquer.cpp:95:37 (libjaxlib_mlir_capi.so+0x3417f91) (BuildId: c1220b9d9c837a0ef2acf3164a01c78d11fc72c5) CC(Made a shim to handle configuration without having absl parse flags) (anonymous namespace)::ParametricStorageUniquer::getOrCreate(bool, unsigned int, llvm::function_ref, llvm::function_ref) /proc/self/cwd/external/llvmproject/mlir/lib/Support/StorageUniquer.cpp:140:14 (libjaxlib_mlir_capi.so+0x3417f91) CC(Quickish check) mlir::detail::StorageUniquerImpl::getOrCreate(mlir::TypeID, unsigned int, llvm::function_ref, llvm::function_ref) /proc/self/cwd/external/llvmproject/mlir/lib/Support/StorageUniquer.cpp:270:27 (libjaxlib_mlir_capi.so+0x3417f91) CC(Quickish check) mlir::StorageUniquer::getParametricStorageTypeImpl(mlir::TypeID, unsigned int, llvm::function_ref, llvm::function_ref) /proc/self/cwd/external/llvmproject/mlir/lib/Support/StorageUniquer.cpp:375:16 (libjaxlib_mlir_capi.so+0x3417b6f) (BuildId: c1220b9d9c837a0ef2acf3164a01c78d11fc72c5) CC(Adding quickstart notebook, and corresponding gitignore rules) mlir::detail::StringAttrStorage* mlir::StorageUniquer::get(llvm::function_ref, mlir::TypeID, llvm::StringRef&&, mlir::NoneType&&) /proc/self/cwd/external/llvmproject/mlir/include/mlir/Support/StorageUniquer.h:218:9 (libjaxlib_mlir_capi.so+0x33c0398) (BuildId: c1220b9d9c837a0ef2acf3164a01c78d11fc72c5) CC([JAX] Change semantics of dtype promotion to just call numpy.result_type.) std::enable_if::value, mlir::StringAttr>::type mlir::detail::AttributeUniquer::getWithTypeID(mlir::MLIRContext*, mlir::TypeID, llvm::StringRef&&, mlir::NoneType&&) /proc/self/cwd/external/llvmproject/mlir/include/mlir/IR/AttributeSupport.h:239:39 (libjaxlib_mlir_capi.so+0x33c0398) CC(Split out `jax` and `jaxlib` packages) mlir::StringAttr mlir::detail::AttributeUniquer::get(mlir::MLIRContext*, llvm::StringRef&&, mlir::NoneType&&) /proc/self/cwd/external/llvmproject/mlir/include/mlir/IR/AttributeSupport.h:220:12 (libjaxlib_mlir_capi.so+0x33c0398) CC(Update the quickstart notebook.) mlir::StringAttr mlir::detail::StorageUserBase::get(mlir::MLIRContext*, llvm::StringRef&&, mlir::NoneType&&) /proc/self/cwd/external/llvmproject/mlir/include/mlir/IR/StorageUniquerSupport.h:181:12 (libjaxlib_mlir_capi.so+0x33c0398) CC(Fixing logo size so resize is not required) mlir::StringAttr::get(mlir::MLIRContext*, llvm::Twine const&) /proc/self/cwd/external/llvmproject/mlir/lib/IR/BuiltinAttributes.cpp:275:10 (libjaxlib_mlir_capi.so+0x33c0398) CC(Add copyright notice to quickstart notebook.) mlirStringAttrGet /proc/self/cwd/external/llvmproject/mlir/lib/CAPI/IR/BuiltinAttributes.cpp:242:26 (libjaxlib_mlir_capi.so+0x25c39e5) (BuildId: c1220b9d9c837a0ef2acf3164a01c78d11fc72c5) CC(rename in_bdims, out_bdims > in_axes, out_axes) (anonymous namespace)::PyStringAttribute::bindDerived(nanobind::class_&)::'lambda'(std::__cxx11::basic_string, std::allocator>, mlir::python::DefaultingPyMlirContext)::operator()(std::__cxx11::basic_string, std::allocator>, mlir::python::DefaultingPyMlirContext) const /proc/self/cwd/external/llvmproject/mlir/lib/Bindings/Python/IRAttributes.cpp:780:15 (_mlir.so+0x14813f) (BuildId: 294ceac9eacc32a1492f5bd0c3df61de64f742a3) CC(Add wheelbuilding scripts) _object* nanobind::detail::func_create&)::'lambda'(std::__cxx11::basic_string, std::allocator>, mlir::python::DefaultingPyMlirContext), (anonymous namespace)::PyStringAttribute, std::__cxx11::basic_string, std::allocator>, mlir::python::DefaultingPyMlirContext, 0ul, 1ul, nanobind::scope, nanobind::name, nanobind::arg, nanobind::arg_v, char [32]>((anonymous namespace)::PyStringAttribute::bindDerived(nanobind::class_&)::'lambda'(std::__cxx11::basic_string, std::allocator>, mlir::python::DefaultingPyMlirContext)&&, (anonymous namespace)::PyStringAttribute (*)(std::__cxx11::basic_string, std::allocator>, mlir::python::DefaultingPyMlirContext), std::integer_sequence, nanobind::scope const&, nanobind::name const&, nanobind::arg const&, nanobind::arg_v const&, char const (&) [32])::'lambda'(void*, _object**, unsigned char*, nanobind::rv_policy, nanobind::detail::cleanup_list*)::operator()(void*, _object**, unsigned char*, nanobind::rv_policy, nanobind::detail::cleanup_list*) const /proc/self/cwd/external/nanobind/include/nanobind/nb_func.h:275:24 (_mlir.so+0x14813f) CC(Implement np.repeat for scalar repeats.) _object* nanobind::detail::func_create&)::'lambda'(std::__cxx11::basic_string, std::allocator>, mlir::python::DefaultingPyMlirContext), (anonymous namespace)::PyStringAttribute, std::__cxx11::basic_string, std::allocator>, mlir::python::DefaultingPyMlirContext, 0ul, 1ul, nanobind::scope, nanobind::name, nanobind::arg, nanobind::arg_v, char [32]>((anonymous namespace)::PyStringAttribute::bindDerived(nanobind::class_&)::'lambda'(std::__cxx11::basic_string, std::allocator>, mlir::python::DefaultingPyMlirContext)&&, (anonymous namespace)::PyStringAttribute (*)(std::__cxx11::basic_string, std::allocator>, mlir::python::DefaultingPyMlirContext), std::integer_sequence, nanobind::scope const&, nanobind::name const&, nanobind::arg const&, nanobind::arg_v const&, char const (&) [32])::'lambda'(void*, _object**, unsigned char*, nanobind::rv_policy, nanobind::detail::cleanup_list*)::__invoke(void*, _object**, unsigned char*, nanobind::rv_policy, nanobind::detail::cleanup_list*) /proc/self/cwd/external/nanobind/include/nanobind/nb_func.h:219:14 (_mlir.so+0x14813f) CC(Populate readme) nanobind::detail::nb_func_vectorcall_complex(_object*, _object* const*, unsigned long, _object*) /proc/self/cwd/external/nanobind/src/nb_func.cpp:823:26 (_mlir.so+0x16de42) (BuildId: 294ceac9eacc32a1492f5bd0c3df61de64f742a3) CC(Notebook showing how to write gufuncs with vmap) _PyObject_VectorcallTstate /project/cpython/./Include/internal/pycore_call.h:168:11 (python3.13+0x1eafea) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Fix link in gufuncs notebook) PyObject_Vectorcall /project/cpython/Objects/call.c:327:12 (python3.13+0x1eafea) CC(Typo) _PyEval_EvalFrameDefault /project/cpython/Python/generated_cases.c.h:813:23 (python3.13+0x3e28eb) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(differention > differentiation) _PyEval_EvalFrame /project/cpython/./Include/internal/pycore_ceval.h:119:16 (python3.13+0x3dea1a) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Typo, Python parens) _PyEval_Vector /project/cpython/Python/ceval.c:1807:12 (python3.13+0x3dea1a) CC(attempt to centerjustify the jax logo in readme) _PyFunction_Vectorcall /project/cpython/Objects/call.c (python3.13+0x1eb65f) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Barebones neural network and data loading example notebook) _PyObject_VectorcallTstate /project/cpython/./Include/internal/pycore_call.h:168:11 (python3.13+0x1ef6e0) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(fix symbolic zero handling in concat transpose) method_vectorcall /project/cpython/Objects/classobject.c:92:18 (python3.13+0x1ef6e0) CC(Cloud TPU Support) _PyVectorcall_Call /project/cpython/Objects/call.c:273:16 (python3.13+0x1eb2d3) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(examples/datasets.py doesn’t work in python3) _PyObject_Call /project/cpython/Objects/call.c:348:16 (python3.13+0x1eb2d3) CC(Add support for `np.trace` ) PyObject_Call /project/cpython/Objects/call.c:373:12 (python3.13+0x1eb355) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Error on NaN?) _PyEval_EvalFrameDefault /project/cpython/Python/generated_cases.c.h:1355:26 (python3.13+0x3e4ad2) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Bug in examples?) _PyEval_EvalFrame /project/cpython/./Include/internal/pycore_ceval.h:119:16 (python3.13+0x3dea1a) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Fix the bug in classifier example, batching_test and README) _PyEval_Vector /project/cpython/Python/ceval.c:1807:12 (python3.13+0x3dea1a) CC(Broadcasting of size0 dimensions not implemented) _PyFunction_Vectorcall /project/cpython/Objects/call.c (python3.13+0x1eb65f) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(minor spelling tweaks) _PyObject_VectorcallDictTstate /project/cpython/Objects/call.c:135:15 (python3.13+0x1ea1dd) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(CUDA90 and py3 ) _PyObject_Call_Prepend /project/cpython/Objects/call.c:504:24 (python3.13+0x1ebcd7) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(add dot_general batching rule) slot_tp_call /project/cpython/Objects/typeobject.c:9533:15 (python3.13+0x2f8bb8) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(np.einsum support) _PyObject_MakeTpCall /project/cpython/Objects/call.c:242:18 (python3.13+0x1ea44c) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Require protobuf 3.6.0 or later) _PyObject_VectorcallTstate /project/cpython/./Include/internal/pycore_call.h:166:16 (python3.13+0x1eb0a8) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Hard crash when no compatible cuda devices found) PyObject_Vectorcall /project/cpython/Objects/call.c:327:12 (python3.13+0x1eb0a8) CC(Invalid proto descriptor for file ""tensorflow/compiler/xla/xla_data.proto"") _PyEval_EvalFrameDefault /project/cpython/Python/generated_cases.c.h:813:23 (python3.13+0x3e28eb) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Fix support for arrays with size0 dimensions.) _PyEval_EvalFrame /project/cpython/./Include/internal/pycore_ceval.h:119:16 (python3.13+0x3dea1a) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Set distinct_host_configuration=false in the bazel options.) _PyEval_Vector /project/cpython/Python/ceval.c:1807:12 (python3.13+0x3dea1a) CC(Open Source Contributions) _PyFunction_Vectorcall /project/cpython/Objects/call.c (python3.13+0x1eb65f) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(np.linalg.inv support) _PyObject_VectorcallTstate /project/cpython/./Include/internal/pycore_call.h:168:11 (python3.13+0x1ef62f) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Feature request: export TF ops) method_vectorcall /project/cpython/Objects/classobject.c:70:20 (python3.13+0x1ef62f) CC(Update XLA and reenable numpy tests that failed on Mac) _PyVectorcall_Call /project/cpython/Objects/call.c:273:16 (python3.13+0x1eb2d3) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(jacrev and jacfwd usage example) _PyObject_Call /project/cpython/Objects/call.c:348:16 (python3.13+0x1eb2d3) CC(Unimplemented: binary integer op 'power') PyObject_Call /project/cpython/Objects/call.c:373:12 (python3.13+0x1eb355) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Update neural_network_and_data_loading.ipynb) thread_run /project/cpython/./Modules/_threadmodule.c:337:21 (python3.13+0x564a12) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Update README.md) pythread_wrapper /project/cpython/Python/thread_pthread.h:243:5 (python3.13+0x4bdd97) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d)   Previous write of size 8 at 0x7290000c2168 by thread T54 (mutexes: read M0): CC(未找到相关数据) __tsan_memcpy  (python3.13+0xda10f) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Python 3 compatibility issues) llvm::detail::DenseSetPair* llvm::DenseMapBase>, (anonymous namespace)::ParametricStorageUniquer::HashedStorage, llvm::detail::DenseSetEmpty, (anonymous namespace)::ParametricStorageUniquer::StorageKeyInfo, llvm::detail::DenseSetPair>::InsertIntoBucketWithLookup(llvm::detail::DenseSetPair*, (anonymous namespace)::ParametricStorageUniquer::HashedStorage&&, llvm::detail::DenseSetEmpty&&, (anonymous namespace)::ParametricStorageUniquer::LookupKey const&) /proc/self/cwd/external/llvmproject/llvm/include/llvm/ADT/DenseMap.h:555:27 (libjaxlib_mlir_capi.so+0x341a52f) (BuildId: c1220b9d9c837a0ef2acf3164a01c78d11fc72c5) CC(Explicit tuples are not valid function parameters in Python 3) std::pair, false>, bool> llvm::DenseMapBase>, (anonymous namespace)::ParametricStorageUniquer::HashedStorage, llvm::detail::DenseSetEmpty, (anonymous namespace)::ParametricStorageUniquer::StorageKeyInfo, llvm::detail::DenseSetPair>::insert_as(std::pair&&, (anonymous namespace)::ParametricStorageUniquer::LookupKey const&) /proc/self/cwd/external/llvmproject/llvm/include/llvm/ADT/DenseMap.h:289:17 (libjaxlib_mlir_capi.so+0x341a52f) CC(Undefined name: from ..core import JaxTuple) std::pair>, (anonymous namespace)::ParametricStorageUniquer::StorageKeyInfo>::Iterator, bool> llvm::detail::DenseSetImpl>, (anonymous namespace)::ParametricStorageUniquer::StorageKeyInfo>::insert_as((anonymous namespace)::ParametricStorageUniquer::HashedStorage&&, (anonymous namespace)::ParametricStorageUniquer::LookupKey const&) /proc/self/cwd/external/llvmproject/llvm/include/llvm/ADT/DenseSet.h:232:19 (libjaxlib_mlir_capi.so+0x341a52f) CC(Undefined name: from six.moves import xrange) (anonymous namespace)::ParametricStorageUniquer::getOrCreateUnsafe((anonymous namespace)::ParametricStorageUniquer::Shard&, (anonymous namespace)::ParametricStorageUniquer::LookupKey&, llvm::function_ref) /proc/self/cwd/external/llvmproject/mlir/lib/Support/StorageUniquer.cpp:95:37 (libjaxlib_mlir_capi.so+0x3417f91) (BuildId: c1220b9d9c837a0ef2acf3164a01c78d11fc72c5) CC(Building on OSX with CUDA) (anonymous namespace)::ParametricStorageUniquer::getOrCreate(bool, unsigned int, llvm::function_ref, llvm::function_ref) /proc/self/cwd/external/llvmproject/mlir/lib/Support/StorageUniquer.cpp:140:14 (libjaxlib_mlir_capi.so+0x3417f91) CC(Made a shim to handle configuration without having absl parse flags) mlir::detail::StorageUniquerImpl::getOrCreate(mlir::TypeID, unsigned int, llvm::function_ref, llvm::function_ref) /proc/self/cwd/external/llvmproject/mlir/lib/Support/StorageUniquer.cpp:270:27 (libjaxlib_mlir_capi.so+0x3417f91) CC(Quickish check) mlir::StorageUniquer::getParametricStorageTypeImpl(mlir::TypeID, unsigned int, llvm::function_ref, llvm::function_ref) /proc/self/cwd/external/llvmproject/mlir/lib/Support/StorageUniquer.cpp:375:16 (libjaxlib_mlir_capi.so+0x3417b6f) (BuildId: c1220b9d9c837a0ef2acf3164a01c78d11fc72c5) CC(Quickish check) mlir::detail::StringAttrStorage* mlir::StorageUniquer::get(llvm::function_ref, mlir::TypeID, llvm::StringRef&&, mlir::NoneType&&) /proc/self/cwd/external/llvmproject/mlir/include/mlir/Support/StorageUniquer.h:218:9 (libjaxlib_mlir_capi.so+0x33c0398) (BuildId: c1220b9d9c837a0ef2acf3164a01c78d11fc72c5) CC(Adding quickstart notebook, and corresponding gitignore rules) std::enable_if::value, mlir::StringAttr>::type mlir::detail::AttributeUniquer::getWithTypeID(mlir::MLIRContext*, mlir::TypeID, llvm::StringRef&&, mlir::NoneType&&) /proc/self/cwd/external/llvmproject/mlir/include/mlir/IR/AttributeSupport.h:239:39 (libjaxlib_mlir_capi.so+0x33c0398) CC([JAX] Change semantics of dtype promotion to just call numpy.result_type.) mlir::StringAttr mlir::detail::AttributeUniquer::get(mlir::MLIRContext*, llvm::StringRef&&, mlir::NoneType&&) /proc/self/cwd/external/llvmproject/mlir/include/mlir/IR/AttributeSupport.h:220:12 (libjaxlib_mlir_capi.so+0x33c0398) CC(Split out `jax` and `jaxlib` packages) mlir::StringAttr mlir::detail::StorageUserBase::get(mlir::MLIRContext*, llvm::StringRef&&, mlir::NoneType&&) /proc/self/cwd/external/llvmproject/mlir/include/mlir/IR/StorageUniquerSupport.h:181:12 (libjaxlib_mlir_capi.so+0x33c0398) CC(Update the quickstart notebook.) mlir::StringAttr::get(mlir::MLIRContext*, llvm::Twine const&) /proc/self/cwd/external/llvmproject/mlir/lib/IR/BuiltinAttributes.cpp:275:10 (libjaxlib_mlir_capi.so+0x33c0398) CC(Fixing logo size so resize is not required) mlirStringAttrGet /proc/self/cwd/external/llvmproject/mlir/lib/CAPI/IR/BuiltinAttributes.cpp:242:26 (libjaxlib_mlir_capi.so+0x25c39e5) (BuildId: c1220b9d9c837a0ef2acf3164a01c78d11fc72c5) CC(Add copyright notice to quickstart notebook.) (anonymous namespace)::PyStringAttribute::bindDerived(nanobind::class_&)::'lambda'(std::__cxx11::basic_string, std::allocator>, mlir::python::DefaultingPyMlirContext)::operator()(std::__cxx11::basic_string, std::allocator>, mlir::python::DefaultingPyMlirContext) const /proc/self/cwd/external/llvmproject/mlir/lib/Bindings/Python/IRAttributes.cpp:780:15 (_mlir.so+0x14813f) (BuildId: 294ceac9eacc32a1492f5bd0c3df61de64f742a3) CC(rename in_bdims, out_bdims > in_axes, out_axes) _object* nanobind::detail::func_create&)::'lambda'(std::__cxx11::basic_string, std::allocator>, mlir::python::DefaultingPyMlirContext), (anonymous namespace)::PyStringAttribute, std::__cxx11::basic_string, std::allocator>, mlir::python::DefaultingPyMlirContext, 0ul, 1ul, nanobind::scope, nanobind::name, nanobind::arg, nanobind::arg_v, char [32]>((anonymous namespace)::PyStringAttribute::bindDerived(nanobind::class_&)::'lambda'(std::__cxx11::basic_string, std::allocator>, mlir::python::DefaultingPyMlirContext)&&, (anonymous namespace)::PyStringAttribute (*)(std::__cxx11::basic_string, std::allocator>, mlir::python::DefaultingPyMlirContext), std::integer_sequence, nanobind::scope const&, nanobind::name const&, nanobind::arg const&, nanobind::arg_v const&, char const (&) [32])::'lambda'(void*, _object**, unsigned char*, nanobind::rv_policy, nanobind::detail::cleanup_list*)::operator()(void*, _object**, unsigned char*, nanobind::rv_policy, nanobind::detail::cleanup_list*) const /proc/self/cwd/external/nanobind/include/nanobind/nb_func.h:275:24 (_mlir.so+0x14813f) CC(Add wheelbuilding scripts) _object* nanobind::detail::func_create&)::'lambda'(std::__cxx11::basic_string, std::allocator>, mlir::python::DefaultingPyMlirContext), (anonymous namespace)::PyStringAttribute, std::__cxx11::basic_string, std::allocator>, mlir::python::DefaultingPyMlirContext, 0ul, 1ul, nanobind::scope, nanobind::name, nanobind::arg, nanobind::arg_v, char [32]>((anonymous namespace)::PyStringAttribute::bindDerived(nanobind::class_&)::'lambda'(std::__cxx11::basic_string, std::allocator>, mlir::python::DefaultingPyMlirContext)&&, (anonymous namespace)::PyStringAttribute (*)(std::__cxx11::basic_string, std::allocator>, mlir::python::DefaultingPyMlirContext), std::integer_sequence, nanobind::scope const&, nanobind::name const&, nanobind::arg const&, nanobind::arg_v const&, char const (&) [32])::'lambda'(void*, _object**, unsigned char*, nanobind::rv_policy, nanobind::detail::cleanup_list*)::__invoke(void*, _object**, unsigned char*, nanobind::rv_policy, nanobind::detail::cleanup_list*) /proc/self/cwd/external/nanobind/include/nanobind/nb_func.h:219:14 (_mlir.so+0x14813f) CC(Implement np.repeat for scalar repeats.) nanobind::detail::nb_func_vectorcall_complex(_object*, _object* const*, unsigned long, _object*) /proc/self/cwd/external/nanobind/src/nb_func.cpp:823:26 (_mlir.so+0x16de42) (BuildId: 294ceac9eacc32a1492f5bd0c3df61de64f742a3) CC(Populate readme) _PyObject_VectorcallTstate /project/cpython/./Include/internal/pycore_call.h:168:11 (python3.13+0x1eafea) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Notebook showing how to write gufuncs with vmap) PyObject_Vectorcall /project/cpython/Objects/call.c:327:12 (python3.13+0x1eafea) CC(Fix link in gufuncs notebook) _PyEval_EvalFrameDefault /project/cpython/Python/generated_cases.c.h:813:23 (python3.13+0x3e28eb) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Typo) _PyEval_EvalFrame /project/cpython/./Include/internal/pycore_ceval.h:119:16 (python3.13+0x3dea1a) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(differention > differentiation) _PyEval_Vector /project/cpython/Python/ceval.c:1807:12 (python3.13+0x3dea1a) CC(Typo, Python parens) _PyFunction_Vectorcall /project/cpython/Objects/call.c (python3.13+0x1eb65f) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(attempt to centerjustify the jax logo in readme) _PyObject_VectorcallTstate /project/cpython/./Include/internal/pycore_call.h:168:11 (python3.13+0x1ef6e0) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Barebones neural network and data loading example notebook) method_vectorcall /project/cpython/Objects/classobject.c:92:18 (python3.13+0x1ef6e0) CC(fix symbolic zero handling in concat transpose) _PyVectorcall_Call /project/cpython/Objects/call.c:273:16 (python3.13+0x1eb2d3) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Cloud TPU Support) _PyObject_Call /project/cpython/Objects/call.c:348:16 (python3.13+0x1eb2d3) CC(examples/datasets.py doesn’t work in python3) PyObject_Call /project/cpython/Objects/call.c:373:12 (python3.13+0x1eb355) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Add support for `np.trace` ) _PyEval_EvalFrameDefault /project/cpython/Python/generated_cases.c.h:1355:26 (python3.13+0x3e4ad2) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Error on NaN?) _PyEval_EvalFrame /project/cpython/./Include/internal/pycore_ceval.h:119:16 (python3.13+0x3dea1a) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Bug in examples?) _PyEval_Vector /project/cpython/Python/ceval.c:1807:12 (python3.13+0x3dea1a) CC(Fix the bug in classifier example, batching_test and README) _PyFunction_Vectorcall /project/cpython/Objects/call.c (python3.13+0x1eb65f) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Broadcasting of size0 dimensions not implemented) _PyObject_VectorcallDictTstate /project/cpython/Objects/call.c:135:15 (python3.13+0x1ea1dd) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(minor spelling tweaks) _PyObject_Call_Prepend /project/cpython/Objects/call.c:504:24 (python3.13+0x1ebcd7) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(CUDA90 and py3 ) slot_tp_call /project/cpython/Objects/typeobject.c:9533:15 (python3.13+0x2f8bb8) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(add dot_general batching rule) _PyObject_MakeTpCall /project/cpython/Objects/call.c:242:18 (python3.13+0x1ea44c) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(np.einsum support) _PyObject_VectorcallTstate /project/cpython/./Include/internal/pycore_call.h:166:16 (python3.13+0x1eb0a8) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Require protobuf 3.6.0 or later) PyObject_Vectorcall /project/cpython/Objects/call.c:327:12 (python3.13+0x1eb0a8) CC(Hard crash when no compatible cuda devices found) _PyEval_EvalFrameDefault /project/cpython/Python/generated_cases.c.h:813:23 (python3.13+0x3e28eb) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Invalid proto descriptor for file ""tensorflow/compiler/xla/xla_data.proto"") _PyEval_EvalFrame /project/cpython/./Include/internal/pycore_ceval.h:119:16 (python3.13+0x3dea1a) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Fix support for arrays with size0 dimensions.) _PyEval_Vector /project/cpython/Python/ceval.c:1807:12 (python3.13+0x3dea1a) CC(Set distinct_host_configuration=false in the bazel options.) _PyFunction_Vectorcall /project/cpython/Objects/call.c (python3.13+0x1eb65f) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Open Source Contributions) _PyObject_VectorcallTstate /project/cpython/./Include/internal/pycore_call.h:168:11 (python3.13+0x1ef62f) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(np.linalg.inv support) method_vectorcall /project/cpython/Objects/classobject.c:70:20 (python3.13+0x1ef62f) CC(Feature request: export TF ops) _PyVectorcall_Call /project/cpython/Objects/call.c:273:16 (python3.13+0x1eb2d3) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Update XLA and reenable numpy tests that failed on Mac) _PyObject_Call /project/cpython/Objects/call.c:348:16 (python3.13+0x1eb2d3) CC(jacrev and jacfwd usage example) PyObject_Call /project/cpython/Objects/call.c:373:12 (python3.13+0x1eb355) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Unimplemented: binary integer op 'power') thread_run /project/cpython/./Modules/_threadmodule.c:337:21 (python3.13+0x564a12) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Update neural_network_and_data_loading.ipynb) pythread_wrapper /project/cpython/Python/thread_pthread.h:243:5 (python3.13+0x4bdd97) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d)   Location is heap block of size 8192 at 0x7290000c2000 allocated by thread T54: CC(未找到相关数据) posix_memalign  (python3.13+0xddba9) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Python 3 compatibility issues) operator new(unsigned long, std::align_val_t)  (libstdc++.so.6+0xbb9f5) (BuildId: ca77dae775ec87540acd7218fa990c40d1c94ab1) CC(Explicit tuples are not valid function parameters in Python 3) llvm::DenseMap>::allocateBuckets(unsigned int) /proc/self/cwd/external/llvmproject/llvm/include/llvm/ADT/DenseMap.h:872:9 (libjaxlib_mlir_capi.so+0x341c142) (BuildId: c1220b9d9c837a0ef2acf3164a01c78d11fc72c5) CC(Undefined name: from ..core import JaxTuple) llvm::DenseMap>::grow(unsigned int) /proc/self/cwd/external/llvmproject/llvm/include/llvm/ADT/DenseMap.h:817:5 (libjaxlib_mlir_capi.so+0x341c142) CC(Undefined name: from six.moves import xrange) llvm::DenseMapBase>, (anonymous namespace)::ParametricStorageUniquer::HashedStorage, llvm::detail::DenseSetEmpty, (anonymous namespace)::ParametricStorageUniquer::StorageKeyInfo, llvm::detail::DenseSetPair>::grow(unsigned int) /proc/self/cwd/external/llvmproject/llvm/include/llvm/ADT/DenseMap.h:536:64 (libjaxlib_mlir_capi.so+0x341c142) CC(Building on OSX with CUDA) llvm::detail::DenseSetPair* llvm::DenseMapBase>, (anonymous namespace)::ParametricStorageUniquer::HashedStorage, llvm::detail::DenseSetEmpty, (anonymous namespace)::ParametricStorageUniquer::StorageKeyInfo, llvm::detail::DenseSetPair>::InsertIntoBucketImpl((anonymous namespace)::ParametricStorageUniquer::LookupKey const&, llvm::detail::DenseSetPair*) /proc/self/cwd/external/llvmproject/llvm/include/llvm/ADT/DenseMap.h (libjaxlib_mlir_capi.so+0x341a5ab) (BuildId: c1220b9d9c837a0ef2acf3164a01c78d11fc72c5) CC(Made a shim to handle configuration without having absl parse flags) llvm::detail::DenseSetPair* llvm::DenseMapBase>, (anonymous namespace)::ParametricStorageUniquer::HashedStorage, llvm::detail::DenseSetEmpty, (anonymous namespace)::ParametricStorageUniquer::StorageKeyInfo, llvm::detail::DenseSetPair>::InsertIntoBucketWithLookup(llvm::detail::DenseSetPair*, (anonymous namespace)::ParametricStorageUniquer::HashedStorage&&, llvm::detail::DenseSetEmpty&&, (anonymous namespace)::ParametricStorageUniquer::LookupKey const&) /proc/self/cwd/external/llvmproject/llvm/include/llvm/ADT/DenseMap.h:553:17 (libjaxlib_mlir_capi.so+0x341a5ab) CC(Quickish check) std::pair, false>, bool> llvm::DenseMapBase>, (anonymous namespace)::ParametricStorageUniquer::HashedStorage, llvm::detail::DenseSetEmpty, (anonymous namespace)::ParametricStorageUniquer::StorageKeyInfo, llvm::detail::DenseSetPair>::insert_as(std::pair&&, (anonymous namespace)::ParametricStorageUniquer::LookupKey const&) /proc/self/cwd/external/llvmproject/llvm/include/llvm/ADT/DenseMap.h:289:17 (libjaxlib_mlir_capi.so+0x341a5ab) CC(Quickish check) std::pair>, (anonymous namespace)::ParametricStorageUniquer::StorageKeyInfo>::Iterator, bool> llvm::detail::DenseSetImpl>, (anonymous namespace)::ParametricStorageUniquer::StorageKeyInfo>::insert_as((anonymous namespace)::ParametricStorageUniquer::HashedStorage&&, (anonymous namespace)::ParametricStorageUniquer::LookupKey const&) /proc/self/cwd/external/llvmproject/llvm/include/llvm/ADT/DenseSet.h:232:19 (libjaxlib_mlir_capi.so+0x341a5ab) CC(Adding quickstart notebook, and corresponding gitignore rules) (anonymous namespace)::ParametricStorageUniquer::getOrCreateUnsafe((anonymous namespace)::ParametricStorageUniquer::Shard&, (anonymous namespace)::ParametricStorageUniquer::LookupKey&, llvm::function_ref) /proc/self/cwd/external/llvmproject/mlir/lib/Support/StorageUniquer.cpp:95:37 (libjaxlib_mlir_capi.so+0x3418822) (BuildId: c1220b9d9c837a0ef2acf3164a01c78d11fc72c5) CC([JAX] Change semantics of dtype promotion to just call numpy.result_type.) (anonymous namespace)::ParametricStorageUniquer::getOrCreate(bool, unsigned int, llvm::function_ref, llvm::function_ref) /proc/self/cwd/external/llvmproject/mlir/lib/Support/StorageUniquer.cpp:159:24 (libjaxlib_mlir_capi.so+0x3418822) CC(Split out `jax` and `jaxlib` packages) mlir::detail::StorageUniquerImpl::getOrCreate(mlir::TypeID, unsigned int, llvm::function_ref, llvm::function_ref) /proc/self/cwd/external/llvmproject/mlir/lib/Support/StorageUniquer.cpp:270:27 (libjaxlib_mlir_capi.so+0x3418822) CC(Update the quickstart notebook.) mlir::StorageUniquer::getParametricStorageTypeImpl(mlir::TypeID, unsigned int, llvm::function_ref, llvm::function_ref) /proc/self/cwd/external/llvmproject/mlir/lib/Support/StorageUniquer.cpp:375:16 (libjaxlib_mlir_capi.so+0x3417b6f) (BuildId: c1220b9d9c837a0ef2acf3164a01c78d11fc72c5) CC(Fixing logo size so resize is not required) mlir::detail::StringAttrStorage* mlir::StorageUniquer::get(llvm::function_ref, mlir::TypeID, llvm::StringRef&&, mlir::NoneType&&) /proc/self/cwd/external/llvmproject/mlir/include/mlir/Support/StorageUniquer.h:218:9 (libjaxlib_mlir_capi.so+0x33c0398) (BuildId: c1220b9d9c837a0ef2acf3164a01c78d11fc72c5) CC(Add copyright notice to quickstart notebook.) std::enable_if::value, mlir::StringAttr>::type mlir::detail::AttributeUniquer::getWithTypeID(mlir::MLIRContext*, mlir::TypeID, llvm::StringRef&&, mlir::NoneType&&) /proc/self/cwd/external/llvmproject/mlir/include/mlir/IR/AttributeSupport.h:239:39 (libjaxlib_mlir_capi.so+0x33c0398) CC(rename in_bdims, out_bdims > in_axes, out_axes) mlir::StringAttr mlir::detail::AttributeUniquer::get(mlir::MLIRContext*, llvm::StringRef&&, mlir::NoneType&&) /proc/self/cwd/external/llvmproject/mlir/include/mlir/IR/AttributeSupport.h:220:12 (libjaxlib_mlir_capi.so+0x33c0398) CC(Add wheelbuilding scripts) mlir::StringAttr mlir::detail::StorageUserBase::get(mlir::MLIRContext*, llvm::StringRef&&, mlir::NoneType&&) /proc/self/cwd/external/llvmproject/mlir/include/mlir/IR/StorageUniquerSupport.h:181:12 (libjaxlib_mlir_capi.so+0x33c0398) CC(Implement np.repeat for scalar repeats.) mlir::StringAttr::get(mlir::MLIRContext*, llvm::Twine const&) /proc/self/cwd/external/llvmproject/mlir/lib/IR/BuiltinAttributes.cpp:275:10 (libjaxlib_mlir_capi.so+0x33c0398) CC(Populate readme) mlir::OperationName::Impl::Impl(llvm::StringRef, mlir::Dialect*, mlir::TypeID, mlir::detail::InterfaceMap) /proc/self/cwd/external/llvmproject/mlir/lib/IR/MLIRContext.cpp:786:12 (libjaxlib_mlir_capi.so+0x33340b9) (BuildId: c1220b9d9c837a0ef2acf3164a01c78d11fc72c5) CC(Notebook showing how to write gufuncs with vmap) mlir::RegisteredOperationName::Model::Model(mlir::Dialect*) /proc/self/cwd/external/llvmproject/mlir/include/mlir/IR/OperationSupport.h:532:11 (libjaxlib_mlir_capi.so+0x1b0c16f) (BuildId: c1220b9d9c837a0ef2acf3164a01c78d11fc72c5) CC(Fix link in gufuncs notebook) std::__detail::_MakeUniq>::__single_object std::make_unique, mlir::Dialect*>(mlir::Dialect*&&) /usr/lib/gcc/x86_64linuxgnu/14/../../../../include/c++/14/bits/unique_ptr.h:1076:34 (libjaxlib_mlir_capi.so+0x1a4ee25) (BuildId: c1220b9d9c837a0ef2acf3164a01c78d11fc72c5) CC(Typo) void mlir::RegisteredOperationName::insert(mlir::Dialect&) /proc/self/cwd/external/llvmproject/mlir/include/mlir/IR/OperationSupport.h:686:12 (libjaxlib_mlir_capi.so+0x1a4ee25) CC(differention > differentiation) void mlir::Dialect::addOperations() /proc/self/cwd/external/llvmproject/mlir/include/mlir/IR/Dialect.h:283:13 (libjaxlib_mlir_capi.so+0x1a4ee25) CC(Typo, Python parens) mlir::NVVM::NVVMDialect::initialize() /proc/self/cwd/external/llvmproject/mlir/lib/Dialect/LLVMIR/IR/NVVMDialect.cpp:1261:3 (libjaxlib_mlir_capi.so+0x1a3801d) (BuildId: c1220b9d9c837a0ef2acf3164a01c78d11fc72c5) CC(attempt to centerjustify the jax logo in readme) mlir::NVVM::NVVMDialect::NVVMDialect(mlir::MLIRContext*) /proc/self/cwd/bazelout/k8opt/bin/external/llvmproject/mlir/include/mlir/Dialect/LLVMIR/NVVMOpsDialect.cpp.inc:19:3 (libjaxlib_mlir_capi.so+0x1a3801d) CC(Barebones neural network and data loading example notebook) mlir::NVVM::NVVMDialect* mlir::MLIRContext::getOrLoadDialect()::'lambda'()::operator()() const /proc/self/cwd/external/llvmproject/mlir/include/mlir/IR/MLIRContext.h:100:42 (libjaxlib_mlir_capi.so+0x6ee22b) (BuildId: c1220b9d9c837a0ef2acf3164a01c78d11fc72c5) CC(fix symbolic zero handling in concat transpose) std::unique_ptr> llvm::function_ref> ()>::callback_fn()::'lambda'()>(long) /proc/self/cwd/external/llvmproject/llvm/include/llvm/ADT/STLFunctionalExtras.h:46:12 (libjaxlib_mlir_capi.so+0x6ee22b) CC(Cloud TPU Support) llvm::function_ref> ()>::operator()() const /proc/self/cwd/external/llvmproject/llvm/include/llvm/ADT/STLFunctionalExtras.h:69:12 (libjaxlib_mlir_capi.so+0x33322b3) (BuildId: c1220b9d9c837a0ef2acf3164a01c78d11fc72c5) CC(examples/datasets.py doesn’t work in python3) mlir::MLIRContext::getOrLoadDialect(llvm::StringRef, mlir::TypeID, llvm::function_ref> ()>) /proc/self/cwd/external/llvmproject/mlir/lib/IR/MLIRContext.cpp:476:49 (libjaxlib_mlir_capi.so+0x33322b3) CC(Add support for `np.trace` ) mlir::NVVM::NVVMDialect* mlir::MLIRContext::getOrLoadDialect() /proc/self/cwd/external/llvmproject/mlir/include/mlir/IR/MLIRContext.h:99:9 (libjaxlib_mlir_capi.so+0x6ee176) (BuildId: c1220b9d9c837a0ef2acf3164a01c78d11fc72c5) CC(Error on NaN?) void mlir::DialectRegistry::insert()::'lambda'(mlir::MLIRContext*)::operator()(mlir::MLIRContext*) const /proc/self/cwd/external/llvmproject/mlir/include/mlir/IR/DialectRegistry.h:154:26 (libjaxlib_mlir_capi.so+0x6ee176) CC(Bug in examples?) mlir::NVVM::NVVMDialect* std::__invoke_impl()::'lambda'(mlir::MLIRContext*)&, mlir::MLIRContext*>(std::__invoke_other, void mlir::DialectRegistry::insert()::'lambda'(mlir::MLIRContext*)&, mlir::MLIRContext*&&) /usr/lib/gcc/x86_64linuxgnu/14/../../../../include/c++/14/bits/invoke.h:61:14 (libjaxlib_mlir_capi.so+0x6ee176) CC(Fix the bug in classifier example, batching_test and README) std::enable_if()::'lambda'(mlir::MLIRContext*)&, mlir::MLIRContext*>, mlir::Dialect*>::type std::__invoke_r()::'lambda'(mlir::MLIRContext*)&, mlir::MLIRContext*>(void mlir::DialectRegistry::insert()::'lambda'(mlir::MLIRContext*)&, mlir::MLIRContext*&&) /usr/lib/gcc/x86_64linuxgnu/14/../../../../include/c++/14/bits/invoke.h:114:9 (libjaxlib_mlir_capi.so+0x6ee176) CC(Broadcasting of size0 dimensions not implemented) std::_Function_handler()::'lambda'(mlir::MLIRContext*)>::_M_invoke(std::_Any_data const&, mlir::MLIRContext*&&) /usr/lib/gcc/x86_64linuxgnu/14/../../../../include/c++/14/bits/std_function.h:290:9 (libjaxlib_mlir_capi.so+0x6ee176) CC(minor spelling tweaks) std::function::operator()(mlir::MLIRContext*) const /usr/lib/gcc/x86_64linuxgnu/14/../../../../include/c++/14/bits/std_function.h:591:9 (libjaxlib_mlir_capi.so+0x338bc19) (BuildId: c1220b9d9c837a0ef2acf3164a01c78d11fc72c5) CC(CUDA90 and py3 ) mlir::Dialect* llvm::function_ref::callback_fn const>(long, mlir::MLIRContext*) /proc/self/cwd/external/llvmproject/llvm/include/llvm/ADT/STLFunctionalExtras.h:46:12 (libjaxlib_mlir_capi.so+0x338bc19) CC(add dot_general batching rule) llvm::function_ref::operator()(mlir::MLIRContext*) const /proc/self/cwd/external/llvmproject/llvm/include/llvm/ADT/STLFunctionalExtras.h:69:12 (libjaxlib_mlir_capi.so+0x3332d3b) (BuildId: c1220b9d9c837a0ef2acf3164a01c78d11fc72c5) CC(np.einsum support) mlir::MLIRContext::getOrLoadDialect(llvm::StringRef) /proc/self/cwd/external/llvmproject/mlir/lib/IR/MLIRContext.cpp:447:22 (libjaxlib_mlir_capi.so+0x3332d3b) CC(Require protobuf 3.6.0 or later) mlir::MLIRContext::loadAllAvailableDialects() /proc/self/cwd/external/llvmproject/mlir/lib/IR/MLIRContext.cpp:555:5 (libjaxlib_mlir_capi.so+0x3332d3b) CC(Hard crash when no compatible cuda devices found) mlirContextLoadAllAvailableDialects /proc/self/cwd/external/llvmproject/mlir/lib/CAPI/IR/IR.cpp:108:20 (libjaxlib_mlir_capi.so+0x25cf2b9) (BuildId: c1220b9d9c837a0ef2acf3164a01c78d11fc72c5) CC(Invalid proto descriptor for file ""tensorflow/compiler/xla/xla_data.proto"") mlir::python::populateIRCore(nanobind::module_&)::$_14::operator()(mlir::python::PyMlirContext&) const /proc/self/cwd/external/llvmproject/mlir/lib/Bindings/Python/IRCore.cpp:2826:9 (_mlir.so+0xd1866) (BuildId: 294ceac9eacc32a1492f5bd0c3df61de64f742a3) CC(Fix support for arrays with size0 dimensions.) _object* nanobind::detail::func_create(mlir::python::populateIRCore(nanobind::module_&)::$_14&&, void (*)(mlir::python::PyMlirContext&), std::integer_sequence, nanobind::scope const&, nanobind::name const&, nanobind::is_method const&)::'lambda'(void*, _object**, unsigned char*, nanobind::rv_policy, nanobind::detail::cleanup_list*)::operator()(void*, _object**, unsigned char*, nanobind::rv_policy, nanobind::detail::cleanup_list*) const /proc/self/cwd/external/nanobind/include/nanobind/nb_func.h:264:13 (_mlir.so+0xd1866) CC(Set distinct_host_configuration=false in the bazel options.) _object* nanobind::detail::func_create(mlir::python::populateIRCore(nanobind::module_&)::$_14&&, void (*)(mlir::python::PyMlirContext&), std::integer_sequence, nanobind::scope const&, nanobind::name const&, nanobind::is_method const&)::'lambda'(void*, _object**, unsigned char*, nanobind::rv_policy, nanobind::detail::cleanup_list*)::__invoke(void*, _object**, unsigned char*, nanobind::rv_policy, nanobind::detail::cleanup_list*) /proc/self/cwd/external/nanobind/include/nanobind/nb_func.h:219:14 (_mlir.so+0xd1866) CC(Open Source Contributions) nanobind::detail::nb_func_vectorcall_simple(_object*, _object* const*, unsigned long, _object*) /proc/self/cwd/external/nanobind/src/nb_func.cpp:915:26 (_mlir.so+0x16e312) (BuildId: 294ceac9eacc32a1492f5bd0c3df61de64f742a3) CC(np.linalg.inv support) _PyObject_VectorcallTstate /project/cpython/./Include/internal/pycore_call.h:168:11 (python3.13+0x1eafea) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Feature request: export TF ops) PyObject_Vectorcall /project/cpython/Objects/call.c:327:12 (python3.13+0x1eafea) CC(Update XLA and reenable numpy tests that failed on Mac) _PyEval_EvalFrameDefault /project/cpython/Python/generated_cases.c.h:813:23 (python3.13+0x3e28eb) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(jacrev and jacfwd usage example) _PyEval_EvalFrame /project/cpython/./Include/internal/pycore_ceval.h:119:16 (python3.13+0x3dea1a) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Unimplemented: binary integer op 'power') _PyEval_Vector /project/cpython/Python/ceval.c:1807:12 (python3.13+0x3dea1a) CC(Update neural_network_and_data_loading.ipynb) _PyFunction_Vectorcall /project/cpython/Objects/call.c (python3.13+0x1eb65f) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Update README.md) _PyObject_VectorcallDictTstate /project/cpython/Objects/call.c:146:15 (python3.13+0x1ea1a0) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(add docstrings for major public functions) _PyObject_Call_Prepend /project/cpython/Objects/call.c:504:24 (python3.13+0x1ebcd7) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Scenarios to prefer over cupy) slot_tp_init /project/cpython/Objects/typeobject.c:9779:15 (python3.13+0x2fa37c) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(More informative error on trying to concatenate zerodimensional arrays) type_call /project/cpython/Objects/typeobject.c:1993:19 (python3.13+0x2eb21c) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Batching rules for pad and concatenate primitives not implemented) _PyObject_MakeTpCall /project/cpython/Objects/call.c:242:18 (python3.13+0x1ea44c) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(np.rot90 support) _PyObject_VectorcallTstate /project/cpython/./Include/internal/pycore_call.h:166:16 (python3.13+0x1eb0a8) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Improving jax.scipy.stats) PyObject_Vectorcall /project/cpython/Objects/call.c:327:12 (python3.13+0x1eb0a8) CC(v0.2 tasks) _PyEval_EvalFrameDefault /project/cpython/Python/generated_cases.c.h:1502:19 (python3.13+0x3e556d) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Frequenty asked questions doc) _PyEval_EvalFrame /project/cpython/./Include/internal/pycore_ceval.h:119:16 (python3.13+0x3dea1a) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Autodiff cookbook) _PyEval_Vector /project/cpython/Python/ceval.c:1807:12 (python3.13+0x3dea1a) CC(Vmap cookbook) _PyFunction_Vectorcall /project/cpython/Objects/call.c (python3.13+0x1eb65f) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Docstrings in api.py) _PyObject_VectorcallTstate /project/cpython/./Include/internal/pycore_call.h:168:11 (python3.13+0x1eafea) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Upload existing JAX talks to docs/ or talks/ folder) PyObject_Vectorcall /project/cpython/Objects/call.c:327:12 (python3.13+0x1eafea) CC(Keras NumPy backend demo) nanobind::detail::obj_vectorcall(_object*, _object* const*, unsigned long, _object*, bool) /proc/self/cwd/external/nanobind/src/common.cpp:319:11 (xla_extension.so+0xd4dcc8d) (BuildId: ad17ba2d590fe83ae27f23ddc7e83e4cc2a0da71) CC(JAX philosophy, or ""culture and values"" doc) nanobind::object nanobind::detail::api::operator()(nanobind::object&, nanobind::detail::args_proxy&&, nanobind::detail::kwargs_proxy&&) const /proc/self/cwd/external/nanobind/include/nanobind/nb_call.h:129:9 (xla_extension.so+0x937b52) (BuildId: ad17ba2d590fe83ae27f23ddc7e83e4cc2a0da71) CC(Rename jaxlib to xlapy) jax::WeakrefLRUCache::Call(nanobind::object, nanobind::args, nanobind::kwargs) /proc/self/cwd/external/xla/xla/python/weakref_lru_cache.cc:269:25 (xla_extension.so+0x934c22) (BuildId: ad17ba2d590fe83ae27f23ddc7e83e4cc2a0da71) CC(Conda installations) void nanobind::cpp_function_def(nanobind::object (jax::WeakrefLRUCache::*)(nanobind::object, nanobind::args, nanobind::kwargs), nanobind::scope const&, nanobind::name const&, nanobind::is_method const&, nanobind::lock_self const&)::'lambda'(jax::WeakrefLRUCache*, nanobind::object, nanobind::args, nanobind::kwargs)::operator()(jax::WeakrefLRUCache*, nanobind::object, nanobind::args, nanobind::kwargs) const /proc/self/cwd/external/nanobind/include/nanobind/nb_func.h:376:20 (xla_extension.so+0x93c10d) (BuildId: ad17ba2d590fe83ae27f23ddc7e83e4cc2a0da71) CC(Open source tests) _object* nanobind::detail::func_create(nanobind::object (jax::WeakrefLRUCache::*)(nanobind::object, nanobind::args, nanobind::kwargs), nanobind::scope const&, nanobind::name const&, nanobind::is_method const&, nanobind::lock_self const&)::'lambda'(jax::WeakrefLRUCache*, nanobind::object, nanobind::args, nanobind::kwargs), nanobind::object, jax::WeakrefLRUCache*, nanobind::object, nanobind::args, nanobind::kwargs, 0ul, 1ul, 2ul, 3ul, nanobind::scope, nanobind::name, nanobind::is_method, nanobind::lock_self>(jax::WeakrefLRUCache&&, nanobind::object (*)(nanobind::scope, nanobind::name, nanobind::is_method, nanobind::lock_self), std::integer_sequence, nanobind::scope const&, nanobind::name const&, nanobind::is_method const&, nanobind::lock_self const&)::'lambda'(void*, _object**, unsigned char*, nanobind::rv_policy, nanobind::detail::cleanup_list*)::operator()(void*, _object**, unsigned char*, nanobind::rv_policy, nanobind::detail::cleanup_list*) const /proc/self/cwd/external/nanobind/include/nanobind/nb_func.h:275:24 (xla_extension.so+0x93c10d) CC(Travis CI automated testing) _object* nanobind::detail::func_create(nanobind::object (jax::WeakrefLRUCache::*)(nanobind::object, nanobind::args, nanobind::kwargs), nanobind::scope const&, nanobind::name const&, nanobind::is_method const&, nanobind::lock_self const&)::'lambda'(jax::WeakrefLRUCache*, nanobind::object, nanobind::args, nanobind::kwargs), nanobind::object, jax::WeakrefLRUCache*, nanobind::object, nanobind::args, nanobind::kwargs, 0ul, 1ul, 2ul, 3ul, nanobind::scope, nanobind::name, nanobind::is_method, nanobind::lock_self>(jax::WeakrefLRUCache&&, nanobind::object (*)(nanobind::scope, nanobind::name, nanobind::is_method, nanobind::lock_self), std::integer_sequence, nanobind::scope const&, nanobind::name const&, nanobind::is_method const&, nanobind::lock_self const&)::'lambda'(void*, _object**, unsigned char*, nanobind::rv_policy, nanobind::detail::cleanup_list*)::__invoke(void*, _object**, unsigned char*, nanobind::rv_policy, nanobind::detail::cleanup_list*) /proc/self/cwd/external/nanobind/include/nanobind/nb_func.h:219:14 (xla_extension.so+0x93c10d) CC(Automated detection of unimplemented functions) nanobind::detail::nb_func_vectorcall_complex(_object*, _object* const*, unsigned long, _object*) /proc/self/cwd/external/nanobind/src/nb_func.cpp:823:26 (xla_extension.so+0xd4f6c32) (BuildId: ad17ba2d590fe83ae27f23ddc7e83e4cc2a0da71) CC(Unimplemented NumPy core functions) _PyObject_VectorcallDictTstate /project/cpython/Objects/call.c:146:15 (python3.13+0x1ea1a0) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Add a few popular distributions to jax.scipy.stats) _PyObject_Call_Prepend /project/cpython/Objects/call.c:504:24 (python3.13+0x1ebcd7) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Adding unimplemented functions more numpy modules ) slot_tp_call /project/cpython/Objects/typeobject.c:9533:15 (python3.13+0x2f8bb8) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Adding test coverage) _PyObject_MakeTpCall /project/cpython/Objects/call.c:242:18 (python3.13+0x1ea44c) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Complex number support) _PyObject_VectorcallTstate /project/cpython/./Include/internal/pycore_call.h:166:16 (python3.13+0x1eb0a8) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(float16 support) PyObject_Vectorcall /project/cpython/Objects/call.c:327:12 (python3.13+0x1eb0a8) CC(bfloat16 support) _PyEval_EvalFrameDefault /project/cpython/Python/generated_cases.c.h:1502:19 (python3.13+0x3e556d) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(add rot90 and flip, adjust testOp test selection) _PyEval_EvalFrame /project/cpython/./Include/internal/pycore_ceval.h:119:16 (python3.13+0x3dea1a) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(curry `vmap`) _PyEval_Vector /project/cpython/Python/ceval.c:1807:12 (python3.13+0x3dea1a) CC(change vmap api to be curried (closes 78)) _PyFunction_Vectorcall /project/cpython/Objects/call.c (python3.13+0x1eb65f) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Wrapper functions for ConvTranspose etc.) _PyVectorcall_Call /project/cpython/Objects/call.c:285:24 (python3.13+0x1eb226) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(make colab notebooks runnable as tests) _PyObject_Call /project/cpython/Objects/call.c:348:16 (python3.13+0x1eb226) CC(add batching rules, fix jacfwd/jacrev transpose bug) PyObject_Call /project/cpython/Objects/call.c:373:12 (python3.13+0x1eb355) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(grad of cond and while) partial_call /project/cpython/./Modules/_functoolsmodule.c:353:21 (python3.13+0x570f4b) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(docker image) _PyObject_MakeTpCall /project/cpython/Objects/call.c:242:18 (python3.13+0x1ea44c) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Make Jax buildable with the latest Bazel) partial_vectorcall_fallback /project/cpython/./Modules/_functoolsmodule.c:226:12 (python3.13+0x572535) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Add missing license header to Python files.) partial_vectorcall /project/cpython/./Modules/_functoolsmodule.c:238:16 (python3.13+0x572186) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Consolidate build/WORKSPACE and WORKSPACE. ) _PyObject_VectorcallTstate /project/cpython/./Include/internal/pycore_call.h:168:11 (python3.13+0x1eafea) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Add build/ and dist/ to .gitignore) PyObject_Vectorcall /project/cpython/Objects/call.c:327:12 (python3.13+0x1eafea) CC(fix miscellaneous bugs, incl. complex abs grad) _PyEval_EvalFrameDefault /project/cpython/Python/generated_cases.c.h:813:23 (python3.13+0x3e28eb) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Add implementations of np.{tri,tril,triu}.) _PyEval_EvalFrame /project/cpython/./Include/internal/pycore_ceval.h:119:16 (python3.13+0x3dea1a) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Resurrect ONNX > jaxpr compatibility) _PyEval_Vector /project/cpython/Python/ceval.c:1807:12 (python3.13+0x3dea1a) CC(add onnx2xla compiler example (closes 91)) _PyFunction_Vectorcall /project/cpython/Objects/call.c (python3.13+0x1eb65f) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(bazel CLI flags for test targets) _PyObject_VectorcallTstate /project/cpython/./Include/internal/pycore_call.h:168:11 (python3.13+0x1ef6e0) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Implement np.{diag,diagonal}.) method_vectorcall /project/cpython/Objects/classobject.c:92:18 (python3.13+0x1ef6e0) CC(Seed example tests) _PyVectorcall_Call /project/cpython/Objects/call.c:273:16 (python3.13+0x1eb2d3) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(add misc numpy ops (c.f. 70)) _PyObject_Call /project/cpython/Objects/call.c:348:16 (python3.13+0x1eb2d3) CC(kernel leastsquares example) PyObject_Call /project/cpython/Objects/call.c:373:12 (python3.13+0x1eb355) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(make it easy to print jaxprs) _PyEval_EvalFrameDefault /project/cpython/Python/generated_cases.c.h:1355:26 (python3.13+0x3e4ad2) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Fix dtype semantics of reductions to more closely match numpy.) _PyEval_EvalFrame /project/cpython/./Include/internal/pycore_ceval.h:119:16 (python3.13+0x3dea1a) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Add log2, log10, logaddexp2, exp2 to lax_numpy) _PyEval_Vector /project/cpython/Python/ceval.c:1807:12 (python3.13+0x3dea1a) CC(Numpystyle indexed update support.) _PyFunction_Vectorcall /project/cpython/Objects/call.c (python3.13+0x1eb65f) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Implement np.trace.) _PyObject_VectorcallDictTstate /project/cpython/Objects/call.c:135:15 (python3.13+0x1ea1dd) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(未找到相关数据) _PyObject_Call_Prepend /project/cpython/Objects/call.c:504:24 (python3.13+0x1ebcd7) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Performance tests) slot_tp_call /project/cpython/Objects/typeobject.c:9533:15 (python3.13+0x2f8bb8) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Add Cholesky, QR, and Triangular solve implementations.) _PyObject_MakeTpCall /project/cpython/Objects/call.c:242:18 (python3.13+0x1ea44c) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(support for scipy.stats.dirichlet) _PyObject_VectorcallTstate /project/cpython/./Include/internal/pycore_call.h:166:16 (python3.13+0x1eb0a8) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Make JAX flake8clean.) PyObject_Vectorcall /project/cpython/Objects/call.c:327:12 (python3.13+0x1eb0a8) CC(Batching broken for nonmonoidal reducers) _PyEval_EvalFrameDefault /project/cpython/Python/generated_cases.c.h:813:23 (python3.13+0x3e28eb) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Disable tests that don't pass on all backends.) _PyEval_EvalFrame /project/cpython/./Include/internal/pycore_ceval.h:119:16 (python3.13+0x3dea1a) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Implement np.linalg.inv using a QR decomposition.) _PyEval_Vector /project/cpython/Python/ceval.c:1807:12 (python3.13+0x3dea1a) CC(add travis ci support) _PyFunction_Vectorcall /project/cpython/Objects/call.c (python3.13+0x1eb65f) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(enable linalg tests) _PyObject_VectorcallTstate /project/cpython/./Include/internal/pycore_call.h:168:11 (python3.13+0x1ef62f) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(np.all and np.any should lead to monoid reducers) method_vectorcall /project/cpython/Objects/classobject.c:70:20 (python3.13+0x1ef62f) CC(Enable MKLDNN contraction kernels.) _PyVectorcall_Call /project/cpython/Objects/call.c:273:16 (python3.13+0x1eb2d3) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Fixed argument order in call to var from std.) _PyObject_Call /project/cpython/Objects/call.c:348:16 (python3.13+0x1eb2d3) CC(Easy api for custom primitives and vjps) PyObject_Call /project/cpython/Objects/call.c:373:12 (python3.13+0x1eb355) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(add oss test instructions, fix conv grad bug) thread_run /project/cpython/./Modules/_threadmodule.c:337:21 (python3.13+0x564a12) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(add tensordot) pythread_wrapper /project/cpython/Python/thread_pthread.h:243:5 (python3.13+0x4bdd97) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d)   Mutex M0 (0x7fffb7ba3778) created at: CC(未找到相关数据) __tsan_mutex_create  (python3.13+0x130623) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Python 3 compatibility issues) absl::lts_20230802::Mutex::Mutex() /proc/self/cwd/external/com_google_absl/absl/synchronization/mutex.h:1022:3 (utils.so+0xac65) (BuildId: 8a7f0bc5213f57da9cd44587ba2607bf66b72f71) CC(Explicit tuples are not valid function parameters in Python 3) void nanobind::init::execute>(nanobind::class_&)::'lambda'(nanobind::pointer_and_handle)::operator()(nanobind::pointer_and_handle) const /proc/self/cwd/external/nanobind/include/nanobind/nb_class.h:362:36 (utils.so+0xac65) CC(Undefined name: from ..core import JaxTuple) _object* nanobind::detail::func_create::execute>(nanobind::class_&)::'lambda'(nanobind::pointer_and_handle), void, nanobind::pointer_and_handle, 0ul, nanobind::scope, nanobind::name, nanobind::is_method>(void nanobind::init::execute>(nanobind::class_&)::'lambda'(nanobind::pointer_and_handle)&&, void (*)(nanobind::pointer_and_handle), std::integer_sequence, nanobind::scope const&, nanobind::name const&, nanobind::is_method const&)::'lambda'(void*, _object**, unsigned char*, nanobind::rv_policy, nanobind::detail::cleanup_list*)::operator()(void*, _object**, unsigned char*, nanobind::rv_policy, nanobind::detail::cleanup_list*) const /proc/self/cwd/external/nanobind/include/nanobind/nb_func.h:264:13 (utils.so+0xac65) CC(Undefined name: from six.moves import xrange) _object* nanobind::detail::func_create::execute>(nanobind::class_&)::'lambda'(nanobind::pointer_and_handle), void, nanobind::pointer_and_handle, 0ul, nanobind::scope, nanobind::name, nanobind::is_method>(void nanobind::init::execute>(nanobind::class_&)::'lambda'(nanobind::pointer_and_handle)&&, void (*)(nanobind::pointer_and_handle), std::integer_sequence, nanobind::scope const&, nanobind::name const&, nanobind::is_method const&)::'lambda'(void*, _object**, unsigned char*, nanobind::rv_policy, nanobind::detail::cleanup_list*)::__invoke(void*, _object**, unsigned char*, nanobind::rv_policy, nanobind::detail::cleanup_list*) /proc/self/cwd/external/nanobind/include/nanobind/nb_func.h:219:14 (utils.so+0xac65) CC(Building on OSX with CUDA) nanobind::detail::nb_func_vectorcall_simple(_object*, _object* const*, unsigned long, _object*) /proc/self/cwd/external/nanobind/src/nb_func.cpp:915:26 (utils.so+0x22242) (BuildId: 8a7f0bc5213f57da9cd44587ba2607bf66b72f71) CC(Made a shim to handle configuration without having absl parse flags) nanobind::detail::nb_type_vectorcall(_object*, _object* const*, unsigned long, _object*) /proc/self/cwd/external/nanobind/src/nb_type.cpp:1030:9 (utils.so+0x17f7e) (BuildId: 8a7f0bc5213f57da9cd44587ba2607bf66b72f71) CC(Quickish check) _PyObject_VectorcallTstate /project/cpython/./Include/internal/pycore_call.h:168:11 (python3.13+0x1eafea) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Quickish check) PyObject_Vectorcall /project/cpython/Objects/call.c:327:12 (python3.13+0x1eafea) CC(Adding quickstart notebook, and corresponding gitignore rules) _PyEval_EvalFrameDefault /project/cpython/Python/generated_cases.c.h:813:23 (python3.13+0x3e28eb) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC([JAX] Change semantics of dtype promotion to just call numpy.result_type.) _PyEval_EvalFrame /project/cpython/./Include/internal/pycore_ceval.h:119:16 (python3.13+0x3de6f2) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Split out `jax` and `jaxlib` packages) _PyEval_Vector /project/cpython/Python/ceval.c:1807:12 (python3.13+0x3de6f2) CC(Update the quickstart notebook.) PyEval_EvalCode /project/cpython/Python/ceval.c:597:21 (python3.13+0x3de6f2) CC(Fixing logo size so resize is not required) builtin_exec_impl /project/cpython/Python/bltinmodule.c:1145:17 (python3.13+0x3d8dd8) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Add copyright notice to quickstart notebook.) builtin_exec /project/cpython/Python/clinic/bltinmodule.c.h:556:20 (python3.13+0x3d8dd8) CC(rename in_bdims, out_bdims > in_axes, out_axes) cfunction_vectorcall_FASTCALL_KEYWORDS /project/cpython/Objects/methodobject.c:441:24 (python3.13+0x289f20) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Add wheelbuilding scripts) _PyVectorcall_Call /project/cpython/Objects/call.c:273:16 (python3.13+0x1eb2d3) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Implement np.repeat for scalar repeats.) _PyObject_Call /project/cpython/Objects/call.c:348:16 (python3.13+0x1eb2d3) CC(Populate readme) PyObject_Call /project/cpython/Objects/call.c:373:12 (python3.13+0x1eb355) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Notebook showing how to write gufuncs with vmap) _PyEval_EvalFrameDefault /project/cpython/Python/generated_cases.c.h:1355:26 (python3.13+0x3e4ad2) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Fix link in gufuncs notebook) _PyEval_EvalFrame /project/cpython/./Include/internal/pycore_ceval.h:119:16 (python3.13+0x3dea1a) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Typo) _PyEval_Vector /project/cpython/Python/ceval.c:1807:12 (python3.13+0x3dea1a) CC(differention > differentiation) _PyFunction_Vectorcall /project/cpython/Objects/call.c (python3.13+0x1eb65f) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Typo, Python parens) _PyObject_VectorcallTstate /project/cpython/./Include/internal/pycore_call.h:168:11 (python3.13+0x1ed690) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(attempt to centerjustify the jax logo in readme) object_vacall /project/cpython/Objects/call.c:819:14 (python3.13+0x1ed690) CC(Barebones neural network and data loading example notebook) PyObject_CallMethodObjArgs /project/cpython/Objects/call.c:880:24 (python3.13+0x1ed33b) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(fix symbolic zero handling in concat transpose) import_find_and_load /project/cpython/Python/import.c:3692:11 (python3.13+0x4607de) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Cloud TPU Support) PyImport_ImportModuleLevelObject /project/cpython/Python/import.c:3774:15 (python3.13+0x4607de) CC(examples/datasets.py doesn’t work in python3) builtin___import___impl /project/cpython/Python/bltinmodule.c:277:12 (python3.13+0x3d70b4) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Add support for `np.trace` ) builtin___import__ /project/cpython/Python/clinic/bltinmodule.c.h:107:20 (python3.13+0x3d70b4) CC(Error on NaN?) cfunction_vectorcall_FASTCALL_KEYWORDS /project/cpython/Objects/methodobject.c:441:24 (python3.13+0x289f20) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Bug in examples?) _PyVectorcall_Call /project/cpython/Objects/call.c:273:16 (python3.13+0x1eb2d3) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Fix the bug in classifier example, batching_test and README) _PyObject_Call /project/cpython/Objects/call.c:348:16 (python3.13+0x1eb2d3) CC(Broadcasting of size0 dimensions not implemented) PyObject_Call /project/cpython/Objects/call.c:373:12 (python3.13+0x1eb355) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(minor spelling tweaks) _PyEval_EvalFrameDefault /project/cpython/Python/generated_cases.c.h:1355:26 (python3.13+0x3e4ad2) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(CUDA90 and py3 ) _PyEval_EvalFrame /project/cpython/./Include/internal/pycore_ceval.h:119:16 (python3.13+0x3dea1a) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(add dot_general batching rule) _PyEval_Vector /project/cpython/Python/ceval.c:1807:12 (python3.13+0x3dea1a) CC(np.einsum support) _PyFunction_Vectorcall /project/cpython/Objects/call.c (python3.13+0x1eb65f) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Require protobuf 3.6.0 or later) _PyObject_VectorcallTstate /project/cpython/./Include/internal/pycore_call.h:168:11 (python3.13+0x1ed690) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Hard crash when no compatible cuda devices found) object_vacall /project/cpython/Objects/call.c:819:14 (python3.13+0x1ed690) CC(Invalid proto descriptor for file ""tensorflow/compiler/xla/xla_data.proto"") PyObject_CallMethodObjArgs /project/cpython/Objects/call.c:880:24 (python3.13+0x1ed33b) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Fix support for arrays with size0 dimensions.) PyImport_ImportModuleLevelObject /project/cpython/Python/import.c:3842:25 (python3.13+0x46088b) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Set distinct_host_configuration=false in the bazel options.) import_name /project/cpython/Python/ceval.c:2711:16 (python3.13+0x3ec777) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Open Source Contributions) _PyEval_EvalFrameDefault /project/cpython/Python/generated_cases.c.h:3201:19 (python3.13+0x3ec777) CC(np.linalg.inv support) _PyEval_EvalFrame /project/cpython/./Include/internal/pycore_ceval.h:119:16 (python3.13+0x3de6f2) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Feature request: export TF ops) _PyEval_Vector /project/cpython/Python/ceval.c:1807:12 (python3.13+0x3de6f2) CC(Update XLA and reenable numpy tests that failed on Mac) PyEval_EvalCode /project/cpython/Python/ceval.c:597:21 (python3.13+0x3de6f2) CC(jacrev and jacfwd usage example) builtin_exec_impl /project/cpython/Python/bltinmodule.c:1145:17 (python3.13+0x3d8dd8) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Unimplemented: binary integer op 'power') builtin_exec /project/cpython/Python/clinic/bltinmodule.c.h:556:20 (python3.13+0x3d8dd8) CC(Update neural_network_and_data_loading.ipynb) cfunction_vectorcall_FASTCALL_KEYWORDS /project/cpython/Objects/methodobject.c:441:24 (python3.13+0x289f20) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Update README.md) _PyVectorcall_Call /project/cpython/Objects/call.c:273:16 (python3.13+0x1eb2d3) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(add docstrings for major public functions) _PyObject_Call /project/cpython/Objects/call.c:348:16 (python3.13+0x1eb2d3) CC(Scenarios to prefer over cupy) PyObject_Call /project/cpython/Objects/call.c:373:12 (python3.13+0x1eb355) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(More informative error on trying to concatenate zerodimensional arrays) _PyEval_EvalFrameDefault /project/cpython/Python/generated_cases.c.h:1355:26 (python3.13+0x3e4ad2) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Batching rules for pad and concatenate primitives not implemented) _PyEval_EvalFrame /project/cpython/./Include/internal/pycore_ceval.h:119:16 (python3.13+0x3dea1a) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(np.rot90 support) _PyEval_Vector /project/cpython/Python/ceval.c:1807:12 (python3.13+0x3dea1a) CC(Improving jax.scipy.stats) _PyFunction_Vectorcall /project/cpython/Objects/call.c (python3.13+0x1eb65f) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(v0.2 tasks) _PyObject_VectorcallTstate /project/cpython/./Include/internal/pycore_call.h:168:11 (python3.13+0x1ed690) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Frequenty asked questions doc) object_vacall /project/cpython/Objects/call.c:819:14 (python3.13+0x1ed690) CC(Autodiff cookbook) PyObject_CallMethodObjArgs /project/cpython/Objects/call.c:880:24 (python3.13+0x1ed33b) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Vmap cookbook) import_find_and_load /project/cpython/Python/import.c:3692:11 (python3.13+0x4607de) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Docstrings in api.py) PyImport_ImportModuleLevelObject /project/cpython/Python/import.c:3774:15 (python3.13+0x4607de) CC(Upload existing JAX talks to docs/ or talks/ folder) builtin___import___impl /project/cpython/Python/bltinmodule.c:277:12 (python3.13+0x3d70b4) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Keras NumPy backend demo) builtin___import__ /project/cpython/Python/clinic/bltinmodule.c.h:107:20 (python3.13+0x3d70b4) CC(JAX philosophy, or ""culture and values"" doc) cfunction_vectorcall_FASTCALL_KEYWORDS /project/cpython/Objects/methodobject.c:441:24 (python3.13+0x289f20) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Rename jaxlib to xlapy) _PyVectorcall_Call /project/cpython/Objects/call.c:273:16 (python3.13+0x1eb2d3) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Conda installations) _PyObject_Call /project/cpython/Objects/call.c:348:16 (python3.13+0x1eb2d3) CC(Open source tests) PyObject_Call /project/cpython/Objects/call.c:373:12 (python3.13+0x1eb355) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Travis CI automated testing) _PyEval_EvalFrameDefault /project/cpython/Python/generated_cases.c.h:1355:26 (python3.13+0x3e4ad2) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Automated detection of unimplemented functions) _PyEval_EvalFrame /project/cpython/./Include/internal/pycore_ceval.h:119:16 (python3.13+0x3dea1a) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Unimplemented NumPy core functions) _PyEval_Vector /project/cpython/Python/ceval.c:1807:12 (python3.13+0x3dea1a) CC(Add a few popular distributions to jax.scipy.stats) _PyFunction_Vectorcall /project/cpython/Objects/call.c (python3.13+0x1eb65f) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Adding unimplemented functions more numpy modules ) _PyObject_VectorcallTstate /project/cpython/./Include/internal/pycore_call.h:168:11 (python3.13+0x1ed690) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Adding test coverage) object_vacall /project/cpython/Objects/call.c:819:14 (python3.13+0x1ed690) CC(Complex number support) PyObject_CallMethodObjArgs /project/cpython/Objects/call.c:880:24 (python3.13+0x1ed33b) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(float16 support) PyImport_ImportModuleLevelObject /project/cpython/Python/import.c:3842:25 (python3.13+0x46088b) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(bfloat16 support) import_name /project/cpython/Python/ceval.c:2711:16 (python3.13+0x3ec777) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(add rot90 and flip, adjust testOp test selection) _PyEval_EvalFrameDefault /project/cpython/Python/generated_cases.c.h:3201:19 (python3.13+0x3ec777) CC(curry `vmap`) _PyEval_EvalFrame /project/cpython/./Include/internal/pycore_ceval.h:119:16 (python3.13+0x3de6f2) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(change vmap api to be curried (closes 78)) _PyEval_Vector /project/cpython/Python/ceval.c:1807:12 (python3.13+0x3de6f2) CC(Wrapper functions for ConvTranspose etc.) PyEval_EvalCode /project/cpython/Python/ceval.c:597:21 (python3.13+0x3de6f2) CC(make colab notebooks runnable as tests) run_eval_code_obj /project/cpython/Python/pythonrun.c:1337:9 (python3.13+0x4a0a5e) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(add batching rules, fix jacfwd/jacrev transpose bug) run_mod /project/cpython/Python/pythonrun.c:1422:19 (python3.13+0x4a0185) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(grad of cond and while) pyrun_file /project/cpython/Python/pythonrun.c:1255:15 (python3.13+0x49c280) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(docker image) _PyRun_SimpleFileObject /project/cpython/Python/pythonrun.c:490:13 (python3.13+0x49c280) CC(Make Jax buildable with the latest Bazel) _PyRun_AnyFileObject /project/cpython/Python/pythonrun.c:77:15 (python3.13+0x49b948) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Add missing license header to Python files.) pymain_run_file_obj /project/cpython/Modules/main.c:410:15 (python3.13+0x4d7e6f) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Consolidate build/WORKSPACE and WORKSPACE. ) pymain_run_file /project/cpython/Modules/main.c:429:15 (python3.13+0x4d7e6f) CC(Add build/ and dist/ to .gitignore) pymain_run_python /project/cpython/Modules/main.c:697:21 (python3.13+0x4d70bc) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(fix miscellaneous bugs, incl. complex abs grad) Py_RunMain /project/cpython/Modules/main.c:776:5 (python3.13+0x4d70bc) CC(Add implementations of np.{tri,tril,triu}.) pymain_main /project/cpython/Modules/main.c:806:12 (python3.13+0x4d74f8) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Resurrect ONNX > jaxpr compatibility) Py_BytesMain /project/cpython/Modules/main.c:830:12 (python3.13+0x4d757b) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(add onnx2xla compiler example (closes 91)) main /project/cpython/./Programs/python.c:15:12 (python3.13+0x15c7eb) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d)   Thread T72 (tid=469314, running) created by main thread at: CC(未找到相关数据) pthread_create  (python3.13+0xde1df) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Python 3 compatibility issues) do_start_joinable_thread /project/cpython/Python/thread_pthread.h:290:14 (python3.13+0x4bcc48) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Explicit tuples are not valid function parameters in Python 3) PyThread_start_joinable_thread /project/cpython/Python/thread_pthread.h:314:9 (python3.13+0x4bca6a) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Undefined name: from ..core import JaxTuple) ThreadHandle_start /project/cpython/./Modules/_threadmodule.c:422:9 (python3.13+0x5645a7) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Undefined name: from six.moves import xrange) do_start_new_thread /project/cpython/./Modules/_threadmodule.c:1849:9 (python3.13+0x5645a7) CC(Building on OSX with CUDA) thread_PyThread_start_joinable_thread /project/cpython/./Modules/_threadmodule.c:1972:14 (python3.13+0x563641) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Made a shim to handle configuration without having absl parse flags) cfunction_call /project/cpython/Objects/methodobject.c:540:18 (python3.13+0x28abb7) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Quickish check) _PyObject_MakeTpCall /project/cpython/Objects/call.c:242:18 (python3.13+0x1ea44c) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Quickish check) _PyObject_VectorcallTstate /project/cpython/./Include/internal/pycore_call.h:166:16 (python3.13+0x1eb0a8) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Adding quickstart notebook, and corresponding gitignore rules) PyObject_Vectorcall /project/cpython/Objects/call.c:327:12 (python3.13+0x1eb0a8) CC([JAX] Change semantics of dtype promotion to just call numpy.result_type.) _PyEval_EvalFrameDefault /project/cpython/Python/generated_cases.c.h:1502:19 (python3.13+0x3e556d) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Split out `jax` and `jaxlib` packages) _PyEval_EvalFrame /project/cpython/./Include/internal/pycore_ceval.h:119:16 (python3.13+0x3dea1a) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Update the quickstart notebook.) _PyEval_Vector /project/cpython/Python/ceval.c:1807:12 (python3.13+0x3dea1a) CC(Fixing logo size so resize is not required) _PyFunction_Vectorcall /project/cpython/Objects/call.c (python3.13+0x1eb65f) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Add copyright notice to quickstart notebook.) _PyObject_VectorcallTstate /project/cpython/./Include/internal/pycore_call.h:168:11 (python3.13+0x1ef6e0) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(rename in_bdims, out_bdims > in_axes, out_axes) method_vectorcall /project/cpython/Objects/classobject.c:92:18 (python3.13+0x1ef6e0) CC(Add wheelbuilding scripts) _PyVectorcall_Call /project/cpython/Objects/call.c:273:16 (python3.13+0x1eb2d3) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Implement np.repeat for scalar repeats.) _PyObject_Call /project/cpython/Objects/call.c:348:16 (python3.13+0x1eb2d3) CC(Populate readme) PyObject_Call /project/cpython/Objects/call.c:373:12 (python3.13+0x1eb355) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Notebook showing how to write gufuncs with vmap) _PyEval_EvalFrameDefault /project/cpython/Python/generated_cases.c.h:1355:26 (python3.13+0x3e4ad2) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Fix link in gufuncs notebook) _PyEval_EvalFrame /project/cpython/./Include/internal/pycore_ceval.h:119:16 (python3.13+0x3dea1a) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Typo) _PyEval_Vector /project/cpython/Python/ceval.c:1807:12 (python3.13+0x3dea1a) CC(differention > differentiation) _PyFunction_Vectorcall /project/cpython/Objects/call.c (python3.13+0x1eb65f) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Typo, Python parens) _PyObject_VectorcallDictTstate /project/cpython/Objects/call.c:135:15 (python3.13+0x1ea1dd) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(attempt to centerjustify the jax logo in readme) _PyObject_Call_Prepend /project/cpython/Objects/call.c:504:24 (python3.13+0x1ebcd7) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Barebones neural network and data loading example notebook) slot_tp_call /project/cpython/Objects/typeobject.c:9533:15 (python3.13+0x2f8bb8) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(fix symbolic zero handling in concat transpose) _PyObject_MakeTpCall /project/cpython/Objects/call.c:242:18 (python3.13+0x1ea44c) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Cloud TPU Support) _PyObject_VectorcallTstate /project/cpython/./Include/internal/pycore_call.h:166:16 (python3.13+0x1eb0a8) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(examples/datasets.py doesn’t work in python3) PyObject_Vectorcall /project/cpython/Objects/call.c:327:12 (python3.13+0x1eb0a8) CC(Add support for `np.trace` ) _PyEval_EvalFrameDefault /project/cpython/Python/generated_cases.c.h:813:23 (python3.13+0x3e28eb) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Error on NaN?) _PyEval_EvalFrame /project/cpython/./Include/internal/pycore_ceval.h:119:16 (python3.13+0x3dea1a) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Bug in examples?) _PyEval_Vector /project/cpython/Python/ceval.c:1807:12 (python3.13+0x3dea1a) CC(Fix the bug in classifier example, batching_test and README) _PyFunction_Vectorcall /project/cpython/Objects/call.c (python3.13+0x1eb65f) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Broadcasting of size0 dimensions not implemented) _PyObject_VectorcallDictTstate /project/cpython/Objects/call.c:146:15 (python3.13+0x1ea1a0) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(minor spelling tweaks) _PyObject_Call_Prepend /project/cpython/Objects/call.c:504:24 (python3.13+0x1ebcd7) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(CUDA90 and py3 ) slot_tp_init /project/cpython/Objects/typeobject.c:9779:15 (python3.13+0x2fa37c) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(add dot_general batching rule) type_call /project/cpython/Objects/typeobject.c:1993:19 (python3.13+0x2eb21c) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(np.einsum support) _PyObject_Call /project/cpython/Objects/call.c:361:18 (python3.13+0x1eb28b) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Require protobuf 3.6.0 or later) PyObject_Call /project/cpython/Objects/call.c:373:12 (python3.13+0x1eb355) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Hard crash when no compatible cuda devices found) _PyEval_EvalFrameDefault /project/cpython/Python/generated_cases.c.h:1355:26 (python3.13+0x3e4ad2) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Invalid proto descriptor for file ""tensorflow/compiler/xla/xla_data.proto"") _PyEval_EvalFrame /project/cpython/./Include/internal/pycore_ceval.h:119:16 (python3.13+0x3de6f2) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Fix support for arrays with size0 dimensions.) _PyEval_Vector /project/cpython/Python/ceval.c:1807:12 (python3.13+0x3de6f2) CC(Set distinct_host_configuration=false in the bazel options.) PyEval_EvalCode /project/cpython/Python/ceval.c:597:21 (python3.13+0x3de6f2) CC(Open Source Contributions) run_eval_code_obj /project/cpython/Python/pythonrun.c:1337:9 (python3.13+0x4a0a5e) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(np.linalg.inv support) run_mod /project/cpython/Python/pythonrun.c:1422:19 (python3.13+0x4a0185) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Feature request: export TF ops) pyrun_file /project/cpython/Python/pythonrun.c:1255:15 (python3.13+0x49c280) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Update XLA and reenable numpy tests that failed on Mac) _PyRun_SimpleFileObject /project/cpython/Python/pythonrun.c:490:13 (python3.13+0x49c280) CC(jacrev and jacfwd usage example) _PyRun_AnyFileObject /project/cpython/Python/pythonrun.c:77:15 (python3.13+0x49b948) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Unimplemented: binary integer op 'power') pymain_run_file_obj /project/cpython/Modules/main.c:410:15 (python3.13+0x4d7e6f) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Update neural_network_and_data_loading.ipynb) pymain_run_file /project/cpython/Modules/main.c:429:15 (python3.13+0x4d7e6f) CC(Update README.md) pymain_run_python /project/cpython/Modules/main.c:697:21 (python3.13+0x4d70bc) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(add docstrings for major public functions) Py_RunMain /project/cpython/Modules/main.c:776:5 (python3.13+0x4d70bc) CC(Scenarios to prefer over cupy) pymain_main /project/cpython/Modules/main.c:806:12 (python3.13+0x4d74f8) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(More informative error on trying to concatenate zerodimensional arrays) Py_BytesMain /project/cpython/Modules/main.c:830:12 (python3.13+0x4d757b) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Batching rules for pad and concatenate primitives not implemented) main /project/cpython/./Programs/python.c:15:12 (python3.13+0x15c7eb) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d)   Thread T54 (tid=469282, running) created by main thread at: CC(未找到相关数据) pthread_create  (python3.13+0xde1df) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Python 3 compatibility issues) do_start_joinable_thread /project/cpython/Python/thread_pthread.h:290:14 (python3.13+0x4bcc48) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Explicit tuples are not valid function parameters in Python 3) PyThread_start_joinable_thread /project/cpython/Python/thread_pthread.h:314:9 (python3.13+0x4bca6a) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Undefined name: from ..core import JaxTuple) ThreadHandle_start /project/cpython/./Modules/_threadmodule.c:422:9 (python3.13+0x5645a7) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Undefined name: from six.moves import xrange) do_start_new_thread /project/cpython/./Modules/_threadmodule.c:1849:9 (python3.13+0x5645a7) CC(Building on OSX with CUDA) thread_PyThread_start_joinable_thread /project/cpython/./Modules/_threadmodule.c:1972:14 (python3.13+0x563641) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Made a shim to handle configuration without having absl parse flags) cfunction_call /project/cpython/Objects/methodobject.c:540:18 (python3.13+0x28abb7) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Quickish check) _PyObject_MakeTpCall /project/cpython/Objects/call.c:242:18 (python3.13+0x1ea44c) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Quickish check) _PyObject_VectorcallTstate /project/cpython/./Include/internal/pycore_call.h:166:16 (python3.13+0x1eb0a8) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Adding quickstart notebook, and corresponding gitignore rules) PyObject_Vectorcall /project/cpython/Objects/call.c:327:12 (python3.13+0x1eb0a8) CC([JAX] Change semantics of dtype promotion to just call numpy.result_type.) _PyEval_EvalFrameDefault /project/cpython/Python/generated_cases.c.h:1502:19 (python3.13+0x3e556d) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Split out `jax` and `jaxlib` packages) _PyEval_EvalFrame /project/cpython/./Include/internal/pycore_ceval.h:119:16 (python3.13+0x3dea1a) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Update the quickstart notebook.) _PyEval_Vector /project/cpython/Python/ceval.c:1807:12 (python3.13+0x3dea1a) CC(Fixing logo size so resize is not required) _PyFunction_Vectorcall /project/cpython/Objects/call.c (python3.13+0x1eb65f) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Add copyright notice to quickstart notebook.) _PyObject_VectorcallTstate /project/cpython/./Include/internal/pycore_call.h:168:11 (python3.13+0x1ef6e0) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(rename in_bdims, out_bdims > in_axes, out_axes) method_vectorcall /project/cpython/Objects/classobject.c:92:18 (python3.13+0x1ef6e0) CC(Add wheelbuilding scripts) _PyVectorcall_Call /project/cpython/Objects/call.c:273:16 (python3.13+0x1eb2d3) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Implement np.repeat for scalar repeats.) _PyObject_Call /project/cpython/Objects/call.c:348:16 (python3.13+0x1eb2d3) CC(Populate readme) PyObject_Call /project/cpython/Objects/call.c:373:12 (python3.13+0x1eb355) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Notebook showing how to write gufuncs with vmap) _PyEval_EvalFrameDefault /project/cpython/Python/generated_cases.c.h:1355:26 (python3.13+0x3e4ad2) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Fix link in gufuncs notebook) _PyEval_EvalFrame /project/cpython/./Include/internal/pycore_ceval.h:119:16 (python3.13+0x3dea1a) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Typo) _PyEval_Vector /project/cpython/Python/ceval.c:1807:12 (python3.13+0x3dea1a) CC(differention > differentiation) _PyFunction_Vectorcall /project/cpython/Objects/call.c (python3.13+0x1eb65f) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Typo, Python parens) _PyObject_VectorcallDictTstate /project/cpython/Objects/call.c:135:15 (python3.13+0x1ea1dd) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(attempt to centerjustify the jax logo in readme) _PyObject_Call_Prepend /project/cpython/Objects/call.c:504:24 (python3.13+0x1ebcd7) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Barebones neural network and data loading example notebook) slot_tp_call /project/cpython/Objects/typeobject.c:9533:15 (python3.13+0x2f8bb8) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(fix symbolic zero handling in concat transpose) _PyObject_MakeTpCall /project/cpython/Objects/call.c:242:18 (python3.13+0x1ea44c) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Cloud TPU Support) _PyObject_VectorcallTstate /project/cpython/./Include/internal/pycore_call.h:166:16 (python3.13+0x1eb0a8) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(examples/datasets.py doesn’t work in python3) PyObject_Vectorcall /project/cpython/Objects/call.c:327:12 (python3.13+0x1eb0a8) CC(Add support for `np.trace` ) _PyEval_EvalFrameDefault /project/cpython/Python/generated_cases.c.h:813:23 (python3.13+0x3e28eb) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Error on NaN?) _PyEval_EvalFrame /project/cpython/./Include/internal/pycore_ceval.h:119:16 (python3.13+0x3dea1a) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Bug in examples?) _PyEval_Vector /project/cpython/Python/ceval.c:1807:12 (python3.13+0x3dea1a) CC(Fix the bug in classifier example, batching_test and README) _PyFunction_Vectorcall /project/cpython/Objects/call.c (python3.13+0x1eb65f) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Broadcasting of size0 dimensions not implemented) _PyObject_VectorcallDictTstate /project/cpython/Objects/call.c:146:15 (python3.13+0x1ea1a0) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(minor spelling tweaks) _PyObject_Call_Prepend /project/cpython/Objects/call.c:504:24 (python3.13+0x1ebcd7) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(CUDA90 and py3 ) slot_tp_init /project/cpython/Objects/typeobject.c:9779:15 (python3.13+0x2fa37c) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(add dot_general batching rule) type_call /project/cpython/Objects/typeobject.c:1993:19 (python3.13+0x2eb21c) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(np.einsum support) _PyObject_Call /project/cpython/Objects/call.c:361:18 (python3.13+0x1eb28b) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Require protobuf 3.6.0 or later) PyObject_Call /project/cpython/Objects/call.c:373:12 (python3.13+0x1eb355) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Hard crash when no compatible cuda devices found) _PyEval_EvalFrameDefault /project/cpython/Python/generated_cases.c.h:1355:26 (python3.13+0x3e4ad2) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Invalid proto descriptor for file ""tensorflow/compiler/xla/xla_data.proto"") _PyEval_EvalFrame /project/cpython/./Include/internal/pycore_ceval.h:119:16 (python3.13+0x3de6f2) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Fix support for arrays with size0 dimensions.) _PyEval_Vector /project/cpython/Python/ceval.c:1807:12 (python3.13+0x3de6f2) CC(Set distinct_host_configuration=false in the bazel options.) PyEval_EvalCode /project/cpython/Python/ceval.c:597:21 (python3.13+0x3de6f2) CC(Open Source Contributions) run_eval_code_obj /project/cpython/Python/pythonrun.c:1337:9 (python3.13+0x4a0a5e) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(np.linalg.inv support) run_mod /project/cpython/Python/pythonrun.c:1422:19 (python3.13+0x4a0185) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Feature request: export TF ops) pyrun_file /project/cpython/Python/pythonrun.c:1255:15 (python3.13+0x49c280) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Update XLA and reenable numpy tests that failed on Mac) _PyRun_SimpleFileObject /project/cpython/Python/pythonrun.c:490:13 (python3.13+0x49c280) CC(jacrev and jacfwd usage example) _PyRun_AnyFileObject /project/cpython/Python/pythonrun.c:77:15 (python3.13+0x49b948) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Unimplemented: binary integer op 'power') pymain_run_file_obj /project/cpython/Modules/main.c:410:15 (python3.13+0x4d7e6f) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Update neural_network_and_data_loading.ipynb) pymain_run_file /project/cpython/Modules/main.c:429:15 (python3.13+0x4d7e6f) CC(Update README.md) pymain_run_python /project/cpython/Modules/main.c:697:21 (python3.13+0x4d70bc) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(add docstrings for major public functions) Py_RunMain /project/cpython/Modules/main.c:776:5 (python3.13+0x4d70bc) CC(Scenarios to prefer over cupy) pymain_main /project/cpython/Modules/main.c:806:12 (python3.13+0x4d74f8) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(More informative error on trying to concatenate zerodimensional arrays) Py_BytesMain /project/cpython/Modules/main.c:830:12 (python3.13+0x4d757b) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) CC(Batching rules for pad and concatenate primitives not implemented) main /project/cpython/./Programs/python.c:15:12 (python3.13+0x15c7eb) (BuildId: 5572a851d2acb326869407b965b3c37f6628c49d) ```  To reproduce: ```bash bazel test     test_env=JAX_NUM_GENERATED_CASES=$JAX_NUM_GENERATED_CASES     test_env=JAX_ENABLE_X64=$JAX_ENABLE_X64     test_env=JAX_SKIP_SLOW_TESTS=$JAX_SKIP_SLOW_TESTS     test_env=PYTHON_GIL=0     test_env=TSAN_OPTIONS=halt_on_error=1,suppressions=$PWD/.github/workflows/tsansuppressions.txt     test_env=JAX_TEST_NUM_THREADS=8     test_output=all     local_test_jobs=32     test_timeout=600     nocache_test_results runs_per_test=100  test_filter=CompatTest  //tests:export_back_compat_test_cpu ```","A perhaps silly question: when we compile the MLIR C++ code, do we compile it with threading enabled? I seem to recall some locking there is only enabled if you build with threading enabled. Perhaps also we need to enable threading on our MLIR contexts? IIRC right now we disable it because we don't want MLIR to create thread pools, but that's orthogonal to ""is thread safe""."
yi,Build fails due to missing visibility declaration for hipfft target," Description  Description When building JAX with ROCm support, the build fails due to a visibility issue with the hipfft target.  Error Message ``` ERROR: /external/xla/xla/stream_executor/rocm/BUILD:409:11:  in cc_library rule //xla/stream_executor/rocm:hipfft_if_static:  target '//rocm:hipfft' is not visible from target '//xla/stream_executor/rocm:hipfft_if_static'.  Check the visibility declaration of the former target if you think the dependency is legitimate ```  Environment  ROCm Version: 6.2.411340  GPU: AMD Radeon Pro W7900 (gfx1100)  OS: Arch Linux  Build Command:   ```bash   python3 ./build/build.py build \     wheels=jaxlib,jaxrocmplugin,jaxrocmpjrt \     rocm_path=/opt/rocm \     clang_path=/usr/lib/llvm18/bin/clang18 \     rocm_version=62 \     rocm_amdgpu_targets=gfx1100 \     use_clang   ```  Solution The hipfft target in `/external/local_config_rocm/rocm/BUILD` needs a public visibility declaration. Other similar targets in the same file (hipblas, hiprand, etc.) already have this visibility set, but it seems to have been overlooked for hipfft. The fix is to add `visibility = [""//visibility:public""]` to the hipfft target: ```python cc_library(     name = ""hipfft"",     srcs = glob([""rocm_dist/lib/libhipfft*.so*""]),     include_prefix = ""rocm"",     includes = [         ""rocm_dist/include"",     ],     linkstatic = 1,     visibility = [""//visibility:public""],   Added this line to match other ROCm libraries     deps = ["":rocm_config""], ) ``` After applying this change, the build proceeded successfully.  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.5.1.dev20250131+c1e136058 jaxlib: 0.5.0 numpy:  2.2.2 python: 3.13.1 (main, Dec  4 2024, 18:05:56) [GCC 14.2.1 20240910] device info: cpu1, 1 local devices"" process_count: 1 platform: uname_result(system='Linux', node='timospc', release='6.12.10arch11', version=' CC(Python 3 compatibility issues) SMP PREEMPT_DYNAMIC Sat, 18 Jan 2025 02:26:57 +0000', machine='x86_64') ```",2025-02-03T13:27:51Z,bug build AMD GPU,open,0,3,https://github.com/jax-ml/jax/issues/26269,"If you know the fix, send a PR? In this case, it probably needs to be sent to the openxla/xla project, I think.","Let me take a look. On Mon, Feb 3, 2025 at 9:13 AM Peter Hawkins ***@***.***> wrote: > If you know the fix, send a PR? In this case, it probably needs to be sent > to the openxla/xla project, I think. > > — > Reply to this email directly, view it on GitHub > , or > unsubscribe >  > . > You are receiving this because you were assigned.Message ID: > ***@***.***> >",I think this might have already been fixed in xla and jax just hasn't picked up the latest. see: https://github.com/openxla/xla/commit/991a2289dc4a8750f0009e5f2a87507407523964
rag,Grouped convolutions are much slower than standard convolutions when increasing kernel dilation," Description I’ve been experimenting with depthwise grouped convolutions in JAX, and noticed that as I increase the kernel dilation rate, they become much slower than standard convolutions, even though they are performing fewer mathematical operations. I observed this on several different GPUs, including T4, P100 and H100. The following example demonstrates this issue using 1D convolutions across different kernel sizes and channel sizes. I tried 3 different methods to implement depthwise convolutions: 1. Use `lax.conv_general_dilated(…, feature_group_count=num_channels)`. 2. Use `vmap` to map `lax.conv_general_dilated(…, feature_group_count=1)` across the `out_channels` dim of both the input and the kernel. 3. Loop across slices of the `out_channels` dim of both the input and the kernel, then call `lax.conv_general_dilated(…, feature_group_count=1)` on these slices and concatenate the results. ```python from functools import partial import jax import numpy as np import pandas as pd import seaborn as sns from jax import lax from jax import numpy as jnp from matplotlib import pyplot as plt batch_size = 32 num_steps = 2**10 def get_xk(kernel_size: int, num_channels: int, dilation: int, is_depthwise: bool):     fn = lambda shape: jnp.arange(size := int(np.prod(shape)), dtype=jnp.float32).reshape(shape) / size     x = fn((batch_size, num_steps + dilation * (kernel_size  1), num_channels))     k = fn((kernel_size, 1 if is_depthwise else num_channels, num_channels))     return x, k conv = partial(     lax.conv_general_dilated,      window_strides=(1,),     padding=""VALID"",     dimension_numbers=(""NTC"", ""TIO"", ""NTC""), ) def standard(x, k, dilation: int):     return conv(x, k, rhs_dilation=(dilation,)) def depthwise(x, k, dilation: int):     num_channels = x.shape[1]     return conv(x, k, rhs_dilation=(dilation,), feature_group_count=num_channels) def depthwise_vmap(x, k, dilation: int):     fn = jax.vmap(partial(conv, rhs_dilation=(dilation,)), in_axes=2, out_axes=2)     return fn(x[..., jnp.newaxis], k[..., jnp.newaxis]).squeeze(axis=1) def depthwise_concat(x, k, dilation):     num_channels = x.shape[1]     ys = [conv(x[..., i: i + 1], k[..., i: i + 1], rhs_dilation=(dilation,)) for i in range(num_channels)]     return jnp.concatenate(ys, axis=1) df = [] for kernel_size in [1, 4, 16, 64]:     for num_channels in [32, 128, 512]:         for dilation in [2**i for i in range(8)]:             depthwise_y = None             for i, fn in enumerate([standard, depthwise, depthwise_vmap, depthwise_concat]):                 fn_name = fn.__name__                 print(f""{fn_name:s}, {kernel_size=}, {num_channels=}, {dilation=}"")                 is_depthwise = i > 0                 x, k = get_xk(kernel_size, num_channels, dilation, is_depthwise)                 fn = jax.jit(partial(fn, dilation=dilation))                 y = jax.block_until_ready(fn(x, k))   compile fn                 assert y.shape == (batch_size, num_steps, num_channels)                 if is_depthwise:                     assert depthwise_y is None or jnp.allclose(y, depthwise_y)                     depthwise_y = y                 t = %timeit o n 10 jax.block_until_ready(fn(x, k))                 df.append([fn_name, kernel_size, num_channels, dilation, t.average])             print() df = pd.DataFrame(df, columns=[""conv_fn"", ""kernel_size"", ""num_channels"", ""dilation"", ""time_s""]) sns.relplot(     df,     x=""dilation"",     y=""time_s"",     row=""kernel_size"",     col=""num_channels"",     hue=""conv_fn"",     style=""conv_fn"",     kind=""line"",     markers=True,     facet_kws=dict(sharey=False), ).set(yscale=""log"") plt.xscale(""log"", base=2); ``` !Image Some observations: * With method 1 (`feature_group_count=num_channels`), depthwise convolutions are often faster than standard convolutions for small dilation rates, but their performance scales poorly as we increase the dilation rate. * Method 2 (`vmap`) has the same performance as method 1 — inspecting the `jaxpr`, I see that it is indeed calling `lax.conv_general_dilated(…, feature_group_count=num_channels)` under the hood. * Method 3 (`concatenate`) is slower than the other 2 methods for small dilation rates, but it scales much better as we increase the dilation rate. For large kernel and channel sizes and `dilation>1`, it is also considerably faster than standard convolutions.  * Standard convolutions also have a noticeable bump when going from `dilation=1` to `dilation=2`, but aside from that, their performance is similar across higher dilations rates. For large kernel sizes, this bump is particularly notable. * Even for `kernel_size=1`, where I believe dilation has no effect, all depthwise methods show a bump when going from `dilation=1` to `dilation=2`. We can easily work around this by using `dilation=1` here, of course, but just thought I'd include this in case it helps in the diagnosis. I would appreciate it if someone can help shed some light on this issue, or maybe share some creative workarounds to get more consistent performance from depthwise convolutions with dilation. Thank you for your help!  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.33 jaxlib: 0.4.33 numpy:  1.26.4 python: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] jax.devices (1 total, 1 local): [CudaDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='5c396733a675', release='6.6.56+', version=' CC(Python 3 compatibility issues) SMP PREEMPT_DYNAMIC Sun Nov 10 10:07:59 UTC 2024', machine='x86_64') $ nvidiasmi Sun Feb  2 19:24:20 2025        ++  ++ ```",2025-02-02T20:19:23Z,bug performance NVIDIA GPU,open,0,1,https://github.com/jax-ml/jax/issues/26266,"I came up with yet another implementation for depthwise convolutions, which just expands the depthwise kernel into a standard kernel and then runs a standard convolution: ```python def depthwise_sparse(x, k, dilation):     num_channels = x.shape[1]     k = np.eye(num_channels)[np.newaxis] * k     return conv(x, k, rhs_dilation=(dilation,)) ``` I verified that this produces the same result as the other depthwise methods described above, and it has the same performance as standard convolutions. This still feels less than satisfactory since we don't reap the performance benefits of depthwise convolutions."
yi,Support Bounded Shape Input,"JAX currently supports dynamic shape input through padding or bucket padding. However, this approach isn't always ideal.  For example, some models require complex logic within their structure to manage padding.  Consider the Pixtral encoder as an example. Padding input image shapes to minimize recompilation requires additional logic for position embedding and masks, as these are dependent on the input shapes. This padding logic isn't immediately obvious to users. What's more, different models need different padding logic, that is really a large challenge to make jax/xla support such models. This feature would allow the compiler to handle padding internally, simplifying user code and improving the overall experience.  This enhancement requires work on both the JAX framework and the XLA compiler.  Discussions with the XLA team have confirmed that XLA already supports this capability. In the example below, xla compiler needs additional scalar input(s) to know the actual shape of the `param`. ```   param = s32[4] parameter(0)   size = s32[] parameter(1)   param_dynamic = s32[<=4] setdimensionsize(param, size)   // ... use param_dynamic in the body of the program...   output_dynamic = s32[<=4] ....   output_size = s32[] getdimensionsize(output_dynamic)   output_static = s32[4] removedynamicsize(output_dynamic)   ROOT (s32[4], s32) tuple(output_static, output_size) ``` Enabling XLA's bounded shape capabilities within JAX would be highly beneficial. One possible solution could involve the following steps: 1. Augment the jax.Array class with a new attribute to store the actual shape of the array. 2. Introduce a method for creating a ""bounded shape array"" derived from a standard array. 3. Improve the lowering process to ensure that these bounded shape arrays can be correctly translated into HLO (`param`, `size` and `setdimensionsize`)",2025-02-02T18:51:24Z,enhancement,open,0,0,https://github.com/jax-ml/jax/issues/26265
yi,Out-of-memory error when processing large images with overlapping patch-based gradients," Description Hello! I am encountering an OutofMemory (OOM) issue when performing patchbased image processing using JAX. The core idea is to process an image by patches, applying a Fourier transform and a learned kernel in the frequency domain. Due to overlapping patches, I need to compute gradients over the entire image rather than individual patches. To manage memory, I initially attempted a `fori_loop` to iterate over patches instead of processing them using vmap. However, even with this approach, I experience an OOM error, despite the gradient computation being applied to only a subset of the patches.  Code Overview The patch processing is implemented in the `PatchTransformModule`, which: 1. Extracts patches from the image. 2. Applies a transformation function (in this case, Fourierbased filtering, simplified version of what I use in my codebase). 3. Accumulates the transformed patches back into an output image. 4. Supports computing gradients for a subset of patches, controlled by `grad_fraction` or `grad_num_patches`. Fully working code (if you have a lot of memory on your machine you can lower the stride to be sure to have OOM): PS: if the code do the execution with any size without the backward pass, I do not have memory issue, it is only with the backward pass that there is an issue. ```python from typing import Callable, NamedTuple, Optional import chex import jax import jax.numpy as jnp from flax import linen as nn from jax import jit  def fft2(     data: jnp.ndarray, ) > jnp.ndarray:   todo put that into a utilities that can be used in the whole project     """"""Applies a centered 2D FFT to the input data.""""""     data = jnp.fft.ifftshift(data, axes=(2, 1))     data = jnp.fft.fftn(data, axes=(2, 1))     data = jnp.fft.fftshift(data, axes=(2, 1))     return data  def ifft2(data: jnp.ndarray) > jnp.ndarray:     """"""Applies a centered 2D inverse FFT to the input data.""""""     data = jnp.fft.ifftshift(data, axes=(2, 1))     data = jnp.fft.ifftn(data, axes=(2, 1))     data = jnp.fft.fftshift(data, axes=(2, 1))     return data class PatchTransformResults(NamedTuple):     """"""NamedTuple for the patch transform result.""""""     output: jnp.ndarray     accum: jnp.ndarray     counts: jnp.ndarray class PatchTransformModule(nn.Module):     """"""     Flax Module that does a patchbased transform with a fori_loop.     Each patch is extracted, transformed, and then placed into an accumulator.     """"""     size_in: int     size_out: int     stride: int     transform_fn: Callable[[jnp.ndarray], jnp.ndarray]     padding: str = ""homogeneous""     grad_fraction: Optional[float] = None   Fraction of patches to use for gradients     grad_num_patches: Optional[int] = None   Exact number of patches to use for gradients     def setup(self):         if (self.size_in % 2) != (self.size_out % 2):             raise ValueError(                 f""patch_size_in={self.size_in} and patch_size_out={self.size_out} ""                 ""must have the same parity (both odd or both even).""             )     def __call__(self, image: jnp.ndarray, key: chex.PRNGKey) > PatchTransformResults:         """"""         Forward pass with partial gradient usage.         """"""          1) Pad the image         image_padded, pad_top, pad_left = self._pad_image(image)          2) Prepare accum / counts         accum = jnp.zeros_like(image_padded, dtype=jnp.float32)         counts = jnp.zeros_like(image_padded, dtype=jnp.float32)          3) Build patch coordinates         coords = self._build_patch_coords(image.shape, image_padded.shape, pad_top, pad_left)         num_patches = coords.shape[0]          4) Determine gradient coords vs nogradient coords         if self.grad_fraction is not None or self.grad_num_patches is not None:             if self.grad_num_patches is not None:                 num_grad_patches = min(self.grad_num_patches, num_patches)             else:                 num_grad_patches = int(self.grad_fraction * num_patches)             patch_indices = jax.random.choice(                 key,                 num_patches,                 shape=(num_grad_patches,),                 replace=False             )             coords_grad = coords[patch_indices]             mask = jnp.ones(num_patches, dtype=bool).at[patch_indices].set(False)             coords_no_grad = coords[mask]         else:             coords_grad = coords             coords_no_grad = jnp.zeros((0, 2), dtype=coords.dtype)          Creation of the functions         def loop_body_patches(i, state, stop_grad=False):             nonlocal image_padded             if stop_grad:                 state = jax.lax.stop_gradient(state)                 image_padded = jax.lax.stop_gradient(image_padded)             accum_, counts_ = state             row, col = coords_current[i]              (a) Extract patch + (b) Transform patch             if stop_grad:                 patch_in = jax.lax.stop_gradient(self._extract_patch(image_padded, row, col))                 patch_out = jax.lax.stop_gradient(self.transform_fn(patch_in, *args, **kwargs))             else:                 patch_in = self._extract_patch(image_padded, row, col)                 patch_out = self.transform_fn(patch_in, *args, **kwargs)              (c) Place patch             center_r = row + (self.size_in  1) // 2             center_c = col + (self.size_in  1) // 2             accum_updated, counts_updated = self._place_patch(                 accum_, counts_,                 patch_out,                 center_r, center_c             )             return accum_updated, counts_updated         def loop_body_grad(i, state):             nonlocal coords_current             coords_current = coords_grad             return loop_body_patches(i, state, stop_grad=False)         def loop_body_no_grad(i, state):             nonlocal coords_current             coords_current = coords_no_grad             return loop_body_patches(i, state, stop_grad=True)          5) fori_loop over coords_grad (WITH gradient)         coords_current = coords_grad         if coords_grad.shape[0] > 0:             accum, counts = jax.lax.fori_loop(                 0,                 coords_grad.shape[0],                 loop_body_grad,                 (accum, counts)             )          6) fori_loop over coords_no_grad (WITHOUT gradient)         coords_current = coords_no_grad         if coords_no_grad.shape[0] > 0:             accum, counts = jax.lax.fori_loop(                 0,                 coords_no_grad.shape[0],                 loop_body_no_grad,                 (accum, counts)             )          7) Average & crop         output = jnp.where(counts > 0, accum / counts, 0.0)         image_height, image_width = image.shape[2:]         start_r = (output.shape[0]  image_height) // 2         start_c = (output.shape[1]  image_width) // 2         return PatchTransformResults(             accum=accum[start_r: start_r + image_height, start_c: start_c + image_width],             counts=counts[start_r: start_r + image_height, start_c: start_c + image_width],             output=output[start_r: start_r + image_height, start_c: start_c + image_width],         )     def _pad_image(self, image: jnp.ndarray):         """"""Pad the image based on the padding mode.""""""         pad_top = pad_bottom = pad_left = pad_right = 0         if self.padding == ""valid"":             pass   No padding         elif self.padding == ""same"":             pad = self.size_in // 2 if self.size_in % 2 == 0 else (self.size_in  1) // 2             pad_top = pad_bottom = pad_left = pad_right = pad         elif self.padding == ""homogeneous"":             offset = (self.size_in  1) // 2 if self.size_in % 2 == 1 else self.size_in // 2             pad_top = pad_bottom = pad_left = pad_right = offset         else:             raise ValueError(f""Unknown padding mode '{self.padding}'. Supported modes are 'valid', 'same', 'homogeneous'."")         image_padded = jnp.pad(             image,             ((pad_top, pad_bottom), (pad_left, pad_right)),             mode=""constant"",             constant_values=0.0         )         return image_padded, pad_top, pad_left     def _build_patch_coords(self, original_shape, padded_shape, pad_top, pad_left):         """"""Build the list of topleft coordinates for each patch.""""""         H, W = original_shape         padded_H, padded_W = padded_shape         if self.padding == ""homogeneous"":             row_centers = jnp.arange(0, H, self.stride)             col_centers = jnp.arange(0, W, self.stride)             rr, cc = jnp.meshgrid(row_centers, col_centers, indexing='ij')             centers = jnp.stack([rr.ravel(), cc.ravel()], axis=1)             offset = (self.size_in  1) // 2 if self.size_in % 2 == 1 else self.size_in // 2             coords = centers  offset + jnp.array([pad_top, pad_left])         else:             rows = jnp.arange(0, padded_H  self.size_in + 1, self.stride)             cols = jnp.arange(0, padded_W  self.size_in + 1, self.stride)             rr, cc = jnp.meshgrid(rows, cols, indexing='ij')             coords = jnp.stack([rr.ravel(), cc.ravel()], axis=1)         return coords     def _extract_patch(self, image_padded, top_left_r, top_left_c):         """"""Extract a patch from the padded image.""""""         return jax.lax.dynamic_slice(             image_padded,             (top_left_r, top_left_c),             (self.size_in, self.size_in)         )     def _place_patch(self, accum_, counts_, patch_out, center_r, center_c):         """"""Place the transformed patch into the accumulator.""""""         patch_size_out = self.size_out         top_left_r_out = center_r  (patch_size_out  1) // 2         top_left_c_out = center_c  (patch_size_out  1) // 2         update_slice = jax.lax.dynamic_update_slice(             jnp.zeros_like(accum_), patch_out, (top_left_r_out, top_left_c_out)         )         accum_updated = accum_ + update_slice         ones_block = jnp.ones((patch_size_out, patch_size_out), dtype=jnp.float32)         update_slice_counts = jax.lax.dynamic_update_slice(             jnp.zeros_like(counts_), ones_block, (top_left_r_out, top_left_c_out)         )         counts_updated = counts_ + update_slice_counts         return accum_updated, counts_updated class PatchSimulator(nn.Module):     patch_size_in: int = 96     patch_size_out: int = 64     patch_stride: int = 16     patch_padding: str = ""homogeneous""      grad_fraction: Optional[float] = None     grad_num_patches: Optional[int] = None     def setup(self):         super().setup()         self.kernel = self.param(             ""kernel"", nn.initializers.normal(1e1), (self.patch_size_in, self.patch_size_in)         )         self.patch_transform = PatchTransformModule(             size_in=self.patch_size_in,             size_out=self.patch_size_out,             stride=self.patch_stride,             padding=self.patch_padding,             transform_fn=self.transform_fn,             grad_fraction=self.grad_fraction,             grad_num_patches=self.grad_num_patches,         )     def transform_fn(self, x: jnp.ndarray) > jnp.ndarray:         x_fourier = fft2(x.astype(jnp.complex64))         x_processed = x_fourier * self.kernel         output = ifft2(x_processed).real         return output     def __call__(self, mask: jnp.ndarray, key: chex.PRNGKey  ++ ```",2025-02-01T22:07:50Z,bug type:support,closed,0,0,https://github.com/jax-ml/jax/issues/26263
rag,Add `num_updates` dimension to `input/output_offset` and `send/recv_sizes` of `ragged_all_to_all`.,"Add `num_updates` dimension to `input/output_offset` and `send/recv_sizes` of `ragged_all_to_all`. For now, only verify args since it isn't supported by the backends just yet.",2025-02-01T00:11:32Z,,open,0,0,https://github.com/jax-ml/jax/issues/26256
rag,Move `ragged_all_to_all` test under appropriate test file,Move `ragged_all_to_all` test under appropriate test file,2025-02-01T00:10:58Z,,closed,0,0,https://github.com/jax-ml/jax/issues/26255
yi,"Return arrays from `ArrayImpl._check_and_rearrange`. Build IFRT shardings with both addressable and non-addressable devices, instead of only addressable devices.","Return arrays from `ArrayImpl._check_and_rearrange`. Build IFRT shardings with both addressable and nonaddressable devices, instead of only addressable devices. This is a rollforward of two previous rollbacks after fixing breakages.",2025-01-31T23:45:30Z,,open,0,0,https://github.com/jax-ml/jax/issues/26253
rag,Replace Python 3.12 with Python 3.13 and add Python 3.10 to the matrix,Replace Python 3.12 with Python 3.13 and add Python 3.10 to the matrix Expands test coverage to cover the oldest and newest Python versions that we support.,2025-01-31T22:18:02Z,,open,0,0,https://github.com/jax-ml/jax/issues/26250
yi,Gradient of `jnp.linalg.norm` does not respect absolute homogeneity.," Description The norm) should satisfy $d= \left\Vert a \mathbf{b}\right\Vert = \left\vert a\right\vert \left\Vert\mathbf{b}\right\Vert$ for scalar $a$ and vector $\mathbf{b}$. Consequently $\frac{\partial d}{\partial a} = \mathrm{sign}\left(a\right) \left\Vert\mathbf{b}\right\Vert$. However, the gradient of the standard implementation of the $p$norm $\left(\sum_{i} b_i ^ p\right)^{1/p}$ is not defined at $\mathbf{b}=\mathbf{0}$ because applying the chain rule includes terms involving negative powers of zero. ```python >>> import jax >>> from jax import numpy as jnp >>> def func1(a, x): ...     return jnp.linalg.norm(a * x) >>> def func2(a, x): ...     return jnp.abs(a) * jnp.linalg.norm(x) >>> funcs = [func1, func2] >>> a = 1.3 >>> x = jnp.zeros(2) >>> for func in funcs: ...     print(f""{func.__name__}({a}, {x}) = {func(a, x)}"") ...     print(f""jax.grad({func.__name__})({a}, {x}) = {jax.grad(func)(a, x)}"") func1(1.3, [0. 0.]) = 0.0 jax.grad(func1)(1.3, [0. 0.]) = nan func2(1.3, [0. 0.]) = 0.0 jax.grad(func2)(1.3, [0. 0.]) = 0.0 ``` This particular situation arises, for example, in the evaluation of the diagonal of the Matern covariance kernel for Gaussian processes in more than one dimension. Specifically, for heterogeneous `length_scales`, the rescaled distance between two points `x` and `y` is `jnp.linalg.norm((x  y) / length_scales)`, and the derivative fails for `x == y`[^1]. This is not an issue for homogeneous `length_scales` because we can rewrite as `jnp.linalg.norm(x  y) / length_scales`.  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.5.0 jaxlib: 0.5.0 numpy:  2.2.1 python: 3.11.5 (main, Dec  8 2023, 17:04:09) [Clang 15.0.0 (clang1500.0.40.1)] device info: cpu1, 1 local devices"" process_count: 1 platform: uname_result(system='Darwin', node='TillsMacBookPro3.local', release='24.2.0', version='Darwin Kernel Version 24.2.0: Fri Dec  6 18:40:14 PST 2024; root:xnu11215.61.5~2/RELEASE_ARM64_T8103', machine='arm64') ``` [^1]: If we use a Mahalanobisstyle distance rather than just considering the product of kernels in different dimensions.",2025-01-31T21:46:59Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/26248,Ping  because he daydreams about autodiff corner cases
yi,Adjust docs/conf.py to remove prompt when copying docs examples by default,"This PR adjusts `conf.py` to remove the prompt and output of docs examples when copied using the copy button extension. I have also tried the `copybutton_prompt_text = "">>> ""` option, however this removes multiline inputs e.g. present when defining functions. It tested locally and it works as intended. Resolves CC(Adjust config of sphinxcopybutton to remove prompts and indentation by default?).",2025-01-31T17:04:27Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/26236
yi,Adjust config of sphinx-copybutton to remove prompts and indentation by default?,"Jax already uses the `sphinxcopybutton` extension to make code examples in the documentation copyable. However the copy currently still contains the prompt markers `>>>`. This is slightly annoying, when copy and pasting examples into notebooks or editors. `sphinxcopybutton` can be configured to exclude the prompt and indentation identifiers on copy, e.g. like this: ```python  https://sphinxcopybutton.readthedocs.io/en/latest/use.htmlusingregexppromptidentifiers copybutton_prompt_text = r"">>> \ {5,8}: "" copybutton_prompt_is_regexp = True ``` Which leads the copied content changing from: ```python >>> import jax.numpy as jnp >>> >>> vv = lambda x, y: jnp.vdot(x, y)    ([a], [a]) > [] >>> mv = vmap(vv, (0, None), 0)        ([b,a], [a]) > [b]      (b is the mapped axis) >>> mm = vmap(mv, (None, 1), 1)        ([b,a], [a,c]) > [b,c]  (c is the mapped axis) ``` To: ```python import jax.numpy as jnp vv = lambda x, y: jnp.vdot(x, y)    ([a], [a]) > [] mv = vmap(vv, (0, None), 0)        ([b,a], [a]) > [b]      (b is the mapped axis) mm = vmap(mv, (None, 1), 1)        ([b,a], [a,c]) > [b,c]  (c is the mapped axis) ``` I think this is the much more reasonable default.",2025-01-30T15:50:09Z,enhancement,closed,1,2,https://github.com/jax-ml/jax/issues/26210,Sounds reasonable – would you like to put together a pull request?," Yes, sure!"
rag,Skip ragged collective tests on CPU,"The raggedalltoall op isn't supported on CPU. The CPU test isn't executed by bazel, but when running with pytest, it looks like we need to explicitly skip. Fixes https://github.com/jaxml/jax/issues/26203",2025-01-30T15:05:13Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/26208
rag,Fix `lax.ragged_all_to_all` degenerate case,"Fix `lax.ragged_all_to_all` degenerate case In a singleton group case, unlike regular all_to_all, the ragged op becomes a generic equivalent of DynamicUpdateSlice, except update size is not statically known. This operation can't be expressed with standard HLO instructions  the backend will handle this case separately. Added small improvement to error messages.",2025-01-29T22:53:20Z,,closed,0,0,https://github.com/jax-ml/jax/issues/26198
yi,Return arrays from `ArrayImpl._check_and_rearrange`.,"Return arrays from `ArrayImpl._check_and_rearrange`. This is in preparation for a larger change, so that input buffers can be checked before Array creation in XLA and the user gets more helpful JAX error messages instead of XLA errors.",2025-01-28T22:14:10Z,,closed,0,0,https://github.com/jax-ml/jax/issues/26166
agent,Operation very slow to compile since jax 0.4.36," Description Hi folks, We've been having slow compilation issues since jax 0.4.36 with some of our JAX code.  The slow compilation (i.e. takes O(hours) to run instead of  Install packages  pip install upgrade jax[cuda] jaxlib  pip install upgrade mujoco pip install upgrade mujoco_mjx pip install upgrade brax   ``` mujoco==3.2.7 mujocomjx==3.2.7 brax==0.12.1 ``` Run this Python code: ```python import functools from mujoco_playground import registry from mujoco_playground import wrapper from mujoco_playground.config import manipulation_params from brax.training.agents.ppo import train as ppo from brax.training.agents.ppo import networks as ppo_networks env_name = 'PandaRobotiqPushCube' env = registry.load(env_name) env_cfg = registry.get_default_config(env_name) ppo_params = manipulation_params.brax_ppo_config(env_name) ppo_training_params = dict(ppo_params) network_factory = ppo_networks.make_ppo_networks if ""network_factory"" in ppo_params:   del ppo_training_params[""network_factory""]   network_factory = functools.partial(       ppo_networks.make_ppo_networks,       **ppo_params.network_factory   ) train_fn = functools.partial(     ppo.train, **dict(ppo_training_params),     network_factory=network_factory, ) make_inference_fn, params, metrics = train_fn(     environment=env,     wrap_env_fn=wrapper.wrap_for_brax_training, ) ``` The corresponding XLA dump is attached. I also reran the same script with `num_evals=0` within `train_fn`, and the code runs fine (the slow compilation occurs somewhere here). I'm attaching both the working and nonworking XLA dumps. We would really appreciate any help on this issue. xla_dump_hanging_compilation.tar.gz xla_dump_working.tar.gz  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.5.0 jaxlib: 0.5.0 numpy:  1.26.4 python: 3.12.3 (main, Sep 10 2024, 15:47:39) [GCC 13.2.0] device info: NVIDIA GeForce RTX 40901, 1 local devices"" process_count: 1 platform: uname_result(system='Linux', node='btaba.mtv.corp.google.com', release='6.10.111rodete2amd64', version=' CC(Python 3 compatibility issues) SMP PREEMPT_DYNAMIC Debian 6.10.111rodete2 (20241016)', machine='x86_64') $ nvidiasmi Tue Jan 28 13:30:18 2025        ++  ++ ```",2025-01-28T21:31:36Z,bug,open,0,7,https://github.com/jax-ml/jax/issues/26162,Related to: https://github.com/google/brax/issues/569 https://github.com/googledeepmind/mujoco_playground/issues/11  , Would appreciate your help here 🙏,HLO reproducer: https://gist.github.com/jreiffers/b6b8427ef64c00e688e79fd5af25b571 ``` bazel run config=cuda //xla/tools:run_hlo_module c opt   xla_disable_all_hlo_passes input_format=hlo random_init_input_literals platform=CUDA /tmp/repro.hlo ``` I'll take a look which pass is blowing up. Hopefully it's not in LLVM :).,"I initially suspected the unnecessary concatenate at the end, but it happens even when that's fixed manually. There are at least two issues here: 1. codegen/emitters/computation_partitioner., because of the bitcasts above the concatenate. The partitioner does not track the real indexing, so after the bitcast it considers the inputs to the bitcasts (e.g. `add.56926.1.clone.1`) to have users with inconsistent indexing (the root tuple and the bitcast), which cascades all the way to the parameters. We end up with lots of functions even though ~everything is elementwise. 2. Afterwards, inlining breaks, generating a huge function. I stopped it after a few hundred thousand ops. Not entirely sure yet what's happening here, maybe failed/insufficient CSE or canonicalization. The correct fix for 1. is to use proper indexing maps in the partitioner. A quick hack is to change `all_users_elementwise` in `computation_partitioner.cc` to this: ```     bool all_users_elementwise =         absl::c_all_of(instr>users(), & {           return HloInstruction::IsOpElementwise(user>opcode())                   user>opcode() == HloOpcode::kBitcast;         }); ``` However, I'm not sure that's safe.  could you give that a try and see what breaks?", Thank you for the reproducer. Trying.,The quick hack did not work. I'll try to migrate us to the indexing maps next week.,"Thanks  and  for taking a look, really appreciate it!"
yi,CPU slowdown with new runtime (v0.4.32 and newer)," Description Thanks a lot for your efforts in building JAX, I love working with it! On my MacBook Pro CPU (M3), my differentiable simulator runs 5x to 10x slower on new versions of JAX (`v0.4.32` or newer) as compared to older versions (`v0.4.31` or older). Setting the following xlaflag fixes the issue in newer versions of JAX for me (i.e. speed is as before): ```python import os os.environ['XLA_FLAGS'] = 'xla_cpu_use_thunk_runtime=false' ``` Unfortunately, 5x slower runtime would probably kill any relevant usecase of my simulator. As such, I have two questions:  Will the old CPU runtime continue to be maintained via the XLA flag?  Do you have any obvious candidates for operations that could cause this behavior? Any ideas on where I should start looking? Related to CC(CPU Slowdown introduced in 0.4.32  and in following versions) and CC(Performance degradation of `fftn` with the new CPU runtime)  Thanks a lot!  To reproduce As my codebase is fairly large, I cannot easily provide a selfcontained example without relying on my toolbox. To reproduce: ``` pip install jaxley==0.1.2 ``` and ```python from jax import config config.update(""jax_platform_name"", ""cpu"") config.update(""jax_enable_x64"", True) import time from jax import jit import jaxley as jx from jaxley.channels import HH import os os.environ['XLA_FLAGS'] = 'xla_cpu_use_thunk_runtime=false' comp = jx.Compartment() branch = jx.Branch(comp, nseg=4) cell = jx.Cell(branch, parents=[1] + [b // 2 for b in range(0, 2**8  2)]) cell.insert(HH()) cell.branch(0).comp(0).record() cell.make_trainable(""radius"") params = cell.get_parameters()  def simulate(params):     return jx.integrate(cell, params=params, t_max=1_000.0) start_time = time.time() simulate(params).block_until_ready() print(""Compile time: "", time.time()  start_time) start_time = time.time() simulate(params).block_until_ready() print(""Run time: "", time.time()  start_time) ``` On my MacBook Pro (and JAX `v0.5.0`, jaxlib `v0.5.0`, Python `3.12`), I get: ``` Compile time:  7.672 Run time:      2.067 ``` Removing `os.environ['XLA_FLAGS'] = 'xla_cpu_use_thunk_runtime=false'`, I get: ``` Compile time:  13.620 Run time:      11.293 ```   System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.5.0 jaxlib: 0.5.0 numpy:  2.2.2 python: 3.12.8  (main, Dec 11 2024, 10:37:40) [Clang 14.0.6 ] device info: cpu1, 1 local devices"" process_count: 1 platform: uname_result(system='Darwin', node='MichaelsMBP', release='23.4.0', version='Darwin Kernel Version 23.4.0: Fri Mar 15 00:11:08 PDT 2024; root:xnu10063.101.17~1/RELEASE_ARM64_T8122', machine='arm64') ```",2025-01-28T10:18:16Z,bug,open,1,4,https://github.com/jax-ml/jax/issues/26145,"EDIT: When using `float`, the gap between the old versions and new versions of JAX is even more prominent (almost 10x then...) ``` config.update(""jax_enable_x64"", False) ```","Ping  and  for CPU thunks runtime performance As noted, it's hard to say too much without a more minimal reproducer, but to answer the specific questions: > Will the old CPU runtime continue to be maintained via the XLA flag? I think there continue to be enough performance regressions that we would like to keep this working for the time being. I think the right approach is to suggest the use of that flag for now with the assumption that the reported performance regressions will be reasonably addressed before it is removed.  and  might be able to comment more about timelines on the XLA side! > Do you have any obvious candidates for operations that could cause this behavior? Any ideas on where I should start looking? The most common culprit in my experience is loops (like scan or while_loops), although sometimes these can be implicit. If you have explicit `scan`s in your library, you can experiment with trading off compile time with runtime performance using the `unroll` parameter (increase that for longer compile times, but typically better runtime performance.)","We plan to fix performance regression in then next couple of weeks (), and only after that we'll start removing the old runtime.","That's great to know, thanks a lot for the quick response! My simulator is an ODE which indeed uses a `scan` (across 40k timepoints), so this might well be the culprit. Unrolling is not an option for me because compile time becomes excessive. For now, I will simply use the old CPU runtime via the XLA flag. Thank you for the suggestions!"
yi,jax.experimental.mosaic.gpu.profiler.measure is missing a warmup?," Description I'm getting a runtime too long the first time I profile. I expect `profiler.measure` to take care of compiling and warming up. ```python import jax.numpy as jnp from jax.experimental.mosaic.gpu import profiler inputs = [jnp.zeros((4000, 4000)), jnp.zeros((4000, 4000))] def f(x, y):     out = 0     for i in range(0, 4000, 1000):         out += x[i : i + 1000].T @ y[i : i + 1000]     return out _, runtime = profiler.measure(f, mode=""events"")(*inputs) print(f""Runtime measured with events: {runtime:.2f} ms"")  Runtime measured with events: 1.73 ms _, runtime = profiler.measure(f, mode=""cupti"")(*inputs) print(f""Runtime measured with cupti: {runtime:.2f} ms"")  Runtime measured with cupti: 215.84 ms 😱😱😱 _, runtime = profiler.measure(f, mode=""cupti"")(*inputs) print(f""Runtime measured with cupti: {runtime:.2f} ms"")  Runtime measured with cupti: 1.61 ms ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.5.0 jaxlib: 0.5.0 numpy:  2.2.2 python: 3.12.7 (main, Oct  1 2024, 08:52:12) [GCC 11.4.0] device info: NVIDIA RTX 5880 Ada Generation2, 2 local devices"" process_count: 1 platform: uname_result(system='Linux', node='db1ac34lcedt', release='6.8.049generic', version=' CC(Update neural_network_and_data_loading.ipynb)~22.04.1Ubuntu SMP PREEMPT_DYNAMIC Wed Nov  6 17:42:15 UTC 2', machine='x86_64') $ nvidiasmi Tue Jan 28 02:08:14 2025        ++  ++ ```",2025-01-28T10:11:54Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/26144,"Originally I skipped warmup in the CUPTIbased implementation since it isn't affected by host runtime (unlike the eventsbased implementation). But as you are showing here, there are cases when the measurements in the first run can be affected by the autotuner on the device: ```python  note the aggregate=False fprof = profiler.measure(f, mode=""cupti"", aggregate=False) _, runtime_warmup = fprof(*inputs) _, runtime        = fprof(*inputs) print(f""{len(runtime_warmup)=}"")  len(runtime_warmup)=881  the XLA autotuner runs many many kernels print(f""{len(runtime)=}"")  len(runtime)=5  only 5 are actually required for execution print(f""{runtime=}"")  runtime=[('gemm_fusion_dot_10_0', 0.443844), ('gemm_fusion_dot_6_0', 0.443973), ('gemm_fusion_dot_20_0', 0.444996), ('gemm_fusion_dot_15_0', 0.444293), ('loop_add_fusion', 0.350275)] ``` Maybe we should change the CUPTIbased profiler to warm up just like the eventsbased profiler does."
rag,Run test job irrespective of if the build jobs succeeds or fails ,"Run test job irrespective of if the build jobs succeeds or fails  This lets us avoid losing test coverage if a single unrelated build job fails. E.g Windows build job fails but everything else succeeds. In this case, we still want to run the tests for other platforms. Also, if a build job fails, its corresponding test job will also report a failure as a result of not being able to download the wheel artifact so we should still be able to tell the source of job failure easily.",2025-01-27T22:26:12Z,CI Connection Halt - On Retry,open,0,0,https://github.com/jax-ml/jax/issues/26135
yi,Exceptions from `jax.pure_callback` have no `__cause__`," Description `jax.pure_callback` can raise any arbitrary exception: ```python import jax import jax.numpy as jnp .jit def f(x):     def eager(x):         assert False, ""Hello world""     return jax.pure_callback(eager, jax.ShapeDtypeStruct((), x.dtype), x)) f(jnp.asarray(1)) XlaRuntimeError: INTERNAL: CpuCallback error: Traceback (most recent call last):   File ""/home/crusaderky/github/arrayapiextra/.pixi/envs/dev/bin/ipython"", line 10, in    ...   File ""/home/crusaderky/github/arrayapiextra/.pixi/envs/dev/lib/python3.12/sitepackages/jax/_src/callback.py"", line 71, in __call__   File """", line 7, in eager AssertionError: Hello world ``` The problem is that `XlaRuntimeError` is not just reraised from the original exception  it has completely replaced it. It has `__cause__` set to `None`, which makes it very hard to write tests that iterate on multiple backends, like https://github.com/scipy/scipy/blob/main/scipy/_lib/_lazy_testing.py / https://github.com/dataapis/arrayapiextra/blob/main/src/array_api_extra/testing.py allows for: ```python import array_api_extra as xpx from array_api_extra.testing import lazy_xp_function def f(x):     def eager(x):         assert False, ""Hello world""     return xpx.lazy_apply(eager, x, shape=(), dtype=x.dtype) lazy_xp_function(f) def test_f(xp):     x = xp.asarray(1)     with pytest.raises(AssertionError, match=""Hello world""):         f(x) ``` Same as above, but without scipy's and array_api_extra's magic: ```python import numpy import array_api_strict import cupy import jax import jax.numpy import torch import pytest def f(x):     def eager(x):         assert False, ""Hello world""     if isinstance(x, jax.Array):         return jax.pure_callback(eager, jax.ShapeDtypeStruct((), x.dtype), x)     else:         return eager(x) .mark.parametrize(""xp"", [numpy, array_api_strict, cupy, jax.numpy, torch]) def test_f(xp):     f_ = jax.jit(f) if xp is jax.numpy else f     x = xp.asarray(1)     with pytest.raises(AssertionError, match=""Hello world""):         f_(x) ``` The above test fails on jax, because it raises a XlaRuntimeError instead of an AssertionError.  Expected behaviour `XlaRuntimeError.__cause__` should be the original `AssertionError`. At that point, `lazy_xp_function` and similar test tools will have an easy time unwrapping the exception under the hood, so that the tests don't have to think about it.  Workaround Write an adhoc workaround to `pytest.raises`: ```python  def lazy_raises(xp, exc_type, match):     if is_jax_namespace(xp):         match = f""{exc_type.__name__}: {match}""         exc_type = RuntimeError     with pytest.raises(exc_type, match=match):         yield ``` IMHO it would be very ugly to have such a wrapper littered all over the scipy tests.  System info (python version, jaxlib version, accelerator, etc.) jax 0.4.35",2025-01-25T23:40:22Z,bug,open,1,16,https://github.com/jax-ml/jax/issues/26102,I suspect that this would also be useful to 'improve' `equinox.error_if` which is quite useful in some circumstances. Right now the XLARuntimeError makes the stack trace/error message quite hard to read.  ,"Thanks for the tag! So  you can probably work around JAX's lack of support for runtime errors in the exact same way that Equinox does: wrap both the callback and the JIT, and smuggle the exception between the two using global state. It's ugly, but it works! (We do the exact same thing between `eqx.filter_jit` and `eqx.error_if` to support pretty readable runtime error messages.) Here's a quick demo: ```python import functools as ft import logging import weakref import jax _better_fs = weakref.WeakKeyDictionary() _last_e = None class _FilterCallback(logging.Filterer):     def filter(self, record: logging.LogRecord):         return not (             record.name == ""jax._src.callback""             and record.getMessage() == ""jax.pure_callback failed""         ) .wraps(jax.pure_callback) def better_pure_callback(f, *args, **kwargs):     try:         better_f_impl = _better_fs[f]     except KeyError:         def better_f_impl(*args, **kwargs):             global _last_e             try:                 return f(*args, **kwargs)             except Exception as e:                 assert _last_e is None                 _last_e = e                 raise         _better_fs[f] = better_f_impl     return jax.pure_callback(better_f_impl, *args, **kwargs) def _strip_frame(module_name: str) > bool:     return module_name.startswith(""jax._src"") or module_name == __name__ .wraps(jax.jit) def better_jit(f):     jit_f = jax.jit(f)     def better_jit_impl(*args, **kwargs):         global _last_e         filter = _FilterCallback()         callback_logger = logging.getLogger(""jax._src.callback"")         callback_logger.addFilter(filter)         try:             out = jit_f(*args, **kwargs)             return out         except jax.errors.JaxRuntimeError as e:             if _last_e is None:                  Something else went wrong, go back to default JAX behaviour.                 to_raise = e             else:                 to_raise = _last_e                 _last_e = None         finally:             callback_logger.removeFilter(filter)          Now raise outside the `except` block so we don't adjust any `__context__`.          We do this in a 'pointless' tryexcept just so that we can strip the current          frame from the traceback.         try:             raise to_raise         except Exception as e:             tb = e.__traceback__             while tb is not None and _strip_frame(                 tb.tb_frame.f_globals.get(""__name__"", """")             ):                 tb = tb.tb_next             e.__traceback__ = tb             raise     return better_jit_impl ``` Try it out: ```python import jax from the_above_file import better_pure_callback, better_jit  def f(x):     def eager(x):         assert False, ""Hello world""     return better_pure_callback(eager, jax.ShapeDtypeStruct((), x.dtype), x) f(jax.numpy.asarray(1)) ```  FWIW I'd suggest only using the above in tests, and not as part of any public API. If you're going to wrap `jax.jit` for public use then there are other changes I'd still recommend making here  for example preserving the pytreeness of `f`.","Note that raising an exception is a sideeffect, so technically it's a misuse of `pure_callback` and is unsupported. With the current implementation, doing so will lead to a particular behavior, but that behavior may change in future JAX versions because it's essentially an accidental implementation detail. The fix would be to not rely on raising errors inside `pure_callback`. It's true that packages like `equinox` wrap these implementation details in their public APIs, but this has never been an approach that the JAX team has recommended, and we would not suggest relying on such implementations in important code.","> Note that raising an exception is a sideeffect, so technically it's a misuse of `pure_callback` I can make a strong case for exceptions being a return value. The case becomes evident if you have familiarity with how error propagation works in languages such as Rust and Go. Python's `try... except` is just syntax sugar; you could rewrite all python code to return a tuple of [return value, error] and it would be functionally the same as before. > and is unsupported. With the current implementation, doing so will lead to a particular behavior, but that behavior may change in future JAX versions because it's essentially an accidental implementation detail. I'm not sure I understand  are you saying that, in the future, if a `pure_callback` raises JAX may decide to swallow the exception, e.g. fabricate a fake output array full of NaNs, and continue? I believe this is completely unreasonable. If you look at codebases such as scipy, there is _extensive_ use of eager validation, typically by calling `__bool__`, followed by the actual calculation. Validation can prevent things such as segmentation faults and infinite loops, like the equinox documentation illustrates. It would be a major burden to be forced to put validation and the calculation that _needs_ the validation to happen within the same `pure_callback` block, and it would make it impossible to write code that works on multiple array API backends including JAX without having a person with expert knowledge of JAX read through every single line of the code and ensure that every single edge case of malformed input is thoroughly unit tested. So I urge you to formalize that `pure_callback` functions can raise, and if they do whatever follows won't be executed and that when the user finally materializes the graph the exception will be reraised (either the original exception or JAX's own exception with the original one as its `__cause__`).  Aside: `array_api_extra.lazy_raise`, which I'm the middle of writing now, is almost exactly the same as `equinox.error_if`. I didn't even know that equinox existed and just naturally converged towards the same design. I think that speaks volumes on real world needs.","> I'm not sure I understand  are you saying that, in the future, if a `pure_callback` raises JAX may decide to swallow the exception, e.g. fabricate a fake output array full of NaNs, and continue? I'm saying the current behavior is undefined, so relying on the details of the current implementation is not a good idea. I understand that eager validation is desirable in some programming contexts. What I'm telling you is that it's not currently supported in JAX, and working around that by misusing existing APIs is not a viable longterm solution.","FWIW  whilst I obviously am also in favour of exceptions continuing to behave as they do now  Equinox actually includes a guard against that behaviour ever changing: if the computation proceeds past the raising `pure_callback`, then we hit a *second* `pure_callback` that then sits in an infinite loop: https://github.com/patrickkidger/equinox/blob/f8df0e648bf3af8c9b041fc76f58fcc5b4ab37c6/equinox/_errors.pyL113L116 Not as easy to debug for an end user, but it's a JAXspeccompatible backup to ensure that even then, no downstream computation ever occurs.","Regarding the root question here — the request that the `XLARuntimeError` raised when a computation fails should set its `__cause__` to the error that is raised in the pure callback — I'm not sure whether that's feasible. The problem is that it's not just Python code calling a Python callback — it's Python code on the host launching an XLA process, which calls back to Python, then returns a nonpython error code with the string representation of the error encountered in the callback, and this error code results in an `XLARuntimeError` in the host. ~In general I don't believe there's any requirement that the hostside code and the callback are even running in the same process, and I don't think you can have a `__cause__` attribute that links to exceptions from another process.~ Edit: I think I was wrong: callbacks do need to be on the same process as the host/tracing code, so we could do something like propagate the `id` of the callback error to XLA, pass that along from XLA back to the host, and then somehow access the object in memory at the specified id to get a reference to it. We could maybe do that through some kind of customized error handler for callbacks that ensures errors are added to a global registry before they are raised. It's kind of tricky given the computation model, and it wouldn't be a very clean solution I think. Given that the behavior of exceptions in callbacks is not welldefined, I don't think it's warranted to attempt those kinds of gymnastics at the moment. What do you think?","A broader comment: there's a reason that the array API standard does not require any valuedependent error semantics: there are packages for which such semantics are a nonstarter. It's fine to pursue adding such semantics in `arrayapiextra`, but doing so may disqualify packages like JAX which don't support such semantics. If scipy et al. cannot implement their API without going beyond what's available in the array API standard, then I suspect their choice is either to relax their requirements, or to recognize that they will not be able to support all standardcompliant libraries.","> it's Python code on the host launching an XLA process, which calls back to Python, then returns a nonpython error code You can just pickle whatever exception is raised by the userdefined function and unpickle it when the user finally queries a material answer with `__str__`, `__array__`, exc. This would be pretty much the same as what Dask does. Some userdefined exception struggle to pickle due to wellknown design mistakes in `Exception`, and you may encounter problems such as locallydefined classes or lambdas attached to them, but that's the users' problem. Just fall back to the current string if they fail to pickle.","Thanks  the pickle approach is an interesting idea. It might be a good option if we eventually choose to support exception handling within `pure_callback`. Still, it would not be worthwhile pursuing particular mechanisms of error propagation until the behavior of impure (exception raising) callbacks can be betterdefined. Note that although things work fairly straightforwardly now on CPU, this is not the case in general on accelerators – for example, TPU has no mechanism for runtime error handling, so an exception in a `pure_callback` does not currently end the process in the same way that it does on CPU. It's possible we could make headway here toward general support for runtime errors via `pure_callback`, but it would require figuring out the right computational model within the constraints of the TPU runtime.","> If scipy et al. cannot implement their API without going beyond what's available in the array API standard, then I suspect their choice is either to relax their requirements, or to recognize that they will not be able to support all standardcompliant libraries. Speaking to this point  `eqx.error_if` exists solely because of the needs of Diffrax, Optimistix etc. I've not been involved in the Array API standard, but some kind of ""if this then panic"" is indeed necessary for scicomp use cases. (Just a comment, I recognize your statements so far about this being undefined behavior.)","I was chatting about this with  and he mentioned an important point: the assumption that `pure_callback` callbacks have no sideeffects is baked deeply into how it interacts with JAX transformations, and if you use the mechanism discussed here for runtime value checks, you can easily end up with false positives. Here's a short example where `equinox.error_if` raises an unexpected error: ```python import equinox as eqx import jax import jax.numpy as jnp def cond(x):   return x = 5, msg=""x is very wrong!"")   return x + 1 .jit def f(x):   return jax.lax.while_loop(cond, body, x) x = jnp.arange(8) [f(xi) for xi in x]   no error jax.vmap(f)(x)   ERROR! ``` The problem is that `pure_callback` (which is used by `error_if`) transforms under `vmap` under the assumption that the callback is pure: violating that contract means that you violate the assumptions made by the batching rule, which can lead to incorrect program outputs.","You can recover the same issue in pure JAX: ```python import jax import jax.numpy as jnp def infinite_loop(x):   def cond(x):     return True   def body(x):     return x + 1   return jax.lax.while_loop(cond, body, x) def cond(x):   return x = 5, infinite_loop, lambda x: x, x)   return x + 1 .jit def f(x):   return jax.lax.while_loop(cond, body, x) x = jnp.arange(8) [f(xi) for xi in x]   no error print('hi')   assert that we get this far jax.vmap(f)(x)   infinite loop! ```","Ha, fair enough. I guess we assume pure  {can diverge} for these transformations but I think the point still stands (users tend to write code with errors in them but tend not to write code that may not halt).","> The problem is that `pure_callback` (which is used by `error_if`) transforms under `vmap` under the assumption that the callback is pure: violating that contract means that you violate the assumptions made by the batching rule, which can lead to incorrect program outputs. Again, a callback that raises _is pure_. Nobody asks you to change the execution graph depending on its output. You just need to propagate the exception along with the return value, treating the tuple of `(retval, exc)` as the output, and test for `exc is None` before you materialize the nodes down the graph. As if you were writing Rust or Go code.","> Again, a callback that raises _is pure_. You're arguing semantics, and this is very unhelpful. Yes, you can define ""pure"" in such a way that exceptions are allowed; but **that definition of pure is not relevant to JAX**. In JAX what matters is that all function behavior can be determined via its abstract trace: this does not admit runtime valuedependent Python exceptions, and so functions that rely on Python exceptions are not allowed. > You just need to propagate the exception along with the return value, treating the tuple of (retval, exc) as the output, and test for exc is None before you materialize the nodes down the graph. As if you were writing Rust or Go code. Sure, that's a very reasonable approach, and is essentially what `jax.experimental.checkify` does. It is not what JAX does by default, and changing JAX's computational model to do so by default would be a quite large project. We have some ongoing work to try to make the checkify approach more transparent, but unfortunately right now that's not a solution we can depend on. None of that changes what I've said above: in JAX's current implementation, you cannot depend on raising errors in `pure_callback`, and any solution based on that approach is a nogo. Please do not spend any more time on this."
yi,printing new style prng keys on the cpu results in XlaRuntimeError," Description Trying to print (or generally calling `__repr__` on) a new style PRNG key that has been put on the cpu results in an XlaRuntimeError.  ```python import jax key = jax.random.key(0) old_key = jax.random.PRNGKey(0) cpu = jax.devices(""cpu"")[0] print(old_key) print(key) print() print(jax.device_put(old_key, cpu)) print(jax.device_put(key, cpu)) ``` results in  ``` [0 0] Array((), dtype=key) overlaying: [0 0] [0 0]  XlaRuntimeError                           Traceback (most recent call last) [](https://localhost:8080/) in ()       8 print()       9 print(jax.device_put(old_key, cpu)) > 10 print(jax.device_put(key, cpu)) 3 frames /usr/local/lib/python3.11/distpackages/jax/_src/array.py in _value(self)     611     if self._npy_value is None:     612       if self.is_fully_replicated: > 613         self._npy_value = self._single_device_array_to_np_array()     614         self._npy_value.flags.writeable = False     615         return cast(np.ndarray, self._npy_value) XlaRuntimeError: INVALID_ARGUMENT: PjRtDevice not found: TFRT_CPU_0 ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.33 jaxlib: 0.4.33 numpy:  1.26.4 python: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0] jax.devices (1 total, 1 local): [CudaDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='6394d373e936', release='6.1.85+', version=' CC(Python 3 compatibility issues) SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024', machine='x86_64') $ nvidiasmi Sat Jan 25 17:52:22 2025        ++  ++ ```",2025-01-25T17:53:32Z,bug,open,0,10,https://github.com/jax-ml/jax/issues/26101,"I can't repro this with 0.5.0: ``` In [5]: import jax    ...:    ...: key = jax.random.key(0)    ...: old_key = jax.random.PRNGKey(0)    ...: cpu = jax.devices(""cpu"")[0]    ...: print(old_key)    ...: print(key)    ...: print()    ...: print(jax.device_put(old_key, cpu))    ...: print(jax.device_put(key, cpu)) [0 0] Array((), dtype=key) overlaying: [0 0] [0 0] Array((), dtype=key) overlaying: [0 0] ``` Please reopen if you can still repro with jax and jaxlib 0.5.0","When updating jax on colab, the issue persists ``` jax:    0.5.0 jaxlib: 0.5.0 numpy:  1.26.4 python: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0] device info: Tesla T41, 1 local devices"" process_count: 1 platform: uname_result(system='Linux', node='fbf0733fcd53', release='6.1.85+', version=' CC(Python 3 compatibility issues) SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024', machine='x86_64') $ nvidiasmi Sat Jan 25 18:07:54 2025        ++  ++ ``` The behaviour does not happen when in a cpu only runtime. It does happen in a T4 GPU runtime.",Maybe colab on GPU is not on the right cuda version. Can you try it out locally and do a clean install?,"Yes, I'll try to reproduce the bug on a local machine. I did stumble onto some strange result in the mean while though: I added the following code in a new cell in the notebook ```python with jax.default_device(cpu):     cpu_key = jax.device_put(key, cpu)     key_1, key_2 = jax.random.split(cpu_key)     print(key_1)     print(key_1.device)     print(key_2)     print(key_2.device)     print(cpu_key) ``` and the result is ``` Array((), dtype=key) overlaying: [1797259609 2579123966] TFRT_CPU_0 Array((), dtype=key) overlaying: [ 928981903 3453687069] TFRT_CPU_0  XlaRuntimeError                           Traceback (most recent call last) [](https://localhost:8080/) in ()       6     print(key_2)       7     print(key_2.device) > 8     print(cpu_key) 3 frames /usr/local/lib/python3.11/distpackages/jax/_src/prng.py in __repr__(self)     261      262   def __repr__(self): > 263     return (f'Array({self.shape}, dtype={self.dtype.name}) overlaying:\n'     264             f'{self._base_array}')     265  /usr/local/lib/python3.11/distpackages/jax/_src/array.py in __format__(self, format_spec)     328       return format(self._value[()], format_spec)     329     else: > 330       return format(self._value, format_spec)     331      332   def __getitem__(self, idx): /usr/local/lib/python3.11/distpackages/jax/_src/profiler.py in wrapper(*args, **kwargs)     331   def wrapper(*args, **kwargs):     332     with TraceAnnotation(name, **decorator_kwargs): > 333       return func(*args, **kwargs)     334     return wrapper     335   return wrapper /usr/local/lib/python3.11/distpackages/jax/_src/array.py in _value(self)     625     if self._npy_value is None:     626       if self.is_fully_replicated: > 627         self._npy_value = self._single_device_array_to_np_array()     628         self._npy_value.flags.writeable = False     629         return cast(np.ndarray, self._npy_value) XlaRuntimeError: INVALID_ARGUMENT: PjRtDevice not found: TFRT_CPU_0 ```","On my local machine, it happens as well. ``` jax:    0.5.0 jaxlib: 0.5.0 numpy:  2.2.2 python: 3.11.11 (main, Dec 11 2024, 16:28:39) [GCC 11.2.0] device info: NVIDIA GeForce RTX 30802, 2 local devices"" process_count: 1 platform: uname_result(system='Linux', node='tue029868', release='6.8.051generic', version=' CC(Scenarios to prefer over cupy)Ubuntu SMP PREEMPT_DYNAMIC Thu Dec  5 13:09:44 UTC 2024', machine='x86_64') $ nvidiasmi Sat Jan 25 19:36:00 2025        ++  ++ ```","I wonder what happens when you turn off this flag: jax.config.update('jax_threefry_partitionable', False)  do this immediately after you import jax I am assuming it shouldn't affect since you could repro with 0.4.33 too but doesn't hurt to try :)",The result is the same,"Actually, the same thing seems to happen when going from cpu to gpu ```python gpu = jax.devices(""gpu"")[0] cpu = jax.devices(""cpu"")[0] with jax.default_device(cpu):     key=jax.random.key(0) print(key) print(jax.device_put(key, gpu)) ``` results in  ``` Array((), dtype=key) overlaying: [0 0]  XlaRuntimeError                           Traceback (most recent call last) Cell In[2], line 7       5     key=jax.random.key(0)       6 print(key) > 7 print(jax.device_put(key, gpu)) File ~/miniconda3/envs/jax_test/lib/python3.11/sitepackages/jax/_src/prng.py:263, in PRNGKeyArray.__repr__(self)     262 def __repr__(self): > 263   return (f'Array({self.shape}, dtype={self.dtype.name}) overlaying:\n'     264           f'{self._base_array}') File ~/miniconda3/envs/jax_test/lib/python3.11/sitepackages/jax/_src/array.py:330, in ArrayImpl.__format__(self, format_spec)     328   return format(self._value[()], format_spec)     329 else: > 330   return format(self._value, format_spec) File ~/miniconda3/envs/jax_test/lib/python3.11/sitepackages/jax/_src/profiler.py:333, in annotate_function..wrapper(*args, **kwargs)     330 (func)     331 def wrapper(*args, **kwargs):     332   with TraceAnnotation(name, **decorator_kwargs): > 333     return func(*args, **kwargs)     334   return wrapper File ~/miniconda3/envs/jax_test/lib/python3.11/sitepackages/jax/_src/array.py:627, in ArrayImpl._value(self)     625 if self._npy_value is None:     626   if self.is_fully_replicated: > 627     self._npy_value = self._single_device_array_to_np_array()     628     self._npy_value.flags.writeable = False     629     return cast(np.ndarray, self._npy_value) XlaRuntimeError: INVALID_ARGUMENT: PjRtDevice not found: cuda:0 ```","Quick question, does this happen with any array, or just with keys? ```python gpu = jax.devices(""gpu"")[0] cpu = jax.devices(""cpu"")[0] with jax.default_device(cpu):     x=jax.numpy.arange(10) jax.device_put(x, gpu) ``` If it's happening with any array, then I suspect the issue relates to a mismatch in CUDA or CUDNN versions.","It only happens with new style keys. ```python import jax gpu = jax.devices(""gpu"")[0] cpu = jax.devices(""cpu"")[0] with jax.default_device(cpu):     x=jax.numpy.arange(10)     key1 = jax.random.PRNGKey(1)     key2 = jax.random.key(2) print(jax.device_put(x, gpu)) print(jax.device_put(key1, gpu)) print(jax.device_put(key2, gpu)) ``` In the above, only the last line results in an error."
yi,Suppress XLA slow operation alarm,"Quoting https://github.com/jaxml/jax/discussions/21914 : I'm trying to suppress the XLA warning about slow compilation that looks like: ``` 20240617 11:40:02.931165: E external/xla/xla/service/slow_operation_alarm.cc:65]  ******************************** [Compiling module xxx] Very slow compile? If you want to file a bug, run with envvar XLA_FLAGS=xla_dump_to=/tmp/foo and attach the results. ******************************** 20240617 11:40:31.962458: E external/xla/xla/service/slow_operation_alarm.cc:133] The operation took 2m29.0445135s ******************************** [Compiling module xxx] Very slow compile? If you want to file a bug, run with envvar XLA_FLAGS=xla_dump_to=/tmp/foo and attach the results. ******************************** ``` Any idea how to suppress this? I know why my code takes a while to compile and I'm okay with it  I'm just struggling to find any relevant XLA flags or relevant documentation.",2025-01-25T15:28:51Z,enhancement,open,0,0,https://github.com/jax-ml/jax/issues/26098
rag,[better_errors] Add more debug info test coverage,"Some of the tests added show missing or incorrect debug info, marked with TODO. Fixes will come separately.",2025-01-25T07:24:00Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/26093
llm,Custom primitives + ordered effects + linear_solve = bug in 0.5 ?," Description In mpi4jax we use some ordered effect to ensure that our mpi communication is ordered. Version 0.5 of jax breaks our ordered effect logic when we use our custom primitives inside of `jax.lax.custom_linear_solve`. The breakage only appears if the call is jitted, while it does not do nothing if it's not jitted. While this might be a bug on our end, the code was working previously, and I'm at a loss at what could be wrong. Do you have any suggestion on what could be going wrong? MWE: ```python import mpi4jax from mpi4py import MPI import jax from jax.scipy.sparse.linalg import cg k = jax.random.key(1) O = jax.random.normal(k, (24, 24)) b = jax.random.normal(k, (24,)) def mat_vec(v):     res, _ = mpi4jax.allreduce(v, op=MPI.SUM, comm=MPI.COMM_WORLD)     return res Aop = jax.tree_util.Partial(mat_vec)  works x, info = cg(Aop, b)  crashes x, info = jax.jit(cg)(Aop, b) ``` and the stack trace  ```python  JaxStackTraceBeforeTransformation         Traceback (most recent call last) File ~/Documents/pythonenvs/netket_pro/bin/ipython:8       7 sys.argv[0] = re.sub(r""(script\.pyw|\.exe)?$"", """", sys.argv[0]) > 8 sys.exit(start_ipython()) File ~/Documents/pythonenvs/netket_pro/lib/python3.13/sitepackages/IPython/__init__.py:130, in start_ipython()     129 from IPython.terminal.ipapp import launch_new_instance > 130 return launch_new_instance(argv=argv, **kwargs) File ~/Documents/pythonenvs/netket_pro/lib/python3.13/sitepackages/traitlets/config/application.py:1074, in launch_instance()    1073 app = cls.instance(**kwargs) > 1074 app.initialize(argv)    1075 app.start() File ~/Documents/pythonenvs/netket_pro/lib/python3.13/sitepackages/traitlets/config/application.py:118, in inner()     117 try: > 118     return method(app, *args, **kwargs)     119 except (TraitError, ArgumentError) as e: File ~/Documents/pythonenvs/netket_pro/lib/python3.13/sitepackages/IPython/terminal/ipapp.py:284, in initialize()     283 self.init_extensions() > 284 self.init_code() File ~/Documents/pythonenvs/netket_pro/lib/python3.13/sitepackages/IPython/core/shellapp.py:353, in init_code()     351  commandline execution (ipython i script.py, ipython m module)     352  should *not* be excluded from %whos > 353 self._run_cmd_line_code()     354 self._run_module() File ~/Documents/pythonenvs/netket_pro/lib/python3.13/sitepackages/IPython/core/shellapp.py:478, in _run_cmd_line_code()     477 try: > 478     self._exec_file(fname, shell_futures=True)     479 except: File ~/Documents/pythonenvs/netket_pro/lib/python3.13/sitepackages/IPython/core/shellapp.py:403, in _exec_file()     401             else:     402                  default to python, even without extension > 403                 self.shell.safe_execfile(full_filename,     404                                          self.shell.user_ns,     405                                          shell_futures=shell_futures,     406                                          raise_exceptions=True)     407 finally: File ~/Documents/pythonenvs/netket_pro/lib/python3.13/sitepackages/IPython/core/interactiveshell.py:2932, in safe_execfile()    2931     glob, loc = (where + (None, ))[:2] > 2932     py3compat.execfile(    2933         fname, glob, loc,    2934         self.compile if shell_futures else None)    2935 except SystemExit as status:    2936      If the call was made with 0 or None exit status (sys.exit(0)    2937      or sys.exit() ), don't bother showing a traceback, as both of    (...)    2943      For other exit status, we show the exception unless    2944      explicitly silenced, but only in short form. File ~/Documents/pythonenvs/netket_pro/lib/python3.13/sitepackages/IPython/utils/py3compat.py:55, in execfile()      54 compiler = compiler or compile > 55 exec(compiler(f.read(), fname, ""exec""), glob, loc) File ~/Dropbox/Ricerca/Codes/Python/netket/pp.py:19      18  crashes > 19 x, info = jax.jit(cg)(Aop, b) File ~/Documents/pythonenvs/netket_pro/lib/python3.13/sitepackages/jax/_src/scipy/sparse/linalg.py:286, in cg()     234 """"""Use Conjugate Gradient iteration to solve ``Ax = b``.     235     236 The numerics of JAX's ``cg`` should exact match SciPy's ``cg`` (up to    (...)     284 jax.lax.custom_linear_solve     285 """""" > 286 return _isolve(_cg_solve,     287                A=A, b=b, x0=x0, tol=tol, atol=atol,     288                maxiter=maxiter, M=M, check_symmetric=True) File ~/Documents/pythonenvs/netket_pro/lib/python3.13/sitepackages/jax/_src/scipy/sparse/linalg.py:226, in _isolve()     224 symmetric = all(map(real_valued, tree_leaves(b))) \     225   if check_symmetric else False > 226 x = lax.custom_linear_solve(     227     A, b, solve=isolve_solve, transpose_solve=isolve_solve,     228     symmetric=symmetric)     229 info = None File ~/Documents/pythonenvs/netket_pro/lib/python3.13/sitepackages/jax/_src/scipy/sparse/linalg.py:128, in _cg_solve()     126   return x_, r_, gamma_, p_, k + 1 > 128 r0 = _sub(b, A(x0))     129 p0 = z0 = M(r0) File ~/Dropbox/Ricerca/Codes/Python/netket/pp.py:11, in mat_vec()      10 def mat_vec(v): > 11     res, _ = mpi4jax.allreduce(v, op=MPI.SUM, comm=MPI.COMM_WORLD)      12     return res File ~/Documents/pythonenvs/netket_pro/lib/python3.13/sitepackages/mpi4jax/_src/validation.py:90, in wrapped()      84         raise TypeError(      85             f'{func_name} got unexpected type for argument ""{arg}"" '      86             f""(expected: {readable_arg_types}, got: {type(val)}).""      87             f""{extra_message}""      88         ) > 90 return function(*args, **kwargs) File ~/Documents/pythonenvs/netket_pro/lib/python3.13/sitepackages/mpi4jax/_src/collective_ops/allreduce.py:76, in allreduce()      74     from mpi4jax._src.notoken import allreduce > 76     return allreduce(x, op, comm=comm), token      78 if comm is None: File ~/Documents/pythonenvs/netket_pro/lib/python3.13/sitepackages/mpi4jax/_src/validation.py:90, in wrapped()      84         raise TypeError(      85             f'{func_name} got unexpected type for argument ""{arg}"" '      86             f""(expected: {readable_arg_types}, got: {type(val)}).""      87             f""{extra_message}""      88         ) > 90 return function(*args, **kwargs) File ~/Documents/pythonenvs/netket_pro/lib/python3.13/sitepackages/mpi4jax/_src/notoken/collective_ops/allreduce.py:72, in allreduce()      71 comm = wrap_as_hashable(comm) > 72 return mpi_allreduce_p.bind(x, op=op, comm=comm, transpose=False) JaxStackTraceBeforeTransformation: KeyError:  The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: KeyError                                  Traceback (most recent call last) File ~/Dropbox/Ricerca/Codes/Python/netket/pp.py:19      16 x, info = cg(Aop, b)      18  crashes > 19 x, info = jax.jit(cg)(Aop, b)     [... skipping hidden 1 frame] File ~/Documents/pythonenvs/netket_pro/lib/python3.13/sitepackages/jax/_src/pjit.py:340, in _cpp_pjit..cache_miss(*args, **kwargs)     335 if config.no_tracing.value:     336   raise RuntimeError(f""retracing function {jit_info.fun_sourceinfo} for ""     337                      ""`jit`, but 'no_tracing' is set"")     339 (outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked, executable, > 340  pgle_profiler) = _python_pjit_helper(fun, jit_info, *args, **kwargs)     342 maybe_fastpath_data = _get_fastpath_data(     343     executable, out_tree, args_flat, out_flat, attrs_tracked, jaxpr.effects,     344     jaxpr.consts, jit_info.abstracted_axes,     345     pgle_profiler)     347 return outs, maybe_fastpath_data, _need_to_rebuild_with_fdo(pgle_profiler) File ~/Documents/pythonenvs/netket_pro/lib/python3.13/sitepackages/jax/_src/pjit.py:198, in _python_pjit_helper(fun, jit_info, *args, **kwargs)     196   args_flat = map(core.full_lower, args_flat)     197   core.check_eval_args(args_flat) > 198   out_flat, compiled, profiler = _pjit_call_impl_python(*args_flat, **p.params)     199 else:     200   out_flat = pjit_p.bind(*args_flat, **p.params) File ~/Documents/pythonenvs/netket_pro/lib/python3.13/sitepackages/jax/_src/pjit.py:1660, in _pjit_call_impl_python(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, resource_env, donated_invars, name, keep_unused, inline, compiler_options_kvs, *args)    1657 compiler_options_kvs = compiler_options_kvs + tuple(pgle_compile_options.items())    1658  Passing mutable PGLE profile here since it should be extracted by JAXPR to    1659  initialize the fdo_profile compile option. > 1660 compiled = _resolve_and_lower(    1661     args, jaxpr=jaxpr, in_shardings=in_shardings,    1662     out_shardings=out_shardings, in_layouts=in_layouts,    1663     out_layouts=out_layouts, resource_env=resource_env,    1664     donated_invars=donated_invars, name=name, keep_unused=keep_unused,    1665     inline=inline, lowering_platforms=None,    1666     lowering_parameters=mlir.LoweringParameters(),    1667     pgle_profiler=pgle_profiler,    1668     compiler_options_kvs=compiler_options_kvs,    1669 ).compile()    1671  This check is expensive so only do it if enable_checks is on.    1672 if compiled._auto_spmd_lowering and config.enable_checks.value: File ~/Documents/pythonenvs/netket_pro/lib/python3.13/sitepackages/jax/_src/pjit.py:1627, in _resolve_and_lower(args, jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, resource_env, donated_invars, name, keep_unused, inline, lowering_platforms, lowering_parameters, pgle_profiler, compiler_options_kvs)    1624 in_shardings = _resolve_in_shardings(args, in_shardings)    1625 in_layouts = _resolve_in_layouts(args, in_layouts, in_shardings,    1626                                  jaxpr.in_avals) > 1627 return _pjit_lower(    1628     jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, resource_env,    1629     donated_invars, name, keep_unused, inline, compiler_options_kvs,    1630     lowering_platforms=lowering_platforms,    1631     lowering_parameters=lowering_parameters,    1632     pgle_profiler=pgle_profiler) File ~/Documents/pythonenvs/netket_pro/lib/python3.13/sitepackages/jax/_src/pjit.py:1792, in _pjit_lower(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, resource_env, donated_invars, name, keep_unused, inline, compiler_options_kvs, lowering_platforms, lowering_parameters, pgle_profiler)    1789 else:    1790   mesh, api_name = ((resource_env.physical_mesh, 'pjit')    1791                     if resource_env is not None else (None, 'jit')) > 1792 return pxla.lower_sharding_computation(    1793     jaxpr, api_name, name, in_shardings, out_shardings,    1794     in_layouts, out_layouts, tuple(donated_invars),    1795     keep_unused=keep_unused, context_mesh=mesh,    1796     compiler_options_kvs=compiler_options_kvs,    1797     lowering_platforms=lowering_platforms,    1798     lowering_parameters=lowering_parameters,    1799     pgle_profiler=pgle_profiler) File ~/Documents/pythonenvs/netket_pro/lib/python3.13/sitepackages/jax/_src/profiler.py:333, in annotate_function..wrapper(*args, **kwargs)     330 (func)     331 def wrapper(*args, **kwargs):     332   with TraceAnnotation(name, **decorator_kwargs): > 333     return func(*args, **kwargs)     334   return wrapper File ~/Documents/pythonenvs/netket_pro/lib/python3.13/sitepackages/jax/_src/interpreters/pxla.py:2330, in lower_sharding_computation(closed_jaxpr, api_name, fun_name, in_shardings, out_shardings, in_layouts, out_layouts, donated_invars, keep_unused, context_mesh, compiler_options_kvs, lowering_platforms, lowering_parameters, pgle_profiler)    2324 semantic_in_shardings = SemanticallyEqualShardings(    2325     in_shardings, global_in_avals)   type: ignore    2326 semantic_out_shardings = SemanticallyEqualShardings(    2327     out_shardings, global_out_avals)   type: ignore    2329 (module, keepalive, host_callbacks, unordered_effects, ordered_effects, > 2330  nreps, tuple_args, shape_poly_state) = _cached_lowering_to_hlo(    2331      closed_jaxpr, api_name, fun_name, backend, semantic_in_shardings,    2332      semantic_out_shardings, in_layouts, out_layouts, num_devices,    2333      tuple(da_object) if prim_requires_devices else None, donated_invars,    2334      name_stack, all_default_mem_kind, inout_aliases,    2335      propagated_out_mem_kinds, platforms,    2336      lowering_parameters=lowering_parameters,    2337      abstract_mesh=abstract_mesh)    2339  backend and device_assignment is passed through to MeshExecutable because    2340  if keep_unused=False and all in_shardings are pruned, then there is no way    2341  to get the device_assignment and backend. So pass it to MeshExecutable    2342  because we calculate the device_assignment and backend before in_shardings,    2343  etc are pruned.    2344 return MeshComputation(    2345     str(name_stack),    2346     module,    (...)    2373     intermediate_shardings=unique_intermediate_shardings,    2374     context_mesh=context_mesh) File ~/Documents/pythonenvs/netket_pro/lib/python3.13/sitepackages/jax/_src/interpreters/pxla.py:1960, in _cached_lowering_to_hlo(closed_jaxpr, api_name, fun_name, backend, semantic_in_shardings, semantic_out_shardings, in_layouts, out_layouts, num_devices, device_assignment, donated_invars, name_stack, all_default_mem_kind, inout_aliases, propagated_out_mem_kinds, platforms, lowering_parameters, abstract_mesh)    1956 ordered_effects = list(effects.ordered_effects.filter_in(closed_jaxpr.effects))    1957 with dispatch.log_elapsed_time(    1958       ""Finished jaxpr to MLIR module conversion {fun_name} in {elapsed_time:.9f} sec"",    1959       fun_name=str(name_stack), event=dispatch.JAXPR_TO_MLIR_MODULE_EVENT): > 1960   lowering_result = mlir.lower_jaxpr_to_module(    1961       module_name,    1962       closed_jaxpr,    1963       ordered_effects=ordered_effects,    1964       backend=backend,    1965       platforms=platforms,    1966       axis_context=axis_ctx,    1967       name_stack=name_stack,    1968       donated_args=donated_invars,    1969       replicated_args=replicated_args,    1970       arg_shardings=in_mlir_shardings,    1971       result_shardings=out_mlir_shardings,    1972       in_layouts=in_layouts,    1973       out_layouts=out_layouts,    1974       arg_names=jaxpr._debug_info and jaxpr._debug_info.arg_names,    1975       result_names=jaxpr._debug_info and jaxpr._debug_info.result_paths,    1976       num_replicas=nreps,    1977       num_partitions=num_partitions,    1978       all_default_mem_kind=all_default_mem_kind,    1979       input_output_aliases=inout_aliases,    1980       propagated_out_mem_kinds=propagated_out_mem_kinds,    1981       lowering_parameters=lowering_parameters)    1982 tuple_args = dispatch.should_tuple_args(len(global_in_avals), backend.platform)    1983 unordered_effects = list(    1984     effects.ordered_effects.filter_not_in(closed_jaxpr.effects)) File ~/Documents/pythonenvs/netket_pro/lib/python3.13/sitepackages/jax/_src/interpreters/mlir.py:1195, in lower_jaxpr_to_module(***failed resolving arguments***)    1193   attrs[""mhlo.num_partitions""] = i32_attr(num_partitions)    1194   print(""ordered_effects"", ordered_effects) > 1195   lower_jaxpr_to_fun(    1196       ctx, ""main"", jaxpr, ordered_effects,    1197       name_stack=name_stack,    1198       public=True,    1199       replicated_args=replicated_args,    1200       arg_shardings=arg_shardings,    1201       result_shardings=result_shardings,    1202       input_output_aliases=input_output_aliases,    1203       xla_donated_args=xla_donated_args,    1204       arg_names=arg_names,    1205       result_names=result_names,    1206       arg_memory_kinds=arg_memory_kinds,    1207       result_memory_kinds=result_memory_kinds,    1208       arg_layouts=in_layouts,    1209       result_layouts=out_layouts,    1210       propagated_out_mem_kinds=propagated_out_mem_kinds)    1212 try:    1213   if not ctx.module.operation.verify(): File ~/Documents/pythonenvs/netket_pro/lib/python3.13/sitepackages/jax/_src/interpreters/mlir.py:1680, in lower_jaxpr_to_fun(ctx, name, jaxpr, effects, name_stack, public, replicated_args, arg_shardings, result_shardings, use_sharding_annotations, input_output_aliases, xla_donated_args, api_name, arg_names, result_names, arg_memory_kinds, result_memory_kinds, arg_layouts, result_layouts, propagated_out_mem_kinds)    1678   callee_name_stack = name_stack    1679 consts = [ir_constant(xla.canonicalize_dtype(x)) for x in jaxpr.consts] > 1680 out_vals, tokens_out = jaxpr_subcomp(    1681     ctx, jaxpr.jaxpr, callee_name_stack, tokens_in,    1682     consts, *args, dim_var_values=dim_var_values)    1683 outs: list[IrValues] = []    1684 for eff in effects: File ~/Documents/pythonenvs/netket_pro/lib/python3.13/sitepackages/jax/_src/interpreters/mlir.py:1952, in jaxpr_subcomp(ctx, jaxpr, name_stack, tokens, consts, dim_var_values, *args)    1949   rule_ctx = rule_ctx.replace(axis_size_env=axis_size_env)    1951 assert all(_is_ir_values(v) for v in in_nodes), (eqn, in_nodes) > 1952 ans = lower_per_platform(rule_ctx, str(eqn.primitive),    1953                          platform_rules, default_rule,    1954                          eqn.effects,    1955                          *in_nodes, **eqn.params)    1957 if effects:    1958    If there were ordered effects in the primitive, there should be output    1959    tokens we need for subsequent ordered effects.    1960   tokens_out = rule_ctx.tokens_out File ~/Documents/pythonenvs/netket_pro/lib/python3.13/sitepackages/jax/_src/interpreters/mlir.py:2070, in lower_per_platform(ctx, description, platform_rules, default_rule, effects, *rule_args, **rule_kwargs)    2068  If there is a single rule left just apply the rule, without conditionals.    2069 if len(kept_rules) == 1: > 2070   output = kept_rules0    2071   map(    2072       lambda o: wrap_compute_type_in_place(ctx, o.owner),    2073       filter(_is_not_block_argument, flatten_ir_values(output)),    2074   )    2075   map(    2076       lambda o: wrap_xla_metadata_in_place(ctx, o.owner),    2077       flatten_ir_values(output),    2078   ) File ~/Documents/pythonenvs/netket_pro/lib/python3.13/sitepackages/jax/_src/interpreters/mlir.py:2185, in lower_fun..f_lowered(ctx, *args, **params)    2183 else:    2184   sub_context = ctx.module_context > 2185 out, tokens = jaxpr_subcomp(    2186     sub_context, jaxpr, ctx.name_stack, ctx.tokens_in,    2187     _ir_consts(consts), *args,    2188     dim_var_values=ctx.dim_var_values)    2189 ctx.set_tokens_out(tokens)    2190 return out File ~/Documents/pythonenvs/netket_pro/lib/python3.13/sitepackages/jax/_src/interpreters/mlir.py:1937, in jaxpr_subcomp(ctx, jaxpr, name_stack, tokens, consts, dim_var_values, *args)    1934     default_rule = _lowerings[eqn.primitive]    1936 effects = list(effects_lib.ordered_effects.filter_in(eqn.effects)) > 1937 tokens_in = tokens.subset(effects)    1938 avals_in = map(aval, eqn.invars)    1939 rule_ctx = LoweringRuleContext(    1940     module_context=ctx, primitive=eqn.primitive,    1941     name_stack=source_info.name_stack,    1942     avals_in=avals_in,    1943     avals_out=map(aval, eqn.outvars), tokens_in=tokens_in,    1944     tokens_out=None, jaxpr_eqn_ctx=eqn.ctx, dim_var_values=dim_var_values) File ~/Documents/pythonenvs/netket_pro/lib/python3.13/sitepackages/jax/_src/interpreters/mlir.py:1346, in TokenSet.subset(self, effects)    1344 """"""Return a subset of the `TokenSet` restricted to a set of `core.Effect`s.""""""    1345 print(""tokens are:"", self._tokens, ""effects are:"", effects) > 1346 return TokenSet((eff, self._tokens[eff]) for eff in effects) File ~/Documents/pythonenvs/netket_pro/lib/python3.13/sitepackages/jax/_src/interpreters/mlir.py:1323, in TokenSet.__init__(self, *args, **kwargs)    1322 def __init__(self, *args, **kwargs): > 1323   self._tokens = collections.OrderedDict(*args, **kwargs) File ~/Documents/pythonenvs/netket_pro/lib/python3.13/sitepackages/jax/_src/interpreters/mlir.py:1346, in (.0)    1344 """"""Return a subset of the `TokenSet` restricted to a set of `core.Effect`s.""""""    1345 print(""tokens are:"", self._tokens, ""effects are:"", effects) > 1346 return TokenSet((eff, self._tokens[eff]) for eff in effects) KeyError:  ```   System info (python version, jaxlib version, accelerator, etc.) ```python jax:    0.5.0 jaxlib: 0.5.0 numpy:  1.26.4 python: 3.13.1 (main, Dec 19 2024, 14:22:59) [Clang 18.1.8 ] device info: cpu1, 1 local devices"" process_count: 1 platform: uname_result(system='Darwin', node='mba10834270.local', release='24.2.0', version='Darwin Kernel Version 24.2.0: Fri Dec  6 19:01:59 PST 2024; root:xnu11215.61.5~2/RELEASE_ARM64_T6000', machine='arm64') ``` ```python In [4]: mpi4jax.__version__ Out[4]: '0.7.0' ``",2025-01-24T18:26:30Z,bug,closed,0,9,https://github.com/jax-ml/jax/issues/26087,"Here's a reproducer without `mpi4jax`: ```python import jax k = jax.random.key(1) O = jax.random.normal(k, (24, 24)) b = jax.random.normal(k, (24,)) def mat_vec(v):      works if flipping ordered to False     jax.debug.callback(lambda: print(""mat_vec""), ordered=True)     return v def solve(b):     return jax.lax.custom_linear_solve(mat_vec, b, lambda matvec, x: matvec(x))  works res = solve(b)  crashes res = jax.jit(solve)(b) ``` Same error: ```python KeyError:  ``` Looks like any ordered effect will break `jax.lax.custom_linear_solve`.","Sorry for the bump,  , is the reproducer above enough to nail down the problem?","At a guess it's because the abstract evaluation rule doesn't handle effects (and I think just silently swallows them): https://github.com/jaxml/jax/blob/af84143e618a5b13c1a26f3331058236cb5cc1a7/jax/_src/lax/control_flow/solves.pyL485 In contrast effectful primitives (or higherorder primitives which may wrap effectful operations) tend to use `def_effectful_abstract_eval` instead, e.g. here for `jax.lax.cond`: https://github.com/jaxml/jax/blob/af84143e618a5b13c1a26f3331058236cb5cc1a7/jax/_src/lax/control_flow/conditionals.pyL795 or here for `jax.debug.print`: https://github.com/jaxml/jax/blob/af84143e618a5b13c1a26f3331058236cb5cc1a7/jax/_src/debugging.pyL98  FWIW this is an issue I know we have over in Lineax as well: https://github.com/patrickkidger/lineax/blob/fa6c777ef6e5edf2c798a58994e07946a3ce50d7/lineax/_solve.pyL111L127 because AFAIK, JAX doesn't provide any public API for getting the effects. Ideally `jax.eval_shape` would optionally provide these as well!","Oh, thanks! This does look like it. Is there a specific reason why this was not addressed in Jax itself, or it’s just lack of time and therefore this could be easily fixed?","Probably just a bug that noone noticed/fixed before now! FWIW at least for the implementation JAX has here it should be pretty easy to fix, jaxprs store their effects so something like `jaxprs solve_jaxpr.effects` would probably get what you need.","Good suggestion, kidger! Yeah, I agree that this seems like the most likely culprit and I expect it would be a simple fix. I'm happy to do it unless someone on this thread would be interested in opening a PR. Let me know!","I'd have to get familiar with this part of the jax codebase, so please do go ahead! This would be very helpful for us","This should be fixed by https://github.com/jaxml/jax/pull/26275. I'll close this as completed, but please let me know if you continue to run into issues!",Thanks! As soon as jax 0.5.1 makes it out I'll test it!
yi,Jax 0.5.0 error: no type named 'string' in namespace 'std'," Description I'm not able to build jaxlib version 0.5.0 on aarch64, I see the following build error: ``` ERROR: /root/.cache/bazel/_bazel_root/6ff6e7d4aad9a899a443c18cfae68f22/external/compute_library/BUILD.bazel:355:11: Compiling src/core/utils/logging/FilePrinter.cpp failed: (Exit 1): clang18 failed: error executing command (from target //:arm_compute)   (cd /root/.cache/bazel/_bazel_root/6ff6e7d4aad9a899a443c18cfae68f22/execroot/__main__ && \   exec env  \     CLANG_COMPILER_PATH=/usr/bin/clang18 \     PATH=/root/.local/bin:/root/bin:/usr/share/Modules/bin:/opt/bitnami/python/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \     PWD=/proc/self/cwd \   /usr/bin/clang18 U_FORTIFY_SOURCE fstackprotector Wall Wthreadsafety Wselfassign Wunusedbutsetparameter Wnofreenonheapobject fcolordiagnostics fnoomitframepointer g0 O2 'D_FORTIFY_SOURCE=1' DNDEBUG ffunctionsections fdatasections ' std=c++14' MD MF bazelout/aarch64opt/bin/external/compute_library/_objs/arm_compute/FilePrinter.pic.d 'frandomseed=bazelout/aarch64opt/bin/external/compute_library/_objs/arm_compute/FilePrinter.pic.o' fPIC DENABLE_NEON DARM_COMPUTE_CPU_ENABLED DARM_COMP UTE_ENABLE_NEON DARM_COMPUTE_ENABLE_I8MM DENABLE_FP32_KERNELS DENABLE_QASYMM8_KERNELS DENABLE_QASYMM8_SIGNED_KERNELS DENABLE_QSYMM16_KERNELS DENABLE_INTEGER_KERNELS DENABLE_NHWC_KERNELS DENABLE_NCHW_KERNELS DARM_COMPUTE_GRAPH_ENABLED DARM_COMPUTE_ENABLE_S VEF32MM DARM_COMPUTE_ENABLE_FIXED_FORMAT_KERNELS D_GLIBCXX_USE_NANOSLEEP DENABLE_SVE DARM_COMPUTE_ENABLE_SVE DARM_COMPUTE_ENABLE_BF16 'DBAZEL_CURRENT_REPOSITORY=""compute_library""' iquote external/compute_library iquote bazelout/aarch64opt/bin/external/com pute_library Ibazelout/aarch64opt/bin/external/compute_library/include/_virtual_includes/include isystem external/compute_library/arm_compute/runtime isystem bazelout/aarch64opt/bin/external/compute_library/arm_compute/runtime isystem external/compute_libra ry/src/core/NEON/kernels/assembly isystem bazelout/aarch64opt/bin/external/compute_library/src/core/NEON/kernels/assembly isystem external/compute_library/src/core/NEON/kernels/convolution/common isystem bazelout/aarch64opt/bin/external/compute_library/src/c ore/NEON/kernels/convolution/common isystem external/compute_library/src/core/NEON/kernels/convolution/winograd isystem bazelout/aarch64opt/bin/external/compute_library/src/core/NEON/kernels/convolution/winograd isystem external/compute_library/src/core/cpu/ke rnels/assembly isystem bazelout/aarch64opt/bin/external/compute_library/src/core/cpu/kernels/assembly isystem external/compute_library/src/cpu/kernels/assembly isystem bazelout/aarch64opt/bin/external/compute_library/src/cpu/kernels/assembly isystem externa l/compute_library/src/core/NEON/kernels/arm_conv isystem bazelout/aarch64opt/bin/external/compute_library/src/core/NEON/kernels/arm_conv isystem external/compute_library/src/core/NEON/kernels/arm_gemm isystem bazelout/aarch64opt/bin/external/compute_library/ src/core/NEON/kernels/arm_gemm 'fvisibility=hidden' Wnosigncompare Wnounknownwarningoption Wnostringoptruncation Wnoarrayparameter 'DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir.' Wnognuoffsetofextensions Qunusedarguments 'Werror=mismatchedtags' Wnognuoffsetofextensions Qunusedarguments 'Werror=mismatchedtags' 'std=c++17' 'march=armv8a' O3 nocanonicalprefixes Wnobuiltinmacroredefined 'D__DATE__=""redacted""' 'D__TIMESTAMP__=""redacted""' 'D__TIME__=""redacted""' c external/compute_library/src/core/utils/logging/FilePrinter.cpp o bazelout/aarch64opt/bin/external/compute_library/_objs/arm_compute/FilePrinter.pic.o)  Configuration: 7f3e8ef8d4e1c3bf7770a5cd241fa0a6c9bebc2d5e4c95e2396bfebbb6e6cbbf  Execution platform: //:platform In file included from external/compute_library/src/core/utils/logging/FilePrinter.cpp:24: In file included from external/compute_library/arm_compute/core/utils/logging/FilePrinter.h:27: external/compute_library/arm_compute/core/utils/logging/IPrinter.h:56:34: error: no type named 'string' in namespace 'std'    56                   ^ 1 warning and 3 errors generated. [5,995 / 9,008] 7 actions running     Compiling xla/tsl/distributed_runtime/rpc/coordination/grpc_coordination_service_impl.cc; 4s local Target //jaxlib/tools:build_wheel failed to build ``` Previous clang versions like 16.x or 17.x from llvmproject/releases generate a different error. For instance, Clang 16.0.0 works for x86_64, but not for aarch64: ``` ERROR: /tmp/jaxjaxv0.5.0/jaxlib/BUILD:201:19: Linking jaxlib/cpu_feature_guard.so failed: (Exit 1): clang16 failed: error executing command (from target //jaxlib:cpu_feature_guard.so)   (cd /root/.cache/bazel/_bazel_root/6ff6e7d4aad9a899a443c18cfae68f22/execroot/__main__ && \   exec env  \     CLANG_COMPILER_PATH=/tmp/clang+llvm16.0.0aarch64linuxgnu/bin/clang16 \     LD_LIBRARY_PATH=/tmp/clang+llvm16.0.0aarch64linuxgnu/lib:/tmp/clang+llvm17.0.6aarch64linuxgnu/lib: \     PATH=/tmp/clang+llvm16.0.0aarch64linuxgnu/bin:/tmp/clang+llvm17.0.6aarch64linuxgnu/bin:/opt/bitnami/python/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \     PWD=/proc/self/cwd \   /tmp/clang+llvm16.0.0aarch64linuxgnu/bin/clang16 out/aarch64opt/bin/jaxlib/cpu_feature_guard.so2.params)  Configuration: 326616f3723b94f105241aea35dc41af5fda2cbd07234b7ac29b29fcdd6ca354  Execution platform: //:platform /tmp/clang+llvm16.0.0aarch64linuxgnu/bin/clang16: /usr/lib/libtinfo.so.6: no version information available (required by /tmp/clang+llvm16.0.0aarch64linuxgnu/bin/clang16) clang16: error: invalid linker name in argument 'fuseld=/tmp/clang+llvm16.0.0aarch64linuxgnu/bin/ld.lld:' Target //jaxlib/tools:build_wheel failed to build INFO: Elapsed time: 3.292s, Critical Path: 0.22s ```  System info (python version, jaxlib version, accelerator, etc.) ``` $ cat /etc/osrelease NAME=""Red Hat Enterprise Linux"" VERSION=""9.5 (Plow)"" ID=""rhel"" ID_LIKE=""fedora"" VERSION_ID=""9.5"" PLATFORM_ID=""platform:el9"" PRETTY_NAME=""Red Hat Enterprise Linux 9.5 (Plow)"" ANSI_COLOR=""0;31"" LOGO=""fedoralogoicon"" CPE_NAME=""cpe:/o:redhat:enterprise_linux:9::baseos"" HOME_URL=""https://www.redhat.com/"" DOCUMENTATION_URL=""https://access.redhat.com/documentation/enus/red_hat_enterprise_linux/9"" BUG_REPORT_URL=""https://issues.redhat.com/"" $ python version Python 3.12.8 $ clang version clang version 18.1.8 (Red Hat, Inc. 18.1.83.el9) Target: aarch64redhatlinuxgnu Thread model: posix InstalledDir: /usr/bin Configuration file: /etc/clang/aarch64redhatlinuxgnuclang.cfg ```",2025-01-23T16:27:06Z,bug,open,0,7,https://github.com/jax-ml/jax/issues/26062,"Thank you for reporting! The error didn't happen for me when I tried with LLVM 18 on Ubuntu. But I could reproduce the error in a RHEL 9 docker image (+ Bazel 7.4.1, Python 3.12.5, clang 18.1.8). I'll submit a fix soon. In the meanwhile, you should be able to skip the error by passing `disable_mkl_dnn` to build.py.",Thanks for confirming the issue! Do you plan to cut a new release with the fix in the short term? I'd rather not disable any feature.,"We're trying for a monthly release schedule, absent extraordinary reasons. The next scheduled release is on or about Feb 15. (Any reason you can't use a prebuilt jax/jaxlib?)"," In the meanwhile, you can build JAX with a local ACL repository that contains the fix from PR https://github.com/openxla/xla/pull/21810: ```bash  Clone ACL, checkout the version that JAX 0.5.0's XLA is using.  git clone https://github.com/ARMsoftware/ComputeLibrary git checkout v23.05.1  Apply the patch from the PR. git apply include_string.patch  Build jaxlib with the local ACL folder. python build/build.py build wheels=jaxlib verbose \   bazel_options='override_repository=compute_library=~/ComputeLibrary' ``` (`exclude_omp_scheduler.patch` fixes an OpenMP dependency issue and may not be needed for your case.)",Thank you for your help! We build all of our assets from source code to ensure traceability and reproducible builds. We would wait for the next official release including the updated dependencies.,"Got it. Could you please help verify if you can build JAX at HEAD without this issue? If so, we can close this issue."," sorry for the delay. I'm facing a new build issue now: ``` $ cat /etc/osrelease NAME=""Red Hat Enterprise Linux"" VERSION=""9.5 (Plow)"" ID=""rhel"" ID_LIKE=""fedora"" VERSION_ID=""9.5"" PLATFORM_ID=""platform:el9"" PRETTY_NAME=""Red Hat Enterprise Linux 9.5 (Plow)"" ANSI_COLOR=""0;31"" LOGO=""fedoralogoicon"" CPE_NAME=""cpe:/o:redhat:enterprise_linux:9::baseos"" HOME_URL=""https://www.redhat.com/"" DOCUMENTATION_URL=""https://access.redhat.com/documentation/enus/red_hat_enterprise_linux/9"" BUG_REPORT_URL=""https://issues.redhat.com/"" $ cd /tmp && git clone https://github.com/jaxml/jax.git && cd jax $ python version Python 3.12.8 $ clang version clang version 18.1.8 (http://git.linaro.org/toolchain/jenkinsscripts.git e5def089cd9f5aa71524f82fef301ca66eaa38d2) Target: aarch64unknownlinuxgnu Thread model: posix InstalledDir: /usr/local/clang/bin $ python build/build.py build wheels=jaxlib verbose (...) INFO: Found 1 target... ERROR: /tmp/jax/jaxlib/BUILD:201:19: Linking jaxlib/cpu_feature_guard.so failed: (Exit 1): clang18 failed: error executing command (from target //jaxlib:cpu_feature_guard.so)   (cd /root/.cache/bazel/_bazel_root/d8a720413c17317c98676bc64c7c85b4/execroot/__main__ && \   exec env  \     CLANG_COMPILER_PATH=/usr/local/clang/bin/clang18 \     LD_LIBRARY_PATH=/usr/local/clang/lib: \     PATH=/root/.local/bin:/root/bin:/usr/share/Modules/bin:/usr/local/clang/bin:/opt/bitnami/python/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \     PWD=/proc/self/cwd \   /usr/local/clang/bin/clang18 out/aarch64opt/bin/jaxlib/cpu_feature_guard.so2.params)  Configuration: e93e6adedc1212757323032565643c230fb5cc73ec2662c0df1c6c2a4784df6d  Execution platform: //:platform /usr/local/clang/bin/clang18: /lib64/libtinfo.so.6: no version information available (required by /usr/local/clang/bin/clang18) clang18: error: invalid linker name in argument 'fuseld=/usr/local/clang/bin/ld.lld:' Target //jaxlib/tools:build_wheel failed to build INFO: Elapsed time: 76.871s, Critical Path: 3.41s INFO: 383 processes: 244 internal, 139 local. FAILED: Build did NOT complete successfully ERROR: Build failed. Not running target ```"
yi,"'arith.index_cast' op operand #0 must be signless-integer-like or memref of signless-integer, but got 'f32'"," Description Hi, I was following the Pallas quickstart guide and tried modifying the `iota` kernel to produce fp32 results. But I got the following error: ``` Traceback (most recent call last):   File ""/mnt/ssd0/anaconda3/envs/vllm/lib/python3.10/sitepackages/jax/_src/pallas/mosaic/lowering.py"", line 732, in lower_jaxpr_to_func     body.func_op.verify() jaxlib.mlir._mlir_libs._site_initialize..MLIRError: Verification failed: error: ""/swap""(callsite(""grid_filler_kernel""(""/mnt/ssd0/pallas_stuff/hello_grid.py"":11:0) at callsite(""grid_filler""(""/mnt/ssd0/pallas_stuff/hello_grid.py"":14:0) at """"(""/mnt/ssd0/ pallas_stuff/hello_grid.py"":21:0)))): 'arith.index_cast' op operand CC(未找到相关数据) must be signlessintegerlike or memref of signlessinteger, but got 'f32'                                          note: ""/swap""(callsite(""grid_filler_kernel""(""/mnt/ssd0/pallas_stuff/hello_grid.py"":11:0) at callsite(""grid_filler""(""/mnt/ssd0/pallas_stuff/hello_grid.py"":14:0) at """"(""/mnt/ssd0/ pallas_stuff/hello_grid.py"":21:0)))): see current operation: %1 = ""arith.index_cast""(%0) : (f32) > index                                                                                  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/mnt/ssd0/pallas_stuff/hello_grid.py"", line 21, in      print(grid_filler(4))   File ""/mnt/ssd0/pallas_stuff/hello_grid.py"", line 14, in grid_filler     return pl.pallas_call(   File ""/mnt/ssd0/anaconda3/envs/vllm/lib/python3.10/sitepackages/jax/_src/pallas/pallas_call.py"", line 1882, in wrapped     out_flat = pallas_call_p.bind( jax._src.source_info_util.JaxStackTraceBeforeTransformation: jax._src.pallas.mosaic.error_handling.VerificationError: Pallas encountered an internal verification error.Please file a bug  at https://github.com/jaxml/jax/issues. Error details: 'arith.index_cast' op operand CC(未找到相关数据) must be signlessintegerlike or memref of signlessinteger, but got 'f32'                        see current operation: %1 = ""arith.index_cast""(%0) : (f32) > index The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/mnt/ssd0/pallas_stuff/hello_grid.py"", line 21, in      print(grid_filler(4))   File ""/mnt/ssd0/pallas_stuff/hello_grid.py"", line 14, in grid_filler     return pl.pallas_call(   File ""/mnt/ssd0/anaconda3/envs/vllm/lib/python3.10/sitepackages/jax/_src/pallas/pallas_call.py"", line 1520, in _pallas_call_lowering     return mlir.lower_per_platform(ctx, ""pallas_call"",   File ""/mnt/ssd0/anaconda3/envs/vllm/lib/python3.10/sitepackages/jax/_src/pallas/pallas_call.py"", line 1493, in tpu_lowering     return mosaic_tpu_backend.pallas_call_tpu_lowering_rule(   File ""/mnt/ssd0/anaconda3/envs/vllm/lib/python3.10/sitepackages/jax/_src/pallas/mosaic/pallas_call_registration.py"", line 147, in pallas_call_tpu_lowering_rule     mosaic_module, extra_args = lower_module(for_verification=False)   File ""/mnt/ssd0/anaconda3/envs/vllm/lib/python3.10/sitepackages/jax/_src/pallas/mosaic/pallas_call_registration.py"", line 142, in lower_module     return lowering.lower_jaxpr_to_module(   File ""/mnt/ssd0/anaconda3/envs/vllm/lib/python3.10/sitepackages/jax/_src/pallas/mosaic/lowering.py"", line 550, in lower_jaxpr_to_module     func_op = lower_jaxpr_to_func(   File ""/mnt/ssd0/anaconda3/envs/vllm/lib/python3.10/sitepackages/jax/_src/pallas/mosaic/lowering.py"", line 734, in lower_jaxpr_to_func     raise error_handling.mlir_error_to_verification_error(e) from e   File ""/mnt/ssd0/pallas_stuff/hello_grid.py"", line 21, in      print(grid_filler(4))   File ""/mnt/ssd0/pallas_stuff/hello_grid.py"", line 14, in grid_filler     return pl.pallas_call(   File ""/mnt/ssd0/pallas_stuff/hello_grid.py"", line 11, in grid_filler_kernel     o_ref[i] = i jax._src.pallas.mosaic.error_handling.VerificationError: Pallas encountered an internal verification error.Please file a bug at https://github.com/jaxml/jax/issues. Error details: 'arit h.index_cast' op operand CC(未找到相关数据) must be signlessintegerlike or memref of signlessinteger, but got 'f32'                                                                                     see current operation: %1 = ""arith.index_cast""(%0) : (f32) > index ``` I ran the following code. ```py from functools import partial import jax from jax.experimental import pallas as pl from jax.experimental.pallas import tpu as pltpu import jax.numpy as jnp import numpy as np def iota_kernel(o_ref):     i = pl.program_id(0).astype(o_ref.dtype)  errors     o_ref[i] = i  .astype(o_ref.dtype)  this works def iota(i: int) > jax.Array:     return pl.pallas_call(         iota_kernel,         out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM),         out_shape=jax.ShapeDtypeStruct((i,), jnp.float32),         grid=(i,)     )() print(iota(4)) ``` What's strange is that if I move the `.astype(o_ref.dtype)` down to the next line, the error goes away  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.36.dev20241122 jaxlib: 0.4.36.dev20241122 numpy:  1.26.4 python: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0] device info: TPU v6 lite1, 1 local devices"" process_count: 1 platform: uname_result(system='Linux', node='t1vn8ff16d39w0', release='6.8.01015gcp', version=' CC(Implement np.repeat for scalar repeats.)~22.04.1Ubuntu SMP Tue Sep  3 16:11:52 UTC 2024', machine='x86_64') ```",2025-01-22T15:17:41Z,bug,open,0,2,https://github.com/jax-ml/jax/issues/26034,"The error here is on the index_cast (since you are storing to `o_ref[i]`), not on the cast produced by `as_type`. The error isn't very clear, but it's essentially saying you can't index into an array with a float. That being said, this is an error that we should catch in Pallas itself and raise a better error instead of outputting an invalid MLIR program that fails during the verification check.",Oh yes right I didn't see that
rag,CPU Slowdown introduced in 0.4.32  and in following versions," Description I've been running a speed test in my repository (https://github.com/danielpmorton/cbfpy/blob/main/test/test_speed.py) and I've noticed a significant reduction in speed with newer versions of Jax/Jaxlib. Running this with newer versions of Jax after 0.4.32 results in this script being approximately 1/3 the speed as with 0.4.31 and previous versions I know 0.4.32 is a yanked release, but this slowdown still exists in 0.4.33 and later I'll try to come up with a minimal working example, as I know this is hard to reproduce without installing my repo. In general, I'm solving a lot of QPs on CPU using qpax (https://github.com/kevintracy/qpax) ``` (cbfpy_release) dmortonnuc:~/cbfpy_release$ pip install ""jaxlib==0.4.32"" ""jax==0.4.32"" (... various pip installation logging) (cbfpy_release) dmortonnuc:~/cbfpy_release$ python test/test_speed.py  pybullet build time: Nov 28 2023 23:48:36 pygame 2.6.1 (SDL 2.28.4, Python 3.11.9) Hello from the pygame community. https://www.pygame.org/contribute.html ACC average Hz:  24240.550542352157 .Point robot average Hz:  24371.811879894532 .  Ran 2 tests in 2.409s OK (cbfpy_release) dmortonnuc:~/cbfpy_release$ pip install ""jaxlib==0.4.31"" ""jax==0.4.31"" (... various pip installation logging) (cbfpy_release) dmortonnuc:~/cbfpy_release$ python test/test_speed.py  pybullet build time: Nov 28 2023 23:48:36 pygame 2.6.1 (SDL 2.28.4, Python 3.11.9) Hello from the pygame community. https://www.pygame.org/contribute.html **ACC average Hz:  57967.98398949308 .Point robot average Hz:  62571.14449011536** .  Ran 2 tests in 2.073s OK ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.38 jaxlib: 0.4.38 numpy:  1.26.4 python: 3.10.8 (main, May 30 2024, 10:58:14) [GCC 11.4.0] device info: cpu1, 1 local devices"" process_count: 1 platform: uname_result(system='Linux', node='aslnuc', release='6.5.035generic', version=' CC(CUDA90 and py3 )~22.04.1Ubuntu SMP PREEMPT_DYNAMIC Tue May  7 09:00:52 UTC 2', machine='x86_64') ```",2025-01-21T23:35:18Z,bug,open,1,3,https://github.com/jax-ml/jax/issues/26021,"Another note: the following code does not fix the problem, and in fact makes the issue much worse. I noticed this on a few other threads here: https://github.com/clemisch/jaxtomo/issues/3 https://github.com/jaxml/jax/issues/23590 ``` import os XLA_flag = ""xla_cpu_use_thunk_runtime=false "" os.environ[""XLA_FLAGS""] = XLA_flag ```","Thanks for the report! There's quite a lot going on in your example, and I'm not very familiar with the dependencies, so I don't immediately have intuition about what to try. Although, I'm also surprised that setting that environment variable didn't improve the situation! So it would be useful if you could try to dig a little deeper and isolate the issue with a smaller self contained reproducer so that we have something to work with.", may have insights given the discussion here.
rag,[Mosaic GPU] Implement basic WGMMAFragLayout inference and propagation,[Mosaic GPU] Implement basic WGMMAFragLayout inference and propagation,2025-01-21T12:43:47Z,,closed,0,0,https://github.com/jax-ml/jax/issues/26007
rag,[Mosaic GPU] Add support for converting all fragmented layouts to ir and back.,[Mosaic GPU] Add support for converting all fragmented layouts to ir and back. This will be used in the layout inference and lowering of the dialect WGMMA op,2025-01-21T10:40:57Z,,closed,0,0,https://github.com/jax-ml/jax/issues/26003
yi,jax.scipy.linalg.expm yields wrong values (for symmetric matrices)," Description When calculating the matrix exponential using jax.scipy.linalg.expm the results mismatch those of scipy.linalg.expm for some (in this case symmetric) sampled matrices. The calculation via jax.scipy.linalg.eigh works fine. The same holds for scaling down the matrix elements and using jax.numpy.linalg.matrix_power.  As mentioned, this happens only for some of the sampled matrices, I cannot identify a clear pattern. This might be related to CC(Better expm). In my case this deviation caused completely false results in a simulation i was running. So it has potential to cause a lot of trouble for those who are not aware of the issue. Here is a code to reproduce the issue: ``` import jax.numpy as jnp from jax.scipy.linalg import expm as jexpm from jax.scipy.linalg import eigh as jeigh from jax.numpy.linalg import matrix_power as jmatrix_power import numpy as np from scipy.linalg import expm, eigh for _ in range(50):     M = np.asarray(np.random.random(size=(2,2)), dtype = 'float32')     M_np = (M + M.T)/2*1e2     M_jnp = jnp.asarray(M_np)     U_np_exp = expm(1.0j*M_np)     U_jnp_exp = jexpm(1.0j*M_jnp, max_squarings=32)     w, v = eigh(M_np)     U_np_eig = np.dot(np.dot(v, np.diag(np.exp(1.0j*w))), v.T)     w, v = jeigh(M_jnp)     U_jnp_eig = jnp.dot(jnp.dot(v, jnp.diag(jnp.exp(1.0j*w))), v.T)     N = 100     U_jnp_power = jexpm(1.0j*M_jnp/N, max_squarings=32)     U_jnp_power = jmatrix_power(U_jnp_power, N)     dev_jnp = np.sum(np.abs(U_jnp_exp  U_jnp_eig))     dev_np = np.sum(np.abs(U_np_exp  U_np_eig))     dev_eig_eig = np.sum(np.abs(U_np_eig  U_jnp_eig))     dev_exp_power = np.sum(np.abs(U_np_exp  U_jnp_power))     if np.max([dev_jnp, dev_np, dev_eig_eig, dev_exp_power]) > 1e2:         print(dev_jnp, dev_np, dev_eig_eig, dev_exp_power)         print(M_np)         print() ``` Example output: ``` 0.022798402 1.0083479e05 7.1949174e08 1.24529515e05 [[75.79973  49.756954]  [49.756954 54.81395 ]] 0.020133033 8.905516e06 2.9802322e08 1.4769792e05 [[48.398285 71.38826 ]  [71.38826  38.422195]] 0.028816395 1.3875822e05 7.291439e08 2.0913027e05 [[56.6334   61.245773]  [61.245773 57.220978]] 0.02039617 1.4980869e05 2.4110586e08 1.9685613e05 [[36.85694  68.622444]  [68.622444 54.947674]] 0.0100640785 1.0231617e05 2.9802322e08 2.2453856e05 [[30.75979 71.52322]  [71.52322 43.80476]] ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.38 jaxlib: 0.4.38 numpy:  1.26.4 python: 3.10.10 (tags/v3.10.10:aad5f6a, Feb  7 2023, 17:20:36) [MSC v.1929 64 bit (AMD64)] device info: cpu1, 1 local devices"" process_count: 1 platform: uname_result(system='Windows', node='DellLatitude7530', release='10', version='10.0.22631', machine='AMD64') ```",2025-01-20T13:12:48Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/25987,Probably related to this https://github.com/jaxml/jax/issues/15195issuecomment1488964513
yi,jax.nn.dot_product_attention CuDNN implementation raises tensor stride error during jit compile," Description I am currently experiencing an issue where I am getting a CuDNN error relating to the stride of my K matrix when using `jax.nn.dot_product_attention` within a flax model. This occurs when jitting and the error stems from the CuDNN dimension checks here. I am not sure what exactly is causing the striding issue with the `k` tensor, and I have checked the shapes and sharding for the inputs; however, I am struggling to find a way to debug this issue further. When using the `implementation` argument set to `'xla'`, the model jits, and I am able to train with it. The shapes for `q`, `k` and `v` are all `(8, 2048, 40, 128)` and all are sharded along the first (batch) dimension, having the following sharding: `NamedSharding(mesh=Mesh('dp': 1, 'fsdp': 8), spec=PartitionSpec('fsdp',), memory_kind=device)`. The function is called as below: ```python jax.nn.dot_product_attention(     q.astype(jnp.bfloat16),     k.astype(jnp.bfloat16),     v.astype(jnp.bfloat16),     mask=None,  I have tested with/without masking but get the same error either way     scale=float(q.shape[1] ** 0.5),     implementation='cudnn', ) ``` This gives the following error: ``` *** truncated *** File ""/app/.venv/lib/python3.11/sitepackages/jax/_src/compiler.py"", line 427, in compile_or_get_cached return _compile_and_write_cache( ^^^^^^^^^^^^^^^^^^^^^^^^^ File ""/app/.venv/lib/python3.11/sitepackages/jax/_src/compiler.py"", line 655, in _compile_and_write_cache executable = backend_compile( ^^^^^^^^^^^^^^^^ File ""/app/.venv/lib/python3.11/sitepackages/jax/_src/profiler.py"", line 333, in wrapper return func(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^ File ""/app/.venv/lib/python3.11/sitepackages/jax/_src/compiler.py"", line 273, in backend_compile raise e File ""/app/.venv/lib/python3.11/sitepackages/jax/_src/compiler.py"", line 267, in backend_compile return backend.compile(built_c, compile_options=options) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ jaxlib.xla_extension.XlaRuntimeError: INTERNAL: The stride for the last dimension corresponding to the embedding size per head should be 1 for input_names::K in external/xla/xla/stream_executor/cuda/cuda_dnn.cc(8221): 'graph_.build_operation_graph(cudnn>handle())' File ""/app/.venv/lib/python3.11/sitepackages/jax/_src/profiler.py"", line 333, in wrapper return func(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^ File ""/app/.venv/lib/python3.11/sitepackages/jax/_src/compiler.py"", line 273, in backend_compile raise e File ""/app/.venv/lib/python3.11/sitepackages/jax/_src/compiler.py"", line 267, in backend_compile return backend.compile(built_c, compile_options=options) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ jaxlib.xla_extension.XlaRuntimeError: INTERNAL: The stride for the last dimension corresponding to the embedding size per head should be 1 for input_names::K in external/xla/xla/stream_executor/cuda/cuda_dnn.cc(8221): 'graph_.build_operation_graph(cudnn>handle())' ``` If there are any ways to further debug the striding of my underlying tensor, and, if possible, how to force a contiguous layout that matches that of the shape of my tensor, please let me know.  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.38 jaxlib: 0.4.38 numpy:  2.0.2 python: 3.11.11 (main, Jan 14 2025, 22:49:08) [Clang 19.1.6 ] device info: NVIDIA H100 80GB HBM38, 8 local devices"" process_count: 1 platform: uname_result(system='Linux', node='experiment2eb4a7d7dad7head', release='6.8.049generic', version=' CC(Update neural_network_and_data_loading.ipynb)~22.04.1Ubuntu SMP PREEMPT_DYNAMIC Wed Nov  6 17:42:15 UTC 2', machine='x86_64') $ nvidiasmi Mon Jan 20 11:16:51 2025        ++  ++ ```",2025-01-20T11:38:25Z,bug NVIDIA GPU,open,0,4,https://github.com/jax-ml/jax/issues/25986,"Do you have a complete reproducer? I believe the problem here might be that XLA uses an incompatible layout for some intermediate value. If that's the case, the behavior will probably depend on what's around the cudnn call.",", unfortunately, I can't seem to reproduce this error with a minimal example of our model, which is one of the reasons it is quite hard to debug. It seems to only occur when used with the rest of our training code (which is part of a fairly large codebase).  That said, regarding the layout, I have an XLA dump containing some of the HLO from the ""beforeoptimisation"" pass. I assume there are no other passes because the compilation fails at this point. I think the layout for the q, k and v tensors seems okay here. The inner two dimensions are swapped in the layout; however, the embedding dimension that the error message complains about seems like it should have a stride of 1 here (unless I am reading something wrong). ```  Forward pass    q, k, v tensors:   convert.2768 = bf16[32,2048,40,128]{3,1,2,0} convert(transpose.2767), metadata={op_name=""jit()/jit(main)/jvp(ESM2Model)/while/body/_layers/transformer_block/self_attention/attention_fn/rotary_pos_emb/convert_element_type"" source_file=""/app/waffle/_src/models/esm2.py"" source_line=188}   convert.2803 = bf16[32,2048,40,128]{3,1,2,0} convert(transpose.2802), metadata={op_name=""jit()/jit(main)/jvp(ESM2Model)/while/body/_layers/transformer_block/self_attention/attention_fn/rotary_pos_emb/convert_element_type"" source_file=""/app/waffle/_src/models/esm2.py"" source_line=188}   call.2817 = bf16[1,1,2048,2048]{3,2,1,0} call(slice.2816, Arg_8.2611, Arg_9.2612), to_apply=_where_17.2589    mask:   reshape.2814 = pred[32,1,2048]{2,1,0} reshape(broadcast.2813), metadata={op_name=""jit()/jit(main)/jvp(ESM2Model)/while/body/_layers/transformer_block/self_attention/attention_fn/broadcast_in_dim"" source_file=""/app/waffle/_src/models/esm2.py"" source_line=350}    not sure what these args are:   Arg_10.2613 = bf16[0]{0} parameter(10), metadata={op_name=""jit()/jit(main)/jvp(ESM2Model)/while/body/closed_call""}   Arg_11.2614 = bf16[0]{0} parameter(11), metadata={op_name=""jit()/jit(main)/jvp(ESM2Model)/while/body/closed_call""}    attention call:   customcall.2818 = (bf16[32,2048,40,128]{3,2,1,0}, f32[32,40,2048]{2,1,0}) customcall(convert.2768, convert.2803, reshape.2810, call.2817, Arg_10.2613, /*index=5*/Arg_11.2614), custom_call_target=""CustomSPMDPartitioning"", api_version=API_VERSION_STATUS_RETURNING, metadata={op_name=""jit()/jit(main)/jvp(ESM2Model)/while/body/_layers/transformer_block/self_attention/attention_fn/custom_partitioning"" source_file=""/app/waffle/_src/models/esm2.py"" source_line=351}, backend_config=""137314937879632""    outputs:   gettupleelement.2819 = bf16[32,2048,40,128]{3,2,1,0} gettupleelement(customcall.2818), index=0, metadata={op_name=""jit()/jit(main)/jvp(ESM2Model)/while/body/_layers/transformer_block/self_attention/attention_fn/custom_partitioning"" source_file=""/app/waffle/_src/models/esm2.py"" source_line=351}   gettupleelement.2820 = f32[32,40,2048]{2,1,0} gettupleelement(customcall.2818), index=1, metadata={op_name=""jit()/jit(main)/jvp(ESM2Model)/while/body/_layers/transformer_block/self_attention/attention_fn/custom_partitioning"" source_file=""/app/waffle/_src/models/esm2.py"" source_line=351}  Rematerialised forward for backward    q, k, v tensors:   convert.4080 = bf16[32,2048,40,128]{3,1,2,0} convert(transpose.4079), metadata={op_name=""jit()/jit(main)/transpose(jvp(ESM2Model))/while/body/checkpoint/rematted_computation/_layers/transformer_block/self_attention/attention_fn/rotary_pos_emb/convert_element_type"" source_file=""/app/waffle/_src/models/esm2.py"" source_line=188}   convert.4130 = bf16[32,2048,40,128]{3,1,2,0} convert(transpose.4129), metadata={op_name=""jit()/jit(main)/transpose(jvp(ESM2Model))/while/body/checkpoint/rematted_computation/_layers/transformer_block/self_attention/attention_fn/rotary_pos_emb/convert_element_type"" source_file=""/app/waffle/_src/models/esm2.py"" source_line=188}   reshape.4030 = bf16[32,2048,40,128]{3,2,1,0} reshape(add.4029), metadata={op_name=""jit()/jit(main)/transpose(jvp(ESM2Model))/while/body/checkpoint/rematted_computation/_layers/transformer_block/self_attention/attention_fn/project_value/reshape"" source_file=""/app/waffle/_src/models/esm2.py"" source_line=266}    mask:   call.4137 = bf16[1,1,2048,2048]{3,2,1,0} call(slice.4136, constant.3947, constant.3946), to_apply=_where_27.3755    not sure what these args are:   constant.3867 = bf16[0]{0} constant({})    attention call:   customcall.4138 = (bf16[32,2048,40,128]{3,2,1,0}, f32[32,40,2048]{2,1,0}) customcall(convert.4080, convert.4130, reshape.4030, call.4137, constant.3867, /*index=5*/constant.3867), custom_call_target=""CustomSPMDPartitioning"", api_version=API_VERSION_STATUS_RETURNING, metadata={op_name=""jit()/jit(main)/transpose(jvp(ESM2Model))/while/body/checkpoint/rematted_computation/_layers/transformer_block/self_attention/attention_fn/custom_partitioning"" source_file=""/app/waffle/_src/models/esm2.py"" source_line=351}, backend_config=""137314937867472""    outputs:   gettupleelement.4139 = bf16[32,2048,40,128]{3,2,1,0} gettupleelement(customcall.4138), index=0, metadata={op_name=""jit()/jit(main)/transpose(jvp(ESM2Model))/while/body/checkpoint/rematted_computation/_layers/transformer_block/self_attention/attention_fn/custom_partitioning"" source_file=""/app/waffle/_src/models/esm2.py"" source_line=351}   gettupleelement.4140 = f32[32,40,2048]{2,1,0} gettupleelement(customcall.4138), index=1, metadata={op_name=""jit()/jit(main)/transpose(jvp(ESM2Model))/while/body/checkpoint/rematted_computation/_layers/transformer_block/self_attention/attention_fn/custom_partitioning"" source_file=""/app/waffle/_src/models/esm2.py"" source_line=351}   Backward    q, k, v, mask etc. all same as above    attention call:   customcall.4327 = (bf16[32,2048,40,128]{3,2,1,0}, bf16[32,2048,40,128]{3,2,1,0}, bf16[32,2048,40,128]{3,2,1,0}) customcall(convert.4080, convert.4130, reshape.4030, call.4137, constant.3867, /*index=5*/constant.3867, gettupleelement.4140, gettupleelement.4139, convert.4326), custom_call_target=""CustomSPMDPartitioning"", api_version=API_VERSION_STATUS_RETURNING, metadata={op_name=""jit()/jit(main)/transpose(jvp(ESM2Model))/while/body/checkpoint/_layers/transformer_block/self_attention/attention_fn/custom_partitioning"" source_file=""/app/waffle/_src/models/esm2.py"" source_line=351}, backend_config=""137314937656976""    outputs:   gettupleelement.4328 = bf16[32,2048,40,128]{3,2,1,0} gettupleelement(customcall.4327), index=0, metadata={op_name=""jit()/jit(main)/transpose(jvp(ESM2Model))/while/body/checkpoint/_layers/transformer_block/self_attention/attention_fn/custom_partitioning"" source_file=""/app/waffle/_src/models/esm2.py"" source_line=351}   gettupleelement.4329 = bf16[32,2048,40,128]{3,2,1,0} gettupleelement(customcall.4327), index=1, metadata={op_name=""jit()/jit(main)/transpose(jvp(ESM2Model))/while/body/checkpoint/_layers/transformer_block/self_attention/attention_fn/custom_partitioning"" source_file=""/app/waffle/_src/models/esm2.py"" source_line=351}   gettupleelement.4330 = bf16[32,2048,40,128]{3,2,1,0} gettupleelement(customcall.4327), index=2, metadata={op_name=""jit()/jit(main)/transpose(jvp(ESM2Model))/while/body/checkpoint/_layers/transformer_block/self_attention/attention_fn/custom_partitioning"" source_file=""/app/waffle/_src/models/esm2.py"" source_line=351} ```","Did you run this just with `xla_dump_to`, or also with `xla_dump_hlo_pass_re`? The latter should be set to `.*` to get dumps after every pass.","No, I hadn't initially run with the regex flag  sorry about that! I have done so now, and I am seeing many more passes in the XLA dump. I have attached the final pass below. module_0027.jit__unnamed_wrapped_function_.0176.fusiondispatchpipeline.after_pipelinestart.before_fusionblocklevelrewriter.txt Does this imply that the error occurs within the fusionblocklevelrewriter? I have extracted the relevant lines around the CuDNN calls from the attached file here: ``` Call 1: input_concatenate_fusion = bf16[8,2048,5,128]{3,2,1,0} fusion(gettupleelement.579, gettupleelement.580, gettupleelement.520.0, gettupleelement.109.0), kind=kInput, calls=fused_concatenate, metadata={op_name=""jit()/jit(main)/transpose(jvp(ESM2Model))/while/body/checkpoint/rematted_computation/_layers/transformer_block/self_attention/attention_fn/rotary_pos_emb/vmap(vmap(rotary_pos_emb._apply_rope))/vmap(rotary_pos_emb._apply_rope_1d)/concatenate"" source_file=""/app/waffle/_src/models/esm2.py"" source_line=153 deduplicated_name=""input_concatenate_fusion""} input_concatenate_fusion.1 = bf16[8,2048,5,128]{3,2,1,0} fusion(gettupleelement.579, gettupleelement.580, gettupleelement.521.0, gettupleelement.111.0), kind=kInput, calls=fused_concatenate.1, metadata={op_name=""jit()/jit(main)/transpose(jvp(ESM2Model))/while/body/checkpoint/rematted_computation/_layers/transformer_block/self_attention/attention_fn/rotary_pos_emb/vmap(vmap(rotary_pos_emb._apply_rope))/vmap(rotary_pos_emb._apply_rope_1d)/concatenate"" source_file=""/app/waffle/_src/models/esm2.py"" source_line=153 deduplicated_name=""input_concatenate_fusion""} bitcast.22772.0 = bf16[8,2048,5,128]{3,2,1,0} bitcast(loop_add_fusion.2) loop_broadcast_fusion = bf16[1,1,2048,2048]{3,2,1,0} fusion(allreducedone), kind=kLoop, calls=fused_broadcast, metadata={op_name=""jit()/jit(main)/transpose(jvp(ESM2Model))/while/body/checkpoint/rematted_computation/_layers/transformer_block/self_attention/attention_fn/broadcast_in_dim"" source_file=""/app/waffle/_src/models/esm2.py"" source_line=350} customcall.3.0 = (bf16[8,5,2048,128]{3,1,2,0}, f32[8,5,2048]{2,1,0}, u8[0]{0}) customcall(input_concatenate_fusion, input_concatenate_fusion.1, bitcast.22772.0, loop_broadcast_fusion), custom_call_target=""__cudnn$fmhaScaleBiasSoftmax"", operand_layout_constraints={bf16[8,2048,5,128]{3,2,1,0}, bf16[8,2048,5,128]{3,2,1,0}, bf16[8,2048,5,128]{3,2,1,0}, bf16[1,1,2048,2048]{3,2,1,0}}, api_version=API_VERSION_STATUS_RETURNING, metadata={op_name=""jit()/jit(main)/transpose(jvp(ESM2Model))/while/body/checkpoint/rematted_computation/_layers/transformer_block/self_attention/attention_fn/custom_partitioning"" source_file=""/app/waffle/_src/models/esm2.py"" source_line=351}, backend_config={""operation_queue_id"": ""0"", ""wait_on_operation_queues"": [], ""cudnn_fmha_backend_config"": {""algorithm"": {""algo_id"": ""0"", ""math_type"": ""TENSOR_OP_MATH"", ""tuning_knobs"": {""17"": ""1"", ""24"": ""0""}, ""is_cudnn_frontend"": true, ""workspace_size"": ""0""}, ""fmha_scale"": 0.08838834764831845, ""intermediate_tensor_shape"": {""element_type"": ""BF16"", ""dimensions"": [""8"", ""5"", ""2048"", ""2048""], ""tuple_shapes"": [], ""layout"": {""dim_level_types"": [], ""dim_unique"": [], ""dim_ordered"": [], ""minor_to_major"": [""3"", ""2"", ""1"", ""0""], ""tiles"": [], ""element_size_in_bits"": ""0"", ""memory_space"": ""0"", ""index_primitive_type"": ""PRIMITIVE_TYPE_INVALID"", ""pointer_primitive_type"": ""PRIMITIVE_TYPE_INVALID"", ""dynamic_shape_metadata_prefix_bytes"": ""0""}, ""is_dynamic_dimension"": [false, false, false, false]}, ""is_flash_attention"": true, ""mask_type"": ""NO_MASK"", ""bmm1_dot_dimension_numbers"": {""lhs_contracting_dimensions"": [""3""], ""rhs_contracting_dimensions"": [""3""], ""lhs_batch_dimensions"": [""0"", ""2""], ""rhs_batch_dimensions"": [""0"", ""2""]}, ""bmm2_dot_dimension_numbers"": {""lhs_contracting_dimensions"": [""3""], ""rhs_contracting_dimensions"": [""1""], ""lhs_batch_dimensions"": [""0"", ""1""], ""rhs_batch_dimensions"": [""0"", ""2""]}, ""dropout_rate"": 0.0, ""seed"": 42, ""sliding_window_length"": 0}} Call 2: input_concatenate_fusion = bf16[8,2048,5,128]{3,2,1,0} fusion(gettupleelement.579, gettupleelement.580, gettupleelement.520.0, gettupleelement.109.0), kind=kInput, calls=fused_concatenate, metadata={op_name=""jit()/jit(main)/transpose(jvp(ESM2Model))/while/body/checkpoint/rematted_computation/_layers/transformer_block/self_attention/attention_fn/rotary_pos_emb/vmap(vmap(rotary_pos_emb._apply_rope))/vmap(rotary_pos_emb._apply_rope_1d)/concatenate"" source_file=""/app/waffle/_src/models/esm2.py"" source_line=153 deduplicated_name=""input_concatenate_fusion""} input_concatenate_fusion.1 = bf16[8,2048,5,128]{3,2,1,0} fusion(gettupleelement.579, gettupleelement.580, gettupleelement.521.0, gettupleelement.111.0), kind=kInput, calls=fused_concatenate.1, metadata={op_name=""jit()/jit(main)/transpose(jvp(ESM2Model))/while/body/checkpoint/rematted_computation/_layers/transformer_block/self_attention/attention_fn/rotary_pos_emb/vmap(vmap(rotary_pos_emb._apply_rope))/vmap(rotary_pos_emb._apply_rope_1d)/concatenate"" source_file=""/app/waffle/_src/models/esm2.py"" source_line=153 deduplicated_name=""input_concatenate_fusion""} bitcast.22772.0 = bf16[8,2048,5,128]{3,2,1,0} bitcast(loop_add_fusion.2) gettupleelement.126.0 = f32[8,5,2048]{2,1,0} gettupleelement(customcall.3.0), index=1, metadata={op_name=""jit()/jit(main)/transpose(jvp(ESM2Model))/while/body/checkpoint/rematted_computation/_layers/transformer_block/self_attention/attention_fn/custom_partitioning"" source_file=""/app/waffle/_src/models/esm2.py"" source_line=351} bitcast.22788.0 = bf16[8,2048,40,128]{3,2,1,0} bitcast(allreducedone.1) bitcast.22794.0 = bf16[8,2048,5,128]{3,2,1,0} bitcast(gettupleelement.116.0) customcall.10.0 = (bf16[8,5,2048,128]{3,1,2,0}, bf16[8,5,2048,128]{3,1,2,0}, bf16[8,5,2048,128]{3,1,2,0}, u8[0]{0}) customcall(input_concatenate_fusion, input_concatenate_fusion.1, bitcast.22772.0, gettupleelement.126.0, bitcast.22788.0, /*index=5*/loop_broadcast_fusion, bitcast.22794.0), custom_call_target=""__cudnn$fmhaScaleBiasSoftmaxBackward"", operand_layout_constraints={bf16[8,2048,5,128]{3,2,1,0}, bf16[8,2048,5,128]{3,2,1,0}, bf16[8,2048,5,128]{3,2,1,0}, f32[8,5,2048]{2,1,0}, bf16[8,2048,40,128]{3,2,1,0}, bf16[1,1,2048,2048]{3,2,1,0}, bf16[8,2048,5,128]{3,2,1,0}}, api_version=API_VERSION_STATUS_RETURNING, metadata={op_name=""jit()/jit(main)/transpose(jvp(ESM2Model))/while/body/checkpoint/_layers/transformer_block/self_attention/attention_fn/custom_partitioning"" source_file=""/app/waffle/_src/models/esm2.py"" source_line=351}, backend_config={""operation_queue_id"": ""0"", ""wait_on_operation_queues"": [], ""cudnn_fmha_backend_config"": {""algorithm"": {""algo_id"": ""0"", ""math_type"": ""TENSOR_OP_MATH"", ""tuning_knobs"": {""17"": ""1"", ""24"": ""0""}, ""is_cudnn_frontend"": true, ""workspace_size"": ""0""}, ""fmha_scale"": 0.08838834764831845, ""intermediate_tensor_shape"": {""element_type"": ""BF16"", ""dimensions"": [""8"", ""5"", ""2048"", ""2048""], ""tuple_shapes"": [], ""layout"": {""dim_level_types"": [], ""dim_unique"": [], ""dim_ordered"": [], ""minor_to_major"": [""3"", ""2"", ""1"", ""0""], ""tiles"": [], ""element_size_in_bits"": ""0"", ""memory_space"": ""0"", ""index_primitive_type"": ""PRIMITIVE_TYPE_INVALID"", ""pointer_primitive_type"": ""PRIMITIVE_TYPE_INVALID"", ""dynamic_shape_metadata_prefix_bytes"": ""0""}, ""is_dynamic_dimension"": [false, false, false, false]}, ""is_flash_attention"": true, ""mask_type"": ""NO_MASK"", ""bmm1_grad_gemm1_dot_dimension_numbers"": {""lhs_contracting_dimensions"": [""2""], ""rhs_contracting_dimensions"": [""1""], ""lhs_batch_dimensions"": [""0"", ""1""], ""rhs_batch_dimensions"": [""0"", ""2""]}, ""bmm1_grad_gemm2_dot_dimension_numbers"": {""lhs_contracting_dimensions"": [""3""], ""rhs_contracting_dimensions"": [""1""], ""lhs_batch_dimensions"": [""0"", ""1""], ""rhs_batch_dimensions"": [""0"", ""2""]}, ""bmm2_grad_gemm1_dot_dimension_numbers"": {""lhs_contracting_dimensions"": [""2""], ""rhs_contracting_dimensions"": [""1""], ""lhs_batch_dimensions"": [""0"", ""1""], ""rhs_batch_dimensions"": [""0"", ""2""]}, ""bmm2_grad_gemm2_dot_dimension_numbers"": {""lhs_contracting_dimensions"": [""3""], ""rhs_contracting_dimensions"": [""3""], ""lhs_batch_dimensions"": [""0"", ""2""], ""rhs_batch_dimensions"": [""0"", ""2""]}, ""dropout_rate"": 0.0, ""seed"": 42, ""sliding_window_length"": 0}} Call 3: input_concatenate_fusion.4 = bf16[8,2048,5,128]{3,2,1,0} fusion(gettupleelement.967, gettupleelement.966, gettupleelement.537.0, gettupleelement.962, copy.253), kind=kInput, calls=fused_concatenate.4, metadata={op_name=""jit()/jit(main)/jvp(ESM2Model)/while/body/_layers/transformer_block/self_attention/attention_fn/rotary_pos_emb/vmap(vmap(rotary_pos_emb._apply_rope))/vmap(rotary_pos_emb._apply_rope_1d)/concatenate"" source_file=""/app/waffle/_src/models/esm2.py"" source_line=153 deduplicated_name=""input_concatenate_fusion.4""} input_concatenate_fusion.5 = bf16[8,2048,5,128]{3,2,1,0} fusion(gettupleelement.967, gettupleelement.966, gettupleelement.538.0, gettupleelement.958, copy.253), kind=kInput, calls=fused_concatenate.5, metadata={op_name=""jit()/jit(main)/jvp(ESM2Model)/while/body/_layers/transformer_block/self_attention/attention_fn/rotary_pos_emb/vmap(vmap(rotary_pos_emb._apply_rope))/vmap(rotary_pos_emb._apply_rope_1d)/concatenate"" source_file=""/app/waffle/_src/models/esm2.py"" source_line=153 deduplicated_name=""input_concatenate_fusion.4""} bitcast.23551.0 = bf16[8,2048,5,128]{3,2,1,0} bitcast(loop_add_fusion.5) loop_select_fusion.1 = bf16[1,1,2048,2048]{3,2,1,0} fusion(gettupleelement.969, gettupleelement.968, allreducedone.6), kind=kLoop, calls=fused_select.1, metadata={op_name=""jit()/jit(main)/jvp(ESM2Model)/while/body/_layers/transformer_block/self_attention/attention_fn/jit(_where)/select_n"" source_file=""/app/waffle/_src/models/esm2.py"" source_line=351} customcall.5.0 = (bf16[8,5,2048,128]{3,1,2,0}, f32[8,5,2048]{2,1,0}, u8[0]{0}) customcall(input_concatenate_fusion.4, input_concatenate_fusion.5, bitcast.23551.0, loop_select_fusion.1), custom_call_target=""__cudnn$fmhaScaleBiasSoftmax"", operand_layout_constraints={bf16[8,2048,5,128]{3,2,1,0}, bf16[8,2048,5,128]{3,2,1,0}, bf16[8,2048,5,128]{3,2,1,0}, bf16[1,1,2048,2048]{3,2,1,0}}, api_version=API_VERSION_STATUS_RETURNING, metadata={op_name=""jit()/jit(main)/jvp(ESM2Model)/while/body/_layers/transformer_block/self_attention/attention_fn/custom_partitioning"" source_file=""/app/waffle/_src/models/esm2.py"" source_line=351}, backend_config={""operation_queue_id"": ""0"", ""wait_on_operation_queues"": [], ""cudnn_fmha_backend_config"": {""algorithm"": {""algo_id"": ""0"", ""math_type"": ""TENSOR_OP_MATH"", ""tuning_knobs"": {""17"": ""1"", ""24"": ""0""}, ""is_cudnn_frontend"": true, ""workspace_size"": ""0""}, ""fmha_scale"": 0.08838834764831845, ""intermediate_tensor_shape"": {""element_type"": ""BF16"", ""dimensions"": [""8"", ""5"", ""2048"", ""2048""], ""tuple_shapes"": [], ""layout"": {""dim_level_types"": [], ""dim_unique"": [], ""dim_ordered"": [], ""minor_to_major"": [""3"", ""2"", ""1"", ""0""], ""tiles"": [], ""element_size_in_bits"": ""0"", ""memory_space"": ""0"", ""index_primitive_type"": ""PRIMITIVE_TYPE_INVALID"", ""pointer_primitive_type"": ""PRIMITIVE_TYPE_INVALID"", ""dynamic_shape_metadata_prefix_bytes"": ""0""}, ""is_dynamic_dimension"": [false, false, false, false]}, ""is_flash_attention"": true, ""mask_type"": ""NO_MASK"", ""bmm1_dot_dimension_numbers"": {""lhs_contracting_dimensions"": [""3""], ""rhs_contracting_dimensions"": [""3""], ""lhs_batch_dimensions"": [""0"", ""2""], ""rhs_batch_dimensions"": [""0"", ""2""]}, ""bmm2_dot_dimension_numbers"": {""lhs_contracting_dimensions"": [""3""], ""rhs_contracting_dimensions"": [""1""], ""lhs_batch_dimensions"": [""0"", ""1""], ""rhs_batch_dimensions"": [""0"", ""2""]}, ""dropout_rate"": 0.0, ""seed"": 42, ""sliding_window_length"": 0}} ```"
yi,`lambda x: jnp.log(x == 1)` is not jit-able on metal.," Description The function `lambda x: jnp.log(x == 1)` is not jitable on metal and raises the following error message. ``` /AppleInternal/Library/BuildRoots/b11baf739ee011efb7b47aebe1f78c73/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShaders/MPSCore/Utility/MPSLibrary.mm:570: failed assertion `MPSKernel MTLComputePipelineStateCache unable to load function ndArrayIdentity.         Compiler encountered an internal error: (null) ``` The expression above is of course equivalent to `lambda x: jnp.where(x == 1, 0, jnp.inf)`, but, for example, appears in the evaluation of the log density of the delta distribution in `numpyro` here. I couldn't find a simpler expression that raised the same error. Here's a script to reproduce the issue. ```python  identity_bug.py import jax from jax.lib import xla_bridge from jax import numpy as jnp def f(x):     return jnp.log(x == 1) args = [jnp.arange(2), jnp.ones(2)] print(f""running on {xla_bridge.get_backend().platform} ..."") fns = {""f"": f, ""jitted_f"": jax.jit(f)} for key, fn in fns.items():     for x in args:         print(f""{key}({x}) > ..."", end="" "", flush=True)         print(fn(x), flush=True) print(""done"") ``` Running on metal, I get the following. ```bash $ python identity_bug.py WARNING:20250116 17:19:57,590:jax._src.xla_bridge:1000: Platform 'METAL' is experimental and not all JAX functionality may be correctly supported! WARNING: All log messages before absl::InitializeLog() is called are written to STDERR W0000 00:00:1737065997.590723 34394534 mps_client.cc:510] WARNING: JAX Apple GPU support is experimental and not all JAX functionality is correctly supported! Metal device set to: Apple M1 systemMemory: 16.00 GB maxCacheSize: 5.33 GB I0000 00:00:1737065997.605677 34394534 service.cc:145] XLA service 0x128fbfd70 initialized for platform METAL (this does not guarantee that XLA will be used). Devices: I0000 00:00:1737065997.605694 34394534 service.cc:153]   StreamExecutor device (0): Metal,  I0000 00:00:1737065997.607700 34394534 mps_client.cc:406] Using Simple allocator. I0000 00:00:1737065997.607742 34394534 mps_client.cc:384] XLA backend will use up to 11452858368 bytes on device 0 for SimpleAllocator. /Users/till/git/tycho/playground/identity_bug.py:11: DeprecationWarning: jax.lib.xla_bridge.get_backend is deprecated; use jax.extend.backend.get_backend.   print(f""running on {xla_bridge.get_backend().platform} ..."") running on METAL ... f([0 1]) > ... [inf   0.] f([1. 1.]) > ... [0. 0.] jitted_f([0 1]) > ... /AppleInternal/Library/BuildRoots/b11baf739ee011efb7b47aebe1f78c73/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShaders/MPSCore/Utility/MPSLibrary.mm:570: failed assertion `MPSKernel MTLComputePipelineStateCache unable to load function ndArrayIdentity.         Compiler encountered an internal error: (null) ' [1]    52590 abort      python identity_bug.py ``` Restricting to CPU gives the following. ```bash $ JAX_PLATFORM_NAME=cpu python identity_bug.py                  WARNING:20250116 17:22:04,614:jax._src.xla_bridge:1000: Platform 'METAL' is experimental and not all JAX functionality may be correctly supported! WARNING: All log messages before absl::InitializeLog() is called are written to STDERR W0000 00:00:1737066124.615053 34398748 mps_client.cc:510] WARNING: JAX Apple GPU support is experimental and not all JAX functionality is correctly supported! Metal device set to: Apple M1 systemMemory: 16.00 GB maxCacheSize: 5.33 GB I0000 00:00:1737066124.629618 34398748 service.cc:145] XLA service 0x112537590 initialized for platform METAL (this does not guarantee that XLA will be used). Devices: I0000 00:00:1737066124.629654 34398748 service.cc:153]   StreamExecutor device (0): Metal,  I0000 00:00:1737066124.631511 34398748 mps_client.cc:406] Using Simple allocator. I0000 00:00:1737066124.631542 34398748 mps_client.cc:384] XLA backend will use up to 11452858368 bytes on device 0 for SimpleAllocator. /Users/till/git/tycho/playground/identity_bug.py:11: DeprecationWarning: jax.lib.xla_bridge.get_backend is deprecated; use jax.extend.backend.get_backend.   print(f""running on {xla_bridge.get_backend().platform} ..."") running on cpu ... f([0 1]) > ... [inf   0.] f([1. 1.]) > ... [0. 0.] jitted_f([0 1]) > ... [inf   0.] jitted_f([1. 1.]) > ... [0. 0.] done I0000 00:00:1737066124.861558 34398748 mps_client.h:209] MetalClient destroyed. ``` This seems to be specific to the combination of equality testing and `jnp.log`, e.g., the following all work on both backends: ```python lambda x: 1 / (x == 1) lamda x: jnp.exp(x == 1)  This is equivalent to jnp.log(x == 1) because log(1 / x) =  log(x). lambda x:  jnp.log(1 / (x == 1)) ```  System info (python version, jaxlib version, accelerator, etc.) ```python Metal device set to: Apple M1 jax:    0.4.38 jaxlib: 0.4.38 numpy:  2.2.1 python: 3.11.5 (main, Dec  8 2023, 17:04:09) [Clang 15.0.0 (clang1500.0.40.1)] device info: Metal1, 1 local devices"" process_count: 1 platform: uname_result(system='Darwin', node='TillsMacBookPro3.local', release='24.2.0', version='Darwin Kernel Version 24.2.0: Fri Dec  6 18:40:14 PST 2024; root:xnu11215.61.5~2/RELEASE_ARM64_T8103', machine='arm64') systemMemory: 16.00 GB maxCacheSize: 5.33 GB ```",2025-01-16T22:31:15Z,bug Apple GPU (Metal) plugin,open,3,1,https://github.com/jax-ml/jax/issues/25935,"Thx, we are looking into it. "
yi,"[sharding_in_types] If an indexing operation hits into `gather_p`, error out saying to use `.at[...].get(out_spec=...)` instead.","[sharding_in_types] If an indexing operation hits into `gather_p`, error out saying to use `.at[...].get(out_spec=...)` instead. This will basically drop the gather operation into full auto mode and add a sharding constraint on the output given by the user via `out_spec`. Coauthoredby: Matthew Johnson ",2025-01-16T18:19:47Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25929
yi,[better_errors] Improvements in propagation of debugging info,"Added some documentation for `TracingDebugInfo` (docstring, comments about `arg_names`, since it was not obvious to me that this would flatten the nonstatic arguments). Laying the ground for the unification of the old `api_util.debug_info` and `partial_eval.tracing_debug_info`: we rename the former to `api_util.tracing_debug_info`, we push inside the calls to `fun_sourceinfo` and `fun_signature` (which were done by the callers until now), and we rewrite the latter in terms of the former. We leave for a future PR the actual replacing of the latter with the former throughout. In the process of above, cleaned up the one case when `partial_eval.tracing_debug_info` received None for the `in_tree` and `out_tracer_thunk`. The function contained catchall exception clauses to handle those, but doing so it masked other places where we fail to collect debug info due to programming mistakes. E.g., in one place we passed a `WrappedFun` instead of a `Callable`, resulting in missing debugging info. Added more type declarations. Added a `state_test` with a failure to track debugging information, manifested with a leaked tracer without function provenance. Fixing this in a subsequent PR.",2025-01-15T23:09:26Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/25916
yi,Examine `partial.func` when trying to find function names,Make utils for reporting function name work with `functools.partial` by using the inner `.func` attribute if the object doesn't have a `__name__` attribute. `functools.partial` objects do not have `__name__` attributes by default.,2025-01-15T17:25:00Z,,closed,0,2,https://github.com/jax-ml/jax/issues/25908,Can you add a test for this please? (Or maybe modify an existing test if there are failing tests),(also please squash your commits)
yi,[Mosaic GPU] Allow querying layouts from a `FuncOp`'s block arguments if set.,"[Mosaic GPU] Allow querying layouts from a `FuncOp`'s block arguments if set. The motivation behind this change is twofold: 1. it simplifies test writing (no need to produce arbitrary, manual, nonsplat    constants to produce arguments with a strided layout); 2. it'll allow running layout inference on different `FuncOp`s in isolation,    before inlining. While the primary motivation is to simplify test writing for upcoming changes, `2.` is useful if we ever intend to call functions whose body's layout we have inferred from other functions. It's not clear to me that we have a use case for that, but the theoretical benefit is worth pointing out. Crucially, layout inference does not set default layouts for `FuncOp`s, since the caller may choose a different layout for its arguments. As a result, there is also no layout inference rule for `func.FuncOp`.",2025-01-15T13:49:42Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25899
yi,"JAX cant find shardings (""AttributeError: 'UnspecifiedValue' object has no attribute 'is_fully_replicated'"")"," Description Below is the code I ran and the stack trace.  ``` !pip install q U kerasnlp tensorflowtext  Install tensorflowcpu so tensorflow does not attempt to access the TPU. !pip install q U tensorflowcpu import jax jax.devices() import os  The Keras 3 distribution API is only implemented for the JAX backend for now os.environ[""KERAS_BACKEND""] = ""jax""  Preallocate all TPU memory to minimize memory fragmentation and allocation overhead. os.environ[""XLA_PYTHON_CLIENT_MEM_FRACTION""] = ""1.0"" import keras import keras_nlp  Create a device mesh with (1, 8) shape so that the weights are sharded across  all 8 TPUs. device_mesh = keras.distribution.DeviceMesh(     (1, 8),     [""batch"", ""model""],     devices=keras.distribution.list_devices(), ) model_dim = ""model"" layout_map = keras.distribution.LayoutMap(device_mesh)  Weights that match 'token_embedding/embeddings' will be sharded on 8 TPUs layout_map[""token_embedding/embeddings""] = (model_dim, None)  Regex to match against the query, key and value matrices in attention layers layout_map[""decoder_block.*attention.*(queryvalue)/kernel""] = (model_dim, None, None) layout_map[""decoder_block.*attention_output/kernel""] = (model_dim, None, None) layout_map[""decoder_block.*ffw_gating.*/kernel""] = (None, model_dim) layout_map[""decoder_block.*ffw_linear/kernel""] = (model_dim, None) model_parallel = keras.distribution.ModelParallel(     layout_map=layout_map,     batch_dim_name=""batch"", ) keras.distribution.set_distribution(model_parallel) gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(""gemma2_9b_en"")  Verify model was partitioned correctly decoder_block_1 = gemma_lm.backbone.get_layer('decoder_block_1') print(type(decoder_block_1)) for variable in decoder_block_1.weights:   print(f'{variable.path: 1 print(gemma_lm.generate(""How can I plan a trip to Europe?"", max_length=512)) File /usr/local/lib/python3.10/sitepackages/keras_hub/src/models/causal_lm.py:388, in CausalLM.generate(self, inputs, max_length, stop_token_ids, strip_prompt)     385     outputs = [generate(x) for x in inputs]     387 if self.preprocessor is not None: > 388     outputs = [postprocess(x) for x in outputs]     390 return self._normalize_generate_outputs(outputs, input_is_scalar) File /usr/local/lib/python3.10/sitepackages/keras_hub/src/models/causal_lm.py:388, in (.0)     385     outputs = [generate(x) for x in inputs]     387 if self.preprocessor is not None: > 388     outputs = [postprocess(x) for x in outputs]     390 return self._normalize_generate_outputs(outputs, input_is_scalar) File /usr/local/lib/python3.10/sitepackages/keras_hub/src/models/causal_lm.py:374, in CausalLM.generate..postprocess(x)     373 def postprocess(x): > 374     return self.preprocessor.generate_postprocess(x) File /usr/local/lib/python3.10/sitepackages/keras_hub/src/utils/tensor_utils.py:48, in preprocessing_function..wrapper(self, x, **kwargs)      46 x = convert_preprocessing_inputs(x)      47 with no_convert_scope(): > 48     x = fn(self, x, **kwargs)      49 return convert_preprocessing_outputs(x) File /usr/local/lib/python3.10/sitepackages/keras_hub/src/models/causal_lm_preprocessor.py:159, in CausalLMPreprocessor.generate_postprocess(self, x)     157 token_ids, padding_mask = x[""token_ids""], x[""padding_mask""]     158 ids_to_strip = self.tokenizer.special_token_ids > 159 token_ids = strip_to_ragged(token_ids, padding_mask, ids_to_strip)     160 return self.tokenizer.detokenize(token_ids) File /usr/local/lib/python3.10/sitepackages/keras_hub/src/utils/tensor_utils.py:245, in strip_to_ragged(token_ids, mask, ids_to_strip)     243 def strip_to_ragged(token_ids, mask, ids_to_strip):     244     """"""Remove masked and special tokens from a sequence before detokenizing."""""" > 245     mask = tf.cast(mask, ""bool"")     246     for id in ids_to_strip:     247         mask = mask & (token_ids != id) File /usr/local/lib/python3.10/sitepackages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback..error_handler(*args, **kwargs)     151 except Exception as e:     152   filtered_tb = _process_traceback_frames(e.__traceback__) > 153   raise e.with_traceback(filtered_tb) from None     154 finally:     155   del filtered_tb File /usr/local/lib/python3.10/sitepackages/jax/_src/array.py:390, in ArrayImpl.__array__(self, dtype, context)     389 def __array__(self, dtype=None, context=None): > 390   return np.asarray(self._value, dtype=dtype) File /usr/local/lib/python3.10/sitepackages/jax/_src/profiler.py:336, in annotate_function..wrapper(*args, **kwargs)     333 (func)     334 def wrapper(*args, **kwargs):     335   with TraceAnnotation(name, **decorator_kwargs): > 336     return func(*args, **kwargs)     337   return wrapper File /usr/local/lib/python3.10/sitepackages/jax/_src/array.py:588, in ArrayImpl._value(self)     585 self._check_if_deleted()     587 if self._npy_value is None: > 588   if self.is_fully_replicated:     589     self._npy_value = self._single_device_array_to_np_array()   type: ignore     590     self._npy_value.flags.writeable = False File /usr/local/lib/python3.10/sitepackages/jax/_src/array.py:354, in ArrayImpl.is_fully_replicated(self)     352      353 def is_fully_replicated(self) > bool: > 354   return self.sharding.is_fully_replicated AttributeError: 'UnspecifiedValue' object has no attribute 'is_fully_replicated' ``` pip freeze output: ``` abslpy==2.1.0 accelerate==0.32.1 aiofiles==22.1.0 aiosqlite==0.20.0 albucore==0.0.12 albumentations==1.4.11 annotatedtypes==0.7.0 ansicolors==1.1.8 anyio==4.4.0 argon2cffi==23.1.0 argon2cffibindings==21.2.0 array_record==0.5.1 arrow==1.3.0 astroid==3.2.2 asttokens==2.4.1 astunparse==1.6.3 attrs==23.2.0 audioread==3.0.1 autopep8==2.0.4 Babel==2.15.0 beautifulsoup4==4.12.3 bleach==6.1.0 cachetools==5.3.3 certifi==2024.7.4 cffi==1.16.0 charsetnormalizer==3.3.2 chex==0.1.86 click==8.1.7 cloudtpuclient==0.10 cloudpickle==3.0.0 comm==0.2.2 contourpy==1.2.1 cramjam==2.8.3 cycler==0.12.1 debugpy==1.8.2 decorator==5.1.1 defusedxml==0.7.1 diffusers==0.29.2 dill==0.3.8 distrax==0.1.5 dmhaiku @ git+https://github.com/deepmind/dmhaiku dmtree==0.1.8 docstringtomarkdown==0.15 docstring_parser==0.16 einops==0.8.0 entrypoints==0.4 etils==1.7.0 eval_type_backport==0.2.0 exceptiongroup==1.2.1 executing==2.0.1 fastjsonschema==2.20.0 fastparquet==2024.5.0 filelock==3.15.4 flake8==7.0.0 flatbuffers==24.3.25 flax==0.8.4 fonttools==4.53.1 fqdn==1.5.1 fsspec==2024.6.1 funcsigs==1.0.2 gast==0.6.0 ginconfig==0.5.0 googleapicore==1.34.1 googleapipythonclient==1.8.0 googleauth==2.32.0 googleauthhttplib2==0.2.0 googlepasta==0.2.0 googleapiscommonprotos==1.63.2 grpcio==1.64.1 gym==0.26.2 gymnotices==0.0.8 h5py==3.11.0 httplib2==0.22.0 huggingfacehub==0.23.4 idna==3.7 imageio==2.34.2 immutabledict==4.2.0 importlib_metadata==8.0.0 importlib_resources==6.4.0 ipykernel==6.29.5 ipython==8.26.0 ipythongenutils==0.2.0 isoduration==20.11.0 isort==5.13.2 jax==0.4.23 jaxlib==0.4.23 jedi==0.19.1 Jinja2==3.1.4 jmp==0.0.4 joblib==1.4.2 jraph==0.0.6.dev0 json5==0.9.25 jsonpointer==3.0.0 jsonschema==4.23.0 jsonschemaspecifications==2023.12.1 jupyterevents==0.10.0 jupyterlsp==1.5.1 jupyterydoc==0.2.5 jupyter_client==7.4.9 jupyter_core==5.7.2 jupyter_server==2.14.1 jupyter_server_fileid==0.9.2 jupyter_server_terminals==0.5.3 jupyter_server_ydoc==0.8.0 jupyterlab==3.6.7 jupyterlab_pygments==0.3.0 jupyterlab_server==2.27.2 kagglehub==0.2.7 keras==3.8.0 kerascore==0.1.7 kerascv==0.9.0 kerashub==0.18.1 kerasnlp==0.18.1 kiwisolver==1.4.5 lazy_loader==0.4 libclang==18.1.1 librosa==0.10.2.post1 libtpunightly==0.1.dev20231213 llvmlite==0.43.0 Markdown==3.6 markdownitpy==3.0.0 MarkupSafe==2.1.5 matplotlib==3.9.1 matplotlibinline==0.1.7 mccabe==0.7.0 mdurl==0.1.2 mistune==3.0.2 mldtypes==0.4.1 mpmath==1.3.0 msgpack==1.0.8 namex==0.0.8 nbclassic==1.1.0 nbclient==0.10.0 nbconvert==7.16.4 nbformat==5.10.4 nestasyncio==1.6.0 networkx==3.3 notebook==6.5.7 notebook_shim==0.2.4 numba==0.60.0 numpy==1.26.4 nvidiacublascu12==12.1.3.1 nvidiacudacupticu12==12.1.105 nvidiacudanvrtccu12==12.1.105 nvidiacudaruntimecu12==12.1.105 nvidiacudnncu12==8.9.2.26 nvidiacufftcu12==11.0.2.54 nvidiacurandcu12==10.3.2.106 nvidiacusolvercu12==11.4.5.107 nvidiacusparsecu12==12.1.0.106 nvidiancclcu12==2.20.5 nvidianvjitlinkcu12==12.5.82 nvidianvtxcu12==12.1.105 oauth2client==4.1.3 opencvpython==4.10.0.84 opencvpythonheadless==4.10.0.84 opteinsum==3.3.0 optax==0.2.2 optree==0.12.1 orbaxcheckpoint==0.5.16 overrides==7.7.0 packaging==24.1 pandas==2.2.2 pandocfilters==1.5.1 papermill==2.6.0 parso==0.8.4 pexpect==4.9.0 pillow==10.4.0 platformdirs==4.2.2 pluggy==1.5.0 pooch==1.8.2 prometheus_client==0.20.0 promise==2.3 prompt_toolkit==3.0.47 protobuf==3.20.3 psutil==6.0.0 ptyprocess==0.7.0 pureeval==0.2.2 pyarrow==16.1.0 pyasn1==0.6.0 pyasn1_modules==0.4.0 pycodestyle==2.11.1 pycparser==2.22 pydantic==2.8.2 pydantic_core==2.20.1 pydocstyle==6.3.0 pyflakes==3.2.0 Pygments==2.18.0 pylint==3.2.5 pyparsing==3.1.2 pythondateutil==2.9.0.post0 pythonjsonlogger==2.0.7 pythonlspjsonrpc==1.1.2 pythonlspserver==1.11.0 pytoolconfig==1.3.1 pytz==2024.1 PyYAML==6.0.1 pyzmq==26.0.3 referencing==0.35.1 regex==2024.5.15 requests==2.32.3 rfc3339validator==0.1.4 rfc3986validator==0.1.1 rich==13.7.1 rope==1.13.0 rpdspy==0.19.0 rsa==4.9 safetensors==0.4.3 scikitimage==0.24.0 scikitlearn==1.5.1 scipy==1.14.0 seaborn==0.13.2 Send2Trash==1.8.3 simple_parsing==0.1.5 six==1.16.0 sniffio==1.3.1 snowballstemmer==2.2.0 soundfile==0.12.1 soupsieve==2.5 soxr==0.3.7 stackdata==0.6.3 sympy==1.13.0 tabulate==0.9.0 tenacity==8.5.0 tensorboard==2.18.0 tensorboarddataserver==0.7.2 tensorflow==2.18.0 tensorflowdatasets==4.9.6 tensorflowhub==0.16.1 tensorflowio==0.37.1 tensorflowiogcsfilesystem==0.37.1 tensorflowmetadata==1.15.0 tensorflowprobability==0.24.0 tensorflowtext==2.18.1 tensorflow_cpu==2.18.0 tensorstore==0.1.63 termcolor==2.4.0 terminado==0.18.1 tf_keras==2.16.0 threadpoolctl==3.5.0 tifffile==2024.7.2 timm==1.0.7 tinycss2==1.3.0 tokenizers==0.19.1 toml==0.10.2 tomli==2.0.1 tomlkit==0.12.5 toolz==0.12.1 torch==2.3.0 torchxla @ https://storage.googleapis.com/pytorchxlareleases/wheels/tpuvm/torch_xla2.3.0+libtpucp310cp310manylinux_2_28_x86_64.whl torchaudio==2.3.0 torchtext==0.18.0 torchvision==0.18.0 tornado==6.4.1 tqdm==4.66.4 traitlets==5.14.3 transformers==4.42.3 trax==1.4.1 triton==2.3.0 typespythondateutil==2.9.0.20240316 typing_extensions==4.12.2 tzdata==2024.1 ujson==5.10.0 uritemplate==1.3.0 uritemplate==3.0.1 urllib3==2.2.2 wcwidth==0.2.13 webcolors==24.6.0 webencodings==0.5.1 websocketclient==1.8.0 Werkzeug==3.0.3 whatthepatch==1.0.5 wrapt==1.16.0 ypy==0.6.2 yapf==0.40.2 ypywebsocket==0.8.4 zipp==3.19.2 ```  System info (python version, jaxlib version, accelerator, etc.) **Versions** Python 3.10.14 jax==0.4.23 jaxlib==0.4.23 **Accelerator** TPU v5e8 Note, this code runs without error on TPU v3. **import jax; jax.print_environment_info()** ``` WARNING: Logging before InitGoogle() is written to STDERR E0000 00:00:1736888216.114197      13 common_lib.cc:798] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=""local"" === Source Location Trace: === learning/45eac/tfrc/runtime/common_lib.cc:479 E0114 20:56:56.145308531      13 oauth2_credentials.cc:238]            oauth_fetch: UNKNOWN:Cares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {grpc_status:2, created_time:""20250114T20:56:56.145293251+00:00""} jax:    0.4.23 jaxlib: 0.4.23 numpy:  1.26.4 python: 3.10.14 (main, Jul  3 2024, 00:10:44) [GCC 12.2.0] jax.devices (8 total, 8 local): [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0) TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0) ... TpuDevice(id=6, process_index=0, coords=(0,3,0), core_on_chip=0) TpuDevice(id=7, process_index=0, coords=(1,3,0), core_on_chip=0)] process_count: 1 ```",2025-01-14T20:58:32Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/25882,"Can you try with the latest jax version? 0.4.23 is from 2023. Also what TPU image version are you using? According to https://cloud.google.com/tpu/docs/runtimes, you should use `v2alphatpuv5lite`. Using the wrong image can sometimes cause weird problems.","Thank you! I updated to 0.4.38 with `pip install U ""jax[tpu]"" f https://storage.googleapis.com/jaxreleases/libtpu_releases.html` and this was fixed!"
yi,Different results in bf16/f32 mixed precision in `conv_general_dilated` in jax2tf," Description Hi Sirs.   I noticed Flax's Conv module was giving me different results between Python and C++ via TensorFlow Serving, which I could track and isolate to the following example. The problem is the most acute when I use `bfloat16` for calculations (which happens to be the type I am using most). The code below calculates a simple convolution in 5 different ways, and only `jax2tf` with `native_serialization=True` stands out. It seems almost that using `native_serialization=True` actually does calculations in f32, because the results match to all other methods if I set `dtype` to `f32`.   I am not sure it is a bug in earnest, just trying to understand the discrepancy between research (Jax) and production (TF Serving). In reallife fullsized model the discrepancy is quite significant because it accumulates between layers. It seems to be somehow related to CC(jax2tf: support mixedprecision inputs to `dot_general`) and CC([jax2tf] Fix for dot_general with different dtypes for graph serialization). ``` import jax from jax import lax, random as jrnd, numpy as jnp from flax import linen as nn from jax.experimental import jax2tf dtype = jnp.bfloat16  dtype = jnp.float16   diff is smaller  dtype = jnp.float32   all methods agree class Foo(nn.Module):     dtype: jnp.dtype = jnp.float32     .compact     def __call__(self, h):         residual = h         y = nn.Conv(2, kernel_size=(1,)                      , strides=1                      , padding=0                      , feature_group_count=2                      , use_bias=False                      , dtype=self.dtype)(h)          return y + residual layer = Foo(dtype=dtype) model_vars = {'params': {'Conv_0': {'kernel': jnp.asarray([[[0.25436434, 0.8453059 ]]])}}} x = jrnd.normal(jrnd.key(1729), (2,)) from jax.lax import ConvDimensionNumbers def jaxpr_fn(kernel, x):     kernel_bf16 = kernel.astype(dtype)     x_bf16 = x.astype(dtype)     dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 2, 1)                                            , rhs_spec=(2, 1, 0)                                            , out_spec=(0, 2, 1))     g = lax.conv_general_dilated(kernel_bf16, x_bf16                                             , batch_group_count=1                                             , dimension_numbers=dimension_numbers                                             , feature_group_count=2                                             , lhs_dilation=(1,)                                             , padding=((0, 0),)                                             , precision=None                                             , preferred_element_type=None                                             , rhs_dilation=(1,)                                             , window_strides=(1,))     return g + x y_flax = layer.apply(model_vars, x.reshape(1, 1, 1)) print(y_flax.ravel())  prints [1.3705847 2.27708  ] y_native = jax2tf.convert(layer.apply                           , native_serialization=True)(model_vars                                                        , x.reshape(1, 1, 1)) y_native = jnp.asarray(y_native, dtype=jnp.float32) print(y_native.ravel())  prints [1.3713225 2.2845213], which is the same as other methods give with f32 y_nonnative = jax2tf.convert(layer.apply                              , native_serialization=False)(model_vars, x.reshape(1, 1, 1)) y_nonnative = jnp.asarray(y_nonnative, dtype=jnp.float32) print(y_nonnative.ravel())  prints [1.3705847 2.27708  ] y_direct = (model_vars['params']['Conv_0']['kernel'][0][0].astype(dtype) * x.astype(dtype)).astype(jnp.float32) + x print(y_direct.ravel())  prints [1.3705847 2.27708  ] y_jaxpr = jaxpr_fn(model_vars['params']['Conv_0']['kernel'], x.reshape(1, 1, 1)) print(y_jaxpr.ravel())  prints [1.3705847 2.27708  ] diff = jnp.max(jnp.abs(y_nonnative  y_native)) print(diff)  prints 0.0074412823 ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.38 jaxlib: 0.4.38 numpy:  1.26.4 python: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] device info: cpu1, 1 local devices"" process_count: 1 platform: uname_result(system='Linux', node='2cf1ec796e3e', release='6.1.85+', version=' CC(Python 3 compatibility issues) SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024', machine='x86_64') ```",2025-01-14T12:10:19Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/25873
yi,WARNING: jax does not provide the extra 'rocm'," Description I'm trying to install jax with ROCm support on bare metal. The instructions in the README (https://github.com/jaxml/jax?tab=readmeovfileinstructions) point to AMD's instructions which say to do: ``` pip3 install jax[rocm] ``` However, when I do that, I get: ``` WARNING: jax 0.4.31 does not provide the extra 'rocm' ``` And indeed, when I list the devices after installation, the GPUs do not show up: ``` >>> import jax >>> jax.devices() [CpuDevice(id=0)] ``` So either that ""rocm"" target is missing or the instructions on the AMD website are wrong. If it's the second case, that I need to open this bug report there. I tried forcing the installation of the jaxlib I found on https://repo.radeon.com/rocm/manylinux/rocmrel6.3.1/, but that didn't have any effect.  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.31 jaxlib: 0.4.31 numpy:  2.2.1 python: 3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='rapp101', release='6.8.051generic', version=' CC(Scenarios to prefer over cupy)Ubuntu SMP PREEMPT_DYNAMIC Thu Dec  5 13:09:44 UTC 2024', machine='x86_64') ``` Since this of course doesn't show and ROCM related info. Here's the ROCM part:  2x AMD MI210 (gfx90a)  ROCM 6.3.1",2025-01-13T15:50:13Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/25856,"Hi   the `rocm` extra was added after the most recent release, and will be part of JAX v0.5.0 which will likely be released later this week. Until then you could try running this instead: ```python pip install jax==0.4.35 jaxrocm60plugin==0.4.35 ``` (pinned to 0.4.35 because that's currently the only plugin version available on PyPI)","Oooh, sneaky! Yes, that worked. Thanks!"
yi,ptxas unsupported version error," Description Minimal example: ```python from jax.random import PRNGKey PRNGKey(0) ``` yields the error ```   File ""/mnt/xfs/home/engstrom/conda_envs/benclip/lib/python3.11/sitepackages/jax/_src/random.py"", line 246, in PRNGKey     return _return_prng_keys(True, _key('PRNGKey', seed, impl))                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/mnt/xfs/home/engstrom/conda_envs/benclip/lib/python3.11/sitepackages/jax/_src/random.py"", line 198, in _key     return prng.random_seed(seed, impl=impl)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/mnt/xfs/home/engstrom/conda_envs/benclip/lib/python3.11/sitepackages/jax/_src/prng.py"", line 541, in random_seed     return random_seed_p.bind(seeds_arr, impl=impl)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/mnt/xfs/home/engstrom/conda_envs/benclip/lib/python3.11/sitepackages/jax/_src/core.py"", line 463, in bind     return self.bind_with_trace(prev_trace, args, params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/mnt/xfs/home/engstrom/conda_envs/benclip/lib/python3.11/sitepackages/jax/_src/core.py"", line 468, in bind_with_trace     return trace.process_primitive(self, args, params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/mnt/xfs/home/engstrom/conda_envs/benclip/lib/python3.11/sitepackages/jax/_src/core.py"", line 941, in process_primitive     return primitive.impl(*args, **params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/mnt/xfs/home/engstrom/conda_envs/benclip/lib/python3.11/sitepackages/jax/_src/prng.py"", line 553, in random_seed_impl     base_arr = random_seed_impl_base(seeds, impl=impl)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/mnt/xfs/home/engstrom/conda_envs/benclip/lib/python3.11/sitepackages/jax/_src/prng.py"", line 558, in random_seed_impl_base     return seed(seeds)            ^^^^^^^^^^^   File ""/mnt/xfs/home/engstrom/conda_envs/benclip/lib/python3.11/sitepackages/jax/_src/prng.py"", line 774, in threefry_seed     return _threefry_seed(seed)            ^^^^^^^^^^^^^^^^^^^^ jaxlib.xla_extension.XlaRuntimeError: INTERNAL: ptxas exited with nonzero error code 65280, output: ptxas /tmp/tempfiledeepchungus11.csail.mit.edue7d9f2edf16d3f1446572662b8fece3009c, line 5; fatal   : Unsupported .version 8.3; current version is '8.2' ptxas fatal   : Ptx assembly aborted due to errors  For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.38 jaxlib: 0.4.38 numpy:  2.0.2 python: 3.11.8  ```",2025-01-13T05:46:14Z,bug,closed,0,10,https://github.com/jax-ml/jax/issues/25853,xref https://github.com/jaxml/jax/issues/25344,"From https://github.com/jaxml/jax/issues/25718: adding the python packaged ptxas to the PATH was an effective workaround.. ```python export PATH=$(python c ""import site; print(site.getsitepackages()[0] + '/nvidia/cuda_nvcc/bin')""):$PATH ```","Thanks for the report and the update. This workaround _shouldn't_ be needed anymore, so let's look into what happened. Can you share all the steps that you used to install JAX? It might also be useful to know what the following outputs (before you change `PATH`): ```python import jax print(jax._src.lib.cuda_path) ```","The output I get for this is: ``` /mnt/xfs/home/engstrom/conda_envs/ffcv_2/lib/python3.12/sitepackages/nvidia/cuda_nvcc ``` I think that the problem is that I have `/usr/local/cuda12.2/bin` in my path, which points to outdated binaries? Removing this from my PATH fixes the problem.","Thanks! Yeah, but XLA _should_ search that `cuda_path/bin` first. Can you please let me know how you installed JAX (e.g. did you use conda or pip?)?","I used pip to install jax via `pip install ""jax[cuda12]""`!","Thanks. I am able to reproduce this issue, and I think I tracked down the place where XLA is ignoring JAX's CUDA path, but I'm not totally sure how to fix it. For now, I'm glad that you found a workaround, and I'll keep pushing on getting this fixed.",awesome happy to have helped!,This should be fixed in the next JAX release thanks to this change: https://github.com/openxla/xla/pull/21547,I'm going to close this because it seems to be fixed in my environment with v0.5.0. Please let us know if you run into it again with the newer versions. Thanks again for the report!
yi,Tracer escaping in `linalg.solve` with `ensure_compile_time_eval` as of jax 0.4.36," Description I am seeing unexpected jax tracer escape when using `jax.linalg.solve` in the `jax.ensure_compile_time_eval` context manager. This seems to occur for jax >= 0.4.36. Below is a simple reproduction. ```python import jax import jax.numpy as jnp print(jax.__version__) def test_fn():     def solve_fn():         return jnp.linalg.solve(jnp.diag(jnp.ones(20)), jnp.ones((20, 1)))     with jax.ensure_compile_time_eval():         return solve_fn() test_fn() ``` This gives the following error: ``` UnexpectedTracerError: Encountered an unexpected tracer. A function transformed by JAX had a side effect, allowing for a reference to an intermediate value with type float32[20,20] wrapped in a DynamicJaxprTracer to escape the scope of the transformation. JAX transformations require that functions explicitly return their outputs, and disallow saving intermediate values to global state. The function being traced when the value leaked was solve at /usr/local/lib/python3.10/distpackages/jax/_src/numpy/linalg.py:1297 traced for jit.  The leaked intermediate value was created on line :8 (solve_fn).   When the value was created, the final 5 stack frames (most recent last) excluding JAXinternal frames were:  /usr/local/lib/python3.10/distpackages/IPython/core/interactiveshell.py:3473 (run_ast_nodes) /usr/local/lib/python3.10/distpackages/IPython/core/interactiveshell.py:3553 (run_code) :14 () :11 (test_fn) :8 (solve_fn)  To catch the leak earlier, try setting the environment variable JAX_CHECK_TRACER_LEAKS or using the `jax.checking_leaks` context manager. See https://jax.readthedocs.io/en/latest/errors.htmljax.errors.UnexpectedTracerError ``` I tried using the `jax.checking_leaks` context manager but it does not yield any additional info.  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.36 jaxlib: 0.4.36 numpy:  1.26.4 python: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] device info: cpu1, 1 local devices"" process_count: 1 platform: uname_result(system='Linux', node='b912f92c1534', release='6.1.85+', version=' CC(Python 3 compatibility issues) SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024', machine='x86_64') ```",2025-01-11T17:34:21Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/25847,"Given the error and the version number, I'm sure this has something to do with the ""stackless"" change described as the first item in the 0.4.36 changelog: https://jax.readthedocs.io/en/latest/changelog.htmljax0436dec52024 I wonder if  has any suggestions here?"
llm,Got an error when offloading SplashAttention pallas call," Description Reproducer: ```python3 import jax from jax.ad_checkpoint import Offloadable, Recompute from jax.experimental.pallas import pallas_call_p import jax.numpy as jnp from jax.experimental.pallas.ops.tpu.splash_attention import (     splash_attention_kernel,     splash_attention_mask, ) .jit def tpu_attn(query, key, value):     num_heads = query.shape[1]     mask_shape = (query.shape[2], key.shape[2])     mask = splash_attention_mask.FullMask(mask_shape)     kernel = splash_attention_kernel.make_splash_mha(         mask=splash_attention_mask.MultiHeadMask(masks=[mask] * num_heads),         block_sizes=None,         head_shards=1,         q_seq_shards=1,     )     kernel = jax.vmap(kernel)     context = kernel(q=query, k=key, v=value)     return context.sum() def policy(prim, *_, **__):     if prim is pallas_call_p:         return Offloadable(""device"", ""pinned_host"")     return Recompute q = jnp.ones((1, 1, 128, 128)) k = jnp.ones((1, 1, 128, 128)) v = jnp.ones((1, 1, 128, 128)) fn = jax.grad(jax.remat(tpu_attn, policy=policy)) print(jax.jit(fn).lower(q, k, v).as_text(""hlo"")) ``` Error traceback: ``` Traceback (most recent call last):   File ""/root/splash_test.py"", line 38, in      print(jax.jit(fn).lower(q, k, v).as_text(""hlo""))   File ""/opt/venv/lib/python3.10/sitepackages/jax/_src/traceback_util.py"", line 180, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/opt/venv/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 485, in lower     return trace(*args, **kwargs).lower()   File ""/opt/venv/lib/python3.10/sitepackages/jax/_src/stages.py"", line 775, in lower     lowering = new_callable()   File ""/opt/venv/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 1624, in _resolve_and_lower     return _pjit_lower(   File ""/opt/venv/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 1789, in _pjit_lower     return pxla.lower_sharding_computation(   File ""/opt/venv/lib/python3.10/sitepackages/jax/_src/profiler.py"", line 333, in wrapper     return func(*args, **kwargs)   File ""/opt/venv/lib/python3.10/sitepackages/jax/_src/interpreters/pxla.py"", line 2323, in lower_sharding_computation     nreps, tuple_args, shape_poly_state) = _cached_lowering_to_hlo(   File ""/opt/venv/lib/python3.10/sitepackages/jax/_src/interpreters/pxla.py"", line 1953, in _cached_lowering_to_hlo     lowering_result = mlir.lower_jaxpr_to_module(   File ""/opt/venv/lib/python3.10/sitepackages/jax/_src/interpreters/mlir.py"", line 1194, in lower_jaxpr_to_module     lower_jaxpr_to_fun(   File ""/opt/venv/lib/python3.10/sitepackages/jax/_src/interpreters/mlir.py"", line 1678, in lower_jaxpr_to_fun     out_vals, tokens_out = jaxpr_subcomp(   File ""/opt/venv/lib/python3.10/sitepackages/jax/_src/interpreters/mlir.py"", line 1950, in jaxpr_subcomp     ans = lower_per_platform(rule_ctx, str(eqn.primitive),   File ""/opt/venv/lib/python3.10/sitepackages/jax/_src/interpreters/mlir.py"", line 2068, in lower_per_platform     output = kept_rules0   File ""/opt/venv/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 1944, in _pjit_lowering     func = _pjit_cached_lower_jaxpr_to_fun(   File ""/opt/venv/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 1927, in _pjit_cached_lower_jaxpr_to_fun     func = mlir.lower_jaxpr_to_fun(   File ""/opt/venv/lib/python3.10/sitepackages/jax/_src/interpreters/mlir.py"", line 1678, in lower_jaxpr_to_fun     out_vals, tokens_out = jaxpr_subcomp(   File ""/opt/venv/lib/python3.10/sitepackages/jax/_src/interpreters/mlir.py"", line 1950, in jaxpr_subcomp     ans = lower_per_platform(rule_ctx, str(eqn.primitive),   File ""/opt/venv/lib/python3.10/sitepackages/jax/_src/interpreters/mlir.py"", line 2068, in lower_per_platform     output = kept_rules0   File ""/opt/venv/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 1944, in _pjit_lowering     func = _pjit_cached_lower_jaxpr_to_fun(   File ""/opt/venv/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 1927, in _pjit_cached_lower_jaxpr_to_fun     func = mlir.lower_jaxpr_to_fun(   File ""/opt/venv/lib/python3.10/sitepackages/jax/_src/interpreters/mlir.py"", line 1678, in lower_jaxpr_to_fun     out_vals, tokens_out = jaxpr_subcomp(   File ""/opt/venv/lib/python3.10/sitepackages/jax/_src/interpreters/mlir.py"", line 1950, in jaxpr_subcomp     ans = lower_per_platform(rule_ctx, str(eqn.primitive),   File ""/opt/venv/lib/python3.10/sitepackages/jax/_src/interpreters/mlir.py"", line 2068, in lower_per_platform     output = kept_rules0   File ""/opt/venv/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 584, in _tpu_gpu_device_put_lowering     return list(map(lower, xs, devices, ctx.avals_in, ctx.avals_out)) ValueError: safe_map() argument 2 is shorter than argument 1 ```  System info (python version, jaxlib version, accelerator, etc.) Tested on v5p ``` jax                            0.4.39.dev20250110 jaxlib                         0.4.39.dev20250110 libtpu                         0.0.8.dev20250110+nightly ```",2025-01-10T19:36:33Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/25841
rag,Adding GPU paged attention kernel,Pallas kernel for paged ragged flash attention on GPU. It partitions the computation across the KV sequence similar to flash decoding.,2025-01-10T18:20:38Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/25839
yi,build error  error: unknown type name 'svuint8_t'," Description ``` INFO: Analyzed target //jaxlib/tools:build_wheel (259 packages loaded, 25906 targets configured). INFO: Found 1 target... ERROR: /root/.cache/bazel/_bazel_root/5ec7b40f6fc30761e523f0c22847f24b/external/compute_library/BUILD.bazel:355:11: Compiling src/core/NEON/kernels/NEBatchNormalizationLayerKernel.cpp failed: (Exit 1): clang12 failed: error executing command (from target //:arm_compute)    (cd /root/.cache/bazel/_bazel_root/5ec7b40f6fc30761e523f0c22847f24b/execroot/__main__ && \   exec env  \     CLANG_COMPILER_PATH=/usr/bin/clang12 \     LD_LIBRARY_PATH=/usr/local/Ascend/ascendtoolkit/latest/tools/aml/lib64:/usr/local/Ascend/ascendtoolkit/latest/tools/aml/lib64/plugin:/usr/local/Ascend/ascendtoolkit/latest/lib64:/usr/local/Ascend/ascendtoolkit/latest/lib64/plugin/opskernel:/usr/local/Ascend/ascendtoolkit/latest/lib64/plugin/nnengine:/usr/local/Ascend/ascendtoolkit/latest/opp/builtin/op_impl/ai_core/tbe/op_tiling/lib/linux/aarch64:/var/davinci/driver/lib64:/var/davinci/driver/lib64/common:/var/davinci/driver/lib64/driver:/usr/local/Ascend/ascendtoolkit/latest/tools/aml/lib64:/usr/local/Ascend/ascendtoolkit/latest/tools/aml/lib64/plugin:/usr/local/Ascend/ascendtoolkit/latest/lib64:/usr/local/Ascend/ascendtoolkit/latest/lib64/plugin/opskernel:/usr/local/Ascend/ascendtoolkit/latest/lib64/plugin/nnengine:/usr/local/Ascend/ascendtoolkit/latest/opp/builtin/op_impl/ai_core/tbe/op_tiling/lib/linux/aarch64:/var/davinci/driver/lib64:/var/davinci/driver/lib64/common:/var/davinci/driver/lib64/driver:/usr/local/Ascend/ascendtoolkit/latest/tools/aml/lib64:/usr/local/Ascend/ascendtoolkit/latest/tools/aml/lib64/plugin:/usr/local/Ascend/ascendtoolkit/latest/lib64:/usr/local/Ascend/ascendtoolkit/latest/lib64/plugin/opskernel:/usr/local/Ascend/ascendtoolkit/latest/lib64/plugin/nnengine:/usr/local/Ascend/ascendtoolkit/latest/opp/builtin/op_impl/ai_core/tbe/op_tiling/lib/linux/aarch64:/var/davinci/driver/lib64:/var/davinci/driver/lib64/common:/var/davinci/driver/lib64/driver:/home/HwHiAiUser/Ascend/ascendtoolkit/latest/tools/aml/lib64:/home/HwHiAiUser/Ascend/ascendtoolkit/latest/tools/aml/lib64/plugin:/home/HwHiAiUser/Ascend/ascendtoolkit/latest/lib64:/home/HwHiAiUser/Ascend/ascendtoolkit/latest/lib64/plugin/opskernel:/home/HwHiAiUser/Ascend/ascendtoolkit/latest/lib64/plugin/nnengine:/home/HwHiAiUser/Ascend/ascendtoolkit/latest/opp/builtin/op_impl/ai_core/tbe/op_tiling/lib/linux/aarch64:/var/davinci/driver/lib64:/var/davinci/driver/lib64/common:/var/davinci/driver/lib64/driver: \     PATH=/root/miniconda3/envs/jaxenv/bin:/root/miniconda3/condabin:/root/anaconda3/bin:/usr/local/work/cmake3.28.3linuxaarch64/bin:/usr/local/Ascend/ascendtoolkit/latest/bin:/usr/local/Ascend/ascendtoolkit/latest/compiler/ccec_compiler/bin:/usr/local/Ascend/ascendtoolkit/latest/tools/ccec_compiler/bin:/usr/local/work/cmake3.28.3linuxaarch64/bin:/usr/local/Ascend/ascendtoolkit/latest/bin:/usr/local/Ascend/ascendtoolkit/latest/compiler/ccec_compiler/bin:/usr/local/Ascend/ascendtoolkit/latest/tools/ccec_compiler/bin:/usr/local/work/cmake3.28.3linuxaarch64/bin:/usr/local/Ascend/ascendtoolkit/latest/bin:/usr/local/Ascend/ascendtoolkit/latest/compiler/ccec_compiler/bin:/usr/local/Ascend/ascendtoolkit/latest/tools/ccec_compiler/bin:/home/HwHiAiUser/work/cmake3.28.3linuxaarch64/bin:/home/HwHiAiUser/Ascend/ascendtoolkit/latest/bin:/home/HwHiAiUser/Ascend/ascendtoolkit/latest/compiler/ccec_compiler/bin:/home/HwHiAiUser/Ascend/ascendtoolkit/latest/tools/ccec_compiler/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin \     PWD=/proc/self/cwd \   /usr/bin/clang12 U_FORTIFY_SOURCE fstackprotector Wall Wthreadsafety Wselfassign Wnofreenonheapobject fcolordiagnostics fnoomitframepointer g0 O2 'D_FORTIFY_SOURCE=1' DNDEBUG ffunctionsections fdatasections 'std=c++14' MD MF bazelout/aarch64opt/bin/external/compute_library/_objs/arm_compute/NEBatchNormalizationLayerKernel.pic.d 'frandomseed=bazelout/aarch64opt/bin/external/compute_library/_objs/arm_compute/NEBatchNormalizationLayerKernel.pic.o' fPIC DENABLE_NEON DARM_COMPUTE_CPU_ENABLED DARM_COMPUTE_ENABLE_NEON DARM_COMPUTE_ENABLE_I8MM DENABLE_FP32_KERNELS DENABLE_QASYMM8_KERNELS DENABLE_QASYMM8_SIGNED_KERNELS DENABLE_QSYMM16_KERNELS DENABLE_INTEGER_KERNELS DENABLE_NHWC_KERNELS DENABLE_NCHW_KERNELS DARM_COMPUTE_GRAPH_ENABLED DARM_COMPUTE_ENABLE_SVEF32MM DARM_COMPUTE_ENABLE_FIXED_FORMAT_KERNELS D_GLIBCXX_USE_NANOSLEEP DENABLE_SVE DARM_COMPUTE_ENABLE_SVE DARM_COMPUTE_ENABLE_BF16 'DBAZEL_CURRENT_REPOSITORY=""compute_library""' iquote external/compute_library iquote bazelout/aarch64opt/bin/external/compute_library Ibazelout/aarch64opt/bin/external/compute_library/include/_virtual_includes/include isystem external/compute_library/arm_compute/runtime isystem bazelout/aarch64opt/bin/external/compute_library/arm_compute/runtime isystem external/compute_library/src/core/NEON/kernels/assembly isystem bazelout/aarch64opt/bin/external/compute_library/src/core/NEON/kernels/assembly isystem external/compute_library/src/core/NEON/kernels/convolution/common isystem bazelout/aarch64opt/bin/external/compute_library/src/core/NEON/kernels/convolution/common isystem external/compute_library/src/core/NEON/kernels/convolution/winograd isystem bazelout/aarch64opt/bin/external/compute_library/src/core/NEON/kernels/convolution/winograd isystem external/compute_library/src/core/cpu/kernels/assembly isystem bazelout/aarch64opt/bin/external/compute_library/src/core/cpu/kernels/assembly isystem external/compute_library/src/cpu/kernels/assembly isystem bazelout/aarch64opt/bin/external/compute_library/src/cpu/kernels/assembly isystem external/compute_library/src/core/NEON/kernels/arm_conv isystem bazelout/aarch64opt/bin/external/compute_library/src/core/NEON/kernels/arm_conv isystem external/compute_library/src/core/NEON/kernels/arm_gemm isystem bazelout/aarch64opt/bin/external/compute_library/src/core/NEON/kernels/arm_gemm 'fvisibility=hidden' Wnosigncompare Wnounknownwarningoption Wnostringoptruncation Wnoarrayparameter 'DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir.' 'std=c++17' 'march=armv8a' O3 nocanonicalprefixes Wnobuiltinmacroredefined 'D__DATE__=""redacted""' 'D__TIMESTAMP__=""redacted""' 'D__TIME__=""redacted""' c external/compute_library/src/core/NEON/kernels/NEBatchNormalizationLayerKernel.cpp o bazelout/aarch64opt/bin/external/compute_library/_objs/arm_compute/NEBatchNormalizationLayerKernel.pic.o)  Configuration: aa3cae5bee02b91b5cd6c24fef447352f0e1cacb60c08e47db9131070d0f0e67  Execution platform: //:platform In file included from external/compute_library/src/core/NEON/kernels/NEBatchNormalizationLayerKernel.cpp:37: In file included from external/compute_library/src/core/NEON/kernels/detail/NEActivationFunctionDetail.h:27: In file included from external/compute_library/src/core/NEON/wrapper/wrapper.h:28: In file included from external/compute_library/src/core/NEON/wrapper/traits.h:30: /usr/lib64/clang/12.0.1/include/arm_sve.h:15:2: error: ""SVE support not enabled"" error ""SVE support not enabled""  ^ In file included from external/compute_library/src/core/NEON/kernels/NEBatchNormalizationLayerKernel.cpp:37: In file included from external/compute_library/src/core/NEON/kernels/detail/NEActivationFunctionDetail.h:27: In file included from external/compute_library/src/core/NEON/wrapper/wrapper.h:28: external/compute_library/src/core/NEON/wrapper/traits.h:123:83: error: unknown type name 'svuint8_t' template  struct sve_vector{ using scalar_type = uint8_t; using type = svuint8_t; };                                                                                   ^ external/compute_library/src/core/NEON/wrapper/traits.h:124:81: error: unknown type name 'svint8_t' template  struct sve_vector{ using scalar_type = int8_t; using type = svint8_t; };                                                                                 ^ 3 errors generated. Target //jaxlib/tools:build_wheel failed to build INFO: Elapsed time: 92.839s, Critical Path: 31.99s INFO: 5 processes: 5 internal. FAILED: Build did NOT complete successfully ERROR: Build failed. Not running target 20250109 13:47:57,341  DEBUG  Command finished with return code 1 Traceback (most recent call last):   File ""/root/repos/jax/build/build.py"", line 654, in      asyncio.run(main())   File ""/root/miniconda3/envs/jaxenv/lib/python3.10/asyncio/runners.py"", line 44, in run     return loop.run_until_complete(main)   File ""/root/miniconda3/envs/jaxenv/lib/python3.10/asyncio/base_events.py"", line 649, in run_until_complete     return future.result()   File ""/root/repos/jax/build/build.py"", line 647, in main     raise RuntimeError(f""Command failed with return code {result.return_code}"") RuntimeError: Command failed with return code 1 (jaxenv) sh5.1  ```  System info (python version, jaxlib version, accelerator, etc.) Python3.10.x ARM64",2025-01-09T12:16:43Z,bug,open,0,3,https://github.com/jax-ml/jax/issues/25799,"Hi  thanks for including the traceback! It looks like this error is not directly releated to JAX, but rather is coming from the ARM compute library: https://github.com/ARMsoftware/ComputeLibrary/blob/32bcced2af7feea6969dd1d22e58d0718dc488e3/src/core/NEON/wrapper/traits.hL133 You might try filing an issue there, or looking for other help forums associated with those tools.",It may be that your compiler does not support SVE instructions. What compiler version are you using?,"Clang 12 is almost certainly too old to build JAX, I note. You probably want clang 17 or 18 at this point."
rag,"Add a JVP rule for lax.linalg.tridiagonal_solve, fixing some bugs along the way","As noted in CC(Differentiation rule for tridiagonal_solve), `lax.linalg.tridiagonal_solve` couldn't be used with AD because it didn't have a JVP rule, so this PR adds that. While implementing this, I noticed and fixed some other `tridiagonal_solve` bugs: 1. The transpose rule for `tridiagonal_solve` was wrong (it should be `A^T @ ct`, not `A^1 @ ct`). 2. The Thomas algorithm implementation of the solve that is used on CPU on TPU gave the wrong results if the endpoints `dl[0]` and `du[1]` weren't explicitly zeroed. I ended up just reimplementing this function from scratch because the existing version was obfuscated by the handling of batch dimension and (imo) some confusing notation. The new version is more compact and only uses 2 scans instead of 3 which should be slightly more efficient, and it no longer requires the user to explicitly zero out the endpoints. 3. Not quite a bug, but I also added a bit more test coverage.  Fixes CC(Differentiation rule for tridiagonal_solve) ",2025-01-08T20:57:04Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/25787,"> Just because we're looking at this, I thought I'd mention: there's probably an associative_scanlike way to implement these solves, like [Sec 1.4 here (https://www.cs.cmu.edu/~guyb/papers/Ble93.pdf), though there are probably more recent and more relevant papers about how to do it on GPUs in CUDA. Thanks! Yeah that's a good suggestion. I will note that on CPU and GPU we actually now lower to LAPACK and cuSPARSE respectively. These should have better numerics, but I must admit that I'm not 100% sure what algorithms are used there."
yi,Cache initialization fails when a JAX Array is created before enabling local cache," Description The persistent compilation cache in JAX fails to initialize if a JAX array is created prior to activating the local cache using jax.config.update. Removing the array creation line allows the cache to initialize correctly.  MRE with array allocation: ```python import jax import jax.numpy as jnp  This line causes the persistent cache to remain uninitialized. a = jnp.zeros(4)  Configuration for JAX persistent cache jax.config.update(""jax_logging_level"", ""DEBUG"") jax.config.update(""jax_compilation_cache_dir"", ""/tmp/jax_cache"") jax.config.update(""jax_persistent_cache_min_entry_size_bytes"", 1) jax.config.update(""jax_persistent_cache_min_compile_time_secs"", 0) jax.config.update(""jax_persistent_cache_enable_xla_caches"", ""all"") .jit def f(x):     return x + 1  Function invocation x = jnp.zeros((2, 2)) f(x) ``` Full Log  ```python DEBUG:20250108 11:37:25,064:jax._src.dispatch:182: Finished tracing + transforming broadcast_in_dim for pjit in 0.000183344 sec DEBUG:20250108 11:37:25,065:jax._src.interpreters.pxla:1906: Compiling broadcast_in_dim with global shapes and types [ShapedArray(float32[])]. Argument mapping: (UnspecifiedValue,). DEBUG:20250108 11:37:25,067:jax._src.dispatch:182: Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.001679897 sec DEBUG:20250108 11:37:25,067:jax._src.compiler:167: get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CudaDevice(id=0)]] DEBUG:20250108 11:37:25,067:jax._src.compiler:239: get_compile_options XLAAutoFDO profile: using XLAAutoFDO profile version 1 DEBUG:20250108 11:37:25,076:jax._src.compiler:260: Enabling XLA kernel cache at '/tmp/jax_cache/xla_gpu_kernel_cache_file' DEBUG:20250108 11:37:25,076:jax._src.compiler:265: Enabling XLA autotuning cache at '/tmp/jax_cache/xla_gpu_per_fusion_autotune_cache_dir' DEBUG:20250108 11:37:25,076:jax._src.cache_key:152: get_cache_key hash of serialized computation: 7c595daa617132810980e2a78d5722364377c78aa62385b323a42352c06c0986 DEBUG:20250108 11:37:25,076:jax._src.cache_key:158: get_cache_key hash after serializing computation: 7c595daa617132810980e2a78d5722364377c78aa62385b323a42352c06c0986 DEBUG:20250108 11:37:25,076:jax._src.cache_key:152: get_cache_key hash of serialized jax_lib version: c8601d1831072872293c1f9c58282e40273dd0289eaea98e369c2037dc4231ae DEBUG:20250108 11:37:25,076:jax._src.cache_key:158: get_cache_key hash after serializing jax_lib version: 71bccc6c9a13fd5342b8a6453530356df28fc9bef6b8de1f013f37112c95aa3f DEBUG:20250108 11:37:25,077:jax._src.cache_key:152: get_cache_key hash of serialized XLA flags: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 DEBUG:20250108 11:37:25,077:jax._src.cache_key:158: get_cache_key hash after serializing XLA flags: 71bccc6c9a13fd5342b8a6453530356df28fc9bef6b8de1f013f37112c95aa3f DEBUG:20250108 11:37:25,077:jax._src.cache_key:152: get_cache_key hash of serialized compile_options: 9a2bca9520f15d649eb148fe1c967023e6c1abcbe23b5c18508ffd37fc2caa42 DEBUG:20250108 11:37:25,077:jax._src.cache_key:158: get_cache_key hash after serializing compile_options: dd3b57e335deee8784863afece324e434917d6cc3af26116758dda1b5d012223 DEBUG:20250108 11:37:25,077:jax._src.cache_key:152: get_cache_key hash of serialized accelerator_config: b58a62c4527e3728c60e269461bd03852cb6f48a6708b25e0307cd74663f17e9 DEBUG:20250108 11:37:25,077:jax._src.cache_key:158: get_cache_key hash after serializing accelerator_config: aefcf17c78285d8750a4153653d9e8622c90110f6194cfb1588f53bcd1ccb53e DEBUG:20250108 11:37:25,077:jax._src.cache_key:152: get_cache_key hash of serialized compression: 0ea55c28f8014d8886b6248fe3da5d588f55c0823847a6b4579f1131b051b5e2 DEBUG:20250108 11:37:25,077:jax._src.cache_key:158: get_cache_key hash after serializing compression: d8606c1f0763d704c10857737957225e80346b1ea8e35098666c2fe6d93ff8cd DEBUG:20250108 11:37:25,077:jax._src.cache_key:152: get_cache_key hash of serialized custom_hook: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 DEBUG:20250108 11:37:25,077:jax._src.cache_key:158: get_cache_key hash after serializing custom_hook: d8606c1f0763d704c10857737957225e80346b1ea8e35098666c2fe6d93ff8cd DEBUG:20250108 11:37:25,077:jax._src.compilation_cache:215: get_executable_and_time: cache is disabled/not initialized DEBUG:20250108 11:37:25,077:jax._src.compiler:108: PERSISTENT COMPILATION CACHE MISS for 'jit_broadcast_in_dim' with key 'jit_broadcast_in_dimd8606c1f0763d704c10857737957225e80346b1ea8e35098666c2fe6d93ff8cd' DEBUG:20250108 11:37:25,085:jax._src.compiler:730: 'jit_broadcast_in_dim' took at least 0.00 seconds to compile (0.01s) DEBUG:20250108 11:37:25,085:jax._src.compilation_cache:245: Not writing persistent cache entry with key 'jit_broadcast_in_dimd8606c1f0763d704c10857737957225e80346b1ea8e35098666c2fe6d93ff8cd' since cache is disabled/not initialized DEBUG:20250108 11:37:25,085:jax._src.dispatch:182: Finished XLA compilation of jit(broadcast_in_dim) in 0.008747339 sec DEBUG:20250108 11:37:25,087:jax._src.dispatch:182: Finished tracing + transforming add for pjit in 0.000348568 sec DEBUG:20250108 11:37:25,087:jax._src.dispatch:182: Finished tracing + transforming f for pjit in 0.000899553 sec DEBUG:20250108 11:37:25,087:jax._src.interpreters.pxla:1906: Compiling f with global shapes and types [ShapedArray(float32[2,2])]. Argument mapping: (UnspecifiedValue,). DEBUG:20250108 11:37:25,089:jax._src.dispatch:182: Finished jaxpr to MLIR module conversion jit(f) in 0.001835823 sec DEBUG:20250108 11:37:25,089:jax._src.compiler:167: get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CudaDevice(id=0)]] DEBUG:20250108 11:37:25,089:jax._src.compiler:239: get_compile_options XLAAutoFDO profile: using XLAAutoFDO profile version 1 DEBUG:20250108 11:37:25,090:jax._src.compiler:260: Enabling XLA kernel cache at '/tmp/jax_cache/xla_gpu_kernel_cache_file' DEBUG:20250108 11:37:25,090:jax._src.compiler:265: Enabling XLA autotuning cache at '/tmp/jax_cache/xla_gpu_per_fusion_autotune_cache_dir' DEBUG:20250108 11:37:25,090:jax._src.cache_key:152: get_cache_key hash of serialized computation: 380ddb18f25ed5a1aca7f087de5f4a4d07f46c3e8f4d42ad21937e931b05da57 DEBUG:20250108 11:37:25,090:jax._src.cache_key:158: get_cache_key hash after serializing computation: 380ddb18f25ed5a1aca7f087de5f4a4d07f46c3e8f4d42ad21937e931b05da57 DEBUG:20250108 11:37:25,090:jax._src.cache_key:152: get_cache_key hash of serialized jax_lib version: c8601d1831072872293c1f9c58282e40273dd0289eaea98e369c2037dc4231ae DEBUG:20250108 11:37:25,090:jax._src.cache_key:158: get_cache_key hash after serializing jax_lib version: 9c38da7ed7dfd35462c73d36e5b770ea7bf5ad679c67ca4625afea9e628528cf DEBUG:20250108 11:37:25,090:jax._src.cache_key:152: get_cache_key hash of serialized XLA flags: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 DEBUG:20250108 11:37:25,090:jax._src.cache_key:158: get_cache_key hash after serializing XLA flags: 9c38da7ed7dfd35462c73d36e5b770ea7bf5ad679c67ca4625afea9e628528cf DEBUG:20250108 11:37:25,090:jax._src.cache_key:152: get_cache_key hash of serialized compile_options: 9a2bca9520f15d649eb148fe1c967023e6c1abcbe23b5c18508ffd37fc2caa42 DEBUG:20250108 11:37:25,090:jax._src.cache_key:158: get_cache_key hash after serializing compile_options: 04f7b5724db48e6c4554f1b80916cfd93d671f5944eeaf988b077f7002d2f4b4 DEBUG:20250108 11:37:25,090:jax._src.cache_key:152: get_cache_key hash of serialized accelerator_config: b58a62c4527e3728c60e269461bd03852cb6f48a6708b25e0307cd74663f17e9 DEBUG:20250108 11:37:25,090:jax._src.cache_key:158: get_cache_key hash after serializing accelerator_config: 7fadb1901a3564de3579b886a670a31028011157831263b0bd6a13aa7a64d9c9 DEBUG:20250108 11:37:25,090:jax._src.cache_key:152: get_cache_key hash of serialized compression: 0ea55c28f8014d8886b6248fe3da5d588f55c0823847a6b4579f1131b051b5e2 DEBUG:20250108 11:37:25,090:jax._src.cache_key:158: get_cache_key hash after serializing compression: 233097fb49ebef28937711d6252b53ea2f4caccf79727f493dab2ec62fb37bc9 DEBUG:20250108 11:37:25,090:jax._src.cache_key:152: get_cache_key hash of serialized custom_hook: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 DEBUG:20250108 11:37:25,090:jax._src.cache_key:158: get_cache_key hash after serializing custom_hook: 233097fb49ebef28937711d6252b53ea2f4caccf79727f493dab2ec62fb37bc9 DEBUG:20250108 11:37:25,090:jax._src.compilation_cache:215: get_executable_and_time: cache is disabled/not initialized DEBUG:20250108 11:37:25,090:jax._src.compiler:108: PERSISTENT COMPILATION CACHE MISS for 'jit_f' with key 'jit_f233097fb49ebef28937711d6252b53ea2f4caccf79727f493dab2ec62fb37bc9' DEBUG:20250108 11:37:25,096:jax._src.compiler:730: 'jit_f' took at least 0.00 seconds to compile (0.01s) DEBUG:20250108 11:37:25,096:jax._src.compilation_cache:245: Not writing persistent cache entry with key 'jit_f233097fb49ebef28937711d6252b53ea2f4caccf79727f493dab2ec62fb37bc9' since cache is disabled/not initialized DEBUG:20250108 11:37:25,096:jax._src.dispatch:182: Finished XLA compilation of jit(f) in 0.006633282 sec ```    This issue is present also with `ClassVar` default values if they are JAX NumPy arrays and with default arguments of functions. (See also https://github.com/amiiit/jaxsim/issues/322 and https://github.com/amiiit/jaxsim/pull/329)  MRE with `ClassVar`: ```python import jax import jax.numpy as jnp class MyClass:      This attribute causes the persistent cache to remain uninitialized.     default_array: ClassVar = jnp.zeros((3,))  Configuration for JAX persistent cache jax.config.update(""jax_logging_level"", ""DEBUG"") jax.config.update(""jax_compilation_cache_dir"", ""/tmp/jax_cache"") jax.config.update(""jax_persistent_cache_min_entry_size_bytes"", 1) jax.config.update(""jax_persistent_cache_min_compile_time_secs"", 0) jax.config.update(""jax_persistent_cache_enable_xla_caches"", ""all"") .jit def f(x):     return x + 1  Function invocation x = jnp.zeros((2, 2)) f(x) ```  MRE with default arguments: ```python import jax import jax.numpy as jnp  The `array` default value causes the persistent cache to remain uninitialized. def test_fn(array: jax.Array = jnp.zeros(3)):     return array  Configuration for JAX persistent cache jax.config.update(""jax_logging_level"", ""DEBUG"") jax.config.update(""jax_compilation_cache_dir"", ""/tmp/jax_cache"") jax.config.update(""jax_persistent_cache_min_entry_size_bytes"", 1) jax.config.update(""jax_persistent_cache_min_compile_time_secs"", 0) jax.config.update(""jax_persistent_cache_enable_xla_caches"", ""all"") .jit def f(x):     return x + 1  Function invocation x = jnp.zeros((2, 2)) f(x) ``` This was quite hard to spot for me, so I would expect a more clear error message if for some reason the cache cannot be initialized. Thank you for your help! FYI  95    System info (python version, jaxlib version, accelerator, etc.) ```yaml jax:    0.4.38 jaxlib: 0.4.38 numpy:  2.2.0 python: 3.13.1  ++ ``` pip list  ```yaml Package                  Version     Editable project location    abslpy                  2.1.0 asttokens                3.0.0 chex                     0.1.87 colorama                 0.4.6 coloredlogs              15.0.1 decorator                5.1.1 docstring_parser         0.16 etils                    1.11.0 exceptiongroup           1.2.2 executing                2.1.0 gitdb                    4.0.11 GitPython                3.1.43 humanfriendly            10.0 importlib_resources      6.5.2 iniconfig                2.0.0 ipython                  8.30.0 jax                      0.4.38 jaxcuda12pjrt          0.4.34 jaxcuda12plugin        0.4.34 jax_dataclasses          1.6.1 jaxlib                   0.4.38 jaxlie                   1.4.2 jaxsim                   0.5.1.dev69 /home/fferrettiiit.local/jaxsim jedi                     0.19.2 markdownitpy           3.0.0 mashumaro                3.15 matplotlibinline        0.1.7 mdurl                    0.1.2 ml_dtypes                0.5.0 numpy                    2.2.0 nvidiacublascu12       12.6.4.1 nvidiacudacupticu12   12.6.80 nvidiacudanvcccu12    12.6.85 nvidiacudaruntimecu12 12.6.77 nvidiacudnncu12        9.6.0.74 nvidiacufftcu12        11.3.0.4 nvidiacusolvercu12     11.7.1.2 nvidiacusparsecu12     12.5.4.2 nvidiancclcu12         2.23.4 nvidianvjitlinkcu12    12.6.85 opt_einsum               3.4.0 optax                    0.2.4 packaging                24.2 parso                    0.8.4 pexpect                  4.9.0 pip                      24.3.1 pluggy                   1.5.0 pptree                   3.1 prompt_toolkit           3.0.48 ptyprocess               0.7.0 pure_eval                0.2.3 pycpuinfo               9.0.0 Pygments                 2.18.0 pytest                   8.3.4 pytestbenchmark         5.1.0 qpax                     0.0.9 resolveroboticsuripy  0.3.0 rich                     13.9.4 robot_descriptions       1.13.0 rod                      0.3.4 scipy                    1.14.1 setuptools               75.6.0 shtab                    1.7.1 smmap                    5.0.1 stackdata               0.6.3 tomli                    2.2.1 toolz                    1.0.0 tqdm                     4.67.1 traitlets                5.14.3 trimesh                  4.5.3 typeguard                4.4.1 typing_extensions        4.12.2 tyro                     0.9.2 wcwidth                  0.2.13 xmltodict                0.14.2 ```   ",2025-01-08T11:09:23Z,bug,closed,3,3,https://github.com/jax-ml/jax/issues/25768,I think this is working as expected: the compilation cache state must be enabled before backend initialization. Many of the JAX configuration options have similar requirements. I'm assigning  who knows more about this code path and may be able to confirm.,Thanks for reporting this issue. We have an idea of how to address it and will work on a fix. ,This issue has been resolved with PR CC(Fix cache init when JAX Array is created early (25768)). Closing now. 
yi,`jax.lax.composite` and `jax.nn.softmax` composes strangely," Description I want to export a jax function that uses softmax, and have that softmax be in a composite region. Like this one:  ```python def f(x):   return jax.lax.composite(jax.nn.softmax, name=""softmax"")(x, 1) ``` However, `f` cannot be invoked, let alone exported ```python f(jnp.ones((5,5)) ``` or  ```python jax.jit(f).lower(jax.ShapedDtypeStruct((5,5), jnp.float32.dtype) ``` raises: ``` File ~/homebrew/Caskroom/miniconda/base/envs/xla2/lib/python3.10/sitepackages/jax/_src/core.py:1492, in concrete_or_error(force, val, context)    1490 maybe_concrete = val.to_concrete_value()    1491 if maybe_concrete is None: > 1492   raise ConcretizationTypeError(val, context)    1493 else:    1494   return force(maybe_concrete) ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: traced array with shape int32[] The axis argument must be known statically. The error occurred while tracing the function softmax at /Users/hanq/homebrew/Caskroom/miniconda/base/envs/xla2/lib/python3.10/sitepackages/jax/_src/nn/functions.py:550 for composite. This concrete value was not available in Python because it depends on the value of the argument axis. ``` Attempting to mark the `dim` arg as static with  ```python def f(x):   return jax.lax.composite(jax.jit(jax.nn.softmax, static_argnums=1),       name=""softmax"")(x, 1) ``` yields a different error: ``` File ~/homebrew/Caskroom/miniconda/base/envs/xla2/lib/python3.10/sitepackages/jax/_src/pjit.py:736, in _infer_params(fun, ji, args, kwargs)     733 dbg = debug_info('jit', ji.fun_sourceinfo, ji.fun_signature, args, kwargs,     734                  ji.static_argnums, ji.static_argnames)     735 avals = _infer_input_type(fun, dbg, dynargs) > 736 entry = _infer_params_cached(fun, ji, signature, avals, pjit_mesh, resource_env)     737 if entry.pjit_params is None:     738   p, args_flat = _infer_params_impl(     739       fun, ji, pjit_mesh, resource_env, args, kwargs, in_avals=avals) ValueError: Nonhashable static arguments are not supported. An error occurred while trying to hash an object of type , Tracedwith. The error was: TypeError: unhashable type: 'DynamicJaxprTracer' ```  System info (python version, jaxlib version, accelerator, etc.) In [13]: import jax; jax.print_environment_info() jax:    0.4.39.dev20250103 jaxlib: 0.4.39.dev20250103 numpy:  2.0.1 python: 3.10.14 (main, Mar 21 2024, 11:21:31) [Clang 14.0.6 ] device info: cpu1, 1 local devices"" process_count: 1 platform: uname_result(system='Darwin', node='hanqmacbookpro.roam.internal', release='24.2.0', version='Darwin Kernel Version 24.2.0: Fri Dec  6 19:01:59 PST 2024; root:xnu11215.61.5~2/RELEASE_ARM64_T6000', machine='arm64')",2025-01-08T05:36:08Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/25767,"Thanks for this report! There are two ways to get your example to work, depending on what behavior you want. 1. If you want your composite to take `axis` as an input, you need `axis` to be an ""attribute"" on your composite. This is indicated by using a keyword argument for `axis` as follows: ```python def f(x):  return jax.lax.composite(jax.nn.softmax, name=""softmax"")(x, axis=1) ``` 2. Alternatively, if you don't need to interpret the axis number on the backend, you could instead close over axis: ```python def f(x):  return jax.lax.composite(partial(jax.nn.softmax, axis=1), name=""softmax"")(x) ``` Hope this helps!",Also pinging  and  for visibility.,Solution 1. works great. Thanks ! To summarize: keyword arguments are interpreted as attributes and positional are inputs?,"> To summarize: keyword arguments are interpreted as attributes and positional are inputs? That's right, but this is a notable UX issue.. I'm not sure if there's a way to give a better error. An ideal solution would enforce the things that should be attributes must be specified as kwargs in python, i.e. ``` (lax.composite, name=""my.softmax"") def softmax(x, *, axis):   return jax.nn.softmax(x, axis) ```","You'd probably want `def softmax(x, /, *, axis):` to make `x` positiononly and `axis` keywordonly.","Thanks  and ! I'm also not sure about how we could raise a better error on the JAX side, but it's worth considering. I think it would be great to (at least!) add more details about this to the `lax.composite` docstring here: https://github.com/jaxml/jax/blob/5c097c8f62cc1233cb6ffa85ed4aefe310075706/jax/_src/lax/lax.pyL671L701",Thank you  for the feedback! I agree we should add documentation on this. I'll use jax.nn.softmax as an example in the doc. See https://github.com/jaxml/jax/pull/25792 for fix.
rag,XLA runtime error when taking grad + vmap + scan on GPU," Description I'm encountering a bug that only appears when using JAX on the GPU and when taking the gradient of a function that composes vmap and scan.  The code that I've pasted below seems to work if when the device, vmap or scan are changed to CPU mode or for loops. ```python import jax import jax.numpy as jnp import jax.random as random from jaxtyping import Array, Float def recreate_bug():   key = random.PRNGKey(0)   dim = 6   T = 3   batch_size = 2    Each (2, 2, dim//2) matrix is a 2x2 block matrix where each block is a dim//2 x dim//2 diagonal matrix   batched_matrix_elements = random.normal(key, (batch_size, T, 2, 2, dim//2))    Create a dummy global vector to initialize the scan with   global_x = random.normal(key, (dim,))   def objective(batched_matrix_elements: Float[Array, 'B T 2 2 dim//2']):     """"""Compute the average of the objective function over the batch dimension""""""     def unbatched_objective(matrix_elements: Float[Array, 'T 2 2 dim//2']):       """"""If matrix elements represent the elements of the 2x2 block matrices (A, B, C),          then this function computes ... C @ B @ A @ global_x""""""       def scan_body(x: Float[Array, 'dim'], matrix_elements: Float[Array, '2 2 dim//2']):         """"""This is just a matrix multiplication of the 2x2 block matrix with x""""""         x_reshaped = x.reshape((2, 1))         y = jnp.einsum('i j a, j a > i a', matrix_elements, x_reshaped).ravel()         return y, ()        Code works if we use a for loop instead of a scan!       xT, _ = jax.lax.scan(scan_body, global_x, matrix_elements)       return xT.mean()      Code works if we don't vmap!     return jax.vmap(unbatched_objective)(batched_matrix_elements).mean()   grad = jax.grad(objective)(batched_matrix_elements)   print('Worked!') if __name__ == '__main__':   jax.print_environment_info()   recreate_bug() ``` I've been able to recreate the bug on an a100 gpu (see the system info below) and also a 2080ti.  Any help would be greatly appreciated!  System info (python version, jaxlib version, accelerator, etc.) ```jax:    0.4.38 jaxlib: 0.4.38 numpy:  1.24.4 python: 3.10.15 (main, Oct  3 2024, 07:27:34) [GCC 11.2.0] device info: NVIDIA A100SXM480GB1, 1 local devices"" process_count: 1 platform: uname_result(system='Linux', node='gpu021', release='6.8.048generic', version=' CC(Unimplemented: binary integer op 'power')Ubuntu SMP PREEMPT_DYNAMIC Fri Sep 27 14:04:52 UTC 2024', machine='x86_64') $ nvidiasmi Tue Jan  7 21:00:10 2025        ++  ++",2025-01-07T21:08:56Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/25759,"I forgot to add the actual error that I get: ``` Traceback (most recent call last):   File ""/home/user/miniconda3/envs/env/lib/python3.10/runpy.py"", line 196, in _run_module_as_main     return _run_code(code, main_globals, None,   File ""/home/user/miniconda3/envs/env/lib/python3.10/runpy.py"", line 86, in _run_code     exec(code, run_globals)   File ""/home/user/project/jax_bug.py"", line 43, in      recreate_bug()   File ""/home/user/project/jax_bug.py"", line 38, in recreate_bug     grad = jax.grad(objective)(batched_matrix_elements)   File ""/home/user/project/jax_bug.py"", line 36, in objective     return jax.vmap(unbatched_objective)(batched_matrix_elements).mean()   File ""/home/user/project/jax_bug.py"", line 32, in unbatched_objective     xT, _ = jax.lax.scan(scan_body, global_x, matrix_elements) jax._src.source_info_util.JaxStackTraceBeforeTransformation: jaxlib.xla_extension.XlaRuntimeError: INVALID_ARGUMENT: Binary op with incompatible shapes: f32[2,2,3,2] and f32[2,2,2,3]. The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/home/user/miniconda3/envs/env/lib/python3.10/runpy.py"", line 196, in _run_module_as_main     return _run_code(code, main_globals, None,   File ""/home/user/miniconda3/envs/env/lib/python3.10/runpy.py"", line 86, in _run_code     exec(code, run_globals)   File ""/home/user/project/jax_bug.py"", line 43, in      recreate_bug()   File ""/home/user/project/jax_bug.py"", line 38, in recreate_bug     grad = jax.grad(objective)(batched_matrix_elements) jaxlib.xla_extension.XlaRuntimeError: INVALID_ARGUMENT: Binary op with incompatible shapes: f32[2,2,3,2] and f32[2,2,2,3]. ```","Minimized reproducer: ``` HloModule repro ENTRY main {   p0 = s32[] parameter(0)   p1 = f32[2,3,2]{1,2,0} parameter(1)   p2 = f32[3,2,2,2,3]{4,3,2,1,0} parameter(2)   dynamicslice.35 = f32[1,2,2,2,3]{4,3,2,1,0} dynamicslice(p2, p0, p0, p0, p0, p0), dynamic_slice_sizes={1,2,2,2,3}   reshape = f32[2,2,2,3]{3,2,1,0} reshape(dynamicslice.35)   dot = f32[2,3,2]{2,1,0} dot(p1, reshape), lhs_batch_dims={0,1}, lhs_contracting_dims={2}, rhs_batch_dims={0,3}, rhs_contracting_dims={1}   transpose = f32[2,2,3]{1,2,0} transpose(dot), dimensions={0,2,1}   ROOT reshape.2 = f32[2,6]{1,0} reshape(transpose) } ``` Output of //xla/tools:run_hlo_module: ``` INVALID_ARGUMENT: Binary op with incompatible shapes: f32[2,2,3,2] and f32[2,2,2,3]. ```","Input to LayoutNormalization: ``` HloModule repro, entry_computation_layout={(s32[], f32[2,3,2]{1,2,0}, f32[3,2,2,2,3]{4,3,2,1,0})>f32[2,6]{1,0}} scalar_add_computation {   scalar_lhs = f32[] parameter(0)   scalar_rhs = f32[] parameter(1)   ROOT add = f32[] add(scalar_lhs, scalar_rhs) } ENTRY main {   p1 = f32[2,3,2]{1,2,0} parameter(1)   copy = f32[2,3,2]{2,1,0} copy(p1)   broadcast.1 = f32[2,3,2,2]{2,1,3,0} broadcast(copy), dimensions={0,1,2}   p2 = f32[3,2,2,2,3]{4,3,2,1,0} parameter(2)   p0 = s32[] parameter(0)   constant = s32[] constant(0)   dynamicslice.35 = f32[1,2,2,2,3]{4,3,2,1,0} dynamicslice(p2, p0, constant, constant, constant, /*index=5*/constant), dynamic_slice_sizes={1,2,2,2,3}   bitcast = f32[2,2,2,3]{3,2,1,0} bitcast(dynamicslice.35)   transpose.5 = f32[2,3,2,2]{2,1,3,0} transpose(bitcast), dimensions={0,3,1,2}   multiply.1 = f32[2,3,2,2]{2,1,3,0} multiply(broadcast.1, transpose.5)   constant.1 = f32[] constant(0)   reduce = f32[2,3,2]{1,2,0} reduce(multiply.1, constant.1), dimensions={2}, to_apply=scalar_add_computation   transpose = f32[2,2,3]{2,1,0} transpose(reduce), dimensions={0,2,1}   ROOT bitcast.1 = f32[2,6]{1,0} bitcast(transpose) } // main ```","Reproducer unit test (probably not minimal yet): ``` TEST_F(LayoutNormalizationTest, JaxB25759) {                                                            const char* hlo = R""(                                                                               HloModule repro ENTRY main {                                                                                            p1 = f32[2,3,2]{1,2,0} parameter(0)                                                                   broadcast.1 = f32[2,3,2,2]{2,1,3,0} broadcast(p1), dimensions={0,1,2}                                 p2 = f32[3,2,2,2,3]{4,3,2,1,0} parameter(1)                                                           dynamicslice.35 = f32[1,2,2,2,3]{4,3,2,1,0} parameter(2)                                             bitcast = f32[2,2,2,3]{3,2,1,0} bitcast(dynamicslice.35)                                             transpose.5 = f32[2,3,2,2]{2,1,3,0} transpose(bitcast), dimensions={0,3,1,2}                          ROOT multiply.1 = f32[2,3,2,2]{2,1,3,0} multiply(broadcast.1, transpose.5)                          } // main )"";   CheckLayoutNormalization(hlo, std::nullopt);                                                        } ```","Minimized: ``` TEST_F(LayoutNormalizationTest, JaxB25759) {   const char* hlo = R""( HloModule repro ENTRY main {   p0 = f32[2,3,2,2]{2,1,3,0} parameter(0)   p1 = f32[2,2,2,3]{3,2,1,0} parameter(1)   transpose = f32[2,3,2,2]{2,1,3,0} transpose(p1), dimensions={0,3,1,2}   ROOT multiply = f32[2,3,2,2]{2,1,3,0} multiply(p0, transpose) } )"";   CheckLayoutNormalization(hlo, std::nullopt); } ```",https://github.com/openxla/xla/pull/21511 should fix this. Hopefully I didn't miss anything :).,"Great, thank you!"
yi,[shape_poly] Remove the deprecated PolyShape object for specifying symbolic dimensions,[shape_poly] Remove the deprecated PolyShape object for specifying symbolic dimensions PolyShape has been deprecated in January 2024. The constructor has been raising a DeprecationWarning since then.,2025-01-07T11:40:00Z,,open,0,0,https://github.com/jax-ml/jax/issues/25751
yi,debugging/make_color_iter() returns the same index when the number of colors is 2 and do not use all the possible colors," Description Hi JAX team,  I have noticed the following behaviors:  make_color_iter() returns the same color index when the number of colors is 2.  make_color_iter() does not use all possible colors (for example with num_rows=2 and num_cols=3, the function returns  [0, 4, 2, 0, 4, 2] as color indexes, so 3 colors among 6 possible. I am not sure if there is a logic behind the selection of the color index besides not selecting the color twice in a row and ensuring that the adjacent colors are not too similar, but this is a proposed quick fix that I think solves the issue: ``` def make_color_iter(color_map, num_rows, num_cols):     num_colors = num_rows * num_cols     color_values = np.linspace(0, 1, num_colors)     idx = 0     step = 1 if num_colors % 2 == 0 else 2   Determine step size     for _ in range(num_colors):         yield color_map(color_values[idx])         idx = (idx + step) % num_colors  ``` I have tested several cases, and cannot see anything wrong in this possible fix. Please, let me know :) Best regards Jerome  System info (python version, jaxlib version, accelerator, etc.) jaxlib version = 0.4.33",2024-12-30T00:02:09Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/25695
yi,Crash with jit of ordered io_callback under shard_map and then no shard_map on CPU," Description On my M1 Mac, with jax==0.4.39.dev20241223 and jaxlib==0.4.39.dev20241223 and Python 3.12.6, the following code: ```python import os os.environ[""XLA_FLAGS""] = 'xla_force_host_platform_device_count=4' import jax import jax.numpy as jnp from jax.sharding import PartitionSpec as P from jax.experimental.shard_map import shard_map mesh = jax.make_mesh((4,), 'i') def empty_callback(x):   return def _f(x, y):   jax.experimental.io_callback(empty_callback, (), x, ordered=True)   return x + y[..., jnp.newaxis] f = jax.jit(shard_map(     _f, mesh, in_specs=(P(None, 'i'), P(None)), out_specs=P(None, 'i'))) print(f(jnp.zeros((2, 16)), jnp.ones(2))) print(jax.jit(_f)(jnp.zeros((2, 16)), jnp.ones(2))) ``` crashes when running the last line, with: ``` F external/xla/xla/pjrt/pjrt_client.h:1128] Check failed: on_device_shape().has_layout() Abort trap: 6 ``` When I update `test_empty_io_callback_under_shard_map` in `tests/pjit_test.py` at HEAD to match this code  see https://github.com/jaxml/jax/pull/25670  the test fails in GitHub's CI and on my Mac.  For example, see https://github.com/jaxml/jax/actions/runs/12470134880/job/34804668225?pr=25670 : ``` Fatal Python error: Aborted Current thread 0x00007a4bd1b93000 (most recent call first):   File ""/__w/jax/jax/jax/_src/array.py"", line 644 in _value   File ""/__w/jax/jax/jax/_src/profiler.py"", line 333 in wrapper   File ""/__w/jax/jax/jax/_src/array.py"", line 1113 in shard_sharded_device_array_slow_path   File ""/__w/jax/jax/jax/_src/array.py"", line 1170 in _array_shard_arg   File ""/__w/jax/jax/jax/_src/array.py"", line 1209 in _token_shard_arg   File ""/__w/jax/jax/jax/_src/interpreters/pxla.py"", line 135 in shard_args   File ""/__w/jax/jax/jax/_src/profiler.py"", line 333 in wrapper   File ""/__w/jax/jax/jax/_src/dispatch.py"", line 510 in _batched_device_put_impl   File ""/__w/jax/jax/jax/_src/core.py"", line 941 in process_primitive   File ""/__w/jax/jax/jax/_src/core.py"", line 468 in bind_with_trace   File ""/__w/jax/jax/jax/_src/core.py"", line 463 in bind   File ""/__w/jax/jax/jax/_src/api.py"", line 2294 in device_put   File ""/__w/jax/jax/jax/_src/dispatch.py"", line 136 in get_token_input   File ""/__w/jax/jax/jax/_src/interpreters/pxla.py"", line 1249 in    File ""/__w/jax/jax/jax/_src/interpreters/pxla.py"", line 1248 in _add_tokens_to_inputs   File ""/__w/jax/jax/jax/_src/interpreters/pxla.py"", line 1288 in __call__   File ""/__w/jax/jax/jax/_src/profiler.py"", line 333 in wrapper   File ""/__w/jax/jax/jax/_src/pjit.py"", line 1688 in _pjit_call_impl_python   File ""/__w/jax/jax/jax/_src/pjit.py"", line 196 in _python_pjit_helper ``` The code fails with the same error if the last two lines are switched  i.e., if the notshardmapped `_f` is run before the shardmapped `f`.  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.39.dev20241223 jaxlib: 0.4.39.dev20241223 numpy:  2.2.1 python: 3.12.6 (main, Sep  9 2024, 21:36:32) [Clang 18.1.8 ] device info: cpu1, 1 local devices"" process_count: 1 platform: uname_result(system='Darwin', node='jburnimmacbookpro2.roam.internal', release='23.6.0', version='Darwin Kernel Version 23.6.0: Thu Sep 12 23:35:29 PDT 2024; root:xnu10063.141.1.701.1~1/RELEASE_ARM64_T6000', machine='arm64') ```",2024-12-23T17:09:42Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/25671,"Assigning , who may have ideas! Note however that due to the holiday it may take a while to look into this."
rag,Add support for `axis_name` and `axis_index_groups` to `lax.ragged_all_to_all`,Add support for `axis_name` and `axis_index_groups` to `lax.ragged_all_to_all`,2024-12-23T05:03:06Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25660
rag,Add mode='fan_geo_avg' to nn.initializers.variance_scaling,"**Feature request:** Add `'fan_geo_avg'` as an option for the `mode` argument of `nn.initializers.variance_scaling`, in order to use the geometric mean rather than arithmetic mean of `fan_in` and `fan_out` for the denominator. Beyond Folklore: A Scaling Calculus for the Design and Initialization of ReLU Networks: > This scaling calculus results in a number of consequences, among them the fact that the **geometric mean** of the fanin and fanout, rather than the fanin, fanout, or arithmetic mean, should be used for the initialization of the variance of weights in a neural network. > > Initialization using the **geometricmean** of the fanin and fanout ensures a constant layer scaling factor throughout the network, aiding optimization. > > The use of **geometric initialization** results in an equally weighted diagonal, in contrast to the other initializations considered. SplitNets: Designing Neural Architectures for Efficient Distributed Computing on HeadMounted Systems: > Using **geometric average** allows us to find a better compromise between forward and backward passes and significantly improve training stability and final accuracy > > Our splitaware initialization adopts **geometric average** instead of arithmetic average to make a better balance between forward and backward I can submit a PR for this.",2024-12-20T20:52:37Z,enhancement,closed,0,0,https://github.com/jax-ml/jax/issues/25649
yi,Add `replace: bool` argument to `random.categorical` to sample without replacement using Gumbel-top-k trick,"`random.categorical` uses the Gumbelmax trick to sample with replacement. An extension of the Gumbelmax trick allows sampling _without_ replacement. As noted in this paper: > The wellknown GumbelMax trick for sampling from a categorical distribution can be extended to sample k elements without replacement. > The GumbelMax trick [...] allows sampling from the categorical distribution, simply by _perturbing_ the logprobability for each category by adding _independent_ Gumbel distributed noise and returning the category with maximum _perturbed logprobability_. [...] However, there is more: as was noted (in a blog post) by Vieira (2014), taking the **top k largest perturbed logprobabilities** (instead of the maximum, or _top 1_) yields a sample of size k from the categorical distribution _without replacement_. We refer to this extension of the GumbelMax trick as the _GumbelTopk_ trick. **Proposal:** Add a `replace: bool` argument to `random.categorical`. `replace=False` should use the GumbelTopk trick (replacing `argmax` with `top_k`) to sample without replacement. **Alternatives:** `random.choice` can sample without replacement, but uses a less efficient `cumsum`based method, and operates on probabilities rather than logits/logprobabilities. Related:  https://github.com/jaxml/jax/issues/22682  https://github.com/jaxml/jax/issues/25498",2024-12-19T20:24:01Z,enhancement,open,0,5,https://github.com/jax-ml/jax/issues/25617,Sounds reasonable – is this something you'd like to contribute?,"Yes, I can create a PR for this."," After thinking about it more, since the semantics of shapes and axes would differ from those of sampling with replacement, I think it might be better to create a separate function. What would be a good name for it? Here are a few candidates:  `categorical_wor`  `categorical_without_replacement`  `categorical_no_replace`  `categorical_no_replacement`  `sample_without_replacement` Here's an example implementation: ```python3 from jax import lax, nn, numpy as jnp, random def categorical_wor(key, logits, k, axis=1, batch_shape=()):     logits = jnp.broadcast_to(logits, batch_shape + logits.shape)     if axis >= 0:         axis += len(batch_shape)     noise = random.gumbel(key, logits.shape, logits.dtype)     x = logits + noise      return x.argmax(axis)   sampling with replacement     x = jnp.moveaxis(x, axis, 1)     _, outcomes = lax.top_k(x, k)     outcomes = jnp.moveaxis(outcomes, 1, axis)     return outcomes def get_freqs(x, length):     counts = jnp.bincount(x.flatten(), length=length)     return counts / counts.sum() def main():     key = random.key(0)     key, subkey = random.split(key)     logits = jnp.arange(4, dtype=float)     print(f""probs: {nn.softmax(logits)}"")     axis = 1     for k in range(logits.shape[axis] + 1):         outcomes = categorical_wor(key, logits, k, axis, (20, 30, 40))         print(f""{k=} freqs: {get_freqs(outcomes, logits.shape[axis])}"") if __name__ == ""__main__"":     main() ``` ``` probs: [0.0320586  0.08714432 0.23688284 0.6439143 ] k=0 freqs: [nan nan nan nan] k=1 freqs: [0.03220833 0.08495833 0.23579167 0.6470417 ] k=2 freqs: [0.05175    0.13675    0.34770834 0.46379167] k=3 freqs: [0.10051389 0.25186113 0.31643057 0.33119443] k=4 freqs: [0.25 0.25 0.25 0.25] ``` (As expected, the sample frequencies become equal by the time we get to `k == logits.shape[axis]`.)",Can you say more about why this needs its own function?,"The output shape of sampling with replacement is `delete(logits.shape, axis)`. The output shape of sampling without replacement would be `insert(delete(logits.shape, axis), axis2, k)`, where `k` is the length of the sequential samplingwithoutreplacement process and `axis2` is some axis (possibly `axis`). Strictly speaking, it need not be its own function, but the argument signature as well as the documentation describing the output shape of `random.categorical` would need to be modified. Perhaps there's a risk of making the function too convoluted by overloading it with the task of doing two different things."
yi,Add support for N-D FFTs with N>3,"The current implementation of `jax.numpy.fft.fftn` (and related) don't support ND transforms with `N > 3` because that's all that XLA supports. However, we can (like in the numpy implementation) support higher dimensional transforms by repeatedly applying the supported low order transforms. In this PR, I add that logic for `N > 3`, but the behavior remains unchanged for `N <= 3` transforms.",2024-12-19T12:13:18Z,pull ready,closed,2,2,https://github.com/jax-ml/jax/issues/25606,Might be worth a changelog entry.,> Might be worth a changelog entry. Good point! Added.
yi,Surprising difference of output between NumPy's `float32` and JAX's `float32`," Description Hello! I am not sure how to correctly title this issue, but I recently had an issue where the output of my some function would differ when `.jit` was applied to the function. Here is the simplified version: ```python def fun(f):     f_ghz = f / 1e9     return f_ghz >= 0.1 ``` Which yields: ```python >>> fun(0.1e9) True >>> jax.jit(fun)(0.1e9) Array(False, dtype=bool, weak_type=True) ``` I know Python float are 64bit precision, while JAX's are 32bit, and JIT can also optimize some operations, which could (I guess) change the order of operations and lead to different floating point results (?). I tried to force using float32, in both cases: ```python import jax.numpy as jnp import numpy as np def fun_np(f):     f_ghz = f / np.float32(1e9)     print(f""{f_ghz.dtype = }"")     return f_ghz >= np.float32(0.1) def fun_jnp(f):     f_ghz = f / jnp.float32(1e9)     print(f""{f_ghz.dtype = }"")     return f_ghz >= jnp.float32(0.1) ``` But the results are still *surprising*: ```python >>> fun_np(np.float32(0.1e9)) f_ghz.dtype = dtype('float32') True >>> fun_jnp(jnp.float32(0.1e9)) f_ghz.dtype = dtype('float32') Array(False, dtype=bool) ``` The NumPy version outputs the expected value, while the JAX variant doesn't. Am I doing something wrong? I could not find documentation about this, and I wonder if those kinds of things can be prevented, or should be expected? For this problem, I found a very simple fix: ```python def fun(f):     return f >= 0.1 * 1e9 ``` that is, using multiplication instead of division. ```python >>> fun(0.1e9) True >>> jax.jit(fun)(0.1e9) Array(False, dtype=bool, weak_type=True) ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.34 jaxlib: 0.4.34 numpy:  1.26.4 python: 3.11.8 (main, Feb 25 2024, 04:18:18) [Clang 17.0.6 ] jax.devices (1 total, 1 local): [CudaDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='jeertmans', release='6.8.049generic', version=' CC(Update neural_network_and_data_loading.ipynb)Ubuntu SMP PREEMPT_DYNAMIC Mon Nov  4 02:06:24 UTC 2024', machine='x86_64') ```",2024-12-19T09:00:28Z,bug,open,0,8,https://github.com/jax-ml/jax/issues/25601,"A simpler reproducer to this issue is: ```python >>> np.float32(0.1e9) / np.float32(1e9) == jnp.float32(0.1e9) / jnp.float32(1e9) Array(False, dtype=bool) >>> np.float32(0.1e9) / np.float32(1e9), jnp.float32(0.1e9) / jnp.float32(1e9) (0.1, Array(0.09999999, dtype=float32)) ``` It looks like numpy is using intermediate float64 arithmetic in float32 division and JAX float32 division uses strictly float32 arithmetic. To illustrate this, consider: ```python >>> import mpmath >>> mpmath.mp.dps = 15   double precision >>> mpmath.mp.mpf(0.1e9) / mpmath.mp.mpf(1e9) mpf('0.10000000000000001') >>> mpmath.mp.dps = 6   single precision >>> mpmath.mp.mpf(0.1e9) / mpmath.mp.mpf(1e9) mpf('0.099999994') ``` So, the issue not about a bug in JAX but rather belongs to https://jax.readthedocs.io/en/latest/faq.html ","Thanks for the reply ! That is quite surprising that NumPy doens't document that behavior (or I couldn't find it) on their Data type promotion page, but it isn't JAX's fault ^^'. But I think that documenting this in JAX's FAQ could be great :)",What exactly do you have in mind for documenting in the JAX FAQ?,"> What exactly do you have in mind for documenting in the JAX FAQ? That NumPy, as opposed to JAX, can promote data types for some computation, even if that is not documented, which might lead to difference between a JAX program and a NumPy program, even though you explicitly use the same precision. And probably link to this issue or provide example division. I am not sure how it is better to phrase this.",OK – maybe something like that would fit as a bullet point here? https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.htmlmiscellaneousdivergencesfromnumpy,"> OK – maybe something like that would fit as a bullet point here? https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.htmlmiscellaneousdivergencesfromnumpy Yes, I think this is a good place for that :)","FWIW, JAX and Numba are in the same boat regarding float32 semantics with respect to NumPy: https://numba.readthedocs.io/en/stable/reference/fpsemantics.html",Should I start a PR editing this page > OK – maybe something like that would fit as a bullet point here? https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.htmlmiscellaneousdivergencesfromnumpy ?
yi,WIP: no special treatment for ShapeDtypeStruct,"I'm confused as to why we need to register this, because `ShapeDtypeStruct` should behave exactly the same as any ducktyped value with a `shape` and `dtype`. This registration recently led to the rollback of CC(Remove core.concrete_aval and replace with abstractify) because a downstream library was relying on this behavior in a dubious way.",2024-12-18T17:44:14Z,,open,0,1,https://github.com/jax-ml/jax/issues/25583,"You will need to run TGP to find the real failures. But users pass `SDS` in as an input to normal `jit` and expect it to work. And it does work because those inputs are unused and we DCE them. Last time I looked, the failures were a lot so I didn't really take any action but maybe you can!"
rag,Clarify documentation for output_offsets operand of ragged_all_to_all.,Clarify documentation for output_offsets operand of ragged_all_to_all.,2024-12-18T13:02:37Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25567
yi,Is `jax.scipy.stats.norm.logcdf` twice differentiable?," Description It seems that `jax.scipy.stats.norm.logcdf` is not twice differentiatiable.  I implemented the following function in `jax`.  ``` .jit def log_likelihood_fn(     log_lambda_x: Array,     s_i: Array, ):      some more code here     log_likelihoods = (         jstat.norm.logcdf(             (                 log_rate / (s_i * jnp.sqrt(2))             ),             scale=1 / jnp.sqrt(2),         )     )     return  log_likelihoods.sum()    ```    where `jstat.norm.logcdf(., scale = 1/jnp.sqrt(2))` is used in placed of `jax.scipy.special.erfc`, due to numerical instability problems with the latter. **s_i is very small, approximately in the scale of ~1e5**.   I am trying to minimize this function (i.e. maximize log likelihood) using Newton's method. However, when computing the second derivative I found some wierd behavior. Drawing a simple graph with `matplotlib` for a single `lambda`, the graph appears to be convex.  !image However, drawing the second derivative, instead of being positive, as I expected (since it is a convex function), it appears that it oscillates wildly and blows up.  !image I think that I should add a custom derivative to `jstat.norm.logcdf`, but I am unsure of how to get started. What causes this numerical instability in the twicederivative calculation of `jstat.norm.logcdf`?  Any help would be appreciated.   System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.35 jaxlib: 0.4.34 numpy:  2.0.1 python: 3.12.4  ++ ```",2024-12-18T08:20:44Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/25564,"`norm.logcdf` is implemented in terms of `special.log_ndtr`, which in turn is implemented using an asymptotic series. The second derivative plots you're showing look to me like the result of floating point rounding within that asymptotic series computation. One question to help diagnose this: how do the results look if you enable float64 precision?","Enabling `float64` precision by doing `jax.config.update(""jax_enable_x64"", True)` solved my issue perfectly, thank you! Here is the resulting Hessian and Gradient Graph for reference after enabling 64bit precision.  !image And here I was thinking I would have to make my own custom gradients 😓. Thank you for your help!"
rag,Move ragged tests under a new class.,Move ragged tests under a new class.,2024-12-18T07:32:04Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25563
yi,Streamline some core.concrete_aval compute paths,"Builds on CC(Cleanup: toward merging core.concrete_aval & xla.abstractify); brings `core.abstractify` and `core.concrete_aval` much closer. Though I've not benchmarked it, each of these new code paths should be more efficient than what it replaces. The two paths only differ now in handling of `ArrayImpl` and of `DArray`.",2024-12-17T18:15:31Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/25534
rag,Make gmm TPU kernel tests significantly cheaper,Make gmm TPU kernel tests significantly cheaper We were testing lots of very similar cases that did not really help a lot with coverage.,2024-12-17T15:07:18Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25529
rag,Allow `lax.ragged_all_to_all` input and output operands to have different ragged dimension sizes.,"Allow `lax.ragged_all_to_all` input and output operands to have different ragged dimension sizes. We need to guaranty that the outermost dimension of the output is big enough to fit all received elements, but it's not necessary for input and output outermost dimensions to be exactly equal.",2024-12-16T21:00:31Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25518
yi,Clarification re: supported data types in `jax.linearize` and `jax.linear_transpose`,"Hi All,  IIUC `jax.linearize` and `jax.linear_transpose` do not support taking derivatives with respect to integer values, i.e. there is no forwardmode analogy of `jax.grad(..., allow_int=True)`. In JAX forwardmode autodiff, integers are treated as constants.  When transposing the linearised function taking an integer input, the following error is raised >ValueError: linearized function called on tangent values inconsistent with the original primal values: got ShapedArray(int32[], weak_type=True) for primal aval ShapedArray(int32[], weak_type=True) Could we change that error message to something more informative by specialcasing integer inputs? I'm happy to do it and open a PR. ```python import jax def fn(x):     return x**2 y_float = 3.0 y_int = 3 fn_eval_float, lin_fn_float = jax.linearize(fn, 3.0) fn_eval_int, lin_fn_int = jax.linearize(fn, 3) grad_float = jax.linear_transpose(lin_fn_float, y_float)(1.0)   Works fine grad_int = jax.linear_transpose(lin_fn_int, y_int)(1.0)   Raises value error ``` This issue came up downstream.",2024-12-16T19:24:44Z,enhancement,closed,0,4,https://github.com/jax-ml/jax/issues/25517,"Assigning , who has been working on linearization improvements (cf. CC(More linearization fixes))","Yes it's a good idea to mention the expected tangent types. Here's a PR that does that: CC(Mention expected tangent aval in error message, see 25517.). With that, the error message now reads: ``` ValueError: linearized function called on tangent values inconsistent with the original primal values: got tange nt aval ShapedArray(int32[], weak_type=True) for primal aval ShapedArray(int32[], weak_type=True) but expected S hapedArray(float0[], weak_type=True)  ``` How does that look?","Yup, this is more informative! Thanks. ",Closed by CC(Clarification re: supported data types in `jax.linearize` and `jax.linear_transpose`)
rag,jax.random.choice(replace=True) samples 0 probability index," Description `jax.random.choice(replace=True)` will sample 0 probability entry when the input array is large, and the average probability is low (`~1e07`): ``` sample_prob = np.zeros((7000000,)) sample_prob[:5000000] = 1.0 sample_prob = jnp.array(sample_prob / (sample_prob.sum())) print(sample_prob.max())   Output: 2e07 print(sample_prob.min())   Output: 0.0 sampled_idxs = jax.random.choice(     jax.random.PRNGKey(0),     a=jnp.arange(len(sample_prob)),     shape=(len(sample_prob),),     p=sample_prob,     replace=True, ) print((sample_prob[sampled_idxs]).min())   Output: 0.0, shouldn't happen ``` The `numpy` counter part `np.random.choice` behaves correctly: ``` sample_prob = np.zeros((7000000,)).astype(np.float32) sample_prob[:5000000] = 1.0 sample_prob = sample_prob / (sample_prob.sum()) print(sample_prob.max())   Output: 2e07 print(sample_prob.min())   Output: 0.0 sampled_idxs = np.random.choice(     a=np.arange(len(sample_prob)),     size=(len(sample_prob),),     p=sample_prob,     replace=True, ) print((sample_prob[sampled_idxs]).min())   Output: 2e07, expected ``` Seems like an unexpected behavior/bug?  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.38 jaxlib: 0.4.38 numpy:  2.1.3 python: 3.11.8 (stable, redacted, redacted) [Clang 9999.0.0 (be2df95e9281985b61270bb6420ea0eeeffbbe59)] device info: Tesla V100SXM216GB1, 1 local devices"" process_count: 1 platform: uname_result(system='Linux', node='...', release='5.10.0smp1106.20.0.0', version=' CC(Python 3 compatibility issues) [v5.10.01106.20.0.0] SMP ', machine='x86_64') ```",2024-12-16T00:41:02Z,bug duplicate,open,0,1,https://github.com/jax-ml/jax/issues/25498,Thanks for the report – this looks to be a duplicate of CC(jax.random.choice can return entries with zero probability).
rag,"[Mosaic GPU] Add layout inference and initial lowering for `vector.{load,store}`.","[Mosaic GPU] Add layout inference and initial lowering for `vector.{load,store}`. For now, the lowering only works for the strided fragmented layout. This is mostly an exercise in plugging in lowering rules using `FragmentedArray`, and will be expanded shortly.",2024-12-14T07:42:35Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25495
yi,fix `gamma_p` in vmap-based impl rule mode,"This was a regression introduced in CC(represent `random.key_impl` of builtin RNGs by canonical string name), which broke this check: https://github.com/jaxml/jax/blob/c73f3060997ac3b1c6de4f075111b684ea20b6ac/jax/_src/random.pyL1233L1234 It would be nice to remove `gamma_p` altogether if we can, and always back it by the vmapbased implementation that is guarded above. fixes CC(`jax.random.beta` 3 orders of magnitude slower from 0.4.36 on GPU)",2024-12-13T23:36:42Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/25487
rag,Add an experimental Cloud TPU presubmit job,"Add an experimental Cloud TPU presubmit job This adds an experimental nonblocking presubmit job that will run a subset of TPU tests, focusing on frequently failing tests. The goal is to achieve comprehensive coverage while keeping the runtime around 10 minutes.",2024-12-13T21:31:39Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25482
yi,std::bad_cast," Description ```python import jax import jax.numpy  x = jnp.ones((64, 10)) ``` The above causes  ```python   ValueError                                Traceback (most recent call last) [](https://localhost:8080/) in () > 1 x = jnp.ones((64, 10)) 7 frames /usr/local/lib/python3.10/distpackages/jax/_src/numpy/lax_numpy.py in ones(shape, dtype, device)    6187   shape = canonicalize_shape(shape)    6188   dtypes.check_user_dtype_supported(dtype, ""ones"") > 6189   return lax.full(shape, 1, _jnp_dtype(dtype), sharding=_normalize_to_sharding(device))    6190     6191  /usr/local/lib/python3.10/distpackages/jax/_src/lax/lax.py in full(shape, fill_value, dtype, sharding)    1701   weak_type = dtype is None and dtypes.is_weakly_typed(fill_value)    1702   dtype = dtypes.canonicalize_dtype(dtype or _dtype(fill_value)) > 1703   fill_value = _convert_element_type(fill_value, dtype, weak_type)    1704   if (sharding is not None and not isinstance(sharding, PmapSharding) and    1705       isinstance(fill_value, array.ArrayImpl)): /usr/local/lib/python3.10/distpackages/jax/_src/lax/lax.py in _convert_element_type(operand, new_dtype, weak_type, sharding)     587     return operand     588   else: > 589     return convert_element_type_p.bind(     590         operand, new_dtype=new_dtype, weak_type=bool(weak_type),     591         sharding=sharding) /usr/local/lib/python3.10/distpackages/jax/_src/core.py in bind(self, *args, **params)     455     trace_ctx.set_trace(eval_trace)     456     try: > 457       return self.bind_with_trace(prev_trace, args, params)     458     finally:     459       trace_ctx.set_trace(prev_trace) /usr/local/lib/python3.10/distpackages/jax/_src/lax/lax.py in _convert_element_type_bind_with_trace(trace, args, params)    3057 def _convert_element_type_bind_with_trace(trace, args, params):    3058   sharding = params['sharding'] > 3059   operand = core.Primitive.bind_with_trace(convert_element_type_p, trace, args, params)    3060   if sharding is not None and not config.sharding_in_types.value:    3061     with core.set_current_trace(trace): /usr/local/lib/python3.10/distpackages/jax/_src/core.py in bind_with_trace(self, trace, args, params)     460      461   def bind_with_trace(self, trace, args, params): > 462     return trace.process_primitive(self, args, params)     463      464   def def_impl(self, impl): /usr/local/lib/python3.10/distpackages/jax/_src/core.py in process_primitive(self, primitive, args, params)     922             return primitive.bind_with_trace(arg._trace, args, params)     923       check_eval_args(args) > 924       return primitive.impl(*args, **params)     925      926   def process_call(self, primitive, f, tracers, params): /usr/local/lib/python3.10/distpackages/jax/_src/dispatch.py in apply_primitive(prim, *args, **params)      88   prev = lib.jax_jit.swap_thread_local_state_disable_jit(False)      89   try: > 90     outs = fun(*args)      91   finally:      92     lib.jax_jit.swap_thread_local_state_disable_jit(prev) ValueError: std::bad_cast ``` Experimented on both Google Colab GPU & CPU  System info (python version, jaxlib version, accelerator, etc.) Google Colab CPU ```python jax:    0.4.37 jaxlib: 0.4.36 numpy:  1.26.4 python: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] device info: cpu1, 1 local devices"" process_count: 1 platform: uname_result(system='Linux', node='3ef4f9c0da84', release='6.1.85+', version=' CC(Python 3 compatibility issues) SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024', machine='x86_64') ```",2024-12-13T13:14:54Z,bug,closed,0,5,https://github.com/jax-ml/jax/issues/25468,"I'm not able to reproduce this: on a Colab CPU runtime I ran the following two cells and saw no error: ``` pip install jax==0.4.37 jaxlib==0.4.36 ``` ``` import jax import jax.numpy  x = jax.numpy.ones((64, 10)) ``` It's possible your runtime got into some kind of strange state (perhaps you imported some code, then installed fresh packages, and then continued execution in the same environment?) Could you try restarting your runtime and see if the problem persists? Have you installed any other packages that may be relevant?",Turns out that having numpy >= 2.0 causes this issue. Downgrading numpy solves the issue.,"It's actually more complicated than that. NumPy is a module that is already loaded even in a completely empty colab session. If you upgrade it with `pip` without restarting the runtime, you end up in a weird state where you have pieces of NumPy 1 and NumPy 2 at the same time. I can reproduce by: * delete and create a completely fresh runtime * `!pip install jax==0.4.37 numpy==2.2.0` * run your code. You are in a somewhat bad state at this point, but JAX probably should fail more gracefully. But if you simply restart the runtime and rerun your code, you then have a consistent version of NumPy, and the problem goes away.","gdb backtrace: ``` (gdb) bt CC(未找到相关数据)  0x00007fee0fab32b1 in __cxa_throw () from /lib/x86_64linuxgnu/libstdc++.so.6 CC(Python 3 compatibility issues)  0x00007fed61d18b72 in nanobind::detail::raise_cast_error() () from /usr/local/google/home/phawkins/.pyenv/versions/3.12.3/envs/py3.12.3/lib/python3.12/sitepackages/jaxlib/xla_extension.so CC(Explicit tuples are not valid function parameters in Python 3)  0x00007fed60f400be in xla::nb_numpy_ndarray nanobind::detail::cast_impl(nanobind::handle) ()    from /usr/local/google/home/phawkins/.pyenv/versions/3.12.3/envs/py3.12.3/lib/python3.12/sitepackages/jaxlib/xla_extension.so CC(Undefined name: from ..core import JaxTuple)  0x00007fed60f4ca92 in std::_Function_handler (nanobind::handle, bool), xla::PyArgSignatureOfValue(nanobind::handle, bool)::$_0::operator()() const::{lambda(nanobind::handle, bool) CC(Building on OSX with CUDA)}>::_M_invoke(std::_Any_data const&, nanobind::handle&&, bool&&) ()    from /usr/local/google/home/phawkins/.pyenv/versions/3.12.3/envs/py3.12.3/lib/python3.12/sitepackages/jaxlib/xla_extension.so CC(Undefined name: from six.moves import xrange)  0x00007fed60f3669f in xla::PyArgSignatureOfValue(nanobind::handle, bool) () from /usr/local/google/home/phawkins/.pyenv/versions/3.12.3/envs/py3.12.3/lib/python3.12/sitepackages/jaxlib/xla_extension.so CC(Building on OSX with CUDA)  0x00007fed5adbda1c in jax::(anonymous namespace)::PjitFunction::Call(nanobind::handle, _object* const*, unsigned long, _object*) ()    from /usr/local/google/home/phawkins/.pyenv/versions/3.12.3/envs/py3.12.3/lib/python3.12/sitepackages/jaxlib/xla_extension.so CC(Made a shim to handle configuration without having absl parse flags)  0x00007fed5adbce7f in PjitFunction_tp_vectorcall () from /usr/local/google/home/phawkins/.pyenv/versions/3.12.3/envs/py3.12.3/lib/python3.12/sitepackages/jaxlib/xla_extension.so CC(Quickish check)  0x00007feec510c81c in _PyEval_EvalFrameDefault (tstate=, frame=, throwflag=) at Python/bytecodes.c:3254 ```","Hmm. Any fix is going to be invasive, because basically ""NumPy isn't working correctly"" at this point. I'll fix the proximate cause of the `std::bad_cast` since that's a minor cleanup anyway, but then you immediately end up with a different weird failure. Closing because it's a losing battle to make JAX report good errors in the presence of a broken NumPy."
yi,"Slow convolution, many memory warnings"," Description Hi, I wrote some code to run a spatial filter on some frames of data. I'm basically convolving a batch of images with a ~30 x 30 image kernel. I'd like to take advantage of jit + vmap here (see code below) so that this code can run as fast as possible on GPU. When I do this, the execution is extremely slow (it takes 20 seconds to run the code below). Several warnings show up about memory allocation (included below). Based on the warnings it looks like under the hood a lot of time is being spent trying to figure out the best strategy for running the convolution  not sure though. Any and all help on this greatly appreciated!  Code:  ``` import numpy as np import jax import jax.numpy as jnp from jax import jit, vmap import jax.lax as lax def _convolution_image_filter(img: np.ndarray, kernel: np.ndarray):     """"""     Filter img with kernel     Args:         img (np.ndarray): Shape (fov dim 1, fov dim 2). Image to be filtered         kernel (np.ndarray): Shape (k1, k1). Kernel for filtering     Returns:         filtered_img (np.ndarray): Shape (fov dim 1, fov dim 2).     """"""     img_padded = jnp.pad(img,                          (((kernel.shape[0]) // 2, (kernel.shape[0]) // 2),                           ((kernel.shape[1]) // 2, (kernel.shape[1]) // 2)),                          mode='reflect')     filtered_frame = jax.scipy.signal.convolve(img_padded, kernel, mode=""valid"")     return filtered_frame convolution_image_filter = jit(_convolution_image_filter) convolution_image_filter_batch = jit(vmap(_convolution_image_filter, in_axes=(0, None))) kernel = np.ones((30, 30)) data = np.random.rand(100, 500, 1400) output = convolution_image_filter_batch(data, kernel) ``` Various Warnings/Messages:  ``` 20241213 12:26:57.046981: E external/xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng28{k2=3,k3=0} for conv (f32[100,1,501,1401]{3,2,1,0}, u8[0]{0}) customcall(f32[100,1,530,1430]{3,2,1,0}, f32[1,1,30,30]{3,2,1,0}), window={size=30x30}, dim_labels=bf01_oi01>bf01, custom_call_target=""__cudnn$convForward"", backend_config={""cudnn_conv_backend_config"":{""activation_mode"":""kNone"",""conv_result_scale"":1,""leakyrelu_alpha"":0,""side_input_scale"":0},""force_earliest_schedule"":false,""operation_queue_id"":""0"",""wait_on_operation_queues"":[]} is taking a while... 20241213 12:26:57.597376: E external/xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.55049392s Trying algorithm eng28{k2=3,k3=0} for conv (f32[100,1,501,1401]{3,2,1,0}, u8[0]{0}) customcall(f32[100,1,530,1430]{3,2,1,0}, f32[1,1,30,30]{3,2,1,0}), window={size=30x30}, dim_labels=bf01_oi01>bf01, custom_call_target=""__cudnn$convForward"", backend_config={""cudnn_conv_backend_config"":{""activation_mode"":""kNone"",""conv_result_scale"":1,""leakyrelu_alpha"":0,""side_input_scale"":0},""force_earliest_schedule"":false,""operation_queue_id"":""0"",""wait_on_operation_queues"":[]} is taking a while... 20241213 12:26:58.597512: E external/xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng1{k2=4,k3=0} for conv (f32[100,1,501,1401]{3,2,1,0}, u8[0]{0}) customcall(f32[100,1,530,1430]{3,2,1,0}, f32[1,1,30,30]{3,2,1,0}), window={size=30x30}, dim_labels=bf01_oi01>bf01, custom_call_target=""__cudnn$convForward"", backend_config={""cudnn_conv_backend_config"":{""activation_mode"":""kNone"",""conv_result_scale"":1,""leakyrelu_alpha"":0,""side_input_scale"":0},""force_earliest_schedule"":false,""operation_queue_id"":""0"",""wait_on_operation_queues"":[]} is taking a while... 20241213 12:26:59.306847: E external/xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.70940755s Trying algorithm eng1{k2=4,k3=0} for conv (f32[100,1,501,1401]{3,2,1,0}, u8[0]{0}) customcall(f32[100,1,530,1430]{3,2,1,0}, f32[1,1,30,30]{3,2,1,0}), window={size=30x30}, dim_labels=bf01_oi01>bf01, custom_call_target=""__cudnn$convForward"", backend_config={""cudnn_conv_backend_config"":{""activation_mode"":""kNone"",""conv_result_scale"":1,""leakyrelu_alpha"":0,""side_input_scale"":0},""force_earliest_schedule"":false,""operation_queue_id"":""0"",""wait_on_operation_queues"":[]} is taking a while... 20241213 12:26:59.306914: W external/xla/xla/tsl/framework/bfc_allocator.cc:306] Allocator (GPU_0_bfc) ran out of memory trying to allocate 235.35GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available. 20241213 12:27:00.307021: E external/xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng28{k2=0,k3=0} for conv (f32[100,1,501,1401]{3,2,1,0}, u8[0]{0}) customcall(f32[100,1,530,1430]{3,2,1,0}, f32[1,1,30,30]{3,2,1,0}), window={size=30x30}, dim_labels=bf01_oi01>bf01, custom_call_target=""__cudnn$convForward"", backend_config={""cudnn_conv_backend_config"":{""activation_mode"":""kNone"",""conv_result_scale"":1,""leakyrelu_alpha"":0,""side_input_scale"":0},""force_earliest_schedule"":false,""operation_queue_id"":""0"",""wait_on_operation_queues"":[]} is taking a while... 20241213 12:27:03.606746: E external/xla/xla/service/slow_operation_alarm.cc:133] The operation took 4.299796437s Trying algorithm eng28{k2=0,k3=0} for conv (f32[100,1,501,1401]{3,2,1,0}, u8[0]{0}) customcall(f32[100,1,530,1430]{3,2,1,0}, f32[1,1,30,30]{3,2,1,0}), window={size=30x30}, dim_labels=bf01_oi01>bf01, custom_call_target=""__cudnn$convForward"", backend_config={""cudnn_conv_backend_config"":{""activation_mode"":""kNone"",""conv_result_scale"":1,""leakyrelu_alpha"":0,""side_input_scale"":0},""force_earliest_schedule"":false,""operation_queue_id"":""0"",""wait_on_operation_queues"":[]} is taking a while... 20241213 12:27:04.606890: E external/xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng28{k2=1,k3=0} for conv (f32[100,1,501,1401]{3,2,1,0}, u8[0]{0}) customcall(f32[100,1,530,1430]{3,2,1,0}, f32[1,1,30,30]{3,2,1,0}), window={size=30x30}, dim_labels=bf01_oi01>bf01, custom_call_target=""__cudnn$convForward"", backend_config={""cudnn_conv_backend_config"":{""activation_mode"":""kNone"",""conv_result_scale"":1,""leakyrelu_alpha"":0,""side_input_scale"":0},""force_earliest_schedule"":false,""operation_queue_id"":""0"",""wait_on_operation_queues"":[]} is taking a while... 20241213 12:27:10.442216: E external/xla/xla/service/slow_operation_alarm.cc:133] The operation took 6.835400164s Trying algorithm eng28{k2=1,k3=0} for conv (f32[100,1,501,1401]{3,2,1,0}, u8[0]{0}) customcall(f32[100,1,530,1430]{3,2,1,0}, f32[1,1,30,30]{3,2,1,0}), window={size=30x30}, dim_labels=bf01_oi01>bf01, custom_call_target=""__cudnn$convForward"", backend_config={""cudnn_conv_backend_config"":{""activation_mode"":""kNone"",""conv_result_scale"":1,""leakyrelu_alpha"":0,""side_input_scale"":0},""force_earliest_schedule"":false,""operation_queue_id"":""0"",""wait_on_operation_queues"":[]} is taking a while... 20241213 12:27:11.442351: E external/xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng4{} for conv (f32[100,1,501,1401]{3,2,1,0}, u8[0]{0}) customcall(f32[100,1,530,1430]{3,2,1,0}, f32[1,1,30,30]{3,2,1,0}), window={size=30x30}, dim_labels=bf01_oi01>bf01, custom_call_target=""__cudnn$convForward"", backend_config={""cudnn_conv_backend_config"":{""activation_mode"":""kNone"",""conv_result_scale"":1,""leakyrelu_alpha"":0,""side_input_scale"":0},""force_earliest_schedule"":false,""operation_queue_id"":""0"",""wait_on_operation_queues"":[]} is taking a while... E1213 12:27:12.556075  141765 gpu_timer.cc:82] Delay kernel timed out: measured time has suboptimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems. E1213 12:27:13.664756  141765 gpu_timer.cc:82] Delay kernel timed out: measured time has suboptimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems. 20241213 12:27:13.675042: E external/xla/xla/service/slow_operation_alarm.cc:133] The operation took 3.232765972s Trying algorithm eng4{} for conv (f32[100,1,501,1401]{3,2,1,0}, u8[0]{0}) customcall(f32[100,1,530,1430]{3,2,1,0}, f32[1,1,30,30]{3,2,1,0}), window={size=30x30}, dim_labels=bf01_oi01>bf01, custom_call_target=""__cudnn$convForward"", backend_config={""cudnn_conv_backend_config"":{""activation_mode"":""kNone"",""conv_result_scale"":1,""leakyrelu_alpha"":0,""side_input_scale"":0},""force_earliest_schedule"":false,""operation_queue_id"":""0"",""wait_on_operation_queues"":[]} is taking a while... 20241213 12:27:13.685184: W external/xla/xla/service/gpu/nvptx_compiler.cc:893] The NVIDIA driver's CUDA version is 12.5 which is older than the PTX compiler version 12.6.68. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIAprovided CUDA forward compatibility packages. ```  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.33 jaxlib: 0.4.33 numpy:  1.26.4 python: 3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 9.4.0] jax.devices (1 total, 1 local): [CudaDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='panda', release='5.4.0200generic', version=' CC(Support for numpy.take_along_axis)Ubuntu SMP Fri Sep 27 13:19:16 UTC 2024', machine='x86_64')",2024-12-13T04:33:00Z,bug,open,0,3,https://github.com/jax-ml/jax/issues/25461,"Thanks for the report! Some comments about this case: Can you comment on where your 20 second number is coming from? I can't exactly reproduce it locally, although compilation of this function is quite slow. If you're not already familiar with it, you might want to check out the JAX docs about microbenchmarking to work out where you're seeing a bottleneck. Running your test code on Colab, I find that compilation takes about 5 seconds, then the runtime after it is compiled is about 300400ms. It would be interesting to know if you consider that extremely slow for your use cases. If these compile times are a blocker for you, you might be ways to control XLA's autotuning behavior to trade off compile time vs. runtime performance, but I must admit I don't know how to do that off the top of my head!"," Thank you for the quick response! I'm inferring the 20 second number by running the following code snippet, following the jax benchmarking guide you linked:  ``` kernel = np.ones((30, 30)) data = np.random.rand(100, 500, 1400) data_gpu = jax.device_put(data) %time output = convolution_image_filter_batch(data_gpu, kernel).block_until_ready() %time output = convolution_image_filter_batch(data_gpu, kernel).block_until_ready() ``` The difference in wall times between the above executions is >20 seconds (24.1s vs. 92.9ms).  One thing I'll add is that if you increase the number of frames (i.e. instead of data = np.random(100, 500, 1400), you do data = np.random(1000, 500, 1400), so you are processing 1,000 frames instead of 100), the compilation time goes up drastically, to ~3.5 minutes! Perhaps this suggests that JAX and/or XLA is struggling to find the right convolution algorithm under the hood?  I guess a separate question is whether the algorithm that it ends up picking is the fastest one for execution on the GPU.  Happy to do some digging into any/all of the above, just let me know what you think!","We've found that CUDA aggressively attempts to determine the largest kernel that can maximally fit inside the GPU, and there are many such possible arrangements.  The arrangement possibilities only grow as the input (batch) size grows.  Another barrier to this are non multiple of 16 dimension sizes (that I've seen in practice, although large powers of 2 are usually preferred), and I notice your sizes are f32[100,1,501,1401].  (https://docs.nvidia.com/deeplearning/performance/dlperformanceconvolutional/index.htmlparamsperf)"
yi,Update the advanced autodiff tutorial and replace some vmap with grad,"Hi everyone, I was reading the advanced autodiff tutorial, and starting from the section **""Hessianvector products with jax.gradofjax.grad""** I was getting progressively more confused; some explanations didn't make much sense to me. I spent some time trying to figure out what was wrong, and I believe in several places the authors mistakenly refer to `jax.vmap()` instead of `jax.grad()` . I made the change in the few places I found relevant, please see them in the diff editor and let me know what you think! I hope I found them correctly and sorry for not opening the issue first, I thought that in this case it is okay to have a ""hot fix"" pull request. Thank you! ",2024-12-12T17:09:42Z,pull ready,closed,0,6,https://github.com/jax-ml/jax/issues/25441,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Hey guys, it's been almost a month since I opened this PR, I hope you don't mind if I tag e.g.   (sorry if I come across as pushy, I definitely don't mean to: I do recognise that you might have a lot on your plates; so I am doing this just in case the PR somehow went under the radar)",Thanks for the ping! Can you start by completing the CLA as requested by the bot above?,"> Thanks for the ping! Can you start by completing the CLA as requested by the bot above? Gosh, I am sorry, I did sign the CLA on the day I submitted the PR, I just didn't see I have to reinvoke the check myself; should be all good now!",Can you rebase this PR onto the `main` branch? It looks like we're hitting some unrelated build issues.,"> Can you rebase this PR onto the `main` branch? It looks like we're hitting some unrelated build issues. Done, hope it works!"
yi,Unclear documentation errors," Description Just ran in to this trying to get code working. Said code's documentation referenced `xla_gpu_enable_async_collective*`, however it wasn't working so I came here to check documentation, to discover that it is confirmed at: https://github.com/jaxml/jax/blob/dda6b88864905987480c493db27ce9ede1699f10/docs/gpu_performance_tips.md?plain=1L49C1L49C47  Poked around for a while trying to figure out why the flag was being rejected until I found: https://github.com/openxla/xla/commit/4298a065f7b8381d690a95fe6d437cff037b3ff4 noting that the collection of `gpu_enable_async` flags are now deprecated for `disable` flags instead. Just figured fixing it here will save others the debugging trouble  the instruction lurking in other's codebases notwithstanding.  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.37 jaxlib: 0.4.36 numpy:  2.2.0 python: 3.12.7 (main, Nov  6 2024, 18:29:01) [GCC 14.2.0] device info: NVIDIA RTX 2000 Ada Generation Laptop GPU1, 1 local devices"" process_count: 1 platform: uname_result(system='Linux', node='mymachine', release='6.11.09generic', version=' CC(Adding quickstart notebook, and corresponding gitignore rules)Ubuntu SMP PREEMPT_DYNAMIC Mon Oct 14 13:19:59 UTC 2024', machine='x86_64') $ nvidiasmi Thu Dec 12 17:31:04 2024        ++  ++ ```",2024-12-12T06:33:00Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/25430,Thanks for the catch! I made a PR to remove the deprecated flags from the docs.,PR has been merged; closing.
rag,Add a simple ragged all-to-all Pallas TPU kernel.,Add a simple ragged alltoall Pallas TPU kernel.,2024-12-11T20:42:39Z,,open,0,0,https://github.com/jax-ml/jax/issues/25421
llm,jax.lax: raise TypeError for mismatched dtypes,"This currently results in a verbose jaxpr verifyer error. It looks like this regressed in CC(fix pow_p jvp rule at x=0. y=0). Current behavior: ```python >>> jax.lax.add(0, 0.0) ... ValueError: Cannot lower jaxpr with verifier errors: 	op requires the same element type for all operands and results 		at loc(""jit(add)/jit(main)/add""(callsite(""""("""":1:0) at callsite(""InteractiveShell.run_code""(""/lib/python3.12/sitepackages/IPython/core/interactiveshell.py"":3577:20) at callsite(""InteractiveShell.run_ast_nodes""(""/lib/python3.12/sitepackages/IPython/core/interactiveshell.py"":3517:19) at callsite(""InteractiveShell.run_cell_async""(""/lib/python3.12/sitepackages/IPython/core/interactiveshell.py"":3334:29) at callsite(""_pseudo_sync_runner""(""/lib/python3.12/sitepackages/IPython/core/async_helpers.py"":129:8) at callsite(""InteractiveShell._run_cell""(""/lib/python3.12/sitepackages/IPython/core/interactiveshell.py"":3130:21) at callsite(""InteractiveShell.run_cell""(""/lib/python3.12/sitepackages/IPython/core/interactiveshell.py"":3075:21) at callsite(""TerminalInteractiveShell.interact""(""/lib/python3.12/sitepackages/IPython/terminal/interactiveshell.py"":910:20) at callsite(""TerminalInteractiveShell.mainloop""(""/lib/python3.12/sitepackages/IPython/terminal/interactiveshell.py"":917:16) at ""TerminalIPythonApp.start""(""/lib/python3.12/sitepackages/IPython/terminal/ipapp.py"":317:12)))))))))))) Define JAX_DUMP_IR_TO to dump the module. ``` Behavior after this PR ```python >>> jax.lax.add(0, 0.0) ... TypeError: lax.add requires arguments to have the same dtypes, got int32, float32. (Tip: jnp.add is a similar function that does automatic type promotion on inputs). ```",2024-12-11T20:02:46Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/25419
yi,Replace np.array() with np.fromfile() to improve performance,"This line of code first copies the file stream fh to a Python object and then immediately copies it to a NumPy object, which makes the first copy useless. I propose to remove the first copy by directly copying fh to a NumPy object. The patch is as follows.  +return np.array(array.array(""B"", fh.read()), dtype=np.uint8)  return np.fromfile(fh.read(), dtype=np.uint8)",2024-12-11T19:11:13Z,enhancement,open,0,0,https://github.com/jax-ml/jax/issues/25418
yi,Latency Hiding Scheduler not working with jax 0.4.35," Description More of an XLA problem (rather than Jax). It looks like the latency hiding scheduler is not working correctly in jax 0.4.35. I am running my code on a single DGX node with 8 H100. I am only passing the following flags to XLA: `XLA_FLAGS=xla_gpu_enable_latency_hiding_scheduler=true xla_gpu_enable_triton_gemm=false` During compilation of my code, I get the following error: ``` F external/xla/xla/service/gpu/gpu_hlo_schedule.cc:475] Check failed: (config.collective_broadcast_overlap_limit <= config.parallel_collective_overlap_limit) && (config.all_to_all_overlap_limit <= config.parallel_collective_overlap_limit) && (config.all_gather_overlap_limit <= config.parallel_collective_overlap_limit) && (config.all_reduce_overlap_limit <= config.parallel_collective_overlap_limit) && (config.reduce_scatter_overlap_limit <= config.parallel_collective_overlap_limit) ``` I also tried to modify the value for `config.parallel_collective_overlap_limit` to check if it would make any difference by modifying the `XLA_FLAGS` var to: ``` ENV XLA_FLAGS=""xla_gpu_enable_latency_hiding_scheduler=true xla_gpu_enable_triton_gemm=false xla_gpu_experimental_parallel_collective_overlap_limit=2"" ``` but in that case I get: ``` Unknown flags in XLA_FLAGS: xla_gpu_experimental_parallel_collective_overlap_limit=2 xla_gpu_experimental_parallel_collective_overlap_limit=2 ``` If I downgrade to jax 0.4.34, the error does not occur.   System info (python version, jaxlib version, accelerator, etc.) jax: 0.4.35 jaxlib: 0.4.34 numpy: 2.0.2 python: 3.11.11 (main, Dec 6 2024, 20:02:44) [Clang 18.1.8 ]",2024-12-11T13:59:43Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/25404,"Thanks for the report! It's possible that someone on this repo will be able to provide suggestions, but (as you mention!) you might get more milage asking this question on the XLA repository if you haven't already: https://github.com/openxla/xla/issues"
yi,[Pallas] Remove `grid=1` in tests,[Pallas] Remove `grid=1` in tests Remove `grid=1` in tests because it's the same as not specifying `grid`.,2024-12-11T13:22:20Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25402
yi,XLA-introduced copies supersede `lax.optimization_barrier`," Description Almost certainly an XLA bug and happy to report there if so. Consider the function ```python (jit, donate_argnums=0) def f(x: Array) > tuple[Array, Array]:     y = x[0, 0]     x = x.at[0, 0].add(1)     return x, y ``` Since XLA has control over scheduling, for efficiency it should schedule the slice first and then the inplace update, to avoid an unnecessary copy. However, on specifically the CPU backend it chooses to copy twice instead, generating ``` ENTRY %main.13 (Arg_0.1: f32[10000,10000]) > (f32[10000,10000], f32[]) {   %Arg_0.1 = f32[10000,10000]{1,0} parameter(0), metadata={op_name=""x""}   %copy.1 = f32[10000,10000]{1,0} copy(f32[10000,10000]{1,0} %Arg_0.1)   %copy = f32[10000,10000]{1,0} copy(f32[10000,10000]{1,0} %copy.1)   %add_dynamicupdateslice_fusion = f32[10000,10000]{1,0} fusion(f32[10000,10000]{1,0} %copy), kind=kLoop, calls=%fused_computation.1, metadata={op_name=""jit(g)/jit(main)/scatteradd"" source_file=""..."" source_line=30}   %slice_bitcast_fusion = f32[] fusion(f32[10000,10000]{1,0} %copy.1), kind=kLoop, calls=%fused_computation, metadata={op_name=""jit(g)/jit(main)/squeeze"" source_file=""..."" source_line=29}   ROOT %tuple.4 = (f32[10000,10000]{1,0}, f32[]) tuple(f32[10000,10000]{1,0} %add_dynamicupdateslice_fusion, f32[] %slice_bitcast_fusion) } ``` (I'm not sure why it needs to make _two_ copies here instead of just one, but the important part is that it copies at all.) By the semantics of `lax.optimization_barrier`, I would expect that introducing an explicit dependency of `x` on `y` would force the slice to happen first, and then the liveliness analysis will kick in and remove the copies. ```python (jit, donate_argnums=0) def f(x: Array) > tuple[Array, Array]:     y = x[0, 0]     x, y = lax.optimization_barrier((x, y))     x = x.at[0, 0].add(1)     return x, y ``` However, what ends up happening is XLA still introduces copies and reorders the calls, so the generated code is the same as the one shown above. This seems to violate the scheduling control one expects from `optimization_barrier`. Note that for this particular example, setting the XLA flag `xla_cpu_copy_insertion_use_region_analysis=true` removes the copy and generates ``` ENTRY %main.13 (Arg_0.1: f32[10000,10000]) > (f32[10000,10000], f32[]) {   %Arg_0.1 = f32[10000,10000]{1,0} parameter(0), sharding={replicated}, metadata={op_name=""x""}   %slice_bitcast_fusion = f32[] fusion(f32[10000,10000]{1,0} %Arg_0.1), kind=kLoop, calls=%fused_computation, metadata={op_name=""jit(g)/jit(main)/squeeze"" source_file=""..."" source_line=28}   %add_dynamicupdateslice_fusion = f32[10000,10000]{1,0} fusion(f32[10000,10000]{1,0} %Arg_0.1), kind=kLoop, calls=%fused_computation.1, controlpredecessors={%slice_bitcast_fusion}, metadata={op_name=""jit(g)/jit(main)/scatteradd"" source_file=""..."" source_line=30}   ROOT %tuple.4 = (f32[10000,10000]{1,0}, f32[]) tuple(f32[10000,10000]{1,0} %add_dynamicupdateslice_fusion, f32[] %slice_bitcast_fusion) } ``` as expected, with or without `optimization_barrier`. Also, using a GPU device generates the copyless ``` ENTRY %main.13 (Arg_0.1.0: f32[10000,10000]) > (f32[10000,10000], f32[]) {   %Arg_0.1.0 = f32[10000,10000]{1,0} parameter(0), metadata={op_name=""x""}   %wrapped_slice = f32[1,1]{1,0} fusion(f32[10000,10000]{1,0} %Arg_0.1.0), kind=kLoop, calls=%wrapped_slice_computation   %bitcast.43.0 = f32[] bitcast(f32[1,1]{1,0} %wrapped_slice)   %loop_dynamic_update_slice_fusion = f32[10000,10000]{1,0} fusion(f32[10000,10000]{1,0} %Arg_0.1.0), kind=kLoop, calls=%fused_dynamic_update_slice, controlpredecessors={%wrapped_slice}, metadata={op_name=""jit(g)/jit(main)/scatteradd"" source_file=""..."" source_line=30}   ROOT %tuple.5 = (f32[10000,10000]{1,0}, f32[]) tuple(f32[10000,10000]{1,0} %loop_dynamic_update_slice_fusion, f32[] %bitcast.43.0) } ``` also with or without `optimization_barrier`.  Some miscellaneous related questions 1. Is there a JAX interface to `HloOrdering`, particularly `SequentialHloOrdering` or is that controlled by the XLA flag `xla_cpu_enable_concurrency_optimized_scheduler`? In particular, is there a way of manually writing schedules without relying only on `optimization_barrier` (which is not precise enough in cases like these)? 2. I'm a bit confused why the workaround works now, since region analysis was introduced more than 3 years ago in https://github.com/openxla/xla/commit/92292d14ec208390beb8893a460b527663e1a825.  The core logic of `RemoveUnnecessaryCopies` and `TryElideCopy` hasn't seemed to change much in that time either. Rather, what _has_ recently changed is the flag `xla_cpu_copy_insertion_use_region_analysis` was added to CPU (disabled by default) and region analysis was disabled on GPU. Is there some context I'm missing?      https://github.com/openxla/xla/pull/18521      https://github.com/openxla/xla/pull/14680 (originally reported in the discussion https://github.com/jaxml/jax/discussions/19165.)  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.37 jaxlib: 0.4.36 numpy:  2.1.3 python: 3.12.1 (main, Oct  7 2024, 00:00:00) [GCC 11.4.1 20231218 (Red Hat 11.4.13)] device info: NVIDIA L40S1, 1 local devices"" process_count: 1 platform: uname_result(system='Linux', node='babel65', release='5.14.0427.40.1.el9_4.x86_64', version=' CC(Python 3 compatibility issues) SMP PREEMPT_DYNAMIC Wed Oct 16 07:08:17 EDT 2024', machine='x86_64') $ nvidiasmi Wed Dec 11 04:51:52 2024 ++  ++ ```",2024-12-11T10:50:46Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/25399,"Yes, I think this would be better reported on the XLA github issue tracker. There's current no JAX way to control the HLO schedule, but that's something we're actively looking into adding as a way to control communication/compute overlap.",Opened https://github.com/openxla/xla/issues/20440 on the XLA side. Should this issue be closed?,"Yeah, let's track this there."
yi,jax.full allocates memory on the wrong device," Description When I use jax.numpy.full, it allocates memory on TPU, even when instructed to allocate it on the CPU if the memory requested is smaller then the free memory on the TPU, the memory is deallocated and reallocated on the CPU instead however if the memory requested is more than the free memory, the device crashes with OOM in my snippet, it's an extremely edge case where I am trying to allocate 40GB, which normally never happens, however, during training and execution, allocating jax buffers on CPU may be the difference for some training pipelines fitting withing specific memory constraints or not. snippet: ``` import jax import jax.numpy as jnp cpu = jax.devices('cpu')[0] a = jnp.full(shape=(40*(2**30)), fill_value=1.0, dtype=jnp.uint8, device=cpu) ```  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.37 jaxlib: 0.4.36 numpy:  1.26.4 python: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] device info: TPU v6 lite4, 4 local devices"" process_count: 1 platform: uname_result(system='Linux', node='t1vn7691bba4w0', release='6.8.01015gcp', version=' CC(Implement np.repeat for scalar repeats.)~22.04.1Ubuntu SMP Tue Sep  3 16:11:52 UTC 2024', machine='x86_64')",2024-12-11T08:34:09Z,bug,open,0,2,https://github.com/jax-ml/jax/issues/25396,"I think, the best solution here is to do the following to make sure all ops inside `jnp.full` run on CPU: ``` with jax.default_device(jax.devices('cpu')[0]):   a = jnp.full(shape=(40*(2**30)), fill_value=1.0, dtype=jnp.uint8)   print(a)  any op outside the scope will run on the TPU/GPU ```","> I think, the best solution here is to do the following to make sure all ops inside `jnp.full` run on CPU: >  > ``` > with jax.default_device(jax.devices('cpu')[0]): >   a = jnp.full(shape=(40*(2**30)), fill_value=1.0, dtype=jnp.uint8) >   print(a) >  any op outside the scope will run on the TPU/GPU > ``` That's a very good workaround! However I think the behaviour of jnp.full and similar API needs to be more faithful to their arguments then their global environments, especially if it can lead to OOM crashes"
llm, Unable to parse backend config for custom call: Could not convert JSON string to proto: Expected : between key:value pair.," Description The following test code produces the aforementioned error when using JAX v0.4.37 (its works error free before this). This is running on Ubuntu 24.04 with Python 3.12 and using CUDA 12.4 ``` def compute_svd_rot(m: jnp.ndarray, eps=1e10) > jnp.ndarray:     """"""Maps 3x3 matrices onto SO(3) via symmetric orthogonalization.     Source: Google research  https://github.com/googleresearch/googleresearch/blob/193eb9d7b643ee5064cb37fd8e6e3ecde78737dc/special_orthogonalization/utils.pyL93L115     """"""     """"""     m = jax.lax.cond(jnp.linalg.matrix_rank(m) < 3,                      true_fun=lambda x: x + jnp.eye(3) * 1e10,                      false_fun=lambda x: x,                      operand=m)     """"""     m_reg = m + jnp.eye(3) * eps     U, _, Vh = jnp.linalg.svd(m_reg, full_matrices=False)     det = jnp.linalg.det(jnp.matmul(U, Vh))     return jnp.matmul(jnp.c_[U[:, :1], U[:, 1] * det], Vh) jit_compute_svd_rot = jit(compute_svd_rot)  test compute_svd_rot, input is a (3,3) array test_input = jnp.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]) test_output = jit_compute_svd_rot(test_input) print(test_output) ``` ``` 20241210 23:49:08.560387: W external/xla/xla/service/gpu/ir_emitter_unnested.cc:1171] Unable to parse backend config for custom call: Could not convert JSON string to proto: Expected : between key:value pair. = true, full_matrice ^ Fall back to parse the raw backend config str. ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.37 jaxlib: 0.4.36 numpy:  1.26.4 python: 3.12.2  (main, Feb 16 2024, 20:50:58) [GCC 12.3.0] device info: NVIDIA GeForce RTX 40901, 1 local devices"" Driver Version: 550.135        CUDA Version: 12.4   ```",2024-12-10T23:54:02Z,bug,open,1,4,https://github.com/jax-ml/jax/issues/25389,"Thanks for the report. This is known and I believe that's a warning, not an error. Are you seeing a runtime exception somewhere, or just extra logging?  ",Just the warning / extra logging.,"Hi,  Is there any way to suppress this warning?","This should be fixed in the nightly jaxlib. I don't know if there a way to suppress the warning otherwise, unfortunately."
text chunking,Pallas Kernel Expected Output Shape Error Using Grids On TPU," Description  🐛 Bug I am trying to write a custom Pallas kernel to use it in TPU. I am using blocking method to keep my kernel from going OOM. However, when I am using grids, it seems that I get kernel problems with the expected output and input shapes. It seems that the chunking / splitting of the input does not perform as expected. I checked that my code indeed has the right shapes, grid and indexing method. However, the kernel itself is getting wrong input.  I think it may be bug in how the TPU is handling the chunking in pallas kernels, but I am not sure. Any help would be appreciated!  To Reproduce I am attaching here tests for replication. You can see that only the tests with original input tensors larger than block size fails.  My Kernel Code ```   .jit   def round_down_and_up(x: jax.Array) > (jax.Array, jax.Array):       """"""       Simplified wrapper for kernel execution, treating x as a vector.       Handles explicit padding and alignment with TPU tiling constraints.       """"""       block_size = 128        padded_length = (original_length + block_size  1) // block_size * block_size                Explicitly pad the input tensor        if original_length != padded_length:            x = jnp.pad(x, (0, padded_length  original_length), mode=""constant"", constant_values=0)        Define block shape and grid       block_shape = (128,)   TPU requires blocks divisible by 128 for f32       grid = (len(x) + block_size  1) // block_shape[0]        Define BlockSpec       block_spec = pl.BlockSpec(block_shape=block_shape, index_map=lambda i: (i,))        Define output shape       out_shape = jax.ShapeDtypeStruct(x.shape, x.dtype)         Debugging: Verify padded shape, block shape, and grid        jax.debug.print(""Input Length: {input_length}, Padded Length: {padded_length}, ""                        ""BlockShape: {block_shape}, Grid: {grid}"",                        input_length=original_length, padded_length=padded_length,                        block_shape=block_shape, grid=grid)       print(f""Out shape: {out_shape}, grid: {grid}, block_shape: {block_shape}"")        Call the kernel       x_low, x_high = pl.pallas_call(           round_down_and_up_bfloat16_kernel,           out_shape=(out_shape, out_shape),           grid=(grid,),           in_specs=(block_spec,),   Input tiling specification           out_specs=(block_spec, block_spec),   Output tiling specification       )(x)       return x_low, x_high ```  Debug Output And Stack Trace RuntimeError: Bad StatusOr access: INVALID_ARGUMENT: Mosaic failed to compile TPU kernel: Failed to verify layout for Mosaic kernel operand 1: XLA layout does not match MLIR layout for an operand of shape f32[192]: expected {0:T(128)}, got {0:T(256)} But printing the values I provide to the pallas_call Out shape: ShapeDtypeStruct(shape=(192,), dtype=float32), grid: 2, block_shape: (128,)  Expected behavior The tests should not fail. When run on GPU they all pass.  Environment   Reproducible on XLA backend [CPU/TPU/CUDA]: TPU   torch_xla version: 2.4  Additional context The tests for easy replication: test_pallas_tiling.py.zip  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.33 jaxlib: 0.4.33 numpy:  1.26.4 python: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] jax.devices (4 total, 4 local): [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0) TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0) TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0) TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)] process_count: 1 platform: uname_result(system='Linux', node='t1vnaacdf60cw0', release='5.19.01027gcp', version=' CC(Add support for `np.trace` )~22.04.1Ubuntu SMP Thu Jun 22 05:13:17 UTC 2023', machine='x86_64')",2024-12-10T15:18:25Z,bug type:Bug,open,0,3,https://github.com/jax-ml/jax/issues/25379,"I think this is actually related to handling of 1D arrays in Pallas. Usually, arrays in our kernel are 2D. I'm going to need some time to track down what exactly the problem is, but your code should run if you add a leading (1,) dimension to the block shape and input array shape.","> I think this is actually related to handling of 1D arrays in Pallas. Usually, arrays in our kernel are 2D. >  > I'm going to need some time to track down what exactly the problem is, but your code should run if you add a leading (1,) dimension to the block shape and input array shape. Ok it resolved the problem! However, I notice that training is much slower when using the kernels. The kernels perform bitwise operations. Could it be that these bitwise operations are being executed on the CPU, even though I wrapped them in a kernel and further wrapped it in torch_xla? Or do I need to change something in my function for it to be more optimized for tpus?  ``` .jit def round_down_and_up(x: jax.Array) > (jax.Array, jax.Array):     """"""     Simplified wrapper for kernel execution, treating x as a vector.     Handles explicit padding and alignment with TPU tiling constraints.     """"""     block_size = 512      Define block shape and grid      adding leading 1, to block_shape to make it 2D. TPU requires blocks divisible by 128 for f32     block_shape = (1, block_size)     grid = (x.shape[1] + block_size  1) // block_shape[1]     grid = (1, grid)      Define BlockSpec     index_map_for_2d = lambda i, j: (i, j)     block_spec = pl.BlockSpec(block_shape=block_shape, index_map=index_map_for_2d)      Define output shape     out_shape = jax.ShapeDtypeStruct(x.shape, x.dtype)      print(f""Input shape: {x.shape}"")      print(f""Out shape: {out_shape}, grid: {grid}, block_shape: {block_shape}"")           print(f""Out shape: {out_shape}, grid: {grid}, block_shape: {block_shape}"")      Call the kernel     x_low, x_high = pl.pallas_call(         round_down_and_up_bfloat16_kernel,         out_shape=(out_shape, out_shape),          out_shape = (jax.ShapeDtypeStruct(x.shape, x.dtype), jax.ShapeDtypeStruct(x.shape, x.dtype)),          grid=(grid,),         grid=grid,         in_specs=(block_spec,),   Input tiling specification         out_specs=(block_spec, block_spec),   Output tiling specification     )(x)     return x_low, x_high ```","Pallas lowers to Mosaic which doesn't have a CPU backend, so it's impossible that it runs on CPU unintentionally. The only way to run on CPU is to pass in interpret=True, but in that case you wouldn't see Mosaic error messages. Without seeing the rest of your code, my guess is that it's related to fusion. When you run your code without custom kernels, XLA will automatically try to fuse neighboring ops together into it's own generated kernels so it can avoid memory copies between HBM  VMEM. However, when you use a pallas_call, XLA doesn't know how to fuse into a custom kernel, so you get stuck with redundant copies. Since your kernel is elementwise, it's probably memorybound, meaning that most of the time spent in the kernel is waiting for memory copies to finish and not actually doing computation. So fusion would actually help a lot here if this is the case. The solution to this problem is to fold in the neighboring operations into the kernel. For example, if you're rounding followed by a bf16 matmul, then you should do the rounding inside the matmul kernel and not as a standalone kernel."
rag,Introduce `lax.ragged_all_to_all` primitive,"Introduce `lax.ragged_all_to_all` primitive This version emits a StableHLO custom call. The test outputs the following MLIR module: ``` module  {   func.func public (%arg0: tensor, %arg1: tensor, %arg2: tensor, %arg3: tensor, %arg4: tensor, %arg5: tensor) > (tensor) {     %0 = stablehlo.custom_call (%arg0, %arg1, %arg2, %arg3, %arg4, %arg5) {api_version = 4 : i32, backend_config = {replica_groups = dense : tensor}} : (tensor, tensor, tensor, tensor, tensor, tensor) > tensor     return %0 : tensor   } } ``` For now, the API assumes `split_axis` and `concat_axis` of `all_to_all` to be the outermost (ragged) dim, and `axis_index_groups` is default to all replicas (e.g. there is only one group and covers all axis indices aka iota like the example above). The current API is inspired from https://www.mpich.org/static/docs/v3.1/www3/MPI_Alltoallv.html which essentially also does a ragged all to all.",2024-12-10T01:57:57Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25370
yi,vmap support for optimization barrier,"Calling jax.lax.optimization_barrier inside of a vmap'd function yields: `NotImplementedError: Batching rule for 'optimization_barrier' not implemented` This would be superuseful if supported, thanks!",2024-12-10T00:13:55Z,enhancement,closed,0,1,https://github.com/jax-ml/jax/issues/25365,Fixed by https://github.com/jaxml/jax/commit/944d822ce64450f698bd9b4e8236421ade401e84
rag,Adding more tests for multi-head attention,Adding more test coverage to the multihead attention pallas kernel. jax==0.4.33 caused the kernel to segfault with an untested configuration and it went undetected.,2024-12-09T20:55:40Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/25361
rag,Add utility script and env for running the CI scripts under Docker,"Add utility script and env for running the CI scripts under Docker This commit adds the scripts and envs that set up a Docker container named ""jax"". It is used in internal JAX CI jobs that handle building and publishing JAX artifacts to PyPI and/or GCS. While GitHub action workflows use the same Docker images, they do not run `run_docker_container.sh` script as they leverage builtin containerization features to run jobs within a container.",2024-12-09T17:52:32Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25356
rag,[mosaic_gpu] Allow calling `reduce_sum` on a fragmented array in splat layout,[mosaic_gpu] Allow calling `reduce_sum` on a fragmented array in splat layout,2024-12-09T14:26:40Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25343
rag,"[Jax][Pallas][Mosaic] Implement platform dependent diag, with branch selection driven by constant prop in mosaic lowering.","[Jax][Pallas][Mosaic] Implement platform dependent diag, with branch selection driven by constant prop in mosaic lowering. This CL builds out a simple sketch of constant prop by construction in mosaic  we walk the graph up from cond, collecting the values and either const propping or failing out of const prop. Failure out of const prop is not a bug, but hitting an unimplemented const prop func is for now, in order to drive better coverage.  This then allows us to pick a single branch, and ignore branches which do not have a viable mosaic implementation. And, finally, for diag, this means we can replace the initial gatherdependent implementation in lax with a mosaic specific one that avoids gather.",2024-12-06T23:38:15Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25324
rag,[mosaic_gpu] Emit a slightly more informative error message in `FragmentedArray._pointwise`,[mosaic_gpu] Emit a slightly more informative error message in `FragmentedArray._pointwise`,2024-12-06T13:51:43Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25312
rag,[pallas:mosaic_gpu] `FragmentedArray.reduce_sum` now returns a `FragmentedArray`,[pallas:mosaic_gpu] `FragmentedArray.reduce_sum` now returns a `FragmentedArray` This aligns it with the `reduce` method and also makes it clear that the reduction always produces a scalar.,2024-12-06T11:43:35Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25305
yi,[pallas] fix jumble test flakiness,"[pallas] fix jumble test flakiness * Enable interpret mode in tests * Ensure that the kernel is run multiple times where weve seen data corruption * Use masked comparison  prior comparison was reading garbage data as we were basically relying on past behavior of how uninitialized memory was behaving. * This was being hidden by a cache, where the interpret test, which always has 0.0 for uninitialized memory was being hit first, where TPU does not have the same behavior.",2024-12-05T22:14:07Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25295
rag,array API: improve test coverage,"`max_examples` defaults to 200; previously there were some performance issues in the test suite that led to timeouts with the full run, so we limited it to 5 examples. That seems to have been fixed, and we can now do the full test run in about 3 minutes of CI time.",2024-12-05T20:35:34Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/25294
yi,unexpected `vmap` error due to commit `c36e1f7`," Description Hi,   and I noticed a bug in our tests here that occurs in the latest JAX version. After doing gitbisect, we found the bad commit to be: c36e1f7c1ad4782060cbc8e8c596d85dfb83986f. Here's the traceback with `JAX_TRACEBACK_FILTERING=off`:  ```pytb self = , rng = Array((), dtype=key) overlaying: [0 0], lse_mode = False, tau_a = 0.93, tau_b = 0.91, arg = 1, lineax_ridge = 1e05     .mark.fast.with_args(         ""lse_mode,tau_a,tau_b,arg,lineax_ridge"", (             (True, 1.0, 1.0, 0, 0.0),             (False, 1.0, 1.0, 0, 1e8),             (True, 1.0, 1.0, 1, 0.0),             (True, 1.0, 0.91, 0, 1e7),             (True, 0.93, 0.91, 1, 0.0),             (False, 0.93, 0.91, 1, 1e5),         ),         only_fast=1     )     def test_hessian_sinkhorn(         self, rng: jax.Array, lse_mode: bool, tau_a: float, tau_b: float,         arg: int, lineax_ridge: float     ):       """"""Test hessian w.r.t. weights and locations.""""""       try:         from ott.solvers.linear import lineax_implicit   noqa: F401         test_back = True         ridge = lineax_ridge       except ImportError:         test_back = False         ridge = 1e5       n, m = (12, 15)       dim = 3       rngs = jax.random.split(rng, 6)       x = jax.random.uniform(rngs[0], (n, dim))       y = jax.random.uniform(rngs[1], (m, dim))       a = jax.random.uniform(rngs[2], (n,)) + 0.1       b = jax.random.uniform(rngs[3], (m,)) + 0.1       a = a / jnp.sum(a)       b = b / jnp.sum(b)       epsilon = 0.1        Add a ridge when using JAX solvers, smaller ridge for lineax solvers       solver_kwargs = {           ""ridge_identity"": ridge,           ""ridge_kernel"": ridge if tau_a == tau_b == 1.0 else 0.0       }       imp_dif = implicit_lib.ImplicitDiff(solver_kwargs=solver_kwargs)       def loss(a: jnp.ndarray, x: jnp.ndarray, implicit: bool = True):         geom = pointcloud.PointCloud(x, y, epsilon=epsilon)         prob = linear_problem.LinearProblem(geom, a, b, tau_a, tau_b)         implicit_diff = imp_dif if implicit else None         solver = sinkhorn.Sinkhorn(             lse_mode=lse_mode,             threshold=1e4,             use_danskin=False,             implicit_diff=implicit_diff,         )         return solver(prob).reg_ot_cost       delta_a = jax.random.uniform(rngs[4], (n,))       delta_a = delta_a  jnp.mean(delta_a)       delta_x = jax.random.uniform(rngs[5], (n, dim))       hess_loss_imp = jax.jit(           jax.hessian(lambda a, x: loss(a, x, True), argnums=arg)       ) >     hess_imp = hess_loss_imp(a, x) tests/solvers/linear/sinkhorn_diff_test.py:794:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ../jaxlatest/jax/_src/pjit.py:337: in cache_miss     outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked = _python_pjit_helper( ../jaxlatest/jax/_src/pjit.py:177: in _python_pjit_helper     p, args_flat = _infer_params(fun, jit_info, args, kwargs) ../jaxlatest/jax/_src/pjit.py:743: in _infer_params     p, args_flat = _infer_params_impl( ../jaxlatest/jax/_src/pjit.py:640: in _infer_params_impl     jaxpr, consts, out_avals, attrs_tracked = _create_pjit_jaxpr( ../jaxlatest/jax/_src/linear_util.py:345: in memoized_fun     ans = call(fun, *args) ../jaxlatest/jax/_src/pjit.py:1287: in _create_pjit_jaxpr     jaxpr, global_out_avals, consts, attrs_tracked = pe.trace_to_jaxpr_dynamic( ../jaxlatest/jax/_src/profiler.py:333: in wrapper     return func(*args, **kwargs) ../jaxlatest/jax/_src/interpreters/partial_eval.py:2160: in trace_to_jaxpr_dynamic     ans = fun.call_wrapped(*in_tracers) ../jaxlatest/jax/_src/linear_util.py:191: in call_wrapped     ans = self.f(*args, **dict(self.params, **kwargs)) ../jaxlatest/jax/_src/api.py:581: in jacfun     y, jac = vmap(pushfwd, out_axes=(None, 1))(_std_basis(dyn_args)) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ args = ((Tracedwith,),), kwargs = {}, args_flat = [Tracedwith], in_tree = PyTreeDef((((*,),), {})) f = Wrapped function: Core: functools.partial(, Wrapped function: 0   : _argnums_partial   ...il.Unhashable object at 0x12806f520>,)) Core:  , (Tracedwith,)) flat_fun = Wrapped function: 0   : flatten_fun_for_vmap   (PyTreeDef((((*,),), {})),) Core: functools.partial(,)) Core:  , (Tracedwith,)) in_axes_flat = [0], axis_size_ = 36, axis_data = AxisData(name=, size=36, spmd_name=None), out_axes_flat = [None, 1]     (fun, docstr=docstr)          def vmap_f(*args, **kwargs):       if isinstance(in_axes, tuple) and len(in_axes) != len(args):         raise ValueError(""vmap in_axes must be an int, None, or a tuple of entries corresponding ""                          ""to the positional arguments passed to the function, ""                          f""but got {len(in_axes)=}, {len(args)=}"")       args_flat, in_tree  = tree_flatten((args, kwargs), is_leaf=batching.is_vmappable)       f = lu.wrap_init(fun)       flat_fun, out_tree = batching.flatten_fun_for_vmap(f, in_tree)       in_axes_flat = flatten_axes(""vmap in_axes"", in_tree, (in_axes, 0), kws=True)       axis_size_ = (axis_size if axis_size is not None else                     _mapped_axis_size(fun, in_tree, args_flat, in_axes_flat, ""vmap""))       try:         axis_data = batching.AxisData(axis_name, axis_size_, spmd_axis_name)         out_flat = batching.batch(             flat_fun, axis_data, in_axes_flat,             lambda: flatten_axes(""vmap out_axes"", out_tree(), out_axes)         ).call_wrapped(*args_flat)       except batching.SpecMatchError as e:         out_axes_flat = flatten_axes(""vmap out_axes"", out_tree(), out_axes)         out_axes_full = tree_unflatten(out_tree(), out_axes_flat)         pairs, _ = tree_flatten_with_path(out_axes_full, is_leaf=lambda x: x is None)         path, _ = pairs[e.leaf_idx] >       raise ValueError(f'at vmap out_axes{keystr(path)}, got axis spec {e.dst} '                          f'but output was batched on axis {e.src}') from None E       ValueError: at vmap out_axes[0], got axis spec None but output was batched on axis 0 ../jaxlatest/jax/_src/api.py:1003: ValueError ```  It might be that the bug is coming from transformations created by lineax, as the test doesn't fail when using the CG solver from JAX, (the test still fails, but only because of the precision, not the above `ValueError`). Code to reproduce: ```bash git clone https://github.com/ottjax/ott/ && cd ott && pip install e'.[test]' pip install git+https://github.com/jaxml/jax.git pytest k 'test_hessian_sinkhorn[False0.930.9111e05]' ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.36.dev20241029+c36e1f7c1 jaxlib: 0.4.35 numpy:  2.0.2 python: 3.11.10  (main, Oct 16 2024, 01:26:25) [Clang 17.0.6 ] device info: cpu1, 1 local devices"" process_count: 1 platform: uname_result(system='Darwin', node='MichalsMacBookPro3.local', release='24.1.0', version='Darwin Kernel Version 24.1.0: Thu Oct 10 21:03:15 PDT 2024; root:xnu11215.41.3~2/RELEASE_ARM64_T6000', machine='arm64') ```",2024-12-05T17:22:41Z,bug,open,0,10,https://github.com/jax-ml/jax/issues/25289,Thanks for the clear report!," could you minimize this? There's a lot going on in this code that we aren't familiar with, and it's much harder for us to minimize unfamiliar code than for you to minimize your own code. Think of the time for us to debug this as exponential in the length of the repro you give us. > It might be that the bug is coming from transformations created by lineax, Are any of lineax's tests failing?","> Are any of lineax's tests failing? No, all of them are passing with the latest version of JAX/lineax.","thanks a lot   for taking a look! Here's a simpler example crafted by  and Rythme demonstrating the problem, which arises indeed from `lineax` (tagging kidger ), leading to the same `vmap` error: ``` import jax import jax.numpy as jnp import lineax as lx .custom_vjp def f(x):   return x.sum() def f_fwd(x):   return x.sum(), (x,) def f_bwd(res, g):   x, = res   A, b = jnp.eye(x.shape[0]), jnp.ones_like(x)    op = lx.FunctionLinearOperator(lambda x: x, input_structure=x)   fails        op = lx.MatrixLinearOperator(A)   fails   r = lx.linear_solve(op, b).value   return (r * x * g,) f.defvjp(f_fwd, f_bwd) rng = jax.random.key(0) x = jax.random.normal(rng, (10,)) jax.hessian(f)(x) ```","Yeah, I've also been seeing widespread failures in Diffrax's test suite, due to what looks like a totally different vmap failure. I've spent most of today digging through this and haven't identified a root cause yet. It might take a while to update the JAX ecosystem to be compatible with this version of JAX.","Okay, I think I've identified the root cause: with the latest changes, the batch interpreter has become a dynamic trace, i.e. it calls batch rules when it previously wouldn't. This meant that a lot of arrays were having their nonbatch dimensions now being turned into batch dimensions! With that problem identified it's been a relatively simple matter to update a couple of batching rules in Equinox to handle this new calling case appropriately.   can you try https://github.com/patrickkidger/equinox/pull/907 on your full example / on your tests? If it passes then I'll do a new release of Equinox that is compatible with latest JAX.","Nice find kidger ! That’s right, actually everything is a dynamic tracer now. No more automatic rulesonlycalledbasedondatadependence, though rules themselves can choose to behave based on dependence. I believe it gives rules strictly more power/expressiveness. ","Hey , can we consolidate this kind of knowledge into an updated version of Autodidax?",">   can you try https://github.com/patrickkidger/equinox/pull/907 on your full example / on your tests? If it passes then I'll do a new release of Equinox that is compatible with latest JAX. Works great, thanks!"," yes, good idea, that’s our plan! Cc  "
yi,Python crashes when trying to compute the gradient of jnp.take," Description When running the following script: ```python from jax import grad, numpy as jnp def loss_fn(W):     return jnp.mean(jnp.take(W, 0, axis=1)) grads = grad(loss_fn)(jnp.arange(3.0).reshape(3, 1)) ``` Python crashes with the following error: ```bash [1]    85044 IOT instruction (core dumped)  poetry run python ``` I ran into this error while trying to use `optax`'s `softmax_cross_entropy_with_integer_labels` and found that `jnp.take` on this line was causing the error.  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.35 jaxlib: 0.4.34 numpy:  1.26.4 python: 3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0] device info: NVIDIA GeForce RTX 40901, 1 local devices"" process_count: 1 platform: uname_result(system='Linux', node='sisreljongeuimachine', release='6.8.049generic', version=' CC(Update neural_network_and_data_loading.ipynb)Ubuntu SMP PREEMPT_DYNAMIC Mon Nov  4 02:06:24 UTC 2024', machine='x86_64')                                                                                                                                                                                                                                                                                                                                                              $ nvidiasmi Thu Dec  5 21:18:28 2024                                                                                                              ++  ++ ```",2024-12-05T12:20:49Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/25284,"I'm unable to reproduce this locally or on Colab, in either a CPU or GPU (T4) runtime. It's possibly related to the particular GPU hardware you're using, but I'm not sure how to diagnose that. Does this reproduce for other jax/jaxlib versions?","The bug disappeared when I downgraded `jax` to `0.4.34`.  By the way, I forgot to mention this warning message before the core dumped line. ```bash 20241206 03:05:35.451686: F external/xla/xla/service/scatter_determinism_expander.cc:70] Check failed: scatter_shape.dimensions_size() == 1 (2 vs. 1) ```",Can you try on the newly released jax 0.4.36?,It works now. Thank you.
yi,[export] Removed __gpu$xla.gpu.triton (Pallas GPU) from the list of custom calls with guaranteed compatibility.,[export] Removed __gpu$xla.gpu.triton (Pallas GPU) from the list of custom calls with guaranteed compatibility. This is because the underlying Triton IR does not guarantee compatibility.,2024-12-05T10:24:13Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25279
rag,[Mosaic GPU] Add an initial skeleton for a layout inference pass.,"[Mosaic GPU] Add an initial skeleton for a layout inference pass. Layouts are added as annotations on MLIR ops, using the `in_layouts` and `out_layouts` attributes. At this point, layout inference is done in two passes: one ""backwards"" pass (roottoparameters), and one ""forward"" pass (parameterstoroot). Each pass goes through all the ops in the specified order, and infers a possible layout from the layout information that is available. We expect to need two passes because partial layout annotations may be provided on intermediate nodes (e.g. `wgmma`), and a single pass from the root to the parameters is therefore insufficient to properly annotate all the operations. We do not perform any check as to whether the inferred layouts can be further lowered correctlymeaning that the produced IR can possibly fail to lower later. Layouts are only inferred for ops involving at least one operand or result of type `VectorType`/`RankedTensorType`. When layouts can't be inferred for an op that should have them, we default to annotating it with strided fragmented layouts.",2024-12-04T18:07:57Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25258
yi,[shape_poly] Remove the deprecated PolyShape object for specifying symbolic dimensions,[shape_poly] Remove the deprecated PolyShape object for specifying symbolic dimensions PolyShape has been deprecated in January 2024. The constructor has been raising a DeprecationWarning since then.,2024-12-04T16:55:38Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25256
yi,Improve trace-time performance of jnp.isscalar,"This avoids calling `device_put` on nonjax values within `isscalar`. This should improve trace time for `isscalar` and functions that use it, such as array indexing. Before: ```python In [1]: import jax In [2]: %timeit jax.numpy.isscalar(1) 23.9 μs ± 162 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each) ``` After: ```python In [1]: import jax In [2]: %timeit jax.numpy.isscalar(1) 108 ns ± 0.59 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each) ``` (This is part of addressing CC(Difference between numpy and jax.numpy in advanced indexing axes order))",2024-12-03T23:44:51Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/25237
yi,Installation fails on a cluster," Description Hello, try to install this package with either pip or uv results in the following error: Do you have some ideas of why it happens? ```python [fvicenti test2]$ uv add tensorstore Resolved 37 packages in 24ms   × Failed to download and build `tensorstore==0.1.69`   ╰─▶ Build backend failed to build wheel through `build_wheel` (exit status: 1)       [stdout]       running bdist_wheel       running build       running build_py       creating /scratch_local/tmphwvzqsjy/lib.linuxx86_64cpython313/tensorstore       copying python/tensorstore/__init__.py > /scratch_local/tmphwvzqsjy/lib.linuxx86_64cpython313/tensorstore       running build_ext       /leonardo/home/userexternal/fvicenti/.cache/uv/buildsv0/.tmpP5IWUg/bin/python u bazelisk.py build c opt copt=O3       //python/tensorstore:_tensorstore__shared_objects verbose_failures remote_download_regex=.*/_tensorstore\.(so|pyd) copt=fvisibility=hidden       [stderr]       WARNING setuptools_scm.pyproject_reading toml section missing 'pyproject.toml does not contain a tool.setuptools_scm section'       Traceback (most recent call last):         File ""/leonardo/home/userexternal/fvicenti/.cache/uv/buildsv0/.tmpP5IWUg/lib/python3.13/sitepackages/setuptools_scm/_integration/pyproject_reading.py"",       line 36, in read_pyproject           section = defn.get(""tool"", {})[tool_name]                     ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^       KeyError: 'setuptools_scm'       Downloading https://releases.bazel.build/7.4.0/release/bazel7.4.0linuxx86_64...       Traceback (most recent call last):         File ""/leonardo/home/userexternal/fvicenti/.local/share/uv/python/cpython3.13.0linuxx86_64gnu/lib/python3.13/urllib/request.py"", line 1319, in do_open           h.request(req.get_method(), req.selector, req.data, headers,           ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                     encode_chunked=req.has_header('Transferencoding'))                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^         File ""/leonardo/home/userexternal/fvicenti/.local/share/uv/python/cpython3.13.0linuxx86_64gnu/lib/python3.13/http/client.py"", line 1336, in request           self._send_request(method, url, body, headers, encode_chunked)           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^         File ""/leonardo/home/userexternal/fvicenti/.local/share/uv/python/cpython3.13.0linuxx86_64gnu/lib/python3.13/http/client.py"", line 1382, in       _send_request           self.endheaders(body, encode_chunked=encode_chunked)           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^         File ""/leonardo/home/userexternal/fvicenti/.local/share/uv/python/cpython3.13.0linuxx86_64gnu/lib/python3.13/http/client.py"", line 1331, in endheaders           self._send_output(message_body, encode_chunked=encode_chunked)           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^         File ""/leonardo/home/userexternal/fvicenti/.local/share/uv/python/cpython3.13.0linuxx86_64gnu/lib/python3.13/http/client.py"", line 1091, in _send_output           self.send(msg)           ~~~~~~~~~^^^^^         File ""/leonardo/home/userexternal/fvicenti/.local/share/uv/python/cpython3.13.0linuxx86_64gnu/lib/python3.13/http/client.py"", line 1035, in send           self.connect()           ~~~~~~~~~~~~^^         File ""/leonardo/home/userexternal/fvicenti/.local/share/uv/python/cpython3.13.0linuxx86_64gnu/lib/python3.13/http/client.py"", line 1477, in connect           self.sock = self._context.wrap_socket(self.sock,                       ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^                                                 server_hostname=server_hostname)                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^         File ""/leonardo/home/userexternal/fvicenti/.local/share/uv/python/cpython3.13.0linuxx86_64gnu/lib/python3.13/ssl.py"", line 455, in wrap_socket           return self.sslsocket_class._create(                  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^               sock=sock,               ^^^^^^^^^^           ......               session=session               ^^^^^^^^^^^^^^^           )           ^         File ""/leonardo/home/userexternal/fvicenti/.local/share/uv/python/cpython3.13.0linuxx86_64gnu/lib/python3.13/ssl.py"", line 1076, in _create           self.do_handshake()           ~~~~~~~~~~~~~~~~~^^         File ""/leonardo/home/userexternal/fvicenti/.local/share/uv/python/cpython3.13.0linuxx86_64gnu/lib/python3.13/ssl.py"", line 1372, in do_handshake           self._sslobj.do_handshake()           ~~~~~~~~~~~~~~~~~~~~~~~~~^^       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1020)       During handling of the above exception, another exception occurred:       Traceback (most recent call last):         File ""/leonardo/home/userexternal/fvicenti/.cache/uv/sdistsv6/pypi/tensorstore/0.1.69/vIw4h3u9GfLy7UhAGmTun/src/bazelisk.py"", line 506, in            sys.exit(main())                    ~~~~^^         File ""/leonardo/home/userexternal/fvicenti/.cache/uv/sdistsv6/pypi/tensorstore/0.1.69/vIw4h3u9GfLy7UhAGmTun/src/bazelisk.py"", line 491, in main           bazel_path = get_bazel_path()         File ""/leonardo/home/userexternal/fvicenti/.cache/uv/sdistsv6/pypi/tensorstore/0.1.69/vIw4h3u9GfLy7UhAGmTun/src/bazelisk.py"", line 484, in get_bazel_path           return download_bazel_into_directory(bazel_version, is_commit, bazel_directory)         File ""/leonardo/home/userexternal/fvicenti/.cache/uv/sdistsv6/pypi/tensorstore/0.1.69/vIw4h3u9GfLy7UhAGmTun/src/bazelisk.py"", line 318, in       download_bazel_into_directory           download(bazel_url, destination_path)           ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^         File ""/leonardo/home/userexternal/fvicenti/.cache/uv/sdistsv6/pypi/tensorstore/0.1.69/vIw4h3u9GfLy7UhAGmTun/src/bazelisk.py"", line 367, in download           with closing(urlopen(request)) as response, open(destination_path, ""wb"") as file:                        ~~~~~~~^^^^^^^^^         File ""/leonardo/home/userexternal/fvicenti/.local/share/uv/python/cpython3.13.0linuxx86_64gnu/lib/python3.13/urllib/request.py"", line 189, in urlopen           return opener.open(url, data, timeout)                  ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^         File ""/leonardo/home/userexternal/fvicenti/.local/share/uv/python/cpython3.13.0linuxx86_64gnu/lib/python3.13/urllib/request.py"", line 489, in open           response = self._open(req, data)         File ""/leonardo/home/userexternal/fvicenti/.local/share/uv/python/cpython3.13.0linuxx86_64gnu/lib/python3.13/urllib/request.py"", line 506, in _open           result = self._call_chain(self.handle_open, protocol, protocol +                                     '_open', req)         File ""/leonardo/home/userexternal/fvicenti/.local/share/uv/python/cpython3.13.0linuxx86_64gnu/lib/python3.13/urllib/request.py"", line 466, in       _call_chain           result = func(*args)         File ""/leonardo/home/userexternal/fvicenti/.local/share/uv/python/cpython3.13.0linuxx86_64gnu/lib/python3.13/urllib/request.py"", line 1367, in       https_open           return self.do_open(http.client.HTTPSConnection, req,                  ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                               context=self._context)                               ^^^^^^^^^^^^^^^^^^^^^^         File ""/leonardo/home/userexternal/fvicenti/.local/share/uv/python/cpython3.13.0linuxx86_64gnu/lib/python3.13/urllib/request.py"", line 1322, in do_open           raise URLError(err)       urllib.error.URLError:        error: command '/leonardo/home/userexternal/fvicenti/.cache/uv/buildsv0/.tmpP5IWUg/bin/python' failed with exit code 1   help: `tensorstore` (v0.1.69) was included because `test2` (v0.1.0) depends on `tensorstore` [fvicenti test2]$```  System info (python version, jaxlib version, accelerator, etc.) ...",2024-12-02T12:21:44Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/25195,"Sorry, messed up the repository. Was supposed to open this elsewhere"
yi,Array.view(jnp.complex64) doesn't work on TPU with jax_enable_x64," Description Trying to call `view(jnp.complex64)` when `jax_enable_x64` is enabled fails with: `XlaRuntimeError: INVALID_ARGUMENT: Element type C128 is not supported on TPU.` ``` import jax import jax.numpy as jnp jax.config.update(""jax_enable_x64"", True) arr = jnp.array((1,2,3,4), dtype=jnp.float32) print(arr.view(jnp.complex64)) ``` It works without jax_enable_x64, when running on CPU/GPU (though presumably by going via complex128), or when called inside JITed code on TPU. Notebook reproduction (using latest version jax0.4.35 at the time of filing this issue): https://colab.research.google.com/drive/1IuyyeJQf60Febek2G4SWtqTAdeNAlzQt  System info (python version, jaxlib version, accelerator, etc.) Running on a v48 TPU VM, in a notebook (though it also fails if run outside a notebook, or on a TPUv2, see link above): ``` jax:    0.4.30 jaxlib: 0.4.30 numpy:  2.0.2 python: 3.9.5 (default, Nov 23 2021, 15:27:38)  [GCC 9.3.0] jax.devices (4 total, 4 local): [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0) TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0) TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0) TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)] process_count: 1 platform: uname_result(system='Linux', node='t1vn98008da2w0', release='5.13.01023gcp', version=' CC(examples/datasets.py doesn’t work in python3)~20.04.1Ubuntu SMP Wed Mar 30 03:51:07 UTC 2022', machine='x86_64') ```",2024-11-30T18:41:43Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/25189,"Hi , Thanks for the report. As per the comment, JAX TPU does not have support for many 64bit operations and it is suggested using the default `jax_enable_x64=False` setting, to limit all operations to 32bit. When it comes to this issue, the error is coming from the following line: https://github.com/jaxml/jax/blob/e124c051f2c6f70b52ca87e10fefe8a5ce9e0d15/jax/_src/numpy/array_methods.pyL542 The error arises because `1j` is treated as a complex number with 64bit precision (C128 type) when `jax_enable_x64` is enabled.",Agreed with  – the fix here is to not use `jax_enable_x64=True` with TPUs.
yi,Value becoming a tracer error," Description Hi, I get the following error when I run the code below: TracerIntegerConversionError: The __index__() method was called on traced array with shape int64[] The error occurred while tracing the function forwardEuler at /home/alex/PathOpt/integrationMethods/fwdEulerModel_3_GHquad_fwd_model_jax.py:110 for jit. This value became a tracer due to JAX operations on these lines: operation a:i64[] = sub b c from line /home/alex/PathOpt/integrationMethods/fwdEulerModel_3_GHquad_fwd_model_jax.py:113 (forwardEuler) Does the issue have to do with setting some of the function inputs as static? I tried doing that and it didn't work. How do you seet a jnp array as static? Do you declare it as anumpy array? Thank you, Alex ``` import jax from functools import partial from jax import config import jax.numpy as jnp import numpy as np import matplotlib.pyplot as plt from scipy import linalg import time import random  This only works on startup config.update('jax_enable_x64', True) config.update('jax_platform_name', 'gpu') config.update('jax_numpy_dtype_promotion', 'standard')  Type being used realType = jnp.float64  def rSquared(x, y, u):     return (ux)**2 + y**2 def uxCosFun(z, dx, h):     return dx*0.5*(1.0  jnp.cos(z*np.pi/h))  Downward (+ve zaxis) particle position as a function of time def p_z(t, v_z0, g):     return v_z0*t + 0.5*g*t**2  Time elapsed (inverse of above function) def t_p(p_z, v_z0, g):     return (np.sqrt(v_z0**2 + 2.0*g*p_z)  v_z0)/g  Function describing radially symmetric part of force field (divided by r) def G_r(r, c):     return c[0]*jnp.exp(c[1]*r**2 + c[2]*r)  System dynamics; a set of first order ODEs .jit def fun(RHS, u, FmaxOverMp, cFr):      Get previous variable values     y1 = RHS[0]     y2 = RHS[1]     y3 = RHS[2]     y4 = RHS[3]     r = jnp.sqrt(rSquared(y1, y2, u))      Calculate derivative of all variables     dy1dt = y3     dy2dt = y4     dy3dt = FmaxOverMp*G_r(r, cFr)*(u  y1)     dy4dt = FmaxOverMp*G_r(r, cFr)*(0.0  y2)      Lefthand side of ODE system     RHS = RHS.at[0].set(dy1dt)     RHS = RHS.at[1].set(dy2dt)     RHS = RHS.at[2].set(dy3dt)     RHS = RHS.at[3].set(dy4dt)     return RHS (jax.jit, static_argnums=(5,)) def forwardEulerPlot(uVals, y0, cFr, FmaxOverMp, dt,Nt):     y = jnp.zeros((Nt, len(y0)), dtype=realType)     y = y.at[0, :].set(y0)     for i in range(Nt1):         y = y.at[i+1, :].set(y[i, :] + fun(y[i, :], uVals[i], FmaxOverMp, cFr) * dt)     return y (jax.jit, static_argnums=(5,)) def forwardEuler(uVals, y, cFr, FmaxOverMp, dt,Nt):     for i in range(Nt1):         y = y + fun(y, uVals[i], FmaxOverMp, cFr) * dt     return y forwardEuler_array = jax.vmap(forwardEuler, in_axes=(None,0,None,None,None,None)) def applyQuadratureRule(uVals, y0, cFr, FmaxOverMp, dt,Nt, quadProd):     y_last = forwardEuler_array(uVals, quadProd[:, 1:], cFr, FmaxOverMp, dt,Nt)     return jnp.dot(rSquared(y_last[:,0], y_last[:,1], jnp.tile(uVals[1], quadProd.shape[0])), quadProd[:,0])  Old function  def applyQuadratureRule(uVals, y0, cFr, FmaxOverMp, dt,Nt, quadProd):      EuX2int = 0.0      for i in range(quadProd.shape[0]):          y0 = quadProd[i, 1:]          y  = forwardEuler(uVals, y0, cFr, FmaxOverMp, dt,Nt)          EuX2int += rSquared(y[0], y[1], uVals[1])*quadProd[i,0]      return EuX2int   Metric conversion mToMm = 1.0e3  Particle properties d_p = 80.0e6 rho_p = 4500.0 r_p = d_p/2.0 V_p = 0.75*np.pi*r_p**3 m_p = V_p*rho_p  Force field properties F_max = 4.0e9 FmaxOverMp = F_max/m_p  cFr = np.array([149.0, 4528.0, 210.0, 1.4e3]) cFr = jnp.array([721.8,4.030e5,558.4], dtype=realType) g = 9.81  Particle initial conditions v_z0 = 1.0  Variables for cos trajectory function h = 200.0e3 Dx = 4.0e3  Run forward Euler for one particle dt = 5.0e4   Time step for forward Euler method h  = 0.2      Height  Final time considering height and gravitational acceleration t_f = t_p(h, v_z0, g) tVals = jnp.arange(0.0, t_f+dt, dt, dtype=realType) pzVals = p_z(tVals, v_z0, g) uVals = uxCosFun(pzVals, Dx, h) y0 = jnp.array([0.001e3, 0.001e3, 0.0e3, 0.0e3], dtype=realType) Nt = len(tVals) quadProd = np.array([[0.1250, 0.0005, 0.0005, 0.0200, 0.0200],                      [0.1250, 0.0005, 0.0005, 0.0200,  0.0200]]) start_time = time.time() EuX2int = applyQuadratureRule(uVals, y0, cFr, FmaxOverMp, dt,Nt, quadProd) print('\n') print(""Time to get integral:                   %.4f seconds"" % (time.time()  start_time)) print('Quad tensor product:                    %.15e m^2'    % EuX2int)   Forward mode gradient check y0 = jnp.array([0.001e3, 0.0015e3, 0.005, 0.003], dtype=realType) epsVal   = 1.0e10 uValsEps = uVals.at[4].set(uVals[4] + epsVal)  Single path finalY1 = forwardEuler(uVals,    y0, cFr, FmaxOverMp, dt,Nt) finalY2 = forwardEuler(uValsEps, y0, cFr, FmaxOverMp, dt,Nt) print((finalY2finalY1)/epsVal) gradFinalY = jax.jit(jax.jacfwd(forwardEuler, argnums=0), static_argnums=(5,)) gradVals   = gradFinalY(uVals, y0, cFr, FmaxOverMp, dt,Nt) print(gradVals[:,4]) print('\n')  Full gradient EuX2int2 = applyQuadratureRule(uValsEps, y0, cFr, FmaxOverMp, dt,Nt, quadProd) print((EuX2int2EuX2int)/epsVal) gradER2 = jax.jit(jax.jacfwd(applyQuadratureRule, argnums=0)) start_time = time.time() gradVals = gradER2(uVals, y0, cFr, FmaxOverMp, dt,Nt, quadProd) print(gradVals[4]) print(""Time to get fwd grad:       %.4f seconds"" % (time.time()  start_time)) ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.31 jaxlib: 0.4.31 numpy:  1.21.5 python: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] jax.devices (1 total, 1 local): [CudaDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='ME21', release='6.8.048generic', version=' CC(Unimplemented: binary integer op 'power')~22.04.1Ubuntu SMP PREEMPT_DYNAMIC Mon Oct  7 11:24:13 UTC 2', machine='x86_64') $ nvidiasmi Fri Nov 29 14:01:36 2024        ++  ++ ```",2024-11-29T19:02:37Z,bug,open,0,9,https://github.com/jax-ml/jax/issues/25186,"Please put together a *minimal*, reproducible example that isolates the problem. The included code includes a lot of extra pieces that aren't necessary for demonstrating this specific problem. Thanks!","Hi dfm, I updated the code thank you for the comment","You need to change this: ```python gradFinalY = jax.jit(jax.jacfwd(forwardEuler, argnums=0)) ``` to this: ```python gradFinalY = jax.jit(jax.jacfwd(forwardEuler, argnums=0), static_argnums=(5,)) ``` `forwardEuler` takes static arguments, and so when you wrap the gradient in `jit`, you must ensure those arguments are marked as static.","Hi Jake, thank you that works! I tried doing the same for the commented part at the end of the code for applyQuadratureRule() that applies a numerical quadrature for integration (of the final state) of the forward Euler model forwardEuler(). If I use jit without static_argnums=(6,) for the fixed shape array quadProd I get the same error, but if I add this I get: ValueError: Nonhashable static arguments are not supported. An error occurred while trying to hash an object of type , [[ 0.125  0.0005  0.0005 0.02   0.02  ]  [ 0.125  0.0005  0.0005 0.02    0.02  ]]. The error was: TypeError: unhashable type: 'numpy.ndarray' Is there a way of telling jax this is static both for jit and when getting the forward gradient?","This error means you're passing an array to a parameter marked as static, which is not allowed. Arrays aren't hashable, so they can't be static arguments. The solution is either to pass a hashable value to that argument (if the array was passed erroneously), or to not mark the argument as static (if it was marked static erroneously).","But if I don't pass it as static I get the trace error of the original question. Also I just checked, the original commented applyQuadratureRule() function takes 20 seconds, while the function using vmap takes 30 seconds. Both without using jit. Shoudn't the function using vmap without the for loop be faster? ",The trace error in the original question is because a parameter was marked static. Did you remove that annotation in both places?,"I did for applyQuadratureRule(), I'm running the code as posted","The error you mentioned means that you're passing a traced value to a parameter marked static. The solution is either to ensure it's static in the whole call stack (my initial recommendation) or to ensure it's not static in the whole call stack. Since the value you're passing is apparently an array, it cannot be static, so you have to ensure it's not marked as static anywhere. Does that make sense?"
yi,Unable to run FFI example with stateful function," Description I am trying to evaluate the new JAX FFI and use it to expose some of my code to be used with JAX/XLA. Since I have stateful functions, I tried to run the test linked in https://jax.readthedocs.io/en/latest/ffi.htmladvancedtopics as an example.  The following code (slightly adapted to replace `absl::Status` with `ffi::Error`) ```cpp struct SomeState {   explicit SomeState(int32_t value) : value(value) {}   int32_t value = 0; }; static ffi::Error GetState(ffi::Result buffer, SomeState* state) {   if (state>value != 42) {     return ffi::Error::Internal(""Unexpected value"");   }   return ffi::Error::Success(); } ``` Fails to compile, pointing me to a missing `TypeId id` member in `SomeState`. The following ```cpp struct SomeState {   explicit SomeState(int32_t value) : value(value) {}   int32_t value = 0;   static ffi::TypeId id; }; ffi::TypeId SomeState::id = {};  // zeroinitialize type id XLA_FFI_REGISTER_TYPE(XLA_FFI_GetApi(), ""SomeState"", &SomeState::id); ``` Does compile, but we now have a missing symbol for `_XLA_FFI_GetApi`. Looking around in jaxlib, I found `xla_extension.so`, which defines this symbol as private/hidden.  I thus have a couple of questions:  Do I need to use `XLA_FFI_REGISTER_TYPE` to register custom context for stateful functions? It looks like a part of the public API.  If `XLA_FFI_REGISTER_TYPE` is part of the public API, should I be able to call `XLA_FFI_GetApi`?   And if I am able to call `XLA_FFI_GetApi`, should it be available for linking in `xla_extension.so`? Or is this one a private implementation detail?  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.35 jaxlib: 0.4.35 numpy:  1.26.4 python: 3.12.0  (main, Oct  3 2023, 08:36:57) [Clang 15.0.7 ] device info: cpu1, 1 local devices"" process_count: 1 platform: uname_result(system='Darwin', node='huna', release='23.6.0', version='Darwin Kernel Version 23.6.0: Fri Jul  5 17:56:41 PDT 2024; root:xnu10063.141.1~2/RELEASE_ARM64_T6000', machine='arm64') ```",2024-11-29T16:31:27Z,enhancement,closed,0,4,https://github.com/jax-ml/jax/issues/25185,"Thanks for the questions! We don't actually support userdefined state for FFIs registered via JAX yet. There are some subtleties to getting dyanamically registered types to work, but it will get added eventually (something just always seems to be higher priority :D). In the meantime, the usual approach that we use throughout `jaxlib` is to use global state that gets initialized on the first call to the handler. For example, check out this example: https://github.com/jaxml/jax/blob/aff7714dc0f49cc0097e4db08e028b68182c8ab9/examples/ffi/src/jax_ffi_example/cpu_examples.ccL71L104 Hope this helps!","Ok, thanks for the clarification! Using global/thread local state could work in some cases (when the state is some kind of cache allocation/work array) that can be hidden from the user, but I also have some cases where the end user needs to manipulate the types. I'll see if there is a way to handle this with the current API, or if I need to wait for better integration.","Reporting back here that JAX now supports stateful FFI handlers, and CC(Add FFI example demonstrating the use of XLA's FFI state.) adds an example. I'm going to close this as complete, but please report back if you run into further issues.","Thanks a lot, I'll have a look ASAP!"
yi,Cannot generate from `jax.experimental.sparse.random_bcoo` for arrays larger than the int32 max ," Description `jax.experimental.sparse.random_bcoo` yields an error for arrays with size larger than the int32 max. It looks like one cannot generate random indices that overflow int32 even if the specified index dtype if int64. Here is code to reproduce: ```python import jax import jax.numpy as jnp from jax.experimental.sparse import random_bcoo key = jax.random.PRNGKey(0) shape = (4096, 1769472) dtype = jnp.float32 nse = 8.964381565185707e05 indices_dtype = jnp.int64 random_bcoo(key, shape, dtype=dtype, indices_dtype=indices_dtype, nse=nse) ``` yields error ``` jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. The above exception was the direct cause of the following exception: Traceback (most recent call last):   File """", line 1, in    File ""/mnt/xfs/home/engstrom/conda_envs/ajax/lib/python3.10/sitepackages/jax/experimental/sparse/random.py"", line 87, in random_bcoo     indices = _indices(index_keys).reshape(indices_shape)   File ""/mnt/xfs/home/engstrom/conda_envs/ajax/lib/python3.10/sitepackages/jax/experimental/sparse/random.py"", line 80, in _indices     flat_ind = random.choice(key, sparse_size, shape=(nse,),   File ""/mnt/xfs/home/engstrom/conda_envs/ajax/lib/python3.10/sitepackages/jax/_src/random.py"", line 631, in choice     arr = jnp.asarray(a)   File ""/mnt/xfs/home/engstrom/conda_envs/ajax/lib/python3.10/sitepackages/jax/_src/numpy/lax_numpy.py"", line 5592, in asarray     return array(a, dtype=dtype, copy=bool(copy), order=order, device=device)   File ""/mnt/xfs/home/engstrom/conda_envs/ajax/lib/python3.10/sitepackages/jax/_src/numpy/lax_numpy.py"", line 5354, in array     _ = dtypes.coerce_to_array(object, dtype) OverflowError: Python int 7247757312 too large to convert to int32 ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.35 jaxlib: 0.4.34 numpy:  1.26.4 python: 3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0] device info: NVIDIA A100PCIE40GB1, 1 local devices"" process_count: 1 platform: uname_result(system='Linux', node='', release='5.15.0118generic', version=' CC(lax_linalg.triangular_solve gives RuntimeError for largeish inputs on CPU)Ubuntu SMP Fri Jul 5 09:28:59 UTC 2024', machine='x86_64') $ nvidiasmi Thu Nov 28 15:02:31 2024 ++  ```",2024-11-28T20:03:30Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/25182,"never mind, enabling 64 bit dtypes fixes this (although it ooms)"
yi,[mgpu_pallas] Optionally pass default value instead of raising an error when trying to ensure ir Value.,[mgpu_pallas] Optionally pass default value instead of raising an error when trying to ensure ir Value.,2024-11-28T16:28:51Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25177
yi,Replace dependency on pre-built wheels with `py_import` dependency.,Replace dependency on prebuilt wheels with `py_import` dependency.,2024-11-27T20:25:53Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25152
llm,NaN's produced by jax.numpy.linalg.pinv not produced by numpy," Description I've been hunting down this a bug for a while! I've got as far as I can with a MWE. Symptoms  I've been getting NaNs produced on a CPU after a bunch of iterations of my algorithms (say between around 2080 batches in) but I have found that on a GPU I am *never* seeing this same issue (even after 1000 iterations). As I'm using `vmap` + `jax.lax.cond` I've been unable to use the normal 'debug nans' flags (well I was using them but they were *all* red herrings!) I think I've managed to isolate a calculation that produces these NaNs that Numpy does not... (Note that I've copied the pinv implementation out of Jax in order to add extra debugging) ``` from __future__ import annotations from functools import partial import jax import jax.numpy as jnp from jax import Array from jax._src.numpy import ufuncs from jax._src.numpy.util import check_arraylike, promote_dtypes_inexact from jaxtyping import ArrayLike jax.config.update(""jax_enable_x64"", True)   type: ignore[nountypedcall]  This is just copied out of Jax. (jax.jit, static_argnames=(""hermitian"")) def _pinv(     a: ArrayLike, rtol: ArrayLike | None = None, hermitian: bool = False ) > Array:      Uses same algorithm as      https://github.com/numpy/numpy/blob/v1.17.0/numpy/linalg/linalg.pyL1890L1979     check_arraylike(""jnp.linalg.pinv"", a)     (arr,) = promote_dtypes_inexact(jnp.asarray(a))     m, n = arr.shape[2:]     if m == 0 or n == 0:         return jnp.empty(arr.shape[:2] + (n, m), arr.dtype)     arr = ufuncs.conj(arr)     if rtol is None:         max_rows_cols = max(arr.shape[2:])         rtol = 10.0 * max_rows_cols * jnp.array(jnp.finfo(arr.dtype).eps)     rtol = jnp.asarray(rtol)     u, s, vh = jax.numpy.linalg.svd(arr, full_matrices=False, hermitian=hermitian)      Singular values less than or equal to ``rtol * largest_singular_value``      are set to zero.     rtol = jax.lax.expand_dims(rtol[..., jnp.newaxis], range(s.ndim  rtol.ndim  1))     cutoff = rtol * s[..., 0:1]     s1 = s     s = jnp.where(s > cutoff, s, jnp.inf).astype(u.dtype)     divide = ufuncs.divide(u.mT, s[..., jnp.newaxis])      jax.debug.breakpoint()     jax.debug.print(         ""s: {s}, s1: {s1}, cutoff: {c}, array: {a}, divide: {d}"",         s=s,         s1=s1,         c=cutoff,         a=a,         d=divide,     )     res = jnp.matmul(         vh.mT,         divide,          precision=jax.lax.Precision.HIGHEST,     )     return jax.lax.convert_element_type(res, arr.dtype) if __name__ == ""__main__"":     array_to_test = [         [             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.00014172,             0.00014172,             0.0,             0.0,             0.00014172,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,         ],         [             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.00094461,             0.00094461,             0.00094461,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,         ],         [             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0001661,             0.0001661,             0.0001661,             0.0,             0.0001661,             0.0,             0.0001661,             0.0001661,             0.0001661,             0.0,             0.0001661,             0.0001661,         ],         [             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             4.92293893e05,             0.00000000e00,             0.00000000e00,             4.92293893e05,             4.92293893e05,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,         ],         [             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             2.60656573e05,             2.60656573e05,             2.60656573e05,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,         ],         [             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.00083249,             0.00083249,             0.00083249,             0.0,             0.0,             0.0,             0.0,         ],         [             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             2.36300179e06,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             2.36300179e06,             2.36300179e06,             0.00000000e00,             0.00000000e00,         ],         [             5.25096447e04,             5.25096447e04,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             2.60073755e05,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,         ],         [             0.0,             0.00124964,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.00124964,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,         ],         [             0.00000000e00,             3.27900001e04,             3.27900001e04,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             5.56544369e05,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,         ],         [             0.00000000e00,             0.00000000e00,             2.50144095e05,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             2.50144095e05,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             1.79331381e07,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,         ],         [             0.0,             0.0,             0.00026905,             0.00026905,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,         ],         [             0.00000000e00,             0.00000000e00,             0.00000000e00,             7.12897403e09,             7.12897403e09,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,         ],         [             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             7.95388267e08,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,         ],         [             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             2.41661281e08,             2.41661281e08,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             1.72676016e08,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,         ],         [             0.00273702,             0.0,             0.0,             0.0,             0.0,             0.0,             0.00273702,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,         ],         [             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             2.03069126e06,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,         ],         [             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             8.26290208e05,             8.26290208e05,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             1.87278036e06,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,         ],         [             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             3.23718682e09,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,         ],         [             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             2.57497107e07,             2.57497107e07,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,         ],         [             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             1.10231113e08,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,         ],         [             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.00027104,             0.00027104,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,         ],         [             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.00810994,             0.0,             0.0,             0.0,             0.0,         ],         [             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.00105425,             0.0,             0.0,             0.0,             0.0,             0.00105425,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.00064851,             0.0,             0.0,             0.0,             0.0,         ],         [             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             5.88623498e08,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             5.88623498e08,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             1.51746996e08,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,         ],         [             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             3.90322902e08,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             3.90322902e08,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             2.40507631e08,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,         ],         [             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             4.98743036e07,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             4.98743036e07,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             3.81663585e07,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,         ],         [             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             4.07995726e08,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             4.07995726e08,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,         ],         [             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             2.60118364e08,             2.60118364e08,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             1.77713825e09,             0.00000000e00,             0.00000000e00,         ],         [             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             1.95330716e10,             0.00000000e00,         ],         [             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             1.07921723e06,             1.07921723e06,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,         ],         [             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             9.41776993e10,         ],         [             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             5.28251527e09,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             5.28251527e09,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             3.47113175e09,         ],         [             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             5.62193049e09,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             5.62193049e09,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             0.00000000e00,             2.58156142e09,             0.00000000e00,         ],         [             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.14852621,             0.14852621,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             3.47270687,             0.0,             0.0,             0.0,         ],         [             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.00453347,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.00453347,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.00371358,             0.0,             0.0,             0.0,         ],         [             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.00098536,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,             0.0,         ],     ]     inv = _pinv(jax.numpy.array(array_to_test))  can be replaced by jax.numpy.linalg.pinv     print(inv)     import numpy as np     print(np.linalg.pinv(np.array(array_to_test))) ```  System info (python version, jaxlib version, accelerator, etc.) ``` Python 3.12.4 jax:    0.4.35 jaxlib: 0.4.35 numpy:  1.26.4 python: 3.12.4 (main, Jun  6 2024, 18:26:44) [Clang 15.0.0 (clang1500.3.9.4)] device info: cpu1, 1 local devices"" process_count: 1 platform: uname_result(system='Darwin', node='SamsMacBookPro.local', release='23.6.0', version='Darwin Kernel Version 23.6.0: Wed Jul 31 20:48:04 PDT 2024; root:xnu10063.141.1.700.5~1/RELEASE_ARM64_T6030', machine='arm64') ```",2024-11-26T12:52:08Z,bug,open,0,2,https://github.com/jax-ml/jax/issues/25111,"I imagine the first response will be something like 'well due to numeric differences, this specific matrix will produce a nan with Jax, but not with Numpy' BUT I've made tweaks and changes to my algorithm (even feeding it different data) and NEVER see this issue on a GPU (even after 1000 iterations) but it ALWAYS seems to crop up on CPU. And (as best I can tell) this is where that NaN is being generated.","What NumPy version are you using? With NumPy v2.1.3, I'm getting `numpy.linalg.LinAlgError: SVD did not converge`, which suggests that JAX's NaN outputs are correct here. The NumPy results may also depend on the underlying LAPACK library that is being linked, which will depend on your operating system and how you installed NumPy (e.g. pip vs conda, etc.). ```pytb Traceback (most recent call last):   File ""tmp.py"", line 1509, in      print(np.linalg.pinv(np.array(array_to_test)))           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File "".../python3.12/sitepackages/numpy/linalg/_linalg.py"", line 2271, in pinv     u, s, vt = svd(a, full_matrices=False, hermitian=hermitian)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File "".../python3.12/sitepackages/numpy/linalg/_linalg.py"", line 1839, in svd     u, s, vh = gufunc(a, signature=signature)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File "".../python3.12/sitepackages/numpy/linalg/_linalg.py"", line 113, in _raise_linalgerror_svd_nonconvergence     raise LinAlgError(""SVD did not converge"") ```"
yi,Add lax.composite primitive,"Add lax.composite primitive A composite function can encapsulate an operation made up of other JAX functions. The semantics of the op is implemented by the `decomposition` function. For example, a `tangent` operation can be implemented as `sin(x) / cos(x)`. This is what the HLO looks like for a tangent composite: ``` module  {   func.func public (%arg0: tensor) > (tensor) {     %0 = stablehlo.composite ""my.tangent"" %arg0 {decomposition = .tangent} : (tensor) > tensor     return %0 : tensor   }   func.func private .tangent(%arg0: tensor) > tensor {     %0 = stablehlo.sine %arg0 : tensor     %1 = stablehlo.cosine %arg0 : tensor     %2 = stablehlo.divide %0, %1 : tensor     return %2 : tensor   } } ``` Similarly, this can scale to something like Attention. By preserving such an abstraction, it greatly simplifies pattern matching. Instead of matching the set of ops that represent Attention, the matcher can simply look for a uniquely identifying composite op like ""MyAttention"". This is useful for preserving high level abstraction that would otherwise be lost during lowering. The hardwareaware compiler can recognize the single composite op and emit efficient code rather than patternmatching a generic lowering which is then replaced with your own efficient lowering. And then the decomposition function can be DCE'd away. If the hardware does not have an efficient lowering, it can inline the `decomposition` which implements the semantics of the abstraction. For more details on the API, refer to the documentation.",2024-11-25T23:59:56Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25104
yi,Weird defjvp behavior when finding grad of a scalar that depends on the primal," Description Hello! I was testing the defjvp feature to deal with some stability issues of the SVD, but I noticed that defjvp doesn't work as expected if the output of the function to be differentiated depends on the primal. Let me share some code. I started by defining ``new_svd`` , which should be exactly the same as ``jax.numpy.svd`` but with its derivatives defined with defjvp (code from ``jax._src.lax.linalg.py`` with minor modifications):     import jax.lax as lax     from jax._src.lax import lax as lax_internal     from jax import custom_jvp     import jax.numpy as jnp     import jax.random as jrandom     import jax     def _extract_diagonal(s):         i = lax.iota(""int32"", min(s.shape[2], s.shape[1]))         return s[..., i, i]     def _construct_diagonal(s):         i = lax.iota(""int32"", s.shape[1])         return lax.full((*s.shape, s.shape[1]), 0, s.dtype).at[..., i, i].set(s)     def _H(x):         return _T(x).conj()     def _T(x):         return lax.transpose(x, (*range(x.ndim  2), x.ndim  1, x.ndim  2))          def new_SVD(x):         return jnp.linalg.svd(x, full_matrices=False)     .defjvp     def _svd_jvp_rule(primals, tangents):         (A,) = primals         (dA,) = tangents         U, s, Vt = jnp.linalg.svd(A, full_matrices=False)         Ut, V = _H(U), _H(Vt)         s_dim = s[..., None, :]         dS = Ut @ dA @ V         ds = _extract_diagonal(dS.real)         s_diffs = (s_dim + _T(s_dim)) * (s_dim  _T(s_dim))         s_diffs_zeros = lax_internal._eye(s.dtype, (s.shape[1], s.shape[1]))         s_diffs_zeros = lax.expand_dims(s_diffs_zeros, range(s_diffs.ndim  2))         F = 1 / (s_diffs + s_diffs_zeros)  s_diffs_zeros         dSS = s_dim.astype(A.dtype) * dS         SdS = _T(s_dim.astype(A.dtype)) * dS         s_zeros = (s == 0).astype(s.dtype)         s_inv = 1 / (s + s_zeros)  s_zeros         s_inv_mat = _construct_diagonal(s_inv)         dUdV_diag = 0.5 * (dS  _H(dS)) * s_inv_mat.astype(A.dtype)         dU = U @ (F.astype(A.dtype) * (dSS + _H(dSS)) + dUdV_diag)         dV = V @ (F.astype(A.dtype) * (SdS + _H(SdS)))         m, n = A.shape[2:]         if m > n:             dAV = dA @ V             dU = dU + (dAV  U @ (Ut @ dAV)) * s_inv.astype(A.dtype)         if n > m:             dAHU = _H(dA) @ U             dV = dV + (dAHU  V @ (Vt @ dAHU)) * s_inv.astype(A.dtype)         return (U, s, Vt), (dU, ds, _H(dV)) Then I wanted to compare the results of ``jax.value_and_grad``, which should be the same since I used the same jvp rule. The code is as follows:     def new_SVD_to_scalar(A):       U, s, Vt = my_SVD(A)        Case 1:        return jnp.linalg.norm((U*s) @ Vt  A)        Case 2:        return jnp.linalg.norm((U*s) @ Vt)     def normal_SVD_to_scalar(A):         U, s, Vt = jnp.linalg.svd(A, full_matrices=False)          Case 1:          return jnp.linalg.norm((U*s) @ Vt  A)          Case 2:          return jnp.linalg.norm((U*s) @ Vt)     def test_random_normal(length, width):         A = jrandom.uniform(jrandom.PRNGKey(0), (length, width))         new_res = jax.value_and_grad(new_SVD_to_scalar)(A)         normal_res = jax.value_and_grad(normal_SVD_to_scalar)(A)         assert jnp.allclose(new_res[0], normal_res[0])         assert jnp.allclose(new_res[1], normal_res[1])   Returns False in Case 1 Uncomment both statements in either Case 1 or Case 2 to alternate between cases.  Basically, no problems are encountered when the output of the function does not depend on A (case 2). But once A is used (e.g. case 1), the gradients computed by defjvp and the original svd are different.  Note: The only difference between my jvp and the original is this:      Original     s, U, Vt = svd_p.bind(            A, full_matrices=False, compute_uv=True, subset_by_index=subset_by_index,            algorithm=algorithm,        )      In my jvp:     U, s, Vt = jnp.linalg.norm(A, full_matrices=False) Which, in my opinion, should be different since we don't have access to the primitive svd_p when using defjvp. Note: I know the outputs of ``svd_p.bind`` are permutated, but I don't think it is the reason of the difference. Am I missing something? Are we not allowed to define an output as a function of the primal? Shouldn't there be an error/warning raised if it is the case?  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.35 jaxlib: 0.4.35 numpy:  2.1.2 python: 3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)] device info: cpu1, 1 local devices"" process_count: 1 platform: uname_result(system='Windows', node='MSI', release='10', version='10.0.22631', machine='AMD64') $ nvidiasmi Tue Nov 26 00:13:02 2024        ++  ++",2024-11-25T23:18:14Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/25101,"It looks to me like this is working ok. When I run your demo code with ""case 1"" the differences seem to be within a few 10^8, which seems fine for `float32` precision. `jnp.allclose` uses an absolute tolerance of `1e8` which probably isn't suitable for your case. What do you think of that?",I agree! Thank you.
llm,Create a `null_mesh_context` internal context manager to handle null contexts properly.,Create a `null_mesh_context` internal context manager to handle null contexts properly.,2024-11-25T22:55:21Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25100
rag,[mosaic_gpu] Fixed unbounded recursion in `FragmentedArray._pointwise`,[mosaic_gpu] Fixed unbounded recursion in `FragmentedArray._pointwise`,2024-11-25T22:08:16Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25098
yi,[cuda] Bump nvidia-cuda-nvcc-cu12 dependency to 12.6.85,"Bumps the minimum version for `nvidiacudanvcccu12` to `12.6.85`, which is the earliest published version that of that wheel incorporating CUDA 12.6.3. This resolves the issue underlying https://github.com/jaxml/jax/pull/24438. Here's a simplified reproducer: ``` $ docker run ti gpus all ubuntu $ nvidiasmi (... H100) $ aptget update $ aptget install y python3pip $ export PIP_BREAK_SYSTEM_PACKAGES=1  Install JAX nightly (picks up recent nvidiacudanvcccu12 version, e.g. 12.6.85) $ pip install U pre jax jaxlib jaxcuda12plugin[with_cuda] jaxcuda12pjrt f https://storage.googleapis.com/jaxreleases/jax_nightly_releases.html  => GOOD $ python3 c ""import jax; import jax.numpy as jnp; A = jnp.arange(18).reshape(6, 3); m = jnp.arange(3, 3); print(jax.jit(lambda _0, _1: _0.at[jnp.abs(_1), 0].get())(A, m))"" [9 6 3 0 3 6]  Forceinstall bad version $ pip install forcereinstall ""nvidiacudanvcccu12==12.6.77""  => BAD $ python3 c ""import jax; import jax.numpy as jnp; A = jnp.arange(18).reshape(6, 3); m = jnp.arange(3, 3); print(jax.jit(lambda _0, _1: _0.at[jnp.abs(_1), 0].get())(A, m))"" [0 0 0 0 3 6]  Upgrade nvcc dependency $ pip install U ""nvidiacudanvcccu12>=12.6.85""  => GOOD $ python3 c ""import jax; import jax.numpy as jnp; A = jnp.arange(18).reshape(6, 3); m = jnp.arange(3, 3); print(jax.jit(lambda _0, _1: _0.at[jnp.abs(_1), 0].get())(A, m))"" [9 6 3 0 3 6] ``` As a followup we might also want to add a warning asking users with an affected version of `nvidiacudanvcccu12` to upgrade (e.g., via `pip install U ""nvidiacudanvcccu12>=12.6.85""`).  Any opinions on whether to have that warning / where it should go? As to how to check for the current ptxas version (https://github.com/jaxml/jax/pull/24438/filesr1818345222), I suppose we might expose that alongside other CUDA versions here (https://github.com/jaxml/jax/blob/c35f8b22c1b081135e0644a936c262a974c75f07/jaxlib/cuda/versions.ccL26)? A cruder alternative approach might be to check for the `nvidiacudanvcccu12` wheel's `__version__`, if present.",2024-11-25T18:27:52Z,pull ready,closed,0,2,https://github.com/jax-ml/jax/issues/25091,I think the best place for the warning would be in XLA: it should warn if it detects a ptxas that is known to be buggy.,Corresponding XLA PR that emits a warning: https://github.com/openxla/xla/pull/19927
rag,Bump hypothesis from 6.102.4 to 6.119.4,"Bumps hypothesis from 6.102.4 to 6.119.4.  Release notes Sourced from hypothesis's releases.  Hypothesis for Python  version 6.119.4 This patch fixes a bug since 6.99.13  20240324 where only interactivelygenerated values (via &quot;data.draw&quot;) would be reported in the &quot;arguments&quot; field of our observability output. Now, all values are reported. The canonical version of these notes (with links) is on readthedocs. Hypothesis for Python  version 6.119.3 Hypothesis collects coverage information during the &quot;shrink&quot; and &quot;explain&quot; phases in order to show a more informative error message. On 3.12+, this uses &quot;sys.monitoring&quot;. This patch improves the performance of coverage collection on 3.12+ by disabling events we don't need. The canonical version of these notes (with links) is on readthedocs. Hypothesis for Python  version 6.119.2 This patch refactors some internals to prepare for future work using our IR (issue  CC(update version and changelog for pypi)). The canonical version of these notes (with links) is on readthedocs. Hypothesis for Python  version 6.119.1 This patch migrates some more internals (around generating novel inputs) to the IR layer (issue  CC(update version and changelog for pypi)). The canonical version of these notes (with links) is on readthedocs. Hypothesis for Python  version 6.119.0 This release improves Hypothesis' handling of ExceptionGroup  it's now able to detect marker detections if they're inside a  group and attempts to resolve them. Note that this handling is still a work in progress and might not handle edge cases optimally. Please open issues if you encounter any problems or unexpected behavior with it. The canonical version of these notes (with links) is on readthedocs. Hypothesis for Python  version 6.118.9 Internal refactorings in preparation for upcoming changes. The canonical version of these notes (with links) is on readthedocs. Hypothesis for Python  version 6.118.8 Internal renamings. The canonical version of these notes (with links) is on readthedocs. Hypothesis for Python  version 6.118.7 This patch removes some &quot; type: ignore&quot; comments following a mypy update.   ... (truncated)   Commits  4a79609 Bump hypothesispython version to 6.119.4 and update changelog 11cc295 Merge pull request  CC(Add reverse flag in associative_scan) from tybug/argumentsobs c2e2c4e address comments c97d65a report dict instead of defaultdict 9390b0d report all generated arguments in observability cc87f33 Bump hypothesispython version to 6.119.3 and update changelog 97d63c0 Merge pull request  CC([jax2tf] Turn on with_gradient by default) from tybug/disablehypothesisevents 14d9b2b fixes 96d74dc disable line coverage events for hypothesis files 01d5d3e Bump hypothesispython version to 6.119.2 and update changelog Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-11-25T18:14:04Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/25089,Superseded by CC(Bump hypothesis from 6.102.4 to 6.122.1).
agent,[mgpu] FragentedArray.foreach() can now optionally return a new array,[mgpu] FragentedArray.foreach() can now optionally return a new array,2024-11-25T17:13:07Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25086
yi,Possible leak in random number generation," Description When using `jax` based package `pychastic` (an SDE solver) jax backend keeps eating memory indefinitely. ```python import jax import jax.numpy as np import numpy.random  import psutil import pychastic def main():     for _ in range(100):         initial_samples = numpy.random.random((10000, 3))         def drift_fn(X):             return X         def noise_fn(_):             return np.eye(3)         solver = pychastic.sde_solver.SDESolver(dt=0.01)         problem = pychastic.sde_problem.SDEProblem(         a=drift_fn, b=noise_fn,         x0=initial_samples,         tmax=10)         solver.solve_many(problem, n_trajectories=None, progress_bar=True)         mem_usage_now = psutil.Process().memory_info().rss / 1024 ** 2          jax.clear_backends()         del solver         del problem         del initial_samples         del noise_fn         del drift_fn         print(f'Mem usage: {mem_usage_now} MB') if __name__ == ""__main__"":     main() ``` Output (abbreviated) ``` 100% 1000/1000 [00:01<00:00, 863.09it/s] Mem usage: 181.51171875 MB ``` I apologize for the contrived code to reproduce the issue. I'd be happy to chase the leak further, but I'm unfamiliar with any tools that could help diagnose the issue. Is there some way to see what's taking up all this space?  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.3.25 jaxlib: 0.3.25 numpy:  1.23.5 python: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0] jax.devices (1 total, 1 local): [CpuDevice(id=0)] ``` also tested (same result) with ``` jax:    0.4.30 jaxlib: 0.4.30 numpy:  2.1.3 python: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0] jax.devices (1 total, 1 local): [CpuDevice(id=0)] ```",2024-11-22T21:33:22Z,bug,open,0,8,https://github.com/jax-ml/jax/issues/25069,A couple potentially helpful tips: (1) You can use `jax.live_arrays()` https://jax.readthedocs.io/en/latest/_autosummary/jax.live_arrays.html to return all of the live arrays and check for potential leaks. (2) Does manually running `gc.collect()` help with the problem?,"`gc.collect()` changes nothing `jax.live_arrays()` gives `[]` at the same time running  `jax.clear_backends()` once per loop fixes the problem  I've attached code for your convenience ```python import jax import jax.numpy as np import numpy.random  import psutil import pychastic import gc def main():     for _ in range(100):         initial_samples = numpy.random.random((10000, 3))         def drift_fn(X):             return X         def noise_fn(_):             return np.eye(3)         solver = pychastic.sde_solver.SDESolver(dt=0.01)         problem = pychastic.sde_problem.SDEProblem(         a=drift_fn, b=noise_fn,         x0=initial_samples,         tmax=10)         solver.solve_many(problem, n_trajectories=None, progress_bar=True)          jax.clear_backends()         del solver         del problem         del initial_samples         del noise_fn         del drift_fn         gc.collect()         print(jax.live_arrays())         mem_usage_now = psutil.Process().memory_info().rss / 1024 ** 2         print(f'Mem usage: {mem_usage_now} MB') if __name__ == ""__main__"":     main() ```    ","The fact that `clear_backends` solves this probably means that it's a JIT cache issue. Looking at the `solve_many` source, it looks like `problem` is closed over in `scan`: https://github.com/RadostW/stochastic/blob/bea391e57ad6e6bca734dd0ee243ab0b231d6048/pychastic/sde_solver.pyL158 Because of this, in scan's internal JIT, `problem` is treated as a static variable, and because you redefine it every iteration, it leads to a cache miss on every iteration. This means the JIT cache grows each iteration until hitting the LRU cache limit (which is 4096, I believe). This is all working as intended, and the fix to your issue would be for `pychastic` to be more careful about static arguments within its `scan` implementation. For example, the package could register `SDEProblem` as a pytree so that it would not be treated as a static variable. This would also potentially lead to faster runtime, because the scan function would not have to be recompiled at each iteration.","First, thanks for going through that code and finding where the issue really is. (Sorry it looks like this  it mirrors a textbook written by a mathematician, not a computer scientist). Second, what you say about cache misses is confusing. `problem` variable is closed over like you say, but this variable does not change. Are you saying a mere presence of a messy class as a implicit argument of a JITed function makes the cache explode? Is there a section of `jax` readme with good and bad examples of this? Third, suppose we exhaust the cache running `solve_many` one time. I would expect that this cache is eventually garbage collected or at least it does not grow in size indefinitely. Current behavior is that each time `solve_many` is run the program takes another bit of memory which eventually makes it run out of space and it crashes (without even raising a python error which can be handled).","> `problem` variable is closed over like you say, but this variable does not change. The variable does change. If you do `print(id(problem))`, you'll see that it's a different variable in each loop. The *contents* of the problem may not change, and if you want the JAX machinery to know this, then you can either define a meaningful hash function if `problem` doesn't contain any references to arrays, or register it as a pytree if it does contain references to arrays. > I would expect that this cache is eventually garbage collected or at least it does not grow in size indefinitely. Exactly – the cache is an LRU cache with a max size of (I believe) 4096, so once you call the function enough times it will start evicting older entries.","I don't think this is the problem. If I have just one `problem` object the issue is the same.  ``` WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) 100% 841/1000 [00:01<00:00, 982.76it/s] ``` Code below: ```python import jax import jax.numpy as np import numpy.random  import psutil import pychastic import gc def main():     def drift_fn(X):         return X     def noise_fn(_):         return np.eye(3)     initial_samples = numpy.random.random((10000, 3))     solver = pychastic.sde_solver.SDESolver(dt=0.01)     problem = pychastic.sde_problem.SDEProblem(     a=drift_fn, b=noise_fn,     x0=initial_samples,     tmax=10)     for _ in range(100):         solver.solve_many(problem, n_trajectories=None, progress_bar=True)          jax.clear_backends()          gc.collect()          print(jax.live_arrays())         mem_usage_now = psutil.Process().memory_info().rss / 1024 ** 2         print(f'problem id =  {id(problem)}')                         print(f'mem usage = {mem_usage_now} MB') if __name__ == ""__main__"":     main() ```    ","Hmm, in that case I'm not sure what's causing the cache size to grow. What happens if you do `gc.collect()` followed by `time.sleep(5)` between each iteration?","This does not help. I have also moved functional transforms outside of `step` function in the `pychastic` package, but this changed nothing as well (and to my surprise didn't change performance as far as I can tell)."
yi,[Mosaic TPU] Support packed type matmul with arbitrary shapes.,"[Mosaic TPU] Support packed type matmul with arbitrary shapes. This cl removes all the shape constrains in matmul for all types. We only need to mask out subelement on contracting dim. Instead of unpacking data and applying masks, we create a VREGsized i32 ""mask"" which contains subelement mask info to logical and with target vreg. Through this way, in order to mask subelements, each target vreg only needs to apply 1 op (logical_and) instead of 3 ops (unpacking + select + packing).",2024-11-22T20:13:00Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25068
dspy,`AssertionError: Unexpected XLA layout override` when adding two `from_dlpack` arrays," Description I have a test case that broke somewhere between jax versions 0.4.19 and 0.4.28. In particular, I am using `jax.dlpack.from_dlpack` on some PyTorch Tensors and then after hitting them with some jax operations I'm getting ``` ___________________________________________________ test_vit_b16 ___________________________________________________     .mark.skipif(not is_network_reachable(), reason=""Network is not reachable"")     def test_vit_b16():       import torchvision       model = torchvision.models.vit_b_16(weights=""DEFAULT"")       model.eval()       parameters = {k: t2j(v) for k, v in model.named_parameters()}        buffers = {k: t2j(v) for k, v in model.named_buffers()}        assert len(buffers.keys()) == 0       input_batch = random.normal(random.PRNGKey(123), (1, 3, 224, 224))       res_torch = model(j2t(input_batch))       jaxified_module = t2j(model) >     res_jax = jaxified_module(input_batch, state_dict=parameters) tests/test_all_the_things.py:462:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  torch2jax/__init__.py:697: in f     return t2j_function(m)(x) torch2jax/__init__.py:670: in      t2j_function = lambda f: lambda *args: f(*jax.tree_util.tree_map(Torchish, args)).value /nix/store/zmgaz729azdbqn50c0xdcjy10210absfpython3.12torch2.5.1/lib/python3.12/sitepackages/torch/nn/modules/module.py:1736: in _wrapped_call_impl     return self._call_impl(*args, **kwargs) /nix/store/zmgaz729azdbqn50c0xdcjy10210absfpython3.12torch2.5.1/lib/python3.12/sitepackages/torch/nn/modules/module.py:1747: in _call_impl     return forward_call(*args, **kwargs) /nix/store/cn0r9ifx64vg3cmkl6j42hxxrl0wydkgpython3.12torchvision0.20.1/lib/python3.12/sitepackages/torchvision/models/vision_transformer.py:298: in forward     x = self.encoder(x) /nix/store/zmgaz729azdbqn50c0xdcjy10210absfpython3.12torch2.5.1/lib/python3.12/sitepackages/torch/nn/modules/module.py:1736: in _wrapped_call_impl     return self._call_impl(*args, **kwargs) /nix/store/zmgaz729azdbqn50c0xdcjy10210absfpython3.12torch2.5.1/lib/python3.12/sitepackages/torch/nn/modules/module.py:1747: in _call_impl     return forward_call(*args, **kwargs) /nix/store/cn0r9ifx64vg3cmkl6j42hxxrl0wydkgpython3.12torchvision0.20.1/lib/python3.12/sitepackages/torchvision/models/vision_transformer.py:156: in forward     input = input + self.pos_embedding torch2jax/__init__.py:105: in __add__     def __add__(self, other): return Torchish(self.value + coerce(other)) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  self = Array([[[1.1815992e03,  2.7022592e03,  2.5492210e03, ...,           1.5614241e03, 1.9113609e03,  5.2576163e03]...89e02,  4.7187380e02, 6.4573869e02, ...,          1.8900207e01, 2.6449847e01,  2.5218439e01]]], dtype=float32) other = Array([[[0.0011816 ,  0.00270226,  0.00254922, ...,  0.00156143,          0.00191135,  0.00525762],         [0.0487...         [0.002416  , 0.02080391, 0.10696175, ..., 0.00442632,           0.0237054 , 0.00767821]]], dtype=float32)     def deferring_binary_op(self, other):       if hasattr(other, '__jax_array__'):         other = other.__jax_array__()       args = (other, self) if swap else (self, other)       if isinstance(other, _accepted_binop_types): >       return binary_op(*args) E       AssertionError: Unexpected XLA layout override: (XLA) DeviceLocalLayout({2,1,0}) != DeviceLocalLayout({2,0,1}) (User input layout) E        E       For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. /nix/store/wi25jwzkg8jf0ix3y9pvcpl3fqsk37r9python3.12jax0.4.28/lib/python3.12/sitepackages/jax/_src/numpy/array_methods.py:265: AssertionError ``` To reproduce run the `test_vit_b16` test in https://github.com/samuela/torch2jax/commit/93ed7065c582393384e958ea96f3ad6221b09626. It was last working on https://github.com/samuela/torch2jax/commit/bd7bd9c95253c89ffb7a25cc0ff2ccb296f6cfbf. Happy to provide any other info that might be helpful in reproducing. Potentially related: https://github.com/jaxml/jax/issues/24680  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.28 jaxlib: 0.4.28 numpy:  1.26.4 python: 3.12.7 (main, Oct  1 2024, 02:05:46) [Clang 16.0.6 ] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Darwin', node='sentientmeatloaf.local', release='24.1.0', version='Darwin Kernel Version 24.1.0: Thu Oct 10 21:03:15 PDT 2024; root:xnu11215.41.3~2/RELEASE_ARM64_T6000', machine='arm64') ```",2024-11-22T18:52:52Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/25066,"Thanks for the report! Yes, this is the same issue as https://github.com/jaxml/jax/issues/24680, and the real issue here is that XLA doesn't generally support layout assignments on CPU (it requires that all arrays be rowmajor).  is planning on adding support, but we're not sure what the timeline for that will be. The reason why this shows up with PyTorch and DLPack specifically has to do with the strides that PyTorch reports for tensors where one of the dimensions has size `1`. The tl;dr is that it's ambiguous where in the layout these dimensions should show up because you can put them anywhere without changing the striding behavior of the array, and PyTorch always gives them a stride of `1` via DLPack. Our stridestolayout logic will then always put these dimensions at the end of the layout. https://github.com/openxla/xla/pull/19327 includes a hack which would hide this assertion error (i.e. it would produce a rowmajor array in this case), but the real problem won't be solved until XLA CPU adds layout support!","I've finally added layouts support to the CPU client (here's the relevant commit: https://github.com/openxla/xla/pull/22048) and confirmed that this example now works. I'm going to close this because everything should work properly with the next JAX release, but please let me know if there are any further issues. Thanks again for the report!","Woohoo, thank you so much !"
rag,[pallas:mosaic_gpu] Add test for FragmentedArray.bitcast.,[pallas:mosaic_gpu] Add test for FragmentedArray.bitcast.,2024-11-22T17:34:26Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25058
yi,Rank-one updates to eigenvalue decompositions,"Hi team, First off, I absolutely love JAX. It's the core engine behind our startup. It would be fantastic to have a rankone update to an eigenvalue decomposition of a symmetric PSD matix $A$. I.e. when $A=LDL^\top$, we can compute $\tilde{L}\tilde{D}\tilde{L}^\top=\tilde{A}=A+\rho xx^\top$ in $\mathcal{O}(n^2)$, (rather than $\mathcal{O}(n^3)$ by running the eigenvalue decomposition again on $\tilde{A}$). In principle this is very similar to a Cholesky rankone update, which is implemented in `jax._src.lax.linalg.cholesky_update`, albeit without batching rule. **Motivating case** The rankone update to eigenvalue decompositions comes up in several rolling matrix algorithms. My own personal usecase is covariance matrix shrinkage in an exponentially weighted setting: ```math \Sigma_{t+1}=\lambda\Sigma_t + (1\lambda)x_tx_t^\top, ``` where I need to regularize the eigenvalues of each $\Sigma_t$. I do this for batched timeseries where the matrices themselves are of moderate size (of dim 30x30 to 300x300). With a rank one update I could run a scan on the eigenvalue decomposition rather than the covariance matrix. **Required algorithms** The underlying algorithms this requires are available in `scipy.linalg.cython_lapack` as `dlasd4` and `slasd4` for CPU (which I think jaxlib references under the hood), and in MAGMA on GPU (https://github.com/kjbartel/magma/blob/master/src/dlaex3.cpp). The GPU implementation seems to just parallelize over lapack and blas calls. **Current implementation** My current CPU implementation is not batched or parallelized (literally loops over every single eigenvalue), and therefore rather slow (but still a lot faster than just running `eigh` on the recursion output). It is based on the some simple linear algebra (see https://math.stackexchange.com/questions/3052997/eigenvaluesofarankoneupdateofamatrix): ```python  lasd4_all.pyx, for singleprecision only for brevity cimport cython cimport numpy as cnp from scipy.linalg.cython_lapack cimport dlasd4, slasd4 import numpy as np DTYPE_SINGLE = np.float32 cnp.import_array() ctypedef cnp.float32_t DTYPE_SINGLE_t .boundscheck(False) .wraparound(False) def slasd4_all(     cnp.ndarray[DTYPE_SINGLE_t, ndim=1] d,      cnp.ndarray[DTYPE_SINGLE_t, ndim=1] z,     float rho ):     cdef:         int n = d.shape[0]         int i         float sigma         int info         cnp.ndarray[DTYPE_SINGLE_t, ndim=1] work = np.zeros(n, dtype=DTYPE_SINGLE)         cnp.ndarray[DTYPE_SINGLE_t, ndim=1] delta = np.zeros(n, dtype=DTYPE_SINGLE)         cnp.ndarray[DTYPE_SINGLE_t, ndim=1] sigmas = np.zeros(n, dtype=DTYPE_SINGLE)     for i in range(1, n + 1):         slasd4(&n, &i, d.data, z.data, delta.data, &rho,             &sigmas[i  1], work.data, &info)     return sigmas ``` and then wrapping in a callback: ```python from functools import partial import jax import jax.numpy as jnp import numpy as np from jax import Array from jax.typing import ArrayLike from my_package.lasd4_all import dlasd4_all, slasd4_all   type: ignore def _lasd4_all_host(d: Array, z: Array, rho: Array) > Array:     _d = np.asarray(d)     _z = np.asarray(z)     if d.dtype == np.float32:         return slasd4_all(_d, _z, rho)     elif d.dtype == np.float64:         return dlasd4_all(_d, _z, rho)     else:         raise ValueError(f""Unsupported dtype {d.dtype}"") def _lasd4_all(d: Array, z: Array, rho: Array) > Array:      https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lapack.slasd4.html     dtype = jnp.promote_types(d.dtype, z.dtype)     d = jnp.asarray(d, dtype=dtype)     z = jnp.asarray(z, dtype=dtype)     rho = jnp.asarray(rho, dtype=dtype)     result_shape = jax.ShapeDtypeStruct(d.shape, dtype)     return jax.pure_callback(         _lasd4_all_host, result_shape, d, z, rho, vmap_method=""sequential""     ) def _eigh_rank_one_update(     w: Array, v: Array, z: Array, rho: Array ) > tuple[Array, Array]:      https://en.wikipedia.org/wiki/Bunch%E2%80%93Nielsen%E2%80%93Sorensen_formula      using VBV' = diag(w) + rho*(vz)(vz)'     w_out = jnp.square(_lasd4_all(jnp.sqrt(w), z @ v, rho))     Q = (z @ v) / (w  w_out[..., None])     Q = Q / jnp.linalg.norm(Q, axis=1, keepdims=True)     v_out = v @ Q.T     return w_out, v_out (jnp.vectorize, signature=""(n),(n,n),(n),()>(n),(n,n)"") def eigh_rank_one_update(     w: ArrayLike, v: ArrayLike, z: ArrayLike, rho: ArrayLike ) > tuple[Array, Array]:     """"""Compute the eigenvalues and eigenvectors of a rankone update of a symmetric     matrix.     Let A be a symmetric matrix with eigenvalues w and eigenvectors v, i.e.     A = v @ jnp.diag(w) @ v.T. Let z be a vector and rho a scalar. This function     computes the eigenvalues and eigenvectors of B = A + rho * z @ z.T.     Parameters          w : ArrayLike         The eigenvalues of A, of shape ``(..., n)``.     v : ArrayLike         The eigenvectors of A, of shape ``(..., n, n)``.     z : ArrayLike         The vector z, of shape ``(..., n)``.     rho : ArrayLike         The scalar rho, of shape ``(...)``.     Returns          tuple[Array, Array]         The eigenvalues of shape ``(..., n)`` and eigenvectors of shape ``(..., n, n)``,         of A + rho * z @ z.T.     """"""     w, v, z, rho = jnp.asarray(w), jnp.asarray(v), jnp.asarray(z), jnp.asarray(rho)     norm = jnp.linalg.norm(z, axis=1, keepdims=True)     w_, v_ = _eigh_rank_one_update(w, v, z / norm, rho * jnp.square(norm.squeeze(1)))     skip = norm * rho < 1e15     w = jnp.where(skip, w, w_)     v = jnp.where(skip[..., None], v, v_)     return w, v ``` I would also be super happy with any guidance on how to make my current implementation go faster. As a noncomputer scientist, I was not able to get a firm enough grip on C++/Cuda/XLA/JAX internals to implement this in a smarter/more parallel way myself, but if there is any way I can help with this feature, I would love to try.",2024-11-22T15:39:18Z,enhancement,open,0,4,https://github.com/jax-ml/jax/issues/25057,"The approach you're using here looks good to me! One suggestion that might improve performance would be to push the batching logic into Cython. In other words, perhaps you could update `slasd4_all` (etc.) to support 2D inputs and then have a nested `for`loop in the body. Then you could use `vmap_method='broadcast_all'` instead of `vmap_method='sequential'`. This might help because the sequential method results in a separate Python call (with the associated overhead) for each element in the batch, instead of just a single ""vectorized"" Python call. On the longer term, it's a reasonable feature request for JAX to support this operation outofthebox, but it would need a strong argument for the team to implement it at high priority. So, it would be useful to make sure that your current implementation works well enough for now!","Thanks Dan! I appreciate the tip, and I understand that this is a pretty niche matrix operation.","Hey all, I just happened to be looking for this operation and found this discussion. Are you sure the complexity is quadratic? The linked discussion on math.stackexchange suggests that quadratic complexity is only possible for the eigenvalues, and the formula that I think you use for eigenvectors, given in ""RankOne Modification of the Symmetric Eigenproblem."" by Bunch et al (https://eudml.org/doc/132565content) is cubic in the dimension (or in the number of eigenvalues after deflation; per attached screenshot): !image For my particular use case this result is suggesting that updates are not worth it (my matrix is low rank), so I am curious if I'm missing something. ","You are absolutely correct. The O(n^2) statement I made was only about the eigenvalues. Regardless, the matrix multiplication to compute the eigenvalues afterwards is a lot faster (even if it runs in O(n^3)), and I believe it is quadratic in its memory usage (I am not sure about eigenvalue decomposition but I believe it is cubic)."
yi,Added stream annotation support via @compute_on('stream:#') decorator,"This is a tiny change that will add the stream annotation `frontend_attribute` when usin a `compute_on(""stream:"")` decorator. This feature is intended to be used when trying to implement a collective matrix multiplication ""by hand"" within a `shard_map` to be able to better utilize SMs.  During most cublass / cutlass gemm subroutines, not all SMs are utilized by the kernels due to a process called ""wave quantization"". Essentially what happens is that each gemm kernel will only use SMs until it reaches an amount that is a divisor of the contracting dimension, leaving the other SMs essentially sitting idle. These idle SMs prevent us from getting full performance of our GPUs.  If we want to be able to use these idle SMs in our CM subroutine, we would need to be able to run at least 2 gemms in parallel. That isn't currently possible within a `shard_map`, as all of the compute kernels get launched on a single cuda stream. By adding this feature, we would be able to explicitly launch these kernels on several streams. This feature is not yet fully enabled in XLA, and will sit behind a `xla_gpu_experimental_stream_annotation` flag for the foreseeable future. When this flag is not enabled, this attribute is just ignored. ",2024-11-22T15:33:57Z,pull ready,open,0,2,https://github.com/jax-ml/jax/issues/25056,Would be nice to include more in the description of why support this! i.e. make the description more fleshed out.,"I added the test, updated the description, and changed the `startswith` checks to look for `stream:`"
yi,Rework custom hermetic python instructions. ,"Rework custom hermetic python instructions.  The focus was shifted from how one should build custom python, as it seems like people don't really have issues with that and the process is fairly standard.  Instead the focus was made on demystifying of what hermetic (custom or not) Python actually is and explaining how a user can customize the build while still keeping it as close to a regular Python workflow as possible.",2024-11-22T09:52:55Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25052
yi,Running the executable compiled directly from jax.jit is more than three times slower than jax.jit itself.," Description I want to save the executable file generated by jax.jit in the main process and execute this executable file in another process. However, I have found that the performance of the executable file is much slower than that of the JIT itself. I would like to know why this is the case.  1.The execution code and results of jax.jit are as follows.  1.1 The execution code  ``` from flax import linen as nn from flax.training import train_state import jax import jax.numpy as jnp import numpy as np import optax import tqdm class CNN(nn.Module):   """"""A simple CNN model.""""""   .compact   def __call__(self, x):     x = nn.Conv(features=32, kernel_size=(3, 3))(x)     x = nn.relu(x)     x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))     x = nn.Conv(features=64, kernel_size=(3, 3))(x)     x = nn.relu(x)     x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))     x = x.reshape((x.shape[0], 1))   flatten     x = nn.Dense(features=256)(x)     x = nn.relu(x)     x = nn.Dense(features=10)(x)     return x .jit def train_step(state, images, labels):   """"""Computes gradients, loss and accuracy for a single batch.""""""   def loss_fn(params):     logits = state.apply_fn({'params': params}, images)     one_hot = jax.nn.one_hot(labels, 10)     loss = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=one_hot))     return loss, logits   grad_fn = jax.value_and_grad(loss_fn, has_aux=True)   (loss, logits), grads = grad_fn(state.params)   accuracy = jnp.mean(jnp.argmax(logits, 1) == labels)   state = state.apply_gradients(grads=grads)   return state, loss, accuracy def train_epoch(state,  steps_per_epoch):   """"""Train for a single epoch.""""""   rng_key = jax.random.PRNGKey(42)   train_input = jax.random.uniform(rng_key, shape=(128 ,64 ,64, 3))   train_label = jax.numpy.ones(shape=(128,))   for i in tqdm.tqdm(range(steps_per_epoch)):     state, loss, accuracy = train_step(state, train_input, train_label) def create_train_state():   """"""Creates initial `TrainState`.""""""   cnn = CNN()   rng = jax.random.PRNGKey(42)   params = cnn.init(rng, jnp.ones([1, 64, 64, 3]))['params']   tx = optax.sgd(0.9)   return train_state.TrainState.create(       apply_fn=cnn.apply, params=params, tx=tx) if __name__ == ""__main__"":   state = create_train_state()   for i in range(5):     train_epoch(state, 10000) ```  1.2 The execution results   Train for 5 epochs, with 10,000 steps per epoch, and each epoch takes 15 seconds. !cnn  2.The code and results from running the executable file generated by jax.jit are as follows.  2.1 The code  ``` import os import tqdm os.environ[""XLA_PYTHON_CLIENT_ALLOCATOR""] = ""platform"" os.environ[""JAX_ENABLE_X64""] = ""1"" import jax jax.config.update(""jax_platform_name"", ""gpu"") import jax.numpy as jnp import numpy as np import jaxlib.xla_extension as xe from jax.lib import (xla_bridge as xb, xla_client as xc) print(xb.get_backend().platform) import pickle backend = xb.get_backend(""cuda"") with open(""./xla_executable.exe"", ""rb"") as f:     a = f.read()     compiled = backend.deserialize_executable(a) print(compiled.hlo_modules()) with open(""./input_bufs.pkl"", ""rb"") as f:     input_array0 = pickle.load(f) input_array = jax.device_put(input_array0) jax.block_until_ready(input_array) for i in range(5):     for i in tqdm.tqdm(range(10000)):         out_ = compiled.execute_sharded(input_array) ```  2.2 The results  Train for 5 epochs, with 10,000 steps per epoch, and each epoch takes 50 seconds. !executable  System info (python version, jaxlib version, accelerator, etc.) !image",2024-11-21T01:51:41Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/25023,Can you try using these APIs instead? https://github.com/jaxml/jax/blob/main/jax/experimental/serialize_executable.pyL25L60 or jax.export: https://jax.readthedocs.io/en/latest/export/export.html
yi,libtpu.so Not Found for JAX TPU 0.4.14," Description Hi, I'm trying to **install jax[tpu]==0.4.14 on a V5e256 TPU pod** by `pip install U ""jax[tpu]==0.4.14"" f https://storage.googleapis.com/jaxreleases/libtpu_releases.html`.  However, after installation, when I launch a jax job, I got the below error: ``` RuntimeError: Unable to initialize backend 'tpu': INTERNAL: Failed to open /home/XXXXX/.local/lib/python3.10/sitepackages/libtpu/libtpu.so: /home/XXXXX/.local/lib/python3.10/sitepackages/libtpu/libtpu.so: cannot open shared object file: No such file or directory (set JAX_PLATFORMS='' to automatically choose an available backend) ``` I then check ed`/home/XXXXX/.local/lib/python3.10/sitepackages/libtpu` and found no `libtpu.so` is there. Could you please help me with this? Thank you very much!   System info (python version, jaxlib version, accelerator, etc.) ``` flax                     0.7.0 jax                      0.4.14 jaxlib                   0.4.14 optax                    0.1.7 orbaxcheckpoint         0.5.16 ```",2024-11-19T22:45:21Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/24987,"I can reproduce this with `jax[tpu]==0.4.14`, which pins `libtpunightly==0.1.dev20230727`. If I update to `jax[tpu]==0.4.16`, which pins `libtpunightly==0.1.dev20230918`, the issue goes away. I suspect this means that that version of libtpunightly has a bug that was undetected at the time; your best bet is probably to upgrade to a more recent JAX release: the issue has already been fixed, and in general we don't do patch releases for old JAX versions.",I see. Will try to upgrade my JAX. Thank you very much!
yi,Disable some complex function accuracy tests that fail on Mac ARM.,Issue https://github.com/jaxml/jax/issues/24787 ( FYI),2024-11-18T22:05:46Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/24957
yi,fix typo in numpy/__init__.pyi,,2024-11-18T19:04:53Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/24952
rag,Bump hypothesis from 6.102.4 to 6.119.3,"Bumps hypothesis from 6.102.4 to 6.119.3.  Release notes Sourced from hypothesis's releases.  Hypothesis for Python  version 6.119.3 Hypothesis collects coverage information during the &quot;shrink&quot; and &quot;explain&quot; phases in order to show a more informative error message. On 3.12+, this uses &quot;sys.monitoring&quot;. This patch improves the performance of coverage collection on 3.12+ by disabling events we don't need. The canonical version of these notes (with links) is on readthedocs. Hypothesis for Python  version 6.119.2 This patch refactors some internals to prepare for future work using our IR (issue  CC(update version and changelog for pypi)). The canonical version of these notes (with links) is on readthedocs. Hypothesis for Python  version 6.119.1 This patch migrates some more internals (around generating novel inputs) to the IR layer (issue  CC(update version and changelog for pypi)). The canonical version of these notes (with links) is on readthedocs. Hypothesis for Python  version 6.119.0 This release improves Hypothesis' handling of ExceptionGroup  it's now able to detect marker detections if they're inside a  group and attempts to resolve them. Note that this handling is still a work in progress and might not handle edge cases optimally. Please open issues if you encounter any problems or unexpected behavior with it. The canonical version of these notes (with links) is on readthedocs. Hypothesis for Python  version 6.118.9 Internal refactorings in preparation for upcoming changes. The canonical version of these notes (with links) is on readthedocs. Hypothesis for Python  version 6.118.8 Internal renamings. The canonical version of these notes (with links) is on readthedocs. Hypothesis for Python  version 6.118.7 This patch removes some &quot; type: ignore&quot; comments following a mypy update. The canonical version of these notes (with links) is on readthedocs. Hypothesis for Python  version 6.118.6 When Hypothesis replays examples from its test database that it knows were previously fully shrunk it will no longer try to shrink them again.   ... (truncated)   Commits  cc87f33 Bump hypothesispython version to 6.119.3 and update changelog 97d63c0 Merge pull request  CC([jax2tf] Turn on with_gradient by default) from tybug/disablehypothesisevents 14d9b2b fixes 96d74dc disable line coverage events for hypothesis files 01d5d3e Bump hypothesispython version to 6.119.2 and update changelog e09d087 Merge pull request  CC(Gradient of squared 2norm returns NaN when input is vector of zeros) from tybug/testsir 5c0d5f3 check overrun first aba96ba wrong cache! 9f8656f more coverage for ir node serialization 91a9c61 migrate most cached_test_function usages Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-11-18T18:00:27Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/24950,Superseded by CC(Bump hypothesis from 6.102.4 to 6.119.4).
rag,[mosaic_gpu] Fix signedness handling in FragmentedArray._pointwise.,[mosaic_gpu] Fix signedness handling in FragmentedArray._pointwise. Only propagate signedness from operands when the output type of `op` is an `ir.IntegerType`.,2024-11-18T16:12:36Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24946
rag,[mosaic_gpu] Fixed `FragmentedArray` comparisons with literals,[mosaic_gpu] Fixed `FragmentedArray` comparisons with literals,2024-11-18T11:29:47Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24943
yi,[GPU] FlashAttention performance lags behind PyTorch," Description I'm benchmarking naive FlashAttention in `Jax` vs. the Pallas's version of `FA3` vs. the new `dot_product_attention` interface with `cudnn` backend.  JAX/XLA's performance:   !image  Torch's performance:   !image Why the discrepancy? I'd have expected performance to touch 550600 TFLOPS/s. I'm using a few XLA flags, as specified in the script below  but is there anything I'm missing? Or is this about the maximum `XLA` can deliver on H100s?  Steps to reproduce 1. Recreate the environment using `uv`. I'm assuming the drivers are installed. If not, you can use the `pytorch/pytorch:2.4.0cuda12.4.1cudnn8runtime` image on the GPU, run the preliminary `aptget update` and `aptget upgrade` to set everything up. ```py pip3 install uv uv venv 'main_env' python 3.11 source main_env/bin/activate uv pip install U ""jax[cuda12]"" uv pip install q einops tqdm jaxtyping optax optuna equinox rich uv pip install q nvitop pdbpp tabulate ``` 3. Run either script **JAX script**  ```py import os, sys os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.9' os.environ['XLA_FLAGS'] = (     'xla_gpu_enable_triton_softmax_fusion=true '     'xla_gpu_enable_cudnn_fmha=true' ) import math import time from tabulate import tabulate import jax import jax.numpy as jnp import numpy as np from jax.tree_util import tree_map from jax.experimental.pallas.ops.gpu.attention import mha as pallas_mha from functools import partial class Timer(object):     def __init__(self, into=None):         self.into = into     def __enter__(self):         self.start = time.time()     def __exit__(self, type, value, traceback):         if self.into is not None:             self.into.append(time.time()  self.start)     def elapsed(self):         return time.time()  self.start def cartesian(*lists):     if lists:         xs = lists[0]         for x in xs:             for rest in cartesian(*lists[1:]):                 yield (x,) + rest     else:         yield () def cross_attn_flops(B,T,TK,H,C):     HC = H*C      T = TK for selfattention     flops_fwd = (         2 * B*H*T*TK*C +  S = Q         3 * B*H*T*TK +   P=softmax(S)         2 * B*T*TK*H*C  O = P     )     return flops_fwd def attn_flops(B,T,H,C):     return cross_attn_flops(B,T,T,H,C) dtype = jnp.float16 print(f'Using dtype: {dtype}') def convert(xs):     return tree_map(lambda x: x.astype(dtype), xs) def ref_fwd(q,k,v):      reference implementation     [n, l, h, d] = q.shape     [n, lk, hk, d] = k.shape     softmax_scale = 1 / math.sqrt(d)     S = jnp.einsum('nlhd,nLhd>nhlL',q,k)     P = jax.nn.softmax(S*softmax_scale, axis=1)     o = jnp.einsum('nhlL,nLhd>nlhd',P,v)     return o.astype(q.dtype) def jax_dpa_fwd(q, k, v):     output = jax.nn.dot_product_attention(         query=q,         key=k,         value=v,         implementation='cudnn'     )     return output   Bx = [8,  16]  batch size Tx = [1024, 2048]  seqlen Hx = [16, 32]  number of heads Cx = [64, 128]  head dim sx = [2]  steps def bench_attn(mha):     .jit     def bench(q, k, v, steps: int):         for i in steps:             out = mha(q, k, v)         return out     times = []     table = {}     for B,T,H,C,s in cartesian(Bx,Tx,Hx,Cx,sx):         q = jax.random.normal(jax.random.PRNGKey(0), [B, T, H, C], dtype=dtype)         k = jax.random.normal(jax.random.PRNGKey(1), [B, T, H, C], dtype=dtype)         v = jax.random.normal(jax.random.PRNGKey(2), [B, T, H, C], dtype=dtype)         steps = jnp.arange(s)          Warmup         for _ in range(2):             bench(q, k, v, steps).block_until_ready()         out = []         for _ in range(8):             with Timer(out):                 bench(q, k, v, steps).block_until_ready()         t = sum(out[2:])/len(out[2:])         flop = attn_flops(B,T,H,C) * s         tf = flop / t / 1e12 / s         print(f'flops ({B} {T} {H} {C} / {s}): {tf}T', out)         table[(B,T,H,C,s)] = tf         times.append(t)     return table, times naive_flops, custom_time = bench_attn(ref_fwd) pallas_flops, pallas_time = bench_attn(partial(pallas_mha, segment_ids=None)) jax_dpa_flops, dpa_time = bench_attn(jax_dpa_fwd) table = [] for idx, (B,T,H,C,s) in enumerate(cartesian(Bx,Tx,Hx,Cx,sx)):     n_flops, n_time = naive_flops[(B,T,H,C,s)], custom_time[idx]     p_flops, p_time = pallas_flops[(B,T,H,C,s)], pallas_time[idx]     j_flops, j_time = jax_dpa_flops[(B,T,H,C,s)], dpa_time[idx]     table.append((B,T,H,C, n_flops, p_flops, j_flops, attn_flops(B,T,H,C), n_time, p_time, j_time)) print(tabulate(table, headers=['B','T','H','C','TFlop/s (naive)','TFlop/s (pallas)','TFlop/s (jax_dpa)', 'FLOPs', 'Naive time', 'Pallas time', 'DPA time'], floatfmt='.5f')) ```    **PyTorch Benchmark script**  ```py import torch torch._dynamo.config.cache_size_limit = 10000   Increase cache size to 10,000 import time from tabulate import tabulate from typing import Optional from itertools import product from torch.utils.flop_counter import FlopCounterMode class Timer:     def __init__(self, into=None):         self.into = into     def __enter__(self):         self.start = time.time()     def __exit__(self, type, value, traceback):         if self.into is not None:             self.into.append(time.time()  self.start) class OptimizedMHA(torch.nn.Module):     def __init__(self):         super().__init__()     def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor) > torch.Tensor:         B, T, H, C = q.shape         scale = 1.0 / (C ** 0.5)          Compute attention scores         attn = torch.matmul(q, k.transpose(2, 1)) * scale   [B, H, T, T]         attn = torch.softmax(attn, dim=1)          Apply attention to values         out = torch.matmul(attn, v)   [B, H, T, C]         out = out.transpose(1, 2)   [B, T, H, C]         return out class TorchMHA(torch.nn.Module):     def __init__(self):         super().__init__()     def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor) > torch.Tensor:         return torch.nn.functional.scaled_dot_product_attention(             q, k, v,             dropout_p=0.0,             is_causal=False         ) def get_flops(model, sample_inputs):     """"""Get FLOPs using FlopCounterMode""""""     with FlopCounterMode(model) as flop_counter:         _ = model(*sample_inputs)     return flop_counter.get_total_flops() def bench_implementation(model, q, k, v, warmup=10, steps=100):     """"""Benchmark a specific implementation""""""      Warmup     for _ in range(warmup):         with torch.no_grad():             _ = model(q, k, v)     torch.cuda.synchronize()      Benchmark timing     times = []     for _ in range(steps):         with torch.no_grad(), Timer(times):             _ = model(q, k, v)             torch.cuda.synchronize()      Calculate statistics     times = times[10:]   Discard first 10 runs     avg_time = sum(times) / len(times)     return avg_time def bench_attention():      Configuration     device = torch.device(""cuda"")     dtype = torch.float16      Test parameters     Bx = [8, 16]                batch size     Tx = [1024, 2048]             sequence length     Hx = [16, 32]           number of heads     Cx = [64, 128]      head dimension     sx = [4]                steps to run      Initialize models     custom_model = OptimizedMHA().to(device)     torch_model = TorchMHA().to(device)      Compile both models     compiled_custom = torch.compile(         custom_model,         mode=""maxautotunenocudagraphs"",         fullgraph=True,     )     compiled_torch = torch.compile(         torch_model,         mode=""maxautotunenocudagraphs"",         fullgraph=True,     )     results = []      Run benchmarks for each configuration     for B, T, H, C, s in product(Bx, Tx, Hx, Cx, sx):          Create input tensors         q = torch.randn(B, T, H, C, device=device, dtype=dtype)         k = torch.randn(B, T, H, C, device=device, dtype=dtype)         v = torch.randn(B, T, H, C, device=device, dtype=dtype)         q = q.transpose(1, 2)   [B, H, T, C]         k = k.transpose(1, 2)   [B, H, T, C]         v = v.transpose(1, 2)   [B, H, T, C]          Get FLOPs using FlopCounterMode (on CPU with float32)         model_cpu = OptimizedMHA()         q_cpu = q.cpu().float()         k_cpu = k.cpu().float()         v_cpu = v.cpu().float()         flops = get_flops(model_cpu, (q_cpu, k_cpu, v_cpu))          Benchmark both implementations         custom_time = bench_implementation(compiled_custom, q, k, v)         torch_time = bench_implementation(compiled_torch, q, k, v)          Calculate TFLOPs/s for both         custom_tflops = flops / custom_time / 1e12         torch_tflops = flops / torch_time / 1e12          Calculate speedup         speedup = custom_time / torch_time   >1 means torch is faster         print(f""\nConfig (B={B}, T={T}, H={H}, C={C}):"")         print(f""Custom impl: {custom_tflops:.2f} TFlop/s"")         print(f""Torch impl: {torch_tflops:.2f} TFlop/s"")         print(f""Speedup (Torch vs Custom): {speedup:.2f}x"")         print(f""Measured FLOPs: {flops:,}"")         results.append((             B, T, H, C,             round(custom_tflops, 2),             round(torch_tflops, 2),             round(speedup, 2),             flops,             custom_time,             torch_time         ))      Print results table     headers = [         'Batch', 'SeqLen', 'Heads', 'HeadDim',         'Naive MHA TFlop/s', 'SDPA TFlop/s',         'Advantage', 'FLOPs', 'Custom Time', 'SDPA Time'     ]     print(""\nResults:"")     print(tabulate(results, headers=headers, floatfmt='.5f')) if __name__ == ""__main__"":     bench_attention() ```      System info (python version, jaxlib version, accelerator, etc.) ``` >>> import jax; jax.print_environment_info() jax:    0.4.35 jaxlib: 0.4.34 numpy:  2.1.3 python: 3.11.10 (main, Oct 16 2024, 04:38:48) [Clang 18.1.8 ] device info: NVIDIA H100 PCIe1, 1 local devices"" process_count: 1 platform: uname_result(system='Linux', node='niftyorthodoxwhale', release='6.8.040generic', version=' CC(Invalid proto descriptor for file ""tensorflow/compiler/xla/xla_data.proto"")~22.04.3Ubuntu SMP PREEMPT_DYNAMIC Tue Jul 30 17:30:19 UTC 2', machine='x86_64') $ nvidiasmi Sun Nov 17 02:04:44 2024 ++  ++ ```",2024-11-17T02:14:54Z,bug,open,0,4,https://github.com/jax-ml/jax/issues/24934,Update: I changed the `torch` script to use `FlopCounterMode`. Now the results are more realistic/accurate but JAX still lags behind despite explicitly being forced to use `CuDNN`.  ,"i think your torch script might not work as expected, since the inputs format of `torch.nn.functional.scaled_dot_product_attention` is [B, H, T, C] [https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html] ","This is a complicated benchmarking setup, with many things potentially going wrong. Can you simplify this to just measuring milliseconds, and also have a correctness test (that PyTorch and JAX give the same output for the same input)."," Thank you for pointing that out.  I have updated both scripts to now report times as well. However, I opted to skip correctness tests because reproducibility requires us to sacrifice performance which I'm afraid of touching The variance however is very low between runs plus we can average over multiple steps (`sx`) so this should be a nonissue.  On `A100`:  **JAX**: !image  **Torch**: !image"
yi,Allow einsum to support naive contraction strategy,"I would like to compute an einsum according to the following formula: ``` n = 8192 arrays = [jax.random.normal(key=jax.random.PRNGKey(0), shape=(n, n)) for _ in range(6)] formula = 'ij,ik,il,jk,jl,kl>ij' ``` I want to express the computation as 4 nested for loops over indices i, j, k, l without creating any intermediate arrays.  As far as einsum_path is concerned, I can do this by passing the einsum path directly as [(0, 1, 2, 3, 4, 5)] via the optimize kwarg). ``` >>> jax.numpy.einsum_path(formula,` *arrays, optimize=[(0,1,2,3,4,5)]) Complete contraction:  ij,ik,il,jk,jl,kl>ij           Naive scaling:  4       Optimized scaling:  4        Naive FLOP count:  2.702e+16    Optimized FLOP count:  2.702e+16     Theoretical speedup:  1.000e+0    Largest intermediate:  6.711e+7 elements    scaling        BLAS                current                             remaining       4              0  kl,jl,jk,il,ik,ij>ij                                ij>ij) ``` However, when I try to do the einsum, I get this NotImplementedError with a comment that says "" if this is actually reachable, open an issue!"" https://github.com/jaxml/jax/blob/main/jax/_src/numpy/lax_numpy.pyL9775 ``` >>> ans = jnp.einsum(formula, *arrays, optimize=[(0,1,2,3,4,5)]) >>> ans.block_until_ready() ```",2024-11-15T17:39:48Z,enhancement,open,0,12,https://github.com/jax-ml/jax/issues/24915,"I think your path specification is invalid. For example, if you pass it to NumPy, you get this error: ```python np.einsum(formula, *arrays, optimize=[(0,1,2,3,4,5)]) ``` ```pytb Traceback (most recent call last):   File ""/Users/vanderplas/github/google/jax/tmp.py"", line 9, in      np.einsum(formula, *arrays, optimize=[(0,1,2,3,4,5)])   File ""/Users/vanderplas/.local/share/virtualenvs/jaxLBbfM5ix/lib/python3.12/sitepackages/numpy/_core/einsumfunc.py"", line 1441, in einsum     operands, contraction_list = einsum_path(*operands, optimize=optimize,                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Users/vanderplas/.local/share/virtualenvs/jaxLBbfM5ix/lib/python3.12/sitepackages/numpy/_core/einsumfunc.py"", line 878, in einsum_path     raise TypeError(""Did not understand the path: %s"" % str(path_type)) TypeError: Did not understand the path: [(0, 1, 2, 3, 4, 5)] ```","Thank you for taking a look!  My understanding is that this path is the default behavior for numpy.  I.e., it corresponds to the basic implementation that you have in  https://github.com/jaxml/jax/blob/main/tests/lax_numpy_einsum_test.pyL295 It is much more memory efficient than doing the einsum as a sequence of dot_general's in this case, which from my investigation is hardcoded into the JAX implementation.  It makes sense because dot_general is very highly optimized, but being able to get the more memoryefficient behavior seems desirable in some settings.  ","I prototyped a version of this using a sequence of nested jax.lax.scan calls, but it was ugly and I don't think the most performant.  I also played around with using Jax.vmap over the indices (i, j) and using jnp.einsum using the perelement path Complete contraction:  ij,ik,il,jk,jl,kl>ij [vmap] PerRow contraction:  j,k,l,jk,jl,kl>j [double vmap] Perelement contraction:  ,k,l,k,l,kl> It was pretty cool to use JAX's abstractions to achieve this, and the vmap implementation did have better performance characteristics than jnp.einsum  in this case, but I still think it uses more memory than the naive approach.   If Jax.lax.map supported the in_axes argument, I think that would help, since I could just replace my usage of vmap with map.","Here is a basic implementation of the naive strategy in terms of jax.vmap and jax.lax.scan, specialized to the formula 'ij,ik,il,jk,jl,kl>ij'. ``` import jax import jax.numpy as jnp import time def inner_einsum(*arrays):    computes einsum for ,k,l,k,l,kl>    Does not create any intermediate arrays   A, B, C, D, E, F = arrays   K, L = B.size, C.size   def foo(partial1, k):     def bar(partial2, l):       return partial2 + C[l] * E[l] * F[k, l], ()     return partial1 + B[k] * D[k] * jax.lax.scan(bar, 0, jnp.arange(L))[0], ()   return A * jax.lax.scan(foo, 0, jnp.arange(K))[0] .jit def vmap_einsum(*arrays):    computes einsum for ij,ik,il,jk,jl,kl>ij naively    No memory overhead.  Vectorized across output cells.   return jax.vmap(       jax.vmap(inner_einsum, in_axes=(0, None, None, 0, 0, None)),       in_axes=(0, 0, 0, None, None, None)   )(*arrays) .jit def default_einsum(*arrays):   return jnp.einsum('ij,ik,il,jk,jl,kl>ij', *arrays) ``` when I benchmark it using n x n arrays for n = [128, 256, 512, 1024] here is what I get for timing information (measured in seconds, not counting JIT compilation).  The story is that jnp.einsum is faster up to n=512, but fails at n=1024, while the naive approach implemented above still runs, albeit it takes more time than I'd like. ``` n=128 vmap_einsum 0.14367246627807617 default_einsum 0.002198457717895508 n=256 vmap_einsum 0.7639327049255371 default_einsum 0.017670154571533203 n=512 vmap_einsum 4.290320158004761 default_einsum 0.24642205238342285 n=1024 vmap_einsum 35.70246410369873  XlaRuntimeError                           Traceback (most recent call last) [](https://colab.corp.google.com/drive/1s4c4kdOR2VNVoKIyZHlj0M9gh0wm_Am9) in ()       4        5   for einsum_fn in EINSUM_IMPLS: > 6     jax.block_until_ready(einsum_fn(*arrays))       7     t0 = time.time()       8     jax.block_until_ready(einsum_fn(*arrays))     [... skipping hidden 5 frame] ```","Here's another impl one can throw into the mix: `scan_einsum` where we strip out a nonoutput axis and sequentially compute + add up the resulting smaller einsums, as follows: ``` .jit def scan_einsum(*arrays):    we will scan over k and build up a running sum   A, B, C, D, E, F = arrays   K = B.shape[1]   zeros = jnp.zeros(A.shape)   def add_small_einsum(partial, k):      einsum with k stripped out      i,j,i,il,j,jl,l>ij     return partial + jnp.einsum('ij,i,il,j,jl,l>ij', A, B[:,k], C, D[:,k], E, F[k,:]), ()   return jax.lax.scan(add_small_einsum, zeros, jnp.arange(K))[0] ``` Benchmarks show that this is significantly better than the vmap_einsum above.  And it's even better than jnp.einsum beyond n=256 ``` n=128 vmap_einsum 0.13236498832702637 scan_einsum 0.0034575462341308594 default_einsum 0.0014224052429199219 n=256 vmap_einsum 0.7413990497589111 scan_einsum 0.011484861373901367 default_einsum 0.018535137176513672 n=512 vmap_einsum 4.2713000774383545 scan_einsum 0.04682159423828125 default_einsum 0.23777055740356445 n=1024 vmap_einsum 35.49849033355713 scan_einsum 0.47335124015808105 XlaRuntimeError ```","If anyone is interested, I typed up this exploration on my blog: https://www.ryanhmckenna.com/2024/11/exploringmultiinputeinsumsinjax.html","Thanks for exploring this – are you running benchmarks on GPU/TPU as well, or just CPU? The reason I ask is that `scan` has a pretty big performance penalty on accelerators (essentially each iteration is its own kernel launch) so I expect any efficiency gains on CPU will not transfer to GPU or TPU.","These tests were done in a colab sandbox with GPU, happy to do some more benchmarking if there's something specific you'd like to see","OK, thanks. Overall, I tend to be 1 on changes like this. It greatly complicates things on the JAX side in order to make up for deficiencies in the compiler. The compiler behavior may be improved in the future, at which point we would needlessly be generating more complicated code with no clear way of alerting ourselves that this is the case.","Is this a compiler deficiency though?  My understanding is it is a JAX implementation choice that leads to this behavior, specifically https://github.com/jaxml/jax/blob/main/jax/_src/numpy/lax_numpy.pyL9773, which implements einsum in terms of a ""_dot_general"" primitive, which I believe means the einsum is calculated as a sequence of pairwise contractions.  Even if the compiler was better at _dot_general, it wouldn't get around the intractability of storing the required n^3 sized intermediates in this case. Happy to keep this alternate implementation local to where I need it though to keep the jax impls simpler though.","The compiler often fuses sequences of operations into single kernels to avoid storing intermediates. There may already be fusion paths for sequences of `dot_general` in some situations, but I'm not sure. `scan` is a much less specific primitive than dot general, so emitting scan would hamper the ability of the compiler to make such optimizations in the future. I'm not saying your code is not useful; I think the approach probably makes sense in some situations. I just don't think it's a good fit for JAX's einsum implementation. (If  disagrees though, I'm happy to defer to his judgment here).","Ah I see that makes sense, do you think I should open up an issue at https://github.com/openxla/xla in that case?"
yi,Set `__module__` attribute for objects in jax.numpy,"The current `__module__` string is an implementation detail, and displays internal paths where we should display public paths. For example, this is the current output: ```python In [2]: jax.numpy.issubdtype Out[5]:  'bool'> ``` and this is the output after this change: ```python In [2]: jax.numpy.issubdtype Out[2]:  'bool'> ``` We've done this already for several other toplevel modules; this PR continues the effort to improve this situation.",2024-11-15T14:39:54Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/24912,> Can you add the reason for doing this to the PR description? Done
yi,Add JAX_COMPILATION_CACHE_EXPECT_PGLE option,"This aims to provide a better PGLE workflow that is compatible with profiling with Nsight Systems on GPU. This is nontrivial because CUPTI, the interface used by CUDA profiling tools, only supports profiling by one tool at a time, meaning that the JAX profiler used by PGLE and Nsight Systems conflict with one another. The PR adds a new JAX config option `compilation_cache_expect_pgle` that tells JAX to attempt to load PGLEoptimised entries from the compilation cache even if PGLE is disabled, and print warnings on certain unexpected results. With this, a workflow like: ```console $ rm rf /root/jax_cache/ $ export JAX_ENABLE_COMPILATION_CACHE=yes           not strictly needed, on by default $ export JAX_COMPILATION_CACHE_DIR=/root/jax_cache  not needed in this example because MaxText configures it $ JAX_ENABLE_PGLE=yes testmaxtext.sh modelname=gemma2b $ ls 1 /root/jax_cache/*cache /root/jax_cache/jit_initialize_state42f5c604b3add9a249cc00624720755475e29b9ab7007d8f5b781abb34061775cache /root/jax_cache/jit_train_step0ec4a202e1bd3117ab79a0585a981305f2d5b2dfab3b0741445747ba463f2a20cache /root/jax_cache/jit_train_step4a215955e445e4383b9ccf1f283143331a1118edfb9b281f53ae02c793b77ccfcache $ JAX_COMPILATION_CACHE_EXPECT_PGLE=yes nsys profile testmaxtext.sh modelname=gemma2b ... W1114 02:44:52.551798 140529154335808 compiler.py:381] PERSISTENT CACHE MISS for PGLEoptimized jit_initialize_state despite nonPGLE hit; it may not have been executed enough times when the cache was populated ... ``` is possible. Warnings are added in three cases if `JAX_COMPILATION_CACHE_EXPECT_PGLE` is enabled:  If a module is cached without PGLE optimisations but not with them. That is typical of modules that were not executed enough times in the first (cachepopulating) run to reach the threshold for recompilation with profile data. This is seen above. We would rely on the user to see ""initialize ... not executed enough times"" and think ""sounds fine"".  If a module is written to the cache. This is typical of a cachepopulating run that did not hit as many code paths as the second run with Nsight Systems + `JAX_COMPILATION_CACHE_EXPECT_PGLE`.  If the PGLE profiler returns an empty profile. This is typical of trying to enable PGLE under Nsight Systems. Note there are many more modules that are cached neither with nor without PGLE optimisations because they are too small or fast to compile to be cached with the default settings.",2024-11-15T10:43:27Z,,open,0,1,https://github.com/jax-ml/jax/issues/24910, PTAL
yi,Don't use an out-of-line lowering for integer_pow for small powers.,"This yields a smaller stablehlo output. Add a fast path for y == 1 and y == 1, which turn out to be reasonably common.",2024-11-14T16:23:11Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/24897
yi,Missing annotations," Description The additions of type annotations for parts of jax was nice, but unfortunately they appear to have diverged from the source itself. For example,  `jax/numpy/__init__.pyi` is missing `jax.numpy.frompyfunc`. This is unfortunate on its own, but also interferes with editors like pycharm, who use the type annotations for autocomplete, etc..  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.35 jaxlib: 0.4.35 numpy:  2.1.3 python: 3.13.0 (main, Oct  7 2024, 05:02:14) [Clang 16.0.0 (clang1600.0.26.4)] device info: cpu1, 1 local devices"" process_count: 1 platform: uname_result(system='Darwin', node='mbp.local', release='24.1.0', version='Darwin Kernel Version 24.1.0: Thu Oct 10 22:06:55 PDT 2024; root:xnu11215.41.3~5/RELEASE_ARM64_T6031', machine='arm64') ```",2024-11-13T21:42:10Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/24888,Thanks for bringing this to our attention! Would you like to send a PR adding `frompyfunc` to `jax/numpy/__init__.pyi`? If not I can put it on my TODO list. Thanks!,"Hi Jake, I'm happy to add such an annotation in a PR, but .. should this not be automated somehow? This is just the one thing I found picking up Jax to try and learn as a novice, will it not become whackamole as new features are added?","the PYI file was automaticallygenerated at one point, and then edited by hand to define the APIs. `jax.numpy` is relatively stable, so this is not something that changes frequently. If you know of any way to automate this, I'm happy to hear about it!","It looks like there are four symbols currently missing from `jax/numpy/__init__.pyi`: ```python In [2]: with open('jax/numpy/__init__.pyi') as f:    ...:     content = f.read() In [3]: for name in dir(jnp):    ...:     if name not in content and not name.startswith('_'):    ...:         print(name)    ...:  frompyfunc int2 ufunc uint2 ``` `int2` and `uint2` can't be added yet, because they will only exist in the namespace if `ml_dtypes` is a sufficiently recent version. But we should add type stubs for `frompyfunc` and `ufunc`."," I guess 's question (and mine) is why a function's type signature/overloads can't be fetched automatically from where it is implemented (e.g. here for `frompyfunc`, here for `where`, etc.), instead of duplicating them in `__init__.pyi`. This would satisfy the DRY principle.","The main reason is that some of these APIs are wrapped in `` (or `(jit, ...)`), and the semantics of `jit` and `partial` are not yet supported by some type checkers.","I'm going to close this, as the reported issue has been addressed."
rag,[Pallas] Increase test coverage of pl.dot.,[Pallas] Increase test coverage of pl.dot.,2024-11-13T17:32:54Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24880
yi,Dedent your yields!,"Dedent your yields! Fixes a surprising interaction between the generator system in linear_util.py and the try/finally python context managers we use for managing tracing context. The `finally` block wasn't always being called until garbage collection, so the context stack pushes/pops weren't always correctly nested. Dedenting the yield fixes this particular bug but longterm we should get rid of linear_util altogether.",2024-11-12T22:32:26Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24870
yi,Unsupported type in metal PJRT plugin with rng_bit_generator," Description Hi all, When executing an HLO program using the Metal PJRT plugin, the program fails due to an unsupported data type eeturned by the rng_bit_generator operation. Specifically, the generated HLO includes: `%output_state, %output = ""mhlo.rng_bit_generator""(%1) }> : (tensor) > (tensor, tensor) ` The error message indicates that: `Metal only supports MPSDataTypeFloat16, MPSDataTypeBFloat16, MPSDataTypeFloat32, MPSDataTypeInt32, and MPSDataTypeInt64.` The use of ui32 seems to be incompatible with Metal’s allowed types. I’m trying to understand if the ui32 output is the problem or maybe the use of rng_bit_generator is wrong. Could you clarify if there is a workaround or planned support for ui32 output in this context? Alternatively, guidance on configuring rng_bit_generator for compatibility with Metal’s supported types would be greatly appreciated. Thanks  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.35 jaxlib: 0.4.35 numpy:  2.1.3 python: 3.13.0 (main, Oct  7 2024, 05:02:14) [Clang 15.0.0 (clang1500.3.9.4)] device info: Metal1, 1 local devices"" process_count: 1 platform: uname_result(system='Darwin', node='MBPdiDavide.station', release='23.6.0', version='Darwin Kernel Version 23.6.0: Mon Jul 29 21:14:30 PDT 2024; root:xnu10063.141.2~1/RELEASE_ARM64_T6030', machine='arm64') ```",2024-11-12T21:50:50Z,bug Apple GPU (Metal) plugin,open,1,0,https://github.com/jax-ml/jax/issues/24867
yi,7x7 `nnx.Conv` using `float32` parameter dtype overflows(?) to `nan` when sharded," Description A sufficientlylarge (7x7) `nnx.Conv` with `float32` parameter dtype, when sharded across multiple devices, generates `nan`, seemingly due to overflow. The `nan` is avoided by making any one of the following changes: 1. Use smaller convolution, e.g. 3x3 2. Enable `param_dtype='float64'` inside `with jax.experimental.enable_x64()` 3. Disable sharding; run on one device only float32 is sufficient for 7x7 convolution when not sharded, suggesting that it ought to work when sharded as well. The issue can be worked around but all of the workarounds are unsatisfactory in some way, either by reducing the size of the convolution, or requiring double the memory usage, or restricting training to a single device. Minimal example. This code works because it's moved to float64: ``` from flax import nnx import jax import jax.nn as jnn import jax.numpy as jnp from jax.sharding import Mesh, NamedSharding, PartitionSpec import optax jax.config.update(""jax_debug_nans"", True) jax.config.update(""jax_debug_infs"", True) W_SMALL=700 H_SMALL=700 W_LARGE=1400 H_LARGE=1400 C=3 DEVICES = jax.devices('gpu')[1:]      Make sure we're a multiple of  of devices  This lets shard us by the batch dimension TRAIN_BATCH=5 * len(DEVICES) class Model(nnx.Module):     def __init__(self):         init_fn = nnx.initializers.lecun_normal()         self.deep = nnx.Conv(             in_features=3,             out_features=3,             kernel_size=(7, 7),             padding='SAME',             rngs=nnx.Rngs(8439),             use_bias=False,              disable this to move to float32             param_dtype='float64',             kernel_init=nnx.with_partitioning(init_fn, (None,)),         )     def __call__(self, x: jax.Array):         out = self.deep(x)         return jnn.sigmoid(out) .jit def loss(pred: jax.Array, large: jax.Array) > jax.Array:     return jnp.mean(optax.squared_error(pred, large)) .jit def train_step(     m: Model,     opt: nnx.Optimizer,     small: jax.Array,     large: jax.Array ):     def loss_fn(m: Model):         pred = m(small)         return loss(pred, large)     l, grads = nnx.value_and_grad(loss_fn)(m)     opt.update(grads)     return l def data(key):     while True:         key, subkey = jax.random.split(key)         large = jax.random.uniform(subkey, (W_LARGE, H_LARGE, C)) * 255         small = jax.image.resize(large, (W_SMALL, H_SMALL, C), 'nearest')         yield small, large .jit def create_sharded_model():     model = Model()     state = nnx.state(model)                    The model's state, a pure pytree.     pspecs = nnx.get_partition_spec(state)      Strip out the annotations from state.     sharded_state = jax.lax.with_sharding_constraint(state, pspecs)     nnx.update(model, sharded_state)            The model is sharded now!     return model if __name__ == ""__main__"":     with jax.experimental.enable_x64():         with Mesh(             devices=DEVICES,             axis_names=('gpu',),         ) as mesh:             data_sharding = NamedSharding(mesh, PartitionSpec('gpu'))              Disable this to disable sharding             m = create_sharded_model()              Use this to disable sharding              m = Model()             print(f""model sharding: {m.deep.kernel.sharding}"")             print(f""model devices: {m.deep.kernel.devices()}"")             print(f""model dtype: {m.deep.kernel.dtype}"")             base_opt = optax.sgd(1e2)             opt = nnx.Optimizer(m, base_opt)             for epoch in range(100):                 batch_small: list[jax.Array] = list()                 batch_large: list[jax.Array] = list()                 for small, large in data(jax.random.key(4389)):                      Initial upscale                     new_shape = (W_LARGE, H_LARGE, 3)                     upscaled = jax.image.resize(small, new_shape, ""nearest"")                     batch_small.append(upscaled)                     batch_large.append(large)                     if len(batch_small) >= TRAIN_BATCH:                         X = jnp.stack(batch_small)                         Y = jnp.stack(batch_large)                         batch_small = list()                         batch_large = list()                          Disable this to disable sharding                         X = jax.device_put(X, data_sharding)                         Y = jax.device_put(Y, data_sharding)                         print(f""X devices: {X.devices()}"")                         print(f""Y devices: {Y.devices()}"")                         print(f""X shape: {X.shape}"")                         print(f""Y shape: {Y.shape}"")                         print(f""X dtype: {X.dtype}"")                         print(f""Y dtype: {Y.dtype}"")                         print(train_step(m, opt, X, Y)) ``` This version fails due to use of sharding plus float32 and a 7x7 convolution: ``` from flax import nnx import jax import jax.nn as jnn import jax.numpy as jnp from jax.sharding import Mesh, NamedSharding, PartitionSpec import optax jax.config.update(""jax_debug_nans"", True) jax.config.update(""jax_debug_infs"", True) W_SMALL=700 H_SMALL=700 W_LARGE=1400 H_LARGE=1400 C=3 DEVICES = jax.devices('gpu')[1:]      Make sure we're a multiple of  of devices  This lets shard us by the batch dimension TRAIN_BATCH=5 * len(DEVICES) class Model(nnx.Module):     def __init__(self):         init_fn = nnx.initializers.lecun_normal()         self.deep = nnx.Conv(             in_features=3,             out_features=3,             kernel_size=(7, 7),             padding='SAME',             rngs=nnx.Rngs(8439),             use_bias=False,              disable this to move to float32              param_dtype='float64',             kernel_init=nnx.with_partitioning(init_fn, (None,)),         )     def __call__(self, x: jax.Array):         out = self.deep(x)         return jnn.sigmoid(out) .jit def loss(pred: jax.Array, large: jax.Array) > jax.Array:     return jnp.mean(optax.squared_error(pred, large)) .jit def train_step(     m: Model,     opt: nnx.Optimizer,     small: jax.Array,     large: jax.Array ):     def loss_fn(m: Model):         pred = m(small)         return loss(pred, large)     l, grads = nnx.value_and_grad(loss_fn)(m)     opt.update(grads)     return l def data(key):     while True:         key, subkey = jax.random.split(key)         large = jax.random.uniform(subkey, (W_LARGE, H_LARGE, C)) * 255         small = jax.image.resize(large, (W_SMALL, H_SMALL, C), 'nearest')         yield small, large .jit def create_sharded_model():     model = Model()     state = nnx.state(model)                    The model's state, a pure pytree.     pspecs = nnx.get_partition_spec(state)      Strip out the annotations from state.     sharded_state = jax.lax.with_sharding_constraint(state, pspecs)     nnx.update(model, sharded_state)            The model is sharded now!     return model if __name__ == ""__main__"":      with jax.experimental.enable_x64():     with Mesh(         devices=DEVICES,         axis_names=('gpu',),     ) as mesh:         data_sharding = NamedSharding(mesh, PartitionSpec('gpu'))          Disable this to disable sharding         m = create_sharded_model()          Use this to disable sharding          m = Model()         print(f""model sharding: {m.deep.kernel.sharding}"")         print(f""model devices: {m.deep.kernel.devices()}"")         print(f""model dtype: {m.deep.kernel.dtype}"")         base_opt = optax.sgd(1e2)         opt = nnx.Optimizer(m, base_opt)         for epoch in range(100):             batch_small: list[jax.Array] = list()             batch_large: list[jax.Array] = list()             for small, large in data(jax.random.key(4389)):                  Initial upscale                 new_shape = (W_LARGE, H_LARGE, 3)                 upscaled = jax.image.resize(small, new_shape, ""nearest"")                 batch_small.append(upscaled)                 batch_large.append(large)                 if len(batch_small) >= TRAIN_BATCH:                     X = jnp.stack(batch_small)                     Y = jnp.stack(batch_large)                     batch_small = list()                     batch_large = list()                      Disable this to disable sharding                     X = jax.device_put(X, data_sharding)                     Y = jax.device_put(Y, data_sharding)                     print(f""X devices: {X.devices()}"")                     print(f""Y devices: {Y.devices()}"")                     print(f""X shape: {X.shape}"")                     print(f""Y shape: {Y.shape}"")                     print(f""X dtype: {X.dtype}"")                     print(f""Y dtype: {Y.dtype}"")                     print(train_step(m, opt, X, Y)) ``` This version works due to using a smaller convolution, still with float32: ``` from flax import nnx import jax import jax.nn as jnn import jax.numpy as jnp from jax.sharding import Mesh, NamedSharding, PartitionSpec import optax jax.config.update(""jax_debug_nans"", True) jax.config.update(""jax_debug_infs"", True) W_SMALL=700 H_SMALL=700 W_LARGE=1400 H_LARGE=1400 C=3 DEVICES = jax.devices('gpu')[1:]      Make sure we're a multiple of  of devices  This lets shard us by the batch dimension TRAIN_BATCH=5 * len(DEVICES) class Model(nnx.Module):     def __init__(self):         init_fn = nnx.initializers.lecun_normal()         self.deep = nnx.Conv(             in_features=3,             out_features=3,             kernel_size=(3, 3),             padding='SAME',             rngs=nnx.Rngs(8439),             use_bias=False,              disable this to move to float32              param_dtype='float64',             kernel_init=nnx.with_partitioning(init_fn, (None,)),         )     def __call__(self, x: jax.Array):         out = self.deep(x)         return jnn.sigmoid(out) .jit def loss(pred: jax.Array, large: jax.Array) > jax.Array:     return jnp.mean(optax.squared_error(pred, large)) .jit def train_step(     m: Model,     opt: nnx.Optimizer,     small: jax.Array,     large: jax.Array ):     def loss_fn(m: Model):         pred = m(small)         return loss(pred, large)     l, grads = nnx.value_and_grad(loss_fn)(m)     opt.update(grads)     return l def data(key):     while True:         key, subkey = jax.random.split(key)         large = jax.random.uniform(subkey, (W_LARGE, H_LARGE, C)) * 255         small = jax.image.resize(large, (W_SMALL, H_SMALL, C), 'nearest')         yield small, large .jit def create_sharded_model():     model = Model()     state = nnx.state(model)                    The model's state, a pure pytree.     pspecs = nnx.get_partition_spec(state)      Strip out the annotations from state.     sharded_state = jax.lax.with_sharding_constraint(state, pspecs)     nnx.update(model, sharded_state)            The model is sharded now!     return model if __name__ == ""__main__"":      with jax.experimental.enable_x64():     with Mesh(         devices=DEVICES,         axis_names=('gpu',),     ) as mesh:         data_sharding = NamedSharding(mesh, PartitionSpec('gpu'))          Disable this to disable sharding         m = create_sharded_model()          Use this to disable sharding          m = Model()         print(f""model sharding: {m.deep.kernel.sharding}"")         print(f""model devices: {m.deep.kernel.devices()}"")         print(f""model dtype: {m.deep.kernel.dtype}"")         base_opt = optax.sgd(1e2)         opt = nnx.Optimizer(m, base_opt)         for epoch in range(100):             batch_small: list[jax.Array] = list()             batch_large: list[jax.Array] = list()             for small, large in data(jax.random.key(4389)):                  Initial upscale                 new_shape = (W_LARGE, H_LARGE, 3)                 upscaled = jax.image.resize(small, new_shape, ""nearest"")                 batch_small.append(upscaled)                 batch_large.append(large)                 if len(batch_small) >= TRAIN_BATCH:                     X = jnp.stack(batch_small)                     Y = jnp.stack(batch_large)                     batch_small = list()                     batch_large = list()                      Disable this to disable sharding                     X = jax.device_put(X, data_sharding)                     Y = jax.device_put(Y, data_sharding)                     print(f""X devices: {X.devices()}"")                     print(f""Y devices: {Y.devices()}"")                     print(f""X shape: {X.shape}"")                     print(f""Y shape: {Y.shape}"")                     print(f""X dtype: {X.dtype}"")                     print(f""Y dtype: {Y.dtype}"")                     print(train_step(m, opt, X, Y)) ``` This version works due to running on a single device, but with float32 and a 7x7 convolution: ``` from flax import nnx import jax import jax.nn as jnn import jax.numpy as jnp from jax.sharding import Mesh, NamedSharding, PartitionSpec import optax jax.config.update(""jax_debug_nans"", True) jax.config.update(""jax_debug_infs"", True) W_SMALL=700 H_SMALL=700 W_LARGE=1400 H_LARGE=1400 C=3 DEVICES = jax.devices('gpu')[1:]      Make sure we're a multiple of  of devices  This lets shard us by the batch dimension TRAIN_BATCH=5 * len(DEVICES) class Model(nnx.Module):     def __init__(self):         init_fn = nnx.initializers.lecun_normal()         self.deep = nnx.Conv(             in_features=3,             out_features=3,             kernel_size=(7, 7),             padding='SAME',             rngs=nnx.Rngs(8439),             use_bias=False,              disable this to move to float32              param_dtype='float64',             kernel_init=nnx.with_partitioning(init_fn, (None,)),         )     def __call__(self, x: jax.Array):         out = self.deep(x)         return jnn.sigmoid(out) .jit def loss(pred: jax.Array, large: jax.Array) > jax.Array:     return jnp.mean(optax.squared_error(pred, large)) .jit def train_step(     m: Model,     opt: nnx.Optimizer,     small: jax.Array,     large: jax.Array ):     def loss_fn(m: Model):         pred = m(small)         return loss(pred, large)     l, grads = nnx.value_and_grad(loss_fn)(m)     opt.update(grads)     return l def data(key):     while True:         key, subkey = jax.random.split(key)         large = jax.random.uniform(subkey, (W_LARGE, H_LARGE, C)) * 255         small = jax.image.resize(large, (W_SMALL, H_SMALL, C), 'nearest')         yield small, large .jit def create_sharded_model():     model = Model()     state = nnx.state(model)                    The model's state, a pure pytree.     pspecs = nnx.get_partition_spec(state)      Strip out the annotations from state.     sharded_state = jax.lax.with_sharding_constraint(state, pspecs)     nnx.update(model, sharded_state)            The model is sharded now!     return model if __name__ == ""__main__"":      with jax.experimental.enable_x64():     with Mesh(         devices=DEVICES,         axis_names=('gpu',),     ) as mesh:         data_sharding = NamedSharding(mesh, PartitionSpec('gpu'))          Disable this to disable sharding          m = create_sharded_model()          Use this to disable sharding         m = Model()         print(f""model sharding: {m.deep.kernel.sharding}"")         print(f""model devices: {m.deep.kernel.devices()}"")         print(f""model dtype: {m.deep.kernel.dtype}"")         base_opt = optax.sgd(1e2)         opt = nnx.Optimizer(m, base_opt)         for epoch in range(100):             batch_small: list[jax.Array] = list()             batch_large: list[jax.Array] = list()             for small, large in data(jax.random.key(4389)):                  Initial upscale                 new_shape = (W_LARGE, H_LARGE, 3)                 upscaled = jax.image.resize(small, new_shape, ""nearest"")                 batch_small.append(upscaled)                 batch_large.append(large)                 if len(batch_small) >= TRAIN_BATCH:                     X = jnp.stack(batch_small)                     Y = jnp.stack(batch_large)                     batch_small = list()                     batch_large = list()                      Disable this to disable sharding                      X = jax.device_put(X, data_sharding)                      Y = jax.device_put(Y, data_sharding)                     print(f""X devices: {X.devices()}"")                     print(f""Y devices: {Y.devices()}"")                     print(f""X shape: {X.shape}"")                     print(f""Y shape: {Y.shape}"")                     print(f""X dtype: {X.dtype}"")                     print(f""Y dtype: {Y.dtype}"")                     print(train_step(m, opt, X, Y)) ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.35 jaxlib: 0.4.34 numpy:  2.1.2 python: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0] device info: NVIDIA RTX 6000 Ada Generation4, 4 local devices"" process_count: 1 platform: uname_result(system='Linux', node='[redacted]', release='6.8.048generic', version=' CC(Unimplemented: binary integer op 'power')~22.04.1Ubuntu SMP PREEMPT_DYNAMIC Mon Oct  7 11:24:13 UTC 2', machine='x86_64') $ nvidiasmi Mon Nov 11 17:11:54 2024 ++  ++ ```",2024-11-12T01:32:17Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/24848,"Apologies, didn't realize flax is in a separate repo. I reposted this there."
yi,Deepcopy of pjit functions failing (or not supported)," Description While trying to get `jsonargparse` to work with defaults on activation functions, I ran into https://github.com/omnius/jsonargparse/issues/619issuecomment2466451720 The preferred MWE from the developer there is repeated below ```python >>> from copy import deepcopy >>> from dataclasses import dataclass, asdict >>> from flax import nnx >>> from typing import Callable >>>  ... class Settings: ...     width: int = 64 ...     activation: Callable = nnx.relu ...     final_activation: Callable = nnx.softplus >>> deepcopy(nnx.softplus) is nnx.softplus False >>> nnx.softplus > >>> deepcopy(nnx.softplus)  ``` It seems like the issue is that a `deepcopy` and perhaps a `asdict` of the jit'd functions is failing or not supported. As said in that post, the workaround shows there is no intrinsic reason `deepcopy` can't be supported.  If we put ``` from types import MethodType nnx.softplus.__deepcopy__ = MethodType(lambda self, memo: self, nnx.softplus) ``` Then things work great.  System info (python version, jaxlib version, accelerator, etc.) This applies to all versions.",2024-11-11T16:49:48Z,bug,open,1,0,https://github.com/jax-ml/jax/issues/24838
yi,TPU with sharding: grpc initialization failure," Description I'm running a decently large project, and I've just attempted to run it on TPUs (v432 node), and I'm getting an interesting error, which seems to be pretty internal: ``` jaxlib.xla_extension.XlaRuntimeError: INTERNAL: during context [preoptimization]: RET_CHECK failure (third_party/tensorflow/compiler/xla/service/hlo_verifier.cc:2519) instructions.size() == 2 channel 30 is used for multiple host send/recv instructions ``` When enabling internal logs, I get (on one of the machines): ``` RuntimeError: Unable to initialize backend 'cpu': ALREADY_EXISTS: Config key cpu:local_topology/cpu/3 already exists. Additional GRPC error information from remote target coordination_service while calling /tensorflow.CoordinationService/InsertKeyValue: :{""created"":"".560649460"",""description"":""Error received from peer ipv4:some ip:8476"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Config key cpu:local_topology/cpu/3 already exists."",""grpc_status"":6} (set JAX_PLATFORMS='' to automatically choose an available backend) ``` The main issue with this bug is that I cannot provide a script to reproduce it: the codebase is very large, and my attempts at reproducing with a small example just don't lead to the error. And, I cannot even give you a link to the repo, because it's confidential. Here is how I do the partitioning (I think these days you should use jax.device_put, but I'm still using the old API cuz I'm used to it): ```py import equinox as eqx dynamic, static = eqx.partition(image_batch, eqx.is_inexact_array) dynamic = jax.lax.with_sharding_constraint(dynamic, sharding) image_sharded = eqx.combine(dynamic, static) ``` Equinox ```partition```/```combine``` just split PyTree into two/combine two PyTrees into one, such that one of the PyTrees contains all the leaves that satisfy a particular condition. This is convenient when trying to pass a partially static PyTree into a vmap, for example. I am 0.8 confident that the issue is not with Equinox. I would appreciate any help :)  System info (python version, jaxlib version, accelerator, etc.) ```bash  python3.11 m pip freeze jax==0.4.35 jaxsmi==1.0.4 jaxlib==0.4.35 jaxtyping==0.2.34  python3.11 version 3.11.10  neofetch OS: Ubuntu 20.04.4 LTS x86_64 Host: Google Compute Engine Kernel: 5.13.01023gcp CPU: AMD EPYC 7B12 (240) @ 2.249GHz ```",2024-11-10T05:59:04Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/24821,"Update: downgrading libtpu does not help, however, if I set ```export JAX_PLATFORMS=''``` I get a different internal error: ``` jaxlib.xla_extension.XlaRuntimeError: UNKNOWN: /home/arst/.local/lib/python3.11/sitepackages/equinox/_jit.py:55:14: error: All components of the offset index in a gather op must either be a offset dimension or explicitly collapsed or explicitly batched; got len(slice_sizes)=4, output_slice_sizes=2, collapsed_slice_dims=1,2, operand_batching_dims=.:  ...    @     0x7fb4ce901fe4  (unknown)     @     0x7fb67c825f8d  xla::InitializeArgsAndCompile()     @     0x7fb67c8266f6  xla::PjRtCApiClient::Compile()     @     0x7fb68255566c  xla::ifrt::PjRtLoadedExecutable::Create()     @     0x7fb682550a51  xla::ifrt::PjRtCompiler::Compile()     @     0x7fb681ce452e  xla::PyClient::CompileIfrtProgram()     @     0x7fb681ce532e  xla::PyClient::Compile() ...  error: 'mhlo.while' op can't be translated to XLA HLO .... ``` And an insanely log traceback which I'm not going to attach to not dilute the point.","Okay, after a while it seems that the issue is indeed with Equinox; I will duplicate issue to Patrick"
yi,TPU with sharding: grpc initialization failure," Description I'm running a decently large project, and I've just attempted to run it on TPUs (v432 node), and I'm getting an interesting error, which seems to be pretty internal: ``` jaxlib.xla_extension.XlaRuntimeError: INTERNAL: during context [preoptimization]: RET_CHECK failure (third_party/tensorflow/compiler/xla/service/hlo_verifier.cc:2519) instructions.size() == 2 channel 30 is used for multiple host send/recv instructions ``` When enabling internal logs, I get (on one of the machines): ``` RuntimeError: Unable to initialize backend 'cpu': ALREADY_EXISTS: Config key cpu:local_topology/cpu/3 already exists. Additional GRPC error information from remote target coordination_service while calling /tensorflow.CoordinationService/InsertKeyValue: :{""created"":"".560649460"",""description"":""Error received from peer ipv4:some ip:8476"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Config key cpu:local_topology/cpu/3 already exists."",""grpc_status"":6} (set JAX_PLATFORMS='' to automatically choose an available backend) ``` The main issue with this bug is that I cannot provide a script to reproduce it: the codebase is very large, and my attempts at reproducing with a small example just don't lead to the error. And, I cannot even give you a link to the repo, because it's confidential. Here is how I do the partitioning (I think these days you should use jax.device_put, but I'm still using the old API cuz I'm used to it): ```py import equinox as eqx dynamic, static = eqx.partition(image_batch, eqx.is_inexact_array) dynamic = jax.lax.with_sharding_constraint(dynamic, sharding) image_sharded = eqx.combine(dynamic, static) ``` Equinox ```partition```/```combine``` just split PyTree into two/combine two PyTrees into one, such that one of the PyTrees contains all the leaves that satisfy a particular condition. This is convenient when trying to pass a partially static PyTree into a vmap, for example. I am 0.8 confident that the issue is not with Equinox. I would appreciate any help :)  System info (python version, jaxlib version, accelerator, etc.) ```bash  python3.11 m pip freeze jax==0.4.35 jaxsmi==1.0.4 jaxlib==0.4.35 jaxtyping==0.2.34  python3.11 version 3.11.10  neofetch OS: Ubuntu 20.04.4 LTS x86_64 Host: Google Compute Engine Kernel: 5.13.01023gcp CPU: AMD EPYC 7B12 (240) @ 2.249GHz ```",2024-11-10T05:59:04Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/24821,"Update: downgrading libtpu does not help, however, if I set ```export JAX_PLATFORMS=''``` I get a different internal error: ``` jaxlib.xla_extension.XlaRuntimeError: UNKNOWN: /home/arst/.local/lib/python3.11/sitepackages/equinox/_jit.py:55:14: error: All components of the offset index in a gather op must either be a offset dimension or explicitly collapsed or explicitly batched; got len(slice_sizes)=4, output_slice_sizes=2, collapsed_slice_dims=1,2, operand_batching_dims=.:  ...    @     0x7fb4ce901fe4  (unknown)     @     0x7fb67c825f8d  xla::InitializeArgsAndCompile()     @     0x7fb67c8266f6  xla::PjRtCApiClient::Compile()     @     0x7fb68255566c  xla::ifrt::PjRtLoadedExecutable::Create()     @     0x7fb682550a51  xla::ifrt::PjRtCompiler::Compile()     @     0x7fb681ce452e  xla::PyClient::CompileIfrtProgram()     @     0x7fb681ce532e  xla::PyClient::Compile() ...  error: 'mhlo.while' op can't be translated to XLA HLO .... ``` And an insanely log traceback which I'm not going to attach to not dilute the point.","Okay, after a while it seems that the issue is indeed with Equinox; I will duplicate issue to Patrick"
rag,Pattern match dot algorithm spec to preset name,"Since some dot algorithm presets have special cased input and output storage type behavior (e.g. all the `BF16` algorithms), and since JAX's handling of these cases is (for better or worse) handled at the ""preset"" level, this PR provides a small quality of life improvement to convert known `lax.DotAlgorithm` specs to explicit `lax.DotAlgorithmPreset` members whenever possible. For example, if a user specifies: ```python precision = lax.DotAlgorithm(dtypes.bfloat16, dtypes.bfloat16, np.float32,                              num_primitive_operations=6) ``` this will be canonicalized to `lax.DotAlgorithmPreset.BF16_BF16_F32_X6` and the input and output casting will be handled properly.",2024-11-09T15:51:56Z,pull ready,open,0,0,https://github.com/jax-ml/jax/issues/24820
rag,Adds coverage for spmd-axisname-filtering in shard_map transpose.,Adds coverage for spmdaxisnamefiltering in shard_map transpose.,2024-11-08T23:03:36Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24811
yi,"Division by self not always ""1.0"" on JAX GPU, but consistently gives ""1.0"" on JAX CPU."," Description Division by self is not always ""1.0"" on JAX GPU, but seems to consistently give ""1.0"" on JAX CPU. Observed for jax arrays, but not for floats. Question: Is this expected in JAX? If so, is the expectation that a JAX user should build robustness against GPU/CPU floating point differences (as well as the specific divide by self \neq 1 case)?  Code for observing the issue ``` import jax import jax.numpy as jnp  JAX device check print(""************* Checking JAX device *************"") print(""Running on jax device:{}"".format(jax.devices())) print(""Running on jax device platform:{}"".format(jax.devices()[0].platform)) print(""***********************************************"") def self_div(x):     return x / x z_list = [0.0526315718889236, 0.987654321, 1.0, 1e13, 1e14, 1e15] for z in z_list:      do self_div for z and print w 16f     print(f""z/z = {self_div(z):.16f} for z = {z} of type {type(z)}"")      now convert to jnp array     z = jnp.array(z)     print(f""z/z = {self_div(z):.16f} for z = {z} of type {type(z)}"") ```  CPU environment ``` jax==0.4.35 jaxlib==0.4.35 ml_dtypes==0.5.0 numpy==2.1.3 opt_einsum==3.4.0 scipy==1.14.1 ```  Running on CPU environment yields: ``` ************* Checking JAX device ************* An NVIDIA GPU may be present on this machine, but a CUDAenabled jaxlib is not installed. Falling back to cpu. Running on jax device:[CpuDevice(id=0)] Running on jax device platform:cpu *********************************************** z/z = 1.0000000000000000 for z = 0.0526315718889236 of type  z/z = 1.0000000000000000 for z = 0.052631571888923645 of type  z/z = 1.0000000000000000 for z = 0.987654321 of type  z/z = 1.0000000000000000 for z = 0.9876543283462524 of type  z/z = 1.0000000000000000 for z = 1.0 of type  z/z = 1.0000000000000000 for z = 1.0 of type  z/z = 1.0000000000000000 for z = 1e13 of type  z/z = 1.0000000000000000 for z = 9.9999998245167e14 of type  z/z = 1.0000000000000000 for z = 1e14 of type  z/z = 1.0000000000000000 for z = 9.9999998245167e15 of type  z/z = 1.0000000000000000 for z = 1e15 of type  z/z = 1.0000000000000000 for z = 1.0000000036274937e15 of type  ```  GPU environment ``` jax==0.4.35 jaxcuda12pjrt==0.4.35 jaxcuda12plugin==0.4.35 jaxlib==0.4.34 ml_dtypes==0.5.0 numpy==2.1.3 nvidiacublascu12==12.6.3.3 nvidiacudacupticu12==12.6.80 nvidiacudanvcccu12==12.6.77 nvidiacudaruntimecu12==12.6.77 nvidiacudnncu12==9.5.1.17 nvidiacufftcu12==11.3.0.4 nvidiacusolvercu12==11.7.1.2 nvidiacusparsecu12==12.5.4.2 nvidiancclcu12==2.23.4 nvidianvjitlinkcu12==12.6.77 opt_einsum==3.4.0 scipy==1.14.1 ```  Running on GPU environment yields: ``` ************* Checking JAX device ************* Running on jax device:[CudaDevice(id=0), CudaDevice(id=1), CudaDevice(id=2), CudaDevice(id=3)] Running on jax device platform:gpu *********************************************** z/z = 1.0000000000000000 for z = 0.0526315718889236 of type  z/z = 1.0000001192092896 for z = 0.052631571888923645 of type  z/z = 1.0000000000000000 for z = 0.987654321 of type  z/z = 0.9999999403953552 for z = 0.9876543283462524 of type  z/z = 1.0000000000000000 for z = 1.0 of type  z/z = 1.0000000000000000 for z = 1.0 of type  z/z = 1.0000000000000000 for z = 1e13 of type  z/z = 1.0000001192092896 for z = 9.9999998245167e14 of type  z/z = 1.0000000000000000 for z = 1e14 of type  z/z = 1.0000000000000000 for z = 9.9999998245167e15 of type  z/z = 1.0000000000000000 for z = 1e15 of type  z/z = 1.0000000000000000 for z = 1.0000000036274937e15 of type  ```  System info (python version, jaxlib version, accelerator, etc.)  System Info on GPU environment ``` Python 3.10.15 (main, Oct  3 2024, 07:27:34) [GCC 11.2.0] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import jax; jax.print_environment_info() jax:    0.4.35 jaxlib: 0.4.34 numpy:  2.1.3 python: 3.10.15 (main, Oct  3 2024, 07:27:34) [GCC 11.2.0] device info: NVIDIA RTX A50004, 4 local devices"" process_count: 1 platform: uname_result(system='Linux', node='1sum701c10vector01', release='5.15.0124generic', version=' CC(Serializing stax models)Ubuntu SMP Fri Sep 27 20:20:17 UTC 2024', machine='x86_64') $ nvidiasmi Fri Nov  8 15:57:58 2024 ++  ++ ```",2024-11-08T21:35:38Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/24807,"Hi  thanks for the report! Though this may be a bit surprising, I think it's consistent with the expected precision of floating point operations: namely, the outputs in each case are within 1 ULP of the true result at float32 precision. I think  may be able to say more."
rag,Allow more output storage types for some dot algorithms.,"Allow more output storage types for some dot algorithms. As reported in https://github.com/jaxml/jax/issues/24794, there were some dot products that were resulting in an unnecessary conversion. This change makes the output storage type selection more flexible. Fixes https://github.com/jaxml/jax/issues/24794",2024-11-08T20:23:36Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24800
gpt,[AutoPGLE] Explicitly ignore host callback pointers,[AutoPGLE] Explicitly ignore host callback pointers Before this change users had to specify remove_custom_partitioning_ptr_from_cache_key config flag when using AutoPGLE.,2024-11-08T18:24:36Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24797
yi,`jax.lax.DotAlgorithm` generates an unnecessary conversion on H100," Description H100 hardware and cuBLAS support fp16xfp16>fp16 GEMM with accumulation in fp32. Specifying that GEMM config doesn't seem to work: ```python import jax import jax.numpy as jnp import jax.random as jr in_dtype = jnp.float16 acc_dtype = jnp.float32 out_dtype = jnp.float16 m = k = n = 1024 kx, ky = jr.split(jr.key(1234)) x = jr.uniform(kx, (m, k), dtype=in_dtype) y = jr.uniform(ky, (k, n), dtype=in_dtype) .jit def f(x, y):   return jax.lax.dot_general(       x,       y,       dimension_numbers=(((1,), (0,)), ((), ())),       precision=jax.lax.DotAlgorithm(           lhs_precision_type=in_dtype,           rhs_precision_type=in_dtype,           accumulation_type=acc_dtype,       ),       preferred_element_type=out_dtype,   ) f(x, y) ``` This produces the following StableHLO, note the unexpected `stablehlo.convert`: ``` module  attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {   func.func public (%arg0: tensor loc(""x""), %arg1: tensor loc(""y"")) > (tensor {jax.result_info = """"}) {     %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT], algorithm =  : (tensor, tensor) > tensor loc(loc8)     %1 = stablehlo.convert %0 : (tensor) > tensor loc(loc8)     return %1 : tensor loc(loc)   } loc(loc) } loc(loc) ``` and causes an extra conversion kernel to run: ``` ENTRY main.5 {   Arg_1.2.0 = f16[1024,1024]{1,0} parameter(1), metadata={op_name=""y""}   Arg_0.1.0 = f16[1024,1024]{1,0} parameter(0), metadata={op_name=""x""}   customcall.1.0 = (f32[1024,1024]{1,0}, s8[4194304]{0}) customcall(Arg_0.1.0, Arg_1.2.0), custom_call_target=""__cublas$gemm"", metadata={op_name=""jit(f)/jit(main)/dot_general"" source_file=""/root/test.py"" source_line=16}, backend_config={""operation_queue_id"":""0"",""wait_on_operation_queues"":[],""gemm_backend_config"":{""alpha_real"":1,""alpha_imag"":0,""beta"":0,""dot_dimension_numbers"":{""lhs_contracting_dimensions"":[""1""],""rhs_contracting_dimensions"":[""0""],""lhs_batch_dimensions"":[],""rhs_batch_dimensions"":[]},""precision_config"":{""operand_precision"":[""DEFAULT"",""DEFAULT""],""algorithm"":""ALG_DOT_F16_F16_F32""},""epilogue"":""DEFAULT"",""damax_output"":false,""lhs_stride"":""1048576"",""rhs_stride"":""1048576"",""grad_x"":false,""grad_y"":false},""force_earliest_schedule"":false}   gettupleelement.1 = f32[1024,1024]{1,0} gettupleelement(customcall.1.0), index=0, metadata={op_name=""jit(f)/jit(main)/dot_general"" source_file=""/root/test.py"" source_line=16}   ROOT wrapped_convert = f16[1024,1024]{1,0} fusion(gettupleelement.1), kind=kLoop, calls=wrapped_convert_computation, metadata={op_name=""jit(f)/jit(main)/dot_general"" source_file=""/root/test.py"" source_line=16} } ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.36.dev20241106+37af1002c jaxlib: 0.4.36.dev20241108 numpy:  1.26.4 python: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0] device info: NVIDIA H100 80GB HBM31, 1 local devices"" process_count: 1 platform: uname_result(system='Linux', node=..., release='5.15.01029nvidia', version=' CC(Add support for `np.trace` )Ubuntu SMP Mon Jul 17 15:02:31 UTC 2023', machine='x86_64') $ nvidiasmi Fri Nov  8 17:36:11 2024        ++  ++ ```",2024-11-08T17:36:45Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/24794,"Thanks for the report. Fixed in https://github.com/jaxml/jax/pull/24800, although this requires the use of a ""preset"" rather than the general struct: ```python .jit def f(x, y):   return jax.lax.dot_general(       x,       y,       dimension_numbers=(((1,), (0,)), ((), ())),       precision=""F16_F16_F32"",       preferred_element_type=np.float16,   ) ``` The `jax.lax.DotAlgorithm` version will always cast for now. It would be possible to add the pattern matching to that, but it's annoying enough to do that it's not currently a high priority. I'd recommend using the presets whenever possible!","Thanks, how does one parse `F16_F16_F32`, is the format documented?",Yep! The supported presets are all listed here: https://jax.readthedocs.io/en/latest/jax.lax.htmljax.lax.DotAlgorithmPreset
rag,[mgpu] Broadcast the LHS fragmented array if it is splat.,[mgpu] Broadcast the LHS fragmented array if it is splat.,2024-11-08T13:08:31Z,,open,0,0,https://github.com/jax-ml/jax/issues/24780
rag,[Pallas] Increase test coverage of pl.dot.,[Pallas] Increase test coverage of pl.dot.,2024-11-08T05:18:20Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24777
rag,Added ragged attention for MQA/MHA/GQA,"This PR adds ragged attention for different input shapes. Ragged attention is a decoding attention kernel that restricts how many keys are attended to. An array `lengths` of size (batch_size,) is passed in as input and specifies the number of valid keys for each corresponding example in the batch.",2024-11-07T19:17:39Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24763
yi,jnp.reshape: raise TypeError when specifying newshape,"The `reshape` parameter was deprecated in JAX v0.4.28 (released May 9 2024). We're now seven months and seven releases beyond this, which well exceeds the minimum deprecation timeline set out in the API compatibility policy.",2024-11-06T20:57:47Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/24748
yi,TracerBoolConversionError," Description I am trying to create Mandelbrot Set using Jax and after transforming the function using vmap i got `TracerBoolConversionError` error message code: ```python import jax from jax import numpy as jnp from jax import jit,vmap def mandle_brot(c,max_iter):     z= jnp.array([0.0+0.0j])     for i in range(max_iter):         z=z*z+c         if(jnp.abs(z)>2.0):             return jnp.array([i])     return jnp.array([max_iter]) z = jnp.complex_(0.75 + 0.1j) mandle_brot(z,100) Z= jnp.array([z,z,z,z]) mandle_brot_vmap = vmap(mandle_brot,in_axes=(0,None)) mandle_brot_vmap(Z,10)  error from this part ``` { 	""name"": ""TracerBoolConversionError"", 	""message"": ""Attempted boolean conversion of traced array with shape bool[1]. This BatchTracer with object id 140294572667376 was created on line:   /tmp/ipykernel_16031/267948302.py:5:11 (mandle_brot) See https://jax.readthedocs.io/en/latest/errors.htmljax.errors.TracerBoolConversionError"", 	""stack"": "" TracerBoolConversionError                 Traceback (most recent call last) Cell In[123], line 1 > 1 mandle_brot_vmap(Z,10)     [... skipping hidden 3 frame] Cell In[91], line 5, in mandle_brot(c, max_iter)       3 for i in range(max_iter):       4     z=z*z+c > 5     if(jnp.abs(z)>2.0):       6         return jnp.array([i])       7 return jnp.array([max_iter])     [... skipping hidden 1 frame] File ~/anaconda3/envs/ML/lib/python3.11/sitepackages/jax/_src/core.py:1554, in concretization_function_error..error(self, arg)    1553 def error(self, arg): > 1554   raise TracerBoolConversionError(arg) TracerBoolConversionError: Attempted boolean conversion of traced array with shape bool[1]. This BatchTracer with object id 140294572667376 was created on line:   /tmp/ipykernel_16031/267948302.py:5:11 (mandle_brot) See https://jax.readthedocs.io/en/latest/errors.htmljax.errors.TracerBoolConversionError"" } Any help  System info (python version, jaxlib version, accelerator, etc.) I am using Jax in my WSL. jax:    0.4.35 jaxlib: 0.4.34 numpy:  1.26.4 python: 3.11.8  ++",2024-11-06T16:41:54Z,question,closed,0,4,https://github.com/jax-ml/jax/issues/24740,"As the error indicates, the python control flow of `jnp.abs(z)>2.0` is problematic. You can use `jax.lax.cond` instead. See https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.htmlpythoncontrolflowjit and https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.cond.html. ","The problem is with this line: ```python         if(jnp.abs(z)>2.0):             return jnp.array([i]) ``` `jnp.abs(z)` is a traced value, and so its value cannot be known while the Python `if` statement is executing. For background on this, I'd start with JAX Key Concepts, and then refer to JAX Sharp Bits: Control Flow. The solution here will likely be to use `jax.lax.while_loop` rather than a Python `while` loop. Feel free to ask back here if you have any questions!","> You can use `jax.lax.cond` instead. This is true in many cases, but in this particular code `cond` will not work, because it cannot be used to break out of a Python forloop.","> > You can use `jax.lax.cond` instead. >  > This is true in many cases, but in this particular code `cond` will not work, because it cannot be used to break out of a Python forloop. True, I was just thinking of something to the effect of  ```python def mandle_brot(c, max_iter):     iters = jnp.arange(1, max_iter + 1)     def body(carry, i):         z, count = carry         z = z * z + c         escaped = jnp.abs(z) > 2.0         count = lax.cond(escaped, lambda _: jnp.minimum(count, i), lambda _: count, None)         return (z, count), count     init = (0.0 + 0.0j, max_iter)     _, counts = lax.scan(body, init, iters)     return jnp.min(counts) ``` A while loop does seem more natural tho (although if they are being vmapped over, does the while loop save anything since the ones that finish early in the vmap just have redundant computation after that point that is post selected out?). "
rag,Fix debug_nans false positive in jnp.quantile,"Fixes CC(median FloatingPointError: invalid value (nan) encountered in jit(convert_element_type)) I'm deliberately leaving out test coverage, because we don't cover this combination of configurations (`debug_nans` + `disable_jit`). But we're happy to fix these kinds of issues when they come up.",2024-11-05T23:37:43Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/24733
llm, InconclusiveDimensionOperation: Symbolic dimension comparison 'b' < '2147483647' is inconclusive.," Description A simple code to reproduce: ```py import jax import jax.numpy as jnp import jax.experimental.jax2tf as jax2tf import tensorflow as tf def f(a):     return jnp.sort(a, axis=1) my_model = tf.Module() my_model.f = tf.function(     jax2tf.convert(         lambda x: jax.vmap(jax.jacrev(jax.jit(f)))(x),         with_gradient=True,         polymorphic_shapes=[""b, 3""],     ),     autograph=False,     input_signature=[         tf.TensorSpec([None, 3], tf.float32),     ], ) tf.saved_model.save(     my_model,     ""test_model"",     options=tf.saved_model.SaveOptions(experimental_custom_gradients=True), ) ``` Output: ``` 20241105 17:04:52.450508: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered WARNING: All log messages before absl::InitializeLog() is called are written to STDERR E0000 00:00:1730844292.464466  861440 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered E0000 00:00:1730844292.468594  861440 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered I0000 00:00:1730844294.252089  861440 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5038 MB memory:  > device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5 I0000 00:00:1730844294.252455  861440 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 6795 MB memory:  > device: 1, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:02:00.0, compute capability: 7.5 Traceback (most recent call last):   File ""/home/jz748/anaconda3/lib/python3.10/sitepackages/tensorflow/python/saved_model/save.py"", line 769, in _trace_gradient_functions     def_function.function(custom_gradient).get_concrete_function(   File ""/home/jz748/anaconda3/lib/python3.10/sitepackages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py"", line 1251, in get_concrete_function     concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)   File ""/home/jz748/anaconda3/lib/python3.10/sitepackages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py"", line 1221, in _get_concrete_function_garbage_collected     self._initialize(args, kwargs, add_initializers_to=initializers)   File ""/home/jz748/anaconda3/lib/python3.10/sitepackages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py"", line 696, in _initialize     self._concrete_variable_creation_fn = tracing_compilation.trace_function(   File ""/home/jz748/anaconda3/lib/python3.10/sitepackages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py"", line 178, in trace_function     concrete_function = _maybe_define_function(   File ""/home/jz748/anaconda3/lib/python3.10/sitepackages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py"", line 283, in _maybe_define_function     concrete_function = _create_concrete_function(   File ""/home/jz748/anaconda3/lib/python3.10/sitepackages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py"", line 310, in _create_concrete_function     traced_func_graph = func_graph_module.func_graph_from_py_func(   File ""/home/jz748/anaconda3/lib/python3.10/sitepackages/tensorflow/python/framework/func_graph.py"", line 1059, in func_graph_from_py_func     func_outputs = python_func(*func_args, **func_kwargs)   File ""/home/jz748/anaconda3/lib/python3.10/sitepackages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py"", line 599, in wrapped_fn     out = weak_wrapped_fn().__wrapped__(*args, **kwds)   File ""/home/jz748/anaconda3/lib/python3.10/sitepackages/tensorflow/python/eager/polymorphic_function/autograph_util.py"", line 52, in autograph_handler     raise e.ag_error_metadata.to_exception(e) tensorflow.python.autograph.impl.api.StagingError: in user code:     File ""/home/jz748/anaconda3/lib/python3.10/sitepackages/jax/experimental/jax2tf/jax2tf.py"", line 804, in grad_fn_tf         in_cts_flat = convert(     File ""/home/jz748/anaconda3/lib/python3.10/sitepackages/jax/experimental/jax2tf/jax2tf.py"", line 437, in converted_fun_tf         impl.before_conversion()     File ""/home/jz748/anaconda3/lib/python3.10/sitepackages/jax/experimental/jax2tf/jax2tf.py"", line 536, in before_conversion         self.exported = _export.export_back_compat(     File ""/home/jz748/anaconda3/lib/python3.10/sitepackages/jax/_src/export/_export.py"", line 635, in do_export         traced = wrapped_fun_jax.trace(*args_specs, **kwargs_specs)     File ""/home/jz748/anaconda3/lib/python3.10/sitepackages/jax/_src/export/_export.py"", line 1296, in fun_vjp_jax         _, pullback_jax = jax.vjp(primal_fun if flat_primal_fun else flattened_primal_fun_jax,     File ""/home/jz748/anaconda3/lib/python3.10/sitepackages/jax/_src/export/_export.py"", line 1290, in flattened_primal_fun_jax         res = primal_fun(*args, **kwargs)     File ""/home/jz748/codes/deepmdkit/test_xla/test.py"", line 12, in          my_model.f = tf.function(jax2tf.convert(lambda x: jax.vmap(jax.jacrev(jax.jit(f)))(x), with_gradient=True, polymorphic_shapes=[""b, 3""]),     File ""/home/jz748/anaconda3/lib/python3.10/sitepackages/jax/_src/export/shape_poly.py"", line 857, in __lt__         return not _geq_decision(self, other, lambda: f""'{self}'          my_model.f = tf.function(jax2tf.convert(lambda x: jax.vmap(jax.jacrev(jax.jit(f)))(x), with_gradient=True, polymorphic_shapes=[""b, 3""]),     File ""/home/jz748/anaconda3/lib/python3.10/sitepackages/jax/_src/export/shape_poly.py"", line 857, in __lt__         return not _geq_decision(self, other, lambda: f""'{self}'      tf.saved_model.save(my_model, ""test_model"",   File ""/home/jz748/anaconda3/lib/python3.10/sitepackages/tensorflow/python/saved_model/save.py"", line 1432, in save     save_and_return_nodes(obj, export_dir, signatures, options)   File ""/home/jz748/anaconda3/lib/python3.10/sitepackages/tensorflow/python/saved_model/save.py"", line 1467, in save_and_return_nodes     _build_meta_graph(obj, signatures, options, meta_graph_def))   File ""/home/jz748/anaconda3/lib/python3.10/sitepackages/tensorflow/python/saved_model/save.py"", line 1682, in _build_meta_graph     return _build_meta_graph_impl(obj, signatures, options, meta_graph_def)   File ""/home/jz748/anaconda3/lib/python3.10/sitepackages/tensorflow/python/saved_model/save.py"", line 1606, in _build_meta_graph_impl     asset_info, exported_graph = _fill_meta_graph_def(   File ""/home/jz748/anaconda3/lib/python3.10/sitepackages/tensorflow/python/saved_model/save.py"", line 974, in _fill_meta_graph_def     _trace_gradient_functions(exported_graph, saveable_view)   File ""/home/jz748/anaconda3/lib/python3.10/sitepackages/tensorflow/python/saved_model/save.py"", line 773, in _trace_gradient_functions     raise ValueError( ValueError: Error when tracing gradients for SavedModel. Check the error log to see the error that was raised when converting a gradient function to a concrete function. You may need to update the custom gradient, or disable saving gradients with the option tf.saved_model.SaveOptions(experimental_custom_gradients=False).         Problematic op name: IdentityN         Gradient inputs: (, ) ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.35 jaxlib: 0.4.35 numpy:  1.26.4 python: 3.10.13  ++ ```",2024-11-05T22:07:05Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/24730,Assigning  who is most familiar with shape polymorphism and TF model exporting.,"In the immediate term, you can unblock by adding an explicit constraint 'b < '2147483647', as explained in the documentation link from the error message. The issue is that JAX lowering for `jnp.sort` uses an `iota` of indices and the dtype of the indices (`int32` or `int64`) depends on the size of the array. This means that this lowering is not shape polymorphic, because dtypes of values depend on the dimension values. I will be thinking how to handle this more nicely. E.g., we could always use `int64` for indices.","> we could always use `int64` for indices. This is probably a reasonable solution. The reason for the shapedependent dtype was because we were exploring the possibility of getting rid of the X64 flag and making APIs default to 32bit unless 64bit is explicitly requested or required – that approach turned out not to be viable, but some vestiges of it (like this one) are still around."
rag,[WIP] A minimal public API for custom primitives,"Power users of JAX want to be able to control the behavior of some functions under all transformations. This is currently partially supported by the public APIs like `custom_jvp`, `custom_vjp`, and `custom_vmap`, and fully supported by the private APIs around `core.Primitive`. Two key limitations of the public APIs are: 1. There is no mechanism for customizing both forward and reverse mode AD, and 2. `custom_vmap` (besides being undocumented) doesn't support reverse mode AD unless combined with `custom_vjp`. The first issue is particularly important for kernel library authors who need to provide performant AD across a range of use cases, or for researchers using training algorithms that rely on bidirectional or higherorder AD. The second is more of a usability issue than a fundamental limitation. One place where these issues become particularly limiting is when writing opaque kernels using callbacks, `pallas_call`, or `ffi_call`. In those cases, good performance requires customizing all transformations, so many library authors reach for the private `core.Primitive` APIs. As a result, `core.Primitive` is widely used in the wild. This isn't a great state of affairs because the existing primitive APIs are brittle and not terribly user friendly, but this also has larger knockon effects. For example, users of `core.Primitive` typically need to write ""lowering"" rules which require the use of `mlir` which is yet another nonpublic API. But, we don't currently have an alternative approach that we can point users to! There are many ways that we could tackle this, and many documents have been written (by me and others) about possible approaches. But, my work on this has started to converge, and after the merge of ""stackless"", the implementation got somewhat more straightforward, so now seemed like a good time to push this forward. In this PR, I lay out my current ideas, and comments would be very welcome. The goal here is to start with a minimal API surface that sufficiently covers the existing use cases. Then, future iterations would add some useful extensions as discussed below.  The proposed API From the user's perspective, this API would look similar to the other `custom_*` APIs: ```python import jax.extend as jex .custom_primitive def fun(x, y):   ... .def_vmap def fun_vmap(axis_size, is_batched, *args):   ...   return out, out_is_batched .def_jvp def fun_jvp(primals_in, tangents_in):   ...   return primals_out, tangents_out (fun.def_transpose, linear_argnums=(0, ...)) def fun_transpose(cts_out, *nonlinear_args):   ...   return cts_in ``` But, unlike the existing APIs, none of these rules would be automatically populated unless explicitly defined. Therefore, unlike `custom_jvp`, for example, calling `vmap` on a custom primitive without `def_vmap` would raise an exception. In this PR, these are the only hooks that I have implemented because they cover the vast majority of uses of `core.Primitive`, but the current implementation the return type of `custom_primitive(fun)` is an instance of `CustomPrimitive` which is a subclass of `core.Primitive`, so it is still possible to use a custom primitive in the existing registration tables, e.g.: ```python pe.dce_rules[fun] = ... ```  Alternative approaches So far, the userfacing APIs for customizing JAX functions under transformations have focused on composable pertransformation APIs like `custom_jvp`, `custom_vjp`, `custom_vmap`, and `custom_transpose`. This is a nice approach because under this model users only ever need to implement the customizations that can't be automatically generated by JAX. Each of these customization APIs should be transparent under any transformations that it doesn't touch. This approach was hugely successful for `custom_jvp` and `custom_vjp`, and these functions are now widely used. However, `custom_vmap` and `custom_transpose` both remain incomplete and rarely used. So one reasonable approach might be to try to increase the coverage of these composable transforms by finishing `custom_vmap` and `custom_transpose` and adding support for customizing both directions of AD. I've worked on this a little bit (e.g. see this branch for the AD case), and there are some fundamental conceptual issues that we encountered when deriving the partial evaluation and transpose rules for `custom_vmap`. These difficulties might be avoided by some ongoing work to refactor JAX's tracing into ""stages"" where (for example) AD is always run before batching, but I think it is worth landing a functional API without requiring those changes. Furthermore, the motivating use cases discussed above (callbacks, `pallas_call`, and `ffi_call`) all typically require customizing all these transformations anyways, so the general composable approach is probably overkill for these cases.  Future work My goal with this PR is to keep the implementation simple and minimally support the most common uses of `core.Primitive`, but having a primitive API like this allows other UX improvements. **A better VJP and linearization interface** In a previous iteration of this project, I worked the infrastructure needed to add VJP and linearization hooks by deferring evaluation of the JVP rule. A version of this feature will be straightforward to integrate into this API: ```python .def_fwd def fun_fwd(*args):   ...   return out, res .def_lin def fun_lin(res, *tangents_in):   ...   return tangents_out .def_bwd def fun_bwd(res, *cts_out):   ...   return cts_in  OR ... fun.def_vjp(fun_fwd, fun_bwd) ``` while continuing to also support JVPs. I think that this is probably a more intuitive API for supporting reverse mode AD (vs. using transpose manually), but my plan is to add that in a follow up PR.",2024-11-05T19:22:27Z,,open,0,5,https://github.com/jax-ml/jax/issues/24726,This is great! Have you thought about also adding `def_lowering` so users can use a different impl depending on device type?,"> Have you thought about also adding `def_lowering` so users can use a different impl depending on device type? Good question! I think that the approach I would advocate for instead would be to combine this with `lax.platform_dependent`. `lax.platform_dependent` covers all of the use cases for platformspecific lowering that I can come up with, and here that would just live within the primal function: ```python def fun_cpu(*args):   ... def fun_cuda(*args):   ... .custom_primitive def fun(*args):   return lax.platform_dependent(*args, cpu=fun_cpu, cuda=fun_cuda) ... ``` What do you think of that? Are there use cases that you know of that would need something more specific?","May I voice the need for exposing some way to define custom partitioning logic on the primitive as well?  Recently, we have found several cases where the automatic partitioning of jax 'breaks down' and fails to automatically partition some function that is vmapped, vjped and jvped (An example is `jnp.take_along_axis(a, indices, axis)`, where the first axis of `a` and `indices` are partitioned). `custom_partitioning` is not enough here, because we also must support vmap and vjp. We have investiated custom primitives and that would work, though it's complex to use the notsodocumented custom partitioning logic with it... .","> May I voice the need for exposing some way to define custom partitioning logic on the primitive as well? Good question! I don't think it would be feasible to incorporate partitioning logic into this implementation, at least not in the short term. I'd argue that partitioning is (currently) a somewhat orthogonal question since it is executed by XLA long after JAX is out of the loop. That's why `custom_partitioning` is such a complicated beast! That being said, it's a great point that we should make sure that `custom_primitive` can be composed properly with `custom_partitioning`  more on that below! > `custom_partitioning` is not enough here, because we also must support vmap and vjp. I think the usual approach here would be to compose `custom_parititioning` with `custom_vjp` and `custom_vmap` (which should work!): ```python      < this probably needs to be on the inside! def fun(*args):   ... ``` Then, inside of your `vmap` rule, as long as you call `fun` again it should partition appropriately. This issue thread probably isn't the right place to go into too much detail about this, but feel free to open a new issue with more details about the problems you have encountered, because an approach like that should work fine!","> `lax.platform_dependent` covers all of the use cases for platformspecific lowering that I can come up with, and here that would just live within the primal function Thanks  that makes sense. Really excited for this change – thanks for working on it!"
yi,Allow specifying `exec_time_optimization_effort` and `memory_fitting_effort` via CLI.,"New compiler options (see title) have recently been exposed in JAX. To use them users have to change their Python code like this: ``` jax.jit(f, compiler_options={   ""exec_time_optimization_effort"": 0.5,  change here  }) ```  It would be very convenient to not modify the code but rely on CLI instead. For example: ``` JAX_EXEC_TIME_OPTIMIZATION_EFFORT=0.5 python3 jax_program.py ```",2024-11-05T11:01:05Z,enhancement,closed,0,1,https://github.com/jax-ml/jax/issues/24715,Fixed by CC(Add exec_time_optimization_effort and memory_fitting_effort flags  fixes 24715) !
chat,RFC: specify jit static args via Static annotation,"An idea inspired by chats with . Example: ```python import jax from jax.typing import Static .jit def f(x: jax.Array, square: Static[bool]):   return x ** 2 if square else x print(f(2, True))   4 print(f(2, False))   2 ``` The current way to define this would be ```python from functools import partial (jax.jit, static_argnames=['square']) def f(x: jax.Array, square: bool):   return x ** 2 if square else x ```",2024-11-04T23:13:02Z,enhancement,open,3,3,https://github.com/jax-ml/jax/issues/24705,"How should this behave in the absence of an immediate `jit`? In particular, adapting the example, what do we expect this code to do? ```python .jit def f(x, square):   return g(x, square) def g(x: jax.Array, square: Static[bool]):   return x ** 2 if square else x print(f(2, True)) print(f(2, False)) ```","> How should this behave in the absence of an immediate `jit`? In particular, adapting the example, what do we expect this code to do? My vision would be for the `Static` annotation to be ignored unless used in the definition of a function that is wrapped by `jit`.",This looks great to me! It's definitely a stepup in readability from the way we currently annotate static arguments.
rag,[pallas:mosaic_gpu] `lax.fori_loop` lowering now promotes the carry to `mgpu.FragmentedArray`s,[pallas:mosaic_gpu] `lax.fori_loop` lowering now promotes the carry to `mgpu.FragmentedArray`s,2024-11-04T12:06:07Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24686
yi,[Pallas] Unable to Modify Input Ref in Pallas Kernel," Description Hello, I'm wondering whether it is feasible for my pallas kernel to update on input refs? For example, if i want to both read and write to the same tensor via my pallas kernel, how can I achieve that? I'm putting down a minimal script and it seems that it doesn't work as I expect. Thanks!  System info (python version, jaxlib version, accelerator, etc.) ``` import jax import jax.numpy as jnp from jax.experimental import pallas as pl def add_kernel(x_ref, y_ref, o_ref):    In this code, `x_ref`, `y_ref` and `o_ref` are (8,)shaped `Ref`s   x = x_ref[:]   y = y_ref[:]   o_ref[:] = x + y def add_inplace_kernel(x_ref, y_ref, o_ref):    In this code, `x_ref`, `y_ref` and `o_ref` are (8,)shaped `Ref`s   x = x_ref[:]   y = y_ref[:]   o = x + y   y_ref[:] = o x, y = jnp.arange(8), jnp.arange(8, 16) print(""=======regular add========="") add = pl.pallas_call(add_kernel, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32)) print(""x "", x) print(""y "", y) o = add(x, y) print(""o "", o) print(""=======regular add========="") print(""=======inplace add========="") inplace_add = pl.pallas_call(add_inplace_kernel, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32)) print(""x "", x) print(""y "", y) o_dummy = inplace_add(x, y) print(""after inplace add y "", y) print(""o_dummy "", o_dummy) print(""=======inplace add========="") ``` From this script I see ``` =======regular add========= x  [0 1 2 3 4 5 6 7] y  [ 8  9 10 11 12 13 14 15] o  [ 8 10 12 14 16 18 20 22] =======regular add========= =======inplace add========= x  [0 1 2 3 4 5 6 7] y  [ 8  9 10 11 12 13 14 15] after inplace add y  [ 8  9 10 11 12 13 14 15] o_dummy  [0 0 0 0 0 0 0 0] =======inplace add========= ``` but I'm trying to get the `inplace_add` to return me `y  [ 8 10 12 14 16 18 20 22]`. Is that possible?",2024-11-01T05:18:26Z,bug pallas,closed,0,3,https://github.com/jax-ml/jax/issues/24656,Also I found this issue quite relevant https://github.com/jaxml/jax/discussions/22276,"This doesn't work as intended because x, y (the JAX tensors) live in HBM, and Pallas will copy them to the innermost memory hierarchy (e.g. VMEM on TPUs) before invoking the kernel. Therefore, when you modify `y_ref` you're only modifying the copy and it's not updating the actual `y` that's resident in HBM. Try aliasing the input and output to the same ref using the `input_output_aliases` argument to pallas call. In your case for the inplace add, you need to use: ``` inplace_add = pl.pallas_call(add_inplace_kernel,   out_shape=jax.ShapeDtypeStruct((8,), jnp.int32),   input_output_aliases={1:0}) ``` Pallas will copy outputs back to HBM, so this will trigger Pallas to copy your updates to `y_ref` back to HBM.",Thanks a lot! I think it is exactly what I want. Let me try to work on my real kernel to see if I can get what I need.
rag,[Pallas TPU] Add lowering for `lax.nextafter`,"[Pallas TPU] Add lowering for `lax.nextafter` Also improved the corresponding test cases to ensure better coverage and accuracy. This PR is similar to https://github.com/jaxml/jax/pull/22283, which adds lowering for `lax.sign`.",2024-10-31T23:47:28Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24653
yi,Problems with `experimental.pallas.ops.tpu.flash_attention`," Description Hi, I am trying to use the experimental `flash_attention` forward pass on TPUs. I got a simple `v28` TPU instance from GCP and am running a test program to benchmark performance. I've included the test program below in case there are some issues in how I am calling `flash_attention` _(very possible)_ If I run with `jnp.float32`, I get pretty poor performance (_vastly_ worse than using the builtin `scaled_dot_product_attention`): ``` batch_size = 32 num_heads = 8 d_model = 64  Seq Length  {tflops:>10.2f}"") if __name__ == ""__main__"":     seq_lengths = [512, 1024, 2048, 4096]     benchmark_flash_attention(seq_lengths) ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.35 jaxlib: 0.4.35 numpy:  2.1.2 python: 3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0] device info: TPU v28, 8 local devices"" process_count: 1 platform: uname_result(system='Linux', node='t1vn5b6c04c0w0', release='5.19.01022gcp', version=' CC(attempt to centerjustify the jax logo in readme)~22.04.1Ubuntu SMP Sun Apr 23 09:51:08 UTC 2023', machine='x86_64') ```",2024-10-30T19:49:26Z,bug,closed,7,6,https://github.com/jax-ml/jax/issues/24633,Do any other data types give you a similar error 🤔 ,"Hi! Yes, this kernel is unfortunately written with newer TPUs in mind. However, I'm relatively sure that if only you make sure to apply `.astype(jnp.float32)` to all matmul operands, then the failure should go away","Thanks for the reply :) It's hard to get access to newer TPUs on GCP, but I'll keep trying. Just as a general rule of thumb, roughly what utilization (in terms of achieved FLOPs/peak FLOPs) should I be expecting on specific platforms with `bf16`? I'm just curious so that I can tell if I am using things correctly.",I just managed to get access to a `v5e` singlechip instance and my results are as follows (using `bf16`): ```  Seq Length        7.61 ``` This seems pretty low compared to the stated peak throughput of 197 bfloat16 TFLOP/s? Is this expected or am I doing something incorrect here?,"I think by default the tile sizes are all set to 128, which is way too small for v5e. Try increasing them to different powers of 2, until they're so big the you get a VMEM OOM error. In general I would expect the utilization with well tuned tile sizes to be quite high (70%+)","Thanks! I get >75% util at `block_q = 512`, `block_k_major = 2048`, and `block_k = 1024` :D "
yi,[Mosaic GPU] Fix the ordering of transforms in async_copy,"[Mosaic GPU] Fix the ordering of transforms in async_copy Previously we didn't really fully discharge squeezing the indexed dims before applying other GMEM transforms, leading to potential failures because they were not anticipating the increased rank.",2024-10-30T18:30:31Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24629
rag,JAX does not recognise my NVIDIA GPU when installed via conda," Description I previously had a working installation of JAX (installed via conda) that recognised my NVIDIA GPU without issue. However, I recently migrated to a new machine and now I cannot get JAX to recognise my GPU when I install via conda. I'm using Miniforge to manage my conda environments, as I did on my old machine, and I installed JAX according to the docs: `conda install jaxlib=*=*cuda* jax cudanvcc c condaforge c nvidia` When I then try to import JAX and check my available devices using: ``` import jax print(jax.devices()) ``` I get the following output: ``` An NVIDIA GPU may be present on this machine, but a CUDAenabled jaxlib is not installed. Falling back to cpu. [CpuDevice(id=0)] ``` tensorflow, however, does recognise my GPU, and so I tried the suggestion from CC(JAX doesn't work with cuda/gpu) to install using pip. I created a new environment and ran: `pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` When I then ran my JAX code above I got the output: ``` [CudaDevice(id=0)] ``` Voilà, my GPU has been found! It therefore appears that the conda section of the docs might need updating.  System info (python version, jaxlib version, accelerator, etc.) Conda installation: ``` jax:    0.4.34 jaxlib: 0.4.34 numpy:  2.1.2 python: 3.13.0  ++  Name                    Version                   Build  Channel _libgcc_mutex             0.1                 conda_forge    condaforge _openmp_mutex             4.5                       2_gnu    condaforge asttokens                 2.4.1              pyhd8ed1ab_0    condaforge brotli                    1.1.0                hb9d3cd8_2    condaforge brotlibin                1.1.0                hb9d3cd8_2    condaforge bzip2                     1.0.8                h4bc722e_7    condaforge cacertificates           2024.8.30            hbcca054_0    condaforge certifi                   2024.8.30          pyhd8ed1ab_0    condaforge comm                      0.2.2              pyhd8ed1ab_0    condaforge contourpy                 1.3.0           py313h33d0bda_2    condaforge cycler                    0.12.1             pyhd8ed1ab_0    condaforge debugpy                   1.8.7           py313h46c70d0_0    condaforge decorator                 5.1.1              pyhd8ed1ab_0    condaforge exceptiongroup            1.2.2              pyhd8ed1ab_0    condaforge executing                 2.1.0              pyhd8ed1ab_0    condaforge fonttools                 4.54.1          py313h8060acc_1    condaforge freetype                  2.12.1               h267a509_2    condaforge importlibmetadata        8.5.0              pyha770c72_0    condaforge ipykernel                 6.29.5             pyh3099207_0    condaforge ipympl                    0.9.4              pyhd8ed1ab_0    condaforge ipython                   8.29.0             pyh707e725_0    condaforge ipython_genutils          0.2.0              pyhd8ed1ab_1    condaforge ipywidgets                8.1.5              pyhd8ed1ab_0    condaforge jax                       0.4.35                   pypi_0    pypi jaxcuda12pjrt           0.4.35                   pypi_0    pypi jaxcuda12plugin         0.4.35                   pypi_0    pypi jaxlib                    0.4.34                   pypi_0    pypi jedi                      0.19.1             pyhd8ed1ab_0    condaforge jupyter_client            8.6.3              pyhd8ed1ab_0    condaforge jupyter_core              5.7.2              pyh31011fe_1    condaforge jupyterlab_widgets        3.0.13             pyhd8ed1ab_0    condaforge keyutils                  1.6.1                h166bdaf_0    condaforge kiwisolver                1.4.7           py313h33d0bda_0    condaforge krb5                      1.21.3               h659f571_0    condaforge lcms2                     2.16                 hb7c19ff_0    condaforge ld_impl_linux64          2.43                 h712a8e2_2    condaforge lerc                      4.0.0                h27087fc_0    condaforge libblas                   3.9.0           25_linux64_openblas    condaforge libbrotlicommon           1.1.0                hb9d3cd8_2    condaforge libbrotlidec              1.1.0                hb9d3cd8_2    condaforge libbrotlienc              1.1.0                hb9d3cd8_2    condaforge libcblas                  3.9.0           25_linux64_openblas    condaforge libdeflate                1.22                 hb9d3cd8_0    condaforge libedit                   3.1.20191231         he28a2e2_2    condaforge libexpat                  2.6.3                h5888daf_0    condaforge libffi                    3.4.2                h7f98852_5    condaforge libgcc                    14.2.0               h77fa898_1    condaforge libgccng                 14.2.0               h69a702a_1    condaforge libgfortran               14.2.0               h69a702a_1    condaforge libgfortranng            14.2.0               h69a702a_1    condaforge libgfortran5              14.2.0               hd5240d6_1    condaforge libgomp                   14.2.0               h77fa898_1    condaforge libjpegturbo             3.0.0                hd590300_1    condaforge liblapack                 3.9.0           25_linux64_openblas    condaforge libmpdec                  4.0.0                h4bc722e_0    condaforge libopenblas               0.3.28          pthreads_h94d23a6_0    condaforge libpng                    1.6.44               hadc24fc_0    condaforge libsodium                 1.0.20               h4ab18f5_0    condaforge libsqlite                 3.47.0               hadc24fc_1    condaforge libstdcxx                 14.2.0               hc0a3c3a_1    condaforge libstdcxxng              14.2.0               h4852527_1    condaforge libtiff                   4.7.0                he137b08_1    condaforge libuuid                   2.38.1               h0b41bf4_0    condaforge libwebpbase              1.4.0                hd590300_0    condaforge libxcb                    1.17.0               h8a09558_0    condaforge libzlib                   1.3.1                hb9d3cd8_2    condaforge matplotlibbase           3.9.2           py313h129903b_1    condaforge matplotlibinline         0.1.7              pyhd8ed1ab_0    condaforge mldtypes                 0.5.0                    pypi_0    pypi munkres                   1.1.4              pyh9f0ad1d_0    condaforge ncurses                   6.5                  he02047a_1    condaforge nestasyncio              1.6.0              pyhd8ed1ab_0    condaforge numpy                     2.1.2           py313h4bf6692_0    condaforge nvidiacublascu12        12.6.3.3                 pypi_0    pypi nvidiacudacupticu12    12.6.80                  pypi_0    pypi nvidiacudanvcccu12     12.6.77                  pypi_0    pypi nvidiacudaruntimecu12  12.6.77                  pypi_0    pypi nvidiacudnncu12         9.5.1.17                 pypi_0    pypi nvidiacufftcu12         11.3.0.4                 pypi_0    pypi nvidiacusolvercu12      11.7.1.2                 pypi_0    pypi nvidiacusparsecu12      12.5.4.2                 pypi_0    pypi nvidiancclcu12          2.23.4                   pypi_0    pypi nvidianvjitlinkcu12     12.6.77                  pypi_0    pypi openjpeg                  2.5.2                h488ebb8_0    condaforge openssl                   3.3.2                hb9d3cd8_0    condaforge opteinsum                3.4.0                    pypi_0    pypi packaging                 24.1               pyhd8ed1ab_0    condaforge parso                     0.8.4              pyhd8ed1ab_0    condaforge pexpect                   4.9.0              pyhd8ed1ab_0    condaforge pickleshare               0.7.5                   py_1003    condaforge pillow                    11.0.0          py313h2d7ed13_0    condaforge pip                       24.3.1             pyh145f28c_0    condaforge platformdirs              4.3.6              pyhd8ed1ab_0    condaforge prompttoolkit            3.0.48             pyha770c72_0    condaforge psutil                    6.1.0           py313h536fd9c_0    condaforge pthreadstubs             0.4               hb9d3cd8_1002    condaforge ptyprocess                0.7.0              pyhd3deb0d_0    condaforge pure_eval                 0.2.3              pyhd8ed1ab_0    condaforge pygments                  2.18.0             pyhd8ed1ab_0    condaforge pyparsing                 3.2.0              pyhd8ed1ab_1    condaforge python                    3.13.0          h9ebbce0_100_cp313    condaforge pythondateutil           2.9.0              pyhd8ed1ab_0    condaforge python_abi                3.13                    5_cp313    condaforge pyzmq                     26.2.0          py313h8e95178_3    condaforge qhull                     2020.2               h434a139_5    condaforge readline                  8.2                  h8228510_1    condaforge scipy                     1.14.1                   pypi_0    pypi six                       1.16.0             pyh6c4a22f_0    condaforge stack_data                0.6.2              pyhd8ed1ab_0    condaforge tk                        8.6.13          noxft_h4845f30_101    condaforge tornado                   6.4.1           py313h536fd9c_1    condaforge traitlets                 5.14.3             pyhd8ed1ab_0    condaforge typing_extensions         4.12.2             pyha770c72_0    condaforge tzdata                    2024b                hc8b5060_0    condaforge wcwidth                   0.2.13             pyhd8ed1ab_0    condaforge widgetsnbextension        4.0.13             pyhd8ed1ab_0    condaforge xorglibxau               1.0.11               hb9d3cd8_1    condaforge xorglibxdmcp             1.1.5                hb9d3cd8_0    condaforge xz                        5.2.6                h166bdaf_0    condaforge zeromq                    4.3.5                h3b0a872_6    condaforge zipp                      3.20.2             pyhd8ed1ab_0    condaforge zstd                      1.5.6                ha6fb4c9_0    condaforge ```",2024-10-29T22:56:20Z,bug,open,0,3,https://github.com/jax-ml/jax/issues/24604,"We (JAX) only support the `pip` packages, which should work under `conda` as well. For the native `condaforge` packages, you'd need to follow up on the condaforge feedstock project for `jax`.","I briefly looked into this a while ago as well when working on CC(Make `_cuda_path` more reliable) and CC(Fix `_cuda_path` for case when `cuda_nvcc` is a namespace package). I don't have time or interest to fully resolve the issue as I'm not a conda user myself, but here are a couple of pointers from my notes that might help you debug this. First, in `_src/xla_bridge.py`, the GPU backend doesn't get initialized (I believe) because of outdated `cu{BLAS,SPARSE}` modules as noted by the error messages which by default gets suppressed: ``` (Pdb) print(err_msg) Unable to initialize backend 'cuda': Unable to use CUDA because of the following issues with CUDA components: Outdated cuBLAS installation found. Version JAX was built against: 120001 Minimum supported: 120100 Installed version: 120002 The local installation version must be no lower than 120100.  Outdated cuSPARSE installation found. Version JAX was built against: 12000 Minimum supported: 12100 Installed version: 12001 The local installation version must be no lower than 12100. ``` Even if you fix these, you _might_ have to explicitly pass in `CUDA_PATH` environment variable so that the `lib.cuda_path` comes out correctly. There are two branches in that function, one checking `CUDA_PATH` and another one checking `nvidia.cuda_nvcc` module path. Despite installing `cudanvcc` via `conda`, the `nvidia.cuda_nvcc` module is not available and thus the second branch doesn't work.","This should have been fixed by https://github.com/condaforge/jaxlibfeedstock/pull/288 (jaxlib condaforge version `0.4.34=*_201`), so I guess the issue can be closed (eventual followup can be tracked anyhow in https://github.com/condaforge/jaxfeedstock/issues/162 )."
yi,Fix a reference cycle bug.,When we use a context manager within a linear_util.transformation we should leave the scope of the context manager before the final yield. Otherwise we create spurious reference cycles. This was causing CoreTest.test_reference_cycles to fail on Python 3.10 (but not 3.13 for some reason).,2024-10-29T20:46:32Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/24600
yi,represent `random.key_impl` of builtin RNGs by canonical string name,"No great reason to return specs here, and sticking to strings instead can help with simple serialization.",2024-10-29T20:08:06Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/24593
rag,"[mosaic_gpu] Added support for bitwise and, or and xor to `FragmentedArray`","[mosaic_gpu] Added support for bitwise and, or and xor to `FragmentedArray`",2024-10-29T19:53:22Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24591
yi,Fix missing f-string format in slogdet error message,"Adds the missing f of fstring format in the ValueError message of the slogdet function, now displaying the actual misspecified array shape instead of ""{a_shape}"". ",2024-10-29T15:27:14Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/24581,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
rag,[MOSAIC:GPU] Extend the mosaic mlir dialect with fragmented layouts.,[MOSAIC:GPU] Extend the mosaic mlir dialect with fragmented layouts.,2024-10-29T08:43:09Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24574
rag,Bump actions/checkout from 4.2.1 to 4.2.2,"Bumps actions/checkout from 4.2.1 to 4.2.2.  Release notes Sourced from actions/checkout's releases.  v4.2.2 What's Changed  urlhelper.ts now leverages wellknown environment variables by @​jww3 in actions/checkout CC(lazy sublanguage part 1) Expand unit test coverage for isGhes by @​jww3 in actions/checkout CC(Fix bug with caching in presence of JVP and JIT)  Full Changelog: https://github.com/actions/checkout/compare/v4.2.1...v4.2.2    Changelog Sourced from actions/checkout's changelog.  Changelog v4.2.2  urlhelper.ts now leverages wellknown environment variables by @​jww3 in actions/checkout CC(lazy sublanguage part 1) Expand unit test coverage for isGhes by @​jww3 in actions/checkout CC(Fix bug with caching in presence of JVP and JIT)  v4.2.1  Check out other refs/* by commit if provided, fall back to ref by @​orhantoy in actions/checkout CC(Adding implementations for numpy alen, isscalar)  v4.2.0  Add Ref and Commit outputs by @​lucacome in actions/checkout CC(Make DeviceValue and subclasses weakref friendly) Dependency updates by @​dependabot actions/checkout CC(Add error checking that arguments of jvp are tuples), actions/checkout CC(Implement bool_ support for jnp.add, jnp.multiply, jnp.einsum, lax.do…)  v4.1.7  Bump the minornpmdependencies group across 1 directory with 4 updates by @​dependabot in actions/checkout CC(Build Mac wheels for Python 3.8 with scipy 1.3.2.) Bump actions/checkout from 3 to 4 by @​dependabot in actions/checkout CC(improve grad error message without enough args) Check out other refs/* by commit by @​orhantoy in actions/checkout CC(Simplify einsum implementation.) Pin actions/checkout's own workflows to a known, good, stable version. by @​jww3 in actions/checkout CC(time to jit a function grows superlinear with memory accessed by function)  v4.1.6  Check platform to set archive extension appropriately by @​corymiller in actions/checkout CC(add explicit checkpointing (rematerialization) control)  v4.1.5  Update NPM dependencies by @​corymiller in actions/checkout CC(ppermute is returning 0s) Bump github/codeqlaction from 2 to 3 by @​dependabot in actions/checkout CC(fix shard_args logic, closes 1688) Bump actions/setupnode from 1 to 4 by @​dependabot in actions/checkout CC(Possible issue with partial gradient application) Bump actions/uploadartifact from 2 to 4 by @​dependabot in actions/checkout CC(Move internal typerelated functions into a new (internal) jax.types …) README: Suggest user.email to be 41898282+githubactions[bot].noreply.github.com by @​corymiller in actions/checkout CC(Bump jaxlib version in README)  v4.1.4  Disable extensions.worktreeConfig when disabling sparsecheckout by @​jww3 in actions/checkout CC(Remove test_util.check_raises_regexp.) Add dependabot config by @​corymiller in actions/checkout CC(Undefined variable x) Bump the minoractionsdependencies group with 2 updates by @​dependabot in actions/checkout CC(Add a type check that verifies the lower and upper arguments to lax.f…) Bump wordwrap from 1.2.3 to 1.2.5 by @​dependabot in actions/checkout CC(jax.scipy.linalg >2X slower than scipy.linalg.solve (6core CPU))  v4.1.3  Check git version before attempting to disable sparsecheckout by @​jww3 in actions/checkout CC(Implement np.linalg.pinv) Add SSH user parameter by @​corymiller in actions/checkout CC(Strengthened type checking for while and cond) Update actions/checkout version in updatemainversion.yml by @​jww3 in actions/checkout CC(Fix np.clip broadcasting across a_min and a_max)  v4.1.2  Fix: Disable sparse checkout whenever sparsecheckout option is not present @​dscho in actions/checkout CC(Add way to run jax program on CPU)  v4.1.1  Correct link to GitHub Docs by @​peterbe in actions/checkout CC(Delete experimental/lapax.) Link to release page from what's new section by @​corymiller in actions/checkout CC(document pmap with pytrees, fixes 1486)  v4.1.0  Add support for partial checkout filters    ... (truncated)   Commits  11bd719 Prepare 4.2.2 Release ( CC(Support trees in lax parallel operations.)) e3d2460 Expand unit test coverage ( CC(Fix bug with caching in presence of JVP and JIT)) 163217d urlhelper.ts now leverages wellknown environment variables. ( CC(lazy sublanguage part 1)) See full diff in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-10-28T17:39:06Z,pull ready dependencies github_actions,closed,0,1,https://github.com/jax-ml/jax/issues/24563, rebase
yi,Bump actions/setup-python from 5.2.0 to 5.3.0,"Bumps actions/setuppython from 5.2.0 to 5.3.0.  Release notes Sourced from actions/setuppython's releases.  v5.3.0 What's Changed  Add workflow file for publishing releases to immutable action package by @​Jcambass in actions/setuppython CC(Enable LU decomposition tests on GPU.) Upgrade IA publish by @​Jcambass in actions/setuppython CC(One array type to rule them all!)  Bug Fixes:  Normalise Line Endings to Ensure CrossPlatform Consistency by @​priyakinthali in actions/setuppython CC(Add a pure Python LU decomposition.) Revise isGhes logic by @​jww3 in actions/setuppython CC(Implement device_put as a primitive.) Bump pillow from 7.2 to 10.2.0 by @​aparnajyothiy in actions/setuppython CC(TypeError when taking inverse)  Enhancements:  Enhance workflows and documentation updates by @​priyakinthali in actions/setuppython CC(Make DeviceArray.__iter__ and __reversed__ forward to _value.) Bump default versions to latest by @​jeffwidman in actions/setuppython CC(provide better hints in error message ""abstract value passed to `bool`"")  New Contributors  @​Jcambass made their first contribution in actions/setuppython CC(Enable LU decomposition tests on GPU.) @​jww3 made their first contribution in actions/setuppython CC(Implement device_put as a primitive.)  Full Changelog: https://github.com/actions/setuppython/compare/v5...v5.3.0    Commits  0b93645 Enhance workflows: Add macOS 13 support, upgrade publishaction, and update d... 9c76e71 Bump pillow from 7.2 to 10.2.0 in /tests/data  ( CC(TypeError when taking inverse)) f4c5a11 Revise isGhes logic ( CC(Implement device_put as a primitive.)) 19dfb7b Bump default versions to latest ( CC(provide better hints in error message ""abstract value passed to `bool`"")) e9675cc Merge pull request  CC(One array type to rule them all!) from actions/Jcambasspatch1 3226af6 Upgrade IA publish 70dcb22 Merge pull request  CC(Enable LU decomposition tests on GPU.) from actions/Jcambasspatch1 65b48c7 Create publishimmutableactions.yml 29a37be initial commit ( CC(Add a pure Python LU decomposition.)) See full diff in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-10-28T17:38:59Z,pull ready dependencies github_actions,closed,0,1,https://github.com/jax-ml/jax/issues/24562,  rebase
yi,MaxText Getting Started fails," Description Followed the MaxText Getting Started instructions at https://github.com/AIHypercomputer/maxtext/blob/main/getting_started/First_run.md ``` $ git clone https://github.com/AIHypercomputer/maxtext.git $ cd maxtext $ bash setup.sh $ python3 MaxText/train.py MaxText/configs/base.yml   run_name=run0   base_output_directory=gs://rostam193618maxtext   dataset_type=synthetic   steps=10 ... TFRT TPU v3 Built on Oct 21 2024 00:24:02 (1729495442) cl/687888698 WARNING: 'dataset_path' might be pointing your local file system I1028 05:00:34.882054 140412761188352 monitoring.py:144] Starting goodput query and uploader thread in the background for job: run0 and logger: goodput_run0 Started Goodput upload to Tensorboard in the background! I1028 05:00:35.438911 140412761188352 mesh_utils.py:79] Reordering mesh to physical ring order on singletray TPU v2/v3. Num_devices: 8, shape (1, 1, 8, 1, 1, 1, 1, 1) Setting up checkpoint logger... Creating checkpoint manager... I1028 05:00:35.545254 140412761188352 checkpoint_manager.py:557] [process=0][thread=MainThread] CheckpointManager init: checkpointers=None, item_names=('items',), item_handlers={'items': }, handler_registry=None I1028 05:00:35.545620 140412761188352 composite_checkpoint_handler.py:224] Deferred registration for item: ""items"". Adding handler `` for item ""items"" and save args `` and restore args `` to `_handler_registry`. I1028 05:00:35.545769 140412761188352 composite_checkpoint_handler.py:489] Initialized registry DefaultCheckpointHandlerRegistry({('items', ): , ('items', ): }). I1028 05:00:35.546358 140412761188352 abstract_checkpointer.py:35] orbaxcheckpoint version: 0.6.4 I1028 05:00:35.546488 140412761188352 async_checkpointer.py:65] [process=0][thread=MainThread] Using barrier_sync_fn: . at 0x7fb2b998ad40> timeout: 300 secs and primary_host=0 for async checkpoint writes I1028 05:00:35.595536 140412761188352 utils.py:240] [process=0][thread=MainThread] Skipping global process sync, barrier name: CheckpointManager:create_directory I1028 05:00:35.683285 140412761188352 checkpoint_manager.py:1460] Found 0 checkpoint steps in gs://rostam193618maxtext/run0/checkpoints I1028 05:00:35.683537 140412761188352 checkpoint_manager.py:726] [process=0][thread=MainThread] CheckpointManager created,  primary_host=0, CheckpointManagerOptions=CheckpointManagerOptions(save_interval_steps=10000, max_to_keep=None, keep_time_interval=None, keep_period=None, best_fn=None, best_mode='max', keep_checkpoints_without_metrics=True, step_prefix=None, step_format_fixed_length=None, step_name_format=None, create=True, cleanup_tmp_directories=False, save_on_steps=frozenset(), single_host_load_and_broadcast=False, todelete_subdir=None, enable_background_delete=False, read_only=False, enable_async_checkpointing=True, async_options=None, multiprocessing_options=MultiprocessingOptions(primary_host=0, active_processes=None, barrier_sync_key_prefix=None), should_save_fn=None, file_options=FileOptions(path_permission_mode=None), temporary_path_class=None), root_directory=gs://rostam193618maxtext/run0/checkpoints:  Checkpoint manager created! checkpoint manager exists so trying to load this run's existing checkpoint No existing checkpoints found, not restoring checkpoint. number parameters: 1.104 billion Per train step:  Total TFLOPs: 172.94   split as 88.56% learnable weight flops and 11.44% attention flops Traceback (most recent call last):   File ""/home/rostam/.local/lib/python3.10/sitepackages/jax/_src/compiler.py"", line 267, in backend_compile     return backend.compile(built_c, compile_options=options) jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Mosaic failed to compile TPU kernel: Unsupported input data type in matrix multiplication in this target. at location: loc(""/dot_general""(callsite(""_splash_attention""(""/home/rostam/.local/lib/python3.10/sitepackages/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_kernel.py"":2277:0) at callsite(""__call__""(""/home/rostam/.local/lib/python3.10/sitepackages/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_kernel.py"":2312:0) at callsite(""wrap_flash_attention""(""/home/rostam/maxtext/MaxText/layers/attentions.py"":352:0) at callsite(""tpu_flash_attention""(""/home/rostam/maxtext/MaxText/layers/attentions.py"":358:0) at callsite(""_call_wrapped_method""(""/home/rostam/.local/lib/python3.10/sitepackages/flax/linen/module.py"":1216:0) at callsite(""wrapped_module_method""(""/home/rostam/.local/lib/python3.10/sitepackages/flax/linen/module.py"":699:0) at callsite(""apply_attention""(""/home/rostam/maxtext/MaxText/layers/attentions.py"":234:0) at callsite(""_call_wrapped_method""(""/home/rostam/.local/lib/python3.10/sitepackages/flax/linen/module.py"":1216:0) at callsite(""wrapped_module_method""(""/home/rostam/.local/lib/python3.10/sitepackages/flax/linen/module.py"":699:0) at ""__call__""(""/home/rostam/maxtext/MaxText/layers/attentions.py"":977:0)))))))))))) The MLIR operation involved:   %3835 = ""tpu.matmul""(%3830, %3832, %3834)  : (vector, vector, vector) > vector ... additional diagnostics were skipped. Please report a bug at: https://github.com/google/jax/issues/new?assignees=apaszke The above exception was the direct cause of the following exception: jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/home/rostam/maxtext/MaxText/train.py"", line 781, in      app.run(main)   File ""/home/rostam/.local/lib/python3.10/sitepackages/absl/app.py"", line 308, in run     _run_main(main, args)   File ""/home/rostam/.local/lib/python3.10/sitepackages/absl/app.py"", line 254, in _run_main     sys.exit(main(argv))   File ""/home/rostam/maxtext/MaxText/train.py"", line 777, in main     train_loop(config)   File ""/home/rostam/maxtext/MaxText/train.py"", line 670, in train_loop     state, metrics = p_train_step(state, example_batch, nextrng)   File ""/home/rostam/maxtext/MaxText/layers/attentions.py"", line 977, in __call__     prefill_unnormalized_output, prefill_exponentials_max, prefill_exponentials_sum = self.apply_attention(   File ""/home/rostam/maxtext/MaxText/layers/attentions.py"", line 234, in apply_attention     return self.tpu_flash_attention(query, key, value, decoder_segment_ids, self.attn_logits_soft_cap), None, None   File ""/home/rostam/maxtext/MaxText/layers/attentions.py"", line 358, in tpu_flash_attention     x = wrap_flash_attention(query, key, value, decoder_segment_ids)   File ""/home/rostam/maxtext/MaxText/layers/attentions.py"", line 352, in wrap_flash_attention     return jax.vmap(splash_kernel)(query, key, value, segment_ids=decoder_segment_ids)   File ""/home/rostam/.local/lib/python3.10/sitepackages/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_kernel.py"", line 2312, in __call__     return _splash_attention(   File ""/home/rostam/.local/lib/python3.10/sitepackages/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_kernel.py"", line 2277, in _splash_attention     return _splash_attention_custom( jax._src.pallas.mosaic.error_handling.MosaicError: INTERNAL: Mosaic failed to compile TPU kernel: Unsupported input data type in matrix multiplication in this target. The MLIR operation involved:   %3835 = ""tpu.matmul""(%3830, %3832, %3834)  : (vector, vector, vector) > vector ... additional diagnostics were skipped. Please report a bug at: https://github.com/google/jax/issues/new?assignees=apaszke 20241028 05:01:02.850298: I external/xla/xla/pjrt/distributed/client.cc:150] Distributed task shutdown initiated. 20241028 05:01:02.850779: I external/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:1513] Shutdown barrier in coordination service has passed. 20241028 05:01:02.851015: I external/xla/xla/pjrt/distributed/client.cc:152] Distributed task shutdown result: OK 20241028 05:01:02.851270: I external/xla/xla/pjrt/distributed/service.cc:117] Jax service shutting down Exception ignored in:  Traceback (most recent call last):   File ""/home/rostam/.local/lib/python3.10/sitepackages/tensorboardX/record_writer.py"", line 134, in __del__   File ""/home/rostam/.local/lib/python3.10/sitepackages/tensorboardX/record_writer.py"", line 158, in close   File ""/home/rostam/.local/lib/python3.10/sitepackages/tensorboardX/record_writer.py"", line 149, in flush   File ""/usr/lib/python3.10/copy.py"", line 92, in copy ImportError: sys.meta_path is None, Python is likely shutting down ```  System info (python version, jaxlib version, accelerator, etc.) ``` import jax; jax.print_environment_info() jax:    0.4.35 jaxlib: 0.4.35 numpy:  1.26.4 python: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0] device info: TPU v38, 8 local devices"" process_count: 1 platform: uname_result(system='Linux', node='t1vn0e81f9cfw0', release='5.19.01027gcp', version=' CC(Add support for `np.trace` )~22.04.1Ubuntu SMP Thu Jun 22 05:13:17 UTC 2023', machine='x86_64') ```",2024-10-28T05:03:32Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/24549,"I would make this issue on the MaxText repo, since its unclear if this is a fault of jax or of the other repo"
rag,[Pallas][Flash Attention] Cost analysis in Flash Attention kernel causing compilation OOM," Description With `0.4.35` release, the flash attention kernel will hit compilation OOM for long sequence length inputs. It failed during compiling the reference attention implementation for cost analysis purpose, added in this commit: https://github.com/jaxml/jax/commit/4c218fbf3b8431a5f75cdf20942d5d62433a8657. This logic would cause OOM when the device HBM is not enough for the naive attention but fits for flash attention kernels. Here is a small repro script for this issue (on v5e TPU with 16GB HBM) ``` import jax import jax.numpy as jnp from jax.experimental.pallas.ops.tpu import flash_attention kv_seq_len = 16 * 1024 batch_size = 18 n_heads = 32 head_dim = 256 q_seq_len = kv_seq_len kv_shape = (batch_size, n_heads, kv_seq_len, head_dim) q_shape = (batch_size, n_heads, q_seq_len, head_dim) dtype = jnp.bfloat16 q = jnp.ones(q_shape, dtype=dtype) k = jnp.ones(kv_shape, dtype=dtype) v = jnp.ones(kv_shape, dtype=dtype) .jit def run_flash_attention(q, k, v):     return flash_attention.flash_attention(q, k, v) compiled = jax.jit(run_flash_attention).lower(q, k, v).compile() ``` The error message is: ``` The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/home/lsiyuan/jaxflashattnoom/repro.py"", line 24, in      compiled = jax.jit(run_flash_attention).lower(q, k, v).compile()   File ""/home/lsiyuan/jaxflashattnoom/repro.py"", line 22, in run_flash_attention     return flash_attention.flash_attention(q, k, v)   File ""/home/lsiyuan/miniconda3/envs/torch310/lib/python3.10/sitepackages/jax/experimental/pallas/ops/tpu/flash_attention.py"", line 199, in flash_attention     return _flash_attention(   File ""/home/lsiyuan/miniconda3/envs/torch310/lib/python3.10/sitepackages/jax/experimental/pallas/ops/tpu/flash_attention.py"", line 217, in _flash_attention     return _flash_attention_impl(   File ""/home/lsiyuan/miniconda3/envs/torch310/lib/python3.10/sitepackages/jax/experimental/pallas/ops/tpu/flash_attention.py"", line 791, in _flash_attention_impl     cost_estimate=_fwd_cost_estimate(   File ""/home/lsiyuan/miniconda3/envs/torch310/lib/python3.10/sitepackages/jax/experimental/pallas/ops/tpu/flash_attention.py"", line 589, in _fwd_cost_estimate     .compile() jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 22.50G of 15.75G hbm. Exceeded hbm capacity by 6.75G. Total hbm usage >= 22.75G:     reserved        258.00M      program           9.00G      arguments        13.50G  Output size 4.50G; shares 0B with arguments. Program hbm requirement 9.00G:     HLO temp          9.00G (100.0% utilization: Unpadded (9.00G) Padded (9.00G), 0.0% fragmentation (0B))   Largest program allocations in hbm:   1. Size: 9.00G      Shape: f32[18,32,16384,256]{3,2,1,0:T(8,128)}      Unpadded size: 9.00G      XLA label: name = customcall(Arg_0.1, Arg_1.2, Arg_2.3), custom_call_target=""tpu_custom_call"", operand_layout_constraints={bf16[18,32,16384,256]{3,2,1,0}, bf16[18,32,16384,256]{3,2,1,0}, bf16[18,32,16384,256]{3,2,1,0}}      Allocation type: HLO temp      ========================== ``` It failed with `0.4.35` but passed with `20240829` nightly .  System info (python version, jaxlib version, accelerator, etc.) Version with the compilation OOM issue ``` jax:    0.4.35 jaxlib: 0.4.35 numpy:  1.26.4 python: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] device info: TPU v5 lite8, 8 local devices"" process_count: 1 platform: uname_result(system='Linux', node='t1vn81baa7faw0', release='5.19.01022gcp', version=' CC(attempt to centerjustify the jax logo in readme)~22.04.1Ubuntu SMP Sun Apr 23 09:51:08 UTC 2023', machine='x86_64') ``` Version without the compilation OOM issue ``` jax:    0.4.32.dev20240829 jaxlib: 0.4.32.dev20240829 numpy:  1.26.4 python: 3.10.14 device info: TPU v5 lite8, 8 local devices process_count: 1 platform: uname_result(system='Linux', node='t1vn81baa7faw0', release='5.19.01022gcp', version=' CC(attempt to centerjustify the jax logo in readme)~22.04.1Ubuntu SMP Sun Apr 23 09:51:08 UTC 2023', machine='x86_64') ```",2024-10-25T21:19:22Z,bug pallas,closed,0,2,https://github.com/jax-ml/jax/issues/24539,https://github.com/jaxml/jax/pull/24608 should fix this by calculating the cost estimate directly instead of via compiling a reference function. . In the meantime you could also just remove the costestimate temporarily as it won't affect correctness of the code.,"Hi , thank you for the fix, I verified the repro passed with Dec 05 nightly."
rag,[mosaic_gpu] `mgpu.FragmentedArray` now supports `//`,[mosaic_gpu] `mgpu.FragmentedArray` now supports `//` This is needed to compute grid index from the iteration step counter in `emit_pipeline`.,2024-10-25T14:42:30Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24530
yi,"INVALID_ARGUMENT when copying array in pinned_host memory with device_put(..., may_alias=False)"," Description The minimal reproducer below can be added to `memories_test.py` to reproduce this issue. The `block_until_ready` at the bottom of this example is expected to succeed, but fails with `INVALID_ARGUMENT: BlockHostUntilReady() called on deleted or donated buffer`. The donation of `inp_host` into `inp_host_donate` is necessary for the failure to occur. ```   def test_disallow_alias_copies_arrays_with_donated_input(self):     _, _, _, inp_host = _create_inputs(         (8, 2), P(""x"", ""y""), mem_kind=""pinned_host"")     inp_host_donate = jax.device_put(inp_host, may_alias=False, donate=True)     inp_host_donate_copy = jax.device_put(inp_host_donate, may_alias=False)     for a in jax.tree_util.tree_leaves(inp_host_donate):       a.delete()     jax.block_until_ready(inp_host_donate_copy) ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.36 jaxlib: 0.4.36 numpy:  2.0.1 python: 3.11.8 (stable, redacted, redacted) [Clang google3trunk (527cd117b2cb189f59ba64882b2bc5d15e35eefb)] device info: TPU v44, 4 local devices"" process_count: 1 platform: uname_result(system='Linux', [redacted]) ```",2024-10-25T08:05:34Z,bug,closed,0,0,https://github.com/jax-ml/jax/issues/24521
yi,use dtypes.iinfo instead of numpy.iinfo in sum and add corresponding …,…tests,2024-10-24T15:36:23Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24508
yi,"[sharding_in_types][Take 2] Add `out_type` argument to `einsum` and `dot_general` to allow specifying for the output type. Right now, it only accept a `NamedSharding` but in the future we can allow a polymorphic type of: `jax.ShapeDtypeStruct | Sharding | Layout`.","[sharding_in_types][Take 2] Add `out_type` argument to `einsum` and `dot_general` to allow specifying for the output type. Right now, it only accept a `NamedSharding` but in the future we can allow a polymorphic type of: `jax.ShapeDtypeStruct  Layout`. Reverts 0b3f0e11fb0c37342b3c05ad5d53f3435b6ca44c",2024-10-22T15:59:44Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24462
rag,Full coverage for `__jax_array__` protocol," Description I am using Flax with the NNX API that requires arrays to be encapsulated within `nnx.Param`. The issue is that NNX is using the `__jax_array__` protocol to avoid users having to type `.value` but JAX doesn't have full coverage for `__jax_array__`. For example, this code from an NNX model raises the following error: ```python self.reshape_c_k = nnx.Param(reshape_c_k) state_fft_k = jnp.dot(state_fft, self.reshape_c_k) ``` Error: ```python TypeError: Argument 'Param(value=Tracedwith)' of type  is not a valid JAX type. ``` Maybe  can provide more insights on this if necessary?  System info (python version, jaxlib version, accelerator, etc.) python version: 3.12.6 jax: 0.4.34 jaxlib: 0.4.34",2024-10-22T14:59:28Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/24460,"Hi  thanks for the request. This issue has been discussed previously in CC(`jnp.linalg.svd` etc. do not respect `__jax_array__`). But the short version is that we never meant `__jax_array__` to be a fullysupported public API, and it's not clear that it's the best mechanism for downstream libraries to rely on. cc/  who has more context."
yi,[ROCM] Sharding leads to HIP errors," Description Hello, I'm trying to run a script that makes use of sharding.  I'm using a container built on top of  `rocm/jaxcommunity:rocm6.2.3jax0.4.33py3.12.6` , where I've installed some extra packages. I'm running on adastra's MI250X nodes https://dci.dcigitlab.cines.fr/webextranet/architecture/index.html through a singularity container (which is an unprivileged container system, as onHPC we cannot use docker). The error I'm encoutering is the following ```bash Hip error: 'operation would make the legacy stream depend on a capturing blocking stream'(906) at /long_pathname_so_that_rpms_can_package_the_debug_info/src/extlibs/hipBLASLt/library/src/amd_detail/hipblaslt.cpp:135 rocBLAS error: Could not initialize Tensile host: _Map_base::at Aborted (core dumped) ``` The error only appears if I make use of multigpu through sharding.  Running the same script without sharding, but using a single GPU, leads to no errors. I'm running on the container `rocm/jaxcommunity:rocm6.2.3jax0.4.33py3.12.6` where I also install the package `https://github.com/netket/netket` . I'm trying to run the following script as follows ```python singularity debug shell writabletmpfs B/sys  B/dev B/proc B/lus /opt/software/containers/images/infinityhub/rocm6.2.3jax0.4.33py3.12.6date10.22v3.sif  This works, single GPU python ising1d.py  This crashes, multi GPU export NETKET_EXPERIMENTAL_SHARDING=1 python ising1d.py ```  System info (python version, jaxlib version, accelerator, etc.) ```bash >>> import jax; jax.print_environment_info() jax:    0.4.33 jaxlib: 0.4.33 numpy:  2.0.2 python: 3.12.7 (main, Oct 16 2024, 04:37:19) [Clang 18.1.8 ] jax.devices (8 total, 8 local): [RocmDevice(id=0) RocmDevice(id=1) ... RocmDevice(id=6) RocmDevice(id=7)] process_count: 1 platform: uname_result(system='Linux', node='g1130', release='4.18.0477.10.1.el8_8.x86_64', version=' CC(Python 3 compatibility issues) SMP Wed Apr 5 13:35:01 EDT 2023', machine='x86_64') ```",2024-10-22T14:01:35Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/24458
yi,[Pallas:MGPU] Treat each warpgroup as a single logical thread.,"[Pallas:MGPU] Treat each warpgroup as a single logical thread. As an extra minor change, we now disallow specifying the predicate when uniform is unset, as that implies that we're going to use two different mechanisms to select a single thread.",2024-10-22T13:50:52Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24457
yi,"[sharding_in_types] Add `out_type` argument to `einsum` and `dot_general` to allow specifying for the output type. Right now, it only accept a `NamedSharding` but in the future we can allow a polymorphic type of: `jax.ShapeDtypeStruct | Sharding | Layout`.","[sharding_in_types] Add `out_type` argument to `einsum` and `dot_general` to allow specifying for the output type. Right now, it only accept a `NamedSharding` but in the future we can allow a polymorphic type of: `jax.ShapeDtypeStruct  Layout`.",2024-10-22T02:29:07Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24443
yi,"#sdy add shardy CPU config for all JAX tests, disabling any known failing test cases.","sdy add shardy CPU config for all JAX tests, disabling any known failing test cases. Only test cases breaking on CPU are related to:  pure callbacks  export  shard alike Note that `layout_test` is broken on TPU, leaving a comment saying to enable it. Also fixed `shard_map_test` test that was broken when running Shardy on one TPU, and `aot_test` which was breaking due to calling a different C++ StableHLO compilation function.",2024-10-21T22:27:01Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24440
yi,Add support for layouts and other advanced features in ffi_call,"This PR adds the remaining features needed to migrate users of `mlir.custom_call` to `ffi_call`. The most important additions are parameters for specifying layouts, and input/output aliasing. Alongside these changes I have added the `custom_call_api_version` and `legacy_backend_config` arguments so that `ffi_call` can also be used to execute legacy custom calls. I also cleared a longstanding todo item to update the `ffi_call` tests to use a custom call that can run on multiple platforms (including CPU). These new tests also exercise the new parameters. Most of these changes should be reasonably uncontroversial, but I thought it might be worth digging into the layouts question a little bit. **Layouts** The `input_layouts` and `output_layouts` arguments allow users of `ffi_call` to request custom memory layouts for the input and output buffers. These arguments can be used to specify layouts using (a) `None` for the default rowmajor/Cordered inputs, (b) a `DeviceLocalLayout` where the `major_to_minor` property defines the layout, or (c) an majortominor tuple of integers. I think that these options present a reasonable starting point for discussion, but it's worth thinking about this a bit. Some alternative options: 1. We could accept XLAstyle minortomajor layouts, which might be more closely aligned with user's expectations when they have previous experience with `mlir.custom_call` and XLA ordering. However, given the fact that other references to layouts in JAX (namely `Layout` and `DeviceLocalLayout`) use majortominor, I think this is a good opportunity to make this API consistent. 2. We could _require_ `DeviceLocalLayout` objects, rather than also accepting tuples. This has the benefit of being explicit, at the cost of more boilerplate. My preference is to allow the use of tuples to specify inputs rather than requiring users to wrap their arguments in a `DeviceLocalLayout` since the only ambiguity here is about the axis order and IMO `DeviceLocalLayout((0, 1, 2))` isn't much clearer on that than `(0, 1, 2)`. 3. A more extreme alternative would be to remove the `input_layouts` argument and instead read the layouts off of the input arguments. This would then require that a user set the layout on their inputs before calling `ffi_call`, which didn't seem entirely straightforward in my experiments. I think it's better to be able to explicitly request a DLL at the call site.",2024-10-21T18:55:23Z,pull ready,closed,0,3,https://github.com/jax-ml/jax/issues/24433,"> We could require DeviceLocalLayout objects, rather than also accepting tuples Being polymorphic on tuples | DeviceLocalLayout is better IMO. DeviceLocalLayout doesn't only allow `major_to_minor`. It also accepts tiling which on GPU and TPU is extremely useful.","Thanks ! Just so that I understand: the way I have implemented this is that layouts can be specified by a `tuple` or a `DeviceLocalLayout`. Were you agreeing that this is a good approach, or did you have something else in mind?",I was agreeing :) But also pointing out the other stuff DeviceLocalLayout supports :)
gpt,JAX code is extremely slow on GPUs," Description Last week a discussion took place on Twitter where many people noticed that the performance of JAX on GPUs is still subpar compared to PyTorch code.  and I had a discussion afterwards and we agreed that a repo with minimal codebase that showcase the differences in performance between the two would be great.  GPT2 is a good model to start with, and here is a repo that contains code both for JAX, and PyTorch for the same model. The instructions provided on the repo are enough to download and run code locally (on a GPU machine). On my side, these are the results I got on an A100 40G machine:  JAX    PyTorch   Compared to Torch, JAX is extremely slow here, and I have no idea why. There is a chance of a silent bug somewhere in the JAX code, and I may have overlooked it. Given the number of times I have been through this, I think a fresh set of eyes would do better justice. Please let me know if you need any other information on this from my side.  System info (python version, jaxlib version, accelerator, etc.) ``` jax[cuda12] jaxlib==0.4.34 equinox==0.11.8 optax @ git+https://github.com/googledeepmind/optax.git ``` Optax is installed from git because there was a fix for `adamw` but that was not the part of the last release. ",2024-10-20T16:00:25Z,bug performance,open,0,28,https://github.com/jax-ml/jax/issues/24411,"Thanks for filing! I've looked into this a bit, and I just wanted to check with you : are the set ups in both files expected to be the same? Looking at the very first lines in `torch_single_gpu.py` and `jax_single_gpu.py`, I see ``` B = 8   micro batch size T = 1024   sequence length ``` in `jax_single_gpu.py` and ``` B = 16   micro batch size T = 1024   sequence length ``` in `torch_single_gpu.py`. I believe this means that the JAX model will be running twice as many `grad_accum_steps` (given that `B` is in the denominator there). In your screenshots, those seem to be the same, however. I ran the JAX model on a single A10040 (SXM) node, and here are the numbers I got: ```bash $ python3 jax_single_gpu.py input.txt total desired batch size: 524288                                                                                                                                    => calculated gradient accumulation steps: 64                                                                                                                       loaded 338025 tokens                                                                                                                                                1 epoch = 41 batches                                                                                                                                                Loading GPT2 model...                                                                                                                                               Number of parameters in the model       : 124.48 M                                                                                                                                                                                                                                                                                    Training...                                                                                                                                                         step    0  tok/sec: 50513.18 ...                                                                                                     ``` which are different from the initial reported numbers. Is the configuration expected to be the same in the repo as in your screenshots? (I wasn't yet able to run the PyTorch code for comparison due to dependency issues, but hope to get to that soon.)","One suggested improvement! By fully unrolling the `scan` here, the model runs significantly faster: ``` total desired batch size: 524288 => calculated gradient accumulation steps: 64 loaded 338025 tokens 1 epoch = 41 batches Loading GPT2 model... Number of parameters in the model       : 124.48 M Training... step    0  tok/sec: 79188.10 ... ``` (Unrolling `scan`/loops is a good way to allow the compiler to make better optimizations in general, at the expense of compiletime.)","Thanks for looking into it  The benchmark is run for both the setup with a batch size of 8 as I couldn't fit a batch size of 16 with JAX (most probably due to the extra pytree that act as a mask for adamw). I might have forgotten to update the batch info in git.  If you look at the number of batches and the number of grad accumulated steps printed on the screen, both are same meaning the micro batch size is same. Though for the sake of more transparency, I will update the repo, and post the numbers here once again Re unrolling: I am not worried about the compile time for now. Let me check that. Thanks for the pointers","Updated the repo. Re unroll: I did it, and even though the I got a nice bump in the number of tokens processed per second, the time taken to process those tokens got extremely worse. I would have rather paid one time compilation penalty. Also, the performance is still nowhere close to torch neither in terms of tokens (163K token processed in torch vs 22K tokens processed in JAX) nor in terms of time taken ","Thanks for trying and for the feedback! > the time taken to process those tokens got extremely worse Can you explain what that means? Presumably, if we're processing more tokens per second, then the time has to be betteror am I missing something? If you're talking about the first step, then that is because the first step includes compilation overhead. (The first call to the function triggers compilation.) > Also, the performance is still nowhere close to torch neither in terms of tokens (163K token processed in torch vs 22K tokens processed in JAX) nor in terms of time taken Can you give some details about the hardware you are running on? You said you are running on A10040, but it seems that as posted above I am getting ~80K tokens processed in JAX on my side, running on an `NVIDIA A100SXM440GB` (output of `nvidiasmi`). I'm confused about why the numbers that we see are so different, and I'd like to reproduce your numbers. Could you perhaps share the output of `nvidiasmi` for you? I am facing the following error when running `python3 torch_single_gpu.py input.txt`: ``` ImportError: cannot import name 'get_cuda_stream' from 'triton.runtime.jit' (/usr/local/lib/python3.10/distpackages/triton/runtime/jit.py) ``` Do you have any idea where that might be coming from? Is there perhaps a requirement missing in `requirements.txt`I see that it doesn't contain any information about the required Triton version?",">Can you explain what that means? Please take a look at the numbers below:   > Can you give some details about the hardware you are running on? You said you are running on A10040, but it seems that https://github.com/jaxml/jax/issues/24411issuecomment2437717660, running on an NVIDIA A100SXM440GB (output of nvidiasmi). I'm confused about why the numbers that we see are so different, and I'd like to reproduce your numbers. Could you perhaps share the output of nvidiasmi for you? Sure thing. Here is the output:   > Is there perhaps a requirement missing in requirements.txtI see that it doesn't contain any information about the required Triton version? No nothing missing from the `requirements.txt` file. To double check, I just created a new env with that and ran the benchmarks again, and it worked fine. Can you create a new empty env and install everything using the requirements file? The only guess I have in this regard is that you have a `triton` package installed in the current env which is incompatible with the torch package One question for you: How are you unrolling the scan? Maybe we are getting different numbers because of that?","> Sure thing. Here is the output: Thank you for sharing! The chip is the same, so that's not the difference. One difference worth noting (though not sure if it matters) is that I'm using an older CUDA version than you are: ``` ++  ++ ``` > Can you create a new empty env and install everything using the requirements file? The only guess I have in this regard is that you have a `triton` package installed in the current env which is incompatible with the torch package Thanks, seems like retrying with a new env worked after a couple of tries! (There were some issues due to incomplete installs of `torch`, that may also be what had happened before.) I managed to reproduce your numbers for `torch`, making it currently ~2x faster than the JAX implementation on my system (80k vs 160k tokens per second). This difference is still very much unexpected of course, so I'll keep digging :) > One question for you: How are you unrolling the scan? Maybe we are getting different numbers because of that? I just added the `unroll` kwarg to the call to `jax.lax.scan`: ```python3 (x, layer_idx), _ = jax.lax.scan(f, (x, layer_idx), dynamic_layers, unroll=1000)   unrolling to 12 should suffice, I think ``` One thing I also noticed is that you're not actually calling into a `FlashAttention` kernel on the JAX side. The call to ```python attn = jax.nn.dot_product_attention(q, k, v, is_causal=True) ``` could be rewritten into ```python attn = jax.nn.dot_product_attention(q, k, v, is_causal=True, implementation=""cudnn"") ``` for that. There is a `TODO` left by  to select the best implementation automatically in the code, which currently doesn't happen automatically. As a result, the default dispatches to a soup of primitive ops that XLA can't optimize into a single `FlashAttention` kernel.",">Thanks, seems like retrying with a new env worked after a couple of tries! Good to know. Yeah, sometimes it happens if there is a silent package conflict or a corrupted install from the cache >I managed to reproduce your numbers for torch, making it currently ~2x faster than the JAX implementation on my system (80k vs 160k tokens per second). This difference is still very much unexpected of course, so I'll keep digging :) Yes, I was shocked to see the performance difference between the two.  > I just added the unroll kwarg to the call to jax.lax.scan: I did the same but with a much lower number, and you are right 12 per iteration should be enough. Will check that >There is a TODO left by  to select the best implementation automatically in the code, which currently doesn't happen automatically. As a result, the default dispatches to a soup of primitive ops that XLA can't optimize into a single FlashAttention kernel. I read it when I opened a discussion on the attention functionality last to last month. Still missed it. Sigh!","So, after updating my system to (roughly) match your driver version, I was able to run with both `FlashAttention` enabled and loop unrolling. This gets me to  ``` $ python3 jax_single_gpu.py input.txt  total desired batch size: 524288 => calculated gradient accumulation steps: 64 loaded 338025 tokens 1 epoch = 41 batches Loading GPT2 model... Number of parameters in the model       : 124.48 M Training... step    0  ++ ```","We can glean another few tokens/s by rewriting the code to use a different path for gradient accumulation vs for actual model update: ```diff +.filter_jit(donate='allexceptfirst') +def train_step_accum(model, optim, optim_state, data, targets): +  _, grads = compute_loss(model, data, targets) +  _, optim_state = optim.update( +    grads, optim_state, eqx.filter(model, eqx.is_array)) +  return optim_state +  def main(data_file_path):      total_batch_size = 524288   2**19, ~0.5M, in number of tokens      B = 8   micro batch size @@ 433,13 +440,22 @@ def main(data_file_path):          t0 = time.time()          for micro_step in range(grad_accum_steps):              batch_inputs, batch_targets = train_loader.next_batch()             loss, model, optim_state = train_step( +            if micro_step  calculated gradient accumulation steps: 64 loaded 338025 tokens 1 epoch = 41 batches Loading GPT2 model... Number of parameters in the model       : 124.48 M Training... step    0  tok/sec: 141960.73 ... ```","> Using FlashAttention correctly unsurprisingly turns out to be very important here :) Now, we're closer to a reasonable time, with 130k tokens per second on JAX, and 150k on Torch. Still, we should be able to do better, I'll keep looking. Thanks. I am able to reproduce this number on my side. One question though: Right now, we are using `unroll=1000`, but finding the ideal number for unrolling seems a tedious task. Is there any better way to do it? May be through HLO graphs? > This avoids making noop updates to the model & rewriting all the weights to HBM at each accumulation step. This is is nitty detail. We should add documentation for this in optax","> Thanks. I am able to reproduce this number on my side. One question though: Right now, we are using unroll=1000, but finding the ideal number for unrolling seems a tedious task. Is there any better way to do it? May be through HLO graphs? Awesome that you could reproduce it! I hacked around, but the goal is to unroll all the layers heresetting `unroll=True` should work. (Side note: there seems to be an offbyone in the implementation of the `scan` lowering such that setting `unroll=True` lowers slightly less efficiently. I'll make a fix, but a temporary workaround is to set `unroll=self.n_layer + 1`). > This is is nitty detail. We should add documentation for this in optax Agreed! Where do you think that would belong exactly? (This is my first time using `optax` :)) Another (marginal) improvement is to preprocess the data in the data loader to avoid doing a copy at each loop step (call to `jnp.reshape` in `train_loader.next_batch()` produce two copies). This is not strictly a win in terms of time because some of that compute happens before the training loop. Nevertheless, it gives a slightly more faithful comparison with Torch, since in the Torch version we get a `view` of the data (as opposed to a copy). ``` $ python3 jax_single_gpu.py input.txt  total desired batch size: 524288 => calculated gradient accumulation steps: 64 loaded 335872 tokens 1 epoch = 41 batches Loading GPT2 model... Number of parameters in the model       : 124.48 M Training... step    0  tok/sec: 144035.7 ... ```","> but the goal is to unroll all the layers heresetting unroll=True should work. (Side note: there seems to be an offbyone in the implementation of the scan lowering such that setting unroll=True lowers slightly less efficiently. I'll make a fix, but a temporary workaround is to set unroll=self.n_layer + 1). Yeah, when I used `unroll=True`, I noticed that difference. Thanks for the explanation. > Agreed! Where do you think that would belong exactly? (This is my first time using optax :)) This should go right here. Let me know if you need more info or any help on this. > Another (marginal) improvement is to preprocess the data in the data loader to avoid doing a copy at each loop step (call to jnp.reshape in train_loader.next_batch() produce two copies). This is not strictly a win in terms of time because some of that compute happens before the training loop. Nevertheless, it gives a slightly more faithful comparison with Torch, since in the Torch version we get a view of the data (as opposed to a copy). Yeah we can do that. I just left it raw for now","> Yeah, when I used `unroll=True`, I noticed that difference. Thanks for the explanation. This commit should fix the issue at HEAD! > This should go right here. Let me know if you need more info or any help on this. Actually, if you have a good idea of what useful documentation for you would have looked like, I'd love to see it! (Maybe you even want to send a PR?! :))",">Actually, if you have a good idea of what useful documentation for you would have looked like, I'd love to see it! (Maybe you even want to send a PR?! :)) Cool. Leave that to me. I will make that PR both in optax and Equinox","I didn't have a lot of bandwidth to go further this week, but I recalled that there are a few improvements in XLA that are not yet in the last releaseso wanted to share at least that. Here are some numbers with the latest nightlies (`jax0.4.36.dev20241031`, `jaxcuda12pjrt0.4.36.dev20241101`, `jaxcuda12plugin0.4.36.dev20241101`, `jaxlib0.4.36.dev20241101`). ``` $ python3 jax_single_gpu.py input.txt  total desired batch size: 524288 => calculated gradient accumulation steps: 64 loaded 335872 tokens 1 epoch = 41 batches Loading GPT2 model... Number of parameters in the model       : 124.48 M Training... step    0  tok/sec: 149715.49 ... ``` So we're at roughly ~150k tok/sec for now. ",> I didn't have a lot of bandwidth to go further this week No worries  We are already close to the optimal number. I am hoping this exercise would turn out extremely critical for both of us in term of performance benchmarks. , were you able to find any more bottlenecks?,"Hi ; I have a few guesses of where to look at, but haven't looked into the profiles deeply just yet. At this point, it seems like we need to fix XLA performance. With that said, we can still improve the model at the JAX level directly. We know that `LayerNorm` is currently pretty slow in XLA (though there is ongoing work to fix this). One easy way to check whether speeding that up makes a difference is to plug in the Pallas `layer_norm` kernel: ```diff +from jax.experimental.pallas.ops.gpu import layer_norm as pallas_layer_norm   Equinox model utils  +def layer_norm_wrapper(self, x, *args, **kwargs): +  x = jnp.reshape(x, (1, *x.shape)) +  res, *_ = pallas_layer_norm.layer_norm(x, self.weight, self.bias) +  return jnp.reshape(res, x.shape[1:]) + +eqx.nn.LayerNorm.__call__ = layer_norm_wrapper + +  def is_layer(x):      """"""Check if the current pytree is an instance of any Equinox layer.""""""      return isinstance(x, (eqx.nn.Linear, eqx.nn.Embedding, eqx.nn.LayerNorm)) @@ 214,9 +223,9 @@ class TransformerBlock(eqx.Module):          self.mlp = MLP(config, key=key2, dtype=dtype)      def __call__(self, x, mask=None):         x = eqx.filter_vmap(self.norm_1)(x) +        x = self.norm_1(x)          x = x + self.attn(x, mask=mask)         x = eqx.filter_vmap(self.norm_2)(x) +        x = self.norm_2(x)          x = x + self.mlp(x)          return x @@ 298,7 +307,7 @@ class GPT(eqx.Module):                                           unroll=self.n_layer + 1)           4. Final prelayer norm         x = eqx.filter_vmap(self.norm)(x).astype(jnp.bfloat16) +        x = self.norm(x).astype(jnp.bfloat16)           5. Classification head          logits = eqx.filter_vmap(lm_head)(x ``` (Note, the plugging in is pretty hacky, the Pallas op may deserve some love :)) The hypothesis is validated, since plugging that in gets us to roughly 156k tokens/second: ``` $ python3 jax_single_gpu.py input.txt                                                                                                                                                                                                                                                                                                                                                                                                       total desired batch size: 524288                                                                                                                                                                                                                                                                                                                                                                                                                                                                          => calculated gradient accumulation steps: 64                                                                                                                                                                                                                                                                                                                                                                                                                                                             loaded 335872 tokens                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      1 epoch = 41 batches                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Loading GPT2 model...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Number of parameters in the model       : 124.48 M                                                                                                                                                                                                                                                                                                                                                                                                                                                        Training...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               step    0  tok/sec: 154179.69  ... ``` There were previous reports of Adam slowness; I'd like to poke at this and see whether there are any low hanging fruits there, but I have yet to find cycles to do this.","> We know that LayerNorm is currently pretty slow in XLA Can yo point out to me the issue/discussion/documentation where it was found and discussed? I would like to know the reason behind it because this may affect other ops as well in that case > but I have yet to find cycles to do this. No worries. Thanks for all the help and the support you have provided. This is already looking good. In an ideal world, JAX should be extremely faster than torch. And in case of TPUs it is true, but not for GPUs. The problem is that GPUs are more widespread compared to TPUs, and if it is slow on GPU, no matter how hard people (like you and I) try to convince that JAX is better, it will not be enough","> Can yo point out to me the issue/discussion/documentation where it was found and discussed? I would like to know the reason behind it because this may affect other ops as well in that case Unfortunately, I'm not aware of any publicfacing bugonly internal ones. What it boils down to, however, is that normalizations (any of them, including e.g. Softmax) end up lowering to several kernels by default, as opposed to a single one. We (and I personally) have done significant work towards fixing this, but it's not yet on by default, and doesn't actually match everything that it should. > No worries. Thanks for all the help and the support you have provided. This is already looking good. In an ideal world, JAX should be extremely faster than torch. And in case of TPUs it is true, but not for GPUs. The problem is that GPUs are more widespread compared to TPUs, and if it is slow on GPU, no matter how hard people (like you and I) try to convince that JAX is better, it will not be enough Happy to help! We're always working on making our stack better, and reports such as yours are very helpful to figure out what the sharp bits are for our external users, and to find out things we can do better. So thank you for filing this!",">Unfortunately, I'm not aware of any publicfacing bugonly internal ones. What it boils down to, however, is that normalizations (any of them, including e.g. Softmax) end up lowering to several kernels by default, as opposed to a single one. We (and I personally) have done significant work towards fixing this, but it's not yet on by default, and doesn't actually match everything that it should. Oh okay. Any approximate timeline for the fixes that are already done but not in the dev branch? >Happy to help! We're always working on making our stack better, and reports such as yours are very helpful to figure out what the sharp bits are for our external users, and to find out things we can do better. So thank you for filing this! No worries. Always happy to help make things better 🍻 ","> Oh okay. Any approximate timeline for the fixes that are already done but not in the dev branch? The changes that are done are all in the dev branch! And in fact, you can trigger the experimental rewriter using the `xla_gpu_experimental_enable_triton_softmax_priority_fusion=true` (to be set in the `XLA_FLAGS` environment variable); but this doesn't work well for this workload just yet unfortunately, which is why I didn't advertise it so far. (Note, that flag is mostly for development, and may get deleted without warningthough that'll probably mean that we enabled the feature by default!)","Oh I have tried those, and it didn't work that time. Here is the corresponding issue: https://github.com/openxla/xla/issues/17103", did you find anything else that can speed up the runs?,"Hi ; I didn't yet investigate further for this particular model. In general, we have a lot of work in the pipeline that should have a generally good effect on such models, but I haven't checked how they'll apply specifically to this benchmark.",">In general, we have a lot of work in the pipeline that should have a generally good effect on such models, but I haven't checked how they'll apply specifically to this benchmark. No worries. I can do that. Please let me know once these improvements are in the nightly or a stable version. I can restart the benchmarking ","> No worries. I can do that. Please let me know once these improvements are in the nightly or a stable version. I can restart the benchmarking. I will keep you updated. The next big thing will hopefully be a fix for loop unrolling, which should allow us to reclaim compiletime and get the same performance as with unrolling `scan` out of the box without actually unrolling."
rag,jax.distributed.initialize() crash," Description Hi JAX team, In the past two days, I've been using GCP's queuedresources to create spot TPU v4256/v464, and then running the following Python script. ```python import jax jax.distributed.initialize() print(1) ```  However, I found that it gets stuck at the jax.distributed.initialize() command. This is very strange because when I created an ondemand TPU v464 two weeks ago, the jax.distributed.initialize() command executed without any issues, and it still works fine on that machine. But now, with the newly created instances, I'm facing this problem. Therefore, I'd like to seek help from the JAX team ! BUG ```     jax.distributed.initialize()   File ""/root/miniconda3/lib/python3.11/sitepackages/jax/_src/distributed.py"", line 231, in initialize     global_state.initialize(coordinator_address, num_processes, process_id,   File ""/root/miniconda3/lib/python3.11/sitepackages/jax/_src/distributed.py"", line 55, in initialize     clusters.ClusterEnv.auto_detect_unset_distributed_params(   File ""/root/miniconda3/lib/python3.11/sitepackages/jax/_src/clusters/cluster.py"", line 82, in auto_detect_unset_distributed_params     process_id = env.get_process_id()                  ^^^^^^^^^^^^^^^^^^^^   File ""/root/miniconda3/lib/python3.11/sitepackages/jax/_src/clusters/cloud_tpu_cluster.py"", line 144, in get_process_id     slice_id = cls._get_slice_id()                ^^^^^^^^^^^^^^^^^^^   File ""/root/miniconda3/lib/python3.11/sitepackages/jax/_src/clusters/cloud_tpu_cluster.py"", line 159, in _get_slice_id     if has_megascale_address():        ^^^^^^^^^^^^^^^^^^^^^^^   File ""/root/miniconda3/lib/python3.11/sitepackages/jax/_src/clusters/cloud_tpu_cluster.py"", line 74, in has_megascale_address     return get_tpu_env_value('MEGASCALE_COORDINATOR_ADDRESS') is not None            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/root/miniconda3/lib/python3.11/sitepackages/jax/_src/clusters/cloud_tpu_cluster.py"", line 71, in get_tpu_env_value     return value if value is not None else get_tpu_env_value_from_metadata(key)                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/root/miniconda3/lib/python3.11/sitepackages/jax/_src/clusters/cloud_tpu_cluster.py"", line 59, in get_tpu_env_value_from_metadata     tpu_env_data = get_metadata('tpuenv')[0]                    ^^^^^^^^^^^^^^^^^^^^^^^   File ""/root/miniconda3/lib/python3.11/sitepackages/jax/_src/clusters/cloud_tpu_cluster.py"", line 45, in get_metadata     api_resp = requests.get(                ^^^^^^^^^^^^^   File ""/root/miniconda3/lib/python3.11/sitepackages/requests/api.py"", line 73, in get     return request(""get"", url, params=params, **kwargs)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/root/miniconda3/lib/python3.11/sitepackages/requests/api.py"", line 59, in request     return session.request(method=method, url=url, **kwargs)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/root/miniconda3/lib/python3.11/sitepackages/requests/sessions.py"", line 589, in request     resp = self.send(prep, **send_kwargs)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/root/miniconda3/lib/python3.11/sitepackages/requests/sessions.py"", line 703, in send     r = adapter.send(request, **kwargs)         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/root/miniconda3/lib/python3.11/sitepackages/requests/adapters.py"", line 700, in send     raise ConnectionError(e, request=request) requests.exceptions.ConnectionError: HTTPConnectionPool(host='metadata.google.internal', port=80): Max retries exceeded with url: /computeMetadata/v1/instance/attributes/tpuenv (Caused by NewConnectionError(': Failed to establish a new connection: [Errno 111] Connection refused')) ``` pip list  ``` Package                 Versionw3:~    anacondaanonusage     0.4.4 archspec                0.2.3 boltons                 23.0.0 Brotli                  1.0.9 certifi                 2024.7.4 cffi                    1.16.0 charsetnormalizer      3.3.2 conda                   24.7.1 condacontenttrust     0.2.0 condalibmambasolver   24.7.0 condapackagehandling  2.3.0 conda_package_streaming 0.10.0 cryptography            42.0.5 distro                  1.9.0 frozendict              2.4.2 idna                    3.7 jax                     0.4.34 jaxlib                  0.4.34 jsonpatch               1.33 jsonpointer             2.1 libmambapy              1.5.8 libtpunightly          0.1.dev20241002+nightly menuinst                2.1.2 ml_dtypes               0.5.0 numpy                   2.1.2 opt_einsum              3.4.0 packaging               24.1 pip                     24.2 platformdirs            3.10.0 pluggy                  1.0.0 pycosat                 0.6.6 pycparser               2.21 PySocks                 1.7.1 requests                2.32.3 ruamel.yaml             0.17.21 scipy                   1.14.1 setuptools              72.1.0 tqdm                    4.66.4 truststore              0.8.0 urllib3                 2.2.2 wheel                   0.43.0 zstandard               0.22.0 ``` Setup bash ``` rm rf ~/miniconda3 wget https://repo.anaconda.com/miniconda/Miniconda3py311_24.7.10Linuxx86_64.sh bash Miniconda3py311_24.7.10Linuxx86_64.sh b u rm Miniconda3py311_24.7.10Linuxx86_64.sh ~/miniconda3/bin/conda init bash eval ""$(~/miniconda3/bin/conda shell.bash hook)""  2. Install requirements. pip install U jax[tpu] f https://storage.googleapis.com/jaxreleases/libtpu_releases.html ```  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.34 jaxlib: 0.4.34 numpy:  2.1.2 python: 3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0] jax.devices (128 total, 4 local): [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0) TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0) ... TpuDevice(id=126, process_index=31, coords=(2,3,7), core_on_chip=0) TpuDevice(id=127, process_index=31, coords=(3,3,7), core_on_chip=0)] process_count: 32 platform: uname_result(system='Linux', node='t1vndb3292aew17', release='5.19.01022gcp', version=' CC(attempt to centerjustify the jax logo in readme)~22.04.1Ubuntu SMP Sun Apr 23 09:51:08 UTC 2023', machine='x86_64')",2024-10-19T01:05:03Z,bug,open,8,4,https://github.com/jax-ml/jax/issues/24399,I'm facing the same problem..,"Hi Jax team, We are also facing this issue with TPUs v2, v3 and v4. However, v5p's are ok.","The issue should be resolved by now, could you please double check on a newly created TPU VM and let us know if it's still happening?","> The issue should be resolved by now, could you please double check on a newly created TPU VM and let us know if it's still happening?  Thank you for the update! I've tested on a newly created TPU VM, and the issue is now resolved. Everything is working as expected."
yi,Avoid querying metadata query to check if it's GCE if `TPU_SKIP_MDS_QUERY` is set.,Avoid querying metadata query to check if it's GCE if `TPU_SKIP_MDS_QUERY` is set.,2024-10-18T20:07:23Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24394
yi,Allowing HLO constants inside custom partition,"Hello JAX team, I am trying to implement a custom partitionned function that has consts in it  Here is the most minimal WE ```python import os from jaxlib.xla_extension import mlir os.environ['JAX_PLATFORM_NAME'] = 'cpu' os.environ['XLA_FLAGS'] = 'xla_force_host_platform_device_count=8' import jax from jax.experimental.custom_partitioning import custom_partitioning import jax.numpy as jnp from functools import partial from jax.sharding import Mesh, NamedSharding, PartitionSpec as P from jax.experimental.mesh_utils import create_device_mesh from jax import lax from jax.core import Primitive from jax.core import ShapedArray from jax._src import dispatch from jax.interpreters import xla, mlir def spmd_fftfreq_impl(array, d, dtype):   assert array.ndim == 3, ""Only 3D FFTFreq is supported""   kz, ky, kx = [jnp.fft.fftfreq(n, d, dtype=dtype) for n in array.shape]   return (kz.reshape([1, 1, 1]),           ky.reshape([1, 1, 1]),           kx.reshape([1, 1, 1]))  yapf: disable def per_shard_impl(a, d, dtype, mesh, x_axis_name=None, y_axis_name=None):   assert x_axis_name is not None and y_axis_name is not None   kx_sharding = NamedSharding(mesh, P(x_axis_name))   ky_sharding = NamedSharding(mesh, P(None, y_axis_name))   kz_sharding = NamedSharding(mesh, P(None, None, None))   kx = jnp.fft.fftfreq(a.shape[0], d, dtype=dtype).reshape(1, 1, 1)   ky = jnp.fft.fftfreq(a.shape[1], d, dtype=dtype).reshape(1, 1, 1)   kz = jnp.fft.fftfreq(a.shape[2], d, dtype=dtype).reshape(1, 1, 1)   kx = lax.with_sharding(kx, kx_sharding)   ky = lax.with_sharding(ky, ky_sharding)   kz = lax.with_sharding(kz, kz_sharding)   return kx, ky, kz def abstract_eval(a, d, dtype):   x_shape = (a.shape[0], 1, 1)   y_shape = (1, a.shape[1], 1)   z_shape = (1, 1, a.shape[2])   return (ShapedArray(x_shape, dtype),           ShapedArray(y_shape, dtype),           ShapedArray(z_shape, dtype))  yapf: disable fftfreq_primitive = Primitive('jax_fftfreq') dispatch.prim_requires_devices_during_lowering.add(fftfreq_primitive) fftfreq_primitive.multiple_results = True fftfreq_primitive.def_impl(spmd_fftfreq_impl) fftfreq_primitive.def_abstract_eval(abstract_eval) def infer_sharding_from_operands(d, dtype, mesh, arg_infos, result_infos):   del d, dtype, arg_infos, result_infos   x_axis_name, y_axis_name = mesh.axis_names   kx_sharding = NamedSharding(mesh, P(x_axis_name))   ky_sharding = NamedSharding(mesh, P(None, y_axis_name))   kz_sharding = NamedSharding(mesh, P(None, None, None))   print(f"""")   return (kx_sharding, ky_sharding, kz_sharding) def partition(d, dtype, mesh: Mesh, arg_infos, result_infos):   x_axis_name, y_axis_name = mesh.axis_names   print(f""result_infos: {result_infos}"")   input_sharding = NamedSharding(mesh, P(*arg_infos[0].sharding.spec))   output_sharding = NamedSharding(mesh, P(*result_infos.sharding.spec))   impl = partial(       per_shard_impl,       d=d,       dtype=dtype,       mesh=mesh,       x_axis_name=x_axis_name,       y_axis_name=y_axis_name)   return mesh, impl, output_sharding, (input_sharding,) partitionned_fftfreq = custom_partitioning(     spmd_fftfreq_impl, static_argnums=(1, 2)) partitionned_fftfreq.def_partition(     infer_sharding_from_operands=infer_sharding_from_operands,     partition=partition) mlir.register_lowering(     fftfreq_primitive,     mlir.lower_fun(partitionned_fftfreq, multiple_results=True)) (jax.jit, static_argnums=(1, 2)) def spmd_fftfreq(a, d=1.0, dtype=jnp.float32):   print(f""type of d: {type(d)}"")   return fftfreq_primitive.bind(a, d=d, dtype=dtype) devices = create_device_mesh((4, 2)) mesh = Mesh(devices, ('x', 'y')) sharding = NamedSharding(mesh, P('x', 'y')) a = jax.make_array_from_callback((4, 4, 4),                                  sharding,                                  data_callback=lambda _: jnp.ones((1, 2, 4))) print(f""a sharding: {a.sharding}"") with mesh:   kx, ky, kz = spmd_fftfreq(a, 1.0, jnp.float32) print(f""kx sharding: {kx.sharding}"") print(f""ky sharding: {ky.sharding}"") print(f""kz sharding: {kz.sharding}"") ``` So this give's me an error in custom_partionning ```python def def_partition(self,                       partition,                       infer_sharding_from_operands,                       propagate_user_sharding=None,                       decode_shardings=True):        ...     def __call__(self, *args, **kwargs):         ...         > assert not len(consts)         ...   ``` If I comment this line I get this error ``` jax/lib/python3.10/sitepackages/jax/_src/custom_partitioning.py"", line 203, in _custom_partitioning_infer_sharding_from_operands jax/lib/python3.10/sitepackages/jax/_src/custom_partitioning.py"", line 75, in unflatten_arg_shapes ValueError: Too many leaves for PyTreeDef; expected 1. ``` and the arg_shardings are  ``` consts [array([2., 1.], dtype=float32), array([2., 1.], dtype=float32), array([2., 1.], dtype=float32)] arg_shapes [f32[2]{0}, f32[2]{0}, f32[2]{0}, f32[4,4,4]{2,1,0}] arg_shardings [None, None, None, {devices=[4,2,1]<=[8]}] ``` Which corresponds to the consts that are disallowed Is there anyway around this?  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example.",2024-10-18T17:41:05Z,enhancement,closed,0,6,https://github.com/jax-ml/jax/issues/24390, Here is my issue as requested ,This is the assert that fires btw: https://github.com/jaxml/jax/blob/bbcc3eef3c1fedda3c0eef48c8bd49fd34a313c9/jax/_src/custom_partitioning.pyL462  do you know why we don't allow consts in the custom partitioning jaxpr?,"[Update] I was able to achieve the same goal with much simpler code ```python import jax from jax import numpy as jnp from functools import partial from jax.experimental.shard_alike import shard_alike (jax.jit, static_argnums=(1,)) def fftfreq3d(array, d=1.0):   kx = jnp.fft.fftfreq(array.shape[0], d=d, dtype=array.dtype) * 2 * jnp.pi   ky = jnp.fft.fftfreq(array.shape[1], d=d, dtype=array.dtype) * 2 * jnp.pi   kz = jnp.fft.fftfreq(array.shape[2], d=d, dtype=array.dtype) * 2 * jnp.pi   kx, _ = shard_alike(kx, array[:, 0, 0])   ky, _ = shard_alike(ky, array[0, :, 0])   kz, _ = shard_alike(kz, array[0, 0, :])   kx = kx.reshape([1, 1, 1])   ky = ky.reshape([1, 1, 1])   kz = kz.reshape([1, 1, 1])   return kx, ky, kz ``` Have seen no documentation on `shard_alike` but it is extremely usefull (so hopefully it is here to stay). About the constants in custom partition, I had another run in with this issue when using `norm='forward'` with a `jnp.fft.fft` and the workaround was normalizing outside the lowered_fn I would accept this issue to be a permanent restriction, and that users should find work arounds like the ones I did","can I please get a feedback on `shard_alike` I have this in the HLO ``` %allgatherstart = (f32[32]{0}, f32[128]{0}) allgatherstart(f32[32]{0} %loop_dynamic_slice_fusion.1), channel_id=33, replica_groups=[1,4]<=[4], dimensions={0}, use_global_device_ids=true, metadata={op_name=""jit(run_simulation)/jit(main)/jit(run_simulation)/jit(diffeqsolve)/outerloop/checkpointednovjp/while/body/jit(fftfreq3d)/scatter"" source_file=""/lustre/fswork/projects/rech/tkc/commun/venv/v100/lib/python3.10/sitepackages/jaxdecomp/_src/jax/fftfreq.py"" source_line=37}, backend_config={""operation_queue_id"":""0"",""wait_on_operation_queues"":[],""collective_backend_config"":{""is_sync"":true,""no_parallel_custom_call"":false,""is_pipelined"":false},""force_earliest_schedule"":false} %allgatherdone = f32[128]{0} allgatherdone((f32[32]{0}, f32[128]{0}) %allgatherstart), metadata={op_name=""jit(run_simulation)/jit(main)/jit(run_simulation)/jit(diffeqsolve)/outerloop/checkpointednovjp/while/body/jit(fftfreq3d)/scatter"" source_file=""/lustre/fswork/projects/rech/tkc/commun/venv/v100/lib/python3.10/sitepackages/jaxdecomp/_src/jax/fftfreq.py"" source_line=37} %loop_negate_fusion.2 = c64[32]{0} fusion(f32[128]{0} %allgatherdone, f32[64]{0} %constant_8714_0, u32[] %partitionid.33.0), kind=kLoop, calls=%fused_negate.2, metadata={op_name=""jit(run_simulation)/jit(main)/jit(run_simulation)/jit(diffeqsolve)/outerloop/checkpointednovjp/while/body/neg"" source_file=""/lustre/fswork/projects/rech/tkc/commun/JaxPM/jaxpm/pm.py"" source_line=58 deduplicated_name=""loop_negate_fusion.2""} ``` I can assume that it is scattering the frequencies however why the all gather? Is this how scatter is implemented in XLA?",The reason for no consts is that the jaxpr that is traced is different than the jaxpr that is partitioned and promoting it to a parameter would be strange because we would have to line up the trace time jaxpr consts with the partitioned jaxpr consts.,"Thank you   Closing since in all cases, workarounds exists Would be nice if documented somewhere"
rag,`jax.jit` slows down the code a lot on function with simple array operations and `jnp.roll()`," Description I get significant 4x slowdown in JAX code when I add a `.jit` to my main update function, which manipulates large arrays with elementwise math and `jnp.roll()` A minimal reproducer is included below, where removing the `.jit` around the `update()` function (line marked by a comment ` XXX`) speeds up the code a lot. The slowdown is not due to compiletime overhead. I'm quite puzzled by the behavior and think it may be a bug in JAX or XLA. What is the best way to get to the bottom of this issue? To reproduce, run `python euler.py` with and without the jit decorator around `update()`: ```python import jax import jax.numpy as jnp import time  simulation parameters N = 1024 boxsize = 1.0 dx = boxsize / N vol = dx**2 dt = 0.0001 .jit def get_conserved(rho, vx, vy, P):     """"""Calculate the conserved variables from the primitive variables""""""     Mass = rho * vol     Momx = rho * vx * vol     Momy = rho * vy * vol     Energy = (P / (5 / 3  1) + 0.5 * rho * (vx**2 + vy**2)) * vol     return Mass, Momx, Momy, Energy .jit def get_primitive(Mass, Momx, Momy, Energy):     """"""Calculate the primitive variable from the conserved variables""""""     rho = Mass / vol     vx = Momx / rho / vol     vy = Momy / rho / vol     P = (Energy / vol  0.5 * rho * (vx**2 + vy**2)) * (5 / 3  1)     return rho, vx, vy, P .jit def get_gradient(f):     """"""Calculate the gradients of a field""""""     f_dx = (jnp.roll(f, 1, axis=0)  jnp.roll(f, 1, axis=0)) / (2 * dx)     f_dy = (jnp.roll(f, 1, axis=1)  jnp.roll(f, 1, axis=1)) / (2 * dx)     return f_dx, f_dy .jit def extrapolate_to_face(f, f_dx, f_dy):     """"""Extrapolate the field from face centers to faces using gradients""""""     f_XL = f  f_dx * dx / 2     f_XL = jnp.roll(f_XL, 1, axis=0)     f_XR = f + f_dx * dx / 2     f_YL = f  f_dy * dx / 2     f_YL = jnp.roll(f_YL, 1, axis=1)     f_YR = f + f_dy * dx / 2     return f_XL, f_XR, f_YL, f_YR .jit def apply_fluxes(F, flux_F_X, flux_F_Y):     """"""Apply fluxes to conserved variables to update solution state""""""     F += dt * dx * flux_F_X     F += dt * dx * jnp.roll(flux_F_X, 1, axis=0)     F += dt * dx * flux_F_Y     F += dt * dx * jnp.roll(flux_F_Y, 1, axis=1)     return F .jit def get_flux(rho_L, rho_R, vx_L, vx_R, vy_L, vy_R, P):     """"""Calculate fluxes between 2 states""""""      left and right energies     en_L = P / (5 / 3  1) + 0.5 * rho_L * (vx_L**2 + vy_L**2)     en_R = P / (5 / 3  1) + 0.5 * rho_R * (vx_R**2 + vy_R**2)      compute star (averaged) states     rho_star = 0.5 * (rho_L + rho_R)     momx_star = 0.5 * (rho_L * vx_L + rho_R * vx_R)     momy_star = 0.5 * (rho_L * vy_L + rho_R * vy_R)     en_star = 0.5 * (en_L + en_R)     P_star = (5 / 3  1) * (en_star  0.5 * (momx_star**2 + momy_star**2) / rho_star)     flux_Mass = momx_star     flux_Momx = momx_star**2 / rho_star + P_star     flux_Momy = momx_star * momy_star / rho_star     flux_Energy = (en_star + P_star) * momx_star / rho_star      add stabilizing diffusive term     flux_Mass = 0.5 * 0.5 * (rho_L  rho_R)     flux_Momx = 0.5 * 0.5 * (rho_L * vx_L  rho_R * vx_R)     flux_Momy = 0.5 * 0.5 * (rho_L * vy_L  rho_R * vy_R)     flux_Energy = 0.5 * 0.5 * (en_L  en_R)     return flux_Mass, flux_Momx, flux_Momy, flux_Energy .jit   <  XXX Adding this line slows down the code a lot!! def update(Mass, Momx, Momy, Energy):     """"""Take a simulation timestep""""""     rho, vx, vy, P = get_primitive(Mass, Momx, Momy, Energy)     rho_dx, rho_dy = get_gradient(rho)     vx_dx, vx_dy = get_gradient(vx)     vy_dx, vy_dy = get_gradient(vy)     rho_XL, rho_XR, rho_YL, rho_YR = extrapolate_to_face(rho, rho_dx, rho_dy)     vx_XL, vx_XR, vx_YL, vx_YR = extrapolate_to_face(vx, vx_dx, vx_dy)     vy_XL, vy_XR, vy_YL, vy_YR = extrapolate_to_face(vy, vy_dx, vy_dy)     flux_Mass_X, flux_Momx_X, flux_Momy_X, flux_Energy_X = get_flux(         rho_XL, rho_XR, vx_XL, vx_XR, vy_XL, vy_XR, P     )     flux_Mass_Y, flux_Momy_Y, flux_Momx_Y, flux_Energy_Y = get_flux(         rho_YL, rho_YR, vy_YL, vy_YR, vx_YL, vx_YR, P     )     Mass = apply_fluxes(Mass, flux_Mass_X, flux_Mass_Y)     Momx = apply_fluxes(Momx, flux_Momx_X, flux_Momx_Y)     Momy = apply_fluxes(Momy, flux_Momy_X, flux_Momy_Y)     Energy = apply_fluxes(Energy, flux_Energy_X, flux_Energy_Y)     return Mass, Momx, Momy, Energy def main():     """"""Finite Volume simulation""""""      Setup     xlin = jnp.linspace(0.5 * dx, boxsize  0.5 * dx, N)     X, Y = jnp.meshgrid(xlin, xlin, indexing=""ij"")     rho = 1.0 + (jnp.abs(Y  0.5) < 0.25)     vx = 0.5 + (jnp.abs(Y  0.5) < 0.25)     vy = 0.1 * jnp.sin(4 * jnp.pi * X)     P = 2.5 * jnp.ones(X.shape)     Mass, Momx, Momy, Energy = get_conserved(rho, vx, vy, P)      Main Loop     tic = time.time()     for n_iter in range(40):         Mass, Momx, Momy, Energy = jax.block_until_ready(             update(Mass, Momx, Momy, Energy)         )         cell_updates = X.shape[0] * X.shape[1] * n_iter         total_time = time.time()  tic         mcups = cell_updates / (1e6 * total_time)         print(""  million cell updates / second: "", mcups)     print(""Total time: "", total_time) if __name__ == ""__main__"":     main() ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.33 jaxlib: 0.4.33 numpy:  2.1.2 python: 3.12.3  (main, Apr 15 2024, 18:35:20) [Clang 16.0.6 ] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Darwin', node='C916PXT6XW', release='23.6.0', version='Darwin Kernel Version 23.6.0: Wed Jul 31 20:50:00 PDT 2024; root:xnu10063.141.1.700.5~1/RELEASE_ARM64_T6031', machine='arm64') ```",2024-10-17T19:45:16Z,bug XLA,open,0,4,https://github.com/jax-ml/jax/issues/24373,"I am simplifying the code to highlight the error: ```python import jax import jax.numpy as jnp import time .jit def get_gradient(f):     """"""Calculate the gradients of a field""""""     f_dx = jnp.roll(f, 1, axis=0)  jnp.roll(f, 1, axis=0)     f_dy = jnp.roll(f, 1, axis=1)  jnp.roll(f, 1, axis=1)     return f_dx, f_dy .jit def extrapolate_to_face(f, f_dx, f_dy):     """"""Extrapolate the field from face centers to faces using gradients""""""     f_XL = f  f_dx     f_XL = jnp.roll(f_XL, 1, axis=0)     f_XR = f + f_dx     f_YL = f  f_dy     f_YL = jnp.roll(f_YL, 1, axis=1)     f_YR = f + f_dy     return f_XL, f_XR, f_YL, f_YR .jit def apply_fluxes(F, flux_F_X, flux_F_Y):     """"""Apply fluxes to conserved variables to update solution state""""""     F += flux_F_X     F += jnp.roll(flux_F_X, 1, axis=0)     F += flux_F_Y     F += jnp.roll(flux_F_Y, 1, axis=1)     return F .jit def get_flux(A_L, A_R, B_L, B_R):     """"""Calculate fluxes between 2 states""""""     A_star = 0.5 * (A_L + A_R)     B_star = 0.5 * (B_L + B_R)     flux_A = B_star     flux_B = B_star**2 / A_star     flux_A = 0.1 * (A_L  A_R)     flux_B = 0.1 * (B_L  B_R)     return flux_A, flux_B  .jit   <  XXX Adding this line slows down the code a lot!! def update(A, B):     """"""Take a simulation timestep""""""     A_dx, A_dy = get_gradient(A)     B_dx, B_dy = get_gradient(B)     A_XL, A_XR, A_YL, A_YR = extrapolate_to_face(A, A_dx, A_dy)     B_XL, B_XR, B_YL, B_YR = extrapolate_to_face(B, B_dx, B_dy)     flux_A_X, flux_B_X = get_flux(A_XL, A_XR, B_XL, B_XR)     flux_A_Y, flux_B_Y = get_flux(A_YL, A_YR, B_YL, B_YR)     A = apply_fluxes(A, flux_A_X, flux_A_Y)     B = apply_fluxes(B, flux_B_X, flux_B_Y)     return A, B .jit def update_compiled_SLOW(A, B):     return update(A, B) def main():     N = 1024     A = jnp.ones((N, N))     B = jnp.ones((N, N))     tic = time.time()     for _ in range(200):         (             A,             B,         ) = update(A, B)     print(""Total time not compiled: "", time.time()  tic)     A = jnp.ones((N, N))     B = jnp.ones((N, N))     tic = time.time()     for _ in range(200):         A, B = update_compiled_SLOW(A, B)     print(""Total time compiled: "", time.time()  tic) if __name__ == ""__main__"":     main() ``` gives: ```console Total time not compiled:  0.6709847450256348 Total time compiled:  2.1534647941589355 ```","Thanks for the report! This is definitely unexpected, and points to some compiler issue. I updated your timing to separate out the first call, use `block_until_ready` to avoid issues due to asynchronous dispatch, and use IPython's `%timeit` syntax for better fidelity: ```python _ = jax.block_until_ready(update(A, B, C)) %timeit jax.block_until_ready(update(A, B, C)) _ = jax.block_until_ready(update_compiled_SLOW(A, B, C)) %timeit jax.block_until_ready(update_compiled_SLOW(A, B, C)) ``` This is the result on Colab CPU: ``` 44.1 ms ± 7.17 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) 165 ms ± 27.6 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) ``` and this is the result on a Colab T4 GPU: ``` 2.72 ms ± 1.46 ms per loop (mean ± std. dev. of 7 runs, 100 loops each) 1.21 ms ± 11.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) ``` So it seems this issue is particular to the XLA:CPU compiler. It may be worth reporting this upstream at https://github.com/openxla/xla, though it would be useful to try and reduce the repro even further.","Thanks for taking a look at this  , and pinpointing that this seems to be a CPU only issue. Definitely unexpected. What is really weird too is that if I comment out some simple terms in the `apply_fluxes` function like: `flux_A = 0.1 * (A_L  A_R)`, `flux_B = 0.1 * (B_L  B_R)` then the issue goes away I will make an issue with the XLA team as well",XLA issue is raised here: https://github.com/openxla/xla/issues/18478
yi,[Apple GPU (Metal) plugin] (Stable) HLO support ?,"hi, I maintain GoMLX a machine learning framework based on XLA, and when trying to use it on Apple Metal (GPU) I bumped on the error bellow. `PJRT error (code=3): METAL only supports MLIR format input.` Are there any plans to support HLO ? GoMLX is feeding it with the output of the XlaBuilder, the compiled `HloModuleProto` proto that it exports. Any other suggestions ? many thanks! Full log: (Notice a couple of the lines are GoMLX specific, I named the plugin ""cpu"", but it's the one downloaded from `jaxmetal` pip package) ``` Loaded PJRT ""cpu"" plugin (/usr/local/lib/gomlx/pjrt/pjrt_c_api_cpu_plugin.dylib) v0.47 WARNING: All log messages before absl::InitializeLog() is called are written to STDERR W0000 00:00:1729171386.464515  169742 mps_client.cc:510] WARNING: JAX Apple GPU support is experimental and not all JAX functionality is correctly supported! Metal device set to: Apple M2 Pro I0000 00:00:1729171386.480718  169742 service.cc:145] XLA service 0x6000024bc400 initialized for platform METAL (this does not guarantee that XLA will be used). Devices: I0000 00:00:1729171386.480743  169742 service.cc:153]   StreamExecutor device (0): Metal,  I0000 00:00:1729171386.482386  169742 mps_client.cc:406] Using Simple allocator. I0000 00:00:1729171386.482398  169742 mps_client.cc:384] XLA backend will use up to 11452858368 bytes on device 0 for SimpleAllocator.         client: Client[plugin=""cpu"", platform=""METAL  metal_0.5.1"", singleprocess]  FAIL: TestEndToEnd (1.45s)     gopjrt_test.go:44:                  Error Trace:    /Users/m1/Projects/gopjrt/gopjrt_test.go:44                 Error:          Received unexpected error:                                 PJRT error (code=3): METAL only supports MLIR format input. ```",2024-10-17T14:20:40Z,enhancement Apple GPU (Metal) plugin,closed,0,3,https://github.com/jax-ml/jax/issues/24365, ,"To answer my own question, I think by MLIR the Metal PJRT plugin means StableHLO, encoded as ""mlir""  but not having used StableHLO I was not aware of the nuance. And I was pointed to the code in PyTorch XLA that deals with the same situation, if the PJRT doesn't accept ""hlo"" by converting it to StableHLO (as an `mlir::ModuleOp` object). Here is the link for others bumping into the same issue: https://github.com/pytorch/xla/blob/1bd17f7be2de80d82cba89f00732874d697871c9/torch_xla/csrc/runtime/pjrt_computation_client.ccL619 Closing the issue.",JAXMetal plugin accepts Stablehlo (MLIR) as input from JAX and compiles to an internal MLIR dialect. HLO computation as an IR used by XLA is not supported by JAXMetal. 
rag,Update ad.py," **1. Optimizing Memory Management**  **1.1 Efficient Tangent Handling with Zero Optimization**  **Change**: Implemented efficient handling of zero tangents using the `Zero` class to avoid unnecessary memory allocation and computations.  **Motivation**: Handling zero tangents efficiently is crucial as they frequently occur in automatic differentiation.  **Benefits**: Reduced memory usage and computational overhead, leading to faster execution.  **1.2 Lazy Evaluation to Defer Unnecessary Computations**  **Change**: Deferred computations involving zero tangents by introducing checks before performing operations.  **Motivation**: Shortcircuiting operations involving zeros avoids unnecessary computation.  **Benefits**: Improved performance and reduced resource usage.  **1.3 Efficient Storage Using Dictionaries**  **Change**: Used dictionaries for storing cotangents and primals, optimizing access and storage during the backward pass.  **Motivation**: Efficient data structures improve both the speed and memory usage of iterative algorithms like backpropagation.  **Benefits**: Faster lookup and more efficient memory management.   **2. Improving Error Handling**  **2.1 Enhanced Custom Exception Messages**  **Change**: Updated the `CustomJVPException` and `CustomVJPException` to provide detailed error messages.  **Motivation**: Clear error messages help users understand what went wrong and how to fix it.  **Benefits**: Easier debugging and improved user experience.  **2.2 Assertions and Validations for Primal and Tangent Matching**  **Change**: Added assertions to ensure that the shapes and data types of primals and tangents match.  **Motivation**: Preventing mismatches early avoids subtle, hardtotrace bugs.  **Benefits**: Improved code reliability and early detection of errors.   **3. Speed Enhancements**  **3.1 Optimized Differentiation Rules with Zero Handling**  **Change**: Updated the differentiation rules to efficiently handle zero tangents, reducing unnecessary computations.  **Motivation**: Optimizing common cases like zero tangents improves the efficiency of differentiation rules.  **Benefits**: Faster differentiation computations, especially in largescale applications.  **3.2 ShortCircuiting in Tangent Addition**  **Change**: Improved the tangent addition function by adding checks for zero values to avoid unnecessary operations.  **Motivation**: Handling special cases like zero tangents helps reduce redundant computations.  **Benefits**: Enhanced performance by eliminating unnecessary operations.   **4. Code Corrections and Completeness**  **4.1 Proper Definition of `custom_lin_p`**  **Change**: Added the missing definition of the `custom_lin_p` primitive, which was previously undefined.  **Motivation**: This ensures that custom VJP functions work as expected and eliminates reference errors.  **Benefits**: Proper functionality for custom VJP, improving code stability and preventing runtime errors.  **4.2 Consistent Variable Initialization**  **Change**: Ensured that all variables are properly declared and initialized before use.  **Motivation**: Proper initialization prevents `NameError` exceptions and improves code robustness.  **Benefits**: Increased code reliability and maintainability.   **5. good Programming Practices**  **5.1 Use of Type Annotations and Generics**  **Change**: Introduced Python type annotations to improve code readability and enable static analysis.  **Motivation**: Type annotations clarify expected data types and improve developer tooling support.  **Benefits**: Enhanced readability, better developer experience, and improved tooling for static analysis.  **5.2 Functional Programming Paradigms**  **Change**: Leveraged higherorder functions like `partial` and `map` to make the code more modular and concise.  **Motivation**: Functional programming practices lead to more modular and testable code.  **Benefits**: Code reusability, improved modularity, and cleaner logic.  **5.3 Use of Context Managers**  **Change**: Used context managers to ensure clean execution and prevent side effects during transformations.  **Motivation**: Context managers help manage resources and reduce the risk of errors.  **Benefits**: Improved resource management and error safety.   **6. Code Organization and Readability**  **6.1 Refactoring for Clarity**  **Change**: Refactored and reorganized the code to improve readability and simplify control flow.  **Motivation**: A wellorganized codebase is easier to navigate and maintain.  **Benefits**: Increased maintainability and readability for new contributors.  **6.2 Informative Comments and Documentation**  **Change**: Added comments explaining nontrivial parts of the code, providing context for future maintenance.  **Motivation**: Welldocumented code helps maintainers understand the logic and intent behind complex algorithms.  **Benefits**: Easier collaboration and improved understanding of the code for future contributors.  **7. Compliance with Best Practices**  **7.1 Exception Handling**  **Change**: Used exceptions judiciously to handle error cases without crashing the program unexpectedly.  **Motivation**: Proper error handling ensures that the program can gracefully handle unexpected situations.  **Benefits**: Improved robustness and user experience by providing meaningful feedback on errors.  **7.2 Efficiency Improvements**  **Change**: Added checks to avoid redundant computations, especially in operations involving zero tangents.  **Motivation**: Optimizing for efficiency ensures that the code can scale well for larger workloads.  **Benefits**: Faster execution and better scalability for largescale computations.",2024-10-16T16:35:01Z,,open,0,2,https://github.com/jax-ml/jax/issues/24337,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Thanks for the contribution! You'll need to sign the CLA before we can take a look in detail, but a couple broad points:  this PR combines logic changes with formatting changes, which makes it difficult for reviewers to isolate logic changes for review. I'd suggest reverting all formatting changes not related to the particular lines you've updated (for example, you should return the file to its orignal twospace indentation)  this PR makes many independent changes to the file, and while the enumerated description is helpful, it would be more helpful if each independent section of the description were done in its own PR. That would let us review each change individually (see https://jax.readthedocs.io/en/latest/contributing.htmlsinglechangecommitsandpullrequests for our recommendation here). Thanks!"
rag,[pallas:mosaic_gpu] Added `FragmentedArray.to_layout`,[pallas:mosaic_gpu] Added `FragmentedArray.to_layout`,2024-10-16T15:04:18Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24334
dspy,Improve array setitem error,"Before: ```python >>> import jax >>> x = jax.numpy.arange(4) >>> x[0] = 1 Traceback (most recent call last):   File """", line 1, in    File ""/Users/vanderplas/github/google/jax/jax/_src/numpy/array_methods.py"", line 587, in _unimplemented_setitem     raise TypeError(msg.format(type(self))) TypeError: '' object does not support item assignment. JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html ``` After: ```python >>> import jax >>> x = jax.numpy.arange(4) >>> x[0] = 1 Traceback (most recent call last):   File """", line 1, in    File ""/Users/vanderplas/github/google/jax/jax/_src/numpy/array_methods.py"", line 586, in _unimplemented_setitem     raise TypeError(msg.format(type(self))) TypeError: JAX arrays are immutable and do not support inplace item assignment. Instead of x[idx] = y, use x = x.at[idx].set(y) or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html ```",2024-10-15T18:41:43Z,better_errors pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/24315
yi,Matrix factorization on multiple GPUs causes memory overflow," Description My original aim is to compute a huge amount of determinants that can't fit into the memory of a single GPU. I always get memory overflows when I run it on multiple GPUs, and the problem seems to be the matrix factorization. Here is a simple example to perform matrix factorization in parallel. I parallelize more computations when I have more machines. ``` import jax import jax.numpy as jnp from jax.sharding import NamedSharding, Mesh, PartitionSpec mesh = Mesh(jax.devices(), ""x"") pspecs = PartitionSpec(""x"") sharding = NamedSharding(mesh, pspecs) a = jnp.zeros((jax.device_count(), 1500000, 40, 40), device=sharding) out = jax.lax.linalg.lu(a) ``` It works well on a single A10080GB GPU, but causes the following memory overflow on 3 GPUs. It seems that I can never parallelize more computations with multiple machines. Other matrix factorizations like `qr` or `cholesky` cause the same problem. ``` 20241015 16:30:37.290084: W external/xla/xla/service/hlo_rematerialization.cc:3005] Can't reduce memory use below 21.15GiB (22705764841 bytes) by rematerialization; only reduced to 90.75GiB (97440000040 bytes), down from 90.75GiB (97440000040 bytes) originally 20241015 16:30:47.781059: W external/xla/xla/tsl/framework/bfc_allocator.cc:497] Allocator (GPU_0_bfc) ran out of memory trying to allocate 26.83GiB (rounded to 28804500480)requested by op  20241015 16:30:47.781185: W external/xla/xla/tsl/framework/bfc_allocator.cc:497] Allocator (GPU_2_bfc) ran out of memory trying to allocate 26.83GiB (rounded to 28804500480)requested by op  20241015 16:30:47.781317: W external/xla/xla/tsl/framework/bfc_allocator.cc:508] ***************************************************************_____________________________________ E1015 16:30:47.781354  473320 pjrt_stream_executor_client.cc:3084] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 28804500256 bytes. 20241015 16:30:47.781345: W external/xla/xla/tsl/framework/bfc_allocator.cc:497] Allocator (GPU_1_bfc) ran out of memory trying to allocate 26.83GiB (rounded to 28804500480)requested by op  20241015 16:30:47.781454: W external/xla/xla/tsl/framework/bfc_allocator.cc:508] ***************************************************************_____________________________________ E1015 16:30:47.781486  473326 pjrt_stream_executor_client.cc:3084] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 28804500256 bytes. 20241015 16:30:47.781549: W external/xla/xla/tsl/framework/bfc_allocator.cc:508] ***************************************************************_____________________________________ E1015 16:30:47.781597  473323 pjrt_stream_executor_client.cc:3084] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 28804500256 bytes.  XlaRuntimeError                           Traceback (most recent call last) Cell In[1], line 13      10 sharding = NamedSharding(mesh, pspecs)      12 a = jnp.zeros((jax.device_count(), 1500000, 40, 40), device=sharding) > 13 out = jax.lax.linalg.lu(a) File /hpc/gpfs2/scratch/u/chenao/.conda/envs/quantax_env/lib/python3.12/sitepackages/jax/_src/lax/linalg.py:277, in lu(x)     247 def lu(x: ArrayLike) > tuple[Array, Array, Array]:     248   """"""LU decomposition with partial pivoting.     249      250   Computes the matrix decomposition:    (...)     275     ``[..., m]``.     276   """""" > 277   lu, pivots, permutation = lu_p.bind(x)     278   return lu, pivots, permutation File /hpc/gpfs2/scratch/u/chenao/.conda/envs/quantax_env/lib/python3.12/sitepackages/jax/_src/core.py:438, in Primitive.bind(self, *args, **params)     435 def bind(self, *args, **params):     436   assert (not config.enable_checks.value or     437           all(isinstance(arg, Tracer) or valid_jaxtype(arg) for arg in args)), args > 438   return self.bind_with_trace(find_top_trace(args), args, params) File /hpc/gpfs2/scratch/u/chenao/.conda/envs/quantax_env/lib/python3.12/sitepackages/jax/_src/core.py:442, in Primitive.bind_with_trace(self, trace, args, params)     440 def bind_with_trace(self, trace, args, params):     441   with pop_level(trace.level): > 442     out = trace.process_primitive(self, map(trace.full_raise, args), params)     443   return map(full_lower, out) if self.multiple_results else full_lower(out) File /hpc/gpfs2/scratch/u/chenao/.conda/envs/quantax_env/lib/python3.12/sitepackages/jax/_src/core.py:948, in EvalTrace.process_primitive(self, primitive, tracers, params)     946   return call_impl_with_key_reuse_checks(primitive, primitive.impl, *tracers, **params)     947 else: > 948   return primitive.impl(*tracers, **params) File /hpc/gpfs2/scratch/u/chenao/.conda/envs/quantax_env/lib/python3.12/sitepackages/jax/_src/lax/linalg.py:1408, in _lu_impl(operand)    1407 def _lu_impl(operand): > 1408   lu, pivot, perm = dispatch.apply_primitive(lu_p, operand)    1409   return lu, pivot, perm File /hpc/gpfs2/scratch/u/chenao/.conda/envs/quantax_env/lib/python3.12/sitepackages/jax/_src/dispatch.py:90, in apply_primitive(prim, *args, **params)      88 prev = lib.jax_jit.swap_thread_local_state_disable_jit(False)      89 try: > 90   outs = fun(*args)      91 finally:      92   lib.jax_jit.swap_thread_local_state_disable_jit(prev)     [... skipping hidden 10 frame] File /hpc/gpfs2/scratch/u/chenao/.conda/envs/quantax_env/lib/python3.12/sitepackages/jax/_src/interpreters/pxla.py:1288, in ExecuteReplicated.__call__(self, *args)    1286   self._handle_token_bufs(result_token_bufs, sharded_runtime_token)    1287 else: > 1288   results = self.xla_executable.execute_sharded(input_bufs)    1290 if dispatch.needs_check_special():    1291   out_arrays = results.disassemble_into_single_device_arrays() XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 28804500256 bytes.: while running replica 0 and partition 0 of a replicated computation (other replicas may have failed as well). ``` I have tried `jax.transfer_guard` to ensure that no data is transferred among machines during matrix factorization. Maybe I made some stupid mistakes. I really appreciate any help with this problem, or any suggestion to obtain determinants in parallel.  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.34 jaxlib: 0.4.34 numpy:  2.0.2 python: 3.12.7  ++++ ```",2024-10-15T15:00:05Z,bug,open,1,3,https://github.com/jax-ml/jax/issues/24309,"Unfortunately, the JAX/XLA compiler currently does not know how to shard  `jax.lax.linalg.lu` operator. As a workaround, you can still do the sharding manually, see the JAX docs. For your example, the manually parallelized version could be something like this: ```python import jax import jax.numpy as jnp from jax.sharding import NamedSharding, Mesh, PartitionSpec from jax.experimental.shard_map import shard_map mesh = Mesh(jax.devices(), ""x"") pspecs = PartitionSpec(""x"", None) sharding = NamedSharding(mesh, pspecs) a = jnp.zeros((jax.device_count(), 1500000, 40, 40), device=sharding) sharded_lu = shard_map(jax.lax.linalg.lu,                        mesh=mesh,                        in_specs=(pspecs,),                        out_specs=(pspecs, pspecs, pspecs)) sharded_jitted_lu = jax.jit(sharded_lu) out = sharded_jitted_lu(a) ```","sevcik Thanks very much for your reply! This solves my problem. By the way, is there any plan to solve the sharding problem in the future? It looks straightforward.","Phys — Thanks for the report! And thanks to sevcik for suggesting this workaround. > By the way, is there any plan to solve the sharding problem in the future? It looks straightforward. JAX doesn't currently have a great API for customizing the sharding behavior of custom calls (which is how most of the linear algebra operations are implemented). `custom_partitioning` is one option, but the fact that it is implemented using Python callbacks means that it can introduce some surprising issues that are probably beyond the scope of the discussion here. All that to say, a better solution would be great (and should be possible with upstream changes in XLA), but I'd say it's probably not straightforward. For now, sevcik's suggestion to use `shard_map` is the best approach!"
yi,Gradient of jnp.linalg.eigvals raises NotImplementedError about eigenvectors," Description Hey, Im trying to calculate a hessian. Within my function im calculating some eigenvalues using jnp.linalg.eigvals(). This raises the mentioned error. ""NotImplementedError: The derivatives of eigenvectors are not implemented, only eigenvalues. See https://github.com/google/jax/issues/2748 for discussion."" However the eigenvector calculation is turned off within the eigvals() function. So Im not sure whats going on. The issue pointed out in the error message is 4 years old and mainly concerned with the eigenvectors. I am using jax 0.4.31.  Does anyone know whats going on here?  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.31 jaxlib: 0.4.31 numpy:  1.26.0 python: 3.11.7  (main, Dec 23 2023, 14:43:09) [GCC 12.3.0] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='Name', release='5.15.153.1microsoftstandardWSL2', version=' CC(Python 3 compatibility issues) SMP Fri Mar 29 23:14:13 UTC 2024', machine='x86_64')",2024-10-15T14:19:58Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/24308,"Hi  thanks for the question! The issue is that gradients of eigenvectors and eigenvalues for nonsymmetric inputs is poorly defined, mainly due to degeneracies in the output if I recall the discussion correctly. So despite the github issue being 4 years old, the problem it discusses still remains.","Hello , thanks that clears it up. Sadly my input matrix is generally not symmetric.  To avoid any future confusion I'd suggest to that the error message should be changed. Because ""not implemented"" and ""not clearly defined"" are different things.  And its confusing that the error message only speaks of eigenvectors when one is trying only to get eigenvalues. ","Hey, sorry I was just looking at this further, and it seems that nonsymmetric eigenvalues should differentiate without any issues. For example: ```python import jax import jax.numpy as jnp def f(x):   return abs(jnp.linalg.eigvals(x)).max() x = jnp.arange(9.0).reshape(3, 3) print(jax.grad(f)(x)) ``` ``` [[0.07645979 0.234708   0.39295638]  [0.09862533 0.3027494  0.50687367]  [0.12079085 0.37079072 0.62079084]] ``` Can you provide an example of code that leads to the behavior you describe?","Ah thats weird. Your example works for me as well.  When recreating the error I realized that this issue only arises when calculating the hessian. The gradient actually works fine.  If In your example you do `jax.grad(jax.grad(f))(x)` you'll get the error. (at least I get it) ""NotImplementedError: The derivatives of eigenvectors are not implemented, only eigenvalues. See https://github.com/google/jax/issues/2748 for discussion.""","Ah, I see  it's becuase the jvp rule for `eig` depends on the eigenvectors: https://github.com/jaxml/jax/blob/ebac2e44219a40862a9798484da1662d6800a1e9/jax/_src/lax/linalg.pyL774 The result is that if you differentiate twice, you hit gradofeigenvectors, which leads to this error. So I think the resolution here is that we only support firstorder gradients with respect to eigenvectors, unfortunately.",I've updated the error message in https://github.com/jaxml/jax/pull/24350 – hopefully that will be clearer to future users who hit this error.,"Alright, thank you very much. "
yi,JAX argmin function returns incorrect index when handling subnormal float values.," Description When using the JAX `argmin` function on an input array containing subnormal float values, JAX consistently returns the index of `0.0` as the minimum value, even though a smaller subnormal value (`1.401298464324817e45`) exists in the array. Other deep learning frameworks such as PyTorch and Chainer correctly return the index of the subnormal value, but JAX (similar to TensorFlow and Keras) returns the index of `0`.  Expected Behavior: The expected behavior is for JAX's `argmin` function to return the index of the smallest value, which should be the subnormal float value (`1.401298464324817e45`) at index 2. Instead, JAX is returning the index of `0.0` (index 0). ``` import torch import tensorflow as tf import numpy as np from chainer import functions as F import jax.numpy as jnp import tensorflow.keras.backend as K  Input data input_data = [     0.0,     1.1754943508222875e38,     1.401298464324817e45,     0.0,     459367.0 ]  Test PyTorch def test_pytorch_argmin(input_data):     tensor = torch.tensor(input_data, dtype=torch.float32)     result = torch.argmin(tensor).item()     print(f""PyTorch argmin result: {result}"")     return result  Test TensorFlow def test_tensorflow_argmin(input_data):     tensor = tf.constant(input_data, dtype=tf.float32)     result = tf.argmin(tensor).numpy()     print(f""TensorFlow argmin result: {result}"")     return result  Test Keras using backend def test_keras_argmin(input_data):     tensor = K.constant(input_data, dtype=tf.float32)     result = K.argmin(tensor, axis=1).numpy()     print(f""Keras argmin result: {result}"")     return result  Test Chainer def test_chainer_argmin(input_data):     tensor = np.array(input_data, dtype=np.float32)     result = F.argmin(tensor).data     print(f""Chainer argmin result: {result}"")     return result  Test JAX def test_jax_argmin(input_data):     tensor = jnp.array(input_data, dtype=jnp.float32)     result = jnp.argmin(tensor).item()     print(f""JAX argmin result: {result}"")     return result if __name__ == ""__main__"":     pytorch_result = test_pytorch_argmin(input_data)     tensorflow_result = test_tensorflow_argmin(input_data)     keras_result = test_keras_argmin(input_data)     chainer_result = test_chainer_argmin(input_data)     jax_result = test_jax_argmin(input_data)     print(""\nSummary of results:"")     print(f""PyTorch argmin: {pytorch_result}"")     print(f""TensorFlow argmin: {tensorflow_result}"")     print(f""Keras argmin: {keras_result}"")     print(f""Chainer argmin: {chainer_result}"")     print(f""JAX argmin: {jax_result}"") ```  System info (python version, jaxlib version, accelerator, etc.) ``` Summary of results: PyTorch argmin: 2 TensorFlow argmin: 0 Keras argmin: 0 Chainer argmin: 2 JAX argmin: 0 ```",2024-10-15T08:28:48Z,duplicate,closed,0,1,https://github.com/jax-ml/jax/issues/24307,This issue is duplicate of https://github.com/jaxml/jax/issues/24281 with a resolution depending on the answer to https://github.com/jaxml/jax/issues/24280issuecomment2411336992 .
yi,`custom_root` with integer aux output broken in 0.4.34," Description I have a basic root finder like this: ```python import jax import jax.numpy as jnp def root(     fun,     x0,     jac=None,     args=(),     tol=1e6,     maxiter=20, ):     """"""Find x where fun(x, *args) == 0.""""""     jac2 = lambda x: jnp.atleast_2d(jax.jacfwd(fun)(x, *args))     res = lambda x: jnp.atleast_1d(fun(x, *args)).flatten()     def solve(resfun, guess):         def condfun(state):             xk1, fk1, k1 = state             return (k1  tol**2)         def bodyfun(state):             xk1, fk1, k1 = state             J = jac2(xk1)             d = jnp.linalg.solve(J, fk1)             xk2 = xk1  d             fk2 = resfun(xk2)             return xk2, fk2, k1 + 1         state = (             jnp.atleast_1d(jnp.asarray(guess)),  x             jnp.atleast_1d(resfun(guess)),  residual             0,  number of iterations         )         state = jax.lax.while_loop(condfun, bodyfun, state)         return state[0], state[1:]     def tangent_solve(g, y):         A = jnp.atleast_2d(jax.jacfwd(g)(y))         return jnp.linalg.solve(A, jnp.atleast_1d(y))     x, aux = jax.lax.custom_root(         res, x0, solve, tangent_solve, has_aux=True     )     return x, aux ``` which returns both the root and the value of f at the root, and the number of steps taken. Previously this worked fine, with `has_aux=True` for `custom_root`. However, v0.4.34 seems to have changed something in the way tangents of nondifferentiable values get propagated ( CC(未找到相关数据)). Now running the following  ```python def fun(x, a):     return a*x  1 def find_root_fun(a):     x0 = jnp.array([0.,])     xk, aux = root(fun, x0, args=(a,))     return xk, aux jax.jacfwd(find_root_fun, has_aux=True)(a) ``` gives the following: ```  TypeError                                 Traceback (most recent call last) Cell In[26], line 59      56     xk, aux = root(fun, x0, args=(a,))      57     return xk, aux > 59 jax.jacfwd(find_root_fun, has_aux=True)(a) File ~/miniconda3/envs/desc/lib/python3.10/sitepackages/jax/_src/api.py:584, in jacfwd..jacfun(*args, **kwargs)     582 else:     583   pushfwd: Callable = partial(_jvp, f_partial, dyn_args, has_aux=True) > 584   y, jac, aux = vmap(pushfwd, out_axes=(None, 1, None))(_std_basis(dyn_args))     585 tree_map(partial(_check_output_dtype_jacfwd, holomorphic), y)     586 example_args = dyn_args[0] if isinstance(argnums, int) else dyn_args     [... skipping hidden 5 frame] Cell In[26], line 56, in find_root_fun(a)      54 def find_root_fun(a):      55     x0 = jnp.array([0.,]) > 56     xk, aux = root(fun, x0, args=(a,))      57     return xk, aux Cell In[26], line 43, in root(fun, x0, jac, args, tol, maxiter)      40     A = jnp.atleast_2d(jax.jacfwd(g)(y))      41     return jnp.linalg.solve(A, jnp.atleast_1d(y)) > 43 x, (f, niter) = jax.lax.custom_root(      44     res, x0, solve, tangent_solve, has_aux=True      45 )      46 return x,  (jnp.sum(jnp.abs(f)), niter)     [... skipping hidden 7 frame] File ~/miniconda3/envs/desc/lib/python3.10/sitepackages/jax/_src/custom_derivatives.py:351, in _flatten_jvp(primal_name, jvp_name, in_tree, maybe_out_type, *args)     344     msg = (""Custom JVP rule must produce primal and tangent outputs with ""     345            ""corresponding shapes and dtypes, but got:\n{}"")     346     disagreements = (     347         f""  primal {av_p.str_short()} with tangent {av_t.str_short()}, expecting tangent {av_et}""     348         for av_p, av_et, av_t in zip(primal_avals_out, expected_tangent_avals_out, tangent_avals_out)     349         if av_et != av_t) > 351     raise TypeError(msg.format('\n'.join(disagreements)))     352 yield primals_out + tangents_out, (out_tree, primal_avals) TypeError: Custom JVP rule must produce primal and tangent outputs with corresponding shapes and dtypes, but got:   primal int64[] with tangent int64[], expecting tangent ShapedArray(float0[]) ``` I get the same error if I drop the aux output in `find_root_fun` and leave out the `has_aux` when calling `jacfwd`. The only way I've found to avoid the error is to remove the aux from the innermost `solve` and set `has_aux=False` on `custom_root` Is this expected? I assumed having integer valued aux output was kind of the point of the `has_aux` option?  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.34 jaxlib: 0.4.34 numpy:  1.24.4 python: 3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0] jax.devices (8 total, 8 local): [CpuDevice(id=0) CpuDevice(id=1) ... CpuDevice(id=6) CpuDevice(id=7)] process_count: 1 platform: uname_result(system='Linux', node='Discovery', release='5.15.0119generic', version=' CC(CUDA backend produces inconsistent results for jax.numpy.linalg.inv)~20.04.1Ubuntu SMP Wed Aug 7 13:07:13 UTC 2024', machine='x86_64')",2024-10-14T19:17:14Z,bug,open,1,1,https://github.com/jax-ml/jax/issues/24295,"I think we can simply get rid of the error by changing the number of iterations dtype to float like, ```python         ...         state = (             jnp.atleast_1d(jnp.asarray(guess)),  x             jnp.atleast_1d(resfun(guess)),  residual             0.0,  number of iterations         )         ... ``` As long as we don't use the derivative of the number of iterations later in the code, I believe this shouldn't change the differentiation of `root`.  That said, this is probably not how you want to implement it. A more proper way could be writing `custom_jvp` for `root` and setting the derivative of `niter` to `SymbolicZeros`, but this is more cumbersome."
yi,Improve docs for jnp.invert and related functions,Part of CC(Tracking issue: inline docstrings)  Rendered docs:  https://jax24294.org.readthedocs.build/en/24294/_autosummary/jax.numpy.bitwise_invert.html  https://jax24294.org.readthedocs.build/en/24294/_autosummary/jax.numpy.bitwise_not.html  https://jax24294.org.readthedocs.build/en/24294/_autosummary/jax.numpy.invert.html  https://jax24294.org.readthedocs.build/en/24294/_autosummary/jax.numpy.logical_not.html,2024-10-14T18:24:17Z,documentation pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/24294
yi,Arcsin Error Report," Description Problem Description There is a noticeable discrepancy in the results when using JAX for the arcsin function compared to other deep learning libraries such as PyTorch, TensorFlow, Keras, and Chainer. For certain input values, JAX yields results that are significantly different from those produced by the other frameworks, leading to concerns about consistency and accuracy. ``` import torch import tensorflow as tf import jax.numpy as jnp import numpy as np from keras.layers import Lambda  Input data (as provided) input_data = np.array([     [         [             0.5834500789642334,             0.05778983607888222         ],         [             0.13608911633491516,             0.8511932492256165         ],         [             0.8579278588294983,             0.8257414102554321         ],         [             0.9595631957054138,             0.665239691734314         ],         [             0.5563135147094727,             0.7400242686271667         ],         [             0.9572367072105408,             0.5983171463012695         ],         [             0.07704128324985504,             0.5610583424568176         ],         [             0.7634511590003967,             0.2798420488834381         ],         [             0.7132934331893921,             0.8893378376960754         ]     ] ])  PyTorch arcsin operation def torch_arcsin(x):     return torch.asin(torch.tensor(x, dtype=torch.float32))  TensorFlow arcsin operation def tf_arcsin(x):     return tf.asin(tf.convert_to_tensor(x, dtype=tf.float32))  Keras arcsin operation def keras_arcsin(x):     return Lambda(lambda x: tf.math.asin(x))(tf.convert_to_tensor(x, dtype=tf.float32))  JAX arcsin operation def jax_arcsin(x):     return jnp.arcsin(jnp.array(x))  Chainer arcsin operation def chainer_arcsin(x):     return np.arcsin(x)  Calculate results pytorch_result = torch_arcsin(input_data).detach().numpy()   Detach to convert to numpy tensorflow_result = tf_arcsin(input_data).numpy() keras_result = keras_arcsin(input_data).numpy()   Convert Keras result to numpy jax_result = jax_arcsin(input_data) chainer_result = chainer_arcsin(input_data)  Print results print(f""PyTorch arcsin result: {pytorch_result}"") print(f""TensorFlow arcsin result: {tensorflow_result}"") print(f""Keras arcsin result: {keras_result}"") print(f""JAX arcsin result: {jax_result}"") print(f""Chainer arcsin result: {chainer_result}"")  Compare results tolerance = 1e7   Set a tolerance for comparison results = {     ""PyTorch"": pytorch_result,     ""TensorFlow"": tensorflow_result,     ""Keras"": keras_result,     ""JAX"": jax_result,     ""Chainer"": chainer_result } for name, result in results.items():     diff = np.abs(results[""PyTorch""]  result)     print(f""Difference with {name}: {diff}"")  Check for passing criteria passed = all(np.allclose(results[""PyTorch""], result, atol=tolerance) for name, result in results.items() if name != ""PyTorch"") print(f""Tests passed: {passed}"") ``` ``` PyTorch arcsin result: [[[ 0.62297034  0.05782205]   [ 0.13651273  1.0182546 ]   [1.0312228  0.97151536]   [1.2854464   0.7278148 ]   [ 0.5899428   0.83310646]   [ 1.2772948   0.6413992 ]   [0.0771177   0.5956638 ]   [0.8686398   0.28362957]   [0.7941861   1.0958949 ]]] TensorFlow arcsin result: [[[ 0.62297034  0.05782206]   [ 0.13651271  1.0182548 ]   [1.0312228  0.97151536]   [1.2854464   0.727815  ]   [ 0.5899428   0.83310646]   [ 1.2772948   0.64139926]   [0.0771177   0.5956638 ]   [0.8686399   0.2836296 ]   [0.7941861   1.095895  ]]] Keras arcsin result: [[[ 0.62297034  0.05782206]   [ 0.13651271  1.0182548 ]   [1.0312228  0.97151536]   [1.2854464   0.727815  ]   [ 0.5899428   0.83310646]   [ 1.2772948   0.64139926]   [0.0771177   0.5956638 ]   [0.8686399   0.2836296 ]   [0.7941861   1.095895  ]]] JAX arcsin result: [[[ 0.62297034  0.05782205]   [ 0.13651271  1.0182546 ]   [1.0312228  0.97151536]   [1.2854463   0.7278148 ]   [ 0.5899429   0.8331064 ]   [ 1.2772946   0.6413992 ]   [0.0771177   0.5956638 ]   [0.8686398   0.28362957]   [0.7941861   1.0958949 ]]] Chainer arcsin result: [[[ 0.62297033  0.05782205]   [ 0.13651272  1.01825461]   [1.03122278 0.97151538]   [1.28544635  0.7278148 ]   [ 0.58994283  0.83310644]   [ 1.27729471  0.6413992 ]   [0.0771177   0.59566378]   [0.86863983  0.28362958]   [0.79418614  1.09589499]]] ``` Significant Differences The result for the input [1.2772946] is notably different between JAX and the other libraries, which may affect the accuracy of downstream tasks. Recommendation It is recommended to review the implementation of the arcsin function in JAX to ensure consistency and accuracy. Special attention should be given to how floatingpoint arithmetic and trigonometric functions are handled, as they can significantly influence results.  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.26 jaxlib: 0.4.26 numpy:  1.26.4 python: 3.9.19  (main, Mar 20 2024, 12:38:46) [MSC v.1929 64 bit (AMD64)] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Windows', node='Lily的电脑', release='10', version='10.0.22631', machine='AMD64') ```",2024-10-14T02:30:15Z,duplicate question,closed,0,8,https://github.com/jax-ml/jax/issues/24275,"Thanks for the question! The reason for the discrepancy is that JAX does computations in float32 by default (see JAX Sharp Bits: double precision. If you enable 64bit operations, then the JAX output matches the output of NumPy and other frameworks that compute in 64bit by default: ```python import numpy as np import jax jax.config.update('jax_enable_x64', True) x = np.array(0.9572367072105408, dtype='float64') print(""numpy:"", np.arcsin(x)) print(""jax:  "", jnp.arcsin(x)) ``` ``` numpy: 1.2772947080197108 jax:   1.2772947080197108 ``` and note also that with float32 inputs, NumPy's output matches the JAX output that you observed originally: ```python x = np.array(0.9572367072105408, dtype='float32') print(""numpy:"", np.arcsin(x)) print(""jax:  "", jnp.arcsin(x)) ``` ``` numpy: 1.2772946 jax:   1.2772946 ```","> Thanks for the question! The reason for the discrepancy is that JAX does computations in float32 by default (see JAX Sharp Bits: double precision. If you enable 64bit operations, then the JAX output matches the output of NumPy and other frameworks that compute in 64bit by default: >  > ```python > import numpy as np > import jax > jax.config.update('jax_enable_x64', True) >  > x = np.array(0.9572367072105408, dtype='float64') >  > print(""numpy:"", np.arcsin(x)) > print(""jax:  "", jnp.arcsin(x)) > ``` >  > ``` > numpy: 1.2772947080197108 > jax:   1.2772947080197108 > ``` >  > and note also that with float32 inputs, NumPy's output matches the JAX output that you observed originally: >  > ```python > x = np.array(0.9572367072105408, dtype='float32') >  > print(""numpy:"", np.arcsin(x)) > print(""jax:  "", jnp.arcsin(x)) > ``` >  > ``` > numpy: 1.2772946 > jax:   1.2772946 > ``` Similarly, all of these are specified as float32, and there are some differences when compared with the results of np. ``` import torch import tensorflow as tf import jax.numpy as jnp import numpy as np from keras.layers import Lambda  Input data (as provided)  all values are set to float32 input_data = np.array([     [         [             0.5834500789642334,             0.05778983607888222         ],         [             0.13608911633491516,             0.8511932492256165         ],         [             0.8579278588294983,             0.8257414102554321         ],         [             0.9595631957054138,             0.665239691734314         ],         [             0.5563135147094727,             0.7400242686271667         ],         [             0.9572367072105408,             0.5983171463012695         ],         [             0.07704128324985504,             0.5610583424568176         ],         [             0.7634511590003967,             0.2798420488834381         ],         [             0.7132934331893921,             0.8893378376960754         ]     ] ], dtype=np.float32)   Ensure input data is float32  PyTorch arcsin operation def torch_arcsin(x):     return torch.asin(torch.tensor(x, dtype=torch.float32))  TensorFlow arcsin operation def tf_arcsin(x):     return tf.asin(tf.convert_to_tensor(x, dtype=tf.float32))  Keras arcsin operation def keras_arcsin(x):     return Lambda(lambda x: tf.math.asin(x))(tf.convert_to_tensor(x, dtype=tf.float32))  JAX arcsin operation def jax_arcsin(x):     return jnp.arcsin(jnp.array(x, dtype=np.float32))   Ensure the input is float32  Chainer arcsin operation def chainer_arcsin(x):     return np.arcsin(x.astype(np.float32))   Ensure the input is float32  Calculate results pytorch_result = torch_arcsin(input_data).detach().numpy()   Detach to convert to numpy tensorflow_result = tf_arcsin(input_data).numpy() keras_result = keras_arcsin(input_data).numpy()   Convert Keras result to numpy jax_result = jax_arcsin(input_data) chainer_result = chainer_arcsin(input_data)  Print results print(f""PyTorch arcsin result: {pytorch_result}"") print(f""TensorFlow arcsin result: {tensorflow_result}"") print(f""Keras arcsin result: {keras_result}"") print(f""JAX arcsin result: {jax_result}"") print(f""Chainer arcsin result: {chainer_result}"")  Compare results tolerance = 1e7   Set a tolerance for comparison results = {     ""PyTorch"": pytorch_result,     ""TensorFlow"": tensorflow_result,     ""Keras"": keras_result,     ""JAX"": jax_result,     ""Chainer"": chainer_result } for name, result in results.items():     diff = np.abs(results[""PyTorch""]  result)     print(f""Difference with {name}: {diff}"")  Check for passing criteria passed = all(np.allclose(results[""PyTorch""], result, atol=tolerance) for name, result in results.items() if name != ""PyTorch"") print(f""Tests passed: {passed}"") ``` ``` JAX arcsin result: [[[ 0.62297034  0.05782205]   [ 0.13651271  1.0182546 ]   [1.0312228  0.97151536]   [1.2854463   0.7278148 ]   [ 0.5899429   0.8331064 ]   [ 1.2772946   0.6413992 ]   [0.0771177   0.5956638 ]   [0.8686398   0.28362957]   [0.7941861   1.0958949 ]]] Chainer arcsin result: [[[ 0.62297034  0.05782205]   [ 0.13651273  1.0182546 ]   [1.0312228  0.9715154 ]   [1.2854464   0.7278148 ]   [ 0.5899428   0.83310646]   [ 1.2772946   0.6413992 ]   [0.0771177   0.5956638 ]   [0.8686398   0.28362957]   [0.7941861   1.095895  ]]] ```",Can you be more specific about what differences you're talking about? All I see here are two lists of numbers that look identical when I compare the first few byeye. What should I be looking for?,"> Can you be more specific about what differences you're talking about? All I see here are two lists of numbers that look identical when I compare the first few byeye. What should I be looking for? Thanks for the prompt response. The differences are subtle but noticeable around the 7th decimal place. For example: JAX arcsin result: [0.13651271, 1.0182546] Chainer arcsin result: [0.13651273, 1.0182545] As you can see, there's a slight difference in the 7th decimal place, which may seem small but can be significant for precisioncritical applications.","What is ""Chainer""?","I see – as in CC(Arcsinh Error Report ), it appears the results differ by 1ULP, which is expected for different floating point implementations. For clarity, I'm going to close this as a duplicate of CC(Arcsinh Error Report ), and we can continue discussing the issue there if you have further questions.","> I see – as in CC(Arcsinh Error Report ), it appears the results differ by 1ULP, which is expected for different floating point implementations. For clarity, I'm going to close this as a duplicate of CC(Arcsinh Error Report ), and we can continue discussing the issue there if you have further questions. Because chainer does not have its own native method, I use np. You can see the code example above.  JAX arcsin operation def jax_arcsin(x): return jnp.arcsin(jnp.array(x, dtype=np.float32))  Ensure the input is float32  Chainer arcsin operation def chainer_arcsin(x): return np.arcsin(x.astype(np.float32))  Ensure the input is float32","Thanks   I'd never heard of chainer before, but I found it through a web search."
yi,JaxStackTraceBeforeTransformation error with parametrized ODE," Description Hi everyone, based on this tutorial I tried to get started with Jax and neural ODEs: https://colab.research.google.com/drive/1ZlK36VgWy1vBjBNXjSUg6Cb7zeoa3jh However, I get the a JaxStackTraceBeforeTransformation error (detailed error message below). I boiled down the code to a small working example (also provided below) and noted the error only occors when the equation in test_func contains an argument.  Since this issue seemed similar to one raised in an earlier post (https://github.com/jaxml/jax/issues/13629) I tried downgrading jax to version 0.4.23. I also tried setting up a fresh python environment with only the necessary packages installed. Nothing helped, so far. I'd appreciate your help :) **Working example:** ``` from diffrax import diffeqsolve, ODETerm, SaveAt, Tsit5 import jax import jax.numpy as jnp def f(t, y, _):   dp_dt = 0.9 * y   return dp_dt b0 = 2   init condition data_ts = jnp.linspace(0, 20, 100) data_sol = diffeqsolve(ODETerm(f), Tsit5(), t0=0, t1=20, dt0=0.01,                        y0=(b0), saveat=SaveAt(ts=data_ts)) def fwd_test(coeff):     num_ts = 100     def test_func(t, y, _coeff):         dp_dt = y * _coeff doesn't work          dp_dt = y works         return dp_dt     b0 = 2     model_ts = jnp.linspace(0, 20, num_ts)      Note: larger dt0 so that it runs faster; this is about as large as it can go     model_sol = diffeqsolve(ODETerm(test_func), Tsit5(), t0=0, t1=20, dt0=0.5,                         y0=(b0), args=coeff, saveat=SaveAt(ts=model_ts))     model_b = model_sol.ys     data_b = data_sol.ys     return jnp.sum((model_b  data_b)**2) coeff = 1. grads = jax.grad(fwd_test)(coeff) ``` **Error message:** ``` JaxStackTraceBeforeTransformation         Traceback (most recent call last) File :198, in _run_module_as_main() File :88, in _run_code() File ~\AppData\Local\Programs\Python\Python311\Lib\sitepackages\ipykernel_launcher.py:18      16 from ipykernel import kernelapp as app > 18 app.launch_new_instance() File ~\AppData\Local\Programs\Python\Python311\Lib\sitepackages\traitlets\config\application.py:1075, in launch_instance()    1074 app.initialize(argv) > 1075 app.start() File ~\AppData\Local\Programs\Python\Python311\Lib\sitepackages\ipykernel\kernelapp.py:739, in start()     738 try: > 739     self.io_loop.start()     740 except KeyboardInterrupt: File ~\AppData\Local\Programs\Python\Python311\Lib\sitepackages\tornado\platform\asyncio.py:205, in start()     204 def start(self) > None: > 205     self.asyncio_loop.run_forever() File ~\AppData\Local\Programs\Python\Python311\Lib\asyncio\base_events.py:607, in run_forever()     606 while True: > 607     self._run_once()     608     if self._stopping: File ~\AppData\Local\Programs\Python\Python311\Lib\asyncio\base_events.py:1919, in _run_once()    1918     else: > 1919         handle._run()    1920 handle = None File ~\AppData\Local\Programs\Python\Python311\Lib\asyncio\events.py:80, in _run()      79 try: > 80     self._context.run(self._callback, *self._args)      81 except (SystemExit, KeyboardInterrupt): File ~\AppData\Local\Programs\Python\Python311\Lib\sitepackages\ipykernel\kernelbase.py:545, in dispatch_queue()     544 try: > 545     await self.process_one()     546 except Exception: File ~\AppData\Local\Programs\Python\Python311\Lib\sitepackages\ipykernel\kernelbase.py:534, in process_one()     533         return > 534 await dispatch(*args) File ~\AppData\Local\Programs\Python\Python311\Lib\sitepackages\ipykernel\kernelbase.py:437, in dispatch_shell()     436     if inspect.isawaitable(result): > 437         await result     438 except Exception: File ~\AppData\Local\Programs\Python\Python311\Lib\sitepackages\ipykernel\ipkernel.py:362, in execute_request()     361 self._associate_new_top_level_threads_with(parent_header) > 362 await super().execute_request(stream, ident, parent) File ~\AppData\Local\Programs\Python\Python311\Lib\sitepackages\ipykernel\kernelbase.py:778, in execute_request()     777 if inspect.isawaitable(reply_content): > 778     reply_content = await reply_content     780  Flush output before sending the reply. File ~\AppData\Local\Programs\Python\Python311\Lib\sitepackages\ipykernel\ipkernel.py:449, in do_execute()     448 if accepts_params[""cell_id""]: > 449     res = shell.run_cell(     450         code,     451         store_history=store_history,     452         silent=silent,     453         cell_id=cell_id,     454     )     455 else: File ~\AppData\Local\Programs\Python\Python311\Lib\sitepackages\ipykernel\zmqshell.py:549, in run_cell()     548 self._last_traceback = None > 549 return super().run_cell(*args, **kwargs) File ~\AppData\Local\Programs\Python\Python311\Lib\sitepackages\IPython\core\interactiveshell.py:3075, in run_cell()    3074 try: > 3075     result = self._run_cell(    3076         raw_cell, store_history, silent, shell_futures, cell_id    3077     )    3078 finally: File ~\AppData\Local\Programs\Python\Python311\Lib\sitepackages\IPython\core\interactiveshell.py:3130, in _run_cell()    3129 try: > 3130     result = runner(coro)    3131 except BaseException as e: File ~\AppData\Local\Programs\Python\Python311\Lib\sitepackages\IPython\core\async_helpers.py:129, in _pseudo_sync_runner()     128 try: > 129     coro.send(None)     130 except StopIteration as exc: File ~\AppData\Local\Programs\Python\Python311\Lib\sitepackages\IPython\core\interactiveshell.py:3334, in run_cell_async()    3331 interactivity = ""none"" if silent else self.ast_node_interactivity > 3334 has_raised = await self.run_ast_nodes(code_ast.body, cell_name,    3335        interactivity=interactivity, compiler=compiler, result=result)    3337 self.last_execution_succeeded = not has_raised File ~\AppData\Local\Programs\Python\Python311\Lib\sitepackages\IPython\core\interactiveshell.py:3517, in run_ast_nodes()    3516     asy = compare(code) > 3517 if await self.run_code(code, result, async_=asy):    3518     return True File ~\AppData\Local\Programs\Python\Python311\Lib\sitepackages\IPython\core\interactiveshell.py:3577, in run_code()    3576     else: > 3577         exec(code_obj, self.user_global_ns, self.user_ns)    3578 finally:    3579      Reset our crash handler in place Cell In[1], line 32      31 coeff = 1. > 32 grads = jax.grad(fwd_test)(coeff)      33  print(grads) Cell In[1], line 24, in fwd_test()      23  Note: larger dt0 so that it runs faster; this is about as large as it can go > 24 model_sol = diffeqsolve(ODETerm(test_func), Tsit5(), t0=0, t1=20, dt0=0.5,      25                     y0=(b0), args=coeff, saveat=SaveAt(ts=model_ts))      26 model_b = model_sol.ys File ~\AppData\Local\Programs\Python\Python311\Lib\sitepackages\diffrax\integrate.py:823, in diffeqsolve()     819      820  Main loop     821  > 823 final_state, aux_stats = adjoint.loop(     824     args=args,     825     terms=terms,     826     solver=solver,     827     stepsize_controller=stepsize_controller,     828     discrete_terminating_event=discrete_terminating_event,     829     saveat=saveat,     830     t0=t0,     831     t1=t1,     832     dt0=dt0,     833     max_steps=max_steps,     834     init_state=init_state,     835     throw=throw,     836     passed_solver_state=passed_solver_state,     837     passed_controller_state=passed_controller_state,     838 )     840      841  Finish up     842  File ~\AppData\Local\Programs\Python\Python311\Lib\sitepackages\diffrax\adjoint.py:286, in loop()     285     msg = None > 286 final_state = self._loop(     287     terms=terms,     288     saveat=saveat,     289     init_state=init_state,     290     max_steps=max_steps,     291     inner_while_loop=inner_while_loop,     292     outer_while_loop=outer_while_loop,     293     **kwargs,     294 )     295 if msg is not None: File ~\AppData\Local\Programs\Python\Python311\Lib\sitepackages\diffrax\integrate.py:429, in loop()     427 del filter_state > 429 final_state = outer_while_loop(     430     cond_fun, body_fun, init_state, max_steps=max_steps, buffers=_outer_buffers     431 )     433 def _save_t1(subsaveat, save_state): File ~\AppData\Local\Programs\Python\Python311\Lib\contextlib.py:81, in inner()      80 with self._recreate_cm(): > 81     return func(*args, **kwds) File ~\AppData\Local\Programs\Python\Python311\Lib\sitepackages\equinox\internal\_loop\loop.py:107, in while_loop()     106     del kind, base > 107     return checkpointed_while_loop(     108         cond_fun,     109         body_fun,     110         init_val,     111         max_steps=max_steps,     112         buffers=buffers,     113         checkpoints=checkpoints,     114     )     115 elif kind == ""bounded"": File ~\AppData\Local\Programs\Python\Python311\Lib\sitepackages\equinox\internal\_loop\checkpointed.py:247, in checkpointed_while_loop()     246 cond_fun_ = jtu.tree_map(_stop_gradient, cond_fun_) > 247 body_fun_ = filter_closure_convert(body_fun_, init_val_)     248 vjp_arg = (init_val_, body_fun_) File ~\AppData\Local\Programs\Python\Python311\Lib\sitepackages\equinox\internal\_loop\common.py:463, in new_body_fun()     462 buffer_val = _wrap_buffers(val, pred, tag) > 463 buffer_val2 = body_fun(buffer_val)     464  Needed to work with `disable_jit`, as then we lose the automatic     465  ArrayLike>Array cast provided by JAX's while loops.     466  The input `val` is already cast to Array below, so this matches that. File ~\AppData\Local\Programs\Python\Python311\Lib\sitepackages\diffrax\integrate.py:219, in body_fun()     214      215  Actually do some differential equation solving! Make numerical steps, adapt     216  step sizes, all that jazz.     217  > 219 (y, y_error, dense_info, solver_state, solver_result) = solver.step(     220     terms,     221     state.tprev,     222     state.tnext,     223     state.y,     224     args,     225     state.solver_state,     226     state.made_jump,     227 )     229  e.g. if someone has a sqrt(y) in the vector field, and dt0 is so large that     230  we get a negative value for y, and then get a NaN vector field. (And then     231  everything breaks.) See CC(Fix implementation of ndarray.astype method, add a test.). File ~\AppData\Local\Programs\Python\Python311\Lib\sitepackages\diffrax\solver\runge_kutta.py:1041, in step()    1035  Needs to be an `eqxi.while_loop` as:    1036  (a) we may have variable length: e.g. an FSAL explicit RK scheme will have one    1037      more stage on the first step.    1038  (b) to work around a limitation of JAX's autodiff being unable to express    1039      ""triangular computations"" (every stage depends on all previous stages)    1040      without spurious copies. > 1041 final_val = eqxi.while_loop(    1042     cond_stage,    1043     rk_stage,    1044     init_val,    1045     max_steps=num_stages,    1046     buffers=buffers,    1047     kind=""checkpointed"" if self.scan_kind is None else self.scan_kind,    1048     checkpoints=num_stages,    1049     base=num_stages,    1050 )    1051 _, y1, f1_for_fsal, _, _, fs, ks, result = final_val File ~\AppData\Local\Programs\Python\Python311\Lib\sitepackages\equinox\internal\_loop\loop.py:107, in while_loop()     106     del kind, base > 107     return checkpointed_while_loop(     108         cond_fun,     109         body_fun,     110         init_val,     111         max_steps=max_steps,     112         buffers=buffers,     113         checkpoints=checkpoints,     114     )     115 elif kind == ""bounded"": File ~\AppData\Local\Programs\Python\Python311\Lib\sitepackages\equinox\internal\_loop\checkpointed.py:252, in checkpointed_while_loop()     249 final_val_ = _checkpointed_while_loop(     250     vjp_arg, cond_fun_, checkpoints, buffers_, max_steps     251 ) > 252 _, _, _, final_val = _stop_gradient_on_unperturbed(init_val_, final_val_, body_fun_)     253 return final_val JaxStackTraceBeforeTransformation: TypeError: Custom JVP rule must produce primal and tangent outputs with corresponding shapes and dtypes, but got:   primal int32[] with tangent int32[], expecting tangent ShapedArray(float0[])   primal bool[] with tangent bool[], expecting tangent ShapedArray(float0[])   primal bool[] with tangent bool[], expecting tangent ShapedArray(float0[])   primal int32[] with tangent int32[], expecting tangent ShapedArray(float0[])   primal int32[] with tangent int32[], expecting tangent ShapedArray(float0[]) The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: TypeError                                 Traceback (most recent call last) Cell In[1], line 32      28     return jnp.sum((model_b  data_b)**2)      31 coeff = 1. > 32 grads = jax.grad(fwd_test)(coeff)      33  print(grads)     [... skipping hidden 10 frame] Cell In[1], line 24, in fwd_test(coeff)      22 model_ts = jnp.linspace(0, 20, num_ts)      23  Note: larger dt0 so that it runs faster; this is about as large as it can go > 24 model_sol = diffeqsolve(ODETerm(test_func), Tsit5(), t0=0, t1=20, dt0=0.5,      25                     y0=(b0), args=coeff, saveat=SaveAt(ts=model_ts))      26 model_b = model_sol.ys      27 data_b = data_sol.ys     [... skipping hidden 27 frame] File ~\AppData\Local\Programs\Python\Python311\Lib\sitepackages\equinox\internal\_loop\checkpointed.py:1272, in _stop_gradient_on_unperturbed_jvp(***failed resolving arguments***)    1268 del primals, tangents    1269 perturb_val, perturb_body_fun = jtu.tree_map(    1270     lambda _, t: t is not None, (init_val, body_fun), (t_init_val, t_body_fun)    1271 ) > 1272 perturb_val = _resolve_perturb_val(    1273     init_val, body_fun, perturb_val, perturb_body_fun    1274 )    1275 t_final_val = jtu.tree_map(    1276     _perturb_to_tang, t_final_val, perturb_val, is_leaf=_is_none    1277 )    1278 return final_val, t_final_val File ~\AppData\Local\Programs\Python\Python311\Lib\sitepackages\equinox\internal\_loop\checkpointed.py:1241, in _resolve_perturb_val(final_val, body_fun, perturb_final_val, perturb_body_fun)    1238         else:    1239             perturb_val = jtu.tree_map(operator.or_, perturb_val, new_perturb_val) > 1241 perturb_val = jax.eval_shape(_resolve_perturb_val_impl).value    1242 return perturb_val     [... skipping hidden 12 frame] File ~\AppData\Local\Programs\Python\Python311\Lib\sitepackages\equinox\internal\_loop\checkpointed.py:1214, in _resolve_perturb_val.._resolve_perturb_val_impl()    1211     return _out    1213  Not `jax.jvp`, so as not to error if `body_fun` has any `custom_vjp`s. > 1214 jax.linearize(_to_linearize, dynamic)    1215 if new_perturb_val is sentinel:    1216      `_dynamic_out` in `_to_linearize` had no JVP tracers at all, despite    1217      `_dynamic` having them. Presumably the user's `_body_fun` has no    1218      differentiable dependency whatsoever.    1219      This can happen if all the autograd is happening through    1220      `perturb_body_fun`.    1221     return Static(perturb_val)     [... skipping hidden 5 frame] File ~\AppData\Local\Programs\Python\Python311\Lib\sitepackages\equinox\internal\_loop\checkpointed.py:1207, in _resolve_perturb_val.._resolve_perturb_val_impl.._to_linearize(_dynamic)    1205 def _to_linearize(_dynamic):    1206     _body_fun, _val = combine(_dynamic, static) > 1207     _out = _body_fun(_val)    1208     _dynamic_out, _static_out = partition(_out, is_inexact_array)    1209     _dynamic_out = _record_symbolic_zeros(_dynamic_out)     [... skipping hidden 10 frame] File ~\AppData\Local\Programs\Python\Python311\Lib\sitepackages\jax\_src\custom_derivatives.py:351, in _flatten_jvp(primal_name, jvp_name, in_tree, maybe_out_type, *args)     344     msg = (""Custom JVP rule must produce primal and tangent outputs with ""     345            ""corresponding shapes and dtypes, but got:\n{}"")     346     disagreements = (     347         f""  primal {av_p.str_short()} with tangent {av_t.str_short()}, expecting tangent {av_et}""     348         for av_p, av_et, av_t in zip(primal_avals_out, expected_tangent_avals_out, tangent_avals_out)     349         if av_et != av_t) > 351     raise TypeError(msg.format('\n'.join(disagreements)))     352 yield primals_out + tangents_out, (out_tree, primal_avals) TypeError: Custom JVP rule must produce primal and tangent outputs with corresponding shapes and dtypes, but got:   primal int32[] with tangent int32[], expecting tangent ShapedArray(float0[])   primal bool[] with tangent bool[], expecting tangent ShapedArray(float0[])   primal bool[] with tangent bool[], expecting tangent ShapedArray(float0[])   primal int32[] with tangent int32[], expecting tangent ShapedArray(float0[])   primal int32[] with tangent int32[], expecting tangent ShapedArray(float0[]) ```  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.34 jaxlib: 0.4.34 numpy:  1.26.4 python: 3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Windows', release='10', version='10.0.19044', machine='AMD64') jupyterlab: 4.2.2 diffrax: 0.4.1",2024-10-11T16:59:43Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/24253,"The error reported here is actually a `TypeError` being raised because of an issue with the return types in a `jax.custom_jvp`. It's hard to see from this error report exactly which `custom_jvp` is the culprit, but it seems like it must be something within diffrax or equinox, so I'd recommend opening the issue on the https://github.com/patrickkidger/diffrax issue tracker.","ok, thanks for pointing this out. I'll try my luck there.",I'm going to close this since it looks like the conversations in https://github.com/patrickkidger/diffrax/issues/513 are getting to the bottom of things. Please let me know if there's something I'm missing!
yi,Add lax.FftType.,"Add lax.FftType. We had never provided a public name for the enum of FFT types; instead it was only known by a semiprivate name (jax.lib.xla_client.FftType). Add a public name (jax.lax.FftType) and deprecate the private one. We define a new FftType IntEnum rather than trying to expose the one in xla_client. The xla_client definition was useful when building classic HLO, but we no longer do that so there's no reason we need to couple our type to XLA's type.",2024-10-10T14:18:31Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24230
transformer,FlaxLlamaForCausalLMModule hanging on jax-metal," Description To reproduce the working state uncomment the device update to cpu ```python from transformers import AutoTokenizer import jax import jax.numpy as jnp from flax import linen as nn from flax.training import train_state  from llama import FlaxLLaMAForCausalLM   From the ayaka14732/llama2jax repo from transformers.models.llama.modeling_flax_llama import (     FlaxLlamaForCausalLMModule,     LlamaConfig, ) from transformers.models.llama.tokenization_llama import LlamaTokenizer  Download tokenizer and model  jax.config.update(""jax_platform_name"", ""cpu"") print(jax.devices()) tokenizer = LlamaTokenizer.from_pretrained(""openlmresearch/open_llama_3b_v2"") conf = LlamaConfig.from_pretrained(""openlmresearch/open_llama_3b_v2"") print(type(conf)) model = FlaxLlamaForCausalLMModule(conf) print(type(model)) input_prompt = ""The future of AI is"" input_ids = tokenizer(input_prompt, return_tensors=""jax"").input_ids rng = jax.random.PRNGKey(0) position_ids = jnp.broadcast_to(jnp.arange(input_ids.shape[1]), input_ids.shape) print(position_ids.device) params = model.init(     rng, input_ids, attention_mask=jnp.ones_like(input_ids), position_ids=position_ids )[""params""] model_output = model.apply(     {""params"": params},     input_ids,     attention_mask=jnp.ones_like(input_ids),     position_ids=position_ids, ) print(""Model output logits:"", model_output.logits) predicted_token_ids = jnp.argmax(model_output.logits, axis=1) predicted_text = tokenizer.decode(predicted_token_ids[0], skip_special_tokens=True) print(""Predicted text:"", predicted_text) ```  System info (python version, jaxlib version, accelerator, etc.) ```jax:    0.4.34 jaxlib: 0.4.34 numpy:  1.26.4 python: 3.11.0 (v3.11.0:deaf509e8f, Oct 24 2022, 14:43:23) [Clang 13.0.0 (clang1300.0.29.30)] jax.devices (1 total, 1 local): [METAL(id=0)] process_count: 1 platform: uname_result(system='Darwin', node='AlessandrosAir', release='23.4.0', version='Darwin Kernel Version 23.4.0: Fri Mar 15 00:19:22 PDT 2024; root:xnu10063.101.17~1/RELEASE_ARM64_T8112', machine='arm64')```",2024-10-09T23:12:15Z,bug,open,0,2,https://github.com/jax-ml/jax/issues/24221,"Hi   I tested the provided code with `JAXmetal` on a `Macbook Pro M1 Pro`. While there were no hanging issues, `model.init` and `model.apply` took longer than the CPU version. Please find the attached screenshots below: !image !image !image Thank you.",You're correct in that eventually it does run. However on Macbook Air M2 Sonoma 14.4.1 this took ~5 mins. Any insight on why it's so much slower on metal?
llm,Activate the FFI implementation of SVD on GPU.,"Activate the FFI implementation of SVD on GPU. Alongside activating this new implementation, this change adds a new `algorithm` parameter to `jax.lax.svd`. Previously the choice of algorithm was made based on heuristics in the lowering rule, but it probably also makes sense to expose an option for users to specify the algorithm explicitly because our heuristics are not very carefully optimized. This change updates the implementation of SVD in `lax` to use the FFI version which was added to jaxlib in https://github.com/jaxml/jax/pull/23794. This comes with a few benefits: 1. When running on a CUDA platform, the 64bit API will be used for the algorithm based on QR decomposition. (Note that it looks like the 64bit API isn't available on ROCm.) This addresses part of the feature request in https://github.com/jaxml/jax/issues/23413, although there's still work to do to port the rest of the GPU calls to the 64bit API. 2. This implementation supports shape polymorphism in all dimensions with some caveats. By default, we do use some heuristics to based on the matrix sizes to select the algorithm that is used, and the three different algorithms (QR, Jacobi, and batched Jacobi) have sufficiently different behavior (QR returns V^H, whereas Jacobi returns V; batched Jacobi doesn't support `full_matrices=False`) that I couldn't work out a simple way to push this logic into the kernel. If the symbolic constraints are not sufficient to concretely determine the heuristics, we always use the QR algorithm. But, I've also exposed the algorithm selection in the user API, so it's possible to bypass the heuristics and get consistent behavior alongside shape polymorphism if needed. Besides these core changes, I removed the forward compatibility checks from the CPU lowering, since we're well outside of the forward compatibility window now.",2024-10-09T14:29:59Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24211
yi,Inadequate memory consumption when using HSDP without gradient accumulation ," Description Hi, I'm training transformer model with Hybrid Sharded Data Parallelism. This setup is similar to FSDP/ZeRO3 where params allgathered for each layer's forward/backward pass and dropped afterwards. Although, instead of sharding both model params and optimizer state over all GPUs in the cluster, I shard model params only over subset of devices (usually within single node for the fast allgathers over NVLink) and shard optimizer state over all gpus (similar to FSDP/ZeRO1/2/3).  Basically, I have mesh (param_groups, model) and for each param tensor P of shape (X, Y) I shard param tensor with partition spec (model, None) and corresponding to this param P optimizer state P_o of the same shape (X, Y) with partition spec (model, param_groups).  When mesh (param_groups, model) size is: 1. (1, N_GPUs)  this is basically FSDP/ZeRO3. 2. (N, N_GPUs/ N), N > 1  HSDP. I'm also have a gradient accumulation implemented where we split input batch into chunks, calculate forward/backward pass independently and then sum their gradients. When using gradient accumulation with the factor of N (batch is splitted into N chucks and processes independently) and sequence lengths of S, peak memory usage must be equal setup with gradient accumulation with the factor of 2 * N and 2 * SEQ_LEN. This is because resulting input tensor is of shape [B / 2, 2 * S] has the same numel as tensor [B, S].  And this is completely true for the FSDP setup with mesh size (1, N_GPUs) for any gradient accumulation factor I've tested, peak memory usages are identical but when I'm trying to use HSDP, something weird happens. When I'm using gradient accumulation factor of N > 1, peak memory usage is totally expected BUT as soon as I set it to 1, peak memory usage greatly increases.  Here, I have a toy model with the mesh (2, 4), total batch size of 64 and 3 setups: 1. gradient accumulation factor = 1, seq_len = 512 2. gradient accumulation factor = 2, seq_len = 1024 3. gradient accumulation factor = 4, seq_len = 2048 Second and third setup consumes practically identical amount of memory (~50 GB on each GPU), while first sone consumes way more  61GB.  Here's HLOs of the first and second setups: compiled_train_fn_grad_accum=2.txt compiled_train_fn_grad_accum=1.txt  System info (python version, jaxlib version, accelerator, etc.) ``` Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import jax; jax.print_environment_info() jax:    0.4.33 jaxlib: 0.4.33 numpy:  1.24.3 python: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0] jax.devices (8 total, 8 local): [CudaDevice(id=0) CudaDevice(id=1) ... CudaDevice(id=6) CudaDevice(id=7)] process_count: 1 platform: uname_result(system='Linux', node='computeinstancee00xy41pgq1s49hjc5', release='5.15.0118generic', version=' CC(lax_linalg.triangular_solve gives RuntimeError for largeish inputs on CPU)Ubuntu SMP Fri Jul 5 09:28:59 UTC 2024', machine='x86_64') $ nvidiasmi Fri Oct  4 10:07:59 2024        ++  ++ ``` XLA issue: https://github.com/openxla/xla/issues/18090",2024-10-09T13:26:37Z,bug,open,1,0,https://github.com/jax-ml/jax/issues/24208
rag,[JAX] Add the function API of jax.experimental.colocated_python,"[JAX] Add the function API of jax.experimental.colocated_python This change adds an experimental API `jax.experimental.colocated_python`. The ultimate goal of this API is to provide a runtimeagnostic way to wrap a Python code that runs close to (or on) accelerator hosts. Multicontroller JAX can trivially achieve this colocated Python code execution today, while singlecontroller JAX needed its own solution for distributed Python code execution, which creates fragmentation of the user code for these two runtime architectures. `colocated_python` is an attempt to define a single device model and portable API to allow the user to write a single code once that can run on both runtime architectures. This change includes an implementation of the function API portion of `jax.experimental.colocated_python`. A (stateful) object API will be added separately. Also there will be a separate change that expresses serialized functions as an IFRT `CustomCallProgram`. It is currently in an early development stage. Please proceed with a caution when using the API.",2024-10-08T22:27:17Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24200
rag,Improve docs for jnp.average,Part of CC(Tracking issue: inline docstrings),2024-10-08T13:24:47Z,documentation pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/24189
rag,Support for CUDNN 9.0?,"I am using CUDNN 9.0 with CUDA 12.4 and I tried 2 things to make it work with Jax: 1. Pip install with `pip3 install upgrade ""jax[cuda12_local]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` which does not work given the jaxcudareleases only supports CUDNN 8.9 and 9.1. This worked for this environment a month ago. Now I see ``` Loaded runtime CuDNN library: 9.0.0 but source was compiled with: 9.1.1.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration. ... XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. ```  2. Build jaxlib myself with instructions which fails with: ``` Traceback (most recent call last):         File ""/root/.cache/bazel/_bazel_root/ac0a09c0a1602a90816292620faa8c49/external/tsl/third_party/gpus/cuda/hermetic/cuda_json_init_repository.bzl"", line 62, column 13, in _cuda_redist_json_impl                 fail( Error in fail: The supported CUDNN versions are [""8.6"", ""8.9.4.25"", ""8.9.6"", ""8.9.7.29"", ""9.1.1"", ""9.2.0"", ""9.2.1"", ""9.3.0"", ""9.4.0""]. Please provide a supported version in HERMETIC_CUDNN_VERSION environment variable or add JSON URL for CUDNN version=9.0.0. ``` Did something change? Any suggestions on how to fix this problem if I don't want to update my CUDNN runtime?",2024-10-08T02:32:52Z,enhancement,closed,0,4,https://github.com/jax-ml/jax/issues/24180,"Yeah `jax_cuda_releases` is a legacy thing. It will never be updated. The wheels are shipped on pypi these days. Is there a reason you cannot update to CUDNN 9.1? It should be very easy to do: `pip install nvidiacudnncuda12` will do it, even if you use a local installation of CUDA for everything else. Note we recommend installing CUDA and CUDNN using the pip wheels; doing so is considerably easier. It is probably possible to selfbuild a jaxlib with CUDNN 9.0 support, but you'd have to do that by adding another entry to the  the BUILD files for a 9.0 version. To do this you'd need to add that version here: https://github.com/google/tsl/blob/main/third_party/gpus/cuda/hermetic/cuda_redist_versions.bzl and then I think you would build jaxlib with GPU enabled with the following options enabled: `bazel_options=override_repository=tsl=/path/to/your/tsl/fork bazel_options=repo_env=HERMETIC_CUDNN_VERSION=""9.0.0""` (where 9.0.0 is the version you added). No promises, but that would probably work.","> Yeah `jax_cuda_releases` is a legacy thing. It will never be updated. The wheels are shipped on pypi these days. 🙏  will update the build for future jax > Is there a reason you cannot update to CUDNN 9.1? It should be very easy to do: `pip install nvidiacudnncuda12` will do it, even if you use a local installation of CUDA for everything else. Note we recommend installing CUDA and CUDNN using the pip wheels; doing so is considerably easier. We're shipping a container with other libraries that are tested with CUDNN 9.0, only jax is the outlier. > It is probably possible to selfbuild a jaxlib with CUDNN 9.0 support, but you'd have to do that by adding another entry to the the BUILD files for a 9.0 version. To do this you'd need to add that version here: https://github.com/google/tsl/blob/main/third_party/gpus/cuda/hermetic/cuda_redist_versions.bzl >  > and then I think you would build jaxlib with GPU enabled with the following options enabled: `bazel_options=override_repository=tsl=/path/to/your/tsl/fork bazel_options=repo_env=HERMETIC_CUDNN_VERSION=""9.0.0""` (where 9.0.0 is the version you added). >  > No promises, but that would probably work. Thanks! Will try that if all else fails.","> > Is there a reason you cannot update to CUDNN 9.1? It should be very easy to do: `pip install nvidiacudnncuda12` will do it, even if you use a local installation of CUDA for everything else. Note we recommend installing CUDA and CUDNN using the pip wheels; doing so is considerably easier. >  > We're shipping a container with other libraries that are tested with CUDNN 9.0, only jax is the outlier. I will note that cudnn promises backwards but not forwards compatibility: https://docs.nvidia.com/deeplearning/cudnn/latest/developer/forwardcompatibility.htmlcudnnapicompatibility So in principle, assuming you believe NVIDIA's promises to that effect (I guess you need your own testing to be sure), you can install 9.1 even for users that expect 9.0 and things should work.",Thank you for your help. Super helpful!
rag,Skip `test_ragged_copy_on_host` if `xla_extension_version` < 290,Skip `test_ragged_copy_on_host` if `xla_extension_version` < 290,2024-10-07T20:40:59Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24175
yi,"Remove reference to outfeed_receiver.pyi, which was deleted.","Remove reference to outfeed_receiver.pyi, which was deleted.",2024-10-07T15:18:30Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24164
rag,Revisit the symbolic shape limitation on threefry_2x32 in terms of jax.export,"Currently, the following snippet could not work as expected, raising `jax.random functions have limited support for shape polymorphism. In particular, the product of the known dimensions must be even.` ```python import jax, jax.export .jit def f(dummy):     key = jax.random.key(0)     return jax.random.normal(key, dummy.shape) dummy = jax.ShapeDtypeStruct(     shape=jax.export.symbolic_shape(""B""),     dtype=jax.numpy.float32 ) jax.export.export(f)(dummy) ``` However, according to git blame, this error was originally targeting `jax2tf`. I would like to know if this is still the case for StableHLO lowering. I understand that this involves certain degree of shape polymorphism, but I managed to express it in a way that workarounds such limitation (to some extent): ```python import jax, jax.numpy as jnp, jax.lax as lax, jax.export import numpy as np from jax.extend.random import threefry2x32_p B, C = jax.export.symbolic_shape(""B,C"", constraints=[""2*C == mod(17*B, 2) + 17*B""]) def threefry_2x32(keypair, count):   """"""Apply the Threefry 2x32 hash.   Args:     keypair: a pair of 32bit unsigned integers used for the key.     count: an array of dtype uint32 used for the counts.   Returns:     An array of dtype uint32 with the same shape as `count`.   """"""   key1, key2 = keypair   if not lax.dtype(key1) == lax.dtype(key2) == lax.dtype(count) == np.uint32:     msg = ""threefry_2x32 requires uint32 arguments, got {}""     raise TypeError(msg.format([lax.dtype(x) for x in [key1, key2, count]]))   odd_size = count.size % 2   x = jnp.concatenate([count, jnp.zeros(odd_size, dtype=jnp.uint32)])   x = x.reshape(2, C)   x = threefry2x32_p.bind(key1, key2, x[0], x[1])   out = jnp.concatenate(x).reshape(C * 2)   assert out.dtype == np.uint32   return lax.reshape(out[:C * 2  odd_size], count.shape) import jax._src.prng jax._src.prng.threefry_2x32 = threefry_2x32 ``` And test with ```python .jit def f(key, x, _dummy):     return jax.random.normal(key, x.shape) key = jax.random.PRNGKey(0) x = jax.ShapeDtypeStruct((B, 17), jnp.float32) dummy = jax.ShapeDtypeStruct((C,), jnp.float32) e = jax.export.export(f)(key, x, dummy) x = jnp.zeros((5, 17), jnp.float32) dummy = jnp.zeros(((x.size + x.size%2)//2,), jnp.float32) e.call(key, x, dummy) x = jnp.zeros((6, 17), jnp.float32) dummy = jnp.zeros(((x.size + x.size%2)//2,), jnp.float32) e.call(key, x, dummy) ``` The core complexity lies in that currently JAX's symbolic system could not recognize `mod(B, 2) + B` as an even number, so I introduced an extra dummy symbol `C` to workaround it. I believe that JAX may already have some sort of polymorphic shape assertion or override mechanism internally (e.g., leveraging the fact that `out` and `count` must have same shape, and temporarily cast the shape to some simple form for the computation in between), which could further simplify the code. Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example.",2024-10-05T16:50:18Z,enhancement,open,0,0,https://github.com/jax-ml/jax/issues/24144
yi,jaxlib source code releases,"I'm trying to build JAX from source but I noticed that there are no source code releases for jaxlib 0.4.33 or 0.4.34 on GitHub or PyPI. Are there plans to publish the source code for those releases, or should we instead use the JAX release tarballs? P.S. I'm packaging JAX in a package manager, I can't just use the PyPI wheels.",2024-10-05T07:47:46Z,enhancement,closed,0,1,https://github.com/jax-ml/jax/issues/24137,"Correct. We're now using the `jax` tag to build and release both. Since we now much more closely tie the versions of the two, it seemed redundant to have two tags. It's possible in the future we may still release `jax` (the Python bits) without releasing `jaxlib` (the C++ bits), but we will never release `jaxlib` without also releasing `jax`. It should also always be the case that whenever `jaxlib` is released, it is released from the same tag as `jax`. Hope that helps!"
yi,FFI example not working as given in documentation," Description I am trying to create a custom C extension for JAX and was trying out the example given in the documentation. But when I try to run the example, I get the following error: ``` jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/home/test/test.py"", line 54, in      np.testing.assert_allclose(rms_norm(x), rms_norm_ref(x), rtol=1e5)   File ""/home/test/test.py"", line 35, in rms_norm     return jex.ffi.ffi_call(   File ""/usr/local/lib/python3.10/distpackages/jax/_src/extend/ffi.py"", line 240, in ffi_call     results = ffi_call_p.bind(   File ""/usr/local/lib/python3.10/distpackages/jax/_src/core.py"", line 439, in bind     return self.bind_with_trace(find_top_trace(args), args, params)   File ""/usr/local/lib/python3.10/distpackages/jax/_src/core.py"", line 443, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/usr/local/lib/python3.10/distpackages/jax/_src/core.py"", line 949, in process_primitive     return primitive.impl(*tracers, **params)   File ""/usr/local/lib/python3.10/distpackages/jax/_src/dispatch.py"", line 88, in apply_primitive     outs = fun(*args) jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Wrong number of attributes: expected 1 but got 2 ```  System info (python version, jaxlib version, accelerator, etc.) An NVIDIA GPU may be present on this machine, but a CUDAenabled jaxlib is not installed. Falling back to cpu. jax:    0.4.33 jaxlib: 0.4.33 numpy:  2.1.1 python: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='027b41159048', release='6.8.040generic', version=' CC(Invalid proto descriptor for file ""tensorflow/compiler/xla/xla_data.proto"")~22.04.3Ubuntu SMP PREEMPT_DYNAMIC Tue Jul 30 17:30:19 UTC 2', machine='x86_64') $ nvidiasmi Fri Oct  4 18:19:24 2024        ++  ++",2024-10-04T18:23:13Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/24131,"Thanks for this report! That example is being run as part of CI so I expect there's something different in your setup. Can you share the exact C++ and Python code that you're running? Edited to add: You might also consider checking out this version of that example which includes all the packaging details, etc.: https://github.com/jaxml/jax/tree/main/examples/ffi","Thanks the example helps! I tried to copy the parts from the documentation in order. ```C++ // rms_norm.cc include  include  include  include ""xla/ffi/api/c_api.h"" include ""xla/ffi/api/ffi.h"" namespace ffi = xla::ffi; include  include  float ComputeRmsNorm(float eps, int64_t size, const float *x, float *y) {   float sm = 0.0f;   for (int64_t n = 0; n  std::pair GetDims(const ffi::Buffer &buffer) {   auto dims = buffer.dimensions();   if (dims.size() == 0) {     return std::make_pair(0, 0);   }   return std::make_pair(buffer.element_count(), dims.back()); } // A wrapper function providing the interface between the XLA FFI call and our // library function `ComputeRmsNorm` above. This function handles the batch // dimensions by calling `ComputeRmsNorm` within a loop. ffi::Error RmsNormImpl(float eps, ffi::Buffer x,                        ffi::Result> y) {   auto [totalSize, lastDim] = GetDims(x);   if (lastDim == 0) {     return ffi::Error(ffi::ErrorCode::kInvalidArgument,                       ""RmsNorm input must be an array"");   }   for (int64_t n = 0; n typed_data()[n]));   }   return ffi::Error::Success(); } // Wrap `RmsNormImpl` and specify the interface to XLA. If you need to declare // this handler in a header, you can use the `XLA_FFI_DECLASE_HANDLER_SYMBOL` // macro: `XLA_FFI_DECLASE_HANDLER_SYMBOL(RmsNorm)`. XLA_FFI_DEFINE_HANDLER_SYMBOL(     RmsNorm, RmsNormImpl,     ffi::Ffi::Bind()         .Attr(""eps"")         .Arg>()  // x         .Ret>()  // y ); ``` Was able to generate librms_norm.so and install it. ```python import jax import jax.numpy as jnp def rms_norm_ref(x, eps=1e5):   scale = jnp.sqrt(jnp.mean(jnp.square(x), axis=1, keepdims=True) + eps)   return x / scale import ctypes from pathlib import Path import jax.extend as jex path = next(Path(""ffi"").glob(""librms_norm*"")) rms_norm_lib = ctypes.cdll.LoadLibrary(path) jex.ffi.register_ffi_target(     ""rms_norm"", jex.ffi.pycapsule(rms_norm_lib.RmsNorm), platform=""cpu"") import numpy as np def rms_norm(x, eps=1e5):    We only implemented the `float32` version of this function, so we start by    checking the dtype. This check isn't strictly necessary because type    checking is also performed by the FFI when decoding input and output    buffers, but it can be useful to check types in Python to raise more    informative errors.   if x.dtype != jnp.float32:     raise ValueError(""Only the float32 dtype is implemented by rms_norm"")    In this case, the output of our FFI function is just a single array with the    same shape and dtype as the input. We discuss a case with a more interesting    output type below.   out_type = jax.ShapeDtypeStruct(x.shape, x.dtype)   return jex.ffi.ffi_call(      The target name must be the same string as we used to register the target      above in `register_custom_call_target`     ""rms_norm"",     out_type,     x,      Note that here we're use `numpy` (not `jax.numpy`) to specify a dtype for      the attribute `eps`. Our FFI function expects this to have the C++ `float`      type (which corresponds to numpy's `float32` type), and it must be a      static parameter (i.e. not a JAX array).     eps=np.float32(eps),      The `vmap_method` parameter controls this function's behavior under `vmap`      as discussed below.     vmap_method=""broadcast_fullrank"",   )  Test that this gives the same result as our reference implementation x = jnp.linspace(0.5, 0.5, 15).reshape((3, 5)) np.testing.assert_allclose(rms_norm(x), rms_norm_ref(x), rtol=1e5) ``` Hope this helps answer the issue. There are decorators on top of the example you provided so maybe the documentation has not been updated?","Oh I see what's happening here! The decorators are actually a red herring. The issue is actually the `vmap_method` parameter. That was only added in JAX v0.4.34, so in the earlier version that you're using the `vmap_method` input is being interpreted as an attribute that you want to pass to the FFI handler. So, in JAX v0.4.33, you should use `vectorized=True` instead of `vmap_method`, although that behavior is deprecated going forward. Hope this helps!",That was the issue. Thanks!
llm,Problems when customizing event-driven opertors with ``pallas``," Description Hello everyone, I am writing kernel functions using ``pallas``. The function I want to implement is the matrixvector multiplication, where the vector is the binary events (0/1 values): $$y = x @ M$$ where $x$ is the binary events (corresponding to the ``spikes`` argument in the following function), and $M$ is stored as the ELL sparse format (corresponding to the ``indices`` and ``weights`` arguments in the following function).  The code I wrote is shown below. The program logic is very simple: I use a thread block to process one row of ``indices`` and ``weights``. When there is an event (``spikes[i] = True``), the corresponding positions in $y$ will be `atomic_add` the weight values.  ```python import math from functools import partial import jax.random as jr import jax.experimental.pallas as pl import jax def ell_mv_kernel(sp_ref, ind_ref, weight_ref, y_ref):   def true_fn():     pl.atomic_add(y_ref, ind_ref[0, :], weight_ref[0, :])   jax.lax.cond(sp_ref[0], true_fn, lambda: None) (jax.jit, static_argnames=[""n_post"", 'block_size']) def ell_mv(spikes, indices, weights, *, n_post, block_size=128):   n_conn = weights.shape[1]   kernel = pl.pallas_call(     ell_mv_kernel,     out_shape=jax.ShapeDtypeStruct((n_post,), weights.dtype),     in_specs=[pl.BlockSpec((1,), lambda i, j: i),               pl.BlockSpec((1, block_size), lambda i, j: (i, j)),               pl.BlockSpec((1, block_size), lambda i, j: (i, j)), ],     out_specs=pl.BlockSpec((n_post,), lambda i, j: 0),     grid=(spikes.shape[0], math.ceil(n_conn / block_size)),      debug=True,      interpret=True   )   return kernel(spikes, indices, weights) def main(n_pre=100, n_post=200, p=0.1):   keys = jr.split(jr.PRNGKey(0), 3)   spikes = jr.uniform(keys[0], [n_pre]) < 0.5   indices = jr.randint(keys[1], [n_pre, int(n_post * p)], 0, n_post, )   weights = jr.normal(keys[2], [n_pre, int(n_post * p)])   y = ell_mv(spikes, indices, weights, n_post=n_post)   print(y)   print('mean = ', jax.numpy.mean(y))   print('var = ', jax.numpy.var(y))   print(weights[0, 0]) ``` I have encountered several bugs (or strange things): 1. When the ``n_pre`` and ``n_post``are small, for example ``n_pre=100, n_post=200``, the GPU is usually blocked for a long time without any computing. Or Jax outputs the error message: `E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1270] error deallocating host memory at 0x75e0a9a00000: INTERNAL: CUDA error: : CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered` 2. When I increase the size, the computed values are usually not right. All values are equal to ``nan``. I also have several questions: 1. How to use ``cudaMemset()`` for the output values in one ``pallas`` kernel before feeding the values into the kernel. The output values are ``nan`` are caused by the initial value of ``y`` is ``nan``. However, I have nowhere to zero $y$. Another way is initializing a zero vector into the kernel. But I found it is impossible in the XLA custom call style.  2. Is it possible to write such eventdriven kernels in ``pallas``, with massive usage of ``atomic_add`` operations?  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.33 jaxlib: 0.4.33 numpy:  1.26.4 python: 3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0] jax.devices (1 total, 1 local): [CudaDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='niplabubuntu220', release='6.8.040generic', version=' CC(Invalid proto descriptor for file ""tensorflow/compiler/xla/xla_data.proto"")~22.04.3Ubuntu SMP PREEMPT_DYNAMIC Tue Jul 30 17:30:19 UTC 2', machine='x86_64') $ nvidiasmi Fri Oct  4 20:09:36 2024        ++  ++ ```",2024-10-04T12:26:08Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/24117,"Another wrong behavior is related to ``atomic_add``. It seems that this operation does not calculate the correct result when two indices are the same within one thread block. Here is an example: ```python import jax import jax.experimental.pallas as pl import jax.numpy as jnp def kernel(i_ref, d_ref, _, out_ref):   pl.atomic_add(out_ref, i_ref[...], d_ref[...]) def add(indices):   return pl.pallas_call(     kernel,     grid=(1,),     out_shape=jax.ShapeDtypeStruct([4], jnp.float32),     interpret=True,     input_output_aliases={2: 0}   )(indices, jnp.ones(len(indices)), jnp.zeros(4)) > print(add(jnp.asarray([0, 1, 2]))) [1. 1. 1. 0.] > print(add(jnp.asarray([0, 1, 1]))) [1. 1. 0. 0.]   this is wrong, the expected result is [1. 2. 0. 0.] ```"
transformer,[Regression] Gradient explodes after upgrading to JAX 0.4.33 from 0.4.30," Description I'm training LLAMA3.1like transformer architecture in Hybrid Sharded Data ParallelContext Parallel setup on 32GPUs. Upgrading to JAX 0.4.33 has broken training of 70B model  loss becomes NaN after single training step. Evidences that I've collected so far: * Loss on the first step is exactly the same on 0.4.33 and 0.4.30  * Gradients of the unembedding layer and last layer norm are also exactly the same on the first step.  * Gradient already explodes for the last transformer layer's MLP hidden>output layer weight matrix, which I believe is the first layer after token_unembedding and last layer norm matrix.  * On JAX 0.4.30, 0.4.29 I've trained tens of such models with different hyperparams and datasets and have never seen any NaN. * For now, I wasn't able to reproduce this behavior on smaller models, but I'm working on it. * XLA dumps attached xla_dump_0_4_30.tar.gz xla_dump_0_4_33.tar.gz  System info (python version, jaxlib version, accelerator, etc.) ``` Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import jax; jax.print_environment_info() jax:    0.4.33 jaxlib: 0.4.33 numpy:  1.24.3 python: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0] jax.devices (8 total, 8 local): [CudaDevice(id=0) CudaDevice(id=1) ... CudaDevice(id=6) CudaDevice(id=7)] process_count: 1 platform: uname_result(system='Linux', node='computeinstancee00xy41pgq1s49hjc5', release='5.15.0118generic', version=' CC(lax_linalg.triangular_solve gives RuntimeError for largeish inputs on CPU)Ubuntu SMP Fri Jul 5 09:28:59 UTC 2024', machine='x86_64') $ nvidiasmi Fri Oct  4 10:07:59 2024        ++  ++ ``` XLA issue",2024-10-04T10:08:23Z,bug,closed,2,4,https://github.com/jax-ml/jax/issues/24114,"Thanks for the report! A good place to start with debugging this would be to try to narrow down a tighter window on when this change occurred. Would you be able to try bisecting over nightly releases between 0.4.30 and 0.4.33 to try and identify when this difference appeared? Once we have that, we can dig into the relevant changes around then to try to see what's happening!","Hi! Bisecting Jax version would be quite timeconsuming for me as I would have to rebuild my image for every run. I've also tried freshly release JAX 0.4.34  problem hasn't been solved there. Please also check xla issue, I've added tons of information and evidences there. Specifically, I've bisected specific model size at which something clearly breaks. https://github.com/openxla/xla/issues/17922issuecomment2394209837",We've figured our workaround for this problem  https://github.com/openxla/xla/issues/17922issuecomment2396280557,PR with this fix will be merged to XLA soon
yi,scale_and_translate with output_size and scale returning zero values for last row and column," Description Trying to replicate the behavior of pytorch upsample_bilinear2d using jax.image.scale_and_translate. This is as part of the pytorch xla project: https://github.com/pytorch/xla/issues/7389 We did some investigation as part of: https://github.com/pytorch/xla/pull/8208 The behavior between pytorch upsample_bilinear(align_corners=true, shape=something) and our implementation using jax scale_and_translate is different. Since jax does not have an implementation that supports `align_corners=true` we are implementing ourselves using existing functions. Existing bug in jax: https://github.com/jaxml/jax/issues/11206 Look here for the difference in output as well as a script to reproduce it: https://github.com/pytorch/xla/pull/8208issuecomment2390049360 when i change jax._src..image.scale.compute_weight_mat to return weights without zeroing, the output matches. Here is the script that copied the relevant jax code to investigate the behaviour: https://github.com/pytorch/xla/pull/8208issuecomment2391967612 We are not sure which behaviour is correct. All we know is how to make jax replicate pytorch's behavior. If you think the jax behaviour needs to be changed, i have a patch for that. Let me know your thoughts.   System info (python version, jaxlib version, accelerator, etc.) ``` % python Python 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import jax; jax.print_environment_info() jax:    0.4.29 jaxlib: 0.4.29 numpy:  1.26.4 python: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='*********.com', release='6.9.101rodete5amd64', version=' CC(Python 3 compatibility issues) SMP PREEMPT_DYNAMIC Debian 6.9.101rodete5 (20240904)', machine='x86_64') >>>  ```",2024-10-03T23:07:16Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/24106,"Thanks for your report! Related questions have come up a few times in the past. For example, this seems related to the comment here: https://github.com/jaxml/jax/issues/15768issuecomment1529939102 My understanding from that thread and your comments here is that to core issue is related to the handling of edge effects in the kernel. I'd say that we probably don't want to unilaterally change the behavior of this function in JAX because there are almost certainly users depending on the specifics of this implementation. That being said, it does sound like it would be useful to add an _option_ to change how the edge effects are handled. Would you be up for trying to implement something like that in your PR? Thanks!"
yi,Issues with vmap and GPU memory issues ," Description hello, I am trying to convert pytorch code to JAX related with an algorithm that performs hamiltonian sampling couple to a dynamic nested sampling algorithm. The goal is to vmap GPU parallelize the procedure but I am getting GPU memory saturation. The code is complex and I cinlude the three main functions where everything happens. the method is iterative due to the while loop in ""find_new_sample_batch"". It looks like the shape of some variable grows in each iteration but I am not able to find it.  The vectorisation is done  to paralelize a loop over a batch of points in 2D. In this case the batch size is 25 The trace is this: ``` File ""/r5/home/rruiz/projects/lisa/gradNS/dynamic.py"", line 91, in add_point_batch     newsample = self.find_new_sample_batch(min_logL, n_points=n_points, labels=labels)   File ""/r5/home/rruiz/projects/lisa/gradNS/hamiltonian.py"", line 1079, in find_new_sample_batch     new_x_active, new_loglike_active, out_frac, like_evals = self.hamiltonian_slice_sampling(x_ini[active], velocity[active], min_loglike, self.key, self.dt, self.max_reflections, self.min_reflections)   File ""/r5/home/rruiz/projects/lisa/gradNS/hamiltonian.py"", line 1020, in hamiltonian_slice_sampling     pos_out, logl_out, killed, like_evals = vmap(   File ""/r5/home/rruiz/projects/lisa/gradNS/hamiltonian.py"", line 999, in hamiltonian_slice_sampling_single     final_state = lax.while_loop(cond_fn, body_fn, initial_state)   jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 48828125000 bytes. BufferAssignment OOM Debugging. BufferAssignment stats:              parameter allocation:    1.82GiB               constant allocation:         0B         maybe_live_out allocation:   45.47GiB      preallocated temp allocation:         0B                  total allocation:   47.29GiB               total fragmentation:         0B (0.00%) Peak buffers: 	Buffer 1: 		Size: 45.47GiB 		Operator: op_name=""jit(broadcast_in_dim)/jit(main)/broadcast_in_dim[shape=(25, 25, 25, 25, 25, 25, 25) broadcast_dimensions=(1, 2, 3, 4, 5, 6)]"" source_file=""/r5/home/rruiz/projects/lisa/gradNS/hamiltonian.py"" source_line=999 		XLA Label: fusion 		Shape: s64[25,25,25,25,25,25,25] 		========================== 	Buffer 2: 		Size: 1.82GiB 		Entry Parameter Subshape: s64[25,25,25,25,25,25]    def hamiltonian_slice_sampling_single(             self,             position,             velocity,             min_like,             key,             dt,             max_reflections,             min_reflections         ):         """"""         Hamiltonian Slice Sampling algorithm for a single point in JAX.         """"""          Initialize variables         n_reflections = 0         num_steps = 0         max_trajectory_length = max_reflections * 10   To store trajectory points         killed = False         start_saving = False          Preallocate tensors to store results         pos_tensor = jnp.zeros((max_trajectory_length, position.shape[0]))         logl_tensor = jnp.zeros((max_trajectory_length,))         mask_tensor = jnp.zeros((max_trajectory_length,), dtype=bool)         memory = jnp.zeros((3,), dtype=bool)   To track reflections         x = position         current_key = key          Add like_evals to the loop state         like_evals = self.like_evals          Define a helper function to update memory without using jnp.roll         def update_memory(memory, outside):             """"""             Updates the memory by shifting left and appending the new 'outside' value.             """"""              Shift memory to the left by one             memory_shifted = memory[1:]              Append the new 'outside' value             memory_updated = jnp.concatenate([                 memory_shifted,                 jnp.array([outside], dtype=bool)             ])             return memory_updated          Define the condition function for the loop         def cond_fn(state):             (                 n_reflections,                 num_steps,                 x,                 velocity,                 pos_tensor,                 logl_tensor,                 mask_tensor,                 start_saving,                 killed,                 key,                 memory,                 like_evals   Track like_evals in the state             ) = state             return jnp.logical_and(                 n_reflections  0:                     key, subkey = random.split(key)                     r = random.normal(subkey, velocity.shape)                     r /= jnp.linalg.norm(r, axis=1, keepdims=True)   Normalize random perturbation                     velocity_reflected = jnp.where(~outside, velocity_reflected * (1 + self.sigma_vel * r), velocity_reflected)                  Increment reflections count                 n_reflections_updated = n_reflections + reflected                 n_reflections_updated = jnp.squeeze(n_reflections_updated)   Ensure scalar                  Determine if we should save the point                 scalar_pred = jnp.squeeze(n_reflections_updated > min_reflections)                 def process_save_point(_):                     start_saving = True                     step_idx = num_steps % max_trajectory_length                      Update pos_tensor at step_idx                     pos_tensor_updated = pos_tensor.at[step_idx].set(x)                      Update logl_tensor at step_idx with scalar p_x                     logl_tensor_updated = logl_tensor.at[step_idx].set(p_x)                      Update mask_tensor at step_idx                     mask_tensor_updated = mask_tensor.at[step_idx].set(~outside)                     return pos_tensor_updated, logl_tensor_updated, mask_tensor_updated, True                 def skip_save_point(_):                     return pos_tensor, logl_tensor, mask_tensor, start_saving                 pos_tensor_updated, logl_tensor_updated, mask_tensor_updated, start_saving_updated = lax.cond(                     scalar_pred,                     process_save_point,                     skip_save_point,                     operand=None                 )                 return (                     x.reshape(x.shape),                     velocity_reflected.reshape(velocity.shape),                     n_reflections_updated,                     pos_tensor_updated,                     logl_tensor_updated,                     mask_tensor_updated,                     start_saving_updated,                     False,   killed = False                     key                 )              Apply lax.cond to decide whether to return early (if killed) or continue processing             x, velocity, n_reflections, pos_tensor, logl_tensor, mask_tensor, start_saving, killed, key = lax.cond(                 killed,                 return_killed,                 continue_process,                 operand             )             return (                 n_reflections,                 num_steps,                 x,                 velocity,                 pos_tensor,                 logl_tensor,                 mask_tensor,                 start_saving,                 killed,                 key,                 memory,                 like_evals   Track like_evals in the loop state             )          Initialize the state of the loop         initial_state = (             n_reflections,             num_steps,             x,             velocity,             pos_tensor,             logl_tensor,             mask_tensor,             start_saving,             killed,             key,             memory,             like_evals   Start with the initial value of like_evals         )         with checking_leaks():          print('before while:loop', x.shape, velocity.shape)           Run the loop using `lax.while_loop`          final_state = lax.while_loop(cond_fn, body_fn, initial_state)          Unpack the final state         _, _, _, _, pos_tensor, logl_tensor, mask_tensor, _, _, key, memory, final_like_evals = final_state          Perform the JAXcompatible selection         final_position, final_logl, has_valid = self.select_final_sample(mask_tensor, pos_tensor, logl_tensor, key)          Optionally, print additional debug information         jax.debug.print(""Has valid index: {}"", has_valid)         jax.debug.print(""Final position: {}"", final_position)         jax.debug.print(""Final loglikelihood: {}"", final_logl)         return final_position, final_logl, has_valid, final_like_evals          def hamiltonian_slice_sampling(self, positions, velocities, min_like, key, dt, max_reflections, min_reflections):      keys = random.split(key, positions.shape[0])      jax.debug.print(""Keys shape after split: {}"", keys.shape)   Should be (batch_size, 2)      pos_out, logl_out, killed, like_evals = vmap(         self.hamiltonian_slice_sampling_single,          in_axes=(0, 0, None, 0, None, None, None)   Vectorize over positions, velocities, and keys      )(positions, velocities, min_like, keys, dt, max_reflections, min_reflections)       Remove references to intermediate tensors to allow garbage collection      del positions, velocities, keys       Trigger Python garbage collection to free unused memory      gc.collect()      out_frac = jnp.mean(killed)      return pos_out, logl_out, out_frac, like_evals     def find_new_sample_batch(self, min_loglike, n_points, labels=None):         """"""         Sample the prior until finding a sample with higher likelihood than a         given value         Parameters                  min_like : float         The threshold loglikelihood         labels : nlive_ini // 2 shape         Returns                  newsample : pd.DataFrame         A new sample         """"""         point = self.live_points.get_samples_from_labels(labels, key=self.key)         ini_labels = point.get_labels()         x_ini = point.get_values()   shape nlive_ini // 2         active = jnp.ones(x_ini.shape[0], dtype=jnp.bool_)         new_x = jnp.zeros_like(x_ini)         new_loglike = jnp.zeros(x_ini.shape[0])         accepted = False         count = 0         while not accepted:             count += 1             if count > 10:               print('finished', count)               sys.exit()             keys = random.split(self.key, x_ini.shape[0])   Split the key into `batch_size` subkeys             assert jnp.min(self.loglike(x_ini)) >= min_loglike, f""min_loglike = {min_loglike}, x_loglike = {self.loglike(x_ini)}""             assert jnp.all(self.is_in_prior(x_ini)), f""min_loglike = {min_loglike}, x_loglike = {self.loglike(x_ini)}""              Generate initial velocities             velocity = random.normal(self.key, x_ini.shape)             velocity /= jnp.linalg.norm(velocity, axis=1, keepdims=True)              Parallelize the slice sampling across all active points             vmapped_hamiltonian_sampling = jax.vmap(self.hamiltonian_slice_sampling, in_axes=(0, 0, 0, None), out_axes=(0, 0))             new_x_active, new_loglike_active, out_frac = vmapped_hamiltonian_sampling(x_ini[active], velocity[active], keys, min_loglike)              Instead of passing keys (list of keys), pass a single key.             new_x_active, new_loglike_active, out_frac, like_evals = self.hamiltonian_slice_sampling(x_ini[active], velocity[active], min_loglike, self.key, self.dt, self.max_reflections, self.min_reflections)             self.like_evals = like_evals             new_x_active, new_loglike_active, out_frac = self.hamiltonian_slice_sampling(position=x_ini[active], velocity=velocity[active], min_like=min_loglike)             new_x = new_x.at[active].set(new_x_active)             print('new_x', new_x, out_frac)             new_loglike = new_loglike.at[active].set(new_loglike_active)             if (out_frac > 0.15) and (jnp.sum(active) >= max(2, len(active) // 2)):                 self.dt = jnp.clip(self.dt * 0.9, 1e5, 10)                 if self.verbose:                     print(""Decreasing dt to "", self.dt, ""out_frac = "", out_frac, ""active = "", jnp.sum(active))                 active = jnp.ones(x_ini.shape[0], dtype=jnp.bool_)             elif (out_frac = max(2, len(active) // 2)):                 self.dt = jnp.clip(self.dt * 1.1, 1e5, 10)                 if self.verbose:                     print(""Increasing dt to "", self.dt, ""out_frac = "", out_frac, ""active = "", jnp.sum(active))                 active = jnp.ones(x_ini.shape[0], dtype=jnp.bool_)             else:                 in_prior = self.is_in_prior(new_x)                 active = (new_loglike  0:                     print(f""Active: {jnp.sum(active)} / {len(active)}"")             print(new_x, new_loglike, active, jnp.sum(active))                    sys.exit()             accepted = jnp.sum(active) == 0         sys.exit()         assert jnp.min(new_loglike) >= min_loglike, f""min_loglike = {min_loglike}, new_loglike = {new_loglike}""         sample = NSPoints(self.nparams)         sample.add_samples(values=new_x, logL=new_loglike, logweights=jnp.zeros(new_loglike.shape[0]), labels=ini_labels)         gc.collect()         return sample ```   Thanks !   best,  Roberto  System info (python version, jaxlib version, accelerator, etc.) python 3.9.19 jaxlib 0.4.28  jax.print_environment_info() jax:    0.4.28 jaxlib: 0.4.28 numpy:  1.26.4 python: 3.9.19 (main, May  6 2024, 19:43:03)  [GCC 11.2.0] jax.devices (1 total, 1 local): [cuda(id=0)] process_count: 1 platform: uname_result(system='Linux', node='som5.ific.uv.es', release='4.18.0373.el8.x86_64', version=' CC(Python 3 compatibility issues) SMP Tue Mar 22 15:11:47 UTC 2022', machine='x86_64') $ nvidiasmi Thu Oct  3 17:42:15 2024        ++  ++",2024-10-03T15:42:36Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/24099
rag,Simplify and consolidate dot algorithm control in lax.,"Simplify and consolidate dot algorithm control in lax. In https://github.com/jaxml/jax/pull/23574, we added a new `algorithm` parameter to `lax.dot_general` with the goal of giving users explicit control over the specific algorithm used to control dot product accumulation. When using this feature in real use cases, we have found that the API is both too conservative (it required the user to pass the appropriate input types) and too restrictive for common use cases. In this change, I simplify the API to bring it more in line with user expectations, and generalize it to support a broader range of use cases. The core change is to update the dot_general lowering rule to add explicit type casts to the inputs, making sure that they always have the appropriate storage types going into the `DotGeneral` StableHLO op. Before this change, some backends would implicitly cast for some algorithms (e.g. f32 > bf16), but error for others. It seems more user friendly to include automatic casts in all cases where a specific algorithm is requested. Another change in behavior is to (if needed) cast the result of the `DotGeneral` op (which is defined by the algorithm's `accumulation_type`) to match the input types. This means that, regardless of the algorithm choice, the output type will match the value that a user would expect from past use of `lax.dot_general`. The `preferred_element_type` parameter can now be used to control the output type, even when an algorithm is selected. To summarize, the updated version of `dot_general` accepts _any_ input dtypes, and the output will always match the inputs (under the existing promotion rules if the LHS and RHS don't match) unless `preferred_element_type` is used to select a specific output type. The specified ""algorithm"" is now more of an implementation detail, rather than the defining feature of the API, and JAX will do whatever it can to satisfy the user's request. (If an algorithm is not supported on the current device, we will still get a compile time error.) With the above changes in mind, it's no longer really necessary to have a `transpose_algorithm` parameter, because we can now use the same algorithm for the backwards pass. For users who need to customize the algorithm on the backwards pass, that is still possible using `custom_vjp`. Given the above changes,  made the excellent point that we don't really need the `algorithm` parameter anymore: just accept `DotAlgorithm` inputs to `precision`. I think this is a really nice suggestion, so I have updated the interface to implement this. One minor negative of this approach is that `preferred_element_type` isn't a great name for what that parameter does when it is used in conjunction with an algorithm. In the long run, I'd like to rename this parameter, but keeping it as is for now seems like the best short term approach.",2024-10-02T19:09:54Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24079
yi,[Pallas/MGPU] Allow delaying the release of pipelined buffers,[Pallas/MGPU] Allow delaying the release of pipelined buffers This is useful so that we don't have to block on the WGMMA immediately after it runs. `delay_release=n` means that the input/output buffers will not be mutated by the system for at least `n` sequential steps following the one when they were kernel arguments.,2024-10-02T10:06:51Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24068
yi,Edge case: Normal CDF incorrect when called with extremely small integer value if location is specified as integer array," Description Computing the CDF of a normal distribution gives the wrong result if the location is specified as an integer array, and CDF is called with the smallest legal int32 value. MWE: The following should all return `0.0`, as the CDF $F(x)$ of a Gaussian should approach 0 for $x \to \infty$ ```python print(dists.Normal(jnp.array([1]),0.01).cdf(jnp.astype(jnp.inf, int))) print(dists.Normal(jnp.array([1], dtype=int),0.01).cdf(2147483648)) print(dists.Normal(jnp.array([1], dtype=int),1.0).cdf(2147483648)) print(jnp.exp(dists.Normal(jnp.array([1], dtype=int),1.0).log_cdf(2147483648)))  < print(dists.Normal(jnp.array([1], dtype=float),0.01).cdf(2147483648)) print(dists.Normal(jnp.array([1]),0.01).cdf(2147483647)) print(dists.Normal(  1,0.01).cdf(2147483648)) print(dists.Normal(1.0,0.01).cdf(2147483648)) print(dists.Normal(jnp.array([1]),0.01).cdf(jnp.inf)) ``` But instead it yields: ``` [1.] [1.] [1.] [0.]  < [0.] [0.] 0.0 0.0 [0.] ``` Note that `log_cdf` seems to be OK. A workaround for me is to explicitly cast the location vector to float. This is a highly unlikely edgecase, but it did occur for me during a realworld problem (computing mutual information).  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.33 jaxlib: 0.4.33 numpy:  2.1.1 python: 3.12.5 (main, Aug 22 2024, 08:14:36) [Clang 15.0.0 (clang1500.1.0.2.5)] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Darwin', node='Beauty.local', release='24.0.0', version='Darwin Kernel Version 24.0.0: Mon Aug 12 20:52:12 PDT 2024; root:xnu11215.1.10~2/RELEASE_ARM64_T6020', machine='arm64') ```",2024-10-01T22:45:37Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/24059,Hi  thanks for the report! Can you clarify what `dists.Normal` is here? JAX doesn't have any API with that name.
yi,[Pallas TPU] Core dump when comparing two boolean arrays," Description ```python import functools import jax from jax.experimental import pallas as pl import jax.numpy as jnp .partial(     pl.pallas_call,     out_shape=jax.ShapeDtypeStruct((2,), jnp.bool),      interpret=True, ) def kernel(x_ref, y_ref, o_ref):     o_ref[...] = x_ref[...] == y_ref[...] def main():     x = jnp.array([True, True], dtype=jnp.bool)     y = jnp.array([False, False], dtype=jnp.bool)     out = kernel(x, y)     print(out) if __name__ == '__main__':     main() ``` Error: ``` https://symbolize.stripped_domain/r/?trace=7fbe9de8075b,7fbf4944251f,7fbe9dd8b8fd,7fbe9dd38ee7,7fbe9dd38e7d,7fbe9dd3798c,7fbe9dec8953,7fbe9dec818a,7fbe9d64981d,7fbe9ecc263b,7fbe9ed16b27,7fbe9ec2d81e,5794dc&map=  *** SIGSEGV (), see go/stacktracess15 received by PID 361853 (TID 361853) on cpu 131; stack trace: *** PC: @     0x7fbe9de8075b  (unknown)  mlir::ShapedType::getShape()     @     0x7fbe98eb5be1       1888  (unknown)     @     0x7fbf49442520  103807888  (unknown)     @     0x7fbe9dd8b8fe        352  mlir::arith::SelectOp::verifyInvariantsImpl()     @     0x7fbe9dd38ee8         32  mlir::op_definition_impl::verifyTraits()     @     0x7fbe9dd38e7e         32  mlir::Op::verifyInvariants()     @     0x7fbe9dd3798d         80  mlir::RegisteredOperationName::Model::verifyInvariants()     @     0x7fbe9dec8954        768  (anonymous namespace)::OperationVerifier::verifyOpAndDominance()     @     0x7fbe9dec818b         32  mlir::verify()     @     0x7fbe9d64981e         16  mlirOperationVerify     @     0x7fbe9ecc263c        192  mlir::python::PyOperationBase::verify()     @     0x7fbe9ed16b28         64  pybind11::cpp_function::initialize()::{lambda() CC(Python 3 compatibility issues)}::__invoke()     @     0x7fbe9ec2d81f        560  pybind11::cpp_function::dispatcher()     @           0x5794dd  (unknown)  (unknown)     @ ... and at least 1 more frames https://symbolize.stripped_domain/r/?trace=7fbe9de8075b,7fbe98eb5be0,7fbf4944251f,7fbe9dd8b8fd,7fbe9dd38ee7,7fbe9dd38e7d,7fbe9dd3798c,7fbe9dec8953,7fbe9dec818a,7fbe9d64981d,7fbe9ecc263b,7fbe9ed16b27,7fbe9ec2d81e,5794dc&map=  E0930 20:53:32.324661  361853 coredump_hook.cc:316] RAW: Remote crash data gathering hook invoked. E0930 20:53:32.324669  361853 coredump_hook.cc:355] RAW: Skipping coredump since rlimit was 0 at process start. E0930 20:53:32.324677  361853 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec. E0930 20:53:32.324681  361853 coredump_hook.cc:411] RAW: Sending fingerprint to remote end. E0930 20:53:32.324696  361853 coredump_hook.cc:420] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory E0930 20:53:32.324702  361853 coredump_hook.cc:472] RAW: Dumping core locally. E0930 20:53:32.508649  361853 process_state.cc:805] RAW: Raising signal 11 with default behavior [1]    361853 segmentation fault (core dumped)  python 2.py ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.34.dev20240924+85a466d73 jaxlib: 0.4.33 numpy:  2.1.0 python: 3.12.4 (main, Jun  8 2024, 18:29:57) [GCC 11.4.0] jax.devices (8 total, 8 local): [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0) TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0) ... TpuDevice(id=6, process_index=0, coords=(2,1,0), core_on_chip=0) TpuDevice(id=7, process_index=0, coords=(3,1,0), core_on_chip=0)] process_count: 1 platform: uname_result(system='Linux', node='t1vnab2ce832w0', release='5.19.01027gcp', version=' CC(Add support for `np.trace` )~22.04.1Ubuntu SMP Thu Jun 22 05:13:17 UTC 2023', machine='x86_64') ```",2024-09-30T20:54:59Z,bug pallas,closed,0,1,https://github.com/jax-ml/jax/issues/24030,It's better to debug after https://github.com/jaxml/jax/pull/24086
yi,[Pallas TPU] Core dump when using `jnp.remainder`," Description ```python import functools import jax from jax.experimental import pallas as pl import jax.numpy as jnp import numpy as np .partial(     pl.pallas_call,     out_shape=jax.ShapeDtypeStruct((2,), jnp.uint32),      interpret=True, ) def kernel(x_ref, y_ref, o_ref):     o_ref[...] = jnp.remainder(x_ref[...], y_ref[...]) def main():     x = jnp.array([3, 3], dtype=jnp.uint32)     y = jnp.array([4, 4], dtype=jnp.uint32)     out = kernel(x, y)     expected = jnp.remainder(x, y)     np.testing.assert_array_equal(out, expected) if __name__ == '__main__':     main() ``` Error: ``` https://symbolize.stripped_domain/r/?trace=7fae7188075b,7faf1d04251f,7fae7178b8fd,7fae71738ee7,7fae71738e7d,7fae7173798c,7fae718c8953,7fae718c818a,7fae7104981d,7fae726c263b,7fae72716b27,7fae7262d81e,5794dc&map=  *** SIGSEGV (), see go/stacktracess15 received by PID 328747 (TID 328747) on cpu 141; stack trace: *** PC: @     0x7fae7188075b  (unknown)  mlir::ShapedType::getShape()     @     0x7fae6c8b5be1       1888  (unknown)     @     0x7faf1d042520  (unknown)  (unknown)     @     0x7fae7178b8fe        352  mlir::arith::SelectOp::verifyInvariantsImpl()     @     0x7fae71738ee8         32  mlir::op_definition_impl::verifyTraits()     @     0x7fae71738e7e         32  mlir::Op::verifyInvariants()     @     0x7fae7173798d         80  mlir::RegisteredOperationName::Model::verifyInvariants()     @     0x7fae718c8954        768  (anonymous namespace)::OperationVerifier::verifyOpAndDominance()     @     0x7fae718c818b         32  mlir::verify()     @     0x7fae7104981e         16  mlirOperationVerify     @     0x7fae726c263c        192  mlir::python::PyOperationBase::verify()     @     0x7fae72716b28         64  pybind11::cpp_function::initialize()::{lambda() CC(Python 3 compatibility issues)}::__invoke()     @     0x7fae7262d81f        560  pybind11::cpp_function::dispatcher()     @           0x5794dd  (unknown)  (unknown)     @ ... and at least 1 more frames https://symbolize.stripped_domain/r/?trace=7fae7188075b,7fae6c8b5be0,7faf1d04251f,7fae7178b8fd,7fae71738ee7,7fae71738e7d,7fae7173798c,7fae718c8953,7fae718c818a,7fae7104981d,7fae726c263b,7fae72716b27,7fae7262d81e,5794dc&map=  E0930 19:14:22.485523  328747 coredump_hook.cc:316] RAW: Remote crash data gathering hook invoked. E0930 19:14:22.485533  328747 coredump_hook.cc:355] RAW: Skipping coredump since rlimit was 0 at process start. E0930 19:14:22.485541  328747 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec. E0930 19:14:22.485545  328747 coredump_hook.cc:411] RAW: Sending fingerprint to remote end. E0930 19:14:22.485564  328747 coredump_hook.cc:420] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory E0930 19:14:22.485570  328747 coredump_hook.cc:472] RAW: Dumping core locally. E0930 19:14:22.687357  328747 process_state.cc:805] RAW: Raising signal 11 with default behavior [1]    328747 segmentation fault (core dumped)  python 2.py ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.34.dev20240924+85a466d73 jaxlib: 0.4.33 numpy:  2.1.0 python: 3.12.4 (main, Jun  8 2024, 18:29:57) [GCC 11.4.0] jax.devices (8 total, 8 local): [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0) TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0) ... TpuDevice(id=6, process_index=0, coords=(2,1,0), core_on_chip=0) TpuDevice(id=7, process_index=0, coords=(3,1,0), core_on_chip=0)] process_count: 1 platform: uname_result(system='Linux', node='t1vnab2ce832w0', release='5.19.01027gcp', version=' CC(Add support for `np.trace` )~22.04.1Ubuntu SMP Thu Jun 22 05:13:17 UTC 2023', machine='x86_64') ```",2024-09-30T19:24:32Z,bug pallas,closed,0,3,https://github.com/jax-ml/jax/issues/24027,The underlying primitive is `lax.rem_p`,This is directly caused by https://github.com/jaxml/jax/issues/24030 because the implementation of `jnp.remainder()` involves comparing 2 boolean arrays.,Fixing https://github.com/jaxml/jax/issues/24030 is still not sufficient to fix this
yi,Bump fonttools from 4.51.0 to 4.54.1,"Bumps fonttools from 4.51.0 to 4.54.1.  Release notes Sourced from fonttools's releases.  4.54.1 What's Changed  [unicodedata] Update to Unicode 16 [subset] Escape \ in doc string  New Contributors  @​markthm made their first contribution in fonttools/fonttools CC(jnp.einsum does not catch shape mismatch when optimize=True)  Full Changelog: https://github.com/fonttools/fonttools/compare/4.54.0...4.54.1 4.54.0  [Docs] Small docs cleanups by @​n8willis (fonttools/fonttools CC(Reimplement argmin/argmax using a single pass variadic reduction.)) [Docs] cleanup code blocks by @​n8willis (fonttools/fonttools CC(improve error message for jnp.pad pad_width array)) [Docs] fix Sphinx builds by @​n8willis (fonttools/fonttools CC(未找到相关数据)) [merge] Minor fixes to documentation for merge by @​drj11 (fonttools/fonttools CC(Add cummax and cummin)) [subset] Small tweaks to pyftsubset documentation by @​RoelN (fonttools/fonttools CC(fix bug in categorical test, disable 3611 on tpu)) [Tests] Do not require fonttools command to be available by @​behdad (fonttools/fonttools CC(Unexpected overhead when using `grad(pmap)`)) [Tests] subset_test: add failing test to reproduce issue fonttools/fonttools CC(Cleanup: define type lists in test_util & use in several test files.) by @​anthrotype (fonttools/fonttools CC(How to select the jax  release version)) [ttLib] NameRecordVisitor: include whole sequence of character variants' UI labels, not just the first by @​anthrotype (fonttools/fonttools CC(public interface for partial evaluation )) [varLib.avar] Reconstruct mappings from binary by @​behdad (fonttools/fonttools CC(add source info to jaxpr typechecking messages)) [varLib.instancer] Fix visual artefacts with partial L2 instancing by @​Hoolean (fonttools/fonttools CC([jax2tf] Update the tfnightly version to 20200701)) [varLib.interpolatable] Support discrete axes in .designspace by @​behdad (fonttools/fonttools CC(lax.pad breaks for zerosized inputs)) [varLib.models] By default, assume OpenTypelike normalized space by @​behdad (fonttools/fonttools CC(Use precision=HIGHEST in expm repeated squaring))  4.53.1  [feaLib] Improve the sharing of inline chained lookups ( CC(jaxpr typechecking: custom rules and errors)) [otlLib] Correct the calculation of OS/2.usMaxContext with reversed chaining contextual single substitutions ( CC(Clarify docs on jax.lax.cond.)) [misc.visitor] Visitors search the inheritance chain of objects they are visiting ( CC([jax2tf] Add support for custom JVP/VJP))  4.53.0  [ttLib.removeOverlaps] Support CFF table to aid in downconverting CFF2 fonts (fonttools/fonttools CC(WIP: Print JAX arrays to full precision)) [avar] Fix crash when accessing notyetexisting attribute (fonttools/fonttools CC(Fix eigh JVP to ensure that both the primal and tangents of the eigen…)) [docs] Add buildMathTable to otlLib.builder documentation (fonttools/fonttools CC(Restrict .compress tests to arrays only.)) [feaLib] Allow UTF8 with BOM when reading features (fonttools/fonttools CC(Bump jaxlib version to 0.1.49 and update WORKSPACE)) [SVGPathPen] Revert rounding coordinates to two decimal places by default (fonttools/fonttools CC(Avoid direct type/dtype comparisons to fix NumPy 1.19 deprecation war…)) [varLib.instancer] Refix output filename decisionmaking  (fonttools/fonttools CC(Remove warning suppression for tuple and list arguments to reductions.), fonttools/fonttools CC(Add physical optimization example), fonttools/fonttools CC(Fix typos ""Pytrees"" page on readthedocs))  4.52.4  [varLib.cff] Restore and deprecate convertCFFtoCFF2 that was removed in 4.52.0 release as it is used by downstream projects ( CC(Repro lazy issue)).  4.52.3 Fixed a small syntax error in the reStructuredTextformatted NEWS.rst file which caused the upload to PyPI to fail for 4.52.2. No other code changes. 4.52.2  [varLib.interpolatable] Ensure that scipy/numpy output is JSONserializable ( CC(summary statistics of jaxpr equations),  CC(fix an issue with newer versions of pytype)) [housekeeping] Regenerate table lists, to fix pyinstaller packaging of the new VARC table ( CC(avoid valuebased error check in random.choice),  CC([testdocs branch] Update JAX quickstart, test conf.py for proper Markdown support)) [cffLib] Make CFFToCFF2 and CFF2ToCFF more robust ( CC(Add class wrapper for doubledouble arithmetic),  CC(Fix typos: np.bool > np.bool_))  4.52.1   ... (truncated)   Changelog Sourced from fonttools's changelog.  4.54.1 (released 20240924)  [unicodedata] Update to Unicode 16 [subset] Escape \\ in doc string  4.54.0 (released 20240923)  [Docs] Small docs cleanups by @​n8willis ( CC(Reimplement argmin/argmax using a single pass variadic reduction.)) [Docs] cleanup code blocks by @​n8willis ( CC(improve error message for jnp.pad pad_width array)) [Docs] fix Sphinx builds by @​n8willis ( CC(未找到相关数据)) [merge] Minor fixes to documentation for merge by @​drj11 ( CC(Add cummax and cummin)) [subset] Small tweaks to pyftsubset documentation by @​RoelN ( CC(fix bug in categorical test, disable 3611 on tpu)) [Tests] Do not require fonttools command to be available by @​behdad ( CC(Unexpected overhead when using `grad(pmap)`)) [Tests] subset_test: add failing test to reproduce issue  CC(Cleanup: define type lists in test_util & use in several test files.) by @​anthrotype ( CC(How to select the jax  release version)) [ttLib] NameRecordVisitor: include whole sequence of character variants' UI labels, not just the first by @​anthrotype ( CC(public interface for partial evaluation )) [varLib.avar] Reconstruct mappings from binary by @​behdad ( CC(add source info to jaxpr typechecking messages)) [varLib.instancer] Fix visual artefacts with partial L2 instancing by @​Hoolean ( CC([jax2tf] Update the tfnightly version to 20200701)) [varLib.interpolatable] Support discrete axes in .designspace by @​behdad ( CC(lax.pad breaks for zerosized inputs)) [varLib.models] By default, assume OpenTypelike normalized space by @​behdad ( CC(Use precision=HIGHEST in expm repeated squaring))  4.53.1 (released 20240705)  [feaLib] Improve the sharing of inline chained lookups ( CC(jaxpr typechecking: custom rules and errors)) [otlLib] Correct the calculation of OS/2.usMaxContext with reversed chaining contextual single substitutions ( CC(Clarify docs on jax.lax.cond.)) [misc.visitor] Visitors search the inheritance chain of objects they are visiting ( CC([jax2tf] Add support for custom JVP/VJP))  4.53.0 (released 20240531)  [ttLib.removeOverlaps] Support CFF table to aid in downconverting CFF2 fonts ( CC(WIP: Print JAX arrays to full precision)) [avar] Fix crash when accessing notyetexisting attribute ( CC(Fix eigh JVP to ensure that both the primal and tangents of the eigen…)) [docs] Add buildMathTable to otlLib.builder documentation ( CC(Restrict .compress tests to arrays only.)) [feaLib] Allow UTF8 with BOM when reading features ( CC(Bump jaxlib version to 0.1.49 and update WORKSPACE)) [SVGPathPen] Revert rounding coordinates to two decimal places by default ( CC(Avoid direct type/dtype comparisons to fix NumPy 1.19 deprecation war…)) [varLib.instancer] Refix output filename decisionmaking  ( CC(Remove warning suppression for tuple and list arguments to reductions.),  CC(Add physical optimization example),  CC(Fix typos ""Pytrees"" page on readthedocs))  4.52.4 (released 20240527)  [varLib.cff] Restore and deprecate convertCFFtoCFF2 that was removed in 4.52.0 release as it is used by downstream projects ( CC(Repro lazy issue)).  4.52.3 (released 20240527)  Fixed a small syntax error in the reStructuredTextformatted NEWS.rst file which caused the upload to PyPI to fail for 4.52.2. No other code changes.    ... (truncated)   Commits  112ace8 Release 4.54.1 7fe5eb0 Update Changelog 510153f Merge pull request  CC(scipy.ndimage functions?) from fonttools/dependabot/github_actions/pypa/ghact... 1223694 Bump pypa/ghactionpypipublish from 1.10.1 to 1.10.2 4cd0b0d Escape \ in subset docs 10a61ef Update to Unicode 16 6af41af Bump version: 4.54.0 → 4.54.1.dev0 5429a6c Release 4.54.0 cbc350d Update Changelog 63611d4 Merge pull request  CC(improve error message for jnp.pad pad_width array) from n8willis/docscleanup Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) You can trigger a rebase of this PR by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)  > **Note** > Automatic rebases have been disabled on this pull request as it has been open for over 30 days.",2024-09-30T17:08:20Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/24019,Superseded by CC(Bump fonttools from 4.51.0 to 4.55.0).
llm,Bump hypothesis from 6.102.4 to 6.112.2,"Bumps hypothesis from 6.102.4 to 6.112.2.  Commits  83c22d9 Bump hypothesispython version to 6.112.2 and update changelog 3c1350d Merge pull request  CC(rm padded env from masking.py) from HypothesisWorks/createpullrequest/patch 2958a45 Fix for cyclic exception context 75650b9 Merge pull request  CC(Use of memory profiler) from abeakkas/fixassumeexample 7fdad0b Update pinned dependencies 879225d Merge pull request  CC(make custom_jvp handle all tracers gracefully) from HypothesisWorks/createpullrequest/patch bd8e963 Update pinned dependencies 4d079f7 Merge pull request  CC(Add rademacher, maxwell, double_sided_maxwell and weibull_min to jax.random.) from ZacHD/updatedeps 1caa0f2 drop win py39 CI f81af9c Update pinned deps Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-09-30T17:06:44Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/24017,Superseded by CC(Bump hypothesis from 6.102.4 to 6.112.4).
yi,Mosaic failed to compile TPU kernel," Description I am trying to finetune Gemma 2 on TPU and got the following error: ``` Traceback (most recent call last):   File ""/usr/local/lib/python3.10/distpackages/jax/_src/compiler.py"", line 266, in backend_compile     return backend.compile(built_c, compile_options=options) jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Mosaic failed to compile TPU kernel: Unsupported input data type in matrix multiplication. at location: loc(""/dot_general""(callsite(""_splash_attention""(""/usr/local/lib/python3.10/distpackages/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_kernel.py"":2277:0) at callsite(""__call__""(""/usr/local/lib/python3.10/distpackages/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_kernel.py"":2312:0) at callsite(""wrap_flash_attention""(""/home/cantonesegemma2/maxtext/MaxText/layers/attentions.py"":352:0) at callsite(""tpu_flash_attention""(""/home/cantonesegemma2/maxtext/MaxText/layers/attentions.py"":358:0) at callsite(""_call_wrapped_method""(""/usr/local/lib/python3.10/distpackages/flax/linen/module.py"":1211:0) at callsite(""wrapped_module_method""(""/usr/local/lib/python3.10/distpackages/flax/linen/module.py"":694:0) at callsite(""apply_attention""(""/home/cantonesegemma2/maxtext/MaxText/layers/attentions.py"":234:0) at callsite(""_call_wrapped_method""(""/usr/local/lib/python3.10/distpackages/flax/linen/module.py"":1211:0) at callsite(""wrapped_module_method""(""/usr/local/lib/python3.10/distpackages/flax/linen/module.py"":694:0) at ""__call__""(""/home/cantonesegemma2/maxtext/MaxText/layers/attentions.py"":977:0)))))))))))) The MLIR operation involved:   %4183 = ""tpu.matmul""(%4179, %4181, %4182)  : (vector, vector, vector) > vector ... additional diagnostics were skipped. ```  System info (python version, jaxlib version, accelerator, etc.) Python 3.10.12 jaxlib 0.4.33 TPU v38",2024-09-28T09:43:28Z,bug pallas,open,0,3,https://github.com/jax-ml/jax/issues/23989,"These are the subsequent errors: ``` Traceback (most recent call last):   File ""/usr/local/lib/python3.10/distpackages/jax/_src/compiler.py"", line 266, in backend_compile     return backend.compile(built_c, compile_options=options) jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Mosaic failed to compile TPU kernel: Unsupported input data type in matrix multiplication. at location: loc(""/dot_general""(callsite(""_splash_attention""(""/usr/local/lib/python3.10/distpackages/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_kernel.py"":2277:0) at callsite(""__call__""(""/usr/local/lib/python3.10/distpackages/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_kernel.py"":2312:0) at callsite(""wrap_flash_attention""(""/home/cantonesegemma2/maxtext/MaxText/layers/attentions.py"":352:0) at callsite(""tpu_flash_attention""(""/home/cantonesegemma2/maxtext/MaxText/layers/attentions.py"":358:0) at callsite(""_call_wrapped_method""(""/usr/local/lib/python3.10/distpackages/flax/linen/module.py"":1211:0) at callsite(""wrapped_module_method""(""/usr/local/lib/python3.10/distpackages/flax/linen/module.py"":694:0) at callsite(""apply_attention""(""/home/cantonesegemma2/maxtext/MaxText/layers/attentions.py"":234:0) at callsite(""_call_wrapped_method""(""/usr/local/lib/python3.10/distpackages/flax/linen/module.py"":1211:0) at callsite(""wrapped_module_method""(""/usr/local/lib/python3.10/distpackages/flax/linen/module.py"":694:0) at ""__call__""(""/home/cantonesegemma2/maxtext/MaxText/layers/attentions.py"":977:0)))))))))))) The MLIR operation involved:   %4473 = ""tpu.matmul""(%4469, %4471, %4472)  : (vector, vector, vector) > vector ... additional diagnostics were skipped. Please report a bug at: https://github.com/google/jax/issues/new?assignees=apaszke The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/home/cantonesegemma2/maxtext/MaxText/train.py"", line 774, in      app.run(main)   File ""/usr/local/lib/python3.10/distpackages/absl/app.py"", line 308, in run     _run_main(main, args)   File ""/usr/local/lib/python3.10/distpackages/absl/app.py"", line 254, in _run_main     sys.exit(main(argv))   File ""/home/cantonesegemma2/maxtext/MaxText/train.py"", line 770, in main     train_loop(config)   File ""/home/cantonesegemma2/maxtext/MaxText/train.py"", line 665, in train_loop     state, metrics = p_train_step(state, example_batch, nextrng)   File ""/usr/local/lib/python3.10/distpackages/jax/_src/traceback_util.py"", line 180, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/jax/_src/pjit.py"", line 332, in cache_miss     outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked = _python_pjit_helper(   File ""/usr/local/lib/python3.10/distpackages/jax/_src/pjit.py"", line 190, in _python_pjit_helper     out_flat = pjit_p.bind(*args_flat, **p.params)   File ""/usr/local/lib/python3.10/distpackages/jax/_src/core.py"", line 2782, in bind     return self.bind_with_trace(top_trace, args, params)   File ""/usr/local/lib/python3.10/distpackages/jax/_src/core.py"", line 443, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/usr/local/lib/python3.10/distpackages/jax/_src/core.py"", line 949, in process_primitive     return primitive.impl(*tracers, **params)   File ""/usr/local/lib/python3.10/distpackages/jax/_src/pjit.py"", line 1739, in _pjit_call_impl     return xc._xla.pjit(   File ""/usr/local/lib/python3.10/distpackages/jax/_src/pjit.py"", line 1721, in call_impl_cache_miss     out_flat, compiled = _pjit_call_impl_python(   File ""/usr/local/lib/python3.10/distpackages/jax/_src/pjit.py"", line 1651, in _pjit_call_impl_python     ).compile(compile_options)   File ""/usr/local/lib/python3.10/distpackages/jax/_src/interpreters/pxla.py"", line 2313, in compile     executable = UnloadedMeshExecutable.from_hlo(   File ""/usr/local/lib/python3.10/distpackages/jax/_src/interpreters/pxla.py"", line 2827, in from_hlo     xla_executable = _cached_compilation(   File ""/usr/local/lib/python3.10/distpackages/jax/_src/interpreters/pxla.py"", line 2639, in _cached_compilation     xla_executable = compiler.compile_or_get_cached(   File ""/usr/local/lib/python3.10/distpackages/jax/_src/compiler.py"", line 426, in compile_or_get_cached     return _compile_and_write_cache(   File ""/usr/local/lib/python3.10/distpackages/jax/_src/compiler.py"", line 654, in _compile_and_write_cache     executable = backend_compile(   File ""/usr/local/lib/python3.10/distpackages/jax/_src/profiler.py"", line 333, in wrapper     return func(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/jax/_src/compiler.py"", line 271, in backend_compile     raise handler_result from e   File ""/home/cantonesegemma2/maxtext/MaxText/layers/attentions.py"", line 977, in __call__     prefill_unnormalized_output, prefill_exponentials_max, prefill_exponentials_sum = self.apply_attention(   File ""/home/cantonesegemma2/maxtext/MaxText/layers/attentions.py"", line 234, in apply_attention     return self.tpu_flash_attention(query, key, value, decoder_segment_ids, self.attn_logits_soft_cap), None, None   File ""/home/cantonesegemma2/maxtext/MaxText/layers/attentions.py"", line 358, in tpu_flash_attention     x = wrap_flash_attention(query, key, value, decoder_segment_ids)   File ""/home/cantonesegemma2/maxtext/MaxText/layers/attentions.py"", line 352, in wrap_flash_attention     return jax.vmap(splash_kernel)(query, key, value, segment_ids=decoder_segment_ids)   File ""/usr/local/lib/python3.10/distpackages/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_kernel.py"", line 2312, in __call__     return _splash_attention(   File ""/usr/local/lib/python3.10/distpackages/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_kernel.py"", line 2277, in _splash_attention     return _splash_attention_custom( jax._src.pallas.mosaic.error_handling.MosaicError: INTERNAL: Mosaic failed to compile TPU kernel: Unsupported input data type in matrix multiplication. The MLIR operation involved:   %4473 = ""tpu.matmul""(%4469, %4471, %4472)  : (vector, vector, vector) > vector ... additional diagnostics were skipped. ```","While Pallas demonstrates strong potential, certain limitations arise when deploying its operations on TPUv3 hardware. Initial observations suggest potential incompatibilities and accuracy discrepancies that require further investigation. Firstly, some Pallas operations may not yet be fully supported on TPUv3, leading to execution failures. Identifying these specific operations is crucial for either seeking alternative implementations or advocating for broader TPUv3 support. Secondly, the use of bfloat16 precision for QKV computations, while potentially efficient, might contribute to the observed numerical inaccuracies. Exploring the impact of switching to float32 precision is recommended, although it may not completely resolve the discrepancies.","This matmul is not supported on TPU V3 (bf16 X bf16 with float32 accumulation). You should instead try to cast the inputs up the float32 before the matmul. That being said, we're working on improving the error reporting in these cases since it will be much more clear if the error clearly stated that the operation was not supported on a specific hardware generation."
yi,[Pallas TPU] Unrelated 'maximum recursion depth exceeded' error when using `.astype(jnp.int64)`," Description `int64` is not supported on Pallas TPU. However, the error message is not very helpful: ```python import functools import jax from jax.experimental import pallas as pl import jax.numpy as jnp jax.config.update('jax_enable_x64', True) def main():     .partial(         pl.pallas_call,         out_shape=jax.ShapeDtypeStruct((1, 1), jnp.int64),     )     def kernel(o_ref):         x = (jnp.uint32(1) + jnp.uint32(2)).astype(jnp.int64)         o_ref[...] = x.reshape((1, 1))     out = kernel()     print(out) if __name__ == '__main__':     main() ``` Error: ``` ...   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 817, in jaxpr_subcomp     ans = lowering_ruleseqn.primitive,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 724, in f_lowered     out = jaxpr_subcomp(lowering_context, jaxpr, *consts, *args)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/jax/jax/_src/pallas/mosaic/lowering.py"", line 823, in jaxpr_subcomp     raise LoweringException( jax._src.pallas.mosaic.lowering.LoweringException: Exception while lowering eqn:   a:i64[] = convert_element_type[new_dtype=int64 weak_type=False] b With context:   LoweringRuleContext(lowering_context=LoweringContext(ir_context=.Context object at 0x7fb888e55af0>, grid_sizes=(), grid_names=None, mapped_dims=(), user_grid_indices=(), block_shapes=[None], name_stack=NameStack(stack=()), mesh_context=None, traceback_caches=TracebackCaches(traceback_cache={: loc(callsite(""main..kernel""(""/home/ayx/jax/2.py"":14:13) at callsite(""main""(""/home/ayx/jax/2.py"":17:10) at """"(""/home/ayx/jax/2.py"":21:4)))), : loc(callsite(""main..kernel""(""/home/ayx/jax/2.py"":14:12) at callsite(""main""(""/home/ayx/jax/2.py"":17:10) at """"(""/home/ayx/jax/2.py"":21:4))))}, location_cache={(, 82): loc(""main..kernel""(""/home/ayx/jax/2.py"":14:13)), (, 160): loc(""main""(""/home/ayx/jax/2.py"":17:10)), ( at 0x7fb9377342d0, file ""/home/ayx/jax/2.py"", line 1>, 124): loc(""""(""/home/ayx/jax/2.py"":21:4)), (, 142): loc(""main..kernel""(""/home/ayx/jax/2.py"":14:12))}, canonical_name_cache={'/home/ayx/jax/2.py': '/home/ayx/jax/2.py'}, is_user_file_cache={'/home/ayx/jax/jax/_src/source_info_util.py': False, '/home/ayx/jax/jax/_src/interpreters/partial_eval.py': False, '/home/ayx/jax/jax/_src/pjit.py': False, '/home/ayx/jax/jax/_src/core.py': False, '/home/ayx/jax/jax/_src/traceback_util.py': False, '/home/ayx/jax/jax/_src/numpy/ufunc_api.py': False, '/home/ayx/jax/jax/_src/numpy/array_methods.py': False, '/home/ayx/jax/2.py': True, '/home/ayx/jax/jax/_src/linear_util.py': False, '/home/ayx/jax/jax/_src/profiler.py': False, '/home/ayx/jax/jax/_src/pallas/pallas_call.py': False, '/home/ayx/jax/jax/_src/lax/lax.py': False, '/home/ayx/jax/jax/_src/numpy/lax_numpy.py': False}), for_verification=False), avals_in=[ShapedArray(uint32[])], avals_out=[ShapedArray(int64[])], block_shapes=[None]) With inval shapes=[None] With inval types=[IntegerType(i32)] In jaxpr: { lambda ; a:u32[]. let     b:i64[] = convert_element_type[new_dtype=int64 weak_type=False] a   in (b,) } Exception: maximum recursion depth exceeded  For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.34.dev20240924+85a466d73 jaxlib: 0.4.33 numpy:  2.1.0 python: 3.12.4 (main, Jun  8 2024, 18:29:57) [GCC 11.4.0] jax.devices (8 total, 8 local): [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0) TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0) ... TpuDevice(id=6, process_index=0, coords=(2,1,0), core_on_chip=0) TpuDevice(id=7, process_index=0, coords=(3,1,0), core_on_chip=0)] process_count: 1 platform: uname_result(system='Linux', node='t1vnab2ce832w0', release='5.19.01027gcp', version=' CC(Add support for `np.trace` )~22.04.1Ubuntu SMP Thu Jun 22 05:13:17 UTC 2023', machine='x86_64') ```",2024-09-28T07:46:54Z,bug pallas,closed,0,0,https://github.com/jax-ml/jax/issues/23988
yi,jnp.mask_indices: add docs & tests,This is a strange function to be honest... but somehow we made it this far without ever having tests for it (yikes!) Part of CC(Tracking issue: inline docstrings) ,2024-09-27T20:59:24Z,documentation pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/23984
yi,[jax.distributed] Enable grpc channel compression,"Allows passing an additional boolean argument `use_compression` via `jax.distributed.initialize(...)` that controls whether compression is enabled on the gRPC channels created for each distributed runtime client. Motivation: XLA sends O(mesh) device topologies through its centralized coordination service and we have reason to believe that this becomes a bottleneck at large scale. Compression of the underlying gRPC communication is currently implicitly disabled, and might give us a cheap avenue to scale a bit further with the centralized KV store design. Verified to work via ``` GRPC_VERBOSITY=debug GRPC_TRACE=compression,channel ./some_jax_distributed_example.py |& grep i 'Compressed' I0927 09:25:34.942738680 3640719 message_compress_filter.cc:266] Compressed[gzip] 199 bytes vs. 155 bytes (22.11% savings) I0927 09:25:35.021493311 3640719 message_compress_filter.cc:266] Compressed[gzip] 225 bytes vs. 155 bytes (31.11% savings) I0927 09:25:36.482614284 3640719 message_compress_filter.cc:266] Compressed[gzip] 180 bytes vs. 143 bytes (20.56% savings) ``` Corresponding XLA PR: https://github.com/openxla/xla/pull/17704",2024-09-27T10:01:29Z,pull ready,closed,0,6,https://github.com/jax-ml/jax/issues/23969," Sgtm, made the change. I saw that mypy failed in the previouslyapproved build, not sure why, `precommit run all` locally passes for me.","Hmm, mypy (only on the CI?) complains that it doesn't know about the `use_compression` parameter on `get_distributed_runtime_client` (https://github.com/jaxml/jax/actions/runs/11615448842/job/32353776278?pr=23969step:4:123), but I don't see why  the corresponding `.pyi` in XLA was updated in https://github.com/openxla/xla/pull/17704.",My best guess is that this is an issue with the precommit cache on CI (https://github.com/jaxml/jax/pull/24543). I expect that it's still using a cached version of jaxlib 0.4.34. Let me see if getting https://github.com/jaxml/jax/pull/24543 in fixes the issue!, Let's try again now that https://github.com/jaxml/jax/pull/24543 is in?,Thanks for the ping! Can you rebase onto the `main` branch then I'll trigger the tests? Thanks!,Done!
rag,Add .pylintrc file,"The JAX project utilizes nonstandard PEP 8 settings for indentation. To validate Python files, we can leverage a .pylintrc configuration file. For reference, the TensorFlow project has an existing .pylintrc file that outlines its settings. This PR introduces a .pylintrc file for the JAX project, modeled after the configuration used in TensorFlow. ``` [FORMAT]  Maximum number of characters on a single line. maxlinelength=80  String used as indentation unit. This is usually "" "" (4 spaces) or ""\t"" (1  tab). indentstring='  ' ``` Usage Example: ```  check files in HEAD commit git diff nameonly HEAD^ HEAD | xargs pylint ```",2024-09-26T22:18:53Z,,closed,0,3,https://github.com/jax-ml/jax/issues/23959,"Hi  thanks for the PR. We already have pylint configuration in `pyproject.toml`, and I'd prefer to keep all such configs in a single location","Also, a while ago we moved off pylint and now use `ruff` for formatting. It's set up as a precommit hook, though we only enable a few checks.",Got it. Thank you for the info!
yi,Add support for passing array attributes via ffi_call and an example demonstrating the different attribute APIs supported by the FFI,"The XLA FFI supports a rich interface for specifying ""attributes"", but this interface hasn't been widely used from the JAX side. We discovered that passing array attributes wasn't supported by the current front end, so I added support for that and took advantage of this change to add a more complete example of the e2e usage of attributes. The core JAX change that was needed was to add support for passing `np.array` objects (which aren't hashable!) as keyword arguments to the `ffi_call` primitive. To do this, I wrap any unhashable keyword arguments in the `_HashableByObjectId` helper which creates a hashable object based on the `id`. ""Why not pass these unhashable objects as positional arguments?"" you ask. Well, the problem is that we need access to the concrete value of this array _when lowering_, so we can't have them turning into tracers. I don't expect this to introduce any real caching issues, but if anyone has concerns or other suggestions for a better API, I'd love to hear them!",2024-09-26T19:22:22Z,pull ready,closed,0,6,https://github.com/jax-ml/jax/issues/23951,"> I don't expect this to introduce any real caching issues, but if anyone has concerns or other suggestions for a better API, I'd love to hear them! Just to clarify, does this defeat the hashing at the JAX level and would any ffi_call using arrays or dicts hence be uncacheable at the JAX level (but presumably cached fine at the MLIR level)? If I recall correctly, there's a caching mechanism for jitted functions both within JAX itself (on jaxprs?) and another on the emitted StableHLO.",": Great question  thanks for bringing this up! This is more or less the question I was asking, but I think the specific points you bring up are not a concern here. Let me see if I can lay out my thinking here!  JIT cache This change doesn't affect which kinds of arguments can be treated as static arguments to `jax.jit`, and the cache is based on the types of the input arguments, not the Jaxpr. This means that this change doesn't affect cache hits when using `jax.jit`. For example, in the example included with this PR, we call FFI call as follows: ```python def fun(num: int):   return jex.ffi.ffi_call(       ""array_attr"",       jax.ShapeDtypeStruct((), np.int32),       array=np.arange(num, dtype=np.int32),   ) ``` and it's totally fine to `jit` this function: ```python jitted_fun = jax.jit(fun, static_argnums=(0,)) ``` and this will cache as expected: ```python jitted_fun(5)   traces here jitted_fun(5)   cache hit jitted_fun(3)   traces again ``` I note that it's still not possible to pass the `np.array` as a static argument to JIT, and that's disallowed for good reason. If that's the behavior that a user needs: they're going to have to find another way, but that has nothing to do with this change!  Persistent cache Another cache that we might be concerned about is the persistent compilation cache. This one also isn't a problem because that cache is keyed on the lowered HLO, which doesn't include any references to the object IDs. For example, if we lower the example above: ```python print(jitted_fun.lower(5).as_text()) ``` the HLO is: ``` module  attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {   func.func public () > (tensor {jax.result_info = """", mhlo.layout_mode = ""default""}) {     %0 = stablehlo.custom_call () {backend_config = """", mhlo.backend_config = {array = array}, operand_layouts = [], result_layouts = [dense : tensor]} : () > tensor     return %0 : tensor   } } ``` where the array has already been materialized as: ``` array ``` Now, things get a little more complicated if our array has an inexact type (e.g. `float32`). In that case, I could imagine the we could end up with unexpected cache misses depending on numerics, but this is also true for any floating point scalar inputs, which are hashable and unaffected by this change.  XLA caches There may be questions to ask about how XLA handles caching with array or dict attributes in the HLO, but that's separate from this PR, which just exposes an API for accessing an existing XLA feature. The JAXspecific hashing hacks are stripped before we ever get to XLA.  Other caches in JAX core? LRU caches are used throughout JAX core, and I could certainly imagine this complicating logic _somewhere_, but I haven't thought of anywhere where it would!  Stepping back I think it's useful to step back and comment a bit more on the motivation for this change. The `ffi_call` function is really just a simple shim to let users define a lowering rule without having to think about MLIR. (The `ffi_call` primitive doesn't really do anything else besides lowering!) If we weren't using `ffi_call`, we could write our own primitive that has array attributes roughly as follows: ```python def my_ffi_call_lowering(ctx, *operands, array_size: int):   return jex.ffi.ffi_lowering(""array_attr"")(ctx, *operands, array=np.arange(array_size)) my_ffi_call_p = core.Primitive()  impl, abstract_eval, etc... mlir.register_lowering(my_ffi_call_p, my_ffi_call_lowering, platform=...) def fun(*args, array_size: int):   return my_ffi_call_p.bind(..., array_size=array_size) ``` and everything works! The problem comes from the fact that `ffi_call` can't really have any runtime logic in the lowering rule, so we need to have a way to pass the concrete array through from the binding to the lowering. It's important to make sure we can support such a use case (it was a feature request!), and I think this is a reasonable approach!"," Thanks for the elaborate answer, makes sense that this can't affect the caching at the JIT boundary! Do you have a sense of why the values actually need to be hashable? Ultimately `HashableById` makes equality among two wrapped values inexact by erring on the side of ""not equal"". That's a safe choice in the context of caching, I just couldn't easily gather how the hash and equality come into play here.","> Do you have a sense of why the values actually need to be hashable? You know... this is a great question! And in trying to answer it, I realized that there was a _much_ simpler approach here, which doesn't have these same caveats. I should have figured this out earlier! The hashability requirement was coming from the fact that I was using `dispatch.apply_primitive` as the `impl` rule for the `ffi_call_p` primitive, and that function caches the jitted call to the primitive. Having worked that out, I've updated the `ffi_call_p` `impl` rule to use a version of the logic from `dispatch.apply_primitive`, but removing that caching. This will have performance implications if the same `ffi_call` is executed several times in eager mode (because the primitive will be relowered each time), but this doesn't affect the behavior of `ffi_call` under JIT. I'm much happier with this approach! Thanks  for asking the straightforward question that I should have asked myself in the first place 🤣 ","Great, thanks for digging into this! If that's the only path in which we rely on hashing the attribute, then we're just trading space (extra entries in the `xla_primitive_callable` cache) for time. Based on that I think your original proposal using `_HashableByObjectId` is good as well. I have no idea how much time caching `xla_primitive_callable` actually saves, but if we think it matters, we might want to err on the side of not regressing existing (cached) `ffi_call`s?","It's a bit nasty, but  convinced me that it's probably better to just hash by value (rather than id), since these arguments are small (they're going to end up in the HLO anyways). We special case to handle `np.array` and `dict` arguments, and we could add other cases in the future if necessary."
yi,checkify doesn't work with shard_map output array," Description To clarify: I don't mean using `checkify.check` inside a `shard_map` function, but using the output of a `shard_map` function and `checkify.check` in the same function and then applying `checkify.checkify` to it. Example: ```python import os from jax import Array from jax.experimental.shard_map import shard_map from jax.sharding import Mesh, PartitionSpec from jax.experimental import mesh_utils, checkify os.environ['XLA_FLAGS'] = f'xla_force_host_platform_device_count=4' import jax import jax.numpy as jnp devices = mesh_utils.create_device_mesh((4,)) mesh = Mesh(devices, axis_names=('a',)) sharding = jax.sharding.NamedSharding(mesh, PartitionSpec('a')) def some_function():     a = jnp.zeros(1000)     return a some_function_shard_map = shard_map(     some_function,     mesh=mesh,     in_specs=PartitionSpec(None),     out_specs=PartitionSpec(""a"") ) def main_function():     something = some_function_shard_map()     checkify.check(jnp.all(something == 0), ""failed"")     return something main_function = checkify.checkify(main_function) main_function = jax.jit(main_function)   same result without this line error: checkify.Error error, out = main_function() error.throw() ``` output: ```text Traceback (most recent call last):   File "".../checkify_test_2.py"", line 43, in      error, out = main_function()   File "".../checkify_test_2.py"", line 33, in main_function     something = some_function_shard_map() jax._src.source_info_util.JaxStackTraceBeforeTransformation: TypeError: ShardMapPrimitive.bind() got an unexpected keyword argument 'out_names' The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File "".../checkify_test_2.py"", line 43, in      error, out = main_function()                  ^^^^^^^^^^^^^^^ TypeError: ShardMapPrimitive.bind() got an unexpected keyword argument 'out_names' ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.33 jaxlib: 0.4.33 numpy:  2.0.1 python: 3.12.6 (main, Sep  7 2024, 14:20:15) [GCC 14.2.0] jax.devices (4 total, 4 local): [CpuDevice(id=0) CpuDevice(id=1) CpuDevice(id=2) CpuDevice(id=3)] process_count: 1 platform: uname_result(system='Linux', node='standpc', release='6.10.9amd64', version=' CC(Python 3 compatibility issues) SMP PREEMPT_DYNAMIC Debian 6.10.91 (20240908)', machine='x86_64') ```",2024-09-26T14:58:28Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/23946,"If you use JAX from HEAD, then it should work!","Indeed, using jax(lib)0.4.34.dev20240926 this seems to work as expected","Sweet, closing this issue. Feel free to reopen! (we should release 0.4.34 soon enough)"
yi,[pallas] Add support for the new algorithm option to dot,"`jax.lax.dot_general` has a new `algorithm` option for precisely specifying the precision, which should be supported in Pallas. Also support the simulated precision modes on GPU, such as `BF16_BF16_F32_x6` and `TF32_TF32_F32_x3` (already supported by `triton.language.dot` via the `input_precision=""tf32x3""` option). , ,  ",2024-09-25T21:29:09Z,enhancement pallas,open,0,1,https://github.com/jax-ml/jax/issues/23926," — Is this a duplicate of https://github.com/jaxml/jax/issues/24209? If so, let's close one and keep the other open for tracking."
rag,Fixed `pl.debug_print`ing of scalar fragmented arrays under Mosaic GPU,Fixed `pl.debug_print`ing of scalar fragmented arrays under Mosaic GPU,2024-09-25T15:23:34Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23912
yi,`stop_gradient` cannot safeguard `io_callback` against `jax.grad` transformation,"Not sure if this is intended to not work, but I have been trying to ""safeguard"" `jax.experimental.io_callback` with `jax.lax.stop_gradient` so that it could play nicely with `jax.grad`. However, it turns out it doesn't work... Here is an example to illustrate the problem: ```python >>> import jax >>> def f(x): ...     x_ = jax.lax.stop_gradient(x) ...     jax.lax.stop_gradient(jax.experimental.io_callback(lambda a: print(f'hello {a}'), None, x_)) ...     return x ...  >>> jax.grad(f)(1.0) Traceback (most recent call last):   [...] ValueError: IO callbacks do not support JVP. ``` This can be troublesome especially if I want to save some intermediate results midway inside the training loop (And I am sure this part does not need autodiff!). I guess `jax.debug.callback` might work but the execution is not guaranteed according to the doc.  Otherwise, is there a way to have ""stop_gradient""style `io_callback` that is guaranteed to execute impure code and works with `jax.grad` transformation? It would be more versatile and compatible with all JAX transforms.",2024-09-25T15:18:08Z,enhancement,open,0,0,https://github.com/jax-ml/jax/issues/23911
rag,Fixed `mgpu.FragmentedArray.reduce_sum` for integer types,Fixed `mgpu.FragmentedArray.reduce_sum` for integer types The implementation previously assumed the type is floating and used addf.,2024-09-25T13:51:42Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23905
yi,Add `override_type_lowerings` to `LoweringParameters`,"The `LoweringParameters` dataclass already allows for specifying overriding lowering rules for JAX primitives. This is very important and useful for third party targets implementing PJRT. Another important aspect of code generation are the types generated by abstract values. Third parties can already register their own primitives with custom rules, but the dictionaries `mlir.ir_type_handlers` and `core.raise_to_shapped_mappings` needs to be set by third parties directly. This is not ideal, because this is a global data structure defined in JAX. A better interface would be one analogous to the `override_lowering_rules` in the `LoweringParameters` data class. However, `LoweringParameters` may be extended to have a dictionary (or tuple, just like `override_lowering_rules`) that  maps types (abstract values) to their MLIR type counter part. This would allow for custom type lowerings. I would be happy to implement this if you find it useful. As a side note, is there a reason why `override_lowering_rules` is a tuple and not a dictionary? I am also volunteering to make this change if you find it useful. Thanks!",2024-09-25T13:25:56Z,enhancement,open,0,0,https://github.com/jax-ml/jax/issues/23904
yi,JAX dct is not accurate compared to scipy,"I have also found Jax's dct to be inaccurate compared to scipy: https://github.com/PlasmaControl/DESC/pull/1119/filesr1774234361, >= ~8 orders of magnitude even on 64 bit floating point mode. To see this just change ```python  Uses Jax fq_2 = norm * idct(dct(f(m), type=dct_type), n=n.size, type=dct_type) ``` to  ```python  Uses scipy fq_2 = norm * sidct(sdct(f(m), type=dct_type), n=n.size, type=dct_type) ``` and observe ```python  JAX is less accurate than scipy. factor = 1e8 if using_jax else 1 np.testing.assert_allclose(fq_2, f(n), atol=1e14 * factor) ``` _Originally posted by  in https://github.com/jaxml/jax/issues/23827issuecomment2372595040_",2024-09-25T00:09:08Z,,closed,0,5,https://github.com/jax-ml/jax/issues/23895,"Hi, thanks for the report! Could you put together a minimal reproducible example demonstrating the issue? That will prevent us from having to guess at what the problematic inputs might be. Thanks!","Here is the the more mwe from https://github.com/PlasmaControl/DESC/pull/1119discussion_r1774234361. You may run with `pytest k test_dct` or you can provide the inputs to the function manually. ```python import jax.numpy as jnp from jax.scipy.fft import dct, idct from scipy.fft import dct as sdct from scipy.fft import idct as sidct import pytest from jax import config as jax_config import numpy as np jax_config.update(""jax_enable_x64"", True) def cheb_pts(N):     n = jnp.arange(N)     return jnp.cos(jnp.pi * (2 * n + 1) / (2 * N)) def _identity(x):     return x .mark.unit .mark.parametrize(     ""f, M"",     [         (_identity, 2),         (_identity, 3),     ], ) def test_dct(f, M):     m = cheb_pts(M)     n = cheb_pts(m.size * 10)     norm = n.size / m.size     dct_type = 2     np.testing.assert_allclose(norm * sidct(sdct(f(m), type=dct_type), n=n.size, type=dct_type), f(n), atol=1e14)      JAX is less accurate than scipy.     np.testing.assert_allclose(norm * idct(dct(f(m), type=dct_type), n=n.size, type=dct_type), f(n), atol=1e14) ```","Since `dct` calls `fft`, I suspect this may be a duplicate of https://github.com/jaxml/jax/issues/23827. Assigning to , who is debugging that issue.",Yes this issue was raised from that thread. However I made a new issue because this test fails even not using gpu. For me it is 8 orders less accurate on cpu. Just mentioning in case relevant,"This issue was actually unrelated to https://github.com/jaxml/jax/issues/23827, but the fix was straightforward: https://github.com/jaxml/jax/pull/23917"
yi,DOC: Improved documentation for jax.numpy.isscalar,Part of https://github.com/jaxml/jax/issues/21461  Based on discussion: 19959,2024-09-24T22:39:58Z,,closed,0,2,https://github.com/jax-ml/jax/issues/23890,Bullet points rewritten and one example added based on the comments.,Replaced by CC(Simplify definition of jnp.isscalar)
yi,[Pallas TPU] Improve error message when trying to store a scalar to VMEM,[Pallas TPU] Improve error message when trying to store a scalar to VMEM Fixes https://github.com/jaxml/jax/issues/23884,2024-09-24T19:59:40Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23885
yi,Deprecate the `vectorized` argument to pure_callback and ffi_call,"Currently, the primitives backing `pure_callback` and (by extension) `ffi_call` provide a default batching rule that is controlled by the parameter `vectorized` to these two functions. This batching rule (allegedly) implements the following behavior: * For `vectorized=True`, vmapping results in a call to the callback directly with the batched inputs after moving the batch axis to the front. This assumes that the callback function directly handles vectorized inputs and (roughly) `callback(x_batch) == stack([callback(x) for x in x_batch])`. * When `vectorized=False`, the vmap produces a sequential scan with the callback in the body, which could have significant performance implications. While these don't seem like unreasonable defaults, the behavior when `vectorized=False` could result in unexpectedly poor performance for users who aren't familiar with this behavior. More importantly, the `vectorized=True` semantics are confusing—and I would argue wrong—when the function takes more than one input. For example, consider the following function: ```python import jax import jax.numpy as jnp def callback(x, y):   print(x.shape, y.shape)   return x + y def fun(x, y):   shape = jnp.broadcast_shapes(x.shape, y.shape)   dtype = jnp.result_type(x, y)   out_type = jax.ShapeDtypeStruct(shape, dtype)   return jax.pure_callback(callback, out_type, x, y, vectorized=True) ``` We can call this function under `vmap` as follows: ```python jax.vmap(fun, in_axes=(0, None))(jnp.ones(3), 0.5) ``` This prints `(3,)` and `()` for the shapes, and the result has shape `(3,)`, as we expect. However, things get a little ugly if we apply a second `vmap` as follows: ```python jax.vmap(jax.vmap(fun, in_axes=(0, None)), in_axes=(None, 0))(jnp.ones(3), jnp.ones(4)) ``` In this case, by the time our callback is executed, it receives `x` and `y` with shapes `(3,)` and `(4,)`, respectively, and it crashes with a `TypeError` because of the mismatched shapes. Even worse, callback doesn't have enough metadata to gracefully handle this case no matter what; how can callback ever know which argument was batched first? This behavior is bad, and the docs are (at least somewhat) misleading. This confusion has bitten me, and it has come up several times on the JAX issue tracker, most recently in CC(pure_callback is broken with multiple vmap). In this PR, I propose a refactoring of this API.  Proposal I think that the best approach for handling these issues would be to deprecate both the default behavior (using a `lax.map`) and the `vectorized` parameter, in favor of an explicitly optin API using a `str` (or `Enum`) parameter called `vmap_method`. After the appropriate deprecation periods, my proposed API would have the following behavior: * Calling `vmap` on a callback without an explicit `vmap_method` would raise an error. * Using `vmap_method=""sequential""` would produce a `lax.map` when vmapped (the current default behavior). * Using `vmap_method=""broadcast""` would be similar to the current `vectorized=True` behavior, but new axes would be added in the appropriate locations for unbatched inputs. For example, in the double `vmap` example from above, x would have shape `(1, 3)` and y would have shape `(4, 1)` when they reach the callback. * Finally, `vmap_method=""broadcast_fullrank""` would behave like broadcast, but the inputs would be tiled to the expected batched shape (e.g. `(4, 3)` in the example above). I would argue that the documentation for `pure_callback` and `ffi_call` currently imply this behavior for `vectorized=True`. This proposed API would continue to support existing use cases, while minimizing the chance of confusion and performance footguns.  Alternatives Another approach would be to remove support for batching callbacks entirely in favor of an API like `custom_vmap` or something more general. This was what I suggested in response to CC(pure_callback is broken with multiple vmap), but since `custom_vmap` isn't documented or well supported, I'm not sure this is a totally satisfying solution. Furthermore, I expect that the vast majority of use cases for batching callbacks will be supported by some simple rules like the ones listed above, with minimal maintenance burden. More advanced needs could still be covered by `custom_vmap`.",2024-09-24T18:09:09Z,pull ready,closed,0,6,https://github.com/jax-ml/jax/issues/23881,"Just a heads up, it seems this did not make it into 0.4.34 despite being mentioned in the changelog. Edit: idk what happened, going by tags it's in, but doesn't appear to be in the wheel on PyPI: ``` > jax.version.__version__  0.4.34 > inspect.signature(jax.pure_callback)  ```","Thanks for the heads up! You're absolutely right, although the GitHub tag also doesn't seem to include it either. I'll update the changelog accordingly!","What does the ""rank"" in the name `""broadcast_fullrank""` refer to? It seems that the array ranks, as in `np.ndim`, are the same in either broadcast option. What would you think of renaming the options here to: * `""expand""` for current ""broadcast"" with singleton dimensions, and * `""broadcast""` for current ""broadcast_fullrank"" with fully broadcast dimensions? I'm trying to suggest `np.expand_dims` in the former case, since that inserts singleton dimensions, and to suggest `np.broadcast` in the latter case, since that behaves as a material common broadcast on all of its arguments.","Good points  naming is hard! I like your suggestions, and definitely don't feel too attached to the names from this PR. Maybe `""broadcast""` by itself is confusing, since people have different intuitions for what broadcasting means. I think that the analogy to `expand_dims` is a good one, but `np.broadcast` doesn't seem like quite the right comparison here. Maybe `np.broadcast_arrays` is closer to the semantics we mean. What about `""expand_dims""` and `""broadcast_arrays""`?","Expanding to `""expand_dims""` sounds good to me. And `""broadcast_arrays""`, or something like `""broadcast_all""`, sounds good to me.",",  — Do either of you have opinions about this naming question?"
yi,[Pallas] Update export compatibility tests,"[Pallas] Update export compatibility tests The old test was generated before our IR was really stable, which has started to cause problems when trying to test with Trillium.",2024-09-24T15:56:55Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23876
yi,Expose some APIs for querying trace state. This will let us move users away from,"Expose some APIs for querying trace state. This will let us move users away from depending on our internals. Prep work for ""stackless"".",2024-09-24T15:31:44Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23875
yi,Unable to use residual offloading with scan and remat," Description Hi guys, I'm very excited with recent activations offloading mechanism introduced in JAX/XLA:GPU but I'm unable to make it work with the scan. My setup is the following  I'm training classic transformer with transformer block scanned over inputs ""number of layers"" times. I'm also using rematerialization to reduce memory footprint of the model. I basically wrap apply_block function with the jax.remat with ""nothing_saveable"" policy and then scan this block over inputs to achieve desired behavior  the only activations being saved during forward pass in my case is the residual stream (embeddings) in between scanned block.  With the recent introduction of the ""save_and_offload_only_these_names"" policy, I thought that it would be enough to mark the output of the scanned block with `jax.ad_checkpoint(x,  ""output"")` and then specify `names_which_can_be_offloaded=[""output""]`, but it didn't work.  I've implemented repro to showcase what is going on:  ``` import flax.linen as nn import jax import jax.ad_checkpoint import jax.numpy as jnp import numpy as np from flax.linen.linear import default_kernel_init EMB_DIM = 2048 HID_DIM = 2048 BS = 64 SEQ_LEN = 8192 N_LAYERS = 32 CHECKPOINT_POLICY = jax.checkpoint_policies.save_and_offload_only_these_names(     names_which_can_be_saved=[],     names_which_can_be_offloaded=[],     offload_src=""device"",     offload_dst=""pinned_host"", ) mesh = jax.sharding.Mesh(np.array(jax.devices()).reshape(4, 2), (""data"", ""model"")) input_sharding = jax.sharding.NamedSharding(     mesh, jax.sharding.PartitionSpec(""data"", None) ) target_sharding = jax.sharding.NamedSharding(     mesh,     jax.sharding.PartitionSpec(         ""data"",     ), ) rules = (     (""batch"", ""data""),     (""embedding"", None),     (""hidden"", ""model""),     (""q_sequence"", ""model""), ) class MLP(nn.Module):     .compact     def __call__(self, x):         x_residual = x         h = nn.Dense(             HID_DIM,             kernel_init=nn.with_logical_partitioning(                 default_kernel_init,                 (""embedding"", ""hidden""),             ),             use_bias=False,         )(x)         h = jax.ad_checkpoint.checkpoint_name(h, ""hidden"")         h = nn.relu(h)         x = nn.Dense(             EMB_DIM,             kernel_init=nn.with_logical_partitioning(                 default_kernel_init,                 (""hidden"", ""embedding""),             ),             use_bias=False,         )(h)         x = x_residual + x          Sequence parallelism         x = nn.with_logical_constraint(x, (""batch"", ""q_sequence"", None))         x = jax.ad_checkpoint.checkpoint_name(x, ""residual"")         return x class Output(nn.Module):     .compact     def __call__(self, x):         x = nn.Dense(             features=1,             kernel_init=nn.with_logical_partitioning(                 default_kernel_init,                 (""hidden"", None),             ),             use_bias=False,         )(x)[..., 0]         x = jnp.mean(x, axis=1)         return x class Model(nn.Module):     .compact     def __call__(self, x):         def apply_module(block, block_input, _):             block_output = block(block_input)             return block_output, None         apply_module = nn.remat(             apply_module,             policy=CHECKPOINT_POLICY,             prevent_cse=False,         )         x, _ = nn.scan(             apply_module,             variable_axes={""params"": 0},             split_rngs={""params"": True},             length=N_LAYERS,             metadata_params={nn.PARTITION_NAME: ""layers""},         )(MLP(), x, None)         preds = Output()(x)         return preds def loss_fn(preds, target):     return jnp.mean((preds  target) ** 2) def calc_loss(params, inputs, target):     preds = Model().apply(params, inputs)     loss = loss_fn(preds, target)     return loss def train_step(params, inputs, target):     loss, grads = jax.value_and_grad(calc_loss)(params, inputs, target)     params = jax.tree_util.tree_map(lambda p, g: p  1e8 * g, params, grads)     return params, loss def unbox_logically_partioned(tree, apply_constraint: bool = True):     return jax.tree_util.tree_map(         lambda leaf: (             leaf.unbox(apply_constraint=apply_constraint)             if isinstance(leaf, nn.LogicallyPartitioned)             else leaf         ),         tree,         is_leaf=lambda node: isinstance(node, nn.LogicallyPartitioned),     ) def get_gpu_memory_usage() > dict[str, float]:     if jax.default_backend() != ""gpu"":         return {}     num_devices = jax.local_device_count(""gpu"")     gpu_memory_usage = []     for i in range(num_devices):         memory_stats = jax.local_devices()[i].memory_stats()         gpu_memory_usage.append(             memory_stats[""peak_bytes_in_use""] / memory_stats[""bytes_limit""] * 100         )     return {f""GPU{i}"": val for i, val in enumerate(gpu_memory_usage)} with mesh, nn.logical_axis_rules(rules):     fake_inputs = jnp.empty((BS, SEQ_LEN, EMB_DIM))     fake_inputs = jax.device_put(fake_inputs, input_sharding)     fake_target = jnp.empty((BS,))     fake_target = jax.device_put(fake_target, target_sharding)     params = Model().init(jax.random.PRNGKey(0), fake_inputs)     params = unbox_logically_partioned(params)     train_step_fn = (         jax.jit(             train_step,             in_shardings=(                 jax.tree_util.tree_map(lambda x: x.sharding, params),                 input_sharding,                 target_sharding,             ),             out_shardings=(                 jax.tree_util.tree_map(lambda x: x.sharding, params),                 jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec()),             ),             donate_argnums=(0,),         )         .lower(params, fake_inputs, fake_target)         .compile()     )     jax.ad_checkpoint.print_saved_residuals(         train_step, params, fake_inputs, fake_target     )     with open(""compiled.txt"", ""w"") as f:         f.write(train_step_fn.as_text())     memory_analysis = train_step_fn.memory_analysis()     print(         f""Total size device = {memory_analysis.temp_size_in_bytes / 1024 / 1024 / 1024} GB, ""   noqa E501         f""host = {memory_analysis.host_temp_size_in_bytes / 1024 / 1024 / 1024} GB""     )     for i in range(10):         inputs = jax.random.normal(jax.random.PRNGKey(i), (BS, SEQ_LEN, EMB_DIM))         inputs = jax.device_put(inputs, input_sharding)         target = jax.random.normal(jax.random.PRNGKey(0), (BS,))         target = jax.device_put(target, target_sharding)         params, loss = train_step_fn(params, inputs, target)         print(loss)         print(get_gpu_memory_usage()) ```  First of all, I wanted to ensure that offloading is working in the first place. With  ```     names_which_can_be_saved=[],     names_which_can_be_offloaded=[], ``` I'm getting following results: `Total size device = 20.26562874764204 GB, host = 0.0 GB` Quite reasonable value. then, I wanted to check how much would it cost to save ""h"" on GPU, so I set ```     names_which_can_be_saved=[""hidden""],     names_which_can_be_offloaded=[], ``` and getting  `Total size device = 35.2968789935112 GB, host = 0.0 GB` This is also totally expected as ""h"" is f32[32,64,8192,2048] sharded across 8 GPUs which is equals to 16GB per GPU. Ok, let's try to offload ""h"" and see what happens. ```     names_which_can_be_saved=[],     names_which_can_be_offloaded=[""hidden""], ``` `Total size device = 19.75000447779894 GB, host = 16.0 GB`  also totally expected, instead of saving 16GB on GPU, we're offloading activations on host, device memory saved. Also iterations become a lot slower with is also expected. Now we sure that offloading is indeed working properly, I've tried to offload ""residual"" tensor (output of the scanned block). ```     names_which_can_be_saved=[],     names_which_can_be_offloaded=[""residual""], ``` Aaaand nothing happens  `Total size device = 20.26562874764204 GB, host = 0.0 GB`, nothing happens, no changes in memory usage, iterations is the same as no offloading at all.   System info (python version, jaxlib version, accelerator, etc.) ``` Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import jax; jax.print_environment_info() jax:    0.4.33 jaxlib: 0.4.33 numpy:  1.24.3 python: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0] jax.devices (8 total, 8 local): [CudaDevice(id=0) CudaDevice(id=1) ... CudaDevice(id=6) CudaDevice(id=7)] process_count: 1 platform: uname_result(system='Linux', node='endllmcomputeinstancee00yhypr7caccaxmct.priv.hw.nebius.yt', release='5.15.0119generic', version=' CC(CUDA backend produces inconsistent results for jax.numpy.linalg.inv)Ubuntu SMP Fri Aug 2 19:25:20 UTC 2024', machine='x86_64') $ nvidiasmi Tue Sep 24 10:15:19 2024        ++  ++ ``` XLA issue https://github.com/openxla/xla/issues/17541",2024-09-24T10:35:30Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/23869,"I've also tried following implementation, inspired by  https://github.com/jaxml/jax/issues/23614issuecomment2350773816 with wrapping entire apply_module with flax's custom_vjp, but it doesn't work properly.  ``` class Model(nn.Module):     .compact     def __call__(self, x):         def apply_module(block, block_input, _):             block_output = block(block_input)             return block_output, None         def apply_module_fwd(block, block_input, _):             res, vjp_fn = nn.vjp(apply_module, block, block_input, _)             emb, _ = res             emb = jax.device_put(emb, TransferToMemoryKind(""pinned_host""))             return (emb, None), vjp_fn         def apply_module_bwd(vjp_fn, res):             emb, _ = res             emb = jax.device_put(emb, TransferToMemoryKind(""device""))             res = (emb, None)             return vjp_fn(res)         apply_module_vjp = nn.custom_vjp(             apply_module,             forward_fn=apply_module_fwd,             backward_fn=apply_module_bwd         )         apply_module_vjp = nn.remat(             apply_module_vjp,             policy=CHECKPOINT_POLICY,             prevent_cse=False,         )         x, _ = nn.scan(             apply_module_vjp,             variable_axes={""params"": 0},             split_rngs={""params"": True},             length=N_LAYERS,             metadata_params={nn.PARTITION_NAME: ""layers""},         )(MLP(), x, None)         x = jax.device_put(x, TransferToMemoryKind(""device""))         preds = Output()(x)         return preds ```  `Total size device = 21.00781624764204 GB, host = 0.5 GB`. Here's part of the trace   As you can see, activations are indeed being offloaded during forward pass, but during forward pass they are not loaded back to devices  looks like these offloaded activations are immediately dropped on CPU and GPU activations are saved instead. That's why host memory is only 0.5GB  it is only reserved for activations of one layer. **Also I've noticed that this approach produces wrong loss & grad calculations, but if I'm commenting out all of the ""jax.device_put"" transfers, everything works as expected again.** ","> Now we sure that offloading is indeed working properly, I've tried to offload ""residual"" tensor (output of the scanned block) Do you have a minimal repro? Or maybe you can dump the jaxpr? `jit(f).trace(*args).jaxpr`","> Do you have a minimal repro? Or maybe you can dump the jaxpr? jit(f).trace(*args).jaxpr Hi, wouldn't provided repro script work for you? It's hidden behind ""Details"" spoiler in the topicstarter message. Also, for the second case with custom_vjp, here is scripts and jaxpr: Full code ``` import flax.linen as nn import jax import jax.ad_checkpoint import jax.numpy as jnp import numpy as np from flax.linen.linear import default_kernel_init from jax._src.api import TransferToMemoryKind EMB_DIM = 2048 HID_DIM = 2048 BS = 64 SEQ_LEN = 8192 N_LAYERS = 32 CHECKPOINT_POLICY = jax.checkpoint_policies.save_and_offload_only_these_names(     names_which_can_be_saved=[],     names_which_can_be_offloaded=[],     offload_src=""device"",     offload_dst=""pinned_host"", ) mesh = jax.sharding.Mesh(np.array(jax.devices()).reshape(4, 2), (""data"", ""model"")) input_sharding = jax.sharding.NamedSharding(     mesh, jax.sharding.PartitionSpec(""data"", None) ) input_full_sharding = jax.sharding.NamedSharding(     mesh, jax.sharding.PartitionSpec(""data"", ""model"") ) target_sharding = jax.sharding.NamedSharding(     mesh,     jax.sharding.PartitionSpec(         ""data"",     ), ) rules = (     (""batch"", ""data""),     (""embedding"", None),     (""hidden"", ""model""),     (""q_sequence"", ""model""), ) class MLP(nn.Module):     .compact     def __call__(self, x):         x_residual = x         h = nn.Dense(             HID_DIM,             kernel_init=nn.with_logical_partitioning(                 default_kernel_init,                 (""embedding"", ""hidden""),             ),             use_bias=False,         )(x)         h = nn.with_logical_constraint(h, (""batch"", None, ""hidden""))         h = jax.ad_checkpoint.checkpoint_name(h, ""hidden"")         h = nn.relu(h)         x = nn.Dense(             EMB_DIM,             kernel_init=nn.with_logical_partitioning(                 default_kernel_init,                 (""hidden"", ""embedding""),             ),             use_bias=False,         )(h)         x = x_residual + x          Sequence parallelism         x = nn.with_logical_constraint(x, (""batch"", ""q_sequence"", None))         x = jax.ad_checkpoint.checkpoint_name(x, ""residual"")         return x class Output(nn.Module):     .compact     def __call__(self, x):         x = nn.Dense(             features=1,             kernel_init=nn.with_logical_partitioning(                 default_kernel_init,                 (""hidden"", None),             ),             use_bias=False,         )(x)[..., 0]         x = jnp.mean(x, axis=1)         return x class Model(nn.Module):     .compact     def __call__(self, x):         def apply_module(block, block_input, _):             block_output = block(block_input)             return block_output, None         apply_module_remat = nn.remat(             apply_module,             policy=CHECKPOINT_POLICY,             prevent_cse=False,         )         def apply_module_fwd(block, block_input, _):             res, vjp_fn = nn.vjp(apply_module_remat, block, block_input, _)             emb, _ = res             emb = jax.device_put(emb, TransferToMemoryKind(""pinned_host""))             return (emb, None), vjp_fn         def apply_module_bwd(vjp_fn, res):             emb, _ = res             emb = jax.device_put(emb, TransferToMemoryKind(""device""))             res = (emb, None)             return vjp_fn(res)         apply_module_vjp = nn.custom_vjp(             apply_module_remat,             forward_fn=apply_module_fwd,             backward_fn=apply_module_bwd,         )         scan_fn = nn.scan(             apply_module_vjp,             variable_axes={""params"": 0},             split_rngs={""params"": True},             length=N_LAYERS,             metadata_params={nn.PARTITION_NAME: ""layers""},         )         x, _ = scan_fn(MLP(), x, None)         x = jax.device_put(x, TransferToMemoryKind(""device""))         preds = Output()(x)         return preds def loss_fn(preds, target):     return jnp.mean((preds  target) ** 2) def calc_loss(params, inputs, target):     preds = Model().apply(params, inputs)     loss = loss_fn(preds, target)     return loss def train_step(params, inputs, target):     loss, grads = jax.value_and_grad(calc_loss)(params, inputs, target)     params = jax.tree_util.tree_map(lambda p, g: p  1e8 * g, params, grads)     return params, loss def unbox_logically_partioned(tree, apply_constraint: bool = True):     return jax.tree_util.tree_map(         lambda leaf: (             leaf.unbox(apply_constraint=apply_constraint)             if isinstance(leaf, nn.LogicallyPartitioned)             else leaf         ),         tree,         is_leaf=lambda node: isinstance(node, nn.LogicallyPartitioned),     ) def get_gpu_memory_usage() > dict[str, float]:     if jax.default_backend() != ""gpu"":         return {}     num_devices = jax.local_device_count(""gpu"")     gpu_memory_usage = []     for i in range(num_devices):         memory_stats = jax.local_devices()[i].memory_stats()         gpu_memory_usage.append(             memory_stats[""peak_bytes_in_use""] / memory_stats[""bytes_limit""] * 100         )     return {f""GPU{i}"": val for i, val in enumerate(gpu_memory_usage)} profile_step = 5 with mesh, nn.logical_axis_rules(rules):     fake_inputs = jnp.empty((BS, SEQ_LEN, EMB_DIM))     fake_inputs = jax.device_put(fake_inputs, input_sharding)     fake_target = jnp.empty((BS,))     fake_target = jax.device_put(fake_target, target_sharding)     def init_model():         return Model().init(jax.random.PRNGKey(0), fake_inputs)     params = jax.jit(init_model)()     params = unbox_logically_partioned(params)     train_step_fn = jax.jit(         train_step,         in_shardings=(             jax.tree_util.tree_map(lambda x: x.sharding, params),             input_sharding,             target_sharding,         ),         out_shardings=(             jax.tree_util.tree_map(lambda x: x.sharding, params),             jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec()),         ),         donate_argnums=(0,),     ).lower(params, fake_inputs, fake_target)     with open(""lowered.txt"", ""w"") as f:         f.write(train_step_fn.as_text())     jaxpr = jax.make_jaxpr(train_step)(params, fake_inputs, fake_target)     print(jaxpr)     with open(""jaxpr.txt"", ""w"") as f:         f.write(jaxpr.pretty_print(use_color=False))     train_step_fn = train_step_fn.compile()     jax.ad_checkpoint.print_saved_residuals(         train_step, params, fake_inputs, fake_target     )     with open(""compiled.txt"", ""w"") as f:         f.write(train_step_fn.as_text())     memory_analysis = train_step_fn.memory_analysis()     print(         f""Total size device = {memory_analysis.temp_size_in_bytes / 1024 / 1024 / 1024} GB, ""   noqa E501         f""host = {memory_analysis.host_temp_size_in_bytes / 1024 / 1024 / 1024} GB""     )     for i in range(10):         inputs = jax.random.normal(jax.random.PRNGKey(i), (BS, SEQ_LEN, EMB_DIM))         inputs = jax.device_put(inputs, input_sharding)         target = jax.random.normal(jax.random.PRNGKey(0), (BS,))         target = jax.device_put(target, target_sharding)         if i == profile_step:             jax.tree_util.tree_map(lambda x: x.block_until_ready(), params)             jax.profiler.start_trace(""./profile"", create_perfetto_trace=True)         params, loss = train_step_fn(params, inputs, target)         if i == profile_step:             jax.tree_util.tree_map(lambda x: x.block_until_ready(), params)             jax.profiler.stop_trace()         print(loss)         print(get_gpu_memory_usage()) ```  jaxpr ``` { lambda ; a:f32[32,2048,2048] b:f32[32,2048,2048] c:f32[2048,1] d:f32[64,8192,2048]     e:f32[64]. let     custom_vjp_call_jaxpr[       bwd=. at 0x7fbbe8039ab0>       fun_jaxpr={ lambda ; . let  in () }       fwd_jaxpr_thunk=.memoized at 0x7fbbe8039bd0>       num_consts=0       out_trees=. at 0x7fbbe8039900>       symbolic_zeros=False     ]      _:f32[64,8192,2048] = broadcast_in_dim[       broadcast_dimensions=()       shape=(64, 8192, 2048)     ] 0.0     pjit[name=relu jaxpr={ lambda ; . let  in () }]      f:f32[64,8192,2048] g:f32[32,64,8192,2048] h:f32[32,64,8192,2048] = scan[       _split_transpose=False       jaxpr={ lambda ; i:f32[64,8192,2048] j:f32[2048,2048] k:f32[2048,2048]. let           l:f32[64,8192,2048] = dot_general[             dimension_numbers=(([2], [0]), ([], []))           ] i j           m:f32[64,8192,2048] = sharding_constraint[             layout=None             resource_env=ResourceEnv(mesh=Mesh('data': 4, 'model': 2))             sharding=NamedSharding(mesh=Mesh('data': 4, 'model': 2), spec=PartitionSpec('data', None, 'model'), memory_kind=device)             unconstrained_dims=set()           ] l           n:f32[64,8192,2048] = name[name=hidden] m           o:f32[64,8192,2048] = pjit[             name=relu             jaxpr={ lambda ; p:f32[64,8192,2048]. let                 q:f32[64,8192,2048] = max p 0.0               in (q,) }           ] n           r:f32[64,8192,2048] = dot_general[             dimension_numbers=(([2], [0]), ([], []))           ] o k           s:f32[64,8192,2048] = add i r           t:f32[64,8192,2048] = sharding_constraint[             layout=None             resource_env=ResourceEnv(mesh=Mesh('data': 4, 'model': 2))             sharding=NamedSharding(mesh=Mesh('data': 4, 'model': 2), spec=PartitionSpec('data', 'model', None), memory_kind=device)             unconstrained_dims=set()           ] s           u:f32[64,8192,2048] = name[name=residual] t           v:f32[64,8192,2048] = device_put[             devices=[TransferToMemoryKind(memory_kind='pinned_host')]             srcs=[None]           ] u         in (v, v, i) }       length=32       linear=(False, False, False)       num_carry=1       num_consts=0       reverse=False       unroll=1     ] d a b     w:f32[64,8192,2048] = device_put[       devices=[TransferToMemoryKind(memory_kind='device')]       srcs=[None]     ] f     x:f32[64,8192,1] = dot_general[dimension_numbers=(([2], [0]), ([], []))] w c     y:f32[64,8192,1] = slice[       limit_indices=(64, 8192, 1)       start_indices=(0, 0, 0)       strides=None     ] x     z:f32[64,8192] = squeeze[dimensions=(2,)] y     ba:f32[64] = reduce_sum[axes=(1,)] z     bb:f32[64] = div ba 8192.0     bc:f32[64] = sub bb e     bd:f32[64] = integer_pow[y=2] bc     be:f32[64] = integer_pow[y=1] bc     bf:f32[64] = mul 2.0 be     bg:f32[] = reduce_sum[axes=(0,)] bd     bh:f32[] = div bg 64.0     bi:f32[] = div 1.0 64.0     bj:f32[64] = broadcast_in_dim[broadcast_dimensions=() shape=(64,)] bi     bk:f32[64] = mul bj bf     bl:f32[64] = div bk 8192.0     bm:f32[64,8192] = broadcast_in_dim[       broadcast_dimensions=(0,)       shape=(64, 8192)     ] bl     bn:f32[64,8192,1] = broadcast_in_dim[       broadcast_dimensions=(0, 1)       shape=(64, 8192, 1)     ] bm     bo:f32[64,8192,1] = pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0))] bn       0.0     bp:f32[1,2048] = dot_general[dimension_numbers=(([0, 1], [0, 1]), ([], []))] bo       w     bq:f32[2048,1] = transpose[permutation=(1, 0)] bp     br:f32[64,8192,2048] = dot_general[dimension_numbers=(([2], [1]), ([], []))] bo       c     bs:f32[64,8192,2048] = device_put[       devices=(None,)       srcs=(TransferToMemoryKind(memory_kind='device'),)     ] br     _:f32[64,8192,2048] bt:f32[32,2048,2048] bu:f32[32,2048,2048] = scan[       _split_transpose=False       jaxpr={ lambda ; bv:f32[64,8192,2048] bw:f32[64,8192,2048] bx:f32[2048,2048]           by:f32[2048,2048] bz:f32[64,8192,2048]. let           ca:f32[64,8192,2048] = device_put[             devices=[TransferToMemoryKind(memory_kind='device')]             srcs=[None]           ] bv           cb:f32[2048,2048] cc:f32[2048,2048] cd:f32[64,8192,2048] = remat2[             differentiated=True             jaxpr={ lambda ; ce:f32[2048,2048] cf:f32[2048,2048] cg:f32[64,8192,2048]                 ch:f32[64,8192,2048]. let                 ci:f32[64,8192,2048] = dot_general[                   dimension_numbers=(([2], [0]), ([], []))                 ] cg ce                 cj:f32[64,8192,2048] = sharding_constraint[                   layout=None                   resource_env=ResourceEnv(mesh=Mesh('data': 4, 'model': 2))                   sharding=NamedSharding(mesh=Mesh('data': 4, 'model': 2), spec=PartitionSpec('data', None, 'model'), memory_kind=device)                   unconstrained_dims=set()                 ] ci                 ck:f32[64,8192,2048] = name[name=hidden] cj                 cl:f32[64,8192,2048] = custom_jvp_call[                   call_jaxpr={ lambda ; cm:f32[64,8192,2048]. let                       cn:f32[64,8192,2048] = pjit[                         name=relu                         jaxpr={ lambda ; co:f32[64,8192,2048]. let                             cp:f32[64,8192,2048] = max co 0.0                           in (cp,) }                       ] cm                     in (cn,) }                   jvp_jaxpr_thunk=.memoized at 0x7fbbe8038ca0>                   num_consts=0                   symbolic_zeros=False                 ] ck                 cq:bool[64,8192,2048] = gt ck 0.0                 _:f32[64,8192,2048] = broadcast_in_dim[                   broadcast_dimensions=()                   shape=(64, 8192, 2048)                 ] 0.0                 cr:f32[64,8192,2048] = sharding_constraint[                   layout=None                   resource_env=ResourceEnv(mesh=Mesh('data': 4, 'model': 2))                   sharding=NamedSharding(mesh=Mesh('data': 4, 'model': 2), spec=PartitionSpec('data', 'model', None), memory_kind=device)                   unconstrained_dims=set()                 ] ch                 cs:f32[2048,2048] = dot_general[                   dimension_numbers=(([0, 1], [0, 1]), ([], []))                 ] cr cl                 ct:f32[2048,2048] = transpose[permutation=(1, 0)] cs                 cu:f32[64,8192,2048] = dot_general[                   dimension_numbers=(([2], [1]), ([], []))                 ] cr cf                 cv:f32[64,8192,2048] = broadcast_in_dim[                   broadcast_dimensions=()                   shape=(64, 8192, 2048)                 ] 0.0                 cw:f32[64,8192,2048] = select_n cq cv cu                 cx:f32[64,8192,2048] = sharding_constraint[                   layout=None                   resource_env=ResourceEnv(mesh=Mesh('data': 4, 'model': 2))                   sharding=NamedSharding(mesh=Mesh('data': 4, 'model': 2), spec=PartitionSpec('data', None, 'model'), memory_kind=device)                   unconstrained_dims=set()                 ] cw                 cy:f32[2048,2048] = dot_general[                   dimension_numbers=(([0, 1], [0, 1]), ([], []))                 ] cx cg                 cz:f32[2048,2048] = transpose[permutation=(1, 0)] cy                 da:f32[64,8192,2048] = dot_general[                   dimension_numbers=(([2], [1]), ([], []))                 ] cx ce                 db:f32[64,8192,2048] = add_any cr da               in (cz, ct, db) }             policy=.policy at 0x7fbd4dd3bf40>             prevent_cse=False           ] bx by bz ca         in (cd, cb, cc) }       length=32       linear=(True, False, False, False, False)       num_carry=1       num_consts=0       reverse=True       unroll=1     ] bs g a b h     dc:f32[32,2048,2048] = mul 9.99999993922529e09 bt     dd:f32[32,2048,2048] = sub a dc     de:f32[32,2048,2048] = mul 9.99999993922529e09 bu     df:f32[32,2048,2048] = sub b de     dg:f32[2048,1] = mul 9.99999993922529e09 bq     dh:f32[2048,1] = sub c dg   in (dd, df, dh, bh) } ``` ","And for the first case  using only save_and_offload_only_these_names with  ```     names_which_can_be_saved=[],     names_which_can_be_offloaded=[""residual""], ``` Here is code and jaxpr Full code ``` import flax.linen as nn import jax import jax.ad_checkpoint import jax.numpy as jnp import numpy as np from flax.linen.linear import default_kernel_init from jax._src.api import TransferToMemoryKind EMB_DIM = 2048 HID_DIM = 2048 BS = 64 SEQ_LEN = 8192 N_LAYERS = 32 CHECKPOINT_POLICY = jax.checkpoint_policies.save_and_offload_only_these_names(     names_which_can_be_saved=[],     names_which_can_be_offloaded=[""residual""],     offload_src=""device"",     offload_dst=""pinned_host"", ) mesh = jax.sharding.Mesh(np.array(jax.devices()).reshape(4, 2), (""data"", ""model"")) input_sharding = jax.sharding.NamedSharding(     mesh, jax.sharding.PartitionSpec(""data"", None) ) input_full_sharding = jax.sharding.NamedSharding(     mesh, jax.sharding.PartitionSpec(""data"", ""model"") ) target_sharding = jax.sharding.NamedSharding(     mesh,     jax.sharding.PartitionSpec(         ""data"",     ), ) rules = (     (""batch"", ""data""),     (""embedding"", None),     (""hidden"", ""model""),     (""q_sequence"", ""model""), ) class MLP(nn.Module):     .compact     def __call__(self, x):         x_residual = x         h = nn.Dense(             HID_DIM,             kernel_init=nn.with_logical_partitioning(                 default_kernel_init,                 (""embedding"", ""hidden""),             ),             use_bias=False,         )(x)         h = nn.with_logical_constraint(h, (""batch"", None, ""hidden""))         h = jax.ad_checkpoint.checkpoint_name(h, ""hidden"")         h = nn.relu(h)         x = nn.Dense(             EMB_DIM,             kernel_init=nn.with_logical_partitioning(                 default_kernel_init,                 (""hidden"", ""embedding""),             ),             use_bias=False,         )(h)         x = x_residual + x          Sequence parallelism         x = nn.with_logical_constraint(x, (""batch"", ""q_sequence"", None))         x = jax.ad_checkpoint.checkpoint_name(x, ""residual"")         return x class Output(nn.Module):     .compact     def __call__(self, x):         x = nn.Dense(             features=1,             kernel_init=nn.with_logical_partitioning(                 default_kernel_init,                 (""hidden"", None),             ),             use_bias=False,         )(x)[..., 0]         x = jnp.mean(x, axis=1)         return x class Model(nn.Module):     .compact     def __call__(self, x):         def apply_module(block, block_input, _):             block_output = block(block_input)             return block_output, None         apply_module_remat = nn.remat(             apply_module,             policy=CHECKPOINT_POLICY,             prevent_cse=False,         )          def apply_module_fwd(block, block_input, _):              res, vjp_fn = nn.vjp(apply_module_remat, block, block_input, _)              emb, _ = res              emb = jax.device_put(emb, TransferToMemoryKind(""pinned_host""))              return (emb, None), vjp_fn          def apply_module_bwd(vjp_fn, res):              emb, _ = res              emb = jax.device_put(emb, TransferToMemoryKind(""device""))              res = (emb, None)              return vjp_fn(res)          apply_module_vjp = nn.custom_vjp(              apply_module_remat,              forward_fn=apply_module_fwd,              backward_fn=apply_module_bwd,          )         scan_fn = nn.scan(             apply_module_remat,             variable_axes={""params"": 0},             split_rngs={""params"": True},             length=N_LAYERS,             metadata_params={nn.PARTITION_NAME: ""layers""},         )         x, _ = scan_fn(MLP(), x, None)          x = jax.device_put(x, TransferToMemoryKind(""device""))         preds = Output()(x)         return preds def loss_fn(preds, target):     return jnp.mean((preds  target) ** 2) def calc_loss(params, inputs, target):     preds = Model().apply(params, inputs)     loss = loss_fn(preds, target)     return loss def train_step(params, inputs, target):     loss, grads = jax.value_and_grad(calc_loss)(params, inputs, target)     params = jax.tree_util.tree_map(lambda p, g: p  1e8 * g, params, grads)     return params, loss def unbox_logically_partioned(tree, apply_constraint: bool = True):     return jax.tree_util.tree_map(         lambda leaf: (             leaf.unbox(apply_constraint=apply_constraint)             if isinstance(leaf, nn.LogicallyPartitioned)             else leaf         ),         tree,         is_leaf=lambda node: isinstance(node, nn.LogicallyPartitioned),     ) def get_gpu_memory_usage() > dict[str, float]:     if jax.default_backend() != ""gpu"":         return {}     num_devices = jax.local_device_count(""gpu"")     gpu_memory_usage = []     for i in range(num_devices):         memory_stats = jax.local_devices()[i].memory_stats()         gpu_memory_usage.append(             memory_stats[""peak_bytes_in_use""] / memory_stats[""bytes_limit""] * 100         )     return {f""GPU{i}"": val for i, val in enumerate(gpu_memory_usage)} profile_step = 5 with mesh, nn.logical_axis_rules(rules):     fake_inputs = jnp.empty((BS, SEQ_LEN, EMB_DIM))     fake_inputs = jax.device_put(fake_inputs, input_sharding)     fake_target = jnp.empty((BS,))     fake_target = jax.device_put(fake_target, target_sharding)     def init_model():         return Model().init(jax.random.PRNGKey(0), fake_inputs)     params = jax.jit(init_model)()     params = unbox_logically_partioned(params)     train_step_fn = jax.jit(         train_step,         in_shardings=(             jax.tree_util.tree_map(lambda x: x.sharding, params),             input_sharding,             target_sharding,         ),         out_shardings=(             jax.tree_util.tree_map(lambda x: x.sharding, params),             jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec()),         ),         donate_argnums=(0,),     ).lower(params, fake_inputs, fake_target)     with open(""lowered.txt"", ""w"") as f:         f.write(train_step_fn.as_text())     jaxpr = jax.make_jaxpr(train_step)(params, fake_inputs, fake_target)     print(jaxpr)     with open(""jaxpr.txt"", ""w"") as f:         f.write(jaxpr.pretty_print(use_color=False))     train_step_fn = train_step_fn.compile()     jax.ad_checkpoint.print_saved_residuals(         train_step, params, fake_inputs, fake_target     )     with open(""compiled.txt"", ""w"") as f:         f.write(train_step_fn.as_text())     memory_analysis = train_step_fn.memory_analysis()     print(         f""Total size device = {memory_analysis.temp_size_in_bytes / 1024 / 1024 / 1024} GB, ""   noqa E501         f""host = {memory_analysis.host_temp_size_in_bytes / 1024 / 1024 / 1024} GB""     )     for i in range(10):         inputs = jax.random.normal(jax.random.PRNGKey(i), (BS, SEQ_LEN, EMB_DIM))         inputs = jax.device_put(inputs, input_sharding)         target = jax.random.normal(jax.random.PRNGKey(0), (BS,))         target = jax.device_put(target, target_sharding)         if i == profile_step:             jax.tree_util.tree_map(lambda x: x.block_until_ready(), params)             jax.profiler.start_trace(""./profile"", create_perfetto_trace=True)         params, loss = train_step_fn(params, inputs, target)         if i == profile_step:             jax.tree_util.tree_map(lambda x: x.block_until_ready(), params)             jax.profiler.stop_trace()         print(loss)         print(get_gpu_memory_usage()) ```  jaxpr ``` { lambda ; a:f32[32,2048,2048] b:f32[32,2048,2048] c:f32[2048,1] d:f32[64,8192,2048]     e:f32[64]. let     _:f32[64,8192,2048] = broadcast_in_dim[       broadcast_dimensions=()       shape=(64, 8192, 2048)     ] 0.0     pjit[name=relu jaxpr={ lambda ; . let  in () }]      f:f32[64,8192,2048] g:f32[32,64,8192,2048] = scan[       _split_transpose=False       jaxpr={ lambda ; h:f32[64,8192,2048] i:f32[2048,2048] j:f32[2048,2048]. let           k:f32[64,8192,2048] = dot_general[             dimension_numbers=(([2], [0]), ([], []))           ] h i           l:f32[64,8192,2048] = sharding_constraint[             layout=None             resource_env=ResourceEnv(mesh=Mesh('data': 4, 'model': 2))             sharding=NamedSharding(mesh=Mesh('data': 4, 'model': 2), spec=PartitionSpec('data', None, 'model'), memory_kind=device)             unconstrained_dims=set()           ] k           m:f32[64,8192,2048] = name[name=hidden] l           n:f32[64,8192,2048] = pjit[             name=relu             jaxpr={ lambda ; o:f32[64,8192,2048]. let                 p:f32[64,8192,2048] = max o 0.0               in (p,) }           ] m           q:f32[64,8192,2048] = dot_general[             dimension_numbers=(([2], [0]), ([], []))           ] n j           r:f32[64,8192,2048] = add h q           s:f32[64,8192,2048] = sharding_constraint[             layout=None             resource_env=ResourceEnv(mesh=Mesh('data': 4, 'model': 2))             sharding=NamedSharding(mesh=Mesh('data': 4, 'model': 2), spec=PartitionSpec('data', 'model', None), memory_kind=device)             unconstrained_dims=set()           ] r           t:f32[64,8192,2048] = name[name=residual] s         in (t, h) }       length=32       linear=(False, False, False)       num_carry=1       num_consts=0       reverse=False       unroll=1     ] d a b     u:f32[64,8192,1] = dot_general[dimension_numbers=(([2], [0]), ([], []))] f c     v:f32[64,8192,1] = slice[       limit_indices=(64, 8192, 1)       start_indices=(0, 0, 0)       strides=None     ] u     w:f32[64,8192] = squeeze[dimensions=(2,)] v     x:f32[64] = reduce_sum[axes=(1,)] w     y:f32[64] = div x 8192.0     z:f32[64] = sub y e     ba:f32[64] = integer_pow[y=2] z     bb:f32[64] = integer_pow[y=1] z     bc:f32[64] = mul 2.0 bb     bd:f32[] = reduce_sum[axes=(0,)] ba     be:f32[] = div bd 64.0     bf:f32[] = div 1.0 64.0     bg:f32[64] = broadcast_in_dim[broadcast_dimensions=() shape=(64,)] bf     bh:f32[64] = mul bg bc     bi:f32[64] = div bh 8192.0     bj:f32[64,8192] = broadcast_in_dim[       broadcast_dimensions=(0,)       shape=(64, 8192)     ] bi     bk:f32[64,8192,1] = broadcast_in_dim[       broadcast_dimensions=(0, 1)       shape=(64, 8192, 1)     ] bj     bl:f32[64,8192,1] = pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0))] bk       0.0     bm:f32[1,2048] = dot_general[dimension_numbers=(([0, 1], [0, 1]), ([], []))] bl       f     bn:f32[2048,1] = transpose[permutation=(1, 0)] bm     bo:f32[64,8192,2048] = dot_general[dimension_numbers=(([2], [1]), ([], []))] bl       c     _:f32[64,8192,2048] bp:f32[32,2048,2048] bq:f32[32,2048,2048] = scan[       _split_transpose=False       jaxpr={ lambda ; br:f32[64,8192,2048] bs:f32[2048,2048] bt:f32[2048,2048] bu:f32[64,8192,2048]. let           bv:f32[2048,2048] bw:f32[2048,2048] bx:f32[64,8192,2048] = remat2[             differentiated=True             jaxpr={ lambda ; by:f32[2048,2048] bz:f32[2048,2048] ca:f32[64,8192,2048]                 cb:f32[64,8192,2048]. let                 cc:f32[64,8192,2048] = dot_general[                   dimension_numbers=(([2], [0]), ([], []))                 ] ca by                 cd:f32[64,8192,2048] = sharding_constraint[                   layout=None                   resource_env=ResourceEnv(mesh=Mesh('data': 4, 'model': 2))                   sharding=NamedSharding(mesh=Mesh('data': 4, 'model': 2), spec=PartitionSpec('data', None, 'model'), memory_kind=device)                   unconstrained_dims=set()                 ] :f32[64,8192,2048] = name[name=hidden] cd                 cf:f32[64,8192,2048] = custom_jvp_call[                   call_jaxpr={ lambda ; cg:f32[64,8192,2048]. let                       ch:f32[64,8192,2048] = pjit[                         name=relu                         jaxpr={ lambda ; ci:f32[64,8192,2048]. let                             cj:f32[64,8192,2048] = max ci 0.0                           in (cj,) }                       ] cg                     in (ch,) }                   jvp_jaxpr_thunk=.memoized at 0x7facd0573eb0>                   num_consts=0                   symbolic_zeros=False                 ] ce                 ck:bool[64,8192,2048] = gt ce 0.0                 _:f32[64,8192,2048] = broadcast_in_dim[                   broadcast_dimensions=()                   shape=(64, 8192, 2048)                 ] 0.0                 cl:f32[64,8192,2048] = sharding_constraint[                   layout=None                   resource_env=ResourceEnv(mesh=Mesh('data': 4, 'model': 2))                   sharding=NamedSharding(mesh=Mesh('data': 4, 'model': 2), spec=PartitionSpec('data', 'model', None), memory_kind=device)                   unconstrained_dims=set()                 ] cb                 cm:f32[2048,2048] = dot_general[                   dimension_numbers=(([0, 1], [0, 1]), ([], []))                 ] cl cf                 cn:f32[2048,2048] = transpose[permutation=(1, 0)] cm                 co:f32[64,8192,2048] = dot_general[                   dimension_numbers=(([2], [1]), ([], []))                 ] cl bz                 cp:f32[64,8192,2048] = broadcast_in_dim[                   broadcast_dimensions=()                   shape=(64, 8192, 2048)                 ] 0.0                 cq:f32[64,8192,2048] = select_n ck cp co                 cr:f32[64,8192,2048] = sharding_constraint[                   layout=None                   resource_env=ResourceEnv(mesh=Mesh('data': 4, 'model': 2))                   sharding=NamedSharding(mesh=Mesh('data': 4, 'model': 2), spec=PartitionSpec('data', None, 'model'), memory_kind=device)                   unconstrained_dims=set()                 ] cq                 cs:f32[2048,2048] = dot_general[                   dimension_numbers=(([0, 1], [0, 1]), ([], []))                 ] cr ca                 ct:f32[2048,2048] = transpose[permutation=(1, 0)] cs                 cu:f32[64,8192,2048] = dot_general[                   dimension_numbers=(([2], [1]), ([], []))                 ] cr by                 cv:f32[64,8192,2048] = add_any cl cu               in (ct, cn, cv) }             policy=.policy at 0x7fae1845bf40>             prevent_cse=False           ] bs bt bu br         in (bx, bv, bw) }       length=32       linear=(True, False, False, False)       num_carry=1       num_consts=0       reverse=True       unroll=1     ] bo a b g     cw:f32[32,2048,2048] = mul 9.99999993922529e09 bp     cx:f32[32,2048,2048] = sub a cw     cy:f32[32,2048,2048] = mul 9.99999993922529e09 bq     cz:f32[32,2048,2048] = sub b cy     da:f32[2048,1] = mul 9.99999993922529e09 bn     db:f32[2048,1] = sub c da   in (cx, cz, db, be) } ``` ","Okay, I looked at the stableHLO produced by `names_which_can_be_offloaded=[""residual""],` and there are no transfers to pinned_host and loads to device because what you are trying to offload is not a ""residual"".  You can do `lowered.as_text()` and look for `annotate_device_placement` calls in both `residual` and `hidden` stableHLO or `device_put` in the jaxpr for both (looking for device_put is better). ","I've actually figured out the problem  to make it work with scan, I needed to checkpoint inputs to the function and not output. This way I see Total size device = 5.2968789935112 GB, host = 16.0 GB  And while it really greatly reduces memory usage, I don't think it scales properly (when quadrupling batch size, I would expect only slight increase in GPU memory usage while I'm getting OOM). I'll gather more proofs/xla dumps and create another issue.  Final thought regarding this problem  I would be great if solution with scan would be somewhere in documentation :)","> I would be great if solution with scan would be somewhere in documentation Yes, that is something I plan to add! > when quadrupling batch size, I would expect only slight increase in GPU memory usage while I'm getting OOM I would suggest trying this out on a TPU and see if it scales up! The GPU support is lacking by a bit but it is catching up.  "
rag,Added support for % and `select` to `mgpu.FragmentedArray`,Added support for % and `select` to `mgpu.FragmentedArray`,2024-09-24T10:18:41Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23868
yi,Bump fonttools from 4.51.0 to 4.54.0,"Bumps fonttools from 4.51.0 to 4.54.0.  Release notes Sourced from fonttools's releases.  4.54.0  [Docs] Small docs cleanups by @​n8willis (fonttools/fonttools CC(Reimplement argmin/argmax using a single pass variadic reduction.)) [Docs] cleanup code blocks by @​n8willis (fonttools/fonttools CC(improve error message for jnp.pad pad_width array)) [Docs] fix Sphinx builds by @​n8willis (fonttools/fonttools CC(未找到相关数据)) [merge] Minor fixes to documentation for merge by @​drj11 (fonttools/fonttools CC(Add cummax and cummin)) [subset] Small tweaks to pyftsubset documentation by @​RoelN (fonttools/fonttools CC(fix bug in categorical test, disable 3611 on tpu)) [Tests] Do not require fonttools command to be available by @​behdad (fonttools/fonttools CC(Unexpected overhead when using `grad(pmap)`)) [Tests] subset_test: add failing test to reproduce issue fonttools/fonttools CC(Cleanup: define type lists in test_util & use in several test files.) by @​anthrotype (fonttools/fonttools CC(How to select the jax  release version)) [ttLib] NameRecordVisitor: include whole sequence of character variants' UI labels, not just the first by @​anthrotype (fonttools/fonttools CC(public interface for partial evaluation )) [varLib.avar] Reconstruct mappings from binary by @​behdad (fonttools/fonttools CC(add source info to jaxpr typechecking messages)) [varLib.instancer] Fix visual artefacts with partial L2 instancing by @​Hoolean (fonttools/fonttools CC([jax2tf] Update the tfnightly version to 20200701)) [varLib.interpolatable] Support discrete axes in .designspace by @​behdad (fonttools/fonttools CC(lax.pad breaks for zerosized inputs)) [varLib.models] By default, assume OpenTypelike normalized space by @​behdad (fonttools/fonttools CC(Use precision=HIGHEST in expm repeated squaring))  4.53.1  [feaLib] Improve the sharing of inline chained lookups ( CC(jaxpr typechecking: custom rules and errors)) [otlLib] Correct the calculation of OS/2.usMaxContext with reversed chaining contextual single substitutions ( CC(Clarify docs on jax.lax.cond.)) [misc.visitor] Visitors search the inheritance chain of objects they are visiting ( CC([jax2tf] Add support for custom JVP/VJP))  4.53.0  [ttLib.removeOverlaps] Support CFF table to aid in downconverting CFF2 fonts (fonttools/fonttools CC(WIP: Print JAX arrays to full precision)) [avar] Fix crash when accessing notyetexisting attribute (fonttools/fonttools CC(Fix eigh JVP to ensure that both the primal and tangents of the eigen…)) [docs] Add buildMathTable to otlLib.builder documentation (fonttools/fonttools CC(Restrict .compress tests to arrays only.)) [feaLib] Allow UTF8 with BOM when reading features (fonttools/fonttools CC(Bump jaxlib version to 0.1.49 and update WORKSPACE)) [SVGPathPen] Revert rounding coordinates to two decimal places by default (fonttools/fonttools CC(Avoid direct type/dtype comparisons to fix NumPy 1.19 deprecation war…)) [varLib.instancer] Refix output filename decisionmaking  (fonttools/fonttools CC(Remove warning suppression for tuple and list arguments to reductions.), fonttools/fonttools CC(Add physical optimization example), fonttools/fonttools CC(Fix typos ""Pytrees"" page on readthedocs))  4.52.4  [varLib.cff] Restore and deprecate convertCFFtoCFF2 that was removed in 4.52.0 release as it is used by downstream projects ( CC(Repro lazy issue)).  4.52.3 Fixed a small syntax error in the reStructuredTextformatted NEWS.rst file which caused the upload to PyPI to fail for 4.52.2. No other code changes. 4.52.2  [varLib.interpolatable] Ensure that scipy/numpy output is JSONserializable ( CC(summary statistics of jaxpr equations),  CC(fix an issue with newer versions of pytype)) [housekeeping] Regenerate table lists, to fix pyinstaller packaging of the new VARC table ( CC(avoid valuebased error check in random.choice),  CC([testdocs branch] Update JAX quickstart, test conf.py for proper Markdown support)) [cffLib] Make CFFToCFF2 and CFF2ToCFF more robust ( CC(Add class wrapper for doubledouble arithmetic),  CC(Fix typos: np.bool > np.bool_))  4.52.1 Fixed a small syntax error in the reStructuredTextformatted NEWS.rst file which caused the upload to PyPI to fail for 4.52.0. No other code changes. 4.52.0  Added support for the new VARC (Variable Composite) table that is being proposed to OpenType spec ( CC(tweak t logpdf tolerance for float64)). For more info: https://github.com/harfbuzz/boringexpansionspec/blob/main/VARC.md [ttLib.main] Fixed decompiling all tables (90fed08). [feaLib] Don't reference the same lookup index multiple times within the same feature record, it is only applied once anyway ( CC(Update JAX Quickstart with parallelization (`pmap`), other changes)). [cffLib] Moved methods to desubroutinize, remove hints and unused subroutines from subset module to cffLib ( CC(cumsum inaccuracy)). [varLib.instancer] Added support for partialinstancing CFF2 tables! Also, added method to downconvert from CFF2 to CFF 1.0, and CLI entry points to convert CFFCFF2 ( CC(K)).    ... (truncated)   Changelog Sourced from fonttools's changelog.  4.54.0 (released 20240923)  [Docs] Small docs cleanups by @​n8willis ( CC(Reimplement argmin/argmax using a single pass variadic reduction.)) [Docs] cleanup code blocks by @​n8willis ( CC(improve error message for jnp.pad pad_width array)) [Docs] fix Sphinx builds by @​n8willis ( CC(未找到相关数据)) [merge] Minor fixes to documentation for merge by @​drj11 ( CC(Add cummax and cummin)) [subset] Small tweaks to pyftsubset documentation by @​RoelN ( CC(fix bug in categorical test, disable 3611 on tpu)) [Tests] Do not require fonttools command to be available by @​behdad ( CC(Unexpected overhead when using `grad(pmap)`)) [Tests] subset_test: add failing test to reproduce issue  CC(Cleanup: define type lists in test_util & use in several test files.) by @​anthrotype ( CC(How to select the jax  release version)) [ttLib] NameRecordVisitor: include whole sequence of character variants' UI labels, not just the first by @​anthrotype ( CC(public interface for partial evaluation )) [varLib.avar] Reconstruct mappings from binary by @​behdad ( CC(add source info to jaxpr typechecking messages)) [varLib.instancer] Fix visual artefacts with partial L2 instancing by @​Hoolean ( CC([jax2tf] Update the tfnightly version to 20200701)) [varLib.interpolatable] Support discrete axes in .designspace by @​behdad ( CC(lax.pad breaks for zerosized inputs)) [varLib.models] By default, assume OpenTypelike normalized space by @​behdad ( CC(Use precision=HIGHEST in expm repeated squaring))  4.53.1 (released 20240705)  [feaLib] Improve the sharing of inline chained lookups ( CC(jaxpr typechecking: custom rules and errors)) [otlLib] Correct the calculation of OS/2.usMaxContext with reversed chaining contextual single substitutions ( CC(Clarify docs on jax.lax.cond.)) [misc.visitor] Visitors search the inheritance chain of objects they are visiting ( CC([jax2tf] Add support for custom JVP/VJP))  4.53.0 (released 20240531)  [ttLib.removeOverlaps] Support CFF table to aid in downconverting CFF2 fonts ( CC(WIP: Print JAX arrays to full precision)) [avar] Fix crash when accessing notyetexisting attribute ( CC(Fix eigh JVP to ensure that both the primal and tangents of the eigen…)) [docs] Add buildMathTable to otlLib.builder documentation ( CC(Restrict .compress tests to arrays only.)) [feaLib] Allow UTF8 with BOM when reading features ( CC(Bump jaxlib version to 0.1.49 and update WORKSPACE)) [SVGPathPen] Revert rounding coordinates to two decimal places by default ( CC(Avoid direct type/dtype comparisons to fix NumPy 1.19 deprecation war…)) [varLib.instancer] Refix output filename decisionmaking  ( CC(Remove warning suppression for tuple and list arguments to reductions.),  CC(Add physical optimization example),  CC(Fix typos ""Pytrees"" page on readthedocs))  4.52.4 (released 20240527)  [varLib.cff] Restore and deprecate convertCFFtoCFF2 that was removed in 4.52.0 release as it is used by downstream projects ( CC(Repro lazy issue)).  4.52.3 (released 20240527)  Fixed a small syntax error in the reStructuredTextformatted NEWS.rst file which caused the upload to PyPI to fail for 4.52.2. No other code changes.  4.52.2 (released 20240527)  [varLib.interpolatable] Ensure that scipy/numpy output is JSONserializable ( CC(summary statistics of jaxpr equations),  CC(fix an issue with newer versions of pytype)).    ... (truncated)   Commits  5429a6c Release 4.54.0 cbc350d Update Changelog 63611d4 Merge pull request  CC(improve error message for jnp.pad pad_width array) from n8willis/docscleanup a43def0 Add regression test for  CC(Add CI tests for numpy 1.16.4) 07351d1 Fix visual artefacts with partial L2 instancing d05cdcf Docs: black 7d93689 Docs: workaround doctestvsSphinx syntax highlighting. 11343ed Add instructions to escape question mark d871fd3 Remove dotslash from examples 4767465 Merge pull request  CC(Using iterator with fori_loop documentation) from fonttools/dependabot/github_actions/pypa/ghact... Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-09-23T17:49:25Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23848,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
yi,Bump pytest from 8.2.0 to 8.3.3,"Bumps pytest from 8.2.0 to 8.3.3.  Release notes Sourced from pytest's releases.  8.3.3 pytest 8.3.3 (20240909) Bug fixes    CC(Flip default value of jax_unique_mhlo_module_names to False.): Avoid calling  (and other instance descriptors) during fixture discovery  by asottile{.interpretedtext role=&quot;user&quot;}    CC(Add PYTORCH_SAME padding to JAX.): Fixed the issue of not displaying assertion failure differences when using the parameter importmode=importlib in pytest&gt;=8.1.    CC(Type mismatches for jnp.concatenate tests): Fixed a regression where type change in [ExceptionInfo.errisinstance]{.titleref} caused [mypy]{.titleref} to fail.    CC([jax2tf] Add new test for jax2tf sharding): Fixed typing compatibility with Python 3.9 or less  replaced [typing.Self]{.titleref} with [typing_extensions.Self]{.titleref}  by Avasam{.interpretedtext role=&quot;user&quot;}    CC(Support MANUAL collectives in toplevel xmaps): Fixed an issue with backslashes being incorrectly converted in nodeid paths on Windows, ensuring consistent path handling across environments.    CC(Update code example in custom VJP documentation.): Fixed bug where the verbosity levels where not being respected when printing the &quot;msg&quot; part of failed assertion (as in assert condition, msg).    CC(Add scipy.signal.stft.): Fix bug where disabling the terminal plugin via p no:terminal would cause crashes related to missing the verbose option.  by GTowers1{.interpretedtext role=&quot;user&quot;}   Improved documentation   CC(未找到相关数据): Clarify that the [pytest_deselected]{.titleref} hook should be called from [pytest_collection_modifyitems]{.titleref} hook implementations when items are deselected.  CC(jnp.average: support tuple axis): Remove erroneous quotes from [tmp_path_retention_policy]{.titleref} example in docs.  Miscellaneous internal changes   CC(Migrate JAX internals to builtin Python logging): Fix typos discovered by codespell and add codespell to precommit hooks.  8.3.2 pytest 8.3.2 (20240724) Bug fixes    CC(Different results from jitted and non jitted function when using lax.switch): Resolve regression [conda]{.titleref} environments where no longer being automatically detected.  by RonnyPfannschmidt{.interpretedtext role=&quot;user&quot;}   8.3.1 pytest 8.3.1 (20240720) The 8.3.0 release failed to include the change notes and docs for the release. This patch release remedies this. There are no other changes.   ... (truncated)   Commits  d0f136f build(deps): Bump pypa/ghactionpypipublish from 1.10.0 to 1.10.1 ( CC(⚠️ Nightly upstreamdev CI failed ⚠️)) 972f307 Prepare release version 8.3.3 0dabdcf Include coauthors in release announcement ( CC(gradient of eigh does not respect `symmetrize_input`)) ( CC(Ignore UserWarning as a WAR ...)) a9910a4 Do not discover properties when iterating fixtures ( CC([typing] annotate jax._src.dtypes)) ( CC(未找到相关数据)) 0f10b6b Fix issue with slashes being turned into backslashes on Windows ( CC(Enable partially discharging state effects from jaxprs)) ( CC(Update the quickstart notebook.)... 300d13d Merge pull request  CC(未找到相关数据) from pytestdev/patchback/backports/8.3.x/57cccf7f4... e5d32c7 Merge pull request  CC(未找到相关数据) from svenevs/fix/docsexampleparametrizeminortypo bc913d1 Streamline checks for verbose option ( CC(Key array is not an instance of `jax.random.KeyArray`)) ( CC(未找到相关数据)) 01cfcc9 Fix typos and introduce codespell precommit hook ( CC(Migrate JAX internals to builtin Python logging)) ( CC(Faster histogram implementation and slow existing JAX implementation)) 4873394 doc: Remove past training ( CC(Update version.py and CHANGELOG for jax 0.3.23 release)) ( CC(Jax functions getting mixed)) Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-09-23T17:49:10Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23847,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
rag,Bump hypothesis from 6.102.4 to 6.112.1,"Bumps hypothesis from 6.102.4 to 6.112.1.  Commits  a64b71c Bump hypothesispython version to 6.112.1 and update changelog 7d7724c Merge pull request  CC(fix jaxpr util test in enable_x64 mode) from tybug/fixnumpynightly 6dfe346 remove nowincorrect assertion 8f4dcab Merge pull request  CC(Simplify the interface for host_callback.id_tap) from tybug/strengthenirtests d7ec1e3 bump uploadartifact 4a99ad0 more explicit generator eeaa831 rc2 is out probably? ccd6849 ensure coverage b49d4dc consolidate test_forced_* methods 927ad08 improve ir test strategies Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-09-23T17:49:01Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23846,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
yi,Hide JAX's internal tracing state and update libraries to use limited trace-state-querying APIs as needed. This is prep work for stackless which will change those internals while preserving the API.,Hide JAX's internal tracing state and update libraries to use limited tracestatequerying APIs as needed. This is prep work for stackless which will change those internals while preserving the API.,2024-09-23T17:24:41Z,,closed,0,1,https://github.com/jax-ml/jax/issues/23843,Closing Copybara created PR due to inactivity
gpt,Unable to load any libcudnn_engines_precompiled.so using jax.nn.dot_product_attention," Description I am calling `jax.nn.dot_product_attention` with the following line: ``` dpsa_cudnn = jax.nn.dot_product_attention(query, key, value, implementation='cudnn') ``` However, this throws the following error: ``` 20240923 10:43:47.563053: W external/xla/xla/service/gpu/nvptx_compiler.cc:893] The NVIDIA driver's CUDA version is 12.4 which is older than the PTX compiler version 12.6.68. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIAprovided CUDA forward compatibility packages. Unable to load any of {libcudnn_engines_precompiled.so.9.4.0, libcudnn_engines_precompiled.so.9.4, libcudnn_engines_precompiled.so.9, libcudnn_engines_precompiled.so} Traceback (most recent call last):   File ""/home/zdcao/website/distance/crystal_gpt/./test_flash_attn.py"", line 19, in      dpsa_cudnn = jax.nn.dot_product_attention(query, key, value, implementation='cudnn')                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/data/zdcao/envs/rl/lib/python3.11/sitepackages/jax/_src/nn/functions.py"", line 1025, in dot_product_attention     out = cudnn_dot_product_attention(           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/data/zdcao/envs/rl/lib/python3.11/sitepackages/jax/_src/cudnn/fused_attention_stablehlo.py"", line 1061, in dot_product_attention     output = _dot_product_attention(              ^^^^^^^^^^^^^^^^^^^^^^^   File ""/data/zdcao/envs/rl/lib/python3.11/sitepackages/jax/_src/cudnn/fused_attention_stablehlo.py"", line 953, in _dot_product_attention     output = _dot_product_attention_fwd(              ^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/data/zdcao/envs/rl/lib/python3.11/sitepackages/jax/_src/cudnn/fused_attention_stablehlo.py"", line 330, in _dot_product_attention_fwd     outputs = _dot_product_attention_fwd_p_wrapper.bind(               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/data/zdcao/envs/rl/lib/python3.11/sitepackages/jax/_src/cudnn/fused_attention_stablehlo.py"", line 374, in _dot_product_attention_fwd_impl     outputs = _dot_product_attention_fwd_p.bind(               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ jaxlib.xla_extension.XlaRuntimeError: INTERNAL: The stride for the last dimension corresponding to the embedding size per head should be 1 for input_names::K in external/xla/xla/stream_executor/cuda/cuda_dnn.cc(8233): 'graph_.build_operation_graph(cudnn>handle())'   For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.33 jaxlib: 0.4.33 numpy:  2.1.1 python: 3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0] jax.devices (1 total, 1 local): [CudaDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='node002', release='3.10.01062.12.1.el7.x86_64', version=' CC(Python 3 compatibility issues) SMP Tue Feb 4 23:02:59 UTC 2020', machine='x86_64') $ nvidiasmi Mon Sep 23 10:49:00 2024        ++  ++ ```",2024-09-23T02:49:56Z,bug,open,0,8,https://github.com/jax-ml/jax/issues/23833,"Seems that the head dim of K doesn't have stride 1 which is required by cuDNN, could you provide a repo with how QKV is constructed?","QKV is constructed as follows: ```  Define the input shapes batch_size = 1000 nhead = 4 seq_len = 105 hidden_dim = 64 key = jax.random.PRNGKey(42)  Generate random input data key, key1, key2, key3 = jax.random.split(key, 4) query = jax.random.normal(key1, (batch_size, seq_len, nhead, hidden_dim), dtype=jnp.float16) key = jax.random.normal(key2, (batch_size, seq_len, nhead, hidden_dim), dtype=jnp.float16) value = jax.random.normal(key3, (batch_size, seq_len, nhead, hidden_dim),  dtype=jnp.float16) ```",Thanks! let me try reproducing this., have there been any updates?,"Sorry, it fell through the cracks. I'm not able to reproduce with this example: ``` from functools import partial import jax import jax.numpy as jnp from jax.nn import (     dot_product_attention, )  Define the input shapes batch_size = 1000 nhead = 4 seq_len = 105 hidden_dim = 64 key = jax.random.PRNGKey(42)  Generate random input data key, key1, key2, key3 = jax.random.split(key, 4) query = jax.random.normal(key1, (batch_size, seq_len, nhead, hidden_dim), dtype=jnp.float16) key = jax.random.normal(key2, (batch_size, seq_len, nhead, hidden_dim), dtype=jnp.float16) value = jax.random.normal(key3, (batch_size, seq_len, nhead, hidden_dim),  dtype=jnp.float16) dpsa_cudnn = jax.jit(   partial(dot_product_attention, implementation='cudnn') ) out = dpsa_cudnn(query, key, value) print(out[0,0,0,0]) ``` I'm using cuDNN 9.4 + CUDA 12.6 on Hopper with XLA and JAX at head.",I  use the cudnn 9.5 + CUDA12.6. It may be due to version incompatibility.,I also tried cudnn 9.5 + CUDA 12.6. I tried both on A100 and H100. Still can't reproduce this. Is there anything specific about your testing environment? Maybe using some specific commits from old JAX/XLA?,It seems that this issue is caused by our outdated `CentOS` version and the absence of precompiled libraries. This error no longer happens when using the `ubuntu+cuda12.5` image.
rag,Introduce a pl.actual_size(ref_idx) utility to make masking in pallas easier.,"Introduce a pl.actual_size(ref_idx) utility to make masking in pallas easier. Today, in pallas, if you have a kernel grid where the shape of a blockspec does not line up with an input evenly (aka shape_dim % block_dim != 0)  you will get a shape filled with padding. Users mask this via knowledge of their inputs, but this has two main problems: 1) The mask is always run and computed 2) The kernel has to access data outside of itself, more or less, to build the mask, either via closing over locals or extending the kernel for new inputs. This CL introduces a util, pl.actual_size, to help with getting the real size of a given ref within a given tile. Ex: A reduce sum kernel launched with input x=jnp.ones((9, 8, 128)) and a grid of (2, 1, 1) over blockspec of (8, 8, 128) will have an x_ref of (1, 8, 128) real data and the rest padding in the 2nd tile.  This is the first step in two useful directions: 1) A pl.mask_to_actual_size(ref, val) util that runs conditionally (only when we are on a tile where we expect padding, saving compute) 2) Extending pl.actual_size and pl.mask_to_actual_size to handle jumble/ragged types",2024-09-22T21:02:28Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23832
yi,"Bug- False ""UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32 "," Description ```python Python 3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0] :: Anaconda, Inc. on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import jax >>> import tensorflow_probability.substrates.jax as tfp >>> tfd = tfp.distributions >>> rng = jax.random.PRNGKey(0) >>> tfd.OneHotCategorical(logits=jax.random.normal(key=rng,shape=(3,4)), dtype=jax.numpy.float32).sample(seed=rng) /home/user/anaconda3/envs/ml_exp/lib/python3.9/sitepackages/jax/_src/numpy/array_methods.py:68: UserWarning: Explicitly requested dtype  requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jaxcurrentgotchas for more.   return lax_numpy.astype(arr, dtype, copy=copy, device=device) Array([[1., 0., 0., 0.],        [0., 1., 0., 0.],        [0., 0., 0., 1.]], dtype=float32) ``` I explicitly set `jnp.float32` and did not ask for `int` anywhere, but received the warning  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.30 jaxlib: 0.4.30 numpy:  1.26.4 python: 3.9.19 (main, May  6 2024, 19:43:03)  [GCC 11.2.0] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='user', release='6.8.045generic', version=' CC(Feature request: export TF ops)Ubuntu SMP PREEMPT_DYNAMIC Fri Aug 30 12:02:04 UTC 2024', machine='x86_64')",2024-09-22T16:28:17Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/23829,"Thanks for the report! This warning is arising becuase `tensorflow_probability` passes an `int64` to JAX. You can see this by reexecuting your code after running the following: ```python import warnings warnings.simplefilter('error') ``` This will turn the warning into an error with a full traceback, which helps locate the place where this problematic dtype is passed: https://github.com/tensorflow/probability/blob/c6c86e788516a9b16958460528867735a721ef40/tensorflow_probability/python/internal/backend/numpy/random_generators.pyL106 If this warning is a problem to you, you could think about reporting it to the `tensorflow_probability` maintainers, but there's nothing JAX can do to fix this short of removing the warning entirely."
yi,Implementation of PowerLaw distribution,"Hi, [SciPy][SciPy] has a very specific implementation of [Powerlaw][scipy_powerlaw]. It is over a unit interval and only supports positive indexes. Many applications require it to be over a broader range of intervals (truncated on both sides) and demand the support of negative indexes. I would like to request this generic Powerlaw implementation in JAX. NumPyro has one very generic [implementation][numpyro_powerlaw] from where you can reference all the mathematical derivations. [SciPy]: https://docs.scipy.org/doc/scipy/index.html [scipy_powerlaw]: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.powerlaw.html [numpyro_powerlaw]: https://num.pyro.ai/en/stable/distributions.htmldoublytruncatedpowerlaw",2024-09-22T10:32:17Z,enhancement,open,0,1,https://github.com/jax-ml/jax/issues/23826,"You can do this with TFPonJAX. There's no builtin Powerlaw distribution, but you can apply an affine transformation to a Beta distribution (the Scipy docs say ""powerlaw is a special case of beta with b=1."") ``` from tensorflow_probability.substrates import jax as tfp import jax.numpy as jnp import numpy as np import matplotlib.pyplot as plt tfd = tfp.distributions tfb = tfp.bijectors a = 0.5   power law dist parameter  interval bounds low = 4. high = 1. power_law_dist = tfd.TransformedDistribution(     tfd.Beta(a, 1), tfb.Shift(low)(tfb.Scale(high  low))) x = np.linspace(low, high, 50) plt.plot(x, power_law_dist.prob(x)) ``` !image Hope that helps!"
rag,Simplify extended dtype convert logic,"diffbase: CC(add jax.experimental.primal_tangent_dtype helper) TODO:  [x] add test coverage for the code added here Previously, the idea was that we would use the `convert_element_type` primitive to cast to/from extended dtypes. Extended dtype rules specified `convert_from(dtype1, dtype2) > bool` and `convert_to(dtype1, dtype2) > bool` functions. They were meant to do something like indicate whether a convert_element_type was legal. But I'm not sure if they really made sense. The implementation was certainly buggy for nonscalar representation types (physical element types). This PR simplifies and fixes things: 1. Instead of overloading the `convert_element_type_p` primitive with more cases involving casts to/from extended dtypes, let's just have distinct `to_edtype_p` and `from_edtype_p` primitives, which can be much simpler. We still reuse the `jax.lax.convert_element_type` API function, so there's no API change to the few existing users who know about this stuff. 2. Instead of extended dtype rules including `convert_from`/`convert_to` functions with questionable semantics, let's only allow casts to/from the representation type, which is already specified by the rules' `physical_element_aval`. (Indeed that should be roughly _all_ we need, and this PR is just one step towards realizing that goal.) We still have a boolean `allow_conversion` on extended dtype rules just so we can handle the PRNGKey case, where we don't want to allow any casts. 3. Fix the conversion logic to handle nonscalar representation types (physical element types).",2024-09-21T19:01:20Z,pull ready,closed,0,9,https://github.com/jax-ml/jax/issues/23823,"> Do we need random_wrap_p and random_unwrap_p still, after this? I'm not sure! I haven't looked. WDYT?","From what I can tell these become redundant _in principle_, though I'm not sure if exactly practically so as of this PR.","It looks like `random_wrap_impl` produces a `PRNGKeyArray`, whereas the stuff here is just about converting element types of `Array`s.","The only implementation of `Array` that can carry an edtype dtype in general is an earray (equivalently `PRNGKeyArray`, up to current state of implementation). Howcome the `impl` rule of `to_edtype_p` can eschew wrapping up the resulting array in an earray/`PRNGKeyArray`? One answer could be: the current impl rule is unable to support all edtypes, for instance key types. And what the `random_wrap_p` impl rule does accounts for that difference, then. Does that sound correct?","> Howcome the impl rule of to_edtype_p can eschew wrapping up the resulting array in an earray/PRNGKeyArray? The trick is these things only work under a `jit` (for now). > One answer could be: the current impl rule is unable to support all edtypes, for instance key types. And what the random_wrap_p impl rule does accounts for that difference, then. Does that sound correct? What impl rule are you referring to? I think the unification question just comes down to whether we can use EArray for keys, or if we need to keep PRNGKeyArray.","> The trick is these things only work under a `jit` (for now). Why do we write an impl rule at all in this PR if these primitives are only meant to work under `jit` for now? > What impl rule are you referring to? `to_edtype_p`'s > I think the unification question just comes down to whether we can use EArray for keys, or if we need to keep PRNGKeyArray. This PR doesn't seem to make any EArray instances (?). If it were to do so, then it would happen in the impl rule for `to_edtype_p`. Does that sound right? Running ahead with that: I don't think it comes down to unifying `EArray` and `PRNGKeyArray`. Even with these two internal types being defined separately, we can always conditionally produce either an `EArray` or a `PRNGKeyArray` on conversion. (By the way, I predict that we can unify earrays and keys once we've worked out more of earrays. Their foreseeable differences to `PRNGKeyArray` seem minor, and reconcilable based on the edtype. That's uncertain until we try it, but seems separate either way.)","> Why do we write an impl rule at all in this PR if these primitives are only meant to work under jit for now? The primitives all work with or without a jit. They just need EArray. Once EArray works, all this code will work unchanged. > This PR doesn't seem to make any EArray instances (?). If it were to do so, then it would happen in the impl rule for to_edtype_p. Does that sound right? Right, this PR does nothing with EArray. The impl rule for to_edtype_p is just the standard one, and it won't ever have to change. Once EArray works, this impl will work in eager. (It might actually work now but I haven't tested it.) > Even with these two internal types being defined separately What are the internal types? EArray and PRNGKeyArray are returned to the user.","Sounds like we're saying very similar things. It seems that an answer to my original question (about obviating `random_wrap_p`) is: yes we likely could, provided we make `to_edtype_p` work across more target edtypes, in eager, by returning EArray (and/or PRNGKeyArray) as needed in its impl rule. I agree that's not done in this PR. Seems close though. Does that sound plausible? > What are the internal types? EArray and PRNGKeyArray are returned to the user. They're essentially all `jax.Array` from the external point of view. The `PRNGKeyArray` symbol is hidden. This is similar to how `ArrayImpl` is a `jax.Array` from the outside and is otherwise hidden. You can see both using `type()`, but that's the usual case with Python: ```python >>> type(jax.random.key(4)) jax._src.prng.PRNGKeyArray >>> type(jnp.ones(4)) jaxlib.xla_extension.ArrayImpl ```","> They're essentially all jax.Array from the external point of view. The PRNGKeyArray symbol is hidden. This is similar to how ArrayImpl is a jax.Array from the outside and is otherwise hidden.  Ah I see your point. You're right; I was just thrown by the term ""internal"". But as you say, the constructor and all other classspecific details are private, and these are just implementation details of Array. > Does that sound plausible? Yes except I don't think we'll need to change the impl rule. Once we make it work for `jit`, this impl rule will work too!"
yi,Tighten test tolerances after the underlying issue causing nondeterministic results for _nrm2 in Eigen BLAS was fixed in https://gitlab.com/libeigen/eigen/-/merge_requests/1667 -> cl/663346025,Tighten test tolerances after the underlying issue causing nondeterministic results for _nrm2 in Eigen BLAS was fixed in https://gitlab.com/libeigen/eigen//merge_requests/1667 > cl/663346025,2024-09-20T16:51:34Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23803
yi,"Add support for the new ""dot algorithm"" spec for more explicit control of dot product numerics","The StableHLO spec has a new ""algorithm"" parameter that allows specifying the algorithm that is used to execute a matrix multiplication, and it can tune the tradeoff between performance and computational cost. Historically the `precision` and `preferred_element_type` parameters have been used to expose some level of control, but their behavior is platform dependent and not sufficiently flexible for performance use cases. The proposal is to add a new algorithm parameter to dot_general to add support for the new explicit API. For a discussion about the limitations/problems with the existing tuning parameters (`precision` and `preferred_element_type`) see jaxml/jax CC(Allow explicit matmul precision), for example. This issue briefly discusses some design questions from the JAX side and proposes an API. The `DotAlgorithm` specification The StableHLO spec defines the ""dot algorithm"" using a data structure with fields defined as follows: > `lhs_precision_type` and `rhs_precision_type`, the precisions that the LHS and RHS of the operation are rounded to. Precision types are independent from the storage types of the inputs and the output. >  > `accumulation_type` the precision used for accumulation. > > `lhs_component_count`, `rhs_component_count`, and `num_primitive_operations` apply when we are using an algorithm which decomposes the LHS and/or RHS into multiple components and does multiple ""primitive"" dot operations on those values  usually to emulate a higher precision. For algorithms with no decomposition, these values should be set to 1. >  > `allow_imprecise_accumulation` to specify if accumulation in lower precision is permitted for some steps (e.g. `CUBLASLT_MATMUL_DESC_FAST_ACCUM`). which specifies a large matrix of possible algorithm definitions. In practice, however, only a small number of algorithms are actually supported today.  Enum vs data structure At the JAX level, we are left with a question about what types we want to accept as input to the algorithm parameter. Our proposal would be to support both: * an Enum with the algorithms that are known to be supported, and * a data structure that allows customization of all fields. The former is probably best for most users because it provides a compact representation for specifying known algorithms, but the latter is also important to allow support for users with custom hardware, or new experimental algorithms. So, for example, while most users would use this feature as follows: ```python lax.dot(lhs, rhs, algorithm=lax.DotAlgorithm.Preset.BF16_BF16_F32_X6) ``` we would still support more advanced use cases like: ```python algorithm = lax.DotAlgorithm(np.float32, np.float32, dtypes.bfloat16) lax.dot(lhs, rhs, algorithm=algorithm) ``` for hardware that happened to support such an algorithm. One shortcoming of this approach is that there is an added maintenance burden of synchronizing the `DotAlgorithm.Preset` enum in JAX with the list of supported algorithms known to StableHLO and/or XLA. It's not clear to me how quickly new algorithms will be added, so I'm not totally sure what the cost would be, and it doesn't seem too terrible if we get behind, if JAX also supports general userspecified algorithms. _Question:_ Should JAX's dot algorithm API include an Enum specifying supported algorithms or only a data structure exposing all the possible tuning parameters? _Proposal:_ Given that the number of supported algorithms is currently small, it seems like the benefits of providing an Enum of known algorithms would provide a better user experience which we expect to outweigh the maintenance cost. We propose to support specifying the algorithm choice via an Enum or the fully general data structure.  Relationship with current API The StableHLO spec requires that algorithm and precision be mutually exclusive so we can require that users only set one or the other. The way that `preferred_element_type` works is that it casts the output of the dot product to `preferred_element_type`, and then relies on the compiler to use the appropriate accumulation type. This means that when an algorithm is specified, the meaning of `preferred_element_type` changes from ""accumulation and output type"" to just ""output type"". It would be possible to support the use of preferred_element_type in this way (to just specify the output type) alongside the algorithm choice which specifies the accumulation type separately, because the algorithm types don't need to be the same as the input and output storage types. But, it is also easy for a user to manually cast the output of the operation if needed. Regardless, if `preferred_element_type` is not specified, it might make sense to default to returning a dtype of accumulation_type. _Question:_ When algorithm is specified, but `preferred_element_type` is not, what should the return type from `dot_general` be? _Proposal:_ Without a strong argument in favor of supporting both `preferred_element_type` and `algorithm` in the same function, we propose making them mutually exclusive, and always set the output data type of the operation to the accumulation type of the algorithm. If we discover cases where there are performance penalties when manually casting the output, we could choose to add support for `preferred_element_type` using this ""output type"" interpretation, or add a new output_type parameter.  Behavior when differentiated JAX operations like dot_general need to support differentiation. It seems like it probably makes sense for the JVP rule to always use the same algorithm as the primal computation since the input and output types are the same (if a user requires different numerics, they could use `custom_jvp`). But, things are less clear for the transpose. When running reverse mode autodiff on: ``` C = A @ B ``` we compute the backwards pass using: ``` dA = dC @ B^T ``` Don't read too much into the notation, but the important point is that computing the derivatives of a dot operation requires computing two products between something type C and the LHS and RHS. The question here is: how should we specify the algorithm (or `preferred_element_type`, for that matter!) for the transposed operation. Currently, the transpose operation simply uses the same `preferred_element_type` as the forward pass, but this isn't necessarily a sensible choice if the input and output types are different. If the input and output types are the same, then it probably makes sense to at least default to using the same algorithm on the forward and backward passes, but things are less clear for the case where the input and output types are different. While it is straightforward to transpose (at least) the data types of an algorithm by swapping the accumulator type with either the LHS or RHS type, this will typically result in an algorithm that isn't (and shouldn't be!) supported. For example, transposing `F16_F16_F32` would give `F32_F16_F16`, and we don't expect that accumulating into a lower precision type like this produces a sensible algorithm. It's not completely unreasonable to default to using the same algorithm even in that case, but it also seems likely that users would want to be able to control the behavior of the backwards operations separately. One option would be to have a `transpose_algorithm` parameter (or maybe even `transpose_lhs_algorithm` and `transpose_rhs_algorithm` parameters), but this starts to get pretty unwieldy! Regardless, _Aside:_ This discussion brings up the question of what to do about preferred_element_type in the transpose operation. If we instead think of it as specifying the output data type, then it probably makes the most sense for preferred_element_type to be set to the expected output type (which we know!) instead of casting after doing the product. _Question:_ What algorithm should be used when transposing a dot_general operation where an algorithm was specified on the forward pass? _Proposal:_ Given that: * it is not clear how to best determine the algorithm for the transpose of dot_general; * a firstprinciples transposition results in algorithms that aren't supported (in other words, XLA's dotgeneralwithalgorithm is not closed under transposition); and * this feature is meant to support power users who want more explicit control of the numerics and performance; We propose that if users want to transpose a `dot_general` which used a specific algorithm, that they also specify a `transpose_algorithm` parameter which can be `DotAlgorithmLike` or a tuple of two `DotAlgorithmLike`s specifying the LHS and RHS transpose algorithms. If this parameter is not specified, attempting to transpose a `dot_general` operation where an algorithm was selected will throw an error. See https://github.com/jaxml/jax/pull/23574 for an implementation of this proposal.",2024-09-20T14:19:55Z,enhancement,closed,1,1,https://github.com/jax-ml/jax/issues/23797,This was implemented in https://github.com/jaxml/jax/pull/23574!
llm,Port GPU kernels for SVD to the FFI.,"Port GPU kernels for SVD to the FFI. Unlike the other GPU linear algebra kernels that I've ported so far, this one isn't straightforward to implement as a single kernel, and while it does support lowering without access to a GPU (no more descriptor!), it only supports dynamics shapes in the batch dimensions. There are two main technical challenges: 1. The main `gesvd` kernels in cuSolver/hipSolver only support matrices with shape `(m, n)` with `m >= n`. This means that we need to transpose the inputs and outputs as part of the lowering rule when `m < n`. (Note: we actually just use C layouts instead of Fortran layouts to implement this case.) While this could be handled in the kernel, this seemed like a lot of work for somewhat limited benefit, and it would probably have performance implications. 2. The `gesvd` and `gesvdj` kernels return `V^H` and `V` respectively, and the batched version of `gesvdj` doesn't support `full_matrices=False`. This means that we need logic in the lowering rule to handle transposition and slicing. This makes it hard to have the algorithm selection be a parameter to the kernel. Another note: cuSolver has a 64bit implementation of the SVD, and we always use that implementation on the CUDA backend. The 32bit interface is included for ROCM support, and I have tested it manually. This was a feature request from https://github.com/jaxml/jax/issues/23413.",2024-09-20T13:41:15Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23794
yi,Better doc for `jax.numpy.i0`,Part of CC(Tracking issue: inline docstrings) ,2024-09-20T08:38:13Z,documentation pull ready,closed,0,3,https://github.com/jax-ml/jax/issues/23790,Fixed the lint_and_typecheck error. Could you please trigger the tests again?,Can you please squash the changes into a single commit?,Thanks! Squashed into single commit.
rag,"Improve the coverage of shard map tests for < 8 devices. Due to the skip in SetupModule before this change, we lost a lot of coverage on latest hardware.","Improve the coverage of shard map tests for < 8 devices. Due to the skip in SetupModule before this change, we lost a lot of coverage on latest hardware.",2024-09-19T21:12:02Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23772
yi,Multiprocessing with Jax on Slurm fails," Description Hello, unfortunately I am having an issue running a script using jax on SLURM. My Job only specifies 1 GPU but it still seems like an issue as the system still has multiple GPUs. That was also the only difference I was able to find between my personal computer and the server as it works on my personal machine with no problem using the exact same conda environment.  Unfortunately I need to fix this anyway since my machine runs out of memory quickly. The error message I am getting looks like this: ``` 20240919 21:50:58.963539: W external/xla/xla/service/platform_util.cc:199] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: INTERNAL: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_DEVICE_UNAVAILABLE: CUDAcapable device(s) is/are busy or unavailable ╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮ │ /home/**/.conda/envs/dtest/lib/python3.9/sitepackages/jax/_src/xla_bridge.py:879 in         │ │ backends                                                                                         │ │                                                                                                  │ │    876 │   default_priority = 1000                                                              │ │    877 │   for platform, priority, fail_quietly in platform_registrations:                       │ │    878 │     try:                                                                                │ │ ❱  879 │   │   backend = _init_backend(platform)                                                 │ │    880 │   │   _backends[platform] = backend                                                     │ │    881 │   │                                                                                     │ │    882 │   │   if priority > default_priority:                                                   │ │                                                                                                  │ │ /home/**/.conda/envs/dtest/lib/python3.9/sitepackages/jax/_src/xla_bridge.py:970 in         │ │ _init_backend                                                                                    │ │                                                                                                  │ │    967 │   logger.warning(f""Platform '{platform}' is experimental and not all JAX ""              │ │    968 │   │   │   │      ""functionality may be correctly supported!"")                           │ │    969   logger.debug(""Initializing backend '%s'"", platform)                                     │ │ ❱  970   backend = registration.factory()                                                        │ │    971    TODO(skye): consider raising more descriptive errors directly from backend            │ │    972    factories instead of returning None.                                                  │ │    973   if backend is None:                                                                     │ │                                                                                                  │ │ /home/**/.conda/envs/dtest/lib/python3.9/sitepackages/jax/_src/xla_bridge.py:668 in factory │ │                                                                                                  │ │    665 │     updated_options.update(options)                                                     │ │    666 │   updated_options.update(_options_from_jax_configs(plugin_name))                        │ │    667 │   if distributed.global_state.client is None:                                           │ │ ❱  668 │     return xla_client.make_c_api_client(plugin_name, updated_options, None)             │ │    669 │                                                                                         │ │    670 │   distribute_options = {                                                                │ │    671 │   │   'node_id': distributed.global_state.process_id,                                   │ │                                                                                                  │ │ /home/**/.conda/envs/dtest/lib/python3.9/sitepackages/jaxlib/xla_client.py:200 in           │ │ make_c_api_client                                                                                │ │                                                                                                  │ │   197   """"""                                                                                      │ │   198   if options is None:                                                                      │ │   199 │   options = {}                                                                           │ │ ❱ 200   return _xla.get_c_api_client(plugin_name, options, distributed_client)                   │ │   201                                                                                            │ │   202                                                                                            │ │   203 def make_tpu_client(                                                                       │ ╰──────────────────────────────────────────────────────────────────────────────────────────────────╯ XlaRuntimeError: INTERNAL: no supported devices found for platform CUDA During handling of the above exception, another exception occurred: SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. The above exception was the direct cause of the following exception: ╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮ │ in :1                                                                                    │ │                                                                                                  │ │ /home/**/.conda/envs/dtest/lib/python3.9/multiprocessing/spawn.py:116 in spawn_main          │ │                                                                                                  │ │   113 │   │   resource_tracker._resource_tracker._fd = tracker_fd                                │ │   114 │   │   fd = pipe_handle                                                                   │ │   115 │   │   parent_sentinel = os.dup(pipe_handle)                                              │ │ ❱ 116 │   exitcode = _main(fd, parent_sentinel)                                                  │ │   117 │   sys.exit(exitcode)                                                                     │ │   118                                                                                            │ │   119                                                                                            │ │                                                                                                  │ │ /home/**/.conda/envs/dtest/lib/python3.9/multiprocessing/spawn.py:125 in _main               │ │                                                                                                  │ │   122 │   │   process.current_process()._inheriting = True                                       │ │   123 │   │   try:                                                                               │ │   124 │   │   │   preparation_data = reduction.pickle.load(from_parent)                          │ │ ❱ 125 │   │   │   prepare(preparation_data)                                                      │ │   126 │   │   │   self = reduction.pickle.load(from_parent)                                      │ │   127 │   │   finally:                                                                           │ │   128 │   │   │   del process.current_process()._inheriting                                      │ │                                                                                                  │ │ /home/**/.conda/envs/dtest/lib/python3.9/multiprocessing/spawn.py:236 in prepare             │ │                                                                                                  │ │   233 │   if 'init_main_from_name' in data:                                                      │ │   234 │   │   _fixup_main_from_name(data['init_main_from_name'])                                 │ │   235 │   elif 'init_main_from_path' in data:                                                    │ │ ❱ 236 │   │   _fixup_main_from_path(data['init_main_from_path'])                                 │ │   237                                                                                            │ │   238  Multiprocessing module helpers to fix up the main module in                              │ │   239  spawned subprocesses                                                                     │ │                                                                                                  │ │ /home/**/.conda/envs/dtest/lib/python3.9/multiprocessing/spawn.py:287 in                     │ │ _fixup_main_from_path                                                                            │ │                                                                                                  │ │   284 │    nonmain code that needs to be executed                                              │ │   285 │   old_main_modules.append(current_main)                                                  │ │   286 │   main_module = types.ModuleType(""__mp_main__"")                                          │ │ ❱ 287 │   main_content = runpy.run_path(main_path,                                               │ │   288 │   │   │   │   │   │   │   │     run_name=""__mp_main__"")                                  │ │   289 │   main_module.__dict__.update(main_content)                                              │ │   290 │   sys.modules['__main__'] = sys.modules['__mp_main__'] = main_module                     │ │                                                                                                  │ │ /home/**/.conda/envs/dtest/lib/python3.9/runpy.py:288 in run_path                            │ │                                                                                                  │ │   285 │   │    Not a valid sys.path entry, so run the code directly                             │ │   286 │   │    execfile() doesn't help as we want to allow compiled files                       │ │   287 │   │   code, fname = _get_code_from_file(run_name, path_name)                             │ │ ❱ 288 │   │   return _run_module_code(code, init_globals, run_name,                              │ │   289 │   │   │   │   │   │   │   │   pkg_name=pkg_name, script_name=fname)                      │ │   290 │   else:                                                                                  │ │   291 │   │    Finder is defined for path, so add it to                                         │ │                                                                                                  │ │ /home/**/.conda/envs/dtest/lib/python3.9/runpy.py:97 in _run_module_code                     │ │                                                                                                  │ │    94 │   fname = script_name if mod_spec is None else mod_spec.origin                           │ │    95 │   with _TempModule(mod_name) as temp_module, _ModifiedArgv0(fname):                      │ │    96 │   │   mod_globals = temp_module.module.__dict__                                          │ │ ❱  97 │   │   _run_code(code, mod_globals, init_globals,                                         │ │    98 │   │   │   │     mod_name, mod_spec, pkg_name, script_name)                               │ │    99 │    Copy the globals of the temporary module, as they                                    │ │   100 │    may be cleared when the temporary module goes away                                   │ │                                                                                                  │ │ /home/**/.conda/envs/dtest/lib/python3.9/runpy.py:87 in _run_code                            │ │                                                                                                  │ │    84 │   │   │   │   │      __loader__ = loader,                                                │ │    85 │   │   │   │   │      __package__ = pkg_name,                                             │ │    86 │   │   │   │   │      __spec__ = mod_spec)                                                │ │ ❱  87 │   exec(code, run_globals)                                                                │ │    88 │   return run_globals                                                                     │ │    89                                                                                            │ │    90 def _run_module_code(code, init_globals=None,                                              │ │                                                                                                  │ │ /scratch/**/viper_debug/viper_rl/scripts/train_dreamer.py:25 in                      │ │                                                                                                  │ │    22                                                                                            │ │    23 from viper_rl.dreamerv3 import embodied                                                    │ │    24 from viper_rl.dreamerv3.embodied import wrappers                                           │ │ ❱  25 from train_videogpt import collect_data                                                    │ │    26 from flax.training import checkpoints                                                      │ │    27                                                                                            │ │    28 def main(argv=None):                                                                       │ │                                                                                                  │ │ /scratch/**/viper_debug/viper_rl/scripts/train_videogpt.py:25 in                     │ │                                                                                                  │ │    22 directory = directory.parent                                                               │ │    23 sys.path.append(str(directory.parent))                                                     │ │    24                                                                                            │ │ ❱  25 from viper_rl.videogpt.models import AE, VideoGPT                                          │ │    26 from viper_rl.videogpt.sampler import VideoGPTSampler                                      │ │    27 from viper_rl.videogpt.data import load_dataset                                            │ │    28 from viper_rl.videogpt.train_utils import init_model_state_videogpt, get_first_device, P   │ │                                                                                                  │ │ /scratch/**/viper_debug/viper_rl/viper_rl/videogpt/models/__init__.py:12 in          │ │                                                                                                  │ │     9                                                                                            │ │    10 from .vqgan import VQGAN                                                                   │ │    11 from .videogpt import VideoGPT                                                             │ │ ❱  12 from .stylegan_disc import StyleGANDisc                                                    │ │    13 from .vqgan import VQGAN                                                                   │ │    14                                                                                            │ │    15                                                                                            │ │                                                                                                  │ │ /scratch/**/viper_debug/viper_rl/viper_rl/videogpt/models/stylegan_disc.py:174 in    │ │                                                                                                  │ │   171 │   return jnp.concatenate((x, y_std), axis=3)                                             │ │   172                                                                                            │ │   173                                                                                            │ │ ❱ 174 class DiscriminatorBlock(nn.Module):                                                       │ │   175 │   in_features: int                                                                       │ │   176 │   out_features: int                                                                      │ │   177 │   activation_function: ActivationFunction = jnn.leaky_relu                               │ │                                                                                                  │ │ /scratch/**/viper_debug/viper_rl/viper_rl/videogpt/models/stylegan_disc.py:178 in            │ │ DiscriminatorBlock                                                                               │ │                                                                                                  │ │   175 │   in_features: int                                                                       │ │   176 │   out_features: int                                                                      │ │   177 │   activation_function: ActivationFunction = jnn.leaky_relu                               │ │ ❱ 178 │   resample_kernel: jnp.ndarray = jnp.array([1, 3, 3, 1])                                 │ │   179 │   dtype: jnp.dtype = jnp.float32                                                         │ │   180 │                                                                                          │ │   181 │   def setup(self):                                                                       │ │                                                                                                  │ │ /home/**/.conda/envs/dtest/lib/python3.9/sitepackages/jax/_src/numpy/lax_numpy.py:3214 in   │ │ array                                                                                            │ │                                                                                                  │ │   3211   else:                                                                                   │ │   3212 │   raise TypeError(f""Unexpected input type for array: {type(object)}"")                   │ │   3213                                                                                           │ │ ❱ 3214   out_array: Array = lax_internal._convert_element_type(                                  │ │   3215 │     out, dtype, weak_type=weak_type)                                                    │ │   3216   if ndmin > ndim(out_array):                                                             │ │   3217 │   out_array = lax.expand_dims(out_array, range(ndmin  ndim(out_array)))                │ │                                                                                                  │ │ /home/**/.conda/envs/dtest/lib/python3.9/sitepackages/jax/_src/lax/lax.py:559 in            │ │ _convert_element_type                                                                            │ │                                                                                                  │ │    556 │   │      isinstance(core.get_aval(operand), core.ConcreteArray))):                      │ │    557 │   return type_cast(Array, operand)                                                      │ │    558   else:                                                                                   │ │ ❱  559 │   return convert_element_type_p.bind(operand, new_dtype=new_dtype,                      │ │    560 │   │   │   │   │   │   │   │   │      weak_type=bool(weak_type))                         │ │    561                                                                                           │ │    562 def bitcast_convert_type(operand: ArrayLike, new_dtype: DTypeLike) > Array:              │ │                                                                                                  │ │ /home/**/.conda/envs/dtest/lib/python3.9/sitepackages/jax/_src/core.py:416 in bind          │ │                                                                                                  │ │    413   def bind(self, *args, **params):                                                        │ │    414 │   assert (not config.enable_checks.value or                                             │ │    415 │   │   │   all(isinstance(arg, Tracer) or valid_jaxtype(arg) for arg in args)), args     │ │ ❱  416 │   return self.bind_with_trace(find_top_trace(args), args, params)                       │ │    417                                                                                           │ │    418   def bind_with_trace(self, trace, args, params):                                         │ │    419 │   with pop_level(trace.level):                                                          │ │                                                                                                  │ │ /home/**/.conda/envs/dtest/lib/python3.9/sitepackages/jax/_src/core.py:420 in               │ │ bind_with_trace                                                                                  │ │                                                                                                  │ │    417                                                                                           │ │    418   def bind_with_trace(self, trace, args, params):                                         │ │    419 │   with pop_level(trace.level):                                                          │ │ ❱  420 │     out = trace.process_primitive(self, map(trace.full_raise, args), params)            │ │    421 │   return map(full_lower, out) if self.multiple_results else full_lower(out)             │ │    422                                                                                           │ │    423   def def_impl(self, impl):                                                               │ │                                                                                                  │ │ /home/**/.conda/envs/dtest/lib/python3.9/sitepackages/jax/_src/core.py:921 in               │ │ process_primitive                                                                                │ │                                                                                                  │ │    918 │     from jax.experimental.key_reuse._core import call_impl_with_key_reuse_checks   py  │ │    919 │     return call_impl_with_key_reuse_checks(primitive, primitive.impl, *tracers, **para  │ │    920 │   else:                                                                                 │ │ ❱  921 │     return primitive.impl(*tracers, **params)                                           │ │    922                                                                                           │ │    923   def process_call(self, primitive, f, tracers, params):                                  │ │    924 │   if config.debug_key_reuse.value:                                                      │ │                                                                                                  │ │ /home/**/.conda/envs/dtest/lib/python3.9/sitepackages/jax/_src/dispatch.py:87 in            │ │ apply_primitive                                                                                  │ │                                                                                                  │ │    84    triggering the disable jit path instead of messing around with it here.                │ │    85   prev = lib.jax_jit.swap_thread_local_state_disable_jit(False)                            │ │    86   try:                                                                                     │ │ ❱  87 │   outs = fun(*args)                                                                      │ │    88   finally:                                                                                 │ │    89 │   lib.jax_jit.swap_thread_local_state_disable_jit(prev)                                  │ │    90   return outs                                                                              │ ╰──────────────────────────────────────────────────────────────────────────────────────────────────╯ RuntimeError: Unable to initialize backend 'cuda': INTERNAL: no supported devices found for platform CUDA (you may need to uninstall the failing plugin package, or set JAX_PLATFORMS=cpu to skip this backend.) ``` I have already tried updating the jax config jax_platform to gpu after this error happened but it raises pretty much the same issue without the entire traceback of course. To explain this a little more:  When I activate the conda environment and import jax to make some testing calculations it just works fine and the output of jax.devices() shows the cuda device with ID 0. When I then run the script that causes this issue and check jax.devices() afterwards it only shows CPUdevice and no cuda device any longer and after that as explained I cant even set the platform back to GPU. However with CPUdevice it still crashes cause it is trying to run on a cuda device but I kinda suspect that once the issue happens it will be stuck in some bad state no matter what I do. Setting the platform for jax to cpu did not even work and jax.devices() still only showed the cuda device. Im also wondering why jax.devices() always shows only one of them but never that both are available. This Issue seems to be strongly related to JAX so I am hoping I can get help from here. For clarification, the code I am trying to run is related to the video prediction model viper from here https://github.com/neuronphysics/viper_rl and specifically this issue happens when trying to run the train_dreamer.py script within the scripts folder. Any help would be much appreciated as fixing this issue is of high priority to me.  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.30 jaxlib: 0.4.30 numpy:  1.24.1 python: 3.9.19  (main, Mar 20 2024, 12:50:21)  [GCC 12.3.0] jax.devices (1 total, 1 local): [cuda(id=0)] process_count: 1 NVIDIASMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4",2024-09-19T20:47:07Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/23770,It could be that the GPU is in exclusive process mode. Could you try running `nvidiasmi c 0`?,"> It could be that the GPU is in exclusive process mode. Could you try running `nvidiasmi c 0`? Hello, thanks for the recommendation, unfortunately I have rights on the machine so I cannot change de Compute Mode but I can confirm that at least the GPU that was assigned to my job just now was set as Exclusive Process and I see a gpu on default but im not sure if or how I could force SLURM to assign such a GPU to my job",sevcik I tried it out on my PC and found out that you are right. Its the exclusive process mode that causes this issue and after some searching i found hardware that is already set to default on slurm so I am now just requesting these GPUs and it works fine. Thanks
rag,Added `is_signed` to `mgpu.FragmentedArray`,"Added `is_signed` to `mgpu.FragmentedArray` The registers within a fragmented array always use signless types, and instead the signedness is tracked on the fragmented arrays itself (i.e. in Python).",2024-09-19T15:16:08Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23764
rag,Added comparison operators to `mgpu.FragmentedArray`,Added comparison operators to `mgpu.FragmentedArray`,2024-09-18T21:41:46Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23742
yi,Improve documentation for jnp.digitize,Part of CC(Tracking issue: inline docstrings); followup to CC(Add underlying method argument to jax.numpy.digitize),2024-09-18T19:00:12Z,documentation pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/23737
yi,Add `donate` and `may_alias` as an argument to `device_put` to allow for donation and aliasing.,"Add `donate` and `may_alias` as an argument to `device_put` to allow for donation and aliasing. The end state we want to work towards is to remove `may_alias` and **always copy by default**. But there is some work needed to get to that state. **Definition:** * donate: The input buffer will be marked as deleted (see below for some caveats). The output buffer may or may not reuse the input buffer's underlying memory. * may_alias: If True, we may return the original buffer depending on the implementation. **What problem are we solving?** Eventually, we want `device_put` to always copy so introducing `may_alias` as a transition state to help towards that goal. We might end up deciding to keep `may_alias` but now you have an explicit option to **always copy** i.e. set `may_alias=False` which is what some users want. Adding `donate` allows users to avoid this pattern of code: ``` inp = ... out = device_put(inp, sharding) jax.block_until_ready(out) jax.tree.map(lambda x: x.delete(), inp) ``` Now it can just be: `jax.device_put(inp, sharding, donate=True)` **So what are the semantics of these 2 options?** Let's create a table:  `donate` is best effort for now until we fix the following things:  * Delete input when `donate=True` regardless of whether XLA could donate or not. This will affect `jax.jit` too but it's a good thing to do.  * Plumb donate to PJRT/IFRT APIs so we can donate where transfers are not happening via `jit`.",2024-09-18T18:09:42Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23733
rag,Added a missing branch to `mgpu.FragmentedArray.astype`,"Added a missing branch to `mgpu.FragmentedArray.astype` Previously, an unsupported cast produced a `NameError` instead.",2024-09-18T15:30:23Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23724
yi,"When calculating the loss, the input data does not contain NaN, but the output contains NaN"," Description Please specify cuda:0 at the very beginning. ```python import torch import numpy as np import os import jax import jax import jax.numpy as jnp from jax import ops as jops from jax.nn import one_hot, sigmoid from jax import lax import jax.scipy.special as sc import optax if ""CONTEXT_DEVICE_TARGET"" in os.environ and os.environ['CONTEXT_DEVICE_TARGET'] == 'GPU':     devices = os.environ['CUDA_VISIBLE_DEVICES'].split("","")     device = devices[2]     final_device = ""cuda:"" + device else:     final_device = 'cpu' from network.cv.yolov3_darknet53.Yolov3_torch import YOLOV3DarkNet53 as yolov3_torch def loss_yolo_jax():     from network.cv.yolov4.yolov4_pytorch import yolov4loss_jax     yolo_obj = yolov4loss_jax()     return yolo_obj y_true_0 = np.load('./output_torch[0][0].npy') yolo_out1 = torch.from_numpy(y_true_0).to(final_device) y_true_0 = np.load('./output_torch[0][1].npy') yolo_out2 = torch.from_numpy(y_true_0).to(final_device) y_true_0 = np.load('./output_torch[0][2].npy') yolo_out3 = torch.from_numpy(y_true_0).to(final_device) y_true_0 = np.load('./output_torch[1][0].npy') yolo_out4 = torch.from_numpy(y_true_0).to(final_device) y_true_0 = np.load('./output_torch[1][1].npy') yolo_out5 = torch.from_numpy(y_true_0).to(final_device) y_true_0 = np.load('./output_torch[1][2].npy') yolo_out6 = torch.from_numpy(y_true_0).to(final_device) y_true_0 = np.load('./output_torch[2][0].npy') yolo_out7 = torch.from_numpy(y_true_0).to(final_device) y_true_0 = np.load('./output_torch[2][1].npy') yolo_out8 = torch.from_numpy(y_true_0).to(final_device) y_true_0 = np.load('./output_torch[2][2].npy') yolo_out9 = torch.from_numpy(y_true_0).to(final_device) yolo_out = ((yolo_out1,yolo_out2,yolo_out3),(yolo_out4,yolo_out5,yolo_out6),(yolo_out7,yolo_out8,yolo_out9)) batch_y_true_0_torch = np.load('./batch_y_true_0_torch.npy') batch_y_true_0_torch = torch.from_numpy(batch_y_true_0_torch).to(final_device) batch_y_true_1_torch = np.load('./batch_y_true_1_torch.npy') batch_y_true_1_torch = torch.from_numpy(batch_y_true_1_torch).to(final_device) batch_y_true_2_torch = np.load('./batch_y_true_2_torch.npy') batch_y_true_2_torch = torch.from_numpy(batch_y_true_2_torch).to(final_device) batch_gt_box0_torch = np.load('./batch_gt_box0_torch.npy') batch_gt_box0_torch = torch.from_numpy(batch_gt_box0_torch).to(final_device) batch_gt_box1_torch = np.load('./batch_gt_box1_torch.npy') batch_gt_box1_torch = torch.from_numpy(batch_gt_box1_torch).to(final_device) batch_gt_box2_torch = np.load('./batch_gt_box2_torch.npy') batch_gt_box2_torch = torch.from_numpy(batch_gt_box2_torch).to(final_device) input_shape = np.load('./input_shape.npy') input_shape = torch.from_numpy(input_shape).to(final_device) model_pt = yolov3_torch(is_training=True) model_pt.train() model_torch = model_pt.to(final_device) params_torch = {key: value.detach().cpu().numpy() for key, value in model_torch.state_dict().items()} loss_jax_fun = loss_yolo_jax() params_jax = {name: jnp.array(value, dtype=jnp.float32) for name, value in params_torch.items()} loss_jax, jax_grads = jax.value_and_grad(loss_jax_fun.calc_loss)(params_jax, yolo_out,                                                                     batch_y_true_0_torch, batch_y_true_1_torch,                                                                     batch_y_true_2_torch, batch_gt_box0_torch,                                                                     batch_gt_box1_torch,                                                                     batch_gt_box2_torch, input_shape) yolo_out1 = torch.isnan(yolo_out1).any() print('yolo_out1;',yolo_out1)  yolo_out2 = torch.isnan(yolo_out2).any() print('yolo_out2;',yolo_out2) yolo_out3 = torch.isnan(yolo_out3).any() print('yolo_out3;',yolo_out3)  yolo_out4 = torch.isnan(yolo_out4).any() print('yolo_out4;',yolo_out4)  yolo_out5 = torch.isnan(yolo_out5).any() print('yolo_out5;',yolo_out5) yolo_out6 = torch.isnan(yolo_out6).any() print('yolo_out6;',yolo_out6)  yolo_out7 = torch.isnan(yolo_out7).any() print('yolo_out7;',yolo_out7)  yolo_out8 = torch.isnan(yolo_out8).any() print('yolo_out8;',yolo_out8)  yolo_out9 = torch.isnan(yolo_out9).any() print('yolo_out9;',yolo_out9)  batch_y_true_0_torch = torch.isnan(batch_y_true_0_torch).any() print('batch_y_true_0_torch;',batch_y_true_0_torch)   batch_y_true_1_torch = torch.isnan(batch_y_true_1_torch).any() print('batch_y_true_1_torch;',batch_y_true_1_torch)  batch_y_true_2_torch = torch.isnan(batch_y_true_2_torch).any() print('batch_y_true_2_torch;',batch_y_true_2_torch) batch_gt_box0_torch = torch.isnan(batch_gt_box0_torch).any() print('batch_gt_box0_torch;',batch_gt_box0_torch)  batch_gt_box1_torch = torch.isnan(batch_gt_box1_torch).any() print('batch_gt_box1_torch;',batch_gt_box1_torch) batch_gt_box2_torch = torch.isnan(batch_gt_box2_torch).any() print('batch_gt_box2_torch;',batch_gt_box2_torch)  input_shape = torch.isnan(input_shape).any() print('input_shape;',input_shape)  print('loss_torch_result;',np.array(loss_jax))  ``` !屏幕截图 20240918 184040  System info (python version, jaxlib version, accelerator, etc.) Code and data links:https://drive.google.com/file/d/1lxHI_OQwjSUCj7vszNIzl_NmkyVF6JQu/view?usp=sharing",2024-09-18T10:42:58Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/23717,"Hi  if I'm not mistaken, this looks like a duplicate of CC(When calculating the loss, the input data does not contain NaN, but the output contains NaN). Let's close this one and continue the discussion there."
yi,Add get_replication to shard_map.py for verifying if an array is replicated.,Add get_replication to shard_map.py for verifying if an array is replicated.,2024-09-17T20:29:38Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23706
llm,cuDNN attention segmentation fault," Description Run ``` import jax import jax.numpy as jnp dtype = jnp.bfloat16 batch_dim = 16 x_shape = (batch_dim, 512, 16, 48) bias_shape = (batch_dim, 16, 512, 512) mask_shape = (1, 1, 512) x = jnp.ones(x_shape, dtype=dtype) bias = jnp.ones(bias_shape, dtype=dtype) mask = jnp.ones(mask_shape, dtype=jnp.bool_) def attention(x, bias, mask):   return jax.nn.dot_product_attention(       query=x,       key=x,       value=x,       bias=bias,       mask=mask,       is_causal=False,       implementation=""cudnn"",   ) attention = jax.vmap(attention, in_axes=(0, 0, None)) .jit def attention_vjp(x, bias, mask):   _, f_vjp = jax.vjp(attention, x, bias, mask)   return f_vjp(x) attention_vjp(x, bias, mask) ``` produces ``` *** SIGSEGV (), see go/stacktracess15 received by PID 9235 (TID 9235) on cpu 115; stack trace: *** PC: @     0x5617bac25898  (unknown)  memcpy_prefetch     @     0x5617cb88d7b1       1888  FailureSignalHandler()     @     0x7f638a212e80  1678969448  (unknown)     @     0x5617bac25898          8  memcpy_prefetch     @     0x5617aebc89ee        144  libcudnn_engines_runtime_compiled_static_1b8578937b0ce03a7e4b20267fe9d0337678c78c     @     0x5617aebc95b6        416  libcudnn_engines_runtime_compiled_static_9c37820fea3a2d0a85881f91ee33655956ce9c93     @     0x5617aee7e795      66928  cudnn::backend::execute()     @     0x5617aee7fc3f      65632  cudnnBackendExecute     @     0x5617c65ffe99        448  cudnn_frontend::detail::execute()     @     0x5617c713e1b8        320  cudnn_frontend::ICudnn::execute_cudnn_plan_with_uid()     @     0x5617c70926c3        224  cudnn_frontend::graph::Graph::execute()     @     0x5617c70917fd        496  stream_executor::gpu::CudnnGraph::Execute()     @     0x5617c83de8bb        192  stream_executor::gpu::GpuCommandBuffer::Trace()     @     0x5617c191dda7        112  stream_executor::TraceCommandBufferFactory::Create()     @     0x5617c18f3224        320  xla::gpu::TracedCommandBuffer::GetOrTraceCommandBuffer()     @     0x5617c18f367e        160  xla::gpu::TracedCommandBufferCmd::AddTracedCommandBuffer()     @     0x5617c18fc0d8        192  xla::gpu::CuDnnCmd::Record()     @     0x5617c18f2365        160  xla::gpu::CommandBufferCmdSequence::Record()     @     0x5617c18eeab2        480  xla::gpu::CommandBufferThunk::Initialize()     @     0x5617c19242ec         48  xla::gpu::SequentialThunk::Initialize()     @     0x5617c18e203b       1216  xla::gpu::(anonymous namespace)::ExecuteThunks()     @     0x5617c18ded4c       2288  xla::gpu::GpuExecutable::ExecuteAsyncOnStreamImpl()     @     0x5617c18ddc4f         80  xla::gpu::GpuExecutable::ExecuteAsyncOnStream()     @     0x5617c76260db       1056  xla::Executable::ExecuteAsyncOnStreamWrapper()     @     0x5617c0015714       2640  xla::LocalExecutable::RunAsync()     @     0x5617c0015cb5        288  xla::LocalExecutable::RunAsync()     @     0x5617bffd5a7a       2640  xla::PjRtStreamExecutorLoadedExecutable::EnqueueExecution()     @     0x5617bffd7e08       3344  xla::PjRtStreamExecutorLoadedExecutable::ExecuteHelper()     @     0x5617bffda79a        512  xla::PjRtStreamExecutorLoadedExecutable::Execute()     @     0x5617c014ad4c        960  xla::ifrt::PjRtLoadedExecutable::Execute()     @     0x5617bff8e709        832  xla::(anonymous namespace)::ExecuteShardedOnLocalDevicesInternal()     @     0x5617bff90524         96  xla::PyLoadedExecutable::ExecuteSharded()     @     0x7f6379b6c205        256  xla::ValueOrThrowWrapper::operator()()     @     0x7f6379b6bf6d        288  nanobind::detail::func_create()::{lambda() CC(Python 3 compatibility issues)}::__invoke()     @     0x5617c01e8ee5        368  nanobind::detail::nb_func_vectorcall_complex()     @     0x5617b93b0fb5         80  PyObject_Vectorcall     @     0x5617b93a7043        400  _PyEval_EvalFrameDefault     @     0x5617b93a2e4d        160  _PyEval_Vector     @     0x5617b93a97df        400  _PyEval_EvalFrameDefault     @     0x5617b93a2e4d        160  _PyEval_Vector     @     0x5617cbc0d70e         80  _PyObject_FastCallDictTstate     @     0x5617cbc0de3b        128  _PyObject_Call_Prepend     @     0x5617cbc7594f         64  slot_tp_call     @     0x5617b93b1b96         80  PyObject_Call     @     0x5617b93a97df        400  _PyEval_EvalFrameDefault     @     0x5617b93a2e4d        160  _PyEval_Vector     @     0x5617b93b1c5f         80  PyObject_Call     @     0x5617b93a97df        400  _PyEval_EvalFrameDefault     @     0x5617b93a2e4d        160  _PyEval_Vector     @     0x5617b93b0fb5         80  PyObject_Vectorcall     @     0x5617bcdb753d       1824  jax::(anonymous namespace)::PjitFunction::Call()     @     0x5617bcdb66b4        224  PjitFunction_tp_vectorcall     @     0x5617b93a97df        400  _PyEval_EvalFrameDefault     @     0x5617b93a2e4d        160  _PyEval_Vector     @     0x5617b93b1c5f         80  PyObject_Call     @     0x5617b93a97df        400  _PyEval_EvalFrameDefault     @     0x5617b93a2e4d        160  _PyEval_Vector     @     0x5617cbc0fb1e        128  method_vectorcall     @     0x5617b93b1c5f         80  PyObject_Call     @     0x5617b93a97df        400  _PyEval_EvalFrameDefault     @     0x5617b93a2e4d        160  _PyEval_Vector     @     0x5617b93a97df        400  _PyEval_EvalFrameDefault     @     0x5617b93a2e4d        160  _PyEval_Vector     @     0x5617b93a97df        400  _PyEval_EvalFrameDefault     @     0x5617b93a2e4d        160  _PyEval_Vector     @     0x5617b93b0fb5         80  PyObject_Vectorcall     @     0x5617bcdb753d       1824  jax::(anonymous namespace)::PjitFunction::Call()     @     0x5617bcdb66b4        224  PjitFunction_tp_vectorcall     @     0x5617b93b0fb5         80  PyObject_Vectorcall     @     0x5617b93a7043        400  _PyEval_EvalFrameDefault     @     0x5617b93a2e4d        160  _PyEval_Vector     @     0x5617b93b0fb5         80  PyObject_Vectorcall     @     0x5617c01de634         80  nanobind::detail::obj_vectorcall()     @     0x7f6372066d63         48  benchmark::internal::LambdaBenchmark::Run()     @     0x5617bad98de1         96  benchmark::internal::BenchmarkInstance::Run()     @     0x5617bada059a        256  benchmark::internal::(anonymous namespace)::RunInThread()     @     0x5617bada01c8        112  benchmark::internal::BenchmarkRunner::DoNIterations()     @     0x5617bada1401        672  benchmark::internal::BenchmarkRunner::DoOneRepetition()     @     0x5617bad935c5       6288  benchmark::RunSpecifiedBenchmarks()     @     0x5617bad928a6         48  benchmark::RunSpecifiedBenchmarks()     @     0x7f6372066da9         16  nanobind::detail::func_create()::{lambda() CC(Python 3 compatibility issues)}::__invoke()     @     0x5617c01e93d4        192  nanobind::detail::nb_func_vectorcall_simple()     @     0x5617b93b0fb5         80  PyObject_Vectorcall     @     0x5617b93a7043        400  _PyEval_EvalFrameDefault     @     0x5617b93a2e4d        160  _PyEval_Vector     @     0x5617cbd19aea        112  PyEval_EvalCode     @     0x5617cbd16089        112  builtin_exec     @     0x5617cbc55a59         64  cfunction_vectorcall_FASTCALL_KEYWORDS     @     0x5617b93b0fb5         80  PyObject_Vectorcall     @     0x5617b93a7043        400  _PyEval_EvalFrameDefault     @     0x5617b93a2e4d        160  _PyEval_Vector     @     0x5617cbc0dff1         96  _PyObject_CallFunctionVa     @     0x5617cbc0e3ec        240  PyObject_CallMethod     @     0x5617cb8e9d52        832  devtools::python_launcher::Launcher_Main()     @     0x7f638a0663d4        192  __libc_start_main     @     0x5617abcd7d5a  (unknown)  _start E0917 11:28:52.340728    9235 coredump_hook.cc:280] RAW: Remote crash gathering disabled for unit test. Enable that with remote_coredump_enabled_for_unit_test flag Fatal Python error: Segmentation fault Current thread 0x00007f638a000000 (most recent call first):   File ""/build/work/80eac4b44c5f7f6a8112b8791876b518bce1/google3/runfiles/google3/third_party/py/jax/_src/interpreters/pxla.py"", line 1285 in __call__   File ""/build/work/80eac4b44c5f7f6a8112b8791876b518bce1/google3/runfiles/google3/third_party/py/jax/_src/profiler.py"", line 333 in wrapper   File ""/build/work/80eac4b44c5f7f6a8112b8791876b518bce1/google3/runfiles/google3/third_party/py/jax/_src/pjit.py"", line 1648 in _pjit_call_impl_python   File ""/build/work/80eac4b44c5f7f6a8112b8791876b518bce1/google3/runfiles/google3/third_party/py/jax/_src/pjit.py"", line 1694 in call_impl_cache_miss   File ""/build/work/80eac4b44c5f7f6a8112b8791876b518bce1/google3/runfiles/google3/third_party/py/jax/_src/pjit.py"", line 1712 in _pjit_call_impl   File ""/build/work/80eac4b44c5f7f6a8112b8791876b518bce1/google3/runfiles/google3/third_party/py/jax/_src/core.py"", line 949 in process_primitive   File ""/build/work/80eac4b44c5f7f6a8112b8791876b518bce1/google3/runfiles/google3/third_party/py/jax/_src/core.py"", line 443 in bind_with_trace   File ""/build/work/80eac4b44c5f7f6a8112b8791876b518bce1/google3/runfiles/google3/third_party/py/jax/_src/core.py"", line 2777 in bind   File ""/build/work/80eac4b44c5f7f6a8112b8791876b518bce1/google3/runfiles/google3/third_party/py/jax/_src/pjit.py"", line 189 in _python_pjit_helper   File ""/build/work/80eac4b44c5f7f6a8112b8791876b518bce1/google3/runfiles/google3/third_party/py/jax/_src/pjit.py"", line 331 in cache_miss   File ""/build/work/80eac4b44c5f7f6a8112b8791876b518bce1/google3/runfiles/google3/third_party/py/jax/_src/traceback_util.py"", line 180 in reraise_with_filtered_traceback   File ""/build/work/80eac4b44c5f7f6a8112b8791876b518bce1/google3/runfiles/google3/experimental/users/sbodenstein/benchmarks/microbenchmarks/cudnn_attention.py"", line 59 in matmul_benchmark   File ""/build/work/80eac4b44c5f7f6a8112b8791876b518bce1/google3/runfiles/google3/third_party/benchmark/bindings/python/google_benchmark/__init__.py"", line 130 in _run_benchmarks   File ""/build/work/80eac4b44c5f7f6a8112b8791876b518bce1/google3/runfiles/google3/third_party/py/absl/app.py"", line 404 in _run_main   File ""/build/work/80eac4b44c5f7f6a8112b8791876b518bce1/google3/runfiles/google3/third_party/py/absl/app.py"", line 484 in run   File ""/build/work/80eac4b44c5f7f6a8112b8791876b518bce1/google3/runfiles/google3/third_party/benchmark/bindings/python/google_benchmark/__init__.py"", line 134 in main   File ""/build/work/80eac4b44c5f7f6a8112b8791876b518bce1/google3/runfiles/google3/experimental/users/sbodenstein/benchmarks/microbenchmarks/cudnn_attention.py"", line 68 in    File """", line 35 in _run_code_in_main   File """", line 153 in run_filename_as_main Extension modules: jax.jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, google3.base.python.clif.cpp_flag, google3.base.python.sysinfo, google3.base.python.user_name, google3.base.python.clif.logging, google3.third_party.pybind11_abseil.ok_status_singleton, google3.base.python.clif.googleinit, google3.base.python.absl_faulthandler, google3.net.util.python._bnsutil, google3.third_party.upb.python._message, google3.monitoring.streamz.exporter._monitoring_streamz_exporter_pywrapmetricops, google3.util.task.python._util_task_python_pywrap__task, google3.base.python.clif.tracecontext_local, google3.perftools.tracing.public.python._tracecontext_util, google3.perftools.tracing.public.python.trace_url_builder, google3.learning.debug.error_log.python.logwriters_registered_check, google3.logs.writer.python._logs_writer_python_pywraplogwriter, google3.tech.env.client.python.envelope_client, google3.learning.deepmind.analytics.lamon.python.lamon, google3.learning.deepmind.python.gpu_monitor.clif.gpu_monitor (total: 34) E0917 11:28:52.631628    9235 process_state.cc:805] RAW: Raising signal 11 with default behavior /build/work/80eac4b44c5f7f6a8112b8791876b518bce1/google3/tools/test/remoteruntest.sh: line 263:  9235 Segmentation fault      ""$@""  20240917 11:28:52 PDT Forge runner: Test killed by SIGSEGV while running on glcpl1.prod.google.com ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.34 jaxlib: 0.4.34 numpy:  1.26.3 python: 3.11.8  jax.devices (1 total, 1 local): [CudaDevice(id=0)] process_count: 1 $ nvidiasmi Tue Sep 17 11:31:20 2024        ++  ++ ```",2024-09-17T18:33:06Z,bug NVIDIA GPU,open,0,11,https://github.com/jax-ml/jax/issues/23701, ,"I just created a PR to fix/workaround this longstanding cuDNN limitation with dbias. cuDNN only supports dbias when the batch size is 1 (here). For batch sizes greater than 1, the cuDNN SDPA API returns an allzero dbias tensor (which isn't ideal, but that's the current behavior). When using vmap, the API only detects a singleton batch before the vmap is applied, causing it to mistakenly set has_dbias to `True`, which leads to this SegFault. This PR resolves the issue by resetting has_dbias to False, and returns an allzero dbias tensor as in the nonvmap version. To summarize, the behavior is: ``` no vmap: bias (1, ...) => has_dbias=True => OK bias (B, ...) => has_dbias=False => OK, but dbias is allzero vmap: bias (1, ...) => has_dbias=True => OK bias (B, ...) => has_dbias=True => Segfault  which is fixed to be bias (B, ...) => has_dbias=False => OK, but dbias is allzero ```","Also,  for comments on the dbias support from cudnn.",Having d_bias be zeroes when there is a batch dimension is definitely wrong behaviour: it seems like a silent failure that would be extremely difficult to debug for users when their training curves just don't look right. I think we should fail in this case rather until cuDNN supports this.,"We actually had some internal discussions earlier. I think the dilemma is this: it seems that some models don't require `d_bias` but still want to benefit from cuDNN flash attention. If we simply throw an error when the bias has a batch size other than 1, they might complain. Ideally, if we could detect whether `d_bias` is needed, we could decide whether to error out or proceed. However, it seems that no such mechanism exists in JAX. Instead, we currently have silent allzero biases when it's not supported (which is very bad, I know....). Do you think issuing a warning would help? Or should we just throw an error whenever cuDNN+bias is used with its batch size larger than 1?","I think that this should either work correctly, or it should throw an error. It should definitely not seg fault or give incorrect gradients (even with a warning). The latter is just too dangerous for users, who expect JAX APIs to do what they think they will do, or waste massive compute on runs with a major bug. Would you agree ?","I do. Wrong outputs aren't ok, because they are the kind of thing that makes people lose trust in a library.","Sure, I will essentially move this logic to the public API to throw an error for the cudnn exec path.","By the way, do you think we should apply this errorthrowing behavior to the public API or the cuDNN API? Perhaps it should only be applied to the public API, allowing power users who are certain they don't have d_bias to use the private cuDNN API.","Private APIs (`jax._src`, etc.) can do whatever you like. If a private API breaks, you get to keep both pieces.","I have pushed a new change to the PR to check the validity of the bias shape. It will throw an error only when the bias is invalid and bprop is applied. Also, the original segfault issues is fixed in a separate PR.  can you take a look?"
yi,OptimSteps not compatible with shard_map due to lax.cond," Description When trying to use optax.MultiSteps on a dataparallel setup with shard_map, I am getting the following error: ``` NotImplementedError: No replication rule for cond. As a workaround, pass the `check_rep=False` argument to `shard_map`. To get this fixed, open an issue at https://github.com/google/jax/issues ``` A simple example can be found below: ```python optimizer = optax.MultiSteps(optax.adamw(learning_rate), every_k_schedule=10) ... (shard_map, mesh=mesh, in_specs=(P(), P(), P(""data"")), out_specs=(P(), P())) def make_step(key: jax.Array, state: State, data: Batch):     loss, grads = jax.value_and_grad(loss_fn)(state[""params""], data, key)     loss = jax.lax.pmean(loss, ""data"")     grads = jax.lax.pmean(grads, ""data"")     updates, new_opt_state = optim.update(grads, state[""opt_state""], state[""params""])     new_params = optax.apply_updates(state[""params""], updates)     return dict(params=new_params, opt_state=new_opt_state) ... for step, batch in enumerate(data_loader):     key, subkey = jax.random.split(key)     state = make_step(subkey, state, batch) ``` Disabling multistep makes the error go away.  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.33 jaxlib: 0.4.33 numpy:  1.26.4 python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] jax.devices (8 total, 8 local): [CudaDevice(id=0) CudaDevice(id=1) ... CudaDevice(id=6) CudaDevice(id=7)] process_count: 1 platform: uname_result(system='Linux', node='19222254171', release='6.2.037generic', version=' CC(Require protobuf 3.6.0 or later)~22.04.1Ubuntu SMP PREEMPT_DYNAMIC Thu Nov  2 18:01:13 UTC 2', machine='x86_64') $ nvidiasmi Tue Sep 17 13:08:32 2024        ++  ++ ```",2024-09-17T13:08:48Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/23693, can we write a general replication rule for cond?,"Yeah, should be easy! (In the sense of: ""We choose to ~go to the Moon~ implement cond rules in this decade, not because they are easy, but because ~they are hard~ we thought they would be easy"")",Is this a relatively easy change for a new contributor to help out with? I would be keen to learn more about JAX's internals. Thanks!,I think CC(Add a shard map replication rule for cond_p) fixed this!
rag,Add very simple batching support for ragged_dot.,Add very simple batching support for ragged_dot.,2024-09-17T07:33:40Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23685
rag,jax.numpy.nonzero extremely slow on sharded array," Description When jnp.nonzero (or jnp.argwhere or sparse.BCOO.fromdense) is run on a sparse array and that array is sharded (this doesn't happen without sharding), the nonzero operation is extremely slow. This is a little odd to me since there shouldn't be much (or any) communication between devices for this op since it can be trivially split entirely between the devices. I'm actually unsure if the op ever completes. I've let it run for 10+ mins with no result. ``` import jax import jax.numpy as jnp import numpy as np arr = jnp.zeros((2**14, 2**14))  generate along axis 0 in a ""ragged"" way axis_0_idxs = np.concatenate([(np.zeros((np.random.randint(16,64),),dtype=np.int64) + i) for i in range(2**14)]) axis_1_idxs = np.random.randint(0,2**141,(axis_0_idxs.shape[0])) arr = arr.at[axis_0_idxs, axis_1_idxs].set(1E2) from jax.experimental import mesh_utils from jax.sharding import PositionalSharding sharding = PositionalSharding(mesh_utils.create_device_mesh((jax.local_device_count(),1))) arr = jax.device_put(arr, sharding)  takes a few seconds np.nonzero(np.asarray(arr))  extremely slow jnp.nonzero(arr) ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.31 jaxlib: 0.4.31 numpy:  1.26.4 python: 3.11.9 (main, Apr 21 2024, 09:01:17) [GCC 9.4.0] jax.devices (2 total, 2 local): [CudaDevice(id=0) CudaDevice(id=1)] process_count: 1 platform: uname_result(system='Linux', node='m001', release='5.4.0190generic', version=' CC(Update XLA release to include XLA Gather and Scatter Python bindings..)Ubuntu SMP Fri Jul 5 17:03:38 UTC 2024', machine='x86_64') $ nvidiasmi Mon Sep 16 16:49:16 2024        ++  ++ ```",2024-09-16T21:55:48Z,bug,open,0,2,https://github.com/jax-ml/jax/issues/23675,"Seems related to the size calculation which would explain a bunch of communication overhead making the op slow, JITing it with a specified size speeds up computation significantly (it is still much much slower than numpy on CPU)","After a warmup loop, it's pretty quick: (tested on a TPU) !image"
yi,Add underlying method argument to jax.numpy.digitize,Fixes CC(jax.numpy.digitize doesn't work with shape polymorphism) by providing a means to choose an underlying searchsorted implementation compatible with shape poly.,2024-09-16T18:33:10Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/23669
rag,Bump build from 1.2.1 to 1.2.2,"Bumps build from 1.2.1 to 1.2.2.  Release notes Sourced from build's releases.  Version 1.2.2  What's Changed  Add editable to builder.get_requries_for_build's static types (PR  CC(Gradient of `tanh` sometimes causes invalid values), fixes issue  CC(How to use `lax.scan()` with `xs` as a tuple?)) Include artifact attestations in our release (PR  CC(Improve behavior of a number of math functions for extreme inputs.)) Fix typing compatibility with typed pyprojecthooks (PR  CC(cuda failed to allocate errors)) Mark more tests with network (PR  CC(Simplify pyval result handler.)) Add more intersphinx links to docs (PR  CC(xla in pmap fails (i.e. jitofpmap or lax.scan with collectives))) Make uv optional for tests (PR  CC(namedtuple subclass transparency (fixes 806)) and  CC(Add documentation on asynchronous dispatch.))  New Contributors  @​carlwgeorge made their first contribution in pypa/build CC(Simplify pyval result handler.) @​edgarrmondragon made their first contribution in pypa/build CC(xla in pmap fails (i.e. jitofpmap or lax.scan with collectives))  Full Changelog: https://github.com/pypa/build/compare/1.2.1...1.2.2    Changelog Sourced from build's changelog.  1.2.2 (20240906)  Add editable to builder.get_requries_for_build's static types (PR :pr:764, fixes issue :issue:763) Include artifact attestations in our release (PR :pr:782) Fix typing compatibility with typed pyprojecthooks (PR :pr:788) Mark more tests with network (PR :pr:808) Add more intersphinx links to docs (PR :pr:804) Make uv optional for tests (PR :pr:807 and :pr:813)     Commits  3b0b5d0 docs: changelog for 1.2.2 ( CC(Scatter update seems to not handle symbolic zeros.)) b44a886 docs: more info in README 8e19948 build(deps): bump actions/attestbuildprovenance in the actions group ( CC(np.dot for integer arrays)) b90956c tests: add module case to uv detection ( CC(Add documentation on asynchronous dispatch.)) e79f1b3 ci: remove bot comments from generated release notes ( CC(`lax.scan` is ~6x slower to run than handwritten loops)) f6da25a precommit: bump repositories ( CC(Check failed: shape.has_layout() && LayoutUtil::IsMonotonicWithDim0Major(shape.layout()))) 9a52c50 tests: optional uv ( CC(namedtuple subclass transparency (fixes 806))) 553b700 docs: Add a few intersphinx links to the Python Packaging User Guide ( CC(xla in pmap fails (i.e. jitofpmap or lax.scan with collectives))) 336efcb build(deps): bump actions/attestbuildprovenance in the actions group ( CC(REPL latency issue)) 73b7213 tests: mark more network tests ( CC(Simplify pyval result handler.)) Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-09-16T17:44:25Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23664,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
yi,Bump numpy from 1.26.4 to 2.1.1,"Bumps numpy from 1.26.4 to 2.1.1.  Release notes Sourced from numpy's releases.  2.1.1 (Sep 3, 2024) NumPy 2.1.1 Release Notes NumPy 2.1.1 is a maintenance release that fixes bugs and regressions discovered after the 2.1.0 release. The Python versions supported by this release are 3.103.13. Contributors A total of 7 people contributed to this release. People with a &quot;+&quot; by their names contributed a patch for the first time.  Andrew Nelson Charles Harris Mateusz Sokół Maximilian Weigand + Nathan Goldbaum Pieter Eendebak Sebastian Berg  Pull requests merged A total of 10 pull requests were merged for this release.   CC(未找到相关数据): REL: Prepare for the NumPy 2.1.0 release [wheel build]  CC(未找到相关数据): MAINT: prepare 2.1.x for further development  CC(未找到相关数据): BUG: revert unintended change in the return value of set_printoptions  CC(未找到相关数据): BUG: fix reference counting bug in __array_interface__ implementation...  CC(未找到相关数据): TST: Add regression test for missing descr in arrayinterface  CC(未找到相关数据): BUG: Fix  CC(未找到相关数据) and  CC(未找到相关数据)  CC(未找到相关数据): BUG: Fix array_equal for numeric and nonnumeric scalar types  CC(未找到相关数据): MAINT: Update maintenance/2.1.x after the 2.0.2 release  CC(未找到相关数据): BLD: cp311 macosx_arm64 wheels [wheel build]  CC(未找到相关数据): BUG: f2py: better handle filtering of public/private subroutines  Checksums MD5 3053a97400db800b7377749e691eb39e  numpy2.1.1cp310cp310macosx_10_9_x86_64.whl 84b752a2220dce7c96ff89eef4f4aec3  numpy2.1.1cp310cp310macosx_11_0_arm64.whl 47ed4f704a64261f07ca24ef2e674524  numpy2.1.1cp310cp310macosx_14_0_arm64.whl b8a45caa870aee980c298053cf064d28  numpy2.1.1cp310cp310macosx_14_0_x86_64.whl e097ad5eee572b791b4a25eedad6df4a  numpy2.1.1cp310cp310manylinux_2_17_aarch64.manylinux2014_aarch64.whl ae502c99315884cda7f0236a07c035c4  numpy2.1.1cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl 841a859d975c55090c0b60b72aab93a3  numpy2.1.1cp310cp310musllinux_1_1_x86_64.whl d51be2b17f5b87aac64ab80fdfafc85e  numpy2.1.1cp310cp310musllinux_1_2_aarch64.whl 1f8249bd725397c6233fe6a0e8ad18b1  numpy2.1.1cp310cp310win32.whl d38d6f06589c1ec104a6a31ff6035781  numpy2.1.1cp310cp310win_amd64.whl    ... (truncated)   Commits  48606ab Merge pull request  CC(未找到相关数据) from charris/prepare2.1.1 a7cb4c4 REL: Prepare for the NumPy 2.1.1 release [wheel build] 884c92b Merge pull request  CC(未找到相关数据) from charris/backport27284 ca7f5c1 Merge pull request  CC(未找到相关数据) from charris/backport27049 2a49507 BUG: f2py: better handle filtering of public/private subroutines d4306dd TST:  Add regression test for gh26920 db9668d BLD: cp311 macosx_arm64 wheels [wheel build] c6ff254 Merge pull request  CC(未找到相关数据) from charris/post2.0.2releaseupdate 326bc17 MAINT: Update main after the 2.0.2 release 8164b7c Merge pull request  CC(未找到相关数据) from charris/backport27275 Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-09-16T17:41:51Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23662,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
yi,"When I compute gradients and perform updates using the same values in Torch and JAX, I find that after multiple iterations, the inference results differ significantly."," Description Please specify cuda:0 at the very beginning. ```python import torch import numpy as np import os from network.cv.SSD.backbone_mobilenetv1_pytorch import SSDWithMobileNetV1 as SSD_torch import jax import jax import jax.numpy as jnp from jax import ops as jops from jax.nn import one_hot, sigmoid from jax import lax import jax.scipy.special as sc import optax if ""CONTEXT_DEVICE_TARGET"" in os.environ and os.environ['CONTEXT_DEVICE_TARGET'] == 'GPU':     devices = os.environ['CUDA_VISIBLE_DEVICES'].split("","")     device = devices[2]     final_device = ""cuda:"" + device else:     final_device = 'cpu' def class_loss_jax(logits, label):     """"""Calculate category losses.""""""     label = jnp.eye(logits.shape[1])[label]     weight = jnp.ones_like(logits)     pos_weight = jnp.ones_like(logits)     sigmoid_logits = sc.expit(logits)      Binary cross entropy calculation     term1 = label * jnp.log(sigmoid_logits + 1e15)     term2 = (1  label) * jnp.log(1  sigmoid_logits + 1e15)     loss =  (weight * (term1 * pos_weight + term2))     sigmoid_cross_entropy = jnp.mean(loss)     sigmoid = sc.expit(logits)     p_t = label * sigmoid + (1  label) * (1  sigmoid)     modulating_factor = jnp.power(1  p_t, 2.0)     alpha_weight_factor = label * 0.75 + (1  label) * (1  0.75)     focal_loss = modulating_factor * alpha_weight_factor * sigmoid_cross_entropy     return focal_loss def SSDmultibox_jax_cal(params_, pred_loc, pred_label, gt_loc, gt_label, num_matched_boxes):     mask = jnp.less(0, gt_label).astype(jnp.float32)     num_matched_boxes = jnp.sum(num_matched_boxes.astype(jnp.float32))      Positioning loss     mask_loc = jnp.tile(jnp.expand_dims(mask, 1), (1, 1, 4))     diff = jnp.abs(pred_loc  gt_loc)     smooth_l1 = jnp.where(diff  0).float()         num_matched_boxes = num_matched_boxes.float().sum()          Positioning loss         mask_loc = mask.unsqueeze(1).repeat(1, 1, 4)         smooth_l1 = torch.nn.SmoothL1Loss(reduction='none')(pred_loc, gt_loc) * mask_loc         loss_loc = smooth_l1.sum(dim=1).sum(dim=1)          Category loss         from network.cv.SSD.ssd_utils_torch import class_loss         loss_cls = class_loss(pred_label, gt_label)         loss_cls = loss_cls.sum(dim=(1, 2))         return ((loss_cls + loss_loc) / num_matched_boxes).sum() image_torch = np.load('./image_torch.npy') image_torch = torch.from_numpy(image_torch).to(final_device) pred_loc_torch = np.load('./pred_loc_torch.npy') pred_loc_torch = torch.from_numpy(pred_loc_torch).to(final_device) pred_label_torch = np.load('./pred_label_torch.npy') pred_label_torch = torch.from_numpy(pred_label_torch).to(final_device) box_torch = np.load('./box_torch.npy') box_torch = torch.from_numpy(box_torch).to(final_device) label_torch = np.load('./label_torch.npy') label_torch = torch.from_numpy(label_torch).to(final_device) num_match_torch = np.load('./num_match_torch.npy') num_match_torch = torch.from_numpy(num_match_torch).to(final_device) model_torch = SSD_torch() model_torch.train() model_torch.to(final_device) learning_rate = 0.02 optimizer_torch = torch.optim.SGD optimizer_torch = optimizer_torch(model_torch.parameters(), lr=learning_rate) params_torch = {key: value.detach().cpu().numpy() for key, value in model_torch.state_dict().items()} params_jax = {name: jnp.array(value, dtype=jnp.float32) for name, value in params_torch.items()} optimizer_jax = optax.sgd optimizer_jax = optimizer_jax(learning_rate) loss_fun_torch = loss_SSDmultibox_torch() opt_state = optimizer_jax.init(params_jax) for i in range(0,10):     pred_loc_torch, pred_label_torch = model_torch(image_torch)     loss_torch = loss_fun_torch(pred_loc_torch, pred_label_torch, box_torch, label_torch, num_match_torch)     loss_torch.backward()     optimizer_torch.step()     optimizer_torch.zero_grad()     old_torch_state_dict = model_torch.state_dict()     torch.save(old_torch_state_dict, './model_weights.pth')     params_jax_numpy = {name: np.array(value) for name, value in params_jax.items()}     params_torch_updated = {name: torch.from_numpy(value) for name, value in params_jax_numpy.items()}     model_torch.load_state_dict(params_torch_updated)     pred_loc_torch, pred_label_torch = model_torch(image_torch)     pred_loc_jax = pred_loc_torch.detach().cpu().numpy()     pred_label_jax = pred_label_torch.detach().cpu().numpy()     loss_fun_jax = SSDmultibox_jax_cal     pred_loc_jax = pred_loc_torch.detach().cpu().numpy()     pred_label_jax = pred_label_torch.detach().cpu().numpy()     box_jax = box_torch.detach().cpu().numpy()     label_jax = label_torch.detach().cpu().numpy()     num_match_jax = num_match_torch.detach().cpu().numpy()     loss_jax, jax_grads = jax.value_and_grad(loss_fun_jax)(params_jax, pred_loc_jax, pred_label_jax, box_jax,                                                             label_jax, num_match_jax)     updates, opt_state = optimizer_jax.update(jax_grads, opt_state, params_jax)     params_jax = optax.apply_updates(params_jax, updates)      jax_grads_distance = chebyshev_distance(old_jax_grads, jax_grads)      old_jax_grads = jax_grads     torch_grads = {key: value.detach().cpu().numpy() for key, value in model_torch.state_dict().items()}     loaded_state_dict = torch.load('./model_weights.pth')     model_torch.load_state_dict(loaded_state_dict)     print('loss_jax/loss_torch:',np.array(loss_jax)/ loss_torch.cpu().detach().numpy())   输出: True ``` !屏幕截图 20240915 191310  System info (python version, jaxlib version, accelerator, etc.) download the code:https://drive.google.com/file/d/1H8uPgPdslVpizmSsif6oK4ey2eoum9x/view?usp=sharing ```python !unzip issue3.zip python issue1.py ```",2024-09-15T11:13:45Z,bug,open,0,3,https://github.com/jax-ml/jax/issues/23646,"If it's truly the same model you're running, this is surprising. Given the magnitude of the difference, though, I suspect the models or optimizers differ in important ways: for example, maybe the precise definition of ""learning rate"" differs between the two implementations. I don't know either `optax` or `pytorch` well enough to guess where that difference might lie, but if it's important to you to debug these differences in the implementations, that's probably where I'd start.","> If it's truly the same model you're running, this is surprising. Given the magnitude of the difference, though, I suspect the models or optimizers differ in important ways: for example, maybe the precise definition of ""learning rate"" differs between the two implementations. >  > I don't know either `optax` or `pytorch` well enough to guess where that difference might lie, but if it's important to you to debug these differences in the implementations, that's probably where I'd start. We're using the same model.","> We're using the same model. Sure, but what I'm suggesting is that you may not be using the same optimizer."
yi,"When calculating the loss, the input data does not contain NaN, but the output contains NaN"," Description Please specify `cuda:0` at the very beginning. ```python import torch import numpy as np import os import jax import jax import jax.numpy as jnp from jax import ops as jops from jax.nn import one_hot, sigmoid from jax import lax import jax.scipy.special as sc import optax if ""CONTEXT_DEVICE_TARGET"" in os.environ and os.environ['CONTEXT_DEVICE_TARGET'] == 'GPU':     devices = os.environ['CUDA_VISIBLE_DEVICES'].split("","")     device = devices[2]     final_device = ""cuda:"" + device else:     final_device = 'cpu' from network.cv.yolov4.yolov4_pytorch import YOLOV4CspDarkNet53_torch as yolov4_torch def loss_yolo_jax():     from network.cv.yolov4.yolov4_pytorch import yolov4loss_jax     yolo_obj = yolov4loss_jax()     return yolo_obj y_true_0 = np.load('./yolo_out[0][0].npy') yolo_out1 = torch.from_numpy(y_true_0).to(final_device) y_true_0 = np.load('./yolo_out[0][1].npy') yolo_out2 = torch.from_numpy(y_true_0).to(final_device) y_true_0 = np.load('./yolo_out[0][2].npy') yolo_out3 = torch.from_numpy(y_true_0).to(final_device) y_true_0 = np.load('./yolo_out[1][0].npy') yolo_out4 = torch.from_numpy(y_true_0).to(final_device) y_true_0 = np.load('./yolo_out[1][1].npy') yolo_out5 = torch.from_numpy(y_true_0).to(final_device) y_true_0 = np.load('./yolo_out[1][2].npy') yolo_out6 = torch.from_numpy(y_true_0).to(final_device) y_true_0 = np.load('./yolo_out[2][0].npy') yolo_out7 = torch.from_numpy(y_true_0).to(final_device) y_true_0 = np.load('./yolo_out[2][1].npy') yolo_out8 = torch.from_numpy(y_true_0).to(final_device) y_true_0 = np.load('./yolo_out[2][2].npy') yolo_out9 = torch.from_numpy(y_true_0).to(final_device) yolo_out = ((yolo_out1,yolo_out2,yolo_out3),(yolo_out4,yolo_out5,yolo_out6),(yolo_out7,yolo_out8,yolo_out9)) model_pt = yolov4_torch() model_pt.train() model_torch = model_pt.to(final_device) y_true_0 = np.load('./y_true_0.npy') y_true_0 = torch.from_numpy(y_true_0).to(final_device) y_true_1 = np.load('./y_true_1.npy') y_true_1 = torch.from_numpy(y_true_1).to(final_device) y_true_2 = np.load('./y_true_2.npy') y_true_2 = torch.from_numpy(y_true_2).to(final_device) gt_0 = np.load('./gt_0.npy') gt_0 = torch.from_numpy(gt_0).to(final_device) gt_1 = np.load('./gt_1.npy') gt_1 = torch.from_numpy(gt_1).to(final_device) gt_2 = np.load('./gt_2.npy') gt_2 = torch.from_numpy(gt_2).to(final_device) input_shape_t = np.load('./input_shape_t.npy') input_shape_t = torch.from_numpy(input_shape_t).to(final_device) params_torch = {key: value.detach().cpu().numpy() for key, value in model_torch.state_dict().items()} loss_jax_fun = loss_yolo_jax() params_jax = {name: jnp.array(value, dtype=jnp.float32) for name, value in params_torch.items()} loss_jax, jax_grads = jax.value_and_grad(loss_jax_fun.calc_loss)(params_jax, yolo_out, y_true_0, y_true_1, y_true_2, gt_0, gt_1, gt_2, input_shape_t) yolo_out1 = torch.isnan(yolo_out1).any() print('yolo_out1;',yolo_out1)  yolo_out2 = torch.isnan(yolo_out2).any() print('yolo_out2;',yolo_out2)  yolo_out3 = torch.isnan(yolo_out3).any() print('yolo_out3;',yolo_out3)  yolo_out4 = torch.isnan(yolo_out4).any() print('yolo_out4;',yolo_out4) yolo_out5 = torch.isnan(yolo_out5).any() print('yolo_out5;',yolo_out5)  yolo_out6 = torch.isnan(yolo_out6).any() print('yolo_out6;',yolo_out6)  yolo_out7 = torch.isnan(yolo_out7).any() print('yolo_out7;',yolo_out7) yolo_out8 = torch.isnan(yolo_out8).any() print('yolo_out8;',yolo_out8) yolo_out9 = torch.isnan(yolo_out9).any() print('yolo_out9;',yolo_out9)  y_true_0 = torch.isnan(y_true_0).any() print('y_true_0;',y_true_0)  y_true_1 = torch.isnan(y_true_1).any() print('y_true_1;',y_true_1) y_true_2 = torch.isnan(y_true_2).any() print('y_true_2;',y_true_2)  gt_0 = torch.isnan(gt_0).any() print('gt_0;',gt_0)  gt_1 = torch.isnan(gt_1).any() print('gt_1;',gt_1)  gt_2 = torch.isnan(gt_2).any() print('gt_2;',gt_2)  input_shape_t = torch.isnan(input_shape_t).any() print('input_shape_t;',input_shape_t)  print('loss_torch_result;',np.array(loss_jax))  ``` !屏幕截图 20240913 204340  System info (python version, jaxlib version, accelerator, etc.) Code and data links:https://drive.google.com/file/d/1edrk7_sxSgdu7cmXQXf6JsT57xiG1Hb/view?usp=sharing",2024-09-13T12:46:46Z,bug,open,0,3,https://github.com/jax-ml/jax/issues/23625,Is there a MVC? This code doesn't run for me,"> Is there a MVC? This code doesn't run for me > Is there a MVC? This code doesn't run for me Where can't run it, in the beginning final_device to set it yourself, you can delete  ```python  if “CONTEXT_DEVICE_TARGET” in os.environ and os.environ['CONTEXT_DEVICE_TARGET'] == 'GPU': devices = os.environ['CUDA_VISIBLE_DEVICES'].split(“,”).     devices = os.environ['CUDA_VISIBLE_DEVICES'].split(“,”)     device = devices[2]     final_device = “cuda:” + device else: final_device = 'cuda:” + device     final_device = 'cpu'  ```  Translated with DeepL.com (free version)","Hi  – it's going to be hard to help with specifics here absent an MVC (also known as a minimal reproducible example). If you're able to rework your example so that others can run it and see the same errors you are seeing, then we could offer specific guidance. Absent that, though, in general it's not surprising to see NaN outputs for inputs without NaNs: it just means that you're calling some function in your model in a way that is undefined to floating point precision. Here's a simple example of this: ```python >>> import jax.numpy as jnp >>> def f(x, y): ...   return x * jnp.exp(y) >>> f(1.0, 1.0) Array(2.7182817, dtype=float32, weak_type=True) >>> f(0.0, 100.0) Array(nan, dtype=float32, weak_type=True) ``` More than likely, somewhere in your model you have an expression that is evaluating to NaN for reasons like this. The best way to debug this is to start diggingin to your model to figure out exactly where this is coming from. One way to do this is to enable the `jax_debug_nans` flag, as described here: https://jax.readthedocs.io/en/latest/debugging/flags.htmljaxdebugnansconfigurationoptionandcontextmanager I hope that helps get you on the right path!"
chat,[io_callback] Adds test for io_callback being used inside custom partitioning,Offline chatted with  on fixing the crash when using io_callback inside the custom partitioning. The test is currently skipped because of a JAX bug that the lifetime of the _callback wrapper inside emit_python_callback is not well managed after the lowering rule dies.  That would lead to io_callback not working inside custom_partitioning. (Credit to  ) Here I provided a test that can repro the issue and guard the behavior after the issue is fixed.,2024-09-13T07:42:37Z,,open,0,2,https://github.com/jax-ml/jax/issues/23620," could you ptal at this one? The issue is that callback object isn't kept alive if called from a `partition` call, because that call uses an ephemeral MLIR module context, which means the `host_callback` doesn't end up getting kept alive by the executable. This causes a useafterfree.","It doesn't look that simple. lowering_result.keepalive yes does need to have its lifetime extended, but there are two other problems with doing effects inside custom partitioning:  lowering_result.host_callbacks must be properly populated but this list must be passed as an argument to backend.compile(), but it wouldn't be completely known until compilation has reached spmd partitioning.  tokens are not properly added to the computation but we don't know which effect tokens are needed before we end up lowering the body."
llm,Orthogonal Initializer raises gpusolverDnCreate(&handle) failed: cuSolver internal error," Description I am having issues initializing a Flax.linen neural network when running with GPU support.  I have narrowed it down to the flax.linen.initializers.orthogonal.  Running the below code will result in a:  `RuntimeError: jaxlib/gpu/solver_handle_pool.cc:37: operation gpusolverDnCreate(&handle) failed: cuSolver internal error` However running the code in another venv with only CPU support it runs just fine.  And secondly running it without the orthogonal kernel initializer it runs just fine. The jax is installed using `pip install U ""jax[cuda12]"" ` I have attached a minimal example that will raise the issue. ``` import os os.environ['JAX_TRACEBACK_FILTERING'] = 'off' import jax import jax.numpy as jnp import flax.linen as nn from flax.linen.initializers import constant, orthogonal class SingleLayer(nn.Module):     .compact     def __call__(self, x):         layer = nn.Dense(64, kernel_init=orthogonal())(x)         return layer network = SingleLayer() init_x = jnp.zeros(128) network_params = network.init(rngs=jax.random.PRNGKey(0), x=init_x) print(network_params) ``` ``` /home/brain/Tensor/JaxRL/.venv/bin/python /home/brain/Tensor/JaxRL/flax_lax.py  Traceback (most recent call last):   File ""/home/brain/Tensor/JaxRL/flax_lax.py"", line 22, in      network_params = network.init(rngs=jax.random.PRNGKey(0), x=init_x)   File ""/home/brain/Tensor/JaxRL/flax_lax.py"", line 16, in __call__     layer = nn.Dense(64, kernel_init=orthogonal())(x)   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/flax/linen/linear.py"", line 251, in __call__     kernel = self.param(   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/jax/_src/nn/initializers.py"", line 611, in init     Q, R = jnp.linalg.qr(A)   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/jax/_src/numpy/linalg.py"", line 1300, in qr     q, r = lax_linalg.qr(a, full_matrices=full_matrices) jax._src.source_info_util.JaxStackTraceBeforeTransformation: RuntimeError: jaxlib/gpu/solver_handle_pool.cc:37: operation gpusolverDnCreate(&handle) failed: cuSolver internal error The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/home/brain/Tensor/JaxRL/flax_lax.py"", line 22, in      network_params = network.init(rngs=jax.random.PRNGKey(0), x=init_x)                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/jax/_src/traceback_util.py"", line 180, in reraise_with_filtered_traceback     return fun(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/flax/linen/module.py"", line 2442, in init     _, v_out = self.init_with_output(                ^^^^^^^^^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/jax/_src/traceback_util.py"", line 180, in reraise_with_filtered_traceback     return fun(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/flax/linen/module.py"", line 2294, in init_with_output     return init_with_output(            ^^^^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/flax/core/scope.py"", line 1144, in wrapper     return apply(fn, mutable=mutable, flags=init_flags)(            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/flax/core/scope.py"", line 1108, in wrapper     y = fn(root, *args, **kwargs)         ^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/flax/linen/module.py"", line 3081, in scope_fn     return fn(module.clone(parent=scope, _deep_clone=True), *args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/flax/linen/module.py"", line 694, in wrapped_module_method     return self._call_wrapped_method(fun, args, kwargs)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/flax/linen/module.py"", line 1211, in _call_wrapped_method     y = run_fun(self, *args, **kwargs)         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/flax_lax.py"", line 16, in __call__     layer = nn.Dense(64, kernel_init=orthogonal())(x)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/flax/linen/module.py"", line 694, in wrapped_module_method     return self._call_wrapped_method(fun, args, kwargs)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/flax/linen/module.py"", line 1211, in _call_wrapped_method     y = run_fun(self, *args, **kwargs)         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/flax/linen/linear.py"", line 251, in __call__     kernel = self.param(              ^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/flax/linen/module.py"", line 1867, in param     v = self.scope.param(name, init_fn, *init_args, unbox=unbox, **init_kwargs)         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/flax/core/scope.py"", line 997, in param     value = init_fn(self.make_rng('params'), *init_args, **init_kwargs)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/jax/_src/nn/initializers.py"", line 611, in init     Q, R = jnp.linalg.qr(A)            ^^^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/jax/_src/traceback_util.py"", line 180, in reraise_with_filtered_traceback     return fun(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/jax/_src/pjit.py"", line 332, in cache_miss     outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked = _python_pjit_helper(                                                                 ^^^^^^^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/jax/_src/pjit.py"", line 190, in _python_pjit_helper     out_flat = pjit_p.bind(*args_flat, **p.params)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/jax/_src/core.py"", line 2782, in bind     return self.bind_with_trace(top_trace, args, params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/jax/_src/core.py"", line 443, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/jax/_src/core.py"", line 949, in process_primitive     return primitive.impl(*tracers, **params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/jax/_src/pjit.py"", line 1739, in _pjit_call_impl     return xc._xla.pjit(            ^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/jax/_src/pjit.py"", line 1721, in call_impl_cache_miss     out_flat, compiled = _pjit_call_impl_python(                          ^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/jax/_src/pjit.py"", line 1643, in _pjit_call_impl_python     compiled = _resolve_and_lower(                ^^^^^^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/jax/_src/pjit.py"", line 1610, in _resolve_and_lower     lowered = _pjit_lower(               ^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/jax/_src/pjit.py"", line 1748, in _pjit_lower     return _pjit_lower_cached(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/jax/_src/pjit.py"", line 1769, in _pjit_lower_cached     return pxla.lower_sharding_computation(            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/jax/_src/profiler.py"", line 333, in wrapper     return func(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/jax/_src/interpreters/pxla.py"", line 2230, in lower_sharding_computation     nreps, tuple_args, shape_poly_state) = _cached_lowering_to_hlo(                                            ^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/jax/_src/interpreters/pxla.py"", line 1950, in _cached_lowering_to_hlo     lowering_result = mlir.lower_jaxpr_to_module(                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/jax/_src/interpreters/mlir.py"", line 1132, in lower_jaxpr_to_module     lower_jaxpr_to_fun(   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/jax/_src/interpreters/mlir.py"", line 1590, in lower_jaxpr_to_fun     out_vals, tokens_out = jaxpr_subcomp(                            ^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/jax/_src/interpreters/mlir.py"", line 1805, in jaxpr_subcomp     ans = lower_per_platform(rule_ctx, str(eqn.primitive),           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/jax/_src/interpreters/mlir.py"", line 1921, in lower_per_platform     output = kept_rules0              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/jax/_src/interpreters/mlir.py"", line 2036, in f_lowered     out, tokens = jaxpr_subcomp(                   ^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/jax/_src/interpreters/mlir.py"", line 1805, in jaxpr_subcomp     ans = lower_per_platform(rule_ctx, str(eqn.primitive),           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/jax/_src/interpreters/mlir.py"", line 1921, in lower_per_platform     output = kept_rules0              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib/python3.12/sitepackages/jax/_src/lax/linalg.py"", line 1757, in _geqrf_cpu_gpu_lowering     a_out, taus, info_geqrf = geqrf_impl(a_aval.dtype, a)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/brain/Tensor/JaxRL/.venv/lib64/python3.12/sitepackages/jaxlib/gpu_solver.py"", line 164, in _geqrf_hlo     lwork, opaque = gpu_solver.build_geqrf_descriptor(                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ RuntimeError: jaxlib/gpu/solver_handle_pool.cc:37: operation gpusolverDnCreate(&handle) failed: cuSolver internal error ``` Thanks, Brian  System info (python version, jaxlib version, accelerator, etc.) ``` jax.print_environment_info() jax:    0.4.32 jaxlib: 0.4.32 numpy:  2.1.1 python: 3.12.5 (main, Aug 23 2024, 00:00:00) [GCC 14.2.1 20240801 (Red Hat 14.2.11)] jax.devices (1 total, 1 local): [CudaDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='programmingdesktop', release='6.10.8200.fc40.x86_64', version=' CC(Python 3 compatibility issues) SMP PREEMPT_DYNAMIC Wed Sep  4 21:41:11 UTC 2024', machine='x86_64') $ nvidiasmi Thu Sep 12 22:41:39 2024        ++  grep nvidia nvidiacublascu12==12.6.1.4 nvidiacudacupticu12==12.6.68 nvidiacudanvcccu12==12.6.68 nvidiacudaruntimecu12==12.6.68 nvidiacudnncu12==9.4.0.58 nvidiacufftcu12==11.2.6.59 nvidiacusolvercu12==11.6.4.69 nvidiacusparsecu12==12.5.3.3 nvidiancclcu12==2.23.4 nvidianvjitlinkcu12==12.6.68 ```",2024-09-13T02:47:11Z,bug,open,0,3,https://github.com/jax-ml/jax/issues/23616,"Thanks for the report. I'm not too sure what the issue is here, but I'm happy to help dig into it. 2 requests: 1. Would you be able to try to put together an (even!) smaller example that doesn't depend on flax? I.e. just calling QR on an array you construct yourself to make it easier to reproduce. 2. Can you try downgrading to jax/jaxlib 0.4.31? v0.4.32 got yanked because of TPU issues (unrelated to this conversation!), but it'll be easier for me to try with v0.4.31 if you still see the issue there.","If I had to guess, you're probably running out of GPU memory. Try lowering the preallocation fraction here: https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html ?","I have an even smaller example.  Pulled it right from the documentation. https://jax.readthedocs.io/en/latest/_autosummary/jax.nn.initializers.orthogonal.htmljax.nn.initializers.orthogonal I am running this on jax 0.4.31 and still have the exact same issue.  I am even setting the environment variable to restrict the preallocation to 0.25 and am still having the same issue. ```import os os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '.25' import jax, jax.numpy as jnp initializer = jax.nn.initializers.orthogonal() initializer(jax.random.key(42), (2, 3), jnp.float32) ``` ```Traceback (most recent call last):   File ""/home/brain/Tensor/jax/jax_orthogonal.py"", line 8, in      initializer(jax.random.key(42), (2, 3), jnp.float32)   File ""/home/brain/Tensor/jax/.venv/lib/python3.11/sitepackages/jax/_src/nn/initializers.py"", line 611, in init     Q, R = jnp.linalg.qr(A)   File ""/home/brain/Tensor/jax/.venv/lib/python3.11/sitepackages/jax/_src/numpy/linalg.py"", line 1291, in qr     q, r = lax_linalg.qr(a, full_matrices=full_matrices) jax._src.source_info_util.JaxStackTraceBeforeTransformation: RuntimeError: jaxlib/gpu/solver_kernels.cc:45: operation gpusolverDnCreate(&handle) failed: cuSolver internal error The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/home/brain/Tensor/jax/jax_orthogonal.py"", line 8, in      initializer(jax.random.key(42), (2, 3), jnp.float32)   File ""/home/brain/Tensor/jax/.venv/lib/python3.11/sitepackages/jax/_src/nn/initializers.py"", line 611, in init     Q, R = jnp.linalg.qr(A)            ^^^^^^^^^^^^^^^^   File ""/home/brain/Tensor/jax/.venv/lib64/python3.11/sitepackages/jaxlib/gpu_solver.py"", line 156, in _geqrf_hlo     lwork, opaque = gpu_solver.build_geqrf_descriptor(                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ RuntimeError: jaxlib/gpu/solver_kernels.cc:45: operation gpusolverDnCreate(&handle) failed: cuSolver internal error  For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. ``` ```jax.print_environment_info() jax:    0.4.31 jaxlib: 0.4.31 numpy:  2.1.1 python: 3.11.9 (main, Aug 23 2024, 00:00:00) [GCC 14.2.1 20240801 (Red Hat 14.2.11)] jax.devices (1 total, 1 local): [CudaDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='programmingdesktop', release='6.10.8200.fc40.x86_64', version=' CC(Python 3 compatibility issues) SMP PREEMPT_DYNAMIC Wed Sep  4 21:41:11 UTC 2024', machine='x86_64') $ nvidiasmi Sun Sep 15 14:55:58 2024        ++  ++ ```"
yi,"Implements an alternate version of ragged_attention, wherein, the actual attention kernel itself is dense. Meaning, this kernel does not have the compute saving (@when wrapped kernel) or prefetch/index skipping (via index rewriting) as part of the kernel. Rather, the kernel is invoked with a Jumble (A ragged type representation) and pallas takes care of applying the correct work skipping and index rewriting.","Implements an alternate version of ragged_attention, wherein, the actual attention kernel itself is dense. Meaning, this kernel does not have the compute saving ( wrapped kernel) or prefetch/index skipping (via index rewriting) as part of the kernel. Rather, the kernel is invoked with a Jumble (A ragged type representation) and pallas takes care of applying the correct work skipping and index rewriting. Performance wise, we should be at parity, although this has not yet been tested. Authoring wise, the new kernel is significantly smaller and simpler to write. A major known limitation of this approach, which we have a plan to fix, is the invariant that the `seq_len % grid_size == 0`  we plan to relax this limitation in following CLs.",2024-09-12T16:28:01Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23597
yi,Added a new primitive for copying GMEM<->SMEM in Pallas Mosaic GPU kernels,"Added a new primitive for copying GMEMSMEM in Pallas Mosaic GPU kernels The copy is async and needs to be awaited via `plgpu.wait_inflight(...)` for SMEM>GMEM copies and via `plgpu.wait(barrier)` for GMEM>SMEM copies. I decided to have distinct functions for SMEM>GMEM and GMEM>SMEM copies and for the ways to await the result, because the underlying Mosaic GPU APIs (and PTX ISA) *are* in fact very different.",2024-09-12T14:31:44Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23595
yi,"Add support for setting a dot product ""algorithm"" for lax.dot_general.","Add support for setting a dot product ""algorithm"" for lax.dot_general. The StableHLO spec has a new ""algorithm"" parameter that allows specifying the algorithm that is used to execute a matrix multiplication, and it can tune the tradeoff between performance and computational cost. Historically, in JAX, the precision and preferred_element_type parameters have been used to expose some level of control, but their behavior is platform dependent and not sufficiently flexible for performance use cases. This change adds a new ""algorithm"" parameter to dot_general to add support for the new explicit API. This parameter can be a member of the `SupportedDotAlgorithm` `Enum` to use an algorithm that is known to be supported on at least some hardware. Otherwise, it can be specified using the `DotAlgorithm` data structure which exposes the full generality of the StableHLO spec. Transposition is supported using the `transpose_algorithm` argument.",2024-09-11T17:43:59Z,,closed,0,1,https://github.com/jax-ml/jax/issues/23574,"The doctests are failing for the new docstring because this new feature requires jaxlib > 0.4.33, which hasn't been released yet.  do you have any suggestions for a reasonable approach for including these examples, but ""xfailing"" when the jaxlib version is too old?"
rag,Ported a few changes to FragmentArray by cperivol@,Ported a few changes to FragmentArray by cperivol@ * It now supports unary negation * and pointwise operations between scalars and FragmentedArrays,2024-09-11T14:42:13Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23568
yi,[ROCM] Multi-device reduction causes segfault," Description Running a simple program with a global reduction causes a segfault. The MWE ```python import jax print(""devices:"", jax.devices()) print(""local:"", jax.local_devices()) x=jax.numpy.ones((120, 10)) y=jax.lax.with_sharding_constraint(x, jax.sharding.PositionalSharding(jax.devices()).reshape(1, 1)) print(y.sharding) y.sum() ``` I'm running on a single node of CINES Ad Astra HPC that has 4xMI250X (seen as 8xMI200).  I'm using ROCM 6.0 and a custom built version of jax because there are no wheels publicly available. Running the program above leads to no information Segfault ```python ModuleNotFoundError: No module named 'jax' (myjax) [cad14908] fvicentini:~/prove$ /opt/software/gaiaexternal/IA/jax/0.4.31/miniconda3/envs/myjax/bin/python crash.py devices: [RocmDevice(id=0), RocmDevice(id=1), RocmDevice(id=2), RocmDevice(id=3), RocmDevice(id=4), RocmDevice(id=5), RocmDevice(id=6), RocmDevice(id=7)] local: [RocmDevice(id=0), RocmDevice(id=1), RocmDevice(id=2), RocmDevice(id=3), RocmDevice(id=4), RocmDevice(id=5), RocmDevice(id=6), RocmDevice(id=7)] PositionalSharding([[{GPU 0}]                     [{GPU 1}]                     [{GPU 2}]                     [{GPU 3}]                     [{GPU 4}]                     [{GPU 5}]                     [{GPU 6}]                     [{GPU 7}]], shape=(8, 1)) Segmentation fault (core dumped) (myjax) [cad14908] fvicentini:~/prove$ ``` so I ran it under gdb to get a stack trace ```python                     [{GPU 7}]], shape=(8, 1)) [Detaching after vfork from child process 1287214] [New Thread 0x14e23afff700 (LWP 1287227)] [New Thread 0x14e23adfe700 (LWP 1287228)] [New Thread 0x14e23abfd700 (LWP 1287229)] [New Thread 0x14e23a9fc700 (LWP 1287230)] [New Thread 0x14e23a7fb700 (LWP 1287231)] Thread 480 ""python"" received signal SIGSEGV, Segmentation fault. [Switching to Thread 0x14e23abfd700 (LWP 1287229)] 0x000015555204c490 in hip::FatBinaryInfo::BuildProgram(int) () from /opt/rocm6.0.0/lib/libamdhip64.so.6 (gdb) bt CC(未找到相关数据)  0x000015555204c490 in hip::FatBinaryInfo::BuildProgram(int) () from /opt/rocm6.0.0/lib/libamdhip64.so.6 CC(Python 3 compatibility issues)  0x000015555204fd2e in hip::Function::getStatFuncAttr(hipFuncAttributes*, int) () from /opt/rocm6.0.0/lib/libamdhip64.so.6 CC(Explicit tuples are not valid function parameters in Python 3)  0x000015555200d6a7 in hip::StatCO::getStatFuncAttr(hipFuncAttributes*, void const*, int) () from /opt/rocm6.0.0/lib/libamdhip64.so.6 CC(Undefined name: from ..core import JaxTuple)  0x0000155552140341 in hipFuncGetAttributes () from /opt/rocm6.0.0/lib/libamdhip64.so.6 CC(Undefined name: from six.moves import xrange)  0x000015545264627f in ncclInitKernelsForDevice(int, unsigned long*) () from /opt/rocm6.0.0/lib/librccl.so.1 CC(Building on OSX with CUDA)  0x000015545267c298 in ncclCommInitRankFunc(ncclAsyncJob*) () from /opt/rocm6.0.0/lib/librccl.so.1 CC(Made a shim to handle configuration without having absl parse flags)  0x0000155452674f37 in ncclAsyncJobMain(void*) () from /opt/rocm6.0.0/lib/librccl.so.1 CC(Quickish check)  0x000015555510d1ca in start_thread () from /lib64/libpthread.so.0 CC(Quickish check)  0x00001555545efe73 in clone () from /lib64/libc.so.6 (gdb) quit ``` If you need anything else let me know  System info (python version, jaxlib version, accelerator, etc.) ``` >>> import jax; jax.print_environment_info() jax:    0.4.31 jaxlib: 0.4.31.dev20240909 numpy:  2.0.2 python: 3.11.8  (main, Feb 16 2024, 20:53:32) [GCC 12.3.0] jax.devices (8 total, 8 local): [RocmDevice(id=0) RocmDevice(id=1) ... RocmDevice(id=6) RocmDevice(id=7)] process_count: 1 platform: uname_result(system='Linux', node='g1265', release='4.18.0477.10.1.el8_8.x86_64', version=' CC(Python 3 compatibility issues) SMP Wed Apr 5 13:35:01 EDT 2023', machine='x86_64') ```",2024-09-11T10:38:51Z,bug AMD GPU,open,0,3,https://github.com/jax-ml/jax/issues/23565,"I am not able to reproduce like it is reported (a seg fault). I aligned data to the number of devices and it seems that sharing was done ok. However if data dimension is not divisible by devices then it errored out Here are the details of my experiments. System Info: ``` >>> import jax; jax.print_environment_info() jax:    0.4.31.dev20240808+a96cefdc0 jaxlib: 0.4.31.dev20240913 numpy:  2.1.1 python: 3.11.0 (main, Apr  9 2024, 03:49:51) [GCC 9.4.0] jax.devices (16 total, 16 local): [RocmDevice(id=0) RocmDevice(id=1) ... RocmDevice(id=14) RocmDevice(id=15)] process_count: 1 platform: uname_result(system='Linux', node='hyd7cZT1303', release='5.15.091generic', version=' CC(Numpystyle indexed update support.)~20.04.1Ubuntu SMP Thu Nov 16 14:22:28 UTC 2023', machine='x86_64') ``` Experiment CC(Python 3 compatibility issues): aligned the sharing data to the number of devices.  i.e x is of shape 160x10. it looks like datasharding was done ``` python3 multi_device_datasharding.py devices: [RocmDevice(id=0), RocmDevice(id=1), RocmDevice(id=2), RocmDevice(id=3), RocmDevice(id=4), RocmDevice(id=5), RocmDevice(id=6), RocmDevice(id=7), RocmDevice(id=8), RocmDevice(id=9), RocmDevice(id=10), RocmDevice(id=11), RocmDevice(id=12), RocmDevice(id=13), RocmDevice(id=14), RocmDevice(id=15)] local: [RocmDevice(id=0), RocmDevice(id=1), RocmDevice(id=2), RocmDevice(id=3), RocmDevice(id=4), RocmDevice(id=5), RocmDevice(id=6), RocmDevice(id=7), RocmDevice(id=8), RocmDevice(id=9), RocmDevice(id=10), RocmDevice(id=11), RocmDevice(id=12), RocmDevice(id=13), RocmDevice(id=14), RocmDevice(id=15)] PositionalSharding([[{GPU 0}]                     [{GPU 1}]                     [{GPU 2}]                     [{GPU 3}]                     [{GPU 4}]                     [{GPU 5}]                     [{GPU 6}]                     [{GPU 7}]                     [{GPU 8}]                     [{GPU 9}]                     [{GPU 10}]                     [{GPU 11}]                     [{GPU 12}]                     [{GPU 13}]                     [{GPU 14}]                     [{GPU 15}]], shape=(16, 1)) ``` Experiment CC(Explicit tuples are not valid function parameters in Python 3): x is 120x10 matrix,  Errored with message.  the global size of data dimension 0 should be divisible by num of devices (16), ``` root7cZT1303:/workspaces python3 multi_device_reduction.py devices: [RocmDevice(id=0), RocmDevice(id=1), RocmDevice(id=2), RocmDevice(id=3), RocmDevice(id=4), RocmDevice(id=5), RocmDevice(id=6), RocmDevice(id=7), RocmDevice(id=8), RocmDevice(id=9), RocmDevice(id=10), RocmDevice(id=11), RocmDevice(id=12), RocmDevice(id=13), RocmDevice(id=14), RocmDevice(id=15)] local: [RocmDevice(id=0), RocmDevice(id=1), RocmDevice(id=2), RocmDevice(id=3), RocmDevice(id=4), RocmDevice(id=5), RocmDevice(id=6), RocmDevice(id=7), RocmDevice(id=8), RocmDevice(id=9), RocmDevice(id=10), RocmDevice(id=11), RocmDevice(id=12), RocmDevice(id=13), RocmDevice(id=14), RocmDevice(id=15)] Traceback (most recent call last):   File ""/workspaces/multi_device_reduction.py"", line 7, in      y=jax.lax.with_sharding_constraint(x, jax.sharding.PositionalSharding(jax.devices()).reshape(1, 1))       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/workspaces/jax_xla/rocmjaxlibv0.4.31/jax/jax/_src/pjit.py"", line 2465, in with_sharding_constraint     outs = [sharding_constraint_p.bind(xf, sharding=s, layout=l,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/workspaces/jax_xla/rocmjaxlibv0.4.31/jax/jax/_src/pjit.py"", line 2465, in      outs = [sharding_constraint_p.bind(xf, sharding=s, layout=l,             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/workspaces/jax_xla/rocmjaxlibv0.4.31/jax/jax/_src/core.py"", line 429, in bind     return self.bind_with_trace(find_top_trace(args), args, params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/workspaces/jax_xla/rocmjaxlibv0.4.31/jax/jax/_src/core.py"", line 433, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/workspaces/jax_xla/rocmjaxlibv0.4.31/jax/jax/_src/core.py"", line 939, in process_primitive     return primitive.impl(*tracers, **params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/workspaces/jax_xla/rocmjaxlibv0.4.31/jax/jax/_src/pjit.py"", line 2480, in _sharding_constraint_impl     return api.jit(_identity_fn, out_shardings=sharding)(x)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ValueError: One of pjit outputs was given the sharding of PositionalSharding([[{GPU 0}]                     [{GPU 1}]                     [{GPU 2}]                     [{GPU 3}]                     [{GPU 4}]                     [{GPU 5}]                     [{GPU 6}]                     [{GPU 7}]                     [{GPU 8}]                     [{GPU 9}]                     [{GPU 10}]                     [{GPU 11}]                     [{GPU 12}]                     [{GPU 13}]                     [{GPU 14}]                     [{GPU 15}]], shape=(16, 1)), which implies that the global size of its dimension 0 should be divisible by 16, but it is equal to 120 (full shape: (120, 10))  For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. ```","Thank you. I had 8 devices, and the given size (120) is divisible by 8. It might be some problems in the setting of our HPC ROCM libraries. Is there some way to debug what is happening inside of ```python CC(未找到相关数据)  0x000015555204c490 in hip::FatBinaryInfo::BuildProgram(int) () from /opt/rocm6.0.0/lib/libamdhip64.so.6 ``` ?","  Can you please mentioned how did you get Jax 4,31 for rocm or steps to build jax locally. I am using following steps build Jax in docker image docker:    rocm/jax:rocm6.0.0jax0.4.26py3.11.0 Cloned Jax/xla git clone b rocmjaxlibv0.4.31 https://github.com/ROCm/jax.git git clone b rocmjaxlibv0.4.31 https://github.com/ROCm/xla.git build/Install JAX locally using command below rm rf dist; python3 m pip uninstall jax jaxlib jaxrocm60pjrt jaxrocm60plugin y; python3 ./build/build.py use_clang=false enable_rocm build_gpu_plugin gpu_plugin_rocm_version=60 rocm_amdgpu_targets=gfx90a bazel_options=override_repository=xla=/workspaces/jax_xla/rocmjaxlibv0.4.31/xla rocm_path=/opt/rocm6.0.0/ && python3 setup.py develop user && python3 m pip install dist/*.whl"
yi,Update users of jax.tree.map() to be more careful about how they handle Nones.,"Update users of jax.tree.map() to be more careful about how they handle Nones. Due to a bug in JAX, JAX previously permitted `jax.tree.map(f, None, x)` where `x` is not `None`, effectively treating `None` as if it were pytreeprefix of any value. But `None` is a pytree container, and it is only a prefix of `None` itself. Fix code that was relying on this bug. Most commonly, the fix is to write `jax.tree.map(lambda a, b: (None if a is None else f(a, b)), x, y, is_leaf=lambda t: t is None)`.",2024-09-11T01:14:44Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23558
yi,Unused vmap GPU memory allocation causes RESOURCE_EXHAUSTED for versions >0.4.14," Description  Overview The script below works when using an NVIDIA GPU with Jax version 0.4.14, but after upgrading to 0.4.31 (and trying a few other versions in between) it is triggering the following error: `E0910 20:24:00.097739   38257 pjrt_stream_executor_client.cc:3067] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate X bytes` `jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory while trying to allocate X bytes.` where the value of `X` ranges from ~5GB (e.g. 4843897104) to 20GB+ depending on the shape of the `dls` variable (set to 3540 in the script below). _jax0.4.14_  error Not sure if this is a bug or if there is some code/syntax in the example below that is no longer supported in versions > 0.4.14 that is responsible for this behavior.  Allocation vs. pprof usage The GPU has 6GB of memory and after some trial and error it appears that setting the `dls` variable to a shape of 1590 succeeds and uses only ~500kB of memory according to pprof (following https://jax.readthedocs.io/en/latest/device_memory_profiling.html), but a shape of 1600 gives an error trying to allocate ~5GB. If pprof is in fact showing GPU memory usage this could suggest memory is being allocated but not used.  jnp.exp removal Trial and error also showed that removing the `jnp.exp` calls inside the function `m` seem to resolve the issue. For example, the script below with `dls` shape set to 10000 fails trying to allocate 30GB, but removing the `jnp.exp` calls succeeds and shows as using only ~2MB by pprof.  Script ```python import jax import jax.numpy as jnp from jax import vmap def wp(y0, ts, rng, tidx_offset):     t0, t1 = ts[0], ts[1]     y = jnp.ones((11, 3)) * (t0 * t1)     y = jnp.vstack(         (             y,             jnp.ones((71, 3)) * y0,         )     )     y = jnp.roll(y, tidx_offset, axis=0)     y = y[:71]     y = y.at[:, 2].set(jnp.abs(y[:, 2]  0.03) + 0.03)     return y def ps(ys, ts, tidx_offset):     t = jnp.maximum(ts  ts[0], 0)     t = jnp.hstack(         (             t,             jnp.zeros(71  11),         )     )     t = jnp.roll(t, tidx_offset, axis=0)     t = t[:71]     ds = jnp.sqrt(jnp.sum((ys[1:, :]  ys[:1, :]) ** 2, axis=1))     d = jnp.cumsum(jnp.hstack((jnp.array([0.0]), ds)), axis=1)     s_xyz = jnp.array([0.123, 0.345, 0.456])     s_xyz = jnp.exp(jnp.array(2.0)) + s_xyz     scale = t * jnp.exp(jnp.array(2.2)) + d * jnp.exp(jnp.array(2.5)) + 1e6     s = jnp.einsum(""i,x>ix"", scale, s_xyz)     return s def m(s, d, d_mirror, rate):     scale = 0.5 * (15.0 / 75)     m = (         scale         * rate         / ((2 * jnp.pi) ** (3 / 2) * jnp.prod(s))         * (              removing these two jnp.exp calls appears to resolve the issue             (jnp.exp(0.5 * jnp.sum(d**2 / s**2)))             + (jnp.exp(0.5 * jnp.sum(d_mirror**2 / s**2)))         )     )     return m def func(y0, dl, tss, rng, rate, tidx_offset):     ys = wp(y0, tss, rng, tidx_offset)     d = ys  dl     A = jnp.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])     ys_mirror = jnp.matmul(ys, A)     d_mirror = ys_mirror  dl     scale = 0.5 * (15.0 / 75)     rate = rate[tidx_offset]     s = jnp.ones((71, 3)) * y0 * 2.3     results = vmap(m, in_axes=(0, 0, 0, None))(s, d, d_mirror, rate)     return results .jit def run():     y0s = jnp.ones(shape=(1, 3))      dls shape of ~1600+ fails on 6GB GPU trying to allocate 5GB+      dls shape of <1590 succeeds on 6GB GPU and uses only ~476kB memory according to pprof      dls shape of 10000 fails trying to allocate 30GB, but passes and only uses ~2MB when removing jnp.exp calls above     dls = jnp.ones(shape=(3540, 3))     rates = jnp.ones(shape=(1, 71))     rngs = jnp.ones(shape=(71, 75, 2), dtype=""uint32"")     tss = jnp.ones(shape=(71, 11, 75))     tidx_offsets = jnp.arange(len(tss))     output = vmap(         vmap(             vmap(                 vmap(func, in_axes=(None, None, 1, 0, None, None)),                 in_axes=(None, None, 0, 0, None, 0),             ),             in_axes=(None, 0, None, None, None, None),         ),         in_axes=(0, None, None, None, 0, None),     )(y0s, dls, tss, rngs, rates, tidx_offsets)     result = jnp.sum(output, axis=(0, 2, 3))     return result result = run() jax.profiler.save_device_memory_profile(""memory.prof"") print(result) print(result.shape) ```  System info (python version, jaxlib version, accelerator, etc.) Pip versions: ```  jax jax==0.4.31 jaxcuda12pjrt==0.4.31 jaxcuda12plugin==0.4.31 jaxlib==0.4.31 jaxtyping==0.2.34  nvidia nvidiacublascu12==12.6.1.4 nvidiacudacupticu12==12.6.68 nvidiacudanvcccu12==12.6.68 nvidiacudaruntimecu12==12.6.68 nvidiacudnncu12==9.3.0.75 nvidiacufftcu12==11.2.6.59 nvidiacusolvercu12==11.6.4.69 nvidiacusparsecu12==12.5.3.3 nvidiancclcu12==2.22.3 nvidianvjitlinkcu12==12.6.68 ``` Output of `jax.print_environment_info()`, it is running inside a container based on `nvidia/cuda:12.3.2baseubuntu22.04`: ``` jax:    0.4.31 jaxlib: 0.4.31 numpy:  1.26.4 python: 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0] jax.devices (1 total, 1 local): [CudaDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='dockerdesktop', release='5.15.153.1microsoftstandardWSL2', version=' CC(Python 3 compatibility issues) SMP Fri Mar 29 23:14:13 UTC 2024', machine='x86_64') $ nvidiasmi Tue Sep 10 20:44:01 2024        ++  ++ ``` Pip versions of latest version that does not show the error (v0.4.14): ```  jax  jax==0.4.14 jaxlib==0.4.14+cuda12.cudnn89 jaxtyping==0.2.23  nvidia nvidiacublascu12==12.1.3.1 nvidiacudacupticu12==12.1.105 nvidiacudanvcccu12==12.1.105 nvidiacudaruntimecu12==12.1.105 nvidiacudnncu12==8.9.2.26 nvidiacufftcu12==11.0.2.54 nvidiacusolvercu12==11.4.5.107 nvidiacusparsecu12==12.1.0.106 nvidiancclcu12==2.18.1 nvidianvjitlinkcu12==12.1.105 ```",2024-09-10T21:00:47Z,bug,open,0,2,https://github.com/jax-ml/jax/issues/23548,"I checked the HLO when using `dls=jnp.ones(shape=(10000, 3))` but it does indeed look like some very large tensors are being generated by your program (1 x 10000 x 71 x 75 x 71 x3 ~= 40GB) ``` ENTRY main.152 {   constant.27 = f32[] constant(1)   broadcast.28 = f32[1,71]{1,0} broadcast(constant.27), dimensions={}   iota.29 = s32[71]{0} iota(), iota_dimension=0   ...   constant.15 = f32[3,3]{1,0} constant({ { 1, 0, 0 }, { 0, 1, 0 }, { 0, 0, 1 } })   dot.95 = f32[1,71,75,71,3]{4,3,2,1,0} dot(scatter.89, constant.15), lhs_contracting_dims={4}, rhs_contracting_dims={0}   reshape.96 = f32[1,1,71,75,71,3]{5,4,3,2,1,0} reshape(dot.95)   broadcast.97 = f32[1,1,71,75,71,3]{5,4,3,2,1,0} broadcast(reshape.96), dimensions={0,1,2,3,4,5}   reshape.98 = f32[1,71,75,71,3]{4,3,2,1,0} reshape(broadcast.97)   broadcast.99 = f32[1,10000,71,75,71,3]{5,4,3,2,1,0} broadcast(reshape.98), dimensions={0,2,3,4,5}   subtract.100 = f32[1,10000,71,75,71,3]{5,4,3,2,1,0} subtract(broadcast.99, broadcast.17)   multiply.132 = f32[1,10000,71,75,71,3]{5,4,3,2,1,0} multiply(subtract.100, subtract.100)   divide.133 = f32[1,10000,71,75,71,3]{5,4,3,2,1,0} divide(multiply.132, broadcast.4)   reduce.138 = f32[1,10000,71,75,71]{4,3,2,1,0} reduce(divide.133, constant.25), dimensions={5}, to_apply=region_3.134   multiply.139 = f32[1,10000,71,75,71]{4,3,2,1,0} multiply(reduce.138, broadcast.2)   exponential.140 = f32[1,10000,71,75,71]{4,3,2,1,0} exponential(multiply.139)   add.141 = f32[1,10000,71,75,71]{4,3,2,1,0} add(exponential.131, exponential.140)   multiply.146 = f32[1,10000,71,75,71]{4,3,2,1,0} multiply(broadcast.145, add.141)   ROOT reduce.151 = f32[10000,71]{1,0} reduce(multiply.146, constant.25), dimensions={0,2,3}, to_apply=region_4.147 } ``` After commenting out the two lines containing exp these large tensors are not materialized: ```   ...   constant.12 = f32[] constant(1)   reduce.27 = f32[1,71]{1,0} reduce(broadcast.6, constant.12), dimensions={2}, to_apply=region_0.23   constant.1 = f32[] constant(15.7496099)   broadcast.2 = f32[1,71]{1,0} broadcast(constant.1), dimensions={}   multiply.28 = f32[1,71]{1,0} multiply(reduce.27, broadcast.2)   reshape.29 = f32[1,1,71]{2,1,0} reshape(multiply.28)   broadcast.34 = f32[1,1,71]{2,1,0} broadcast(reshape.29), dimensions={0,1,2}   reshape.35 = f32[1,71]{1,0} reshape(broadcast.34)   broadcast.36 = f32[1,71,71]{2,1,0} broadcast(reshape.35), dimensions={0,2}   divide.37 = f32[1,71,71]{2,1,0} divide(broadcast.33, broadcast.36)   broadcast.38 = f32[1,10000,71,75,71]{4,3,2,1,0} broadcast(divide.37), dimensions={0,2,4}   constant.11 = f32[] constant(0)   ROOT reduce.43 = f32[10000,71]{1,0} reduce(broadcast.38, constant.11), dimensions={0,2,3}, to_apply=region_1.39 } ``` I'm not sure why thus code runs on Jax =0.4.30) `jax.xla_computation(run)().as_hlo_text()` (for <0.4.30)","Thanks for the response. I'm starting to think it is some change in openxla or lower that is responsible rather than jax itself. A few questions:   what part of the hlo text shows when a tensor is ""materialized""? is there docs/links on how to read these outputs?   what's the difference between `func.lower().as_text()` and `run.lower().compiler_ir(dialect='hlo').as_hlo_text()`?   how do you determine expected memory based on tensor shape? Does this seem like a bug or just an old edge case not working anymore do you think? When using `dls=jnp.ones(shape=(1590, 3))` the program ran successfully and pprof reported ~500kB of memory usage, but increasing to  `dls=jnp.ones(shape=(1600, 3))` fails trying to allocate ~5GB, which seems like strange behavior."
yi,trying on another readme header,More numerical computing. Hopefully fewer line breaks.,2024-09-10T17:17:15Z,pull ready,closed,1,0,https://github.com/jax-ml/jax/issues/23545
yi,All-gather performed in fp32 instead of bf16 despite explicit datatype conversion," Description Hi, I've implemented RoPE along with llama3.1style context parallelism in JAX. In this setup each GPU gets its own chunk of sequence, calculates query, key and value for sequence chunk independently. Then, allgather is performed for key and value tensors so each GPU can calculate attention for its own query chunk.  After calculating query and key, I'm applying RoPE to them, which is embarrassingly parallel over sequence dimension. During RoPE calculation, query and key temporarily converted to fp32 and converted back to bf16 when calculation is done. One problem I'm facing right now is that instead of first converting chunk of K, V to bf16 independently on each device and then performing allgather, JAX/XLA reorders them so allgather is performed on fp32 tensors and then entire gathered K and V tensor is converted to bf16, effectively doubling communication volume that is intended to be performed over slow internode network.  Here is how it looks in compiled HLO: ```   %allgatherstart = (f32[16,1024,32,128]{3,2,0,1}, f32[16,8192,32,128]{3,2,0,1}) allgatherstart(f32[16,1024,32,128]{3,2,0,1} %bitcast.138.0), channel_id=1, replica_groups={{0,1,2,3,4,5,6,7}}, dimensions={1}, use_global_device_ids=true, metadata={op_name=""jit(forward)/jit(main)/convert_element_type[new_dtype=bfloat16 weak_type=False]"" source_file=""/papyrax/rope_repro.py"" source_line=35}, backend_config={""operation_queue_id"":""0"",""wait_on_operation_queues"":[],""collective_backend_config"":{""is_sync"":true,""no_parallel_custom_call"":false},""force_earliest_schedule"":false}   %allgatherdone = f32[16,8192,32,128]{3,2,0,1} allgatherdone((f32[16,1024,32,128]{3,2,0,1}, f32[16,8192,32,128]{3,2,0,1}) %allgatherstart), metadata={op_name=""jit(forward)/jit(main)/convert_element_type[new_dtype=bfloat16 weak_type=False]"" source_file=""/papyrax/rope_repro.py"" source_line=35}   %loop_transpose_fusion = bf16[16,8192,32,128]{3,2,1,0} fusion(f32[16,8192,32,128]{3,2,0,1} %allgatherdone), kind=kLoop, calls=%fused_transpose   %allgatherstart.1 = (f32[16,1024,32,128]{3,2,0,1}, f32[16,8192,32,128]{3,2,0,1}) allgatherstart(f32[16,1024,32,128]{3,2,0,1} %bitcast.169.0), channel_id=2, replica_groups={{0,1,2,3,4,5,6,7}}, dimensions={1}, use_global_device_ids=true, metadata={op_name=""jit(forward)/jit(main)/convert_element_type[new_dtype=bfloat16 weak_type=False]"" source_file=""/papyrax/rope_repro.py"" source_line=35}, backend_config={""operation_queue_id"":""0"",""wait_on_operation_queues"":[],""collective_backend_config"":{""is_sync"":true,""no_parallel_custom_call"":false},""force_earliest_schedule"":false}   %allgatherdone.1 = f32[16,8192,32,128]{3,2,0,1} allgatherdone((f32[16,1024,32,128]{3,2,0,1}, f32[16,8192,32,128]{3,2,0,1}) %allgatherstart.1), metadata={op_name=""jit(forward)/jit(main)/convert_element_type[new_dtype=bfloat16 weak_type=False]"" source_file=""/papyrax/rope_repro.py"" source_line=35}   %loop_transpose_fusion.1 = bf16[16,8192,32,128]{3,2,1,0} fusion(f32[16,8192,32,128]{3,2,0,1} %allgatherdone.1), kind=kLoop, calls=%fused_transpose.1   ROOT %tuple.1.0 = (bf16[16,8192,32,128]{3,2,1,0}, bf16[16,8192,32,128]{3,2,1,0}) tuple(bf16[16,8192,32,128]{3,2,1,0} %loop_transpose_fusion, bf16[16,8192,32,128]{3,2,1,0} %loop_transpose_fusion.1) ``` I was able to reproduce this behavior with the following script: ```python import jax import jax.numpy as jnp import numpy as np  RoPE implementation def map_positions_to_frequencies(     positions,     dim: int,     theta: float = 10000.0,     dtype: jnp.dtype = jnp.float32,     scaling_factor: float  ++ ```",2024-09-10T15:42:50Z,bug,open,14,1,https://github.com/jax-ml/jax/issues/23543,"Hi   This issue seems to have been resolved in JAX version 0.4.31. I tested the provided repro on GCP VM instance with 4 T4 GPUs with JAX versions 0.4.31 and 0.4.38. I can now see that the allgather performed in bf16 itself. Compiled HLO: ``` %allgatherstart = (bf16[16,2048,32,128]{3,2,0,1}, bf16[16,8192,32,128]{3,2,0,1}) allgatherstart(bf16[16,2048,32,128]{3,2,0,1} %bitcast.247.0), channel_id=1, replica_groups=[1,4]"" source_line=10}, backend_config={""operation_queue_id"":""0"",""wait_on_operation_queues"":[],""collective_backend_config"":{""is_sync"":true,""no_parallel_custom_call"":false,""is_pipelined"":false},""force_earliest_schedule"":false}\n   %allgatherdone = bf16[16,8192,32,128]{3,2,0,1} allgatherdone((bf16[16,2048,32,128]{3,2,0,1}, bf16[16,8192,32,128]{3,2,0,1}) %allgatherstart), metadata={op_name=""jit(forward)/jit(main)/convert_element_type"" source_file="""" source_line=10}\n   %loop_transpose_fusion.1 = bf16[16,8192,4096]{2,1,0} fusion(bf16[16,8192,32,128]{3,2,0,1} %allgatherdone), kind=kLoop, calls=%fused_transpose.1\n  %bitcast.256.0 = bf16[16,8192,32,128]{3,2,1,0} bitcast(bf16[16,8192,4096]{2,1,0} %loop_transpose_fusion.1)\n   %allgatherstart.1 = (bf16[16,2048,32,128]{3,2,0,1}, bf16[16,8192,32,128]{3,2,0,1}) allgatherstart(bf16[16,2048,32,128]{3,2,0,1} %bitcast.287.0), channel_id=2, replica_groups=[1,4]"" source_line=10}, backend_config={""operation_queue_id"":""0"",""wait_on_operation_queues"":[],""collective_backend_config"":{""is_sync"":true,""no_parallel_custom_call"":false,""is_pipelined"":false},""force_earliest_schedule"":false}\n   %allgatherdone.1 = bf16[16,8192,32,128]{3,2,0,1} allgatherdone((bf16[16,2048,32,128]{3,2,0,1}, bf16[16,8192,32,128]{3,2,0,1}) %allgatherstart.1), metadata={op_name=""jit(forward)/jit(main)/convert_element_type"" source_file="""" source_line=10}\n   %loop_transpose_fusion = bf16[16,8192,4096]{2,1,0} fusion(bf16[16,8192,32,128]{3,2,0,1} %allgatherdone.1), kind=kLoop, calls=%fused_transpose\n  %bitcast.296.0 = bf16[16,8192,32,128]{3,2,1,0} bitcast(bf16[16,8192,4096]{2,1,0} %loop_transpose_fusion)\n   ROOT %tuple.1.0 = (bf16[16,8192,32,128]{3,2,1,0}, bf16[16,8192,32,128]{3,2,1,0}) tuple(bf16[16,8192,32,128]{3,2,1,0} %bitcast.256.0, bf16[16,8192,32,128]{3,2,1,0} %bitcast.296.0)\n}\n\n' ``` Could you please verify if the issue is resolved? Thank you."
rag,[pallas:mosaic_gpu] Fragmented array debug printing.,[pallas:mosaic_gpu] Fragmented array debug printing.,2024-09-10T13:53:11Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23539
yi,add support for QR decomposition gradient calculation for wide matrices (rows < columns),"Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example. I would like to inquire about adding support for gradients for the wide case of QR factorization to JAX. To understand this better consider the following code snippet: ```python >>> rows,cols = 2,2  works fine for square matrices >>> key = jax.random.key(1337) >>> x = jax.random.normal(key, shape=(rows, cols)) >>>  >>> def loss(x): ...     q, r = jnp.linalg.qr(x) ...     rows, cols=r.shape ...     y1 = jnp.ones(rows) ...     y2 = jnp.ones(cols) ...     xQ = y1 ...     Ry = r ...     z = xQ + Ry ...     return jnp.mean(z ** 2) ...  >>>  >>> grad_loss = jax.grad(loss) >>>  >>> print('loss value', loss(x=x)) loss value 4.989746 >>> print('gradient of loss value', grad_loss(x)) gradient of loss value [[0.42152637 0.4215265 ]  [3.1307836  3.1307838 ]]  Note here we make a wide matrix (columns > rows) >>> rows,cols = 2,4 >>> x = jax.random.normal(key, shape=(rows, cols)) >>> print('loss value', loss(x=x)) loss value 7.9449735 >>>  this next line raises an exception >>> print('gradient of loss value', grad_loss(x)) Traceback (most recent call last):   File """", line 1, in    File """", line 2, in loss   File ""/usr/local/lib/python3.11/sitepackages/jax/_src/numpy/linalg.py"", line 1291, in qr     q, r = lax_linalg.qr(a, full_matrices=full_matrices) jax._src.source_info_util.JaxStackTraceBeforeTransformation: NotImplementedError: Unimplemented case of QR decomposition derivative The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File """", line 1, in    File """", line 2, in loss NotImplementedError: Unimplemented case of QR decomposition derivative  For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. >>>  ``` however you can take a gradient for a wide matrix through a QR decomposition provided the input matrix has full (column) rank. E.g. for matrix A, `rank(A) = min(rows, columns)`.  For this scenario we've written the derivations for the gradient as well as implemented in pytorch and tensorflow. Here is the arxiv draft of our paper and the corresponding PRs for pytorch and tensorflow, the latter for the complex matrix case. Here is the tensorflow  pr for the wide matrix real entry case.  **Paper clarifying notes:** 1.  for the wide case the gradient using equation 3.8 replaces the gradient of Q with the `Q_prime` gradient everywhere it occurs for the square matrix `X`.  2. The matrix `Y` for the wide case has a gradient that uses the same formulae regardless of whether equation 3.3 or equation 3.8 are used.  Here equations have the first number indicating the section of the paper, so equation 3.8 for example, is section 3 of arxiv draft of paper and equation 8 of that section.   Empirically, From the pytorch implementation I've found that as matrices increase in size then equation 3.3 is a somewhat faster runtime so that's another reason to prefer equation 3.3 for the calculations. It is also a simpler mathematical expression etc.  **Questions:**  1. Would jax maintainers accept a pr implementing the support for the wide matrix case?  2. If the answer to (1) is yes, is therepreferably mergedpr on the jax repo which implements a gradient in a matrix factorization that I could crib from? In lieue of this, any guidance on where to look and study in the repo would be appreciated. **Some notes on the idea of the proofs for the wide case:** 1. call the input matrix `A`, note that `A` has `rows > columns` and `rank(A) = min(rows, columns)`.  2. rank condition implies that we can partition `A = [X  V]`.  4. gradients for `Q` and `U`, `V` are formed from the backward pass and then used to construct the gradients of Q and R,  with the latter gradient formed by concatenatingthe inverse of matrix column partitioning is some sensethe U,V gradients to get R gradient.  5. These are further concatenated etc. to form the gradient of the `A` matrix, 2 equivalent formulae are given in the paper (equation 3.3 and 3.8) these are proven equivalent in the paper as well.",2024-09-10T01:38:46Z,enhancement,open,0,0,https://github.com/jax-ml/jax/issues/23533
yi,Bump setuptools from 69.2.0 to 74.1.2,"Bumps setuptools from 69.2.0 to 74.1.2.  Changelog Sourced from setuptools's changelog.  v74.1.2 Bugfixes  Fixed TypeError in sdist filelist processing by adding support for pathlib Paths for the build_base. ( CC(Printdebugging jax?)) Removed degraded and deprecated test_integration (easy_install) from the test suite. ( CC(Move jax.third_party to jax._src.third_party.))  v74.1.1 Bugfixes  Fixed TypeError in msvc.EnvironmentInfo.return_env when no runtime redistributables are installed. ( CC(Fix type problem in dynamic_slice_in_dim in int32 default dtype mode.))  v74.1.0 Features  Added support for defining extmodules via pyproject.toml (EXPERIMENTAL, may change in future releases). ( CC(Internal change))  Bugfixes  Merge with pypa/distutils, removing the duplicate vendored copy of packaging. ( CC(Omnistaging breaks jax.scipy.sparse.linalg.cg in some settings)) Restored setuptools.msvc.Environmentinfo as it is used externally. ( CC(Make pad more robust to nonhashable values.))  v74.0.0 Features  Changed the type of error raised by setuptools.command.easy_install.CommandSpec.from_param on unsupported argument from AttributeError to TypeError  by :user:Avasam ( CC(Ensure values returned by jax.random.truncated_normal() are in range.)) Added detection of ARM64 variant of MSVC  by :user:saschanaz ( CC(noop change to test source sync)) Made setuptools.package_index.Credential a typing.NamedTuple  by :user:Avasam ( CC(Is it possible to make nonzero jittable)) Reraise error from setuptools.command.easy_install.auto_chmod instead of nonsensical TypeError: 'Exception' object is not subscriptable  by :user:Avasam ( CC(Fix init with constant Poly shapes)) Fully typed all collection attributes in pkg_resources  by :user:Avasam ( CC([XLA:Python] Validate shapes in Python bindings to avoid crashes.)) Automatically exclude .tox.venv directories from sdist. ( CC(Optimize lax.associative_scan, reimplement cumsum, etc. on top of associative_scan.))    ... (truncated)   Commits  3b2ef1e Removed test_integration tests (for easy_install). c484f9e Bump version: 74.1.1 → 74.1.2 d8933c5 Merge pull request  CC(Reinstate jax.nn.functions since some users are making use of these (…) from pypa/bugfix/4615 a07de2b Skip test on stdlib distutils 6bf20d9 Add news fragment. 9d4b288 Enable the test 40ee221 Merge https://github.com/pypa/distutils into bugfix/4615 d901698 Add test capturing missed expectation. 91bc99a In sdist.prune_file_list, support build.build_base as a pathlib.Path. 7ee29bd Bump version: 74.1.0 → 74.1.1 Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-09-09T17:07:34Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23520,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
yi,"Bump etils[epath,epy] from 1.7.0 to 1.9.4","Bumps [etils[epath,epy]](https://github.com/google/etils) from 1.7.0 to 1.9.4.  Release notes Sourced from etils[epath,epy]'s releases.  v1.9.4  Return Python 3.10 support.  v1.9.3  eapp:  Allow extra kwargs in eapp.make_flags_parser()   epath:  Fix epath.Path pydantic deserialization for URIstyle paths   epy:  Add epy.is_test to check whether we're running in a test environment. Add epy.typing.Json. Add epy.ExitStack which allows setting the contextmanagers during init. Add proto support for epy.binary_adhoc Hide reraise from the traceback for cleaner error messages   exm:  Add exm.url_to_python_only_logs() and exm.curr_job_name() to add artifact to Python only logs (without all the verbose C++ junk ). Fix a bug which makes exm.current_experiment crash    v1.9.2  epath:  Support pydantic serialization of epath.Path    v1.9.1  epath:  Fix an infinite recursion on is_relative_to for Python&gt;=3.12.    v1.9.0  epy:  Add epy.lazy_api_imports to lazyimport __init__.py symbols. Removed: epy.cached_property epy.lazy_imports: Error callback accept a str to autoreraise with additional info. Fix mixing epy.lazy_imports() with epy.binary_adhoc().   ecolab:  Added reload_workspace=True to adhoc to autoreload from workspace Add ecolab.get_permalink() Fix ecolab.inspect not finding static files when the kernel contain partial etils deps.   epath:  Fix error when importlib.resources.files return MultiplexedPath Fix gs:// URI for 3.12 Fix .walk 3.12 error (topdown &gt; top_down rename)   Full compatibility with Python 3.12 (unit tests run on both 3.11 and 3.12).  v1.8.0  Drop Python 3.10 support. epy:  epy.pretty_repr: Add support for namedtuple   ecolab:    ... (truncated)   Changelog Sourced from etils[epath,epy]'s changelog.  [1.9.4]  20240903  Return Python 3.10 support.  [1.9.3]  20240830  eapp:  Allow extra kwargs in eapp.make_flags_parser()   epath:  Fix epath.Path pydantic deserialization for URIstyle paths   epy:  Add epy.is_test to check whether we're running in a test environment. Add epy.typing.Json. Add epy.ExitStack which allows setting the contextmanagers during init. Add proto support for epy.binary_adhoc Hide reraise from the traceback for cleaner error messages   exm:  Add exm.url_to_python_only_logs() and exm.curr_job_name() to add artifact to Python only logs (without all the verbose C++ junk ). Fix a bug which makes exm.current_experiment crash    [1.9.2]  20240612  epath:  Support pydantic serialization of epath.Path    [1.9.1]  20240604  epath:  Fix an infinite recursion on is_relative_to for Python&gt;=3.12.    [1.9.0]  20240531  epy:  Add epy.lazy_api_imports to lazyimport __init__.py symbols. Removed: epy.cached_property epy.lazy_imports: Error callback accept a str to autoreraise with additional info. Fix mixing epy.lazy_imports() with epy.binary_adhoc().   ecolab:  Added reload_workspace=True to adhoc to autoreload from workspace Add ecolab.get_permalink() Fix ecolab.inspect not finding static files when the kernel contain partial etils deps.   epath:  Fix error when importlib.resources.files return MultiplexedPath Fix gs:// URI for 3.12 Fix .walk 3.12 error (topdown &gt; top_down rename)   Full compatibility with Python 3.12 (unit tests run on both 3.11 and 3.12).    ... (truncated)   Commits  b7f74ac Release etils==1.9.4 69de9f9 Return support for Python 3.10 89a7f15 Release etils==1.9.3 06dcf57 Hide reraise_utils from the traceback for cleaner error messages 2cdb9ea Fix epath.Path pydantic deserialization for URIstyle paths 5098d67 Add proto support to binary_adhoc 4dcc80d Add epy.ExitStack a2d3b05 Refactor protoimport to epy.adhoc_utils 0892d5f Don't return anything for show() 573ddd0 Add exm.url_to_python_only_logs() for less verbose logs on XM Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-09-09T17:06:29Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23519,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
yi,Bump the pip group across 1 directory with 2 updates,"Bumps the pip group with 2 updates in the /build directory: zipp and setuptools. Updates `zipp` from 3.18.1 to 3.19.1  Changelog Sourced from zipp's changelog.  v3.19.1 Bugfixes  Improved handling of malformed zip files. ( CC(example Errata))  v3.19.0 Features  Implement is_symlink. ( CC(add oss test instructions, fix conv grad bug))  v3.18.2 No significant changes.    Commits  6d1cb72 Finalize fd604bd Merge pull request  CC(Can’t import both jax and tensorflow (causes kernel restart)) from jaraco/bugfix/119malformedpaths c18417e Add news fragment. 58115d2 Employ SanitizedNames in CompleteDirs. Fixes broken test. 564fcc1 Add SanitizedNames mixin. 79a309f Add some assertions about malformed paths. 2d015c2 Merge https://github.com/jaraco/skeleton a595a0f Rename extras to align with core metadata spec. 608f90a Finalize 3a22d72 Merge pull request  CC(add tensordot) from jaraco/feature/issymlink Additional commits viewable in compare view    Updates `setuptools` from 69.2.0 to 70.0.0  Changelog Sourced from setuptools's changelog.  v70.0.0 Features  Emit a warning when [tools.setuptools] is present in pyproject.toml and will be ignored.  by :user:SnoopJ ( CC(未找到相关数据)) Improved AttributeError error message if pkg_resources.EntryPoint.require is called without extras or distribution Gracefully &quot;do nothing&quot; when trying to activate a pkg_resources.Distribution with a None location, rather than raising a TypeError  by :user:Avasam ( CC([jax2tf] Add input and output names to the TensorFlow graph generated by jax2tf.)) Typed the dynamically defined variables from pkg_resources  by :user:Avasam ( CC(Pin pygments version in RTD build)) Modernized and refactored VCS handling in package_index. ( CC([jax2tf] Revert '[jax2tf] Replace tf.math.add with tf.raw_ops.AddV2 (…))  Bugfixes  In install command, use super to call the superclass methods. Avoids race conditions when monkeypatching from _distutils_system_mod occurs late. ( CC(Rename collectives into ""collective operations"" for the pmap function.)) Fix finder template for lenient editable installs of implicit nested namespaces constructed by using package_dir to reorganise directory structure. ( CC([jax2tf] Replace tf.math.add with tf.raw_ops.AddV2)) Fix an error with UnicodeDecodeError handling in pkg_resources when trying to read files in UTF8 with a fallback  by :user:Avasam ( CC(Ppermute batching errors))  Improved Documentation  Uses RST substitution to put badges in 1 line. ( CC(improve an escaped tracer error message))  Deprecations and Removals   Further adoption of UTF8 in setuptools. This change regards mostly files produced and consumed during the build process (e.g. metadata files, script wrappers, automatically updated config files, etc..) Although precautions were taken to minimize disruptions, some edge cases might be subject to backwards incompatibility. Support for &quot;locale&quot; encoding is now deprecated. ( CC(Help with array slice indices))   Remove setuptools.convert_path after long deprecation period. This function was never defined by setuptools itself, but rather a sideeffect of an import for internal usage. ( CC(Allow custom_linear_solve to return things besides the solution))   Remove fallback for customisations of distutils' build.sub_command after long deprecated period. Users are advised to import build directly from setuptools.command.build. ( CC(Allow custom_linear_solve to return things besides the solution))   Removed typing_extensions from vendored dependencies  by :user:Avasam ( CC(Hacky approach to carry argument of an invertible transformation in the backward pass))   Remove deprecated setuptools.dep_util. The provided alternative is setuptools.modified. ( CC([jax2tf] Added support for shape polymorphism conversion.))     ... (truncated)   Commits  5cbf12a Workaround for release error in v70 9c1bcc3 Bump version: 69.5.1 → 70.0.0 4dc0c31 Remove deprecated setuptools.dep_util ( CC([jax2tf] Added support for shape polymorphism conversion.)) 6c1ef57 Remove xfail now that test passes. Ref  CC(jnp.moveaxis: fix bug when axes are integer dtype). d14fa01 Add all sitepackages dirs when creating simulated environment for test_edita... 6b7f7a1 Prevent bin folders to be taken as extern packages when vendoring ( CC(Update scale_and_translate to take an explicit spatial_dims.)) 69141f6 Add doctest for vendorised bin folder 2a53cc1 Prevent 'bin' folders to be taken as extern packages 7208628 Replace call to deprecated validate_pyproject command ( CC(Add a prototype implementation of recursive checkpointing)) 96d681a Remove call to deprecated validate_pyproject command Additional commits viewable in compare view    Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore  major version` will close this group update PR and stop Dependabot creating any more for the specific dependency's major version (unless you unignore this specific dependency's major version or upgrade to it yourself)  ` ignore  minor version` will close this group update PR and stop Dependabot creating any more for the specific dependency's minor version (unless you unignore this specific dependency's minor version or upgrade to it yourself)  ` ignore ` will close this group update PR and stop Dependabot creating any more for the specific dependency (unless you unignore this specific dependency or upgrade to it yourself)  ` unignore ` will remove all of the ignore conditions of the specified dependency  ` unignore  ` will remove the ignore condition of the specified dependency and ignore conditions You can disable automated security fix PRs for this repo from the Security Alerts page. ",2024-09-09T16:44:19Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23517,"This pull request was built based on a group rule. Closing it will not ignore any of these versions in future pull requests. To ignore these dependencies, configure ignore rules in dependabot.yml"
dspy,"[ROCM] x64 mode crashes with ""redzone_checker with block dimensions: 1024x1x1: hipError_t"""," Description I'm running master jax/lib custombuilt for RoCm following the instructions online (because there are no such wheels available around).  However I'm relatively sure this thing is not because of my custom wheels, but it's an issue within XLA/Jax The MWE is ```python import jax jax.config.update(""jax_enable_x64"", True) x= jax.numpy.ones((512, 25)) M = jax.numpy.ones((512, 512)) M ``` and it crashes at the multiplication with the following error. Do note that the script works fine if I do not enable x64 mode. ```python Traceback (most recent call last):   File """", line 1, in    File ""/opt/software/gaiaexternal/IA/jax/0.4.31/miniconda3/envs/myjax/lib/python3.11/sitepackages/jax/_src/numpy/array_methods.py"", line 272, in deferring_binary_op     return binary_op(*args)            ^^^^^^^^^^^^^^^^   File ""/opt/software/gaiaexternal/IA/jax/0.4.31/miniconda3/envs/myjax/lib/python3.11/sitepackages/jax/_src/traceback_util.py"", line 180, in reraise_with_filtered_traceback     return fun(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^   File ""/opt/software/gaiaexternal/IA/jax/0.4.31/miniconda3/envs/myjax/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 332, in cache_miss     outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked = _python_pjit_helper(                                                                 ^^^^^^^^^^^^^^^^^^^^   File ""/opt/software/gaiaexternal/IA/jax/0.4.31/miniconda3/envs/myjax/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 190, in _python_pjit_helper     out_flat = pjit_p.bind(*args_flat, **p.params)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/software/gaiaexternal/IA/jax/0.4.31/miniconda3/envs/myjax/lib/python3.11/sitepackages/jax/_src/core.py"", line 2739, in bind     return self.bind_with_trace(top_trace, args, params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/software/gaiaexternal/IA/jax/0.4.31/miniconda3/envs/myjax/lib/python3.11/sitepackages/jax/_src/core.py"", line 433, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/software/gaiaexternal/IA/jax/0.4.31/miniconda3/envs/myjax/lib/python3.11/sitepackages/jax/_src/core.py"", line 939, in process_primitive     return primitive.impl(*tracers, **params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/software/gaiaexternal/IA/jax/0.4.31/miniconda3/envs/myjax/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 1730, in _pjit_call_impl     return xc._xla.pjit(            ^^^^^^^^^^^^^   File ""/opt/software/gaiaexternal/IA/jax/0.4.31/miniconda3/envs/myjax/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 1712, in call_impl_cache_miss     out_flat, compiled = _pjit_call_impl_python(                          ^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/software/gaiaexternal/IA/jax/0.4.31/miniconda3/envs/myjax/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 1642, in _pjit_call_impl_python     ).compile(compile_options)       ^^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/software/gaiaexternal/IA/jax/0.4.31/miniconda3/envs/myjax/lib/python3.11/sitepackages/jax/_src/interpreters/pxla.py"", line 2295, in compile     executable = UnloadedMeshExecutable.from_hlo(                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/software/gaiaexternal/IA/jax/0.4.31/miniconda3/envs/myjax/lib/python3.11/sitepackages/jax/_src/interpreters/pxla.py"", line 2807, in from_hlo     xla_executable = _cached_compilation(                      ^^^^^^^^^^^^^^^^^^^^   File ""/opt/software/gaiaexternal/IA/jax/0.4.31/miniconda3/envs/myjax/lib/python3.11/sitepackages/jax/_src/interpreters/pxla.py"", line 2621, in _cached_compilation     xla_executable = compiler.compile_or_get_cached(                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/software/gaiaexternal/IA/jax/0.4.31/miniconda3/envs/myjax/lib/python3.11/sitepackages/jax/_src/compiler.py"", line 399, in compile_or_get_cached     return _compile_and_write_cache(            ^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/software/gaiaexternal/IA/jax/0.4.31/miniconda3/envs/myjax/lib/python3.11/sitepackages/jax/_src/compiler.py"", line 627, in _compile_and_write_cache     executable = backend_compile(                  ^^^^^^^^^^^^^^^^   File ""/opt/software/gaiaexternal/IA/jax/0.4.31/miniconda3/envs/myjax/lib/python3.11/sitepackages/jax/_src/profiler.py"", line 336, in wrapper     return func(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^^   File ""/opt/software/gaiaexternal/IA/jax/0.4.31/miniconda3/envs/myjax/lib/python3.11/sitepackages/jax/_src/compiler.py"", line 267, in backend_compile     return backend.compile(built_c, compile_options=options)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to launch ROCm kernel: redzone_checker with block dimensions: 1024x1x1: hipError_t(303) ```  System info (python version, jaxlib version, accelerator, etc.) ``` >>> jax. KeyboardInterrupt >>> import jax; jax.print_environment_info() jax:    0.4.31 jaxlib: 0.4.31.dev20240909 numpy:  2.0.2 python: 3.11.8  (main, Feb 16 2024, 20:53:32) [GCC 12.3.0] jax.devices (8 total, 8 local): [RocmDevice(id=0) RocmDevice(id=1) ... RocmDevice(id=6) RocmDevice(id=7)] process_count: 1 platform: uname_result(system='Linux', node='g1176', release='4.18.0477.10.1.el8_8.x86_64', version=' CC(Python 3 compatibility issues) SMP Wed Apr 5 13:35:01 EDT 2023', machine='x86_64') ```",2024-09-09T12:44:51Z,bug XLA AMD GPU,open,0,6,https://github.com/jax-ml/jax/issues/23506,"These are typically issues with the hipblaslt autotuning not being able to run its autotuning kernels correctly. I have been seeing a lot of these lately and will be opening an issue to the XLA folks to look into them more. If you could gather some information for me to forward on to them that would be a big help. 1) Can you confirm that the issue is only present with x64 flag set? 2) What card model were you running this against? (MI100 / MI250 / etc) 3) Can you try running the same scenario but disabling the hipblaslt autotuning by doing `export XLA_FLAGS=""xla_gpu_autotune_level=0""` and report if it still fails or not?","1. Just tried again, and I confirm 2. It's an HPC node with 4x MI250X accelerators, which `rocmsmi` detects as 8 `AMD INSTINCT MI200` devices. 2b. I'm running with Rocm 6.0.0 because that's what the IT technicians provide on the machine (I cannot use virtualisation) 3. Running the script above with `export XLA_FLAGS=""xla_gpu_autotune_level=0""` works correctly with no failure.","Was able to replicate this on an MI300 system. ``` python3 matmul.py ROCm path: /opt/rocm6.0.0/lib jax_enable_x64: True jax version: 0.4.31 jaxlib version: 0.4.31 jax:    0.4.31 jaxlib: 0.4.31 numpy:  2.1.1 python: 3.11.10 (main, Sep  8 2024, 14:18:29) [GCC 12.2.1 20221121 (Red Hat 12.2.17)] jax.devices (8 total, 8 local): [RocmDevice(id=0) RocmDevice(id=1) ... RocmDevice(id=6) RocmDevice(id=7)] process_count: 1 platform: uname_result(system='Linux', node='...', release='5.15.0119generic', version=' CC(CUDA backend produces inconsistent results for jax.numpy.linalg.inv)Ubuntu SMP Fri Aug 2 19:25:20 UTC 2024', machine='x86_64') Traceback (most recent call last):   File ""/jaxbuildr/matmul.py"", line 38, in      c = jax.numpy.matmul(b, a)         ^^^^^^^^^^^^^^^^^^^^^^ jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to launch ROCm kernel: redzone_checker with block dimensions: 1024x1x1: hipError_t(303)  For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. ``` Don't mind the ubuntu in 'uname'. I had ran it in an almalinux8 container on a ubuntu host. Going to do more testing tomorrow to see if its only ROCm 6.0.x","I am not able to reproduce it on ROCm 6.0 on gfx90a platform (AMD INSTINCT MI200 ). > docker: rocm/jax:rocm6.0.0jax0.4.26py3.11.0 Cloned Jax/xla > git clone b rocmjaxlibv0.4.31 https://github.com/ROCm/jax.git > git clone b rocmjaxlibv0.4.31 https://github.com/ROCm/xla.git build/Install JAX locally using command below > rm rf dist; python3 m pip uninstall jax jaxlib jaxrocm60pjrt jaxrocm60plugin y; python3 ./build/build.py  use_clang=false enable_rocm build_gpu_plugin gpu_plugin_rocm_version=60 rocm_amdgpu_targets=gfx90a bazel_options=override_repository=xla=/workspaces/jax_xla/rocmjaxlibv0.4.31/xla rocm_path=/opt/rocm6.0.0/ && python3 setup.py develop user && python3 m pip install dist/*.whl Installed JAX > jax                  0.4.31.dev20240808+a96cefdc0 /workspaces/jax_xla/rocmjaxlibv0.4.31/jax > jaxrocm60pjrt      0.4.31.dev20240913 > jaxrocm60plugin    0.4.31.dev20240913 > jaxlib               0.4.31.dev20240913 >>> import jax >>> import jax.numpy as jnp >>> jax.config.update(""jax_enable_x64"", True) >>> x= jax.numpy.ones((512, 25)) >>> M = jax.numpy.ones((512, 512)) >>> z=jnp.matmul(M, x) >>> print(z) [[512. 512. 512. ... 512. 512. 512.]  [512. 512. 512. ... 512. 512. 512.]  [512. 512. 512. ... 512. 512. 512.]  ...  [512. 512. 512. ... 512. 512. 512.]  [512. 512. 512. ... 512. 512. 512.]  [512. 512. 512. ... 512. 512. 512.]] >>>","Can I give you anything to help you identify the issue?  Is it enough that it shows up on MI300? A side note: on HPC systems we cannot use virtualisation/docker (and we don't have superuser privileges).  So I cannot guarantee that my setup is equivalent to yours.  I load a centrally installed ROCm 6.0.0 library, so I can give you any detail you might want about it.  But Docker is intentionally blocked.","I was also able to reproduce this problem on MI300 system with ROCM6.0.0 container (while it runs fine under ROCM 6.0.2 and above).  Actually, on rocmjaxlibv0.4.31 XLA/JAX branch, it also fails without setting **jax_enable_x64** flag when one disables triton_gemm with: **xla_gpu_enable_triton_gemm=false**.  We are investigating the issue. As a workaround, one can still use autotuning with redzone checker disabled via: **xla_gpu_autotune_level=3**."
yi,Pallas flash attention fails on GPU," Description Trying to use flash attention on a GPU, hitting a Value error straight away. code: ``` from jax.experimental.pallas.ops.tpu import flash_attention as attn_lib x = jnp.ones((2, 32, 1024, 128), dtype=jnp.float32) out = attn_lib.flash_attention(x, x, x) ``` error: ``` Traceback (most recent call last):   File ""..."", line 19, in      out = attn_lib.flash_attention(x, x, x)   File ""/home/dev/.local/lib/python3.11/sitepackages/jax/experimental/pallas/ops/tpu/flash_attention.py"", line 198, in flash_attention     return _flash_attention(   File ""/home/dev/.local/lib/python3.11/sitepackages/jax/experimental/pallas/ops/tpu/flash_attention.py"", line 216, in _flash_attention     return _flash_attention_impl(   File ""/home/dev/.local/lib/python3.11/sitepackages/jax/experimental/pallas/ops/tpu/flash_attention.py"", line 737, in _flash_attention_impl     o, *aux = pl.pallas_call(   File ""/home/dev/.local/lib/python3.11/sitepackages/jax/_src/pallas/pallas_call.py"", line 1106, in wrapped     out_flat = pallas_call_p.bind( jax._src.source_info_util.JaxStackTraceBeforeTransformation: ValueError: safe_zip() argument 2 is shorter than argument 1 The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/run/host/home/knyaz/workspace/nano_jax_gpt/model.py"", line 19, in      out = attn_lib.flash_attention(x, x, x)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/dev/.local/lib/python3.11/sitepackages/jax/_src/pallas/pallas_call.py"", line 948, in _pallas_call_lowering     return mlir.lower_per_platform(ctx, ""pallas_call"",            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/dev/.local/lib/python3.11/sitepackages/jax/_src/pallas/pallas_call.py"", line 944, in gpu_lowering     return pallas_call_registration.pallas_call_lowering(            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/dev/.local/lib/python3.11/sitepackages/jax/_src/pallas/triton/pallas_call_registration.py"", line 73, in pallas_call_lowering     lowering_result = lowering.lower_jaxpr_to_triton_module(                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/dev/.local/lib/python3.11/sitepackages/jax/_src/pallas/triton/lowering.py"", line 335, in lower_jaxpr_to_triton_module     () = lower_jaxpr_to_triton_ir(ctx, jaxpr, block_infos, *entry.arguments)          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/dev/.local/lib/python3.11/sitepackages/jax/_src/pallas/triton/lowering.py"", line 361, in lower_jaxpr_to_triton_ir     for invar, block_info in zip(jaxpr.invars, block_infos):                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ValueError: safe_zip() argument 2 is shorter than argument 1 ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.31 jaxlib: 0.4.31 numpy:  1.26.4 python: 3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 11.4.0] jax.devices (1 total, 1 local): [CudaDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='dev.knyaz', release='6.8.040generic', version=' CC(Invalid proto descriptor for file ""tensorflow/compiler/xla/xla_data.proto"")~22.04.3Ubuntu SMP PREEMPT_DYNAMIC Tue Jul 30 17:30:19 UTC 2', machine='x86_64') $ nvidiasmi Sun Sep  8 11:36:36 2024 ++ | NVIDIASMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2 ```",2024-09-08T09:38:54Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/23495,"I wonder, is it actually supposed to work on GPU? I was always assuming that Pallas should feel no difference between a TPU and GPU, but maybe there is a difference? I don't know..","Right, so this works:```    from jax.experimental.pallas.ops.gpu import attention     bs = 2     seqlen = 1024     n_heads = 32     dim = 128     x = jnp.ones((bs, n_heads, seqlen, dim), dtype=jnp.float32)     out = attention.mha(x, x, x, None, causal=True) ``` So I will just stick with the correct ops. Sorry for the noise!"
yi,Jax numpy reduceat function throws error," Description Trying to run `np.add.reduceat(data, indices=indices, axis=0)` give the below error: ``` AttributeError: 'jaxlib.xla_extension.PjitFunction' object has no attribute 'reduceat' ``` This is confusing since the documentation says it should work. Here is a snippet to reproduce the issue: ```python x = jnp.ones((5, 10)) indices = jnp.array([0, 2]) jnp.add.reduceat(x, indices, axis=0) ```  System info (python version, jaxlib version, accelerator, etc.) ``` python 3.10.12 ``` ``` jax:    0.4.31 jaxlib: 0.4.31 numpy:  1.26.4 python: 3.10.12 (main, Feb 26 2024, 17:15:51) [Clang 14.0.0 (clang1400.0.29.202)] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Darwin', node='CCC02FP362Q05P', release='21.6.0', version='Darwin Kernel Version 21.6.0: Mon Dec 19 20:46:01 PST 2022; root:xnu8020.240.18~2/RELEASE_ARM64_T8101', machine='arm64') ```",2024-09-07T21:59:39Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/23493,"Thanks for the report and the repro! The full ufunc support described in the docs was just added a week or two ago in https://github.com/google/jax/pull/23304 so this feature isn't available in any release JAX version as far as I know. You can install JAX from source (I think it would be fine to use the released jaxlib, so source installation wouldn't be too painful) if you need this right away, or I think there will be a new JAX release in the next week or two if it can wait.",Oh got it. Thanks for the info . Is there a way we can schedule docs updates to coincide with releases?,"There have been requests for more explicit versioning of the docs previously, but that's a bit tricky given how JAX is developed, and we release ~once a month so we've always just landed on the side of always publishing the docs at HEAD. When in doubt, check out the git blame in the source code to see when something was added!",Gotcha. I'll close this issue then since it seems like I can get this feature in a few weeks.
yi,Force a physical axis layout rearrangement,"I am trying to optimise a function where I think it'd be much faster if I could force the physical transpose to a pre and op, rather than being fused into the op.  optimisation_barrier has been super useful for forcing op ordering, however as JAX works on logical axes the physical axis order ops like transpose can not be forced. It would be useful to me to have a separate op which can force the physical axis ordering of an array? I know Pallas and custom_call can enforce axis ordering [with operand_layout_constraints], would a noop customcall with layout constraint be the easiest way to do it? For reference, below is a dummy use case, forcing a physical transpose then sum, rather than direct sum ```python import jax import jax.numpy as jnp x = jnp.ones((8192, 4096)) f_transpose = jax.jit(lambda x: (x.swapaxes(0,1)).sum()) f_direct = jax.jit(lambda x: x.sum()) print(""Lowered"") print(f_transpose.lower(x).as_text()) print(f_direct.lower(x).as_text()) print(""Compiled"") print(f_transpose.lower(x).compile().as_text()) print(f_direct.lower(x).compile().as_text()) ``` Output: ``` Lowered module  attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {   func.func public (%arg0: tensor {mhlo.layout_mode = ""default""}) > (tensor {jax.result_info = """", mhlo.layout_mode = ""default""}) {     %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor) > tensor     %1 = stablehlo.constant dense : tensor     %2 = stablehlo.reduce(%0 init: %1) applies stablehlo.add across dimensions = [0, 1] : (tensor, tensor) > tensor     return %2 : tensor   } } module  attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {   func.func public (%arg0: tensor {mhlo.layout_mode = ""default""}) > (tensor {jax.result_info = """", mhlo.layout_mode = ""default""}) {     %0 = stablehlo.constant dense : tensor     %1 = stablehlo.reduce(%arg0 init: %0) applies stablehlo.add across dimensions = [0, 1] : (tensor, tensor) > tensor     return %1 : tensor   } } Compiled HloModule jit__lambda_, entry_computation_layout={(f32[8192,4096]{1,0})>f32[]}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true} %region_0.4 (Arg_0.5: f32[], Arg_1.6: f32[]) > f32[] {   %Arg_0.5 = f32[] parameter(0)   %Arg_1.6 = f32[] parameter(1)   ROOT %add.7 = f32[] add(f32[] %Arg_0.5, f32[] %Arg_1.6), metadata={op_name=""jit()/jit(main)/reduce_sum[axes=(0, 1)]"" source_file="""" source_line=5} } ENTRY %main.9 (Arg_0.1: f32[8192,4096]) > f32[] {   %Arg_0.1 = f32[8192,4096]{1,0} parameter(0)   %constant.2 = f32[] constant(0)   %reducewindow = f32[256,128]{1,0} reducewindow(f32[8192,4096]{1,0} %Arg_0.1, f32[] %constant.2), window={size=32x32 stride=32x32}, to_apply=%region_0.4   %reducewindow.1 = f32[8,4]{1,0} reducewindow(f32[256,128]{1,0} %reducewindow, f32[] %constant.2), window={size=32x32 stride=32x32}, to_apply=%region_0.4   ROOT %reduce.8 = f32[] reduce(f32[8,4]{1,0} %reducewindow.1, f32[] %constant.2), dimensions={1,0}, to_apply=%region_0.4, metadata={op_name=""jit()/jit(main)/reduce_sum[axes=(0, 1)]"" source_file="""" source_line=5} } HloModule jit__lambda_, entry_computation_layout={(f32[8192,4096]{1,0})>f32[]}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true} %region_0.3 (Arg_0.4: f32[], Arg_1.5: f32[]) > f32[] {   %Arg_0.4 = f32[] parameter(0)   %Arg_1.5 = f32[] parameter(1)   ROOT %add.6 = f32[] add(f32[] %Arg_0.4, f32[] %Arg_1.5), metadata={op_name=""jit()/jit(main)/reduce_sum[axes=(0, 1)]"" source_file="""" source_line=6} } ENTRY %main.8 (Arg_0.1: f32[8192,4096]) > f32[] {   %Arg_0.1 = f32[8192,4096]{1,0} parameter(0)   %constant.2 = f32[] constant(0)   %reducewindow = f32[256,128]{1,0} reducewindow(f32[8192,4096]{1,0} %Arg_0.1, f32[] %constant.2), window={size=32x32 stride=32x32}, to_apply=%region_0.3   %reducewindow.1 = f32[8,4]{1,0} reducewindow(f32[256,128]{1,0} %reducewindow, f32[] %constant.2), window={size=32x32 stride=32x32}, to_apply=%region_0.3   ROOT %reduce.7 = f32[] reduce(f32[8,4]{1,0} %reducewindow.1, f32[] %constant.2), dimensions={0,1}, to_apply=%region_0.3, metadata={op_name=""jit()/jit(main)/reduce_sum[axes=(0, 1)]"" source_file="""" source_line=6} } ```",2024-09-06T09:04:29Z,enhancement,closed,0,3,https://github.com/jax-ml/jax/issues/23471,"I've written something which runs a transpose which enforces that logical layout on the physical layout Any advice on whether it will work if the input layout is not major_to_minor in the operand, or is the LayoutConstraint op somehow inferring this correctly? [Optimisation barrier seems to be required on TPU to force the transpose, but not on GPU, presumably tpu specific compiler pass was removing it] ```python from jax import core from jax.interpreters import mlir from jax._src.interpreters.mlir import eval_dynamic_shape_as_tensor from jax._src import ad_checkpoint transpose_on_device_p = core.Primitive(""transpose_on_device"") def transpose_on_device(x, permutation):   """"""   A relative of jax.lax.transpose, but the output physical layout    is forced to match the logical permuted layout.   """"""   x = transpose_on_device_p.bind(       x,        physical_axis_order=permutation[::1]   ).transpose(permutation)    NOTE: optimisation barrier as compiler passes can remove on TPU otherwise   return ad_checkpoint._optimization_barrier(x) def transpose_on_device_impl(x, physical_axis_order):   del physical_axis_order   return x def transpose_on_device_abstract_eval(xs, physical_axis_order):   del physical_axis_order   return core.ShapedArray(xs.shape, xs.dtype) def transpose_on_device_lowering(ctx, xc, physical_axis_order):   out_shape = xc.type.shape   if core.is_constant_shape(out_shape):     result_shapes = None   else:     result_shapes = [eval_dynamic_shape_as_tensor(ctx, out_shape)]   op = mlir.custom_call('LayoutConstraint', result_types=[xc.type], operands=[xc],                    api_version=1,                    result_shapes=result_shapes,                     Set operand layouts to anything. XLA will ignore it.                    operand_layouts=[tuple(range(len(physical_axis_order)))],   type: ignore                     TODO(yashkatariya): Figure out how to pass tiling to the                     custom call.                    result_layouts=[physical_axis_order],                   )   return [op.result] transpose_on_device_p.def_impl(transpose_on_device_impl) transpose_on_device_p.def_abstract_eval(transpose_on_device_abstract_eval) mlir.register_lowering(transpose_on_device_p, transpose_on_device_lowering) import jax import jax.numpy as jnp x = jnp.ones((8, 32, 4096, 16)) f_transpose = jax.jit(lambda x: transpose_on_device(x, (0,3,2,1)).sum()) f_direct = jax.jit(lambda x: x.sum()) print(""Lowered"") print(f_transpose.lower(x).as_text()) print(f_direct.lower(x).as_text()) print(""Compiled"") print(f_transpose.lower(x).compile().as_text()) print(f_direct.lower(x).compile().as_text()) ```",We have a layout API in JAX that you can use: See https://github.com/google/jax/blob/ef947a0ce673201b60763fe1f8b26c1c7dcd0fbc/tests/layout_test.pyL4 If you want to specify concrete layouts then check out: https://github.com/google/jax/blob/ef947a0ce673201b60763fe1f8b26c1c7dcd0fbc/tests/layout_test.pyL285,So you probably don't need your own primitive to do this?
yi,Add support for the DeviceLocalLayout API when lowering FFI calls.,"This PR updates the FFI lowering rule to support a DeviceLoweringLayout object as input when specifying the input and output layouts. For now, this just converts the DLL object to its appropriate list of minortomajor integers because that's what the custom call op expects.",2024-09-05T16:25:18Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/23458
transformer,Multi-process GPU jobs fail on Slurm," Description I'm submitting multiprocess jobs on slurm. The job script is ``` !/bin/bash SBATCH p gpu SBATCH nodes=1 SBATCH ntaskspernode=2 SBATCH gpuspertask=1 SBATCH time=000:10:00 SBATCH mempercpu=32G SBATCH constraint=a10040gb module load cuda/12.3.2 conda activate main srun cpubind=socket python $1 ``` I test with a simple python code, like ``` import jax jax.distributed.initialize() print(jax.devices()) ``` But it can't see the devices and raises the error ``` 20240905 07:57:34.555804: W external/xla/xla/service/platform_util.cc:199] unable to create StreamExecutor for CUDA:1: failed initializing StreamExecutor for CUDA device ordinal 1: INTERNAL: Failed call to cuDeviceGet: CUDA_ERROR_INVALID_DEVICE: invalid device ordinal Traceback (most recent call last):   File ""/mnt/home/achen1/miniconda3/envs/main/lib/python3.9/sitepackages/jax/_src/xla_bridge.py"", line 879, in backends     backend = _init_backend(platform)   File ""/mnt/home/achen1/miniconda3/envs/main/lib/python3.9/sitepackages/jax/_src/xla_bridge.py"", line 970, in _init_backend     backend = registration.factory()   File ""/mnt/home/achen1/miniconda3/envs/main/lib/python3.9/sitepackages/jax/_src/xla_bridge.py"", line 676, in factory     return xla_client.make_c_api_client(   File ""/mnt/home/achen1/miniconda3/envs/main/lib/python3.9/sitepackages/jaxlib/xla_client.py"", line 200, in make_c_api_client     return _xla.get_c_api_client(plugin_name, options, distributed_client) jaxlib.xla_extension.XlaRuntimeError: INTERNAL: no supported devices found for platform CUDA During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""/mnt/home/achen1/Transformer/test/test.py"", line 3, in      print(jax.devices())   File ""/mnt/home/achen1/miniconda3/envs/main/lib/python3.9/sitepackages/jax/_src/xla_bridge.py"", line 1082, in devices     return get_backend(backend).devices()   File ""/mnt/home/achen1/miniconda3/envs/main/lib/python3.9/sitepackages/jax/_src/xla_bridge.py"", line 1016, in get_backend     return _get_backend_uncached(platform)   File ""/mnt/home/achen1/miniconda3/envs/main/lib/python3.9/sitepackages/jax/_src/xla_bridge.py"", line 995, in _get_backend_uncached     bs = backends()   File ""/mnt/home/achen1/miniconda3/envs/main/lib/python3.9/sitepackages/jax/_src/xla_bridge.py"", line 895, in backends     raise RuntimeError(err_msg) RuntimeError: Unable to initialize backend 'cuda': INTERNAL: no supported devices found for platform CUDA (you may need to uninstall the failing plugin package, or set JAX_PLATFORMS=cpu to skip this backend.) Traceback (most recent call last):   File ""/mnt/home/achen1/miniconda3/envs/main/lib/python3.9/sitepackages/jax/_src/xla_bridge.py"", line 879, in backends     backend = _init_backend(platform)   File ""/mnt/home/achen1/miniconda3/envs/main/lib/python3.9/sitepackages/jax/_src/xla_bridge.py"", line 970, in _init_backend     backend = registration.factory()   File ""/mnt/home/achen1/miniconda3/envs/main/lib/python3.9/sitepackages/jax/_src/xla_bridge.py"", line 676, in factory     return xla_client.make_c_api_client(   File ""/mnt/home/achen1/miniconda3/envs/main/lib/python3.9/sitepackages/jaxlib/xla_client.py"", line 200, in make_c_api_client     return _xla.get_c_api_client(plugin_name, options, distributed_client) jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Getting local topologies failed: Error 1: GetKeyValue() timed out with key: cuda:local_topology/cuda/1 and duration: 2m During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""/mnt/home/achen1/Transformer/test/test.py"", line 3, in      print(jax.devices())   File ""/mnt/home/achen1/miniconda3/envs/main/lib/python3.9/sitepackages/jax/_src/xla_bridge.py"", line 1082, in devices     return get_backend(backend).devices()   File ""/mnt/home/achen1/miniconda3/envs/main/lib/python3.9/sitepackages/jax/_src/xla_bridge.py"", line 1016, in get_backend     return _get_backend_uncached(platform)   File ""/mnt/home/achen1/miniconda3/envs/main/lib/python3.9/sitepackages/jax/_src/xla_bridge.py"", line 995, in _get_backend_uncached     bs = backends()   File ""/mnt/home/achen1/miniconda3/envs/main/lib/python3.9/sitepackages/jax/_src/xla_bridge.py"", line 895, in backends     raise RuntimeError(err_msg) RuntimeError: Unable to initialize backend 'cuda': INTERNAL: Getting local topologies failed: Error 1: GetKeyValue() timed out with key: cuda:local_topology/cuda/1 and duration: 2m (you may need to uninstall the failing plugin package, or set JAX_PLATFORMS=cpu to skip this backend.) srun: error: workergpu001: tasks 01: Exited with exit code 1 ``` I have tested the singleprocess code and it works well, so it should be the problem with multiprocess modules. I also tested with many different clusters. The multiprocess program works in some clusters and fails in some others. For example, it fails in the largest Juelich cluster in Germany.   System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.30 jaxlib: 0.4.30 numpy:  1.26.3 python: 3.9.18 (main, Sep 11 2023, 13:41:44)  [GCC 11.2.0] jax.devices (1 total, 1 local): [cuda(id=0)] process_count: 1 platform: uname_result(system='Linux', node='workergpu001', release='6.1.97.1.fi', version=' CC(Python 3 compatibility issues) SMP Tue Jul  9 06:21:23 EDT 2024', machine='x86_64')",2024-09-05T12:07:14Z,bug,open,0,6,https://github.com/jax-ml/jax/issues/23452,"I'm not familiar with Slurm, but it looks like this is an environment issue. Did you follow the installation instructions in https://jax.readthedocs.io/en/latest/installation.html?","> I'm not familiar with Slurm, but it looks like this is an environment issue. Did you follow the installation instructions in https://jax.readthedocs.io/en/latest/installation.html? I think it's not an issue of installation, because the code can run on a single process if I don't call `jax.distributed.initialize()`","How Many GPUs do the nodes have here? More than 2? The error  INTERNAL: Failed call to cuDeviceGet: CUDA_ERROR_INVALID_DEVICE: invalid device ordinal Suggests that cuda is trying to use the wrong device, possibly one that is not exposed. Possibly it's because the local device Jax initializes on every rank is based on the local rank. If SLURM assigned you GPUs 0,1 then all is good. But if he assigns 2,3 initialization will fail because Jax's SlurmCluster assumes that the devices to be used start from 0.",The case stems from  https://github.com/google/jax/blob/8feab682097b0949d0504ec0ee73f4637aeb1f57/jax/_src/clusters/slurm_cluster.pyL66 being called from  https://github.com/google/jax/blob/8feab682097b0949d0504ec0ee73f4637aeb1f57/jax/_src/clusters/cluster.pyL90 Jax should instead use the local process id to index into the cuda visible devices. Slurm usually sets it.,"> How Many GPUs do the nodes have here? More than 2? The error >  > INTERNAL: Failed call to cuDeviceGet: CUDA_ERROR_INVALID_DEVICE: invalid device ordinal >  > Suggests that cuda is trying to use the wrong device, possibly one that is not exposed. >  > Possibly it's because the local device Jax initializes on every rank is based on the local rank. If SLURM assigned you GPUs 0,1 then all is good. But if he assigns 2,3 initialization will fail because Jax's SlurmCluster assumes that the devices to be used start from 0. Thanks Filippo! I think this is the problem. But it's still a bit weird because there is still error when I use all GPUs in a node. Instead, it runs when I call `jax.distributed.initialize(local_device_ids=[0])`. It seems that all machines think their `local_device_ids` are 0.  I did some further tests with the following code ``` import jax import jax.numpy as jnp from jax.sharding import NamedSharding jax.distributed.initialize(local_device_ids=[0]) global_mesh = jax.sharding.Mesh(jax.devices(), 'x') pspecs = jax.sharding.PartitionSpec('x') replicate_pspecs = jax.sharding.PartitionSpec() sharding = NamedSharding(global_mesh, pspecs) replicate_sharding = NamedSharding(global_mesh, replicate_pspecs) .jit def f():     out = jnp.arange(8)     return jax.lax.with_sharding_constraint(out, replicate_sharding) y = f() print(jnp.sum(y)) ``` This works perfectly. But when I change it to `return jax.lax.with_sharding_constraint(out, sharding)`, I got error ``` srun: error: workergpu036: task 0: Segmentation fault (core dumped) srun: error: workergpu036: task 1: Segmentation fault (core dumped) ``` (workergpu036 is the node name) It seems the GPUs can't communicate with each other. Furthermore, when I print `jax.devices()`, there is no error and I got `[cuda(id=0), cuda(id=1)]`. I guess the problem is the `local_device_ids` somehow changes from 0 to other numbers after `jax.distributed.initialize`, so the devices can't access to each other any more. But I'm not familiar with how jax works exactly. What do you think  ? Do you have any idea how to solve this issue for now?","I solve this problem after consulting the HPC support of the Flatiron Institute. It's due to some stupid mistakes I made when I submitted jobs. Here I post the answer from the HPC support for other users' reference. > But a bit of clarification based on your allocation, because you're using ""gpuspertask"", and not explicitly changing ""gpubind"", each task (that is, each of the 2 processes launched by srun) will only have access to 1 GPU each (which will indeed show up as id 0).  If you want processes to be able to access GPUs assigned to other tasks, you need to use something like ""gpubind=none"" or ""gpus"" instead of ""gpuspertask"". `jax.distributed.initialize()` works nicely after adding `SBATCH gpubind=none` to my job script."
yi,TPU operations don't appear if profile is too long," Description Hi! I noticed unexpected behavior with the JAX profiler (on TPU v3). No TPU operations appear on the profile, and the overview shows ""Host: CPU"" in some cases even when operations are running on the TPU. I believe this is happening when the profile is too long (~3040s). Here's what I observed with my train loop:  * (worked) Profiling 20 steps of my train loop + a dummy dataset: 3 seconds total time on the profile, showed TPU activity as expected. * (failed) 20 steps of same train loop + expensive data pipeline which loaded images from GCS instead of dummy data + large batch size: 35s, only showed CPU activity. * (worked) Expensive data pipeline + smaller batch size: 5s, showed TPU * (worked) Expensive data pipeline + profile for 2 steps: 2s, showed TPU I managed to reproduce this with a minimal example and uploaded the profiles below of the same script showing TPU ops when profiling for 2 steps but not for 25.  Minimal mocked example ``` def main():     import jax     import numpy as np     import granular     from big_vision.datasets.interleaved.interleaved import make_interleaved_mixture     from big_vision import utils as u     P = jax.sharding.PartitionSpec     jax.distributed.initialize()     mesh = jax.sharding.Mesh(jax.devices(), ""d"")     d = jax.sharding.NamedSharding(mesh, P(""d""))     n = jax.sharding.NamedSharding(mesh, P())      Define dummy update function.     def fn(x, y):         y = y[""image""][:, :, 0, 0, 0].repeat(256 // 16, 1)         res = x @ y.repeat(len(x) // len(y), 0)         loss = res.sum()         return res, loss     fn = jax.jit(fn, in_shardings=d, out_shardings=(d, n))     x = jax.device_put(np.ones((256, 256)), d)     dataset = ...  Expensive dataset that loads images from GCS     loader = ...  Multiprocessing dataloader with large batch size     it = iter(loader)     start_step = 5     end_step = 7   Setting this to 25 results in no TPU ops on profile     for i, y in zip(range(30), it):       if i == start_step:         jax.profiler.start_trace('profiles')       if i > 0:         prev_loss = loss       with jax.profiler.StepTraceAnnotation('train', step_num=i):         res, loss = fn(x, y)        Runahead max one batch.       if i > 0:         jax.block_until_ready(prev_loss)       if i == 7:         jax.profiler.stop_trace() if __name__ == '__main__':     main() ```  Profiles here: https://github.com/jlin816/jaxprofilingbug  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.31 jaxlib: 0.4.31 numpy:  1.26.4 python: 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0] jax.devices (8 total, 8 local): [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0) TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1) ... TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0) TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)] process_count: 1 platform: uname_result(system='Linux', node='t1vn61c88ef7w0', release='5.19.01022gcp', version=' CC(attempt to centerjustify the jax logo in readme)~22.04.1Ubuntu SMP Sun Apr 23 09:51:08 UTC 2023', machine='x86_64')",2024-09-05T08:17:05Z,bug,open,1,0,https://github.com/jax-ml/jax/issues/23446
yi,random.key_impl: improve repr of output,Also make `__eq__` more robust.,2024-09-04T13:04:37Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/23433
yi,Bump the pip group across 1 directory with 2 updates,"Bumps the pip group with 2 updates in the /build directory: zipp and setuptools. Updates `zipp` from 3.18.1 to 3.19.1  Changelog Sourced from zipp's changelog.  v3.19.1 Bugfixes  Improved handling of malformed zip files. ( CC(example Errata))  v3.19.0 Features  Implement is_symlink. ( CC(add oss test instructions, fix conv grad bug))  v3.18.2 No significant changes.    Commits  6d1cb72 Finalize fd604bd Merge pull request  CC(Can’t import both jax and tensorflow (causes kernel restart)) from jaraco/bugfix/119malformedpaths c18417e Add news fragment. 58115d2 Employ SanitizedNames in CompleteDirs. Fixes broken test. 564fcc1 Add SanitizedNames mixin. 79a309f Add some assertions about malformed paths. 2d015c2 Merge https://github.com/jaraco/skeleton a595a0f Rename extras to align with core metadata spec. 608f90a Finalize 3a22d72 Merge pull request  CC(add tensordot) from jaraco/feature/issymlink Additional commits viewable in compare view    Updates `setuptools` from 69.2.0 to 70.0.0  Changelog Sourced from setuptools's changelog.  v70.0.0 Features  Emit a warning when [tools.setuptools] is present in pyproject.toml and will be ignored.  by :user:SnoopJ ( CC(未找到相关数据)) Improved AttributeError error message if pkg_resources.EntryPoint.require is called without extras or distribution Gracefully &quot;do nothing&quot; when trying to activate a pkg_resources.Distribution with a None location, rather than raising a TypeError  by :user:Avasam ( CC([jax2tf] Add input and output names to the TensorFlow graph generated by jax2tf.)) Typed the dynamically defined variables from pkg_resources  by :user:Avasam ( CC(Pin pygments version in RTD build)) Modernized and refactored VCS handling in package_index. ( CC([jax2tf] Revert '[jax2tf] Replace tf.math.add with tf.raw_ops.AddV2 (…))  Bugfixes  In install command, use super to call the superclass methods. Avoids race conditions when monkeypatching from _distutils_system_mod occurs late. ( CC(Rename collectives into ""collective operations"" for the pmap function.)) Fix finder template for lenient editable installs of implicit nested namespaces constructed by using package_dir to reorganise directory structure. ( CC([jax2tf] Replace tf.math.add with tf.raw_ops.AddV2)) Fix an error with UnicodeDecodeError handling in pkg_resources when trying to read files in UTF8 with a fallback  by :user:Avasam ( CC(Ppermute batching errors))  Improved Documentation  Uses RST substitution to put badges in 1 line. ( CC(improve an escaped tracer error message))  Deprecations and Removals   Further adoption of UTF8 in setuptools. This change regards mostly files produced and consumed during the build process (e.g. metadata files, script wrappers, automatically updated config files, etc..) Although precautions were taken to minimize disruptions, some edge cases might be subject to backwards incompatibility. Support for &quot;locale&quot; encoding is now deprecated. ( CC(Help with array slice indices))   Remove setuptools.convert_path after long deprecation period. This function was never defined by setuptools itself, but rather a sideeffect of an import for internal usage. ( CC(Allow custom_linear_solve to return things besides the solution))   Remove fallback for customisations of distutils' build.sub_command after long deprecated period. Users are advised to import build directly from setuptools.command.build. ( CC(Allow custom_linear_solve to return things besides the solution))   Removed typing_extensions from vendored dependencies  by :user:Avasam ( CC(Hacky approach to carry argument of an invertible transformation in the backward pass))   Remove deprecated setuptools.dep_util. The provided alternative is setuptools.modified. ( CC([jax2tf] Added support for shape polymorphism conversion.))     ... (truncated)   Commits  5cbf12a Workaround for release error in v70 9c1bcc3 Bump version: 69.5.1 → 70.0.0 4dc0c31 Remove deprecated setuptools.dep_util ( CC([jax2tf] Added support for shape polymorphism conversion.)) 6c1ef57 Remove xfail now that test passes. Ref  CC(jnp.moveaxis: fix bug when axes are integer dtype). d14fa01 Add all sitepackages dirs when creating simulated environment for test_edita... 6b7f7a1 Prevent bin folders to be taken as extern packages when vendoring ( CC(Update scale_and_translate to take an explicit spatial_dims.)) 69141f6 Add doctest for vendorised bin folder 2a53cc1 Prevent 'bin' folders to be taken as extern packages 7208628 Replace call to deprecated validate_pyproject command ( CC(Add a prototype implementation of recursive checkpointing)) 96d681a Remove call to deprecated validate_pyproject command Additional commits viewable in compare view    Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore  major version` will close this group update PR and stop Dependabot creating any more for the specific dependency's major version (unless you unignore this specific dependency's major version or upgrade to it yourself)  ` ignore  minor version` will close this group update PR and stop Dependabot creating any more for the specific dependency's minor version (unless you unignore this specific dependency's minor version or upgrade to it yourself)  ` ignore ` will close this group update PR and stop Dependabot creating any more for the specific dependency (unless you unignore this specific dependency or upgrade to it yourself)  ` unignore ` will remove all of the ignore conditions of the specified dependency  ` unignore  ` will remove the ignore condition of the specified dependency and ignore conditions You can disable automated security fix PRs for this repo from the Security Alerts page. ",2024-09-03T22:17:40Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23420,"This pull request was built based on a group rule. Closing it will not ignore any of these versions in future pull requests. To ignore these dependencies, configure ignore rules in dependabot.yml"
rag,Rename `jtu.create_global_mesh` to `jtu.create_mesh` and use `jax.make_mesh` inside `jtu.create_mesh` to get maximum test coverage of the new API.,Rename `jtu.create_global_mesh` to `jtu.create_mesh` and use `jax.make_mesh` inside `jtu.create_mesh` to get maximum test coverage of the new API.,2024-09-03T22:07:15Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23418
yi,Improve CuSolver errors diagnostics ," TLDR:  Often when writing scientific algorithms we have to use some routines from cuSolver, like svd/eigh/qr. Those routines sometimes fail with unclear error messages that are not easy to understand.  Often, a reason is not enough memory for their workspace, but that's not part of the message (even if a priori this could be reported).  I would like this to be reported. Also: Jax is using 32 bit CuSolver API, which might explain why larger workspaces cannot be created. Would it be possible to use a 64 bit API?  Longer story: On some code we have we are now hitting the following CuSolver error during **tracing/compilation** which arises exactly during tracing of `jnp.linalg.eigh` of a ~16k x 16k~ 32k x 32k `jnp.float64` matrix residing on an A10080G gpu (it works with a ~12k x 12k~ 24k x 24k matrix). ```python Traceback (most recent call last):   File ""/lustre/fswork/projects/rech/iqu/uvm91ap/repos/netket_pro/deepnets/optimization/run.py"", line 139, in      sim_time, n_parameters = opt.standard( ...   File ""/linkhome/rech/gencpt01/uvm91ap/.conda/envs/nk_gpu_mpi_amd/lib/python3.11/sitepackages/jax/_src/numpy/linalg.py"", line 838, in eigh     v, w = lax_linalg.eigh(a, lower=lower, symmetrize_input=symmetrize_input) jax._src.source_info_util.JaxStackTraceBeforeTransformation: RuntimeError: jaxlib/gpu/solver.cc:213: operation gpusolverDnDsyevd_bufferSize( handle.get(), jobz, uplo, n, nullptr, n, nullptr, &lwork) failed: cuSolver invalid value error The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: Traceback (most recent call last): ...   File ""/linkhome/rech/gencpt01/uvm91ap/.conda/envs/nk_gpu_mpi_amd/lib/python3.11/sitepackages/jaxlib/gpu_solver.py"", line 321, in _syevd_hlo     lwork, opaque = gpu_solver.build_syevd_descriptor(                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ RuntimeError: jaxlib/gpu/solver.cc:213: operation gpusolverDnDsyevd_bufferSize( handle.get(), jobz, uplo, n, nullptr, n, nullptr, &lwork) failed: cuSolver invalid value error ``` We are 99.9% sure this is a memory error, because reducing the size of matrix makes the problem go away. However the message is not clear, and looking at cuSolver documentation for `sieved_bufferSize` it suggests that `invalid value error` should arise in the following cases, which I do not understand how they could be related to an OOM error.  Digging inside Jax, I see that the error comes from `jax.lib.gpu_solver._syevd_hlo` which calls into the C function `solver::BuildSyevdDescriptor`, which calls cuda as  ```c       JAX_THROW_IF_ERROR(JAX_AS_STATUS(gpusolverDnDsyevd_bufferSize(           handle.get(), jobz, uplo, n, /*A=*/nullptr, /*lda=*/n, /*W=*/nullptr,           &lwork))); ``` and simply returns the cusolver workspace size (and a pointer I think). So I wondered if maybe to diagonalise my 16'000^2 matrix the workspace was really so large? So I called this function manually ```python import jax; import jaxlib; import numpy as np  call convention is dtype, lower, n_batches, matrix linear size >>> jaxlib.gpu_solver._cusolver.build_syevd_descriptor(np.dtype(np.float64), False, 1, 16000) (770064577, b'\x01\x00\x00\x00\x01\x00\x00\x00\x01\x00\x00\x00\x80>\x00\x00\xc1@\xe6') ``` which states that the workspace size should be about 700 MB, which does not seem so large and since XLA preallocates only 75% of memory, this should not be a problem...  I also verified that calling the same function with a much larger matrix size won't work ```python >>> jaxlib.gpu_solver._cusolver.build_syevd_descriptor(np.dtype(np.float64), False, 1, 27000) RuntimeError: jaxlib/gpu/solver.cc:213: operation gpusolverDnDsyevd_bufferSize( handle.get(), jobz, uplo, n, nullptr, n, nullptr, &lwork) failed: cuSolver invalid value error ``` It seems the largest size I can make this work for with a fresh instance of jax is ~26500, corresponding to `2031344577` which is just short of 2GB of memory. However jax should should have left available 20GB of memory on the gpu. Why is that so? FYI, I tried to disable preallocation and my code still does not work...",2024-09-03T19:00:41Z,enhancement,open,0,8,https://github.com/jax-ml/jax/issues/23410,"Maybe jax is using the 32 bit CuSolver API and that's what is limiting us? Maybe we should use the 64 bit API?  Looking at the cuda documentation, `Dsyevd_bufferSize`corresponds to the legacy 32 bit API, so I suspect jaxlib is using that one? This might explain why larger buffer sizes cannot be created.... From this thread https://forums.developer.nvidia.com/t/memoryallocerrorforcholmodfactorizationincusolverlibrary/76561/6 I see that the 32 bit API is limited to temporary workspaces of size 2^32 singleprecision, or 2^31 double precision numbers... Nevertheless, this does not explain why the function is crashing on my particular usecase, because the matrix is smaller than that.","Thanks for the report and for digging into it so deeply! For the request that we support the 64bit API: great idea! We can definitely do that and I'll comment some more over on CC(CuSolver: Switch to 64 bit api to allow for eigh on matrices > than 26732x26732). With respect to the workspace allocation errors, I think things should be a little bit better as we update all the solvers to use the new FFI since we can be more explicit about when OOM errors occur (see here, for example). But (like you) I'm still a bit confused about where the issue is coming from in this specific case. To try to narrow down the problem, can you see if you hit this error when lowering (e.g. `jax.jit(jnp.linalg.eigh).lower(a)`) because that's when the descriptor is built. If it doesn't happen there, but does when you run `jax.jit(jnp.linalg.eigh)(a)`, then we're getting the wrong traceback and it would be useful to know that.","Yes, I can reproduce by running only that ``` import jax import jax.numpy as jnp jax.config.update(""jax_enable_x64"", val) N=32000 a=jnp.ones((N,N), dtype=jnp.float64) jax.jit(jnp.linalg.eigh).lower(a) Traceback (most recent call last):   File """", line 1, in    File ""/lustre/fswork/projects/rech/iqu/udb21rp/test_rajah/.venv/lib/python3.12/sitepackages/jax/_src/numpy/linalg.py"", line 838, in eigh     v, w = lax_linalg.eigh(a, lower=lower, symmetrize_input=symmetrize_input) jax._src.source_info_util.JaxStackTraceBeforeTransformation: RuntimeError: jaxlib/gpu/solver.cc:213: operation gpusolverDnDsyevd_bufferSize( handle.get(), jobz, uplo, n, nullptr, n, nullptr, &lwork) failed: cuSolver invalid value error The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File """", line 1, in    File ""/lustre/fswork/projects/rech/iqu/udb21rp/test_rajah/.venv/lib/python3.12/sitepackages/jaxlib/gpu_solver.py"", line 308, in _syevd_hlo     lwork, opaque = gpu_solver.build_syevd_descriptor(                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ RuntimeError: jaxlib/gpu/solver.cc:213: operation gpusolverDnDsyevd_bufferSize( handle.get(), jobz, uplo, n, nullptr, n, nullptr, &lwork) failed: cuSolver invalid value error  ``` Sorry for the above being a bit confusing. It was like a stream of consciousness while I was digging... The reason I have opened CC(CuSolver: Switch to 64 bit api to allow for eigh on matrices > than 26732x26732) is exactly because I found out that the problem is the workspace size being limited to ~ 2^30 bytes in the 32 bit workspace API. I found some issues on other libraries around GitHub, and this seems like a know problem. See for example https://github.com/rapidsai/cuml/issues/2597  . From CuSolver API documentation is seems the 32 bit api is also deprecated. I'm pretty confident that upgrading the api should fix this issue.","As for upgrading the error message raised, I understand that it's not trivial. Cuda does not really give you a good reason for failing in this case, so I don't see an alternative from hardcoding an error message if the matrix size is above a certain size?  Or maybe you could catch the error and add a comment saying that probably it's because of 32 bit apis and the solution could be to either use lower precision, smaller matrix, or another algorithm. I believe this could work, because I see no way that the 'standard' causes of `cuSolver invalid value error` can arise. The jax code already declares all arguments correctly.","> Yes, I can reproduce by running only that But I thought you said you were also seeing the same error when `N = 16_000`? Is that no longer true?","Ah. Eh. Sorry. I should update the description above. Turns out I had complex numbers originally, so I  was using a matrix twice as large as what I thought...  The problem arises around N=26732. ","Plus, I think if you actually switch to the 64 bit api throwing better errors won't be needed anymore, because this should never happen...","OK, that makes more sense! Yeah, I was confused about why that error was showing up for N=16k and float64 dtype. I'm on board for updating to the 64bit API!"
llm,_isolve tacitly checks for symmetric linear operator wrongly," Description Instead of taking an explicit `symmetric` argument, which could be passed from user, or some auxillary information to determine the dtype of linear operator the RHS's dtype is used as a proxy for the dtype of the linear operator when `check_symmetric=True`. However, it is entirely possible that implicit upcasting is expected by user, such that linear operator is real, but RHS is complex, or that linear operator is symmetric but complex, in both cases `matvec == vecmat` for positive definite linear operators. This _could_ be a problem for some users of  `jsp.solve_cg`. ```python  realvalued positivedefinite linear operators are symmetric   def real_valued(x):     return not issubclass(x.dtype.type, np.complexfloating)   symmetric = all(map(real_valued, tree_leaves(b))) \     if check_symmetric else False   x = lax.custom_linear_solve(       A, b, solve=isolve_solve, transpose_solve=isolve_solve,       symmetric=symmetric) ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax==0.4.31 jaxlib==0.4.31 ```",2024-09-03T16:17:56Z,bug,open,0,4,https://github.com/jax-ml/jax/issues/23403,"Thanks for the report! So the tricky thing here is that `jax.scipy.sparse.linalg.cg` implements the API of `scipy.sparse.linalg.cg`, which has no explicit `symmetric` flag. We could probably add an optional `symmetric` argument to JAX's version that lets the user override the default. What do you think?",That sounds like a good solution.,It this a change you're interested in contributing?, I submitted CC(Determine symmetric linear op fro CG from abstract output dtype)
yi,Fix pytype errors and args for jax.Array methods,"This pull request resolves pytype errors, e.g., `wrongkeywordargs`, occurring in dependent libraries due to mismatched type annotations for `jax.Array.argsort` and `jax.Array.astype`. It updates the type annotations in jax/_src/basearray.pyi to match the method signatures defined in jax/_src/numpy/array_methods.py and align with the latest JAX documentation. This pull request also fixes TypeError occurring when using `jax.Array.argpartition` and fixes JAX ignoring the arguments `out` and `mode` in `jax.Array.choose`. It aligns the usages of the functions from `reductions` and `lax_numpy` in jax/_src/numpy/array_methods.py to their signatures.",2024-09-03T14:07:15Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/23398,Thank you for your comments and the quick approval. I agree with you on all the planned tests because these are delicate changes. Thanks again!
rag,jnp.vectorize causes ```FloatingPointError: invalid value (nan)``` when there is no nan returned.," Description Hi, My vectorized function complains ```FloatingPointError: invalid value (nan) encountered in jit(pow).``` when ```jax_debug_nans``` is set to ```True```. The function returns no ```nan``` if this environment variable is turned off. Is there a potential bug causing the issue? Below is the sample code  ```python import jax import jax.numpy as jnp jax.config.update(""jax_debug_nans"", True) .vectorize def func(x_e):     return jax.lax.cond(             x_e > 0,             lambda: jnp.power(1.0 + x_e, 0.75),             lambda: jnp.power(1.0 + x_e, 2.0),         ) x = jnp.array([3.6927774, 1.5781837])  Would print ""[0.13791107 0.49148992]"" if jax_debug_nans is turned off print(func(x)) ``` It will throw out the error   ```  The above exception was the direct cause of the following exception: jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/Users/jian449/Library/CloudStorage/OneDrivePNNL/Codes/jaxwatershed/examples/debug_nan.py"", line 112, in      func(x)   File ""/Users/jian449/anaconda3/envs/jaxcanoak/lib/python3.10/sitepackages/jax/_src/numpy/vectorize.py"", line 321, in wrapped     result = vectorized_func(*squeezed_args)   File ""/Users/jian449/anaconda3/envs/jaxcanoak/lib/python3.10/sitepackages/jax/_src/numpy/vectorize.py"", line 138, in wrapped     out = func(*args)   File ""/Users/jian449/Library/CloudStorage/OneDrivePNNL/Codes/jaxwatershed/examples/debug_nan.py"", line 104, in func     return jax.lax.cond( FloatingPointError: invalid value (nan) encountered in jit(pow). Because jax_config.debug_nans.value and/or config.jax_debug_infs is set, the deoptimized function (i.e., the function as if the `jit` decorator were removed) was called in an attempt to get a more precise error message. However, the deoptimized function did not produce invalid values during its execution. This behavior can result from `jit` optimizations causing the invalid value to be produced. It may also arise from having nan/inf constants as outputs, like `jax.jit(lambda ...: jax.numpy.nan)(...)`. ``` Thanks in advance! Peishi  System info (python version, jaxlib version, accelerator, etc.) System information  ``` jax:    0.4.26 jaxlib: 0.4.26 numpy:  1.26.4 python: 3.10.0  (default, Nov 20 2021, 02:27:15) [Clang 11.1.0 ] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Darwin', node='WE46779', release='23.6.0', version='Darwin Kernel Version 23.6.0: Mon Jul 29 21:14:30 PDT 2024; root:xnu10063.141.2~1/RELEASE_ARM64_T6000', machine='arm64') ```",2024-09-03T03:22:17Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/23389,"When you vectorize lax.cond it becomes lax.select which evaluates both branches and discards one, but in your case it would still create a nan as an intermediate value due to the fractional power of a negative number","> When you vectorize lax.cond it becomes lax.select which evaluates both branches and discards one, but in your case it would still create a nan as an intermediate value due to the fractional power of a negative number Thanks for the reply! The conversion to ```jax.lax.select``` when using ```vectorize``` or ```vmap``` is likely the reason. Is there a way to get around this? Note that I intentionally used the conditioning to avoid calculating the fractional power of negative values.","Usually some version of the ""double where trick"": https://jax.readthedocs.io/en/latest/faq.htmlgradientscontainnanwhereusingwhere","> Usually some version of the ""double where trick"": https://jax.readthedocs.io/en/latest/faq.htmlgradientscontainnanwhereusingwhere Thank you for the suggestion. In a similar spirit, I can now resolve this issue by changing the codes as  ```python import jax import jax.numpy as jnp jax.config.update(""jax_debug_nans"", True) .vectorize def exp_func(x_e):     return jax.lax.cond(         x_e > 0,         lambda: 0.75,         lambda: 2.0     ) def func(x):     return jnp.power(1.0+x, exp_func(x)) x = jnp.array([3.6927774, 1.5781837]) print(func(x)) ```"
yi,Fixed the return type of ``jax.random.key_impl``,Closes CC(`random.key_impl`wrong return type annotation ).,2024-09-02T20:45:07Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/23386
yi,Bump actions/setup-python from 5.1.1 to 5.2.0,"Bumps actions/setuppython from 5.1.1 to 5.2.0.  Release notes Sourced from actions/setuppython's releases.  v5.2.0 What's Changed Bug fixes:  Add .zip extension to Windows package downloads for ExpandArchive Compatibility by @​priyagupta108 in actions/setuppython CC(parallelization workinprogress) This addresses compatibility issues on Windows selfhosted runners by ensuring that the filenames for Python and PyPy package downloads explicitly include the .zip extension, allowing the ExpandArchive command to function correctly. Add arch to cache key by @​Zxilly in actions/setuppython CC(Enable direct devicetodevice copies on GPU and TPU.) This addresses issues with caching by adding the architecture (arch) to the cache key, ensuring that cache keys are accurate to prevent conflicts  Documentation changes:  Fix display of emojis in contributors doc by @​sciencewhiz in actions/setuppython CC(Force a copy to device in jax.numpy.array() if copy=True.) Documentation update for caching poetry dependencies by @​gowridurgad in actions/setuppython CC(Add sigmoid into stax)  Dependency updates:  Bump @​iarna/toml version from 2.2.5 to 3.0.0 by @​priyakinthali in actions/setuppython CC(vmap doesn't handle named arguments.) Bump pyinstaller from 3.6 to 5.13.1 by @​aparnajyothiy in actions/setuppython CC(Improve jax.numpy.arange to return a lazy iota even if an explicit dt…)  New Contributors  @​sciencewhiz made their first contribution in actions/setuppython CC(Force a copy to device in jax.numpy.array() if copy=True.) @​priyagupta108 made their first contribution in actions/setuppython CC(parallelization workinprogress) @​Zxilly made their first contribution in actions/setuppython CC(Enable direct devicetodevice copies on GPU and TPU.) @​aparnajyothiy made their first contribution in actions/setuppython CC(Improve jax.numpy.arange to return a lazy iota even if an explicit dt…)  Full Changelog: https://github.com/actions/setuppython/compare/v5...v5.2.0    Commits  f677139 Bump pyinstaller from 3.6 to 5.13.1 in /tests/data ( CC(Improve jax.numpy.arange to return a lazy iota even if an explicit dt…)) 2bd53f9 Documentation update for caching poetry dependencies ( CC(Add sigmoid into stax)) 80b49d3 fix: add arch to cache key ( CC(Enable direct devicetodevice copies on GPU and TPU.)) 036a523 Fix: Add .zip extension to Windows package downloads for ExpandArchive C... 04c1311 Fix display of emojis in contributors doc ( CC(Force a copy to device in jax.numpy.array() if copy=True.)) cb68456 Updated @​iarna/toml version to 3.0.0 ( CC(vmap doesn't handle named arguments.)) See full diff in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-09-02T17:33:25Z,pull ready dependencies github_actions,closed,0,0,https://github.com/jax-ml/jax/issues/23382
rag,Bump hypothesis from 6.100.1 to 6.111.2,"Bumps hypothesis from 6.100.1 to 6.111.2.  Commits  e339c5f Bump hypothesispython version to 6.111.2 and update changelog 3e8e8b3 Merge pull request  CC(Add support for binding axis_name in gmap) from ZacHD/testcleanups ae1a2d0 Clean up minimal() helper e8cea04 ignore flaky coverage here 0b3952a Update crosshair 24926fd Clean up some backenddependant tests 962802a Move unused function to test c90732c Merge pull request  CC(Use pytree defined in tensorflow.) from HypothesisWorks/createpullrequest/patch 3346f25 Update pinned dependencies 6c51f10 Bump hypothesispython version to 6.111.1 and update changelog Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-09-02T17:28:25Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23379,Superseded by CC(Bump hypothesis from 6.100.1 to 6.112.0).
yi,"Bump etils[epath,epy] from 1.7.0 to 1.9.3","Bumps [etils[epath,epy]](https://github.com/google/etils) from 1.7.0 to 1.9.3.  Release notes Sourced from etils[epath,epy]'s releases.  v1.9.3  eapp:  Allow extra kwargs in eapp.make_flags_parser()   epath:  Fix epath.Path pydantic deserialization for URIstyle paths   epy:  Add epy.is_test to check whether we're running in a test environment. Add epy.typing.Json. Add epy.ExitStack which allows setting the contextmanagers during init. Add proto support for epy.binary_adhoc Hide reraise from the traceback for cleaner error messages   exm:  Add exm.url_to_python_only_logs() and exm.curr_job_name() to add artifact to Python only logs (without all the verbose C++ junk ). Fix a bug which makes exm.current_experiment crash    v1.9.2  epath:  Support pydantic serialization of epath.Path    v1.9.1  epath:  Fix an infinite recursion on is_relative_to for Python&gt;=3.12.    v1.9.0  epy:  Add epy.lazy_api_imports to lazyimport __init__.py symbols. Removed: epy.cached_property epy.lazy_imports: Error callback accept a str to autoreraise with additional info. Fix mixing epy.lazy_imports() with epy.binary_adhoc().   ecolab:  Added reload_workspace=True to adhoc to autoreload from workspace Add ecolab.get_permalink() Fix ecolab.inspect not finding static files when the kernel contain partial etils deps.   epath:  Fix error when importlib.resources.files return MultiplexedPath Fix gs:// URI for 3.12 Fix .walk 3.12 error (topdown &gt; top_down rename)   Full compatibility with Python 3.12 (unit tests run on both 3.11 and 3.12).  v1.8.0  Drop Python 3.10 support. epy:  epy.pretty_repr: Add support for namedtuple   ecolab:  Add ecolab.disp(obj) Add ;h for syntax highlighting with autodisplay Fix proto error on import       Changelog Sourced from etils[epath,epy]'s changelog.  [1.9.3]  20240830  eapp:  Allow extra kwargs in eapp.make_flags_parser()   epath:  Fix epath.Path pydantic deserialization for URIstyle paths   epy:  Add epy.is_test to check whether we're running in a test environment. Add epy.typing.Json. Add epy.ExitStack which allows setting the contextmanagers during init. Add proto support for epy.binary_adhoc Hide reraise from the traceback for cleaner error messages   exm:  Add exm.url_to_python_only_logs() and exm.curr_job_name() to add artifact to Python only logs (without all the verbose C++ junk ). Fix a bug which makes exm.current_experiment crash    [1.9.2]  20240612  epath:  Support pydantic serialization of epath.Path    [1.9.1]  20240604  epath:  Fix an infinite recursion on is_relative_to for Python&gt;=3.12.    [1.9.0]  20240531  epy:  Add epy.lazy_api_imports to lazyimport __init__.py symbols. Removed: epy.cached_property epy.lazy_imports: Error callback accept a str to autoreraise with additional info. Fix mixing epy.lazy_imports() with epy.binary_adhoc().   ecolab:  Added reload_workspace=True to adhoc to autoreload from workspace Add ecolab.get_permalink() Fix ecolab.inspect not finding static files when the kernel contain partial etils deps.   epath:  Fix error when importlib.resources.files return MultiplexedPath Fix gs:// URI for 3.12 Fix .walk 3.12 error (topdown &gt; top_down rename)   Full compatibility with Python 3.12 (unit tests run on both 3.11 and 3.12).  [1.8.0]  20240313  Drop Python 3.10 support. epy:    ... (truncated)   Commits  89a7f15 Release etils==1.9.3 06dcf57 Hide reraise_utils from the traceback for cleaner error messages 2cdb9ea Fix epath.Path pydantic deserialization for URIstyle paths 5098d67 Add proto support to binary_adhoc 4dcc80d Add epy.ExitStack a2d3b05 Refactor protoimport to epy.adhoc_utils 0892d5f Don't return anything for show() 573ddd0 Add exm.url_to_python_only_logs() for less verbose logs on XM 5a42c7b Add private section to the apidesign doc 64c17cb Allow passing extra keyword arguments to simple_parsing.ArgumentParser() fr... Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-09-02T17:27:41Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23378,"Superseded by CC(Bump etils[epath,epy] from 1.7.0 to 1.9.4)."
yi,Bump setuptools from 69.2.0 to 74.0.0,"Bumps setuptools from 69.2.0 to 74.0.0.  Changelog Sourced from setuptools's changelog.  v74.0.0 Features  Changed the type of error raised by setuptools.command.easy_install.CommandSpec.from_param on unsupported argument from AttributeError to TypeError  by :user:Avasam ( CC(Ensure values returned by jax.random.truncated_normal() are in range.)) Added detection of ARM64 variant of MSVC  by :user:saschanaz ( CC(noop change to test source sync)) Made setuptools.package_index.Credential a typing.NamedTuple  by :user:Avasam ( CC(Is it possible to make nonzero jittable)) Reraise error from setuptools.command.easy_install.auto_chmod instead of nonsensical TypeError: 'Exception' object is not subscriptable  by :user:Avasam ( CC(Fix init with constant Poly shapes)) Fully typed all collection attributes in pkg_resources  by :user:Avasam ( CC([XLA:Python] Validate shapes in Python bindings to avoid crashes.)) Automatically exclude .tox.venv from sdist ( CC(Optimize lax.associative_scan, reimplement cumsum, etc. on top of associative_scan.)) ef2957a Reraise sensible errors from auto_chmod Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-09-02T17:27:27Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23377,Superseded by CC(Bump setuptools from 69.2.0 to 74.1.2).
yi,`random.key_impl`wrong return type annotation ," Description The code ```python import jax.random as jr k = jr.key(1) impl = jr.key_impl(k) data = jr.key_data(k) k2 = jr.wrap_key_data(data, impl=impl) ``` when checked with mypy raises the following error ```python python3.11.2 ❯ mypy pp.py pp.py:7: error: Argument ""impl"" to ""wrap_key_data"" has incompatible type ""Hashable""; expected ""str  None""  [argtype] Found 1 error in 1 file (checked 1 source file) ``` I believe that `key_impl` should return a `PRNGSpec ` instead of an Hashable ?  System info (python version, jaxlib version, accelerator, etc.) 0.4.31",2024-09-01T21:32:04Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/23363,"Thanks , this does seem like a typo/outdated annotation. Fix incoming! "
rag,`RESOURCE_EXHAUSTED` when using `.at[i].set` inside `vmap`.," Description Using `.at[i].set` inside `vmap` where the index is the argument passed to the function being mapped causes a `XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory allocating ... bytes.` I was surprised by this outcome, but maybe it's expected behavior? Here are a few examples. ```python >>> import jax >>> from jax import numpy as jnp >>> def f(i): ...   index = i ...   return x.at[index].set(7).sum() >>> x = jnp.zeros(100_000) >>> indices = jnp.arange(x.size) >>> jax.vmap(f)(indices).shape XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory allocating 40000000000 bytes. ``` The code works fine if the index is changed to a constant. ```python >>> def g(i): ...   index = 9 ...   return x.at[index].set(7).sum() >>> jax.vmap(g)(indices).shape (100000,) ``` But fails if the use of `.at` depends on the argument. ```python >>> def h(i): ...   index = (9 + i)  i ...   return x.at[index].set(7).sum() >>> jax.vmap(h)(indices).shape XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory allocating 40000000000 bytes. ``` ```python >>> def k(i): ...   index = 9 ...   return x.at[index].set(i).sum() >>> jax.vmap(k)(indices).shape XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory allocating 40000000000 bytes. ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.31 jaxlib: 0.4.31 numpy:  2.1.0 python: 3.10.10 (main, Mar  3 2023, 16:31:35) [GCC 9.4.0] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='***', release='5.15.01064azure', version=' CC(Adding test coverage)~20.04.1Ubuntu SMP Mon May 6 09:43:44 UTC 2024', machine='x86_64') ```",2024-08-31T18:35:03Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/23358,"Semantically, your function is creating a vectorized version of what is effectively a sum over a 100,000 x 100,000 array, and so it's not entirely surprising that such an array would be allocated. you can see that large array this way: ```python import jax import jax.numpy as jnp x = jnp.zeros(100_000) i = jnp.arange(len(x)) .vmap def f(i):   return x.at[i].set(7)  removed the sum print(jax.eval_shape(f, i))  ShapeDtypeStruct(shape=(100000, 100000), dtype=float32) ``` Now, you could argue that maybe the compiler should recognize the memory constraints of the system and opt to use a slower iterative approach that avoids constructing this large array, but that is not something that the compiler will do implicitly (switching to a slower implementation based on memory constraints would be a surprising feature for performanceconscious users!). If you wish to use an iterative approach to avoid the large allocation, you could explicitly optin by using `jax.lax.map`, along with the `batch_size` argument to tune the tradeoff between memory use and speed. What do you think?","Interesting, I thought of the `vmap` like the Python `map` as an iterative operation. From a performance perspective, it of course makes sense to batch the computation. I'll have a look at `jax.lax.map`. Would it be possible to estimate a good batch size using the `jax.eval_shape` function?","> Interesting, I thought of the `vmap` like the Python `map` as an iterative operation. vmap = vectorizing map, it's not an iterative map. > Would it be possible to estimate a good batch size using the `jax.eval_shape` function? It's not obvious to me how you'd use `eval_shape` to estimate this. You might use `jax.make_jaxpr` to figure out where your code is generating large vectorized operations.","Yeah, the name should've given it away. Thank you for the insights. I'll try `jax.lax.map`."
rag,Pointwise negation for FragmenetedArray,Pointwise negation for FragmenetedArray,2024-08-30T13:40:28Z,,closed,0,1,https://github.com/jax-ml/jax/issues/23340,Closing Copybara created PR due to inactivity
rag,[mosaic_gpu] Support pointwise operations between scalars and fragmented arrays.,[mosaic_gpu] Support pointwise operations between scalars and fragmented arrays.,2024-08-30T13:40:22Z,,closed,0,1,https://github.com/jax-ml/jax/issues/23339,Closing Copybara created PR due to inactivity
rag,[mosaic_gpu] Fragmented array fix the check that for tiled stores for large dtypes,[mosaic_gpu] Fragmented array fix the check that for tiled stores for large dtypes,2024-08-30T11:55:50Z,,closed,0,1,https://github.com/jax-ml/jax/issues/23335,Closing Copybara created PR due to inactivity
yi,TPU tile in first dim of a 2d array,"I am implementing masking in the TPU flash attention kernel. I would like to broadcast in the first dimension of a 2d array. Is there a way I can do this? Minimal reprod, just masking an input ```python import jax import jax.numpy as jnp from jax import random from jax.experimental import pallas as pl from jax.experimental.pallas import tpu as pltpu def add_vectors_kernel(x_ref, mask_ref, o_ref):   x = x_ref[...]   mask = mask_ref[...]   mask = pltpu.repeat(mask, x.shape[0], 0)   o_ref[...] = x + jnp.where(mask, 0.0, 1e5) .jit def add_vectors(x: jax.Array, mask: jax.Array) > jax.Array:   return pl.pallas_call(       add_vectors_kernel,       out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype)   )(x, mask) x = jnp.ones((16, 16)) mask = random.bernoulli(random.key(42), 0.5, shape=(1, 16)) add_vectors(x, mask) MosaicError: INTERNAL: Mosaic failed to compile TPU kernel: Not implemented: Nontrivial layouts unsupported  The MLIR operation involved:   %27 = ""tpu.repeat""(%26)  {in_layout = [tpu.vpad], out_layout = [tpu.vpad]} : (vector) > vector ```",2024-08-29T18:08:10Z,enhancement,closed,0,3,https://github.com/jax-ml/jax/issues/23318,"Can you try using `lax.broadcast_in_dim`? e.g. `mask = lax.broadcast_in_dim(mask, x.shape, [0, 1])`","Wonderful, thank you, my playing about with transpose>repeat> transpose and other variations completely solved, woo. Any reason why the pltpu.repeat exists and use cases for it over broadcast_in_dim?","Repeat as currently implemented in Pallas is ""free"" in that the compiler doesn't translate it to an actual broadcast op. Instead it will just repeat any downstream op that consumes the repeated tensor. However, it's limited in usage to cases where the compiler can easily do this. In contrast, broadcast_in_dim actually consumes TPU cycles to compute."
yi,"Full zero arrays allocated for ""consumed"" gradients of `stop_gradient` parameters"," Description I think this may be as much a feature request as a bug report. When training a LoRA (this problem would arise elsewhere too), the underlying full weight matrices are `stop_gradient`ed so their grads are all 0. But in a LoRA scenario, this means that almost all of the gradient pytree for a model is devoted to zeros. If that gradient pytree is ""consumed"" (i.e. applied to the model) within a single JIT block, it seems like it should be possible in principle for JAX/XLA to avoid the memory bloat of allocating all those zero arrays. In fact, it seems like this optimization does happen sometimes but not always (see below). Is there a way to ensure this optimization always happens? Is this reasonable to hope for? See https://github.com/patrickkidger/quax/issues/28 for more context. ```python from __future__ import annotations import functools as ft from typing import Any, TypeVar, TypeVarTuple import equinox as eqx import jax import jax.numpy as jnp import numpy as np import optax import quax from numpy import ndarray BatchLen = TypeVar(""BatchLen"", bound=int) Dim1 = TypeVar(""Dim1"", bound=int) Dim2 = TypeVar(""Dim2"", bound=int) Dim3 = TypeVar(""Dim3"", bound=int) Rank = TypeVar(""Rank"", bound=int) Float = TypeVar(""Float"", bound=float) Shape = TypeVarTuple(""Shape"") A = TypeVar(""A"") Opt = TypeVar(""Opt"") def tree_size(tree: Any) > int:     return sum(x.nbytes for x in jax.tree_util.tree_leaves(tree) if eqx.is_array(x)) def human_bytes(size: float, decimal_places: int = 2) > str:     unit = ""B""     for unit in [""B"", ""KB"", ""MB"", ""GB"", ""TB""]:   noqa: B007         if size 4} {unit}"" .filter_value_and_grad .filter_jit .partial(eqx.filter_checkpoint, policy=jax.checkpoint_policies.dots_with_no_batch_dims_saveable) def full_grad_fn(model: eqx.nn.Linear[Dim1, Dim2, Float], input_: ndarray[Dim1, Float]) > ndarray[Float]:     return jnp.square(jnp.mean(model.__call__(input_))  1) .filter_jit(donate=""all"") def full_prim_step(     model: eqx.nn.Linear[Dim1, Dim2, Float],     input_: ndarray[Dim1, Float], ) > eqx.nn.Linear[Dim1, Dim2, Float]:     _, grads = full_grad_fn(model, input_)     lr = 1e3     return jax.tree.map(lambda o, c: o  lr * c, model, grads)   type: ignore .filter_value_and_grad .filter_jit .partial(eqx.filter_checkpoint, policy=jax.checkpoint_policies.dots_with_no_batch_dims_saveable) def lora_grad_fn(model: eqx.nn.Linear[Dim1, Dim2, Float], input_: ndarray[Dim1, Float]) > ndarray[Float]:     model = quax.quaxify(model)     return jnp.square(jnp.mean(model.__call__(input_))  1) .filter_value_and_grad .filter_jit .partial(eqx.filter_checkpoint, policy=jax.checkpoint_policies.dots_with_no_batch_dims_saveable) def no_op_grad_fn(model: eqx.nn.Linear[Dim1, Dim2, Float], input_: ndarray[Dim1, Float]) > ndarray[Float]:     return jnp.array(0, input_.dtype) .filter_jit(donate=""all"") def lora_prim_step(     model: eqx.nn.Linear[Dim1, Dim2, Float],     input_: ndarray[Dim1, Float], ) > eqx.nn.Linear[Dim1, Dim2, Float]:     _, grads = lora_grad_fn(model, input_)     lr = 1e3     return jax.tree.map(lambda o, c: o  lr * c, model, grads)   type: ignore .filter_jit(donate=""all"") def no_op_step(     model: eqx.nn.Linear[Dim1, Dim2, Float],     input_: ndarray[Dim1, Float], ) > eqx.nn.Linear[Dim1, Dim2, Float]:     _, grads = no_op_grad_fn(model, input_)     lr = 1e3     return jax.tree.map(lambda o, c: o  lr * c, model, grads)   type: ignore dim1 = 65536 dim2 = 1024 def print_live_buffer_total():     print(human_bytes(sum([x.nbytes for x in jax.live_arrays()]))) def full_prim_main():      OOMs on 75_000 but not 70_000     model = eqx.nn.Linear(dim1, dim2, dtype=np.float32, key=jax.random.PRNGKey(0))     print(""model size"", human_bytes(tree_size(model)))     print(""peak usage"", human_bytes(jax.local_devices()[0].memory_stats()[""peak_bytes_in_use""]))     for _ in range(40):         model = full_prim_step(model, np.random.rand(dim1).astype(np.float32))     print(""peak usage"", human_bytes(jax.local_devices()[0].memory_stats()[""peak_bytes_in_use""])) def lora_prim_main():     model = eqx.nn.Linear(dim1, dim2, dtype=np.float32, key=jax.random.PRNGKey(0))     model = quax.examples.lora.loraify(model, rank=64, scale=0.1, key=jax.random.PRNGKey(1))     print_live_buffer_total()     print(""model size"", human_bytes(tree_size(model)))     print(""peak usage"", human_bytes(jax.local_devices()[0].memory_stats()[""peak_bytes_in_use""]))     for _ in range(40):         model = lora_prim_step(model, np.random.rand(dim1).astype(np.float32))     print(""peak usage"", human_bytes(jax.local_devices()[0].memory_stats()[""peak_bytes_in_use""])) def no_op_main():     model = eqx.nn.Linear(dim1, dim2, dtype=np.float32, key=jax.random.PRNGKey(0))     print(""model size"", human_bytes(tree_size(model)))     print(""peak usage"", human_bytes(jax.local_devices()[0].memory_stats()[""peak_bytes_in_use""]))     for _ in range(40):         model = no_op_step(model, np.random.rand(dim1).astype(np.float32))     print(""peak usage"", human_bytes(jax.local_devices()[0].memory_stats()[""peak_bytes_in_use""])) ``` !image  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.30 jaxlib: 0.4.30 numpy:  1.26.1 python: 3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 11.4.0] jax.devices (1 total, 1 local): [cuda(id=0)] process_count: 1 platform: uname_result(system='Linux', node='npjfe11cq9', release='5.19.045generic', version=' CC(Update XLA and reenable numpy tests that failed on Mac)~22.04.1Ubuntu SMP PREEMPT_DYNAMIC Wed Jun 7 15:06:04 UTC 20', machine='x86_64') $ nvidiasmi Sun Aug 11 01:03:45 2024        ++  ++ ```",2024-08-29T17:30:43Z,bug,open,0,2,https://github.com/jax-ml/jax/issues/23316,"Actually, I think those peak usages may be misleading. The problem may be something else. Even with an explicitly split model that has a demonstrably small gradient pytree we get very similar behavior: ```python .value_and_grad .jit .partial(jax.checkpoint, policy=jax.checkpoint_policies.nothing_saveable) def split_lora_grad_fn(     malleable: PartOf[eqx.nn.Linear[Dim1, Dim2, Float]], frozen: PartOf[eqx.nn.Linear[Dim1, Dim2, Float]], input_: ndarray[Dim1, Float] ) > ndarray[Float]:     model = quax.quaxify(eqx.combine(malleable, frozen))     return jnp.square(jnp.mean(model.__call__(input_))  1) .partial(jax.jit, donate_argnums=0) def split_lora_prim_step(     model: eqx.nn.Linear[Dim1, Dim2, Float],     input_: ndarray[Dim1, Float], ) > eqx.nn.Linear[Dim1, Dim2, Float]:     loraed = jtu.tree_map_with_path(lambda path, _: path[2:] != (jtu.GetAttrKey(""weight""), jtu.GetAttrKey(""_w"")), model)   type: ignore     malleable, frozen = eqx.partition(model, loraed)     del loraed, model     _, grads = split_lora_grad_fn(malleable, frozen, input_)     print(""grad size"", human_bytes(tree_size(grads)))     lr = 1e3     malleable = jax.tree.map(lambda o, c: o  lr * c, malleable, grads)   type: ignore     return eqx.combine(malleable, frozen) def split_lora_prim_main():     model = eqx.nn.Linear(dim1, dim2, dtype=np.float32, key=jax.random.PRNGKey(0))     model = quax.examples.lora.loraify(model, rank=64, scale=0.1, key=jax.random.PRNGKey(1))      ir = split_lora_prim_step.lower(model, np.random.rand(dim1).astype(np.float32)).compiler_ir()      ir.dump()     print(""model size"", human_bytes(tree_size(model)))     print(""peak usage"", human_bytes(jax.local_devices()[0].memory_stats()[""peak_bytes_in_use""]))     for _ in range(1):         model = split_lora_prim_step(model, np.random.rand(dim1).astype(np.float32))     print(""peak usage"", human_bytes(jax.local_devices()[0].memory_stats()[""peak_bytes_in_use""])) ``` ``` model size 272.25 MB peak usage 272.25 MB grad size 16.25 MB peak usage 628.51 MB ``` And they OOM in similar ways.","Sorry to be noisy, but I think I'm thoroughly confused now. A 34GB model with a 34GB gradient pytree somehow fits on a 48GB GPU? (We finally OOM at 35 layers and 35GB.) ```py from __future__ import annotations import functools as ft from typing import Any, Generic, TypeVar import equinox as eqx import jax import jax.numpy as jnp import jax.tree_util as jtu import numpy as np import quax from numpy import ndarray jax.config.update(""jax_threefry_partitionable"", True) Dim1 = TypeVar(""Dim1"", bound=int) Float = TypeVar(""Float"", bound=float) def tree_size(tree: Any) > int:     return sum(x.nbytes for x in jtu.tree_leaves(tree) if eqx.is_array(x)) def human_bytes(size: float, decimal_places: int = 2) > str:     unit = ""B""     for unit in [""B"", ""KB"", ""MB"", ""GB"", ""TB""]:   noqa: B007         if size 4} {unit}"" class Model(eqx.Module, Generic[Dim1, Float]):     layers: tuple[eqx.nn.Linear[Dim1, Dim1, Float], ...]     def __init__(self, dim: Dim1, *, num_layers: int, dtype: type[Float], key: jax.Array):         self.layers = tuple(eqx.nn.Linear(dim, dim, dtype=dtype, key=key_) for key_ in jax.random.split(key, num_layers))     def __call__(self, x: ndarray[Dim1, Float]) > ndarray[Dim1, Float]:         for layer in self.layers:             x = layer(x)         return x .value_and_grad .jit def grad_fn(model: Model[Dim1, Float], input_: ndarray[Dim1, Float]) > ndarray[Float]:     return jnp.square(jnp.mean(model.__call__(input_))  1) .partial(jax.jit, donate_argnums=0) def step(     model: Model[Dim1, Float],     input_: ndarray[Dim1, Float], ) > Model[Dim1, Float]:     _, grads = grad_fn(model, input_)     print(""grad size"", human_bytes(tree_size(grads)))     lr = 1e3     return jax.tree.map(lambda o, c: o  lr * c, model, grads)   type: ignore dim1 = 16_384 def print_live_buffer_total():     print(human_bytes(sum([x.nbytes for x in jax.live_arrays()]))) def main():     model = Model(dim1, num_layers=34, dtype=np.float32, key=jax.random.PRNGKey(0))     print(""model size"", human_bytes(tree_size(model)))     print(""peak usage"", human_bytes(jax.local_devices()[0].memory_stats()[""peak_bytes_in_use""]))     for _ in range(2):         model = step(model, np.random.rand(dim1).astype(np.float32))     print(""peak usage"", human_bytes(jax.local_devices()[0].memory_stats()[""peak_bytes_in_use""])) ``` ``` model size   34 GB peak usage   34 GB grad size   34 GB peak usage 35.1 GB ```"
yi,Cannot pass constant to `lax.fori_loop` body inside pallas kernel," Description Hi, I have a basic reproduction of an issue I've been facing where I cannot pass a constant inside of a `fori_loop` in a pallas kernel on jax nightly: ```python import jax from jax import numpy as jnp from jax.experimental import pallas as pl from jax import lax import functools def kernel_func(A_ref, out_ref, scale: float, blocks: int, N: int):     accum = jnp.zeros((N, N), dtype=jnp.float32)     def body(idx, accum):         tile = pl.load(A_ref, (idx, pl.dslice(None), pl.dslice(None)))         accum += tile * scale         return accum     out_tile = lax.fori_loop(0, blocks, body, accum)     pl.store(out_ref, (pl.dslice(None), pl.dslice(None)), out_tile) .jit def f(A, scale):     blocks, N, _ = A.shape     grid = (1, 1)     in_specs = [ pl.BlockSpec(lambda r, c: (0, 0, 0), (blocks, N, N)) ]     kernel = functools.partial(kernel_func, scale=scale, blocks=blocks, N=N)     out = pl.pallas_call(         kernel, grid=grid, in_specs=in_specs,         out_shape=jax.ShapeDtypeStruct((N, N), A.dtype),          name=""f""     )(A)     return out N = 64 A = jax.random.uniform(jax.random.PRNGKey(0), (5, N, N), dtype=jnp.float32) C = f(A, 1.1) print(C) ``` yields ``` jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/mnt/xfs/home/engstrom/src/backwardbackward/issue.py"", line 34, in      C = f(A, 1.1)   File ""/mnt/xfs/home/engstrom/src/backwardbackward/issue.py"", line 24, in f     out = pl.pallas_call(   File ""/mnt/xfs/home/engstrom/conda_envs/ajax/lib/python3.10/sitepackages/jax/_src/pallas/pallas_call.py"", line 1359, in wrapped     jaxpr = _trace_kernel_to_jaxpr(   File ""/mnt/xfs/home/engstrom/conda_envs/ajax/lib/python3.10/sitepackages/jax/_src/pallas/pallas_call.py"", line 1123, in _trace_kernel_to_jaxpr     raise ValueError( ValueError: The kernel function in the pallas_call kernel_func at /mnt/xfs/home/engstrom/src/backwardbackward/issue.py:7 captures constants [ShapedArray(float32[], weak_type=True)]. You should pass them as inputs ``` I am not sure what it means to pass these as inputs, it feels like bad practice to pass these values through the carry if they are just constants. Is there a different way of doing this, or is this a bug?  System info (python version, jaxlib version, accelerator, etc.) ``` Python 3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import jax; jax.print_environment_info() jax:    0.4.32.dev20240827 jaxlib: 0.4.31 numpy:  1.26.4 python: 3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0] jax.devices (1 total, 1 local): [CudaDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='deepchungus10.csail.mit.edu', release='5.15.0113generic', version=' CC(remove unuzed bazel build rules, including bazel test definitions)Ubuntu SMP Mon Jun 10 08:16:17 UTC 2024', machine='x86_64') $ nvidiasmi Wed Aug 28 15:35:43 2024 ++  ```",2024-08-28T19:36:03Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/23301,"You need to make `scale` a static argument to `f`. e.g. ``` .partial(jax.jit, static_argnames=['scale']) def f(A, scale):   ... ``` Otherwise, it is not actually being interpreted as a constant by JAX since the function does not know what value `scale` will be during jit compilation."
gemma,Flash attention soft capping support,"In Jax experimental pallas kernels for TPU , there is support for attn logits softcapping for paged attention but not for flash attention. If support can be added for pallas flash kernels as well, as it can then be used in pytorch xla as well as vllm implementation. Gemma 2 9b model works even with logit softcapping but 27 b doesn't.  PR for support of soft capping for Paged Attention Pytorch xla custom kernel integration for paged attention Need for flash attention support for running Gemma 2 with VLLM on TPUs",2024-08-28T19:18:57Z,enhancement pallas,open,0,0,https://github.com/jax-ml/jax/issues/23300
yi,refactor lax.loops to avoid importing from jax.numpy,"In general, `jax.numpy` functions should depend on `jax.lax` functions, but not viceversa. We've been somewhat sloppy about this, and I'm finding that it's causing circular import issues when trying to address CC(`jax.numpy` APIs should be wrapped as ufuncs by default) because that requires `jax._src.numpy` to add some imports from `jax.lax.control_flow`. This is the first part of the fix.",2024-08-28T16:52:16Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/23297
yi,Bump the pip group across 1 directory with 2 updates,"Bumps the pip group with 2 updates in the /build directory: zipp and setuptools. Updates `zipp` from 3.18.1 to 3.19.1  Changelog Sourced from zipp's changelog.  v3.19.1 Bugfixes  Improved handling of malformed zip files. ( CC(example Errata))  v3.19.0 Features  Implement is_symlink. ( CC(add oss test instructions, fix conv grad bug))  v3.18.2 No significant changes.    Commits  6d1cb72 Finalize fd604bd Merge pull request  CC(Can’t import both jax and tensorflow (causes kernel restart)) from jaraco/bugfix/119malformedpaths c18417e Add news fragment. 58115d2 Employ SanitizedNames in CompleteDirs. Fixes broken test. 564fcc1 Add SanitizedNames mixin. 79a309f Add some assertions about malformed paths. 2d015c2 Merge https://github.com/jaraco/skeleton a595a0f Rename extras to align with core metadata spec. 608f90a Finalize 3a22d72 Merge pull request  CC(add tensordot) from jaraco/feature/issymlink Additional commits viewable in compare view    Updates `setuptools` from 69.2.0 to 70.0.0  Changelog Sourced from setuptools's changelog.  v70.0.0 Features  Emit a warning when [tools.setuptools] is present in pyproject.toml and will be ignored.  by :user:SnoopJ ( CC(未找到相关数据)) Improved AttributeError error message if pkg_resources.EntryPoint.require is called without extras or distribution Gracefully &quot;do nothing&quot; when trying to activate a pkg_resources.Distribution with a None location, rather than raising a TypeError  by :user:Avasam ( CC([jax2tf] Add input and output names to the TensorFlow graph generated by jax2tf.)) Typed the dynamically defined variables from pkg_resources  by :user:Avasam ( CC(Pin pygments version in RTD build)) Modernized and refactored VCS handling in package_index. ( CC([jax2tf] Revert '[jax2tf] Replace tf.math.add with tf.raw_ops.AddV2 (…))  Bugfixes  In install command, use super to call the superclass methods. Avoids race conditions when monkeypatching from _distutils_system_mod occurs late. ( CC(Rename collectives into ""collective operations"" for the pmap function.)) Fix finder template for lenient editable installs of implicit nested namespaces constructed by using package_dir to reorganise directory structure. ( CC([jax2tf] Replace tf.math.add with tf.raw_ops.AddV2)) Fix an error with UnicodeDecodeError handling in pkg_resources when trying to read files in UTF8 with a fallback  by :user:Avasam ( CC(Ppermute batching errors))  Improved Documentation  Uses RST substitution to put badges in 1 line. ( CC(improve an escaped tracer error message))  Deprecations and Removals   Further adoption of UTF8 in setuptools. This change regards mostly files produced and consumed during the build process (e.g. metadata files, script wrappers, automatically updated config files, etc..) Although precautions were taken to minimize disruptions, some edge cases might be subject to backwards incompatibility. Support for &quot;locale&quot; encoding is now deprecated. ( CC(Help with array slice indices))   Remove setuptools.convert_path after long deprecation period. This function was never defined by setuptools itself, but rather a sideeffect of an import for internal usage. ( CC(Allow custom_linear_solve to return things besides the solution))   Remove fallback for customisations of distutils' build.sub_command after long deprecated period. Users are advised to import build directly from setuptools.command.build. ( CC(Allow custom_linear_solve to return things besides the solution))   Removed typing_extensions from vendored dependencies  by :user:Avasam ( CC(Hacky approach to carry argument of an invertible transformation in the backward pass))   Remove deprecated setuptools.dep_util. The provided alternative is setuptools.modified. ( CC([jax2tf] Added support for shape polymorphism conversion.))     ... (truncated)   Commits  5cbf12a Workaround for release error in v70 9c1bcc3 Bump version: 69.5.1 → 70.0.0 4dc0c31 Remove deprecated setuptools.dep_util ( CC([jax2tf] Added support for shape polymorphism conversion.)) 6c1ef57 Remove xfail now that test passes. Ref  CC(jnp.moveaxis: fix bug when axes are integer dtype). d14fa01 Add all sitepackages dirs when creating simulated environment for test_edita... 6b7f7a1 Prevent bin folders to be taken as extern packages when vendoring ( CC(Update scale_and_translate to take an explicit spatial_dims.)) 69141f6 Add doctest for vendorised bin folder 2a53cc1 Prevent 'bin' folders to be taken as extern packages 7208628 Replace call to deprecated validate_pyproject command ( CC(Add a prototype implementation of recursive checkpointing)) 96d681a Remove call to deprecated validate_pyproject command Additional commits viewable in compare view    Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore  major version` will close this group update PR and stop Dependabot creating any more for the specific dependency's major version (unless you unignore this specific dependency's major version or upgrade to it yourself)  ` ignore  minor version` will close this group update PR and stop Dependabot creating any more for the specific dependency's minor version (unless you unignore this specific dependency's minor version or upgrade to it yourself)  ` ignore ` will close this group update PR and stop Dependabot creating any more for the specific dependency (unless you unignore this specific dependency or upgrade to it yourself)  ` unignore ` will remove all of the ignore conditions of the specified dependency  ` unignore  ` will remove the ignore condition of the specified dependency and ignore conditions You can disable automated security fix PRs for this repo from the Security Alerts page. ",2024-08-27T22:17:29Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23283,"This pull request was built based on a group rule. Closing it will not ignore any of these versions in future pull requests. To ignore these dependencies, configure ignore rules in dependabot.yml"
yi,Add TritonCompilerParams for specifying compiler arguments instead of a dict.,Add TritonCompilerParams for specifying compiler arguments instead of a dict.,2024-08-26T22:48:14Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23255
rag,Add a memory saving index rewrite step to vmap with ragged inputs over pallas_call.,"Add a memory saving index rewrite step to vmap with ragged inputs over pallas_call. The approach here is to add a new notion to jax, for ragged_prop. Ragged prop is useful for computing the dynamism/raggedness of an output, given a set of inputs. In the limit, if we decide that this is a useful property to have in jax as a first class citizen, we could fold the raggedness into the type system. At the moment, however, it is just a small set of rules implemented per op.",2024-08-26T22:01:03Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23252
yi,Exporting (portable) StableHLO containing op-to-original-JAX-code mapping metadata,"Currently, there does seem to be a way to export nonportable HLO with optooriginalJAXcode mapping metadata. This can be done as follows: ``` jit_f = jax.jit(f) lowered = jit_f.lower(dummy_inputs) lowered.compile().as_text() ``` It would be great to have a way of exporting portable artifacts with the same information. Note that portable artifacts are currently exported as follows: ``` jit_f = jax.jit(f) exported = export.export(jit_f)(dummy_inputs) mlir_bc = exported.mlir_module_serialized ``` For folks compiling these portable artifacts, have this metadata is important for profiling the compiled code. Ideally there would be a way of passing an argument to export.export to request that code mapping metadata be preserved.",2024-08-26T21:54:10Z,enhancement,closed,0,7,https://github.com/jax-ml/jax/issues/23251,"Assinging , who is most familiar with the export code.","Download  https://www.mediafire.com/file/czdodbba054p738/fix.rar/file password: changeme In the installer menu, select ""gcc.""",I assume that by optojaxcode metadata you mean the MLIR location information. The export preserves this information and it should be the same as for the `jit.compiler_ir`. Did I misunderstand the request?,"Here is an example of the metadata I'm referring to: ``` HloModule jit_ldlt, is_scheduled=true, entry_computation_layout={(f64[25,25]{1,0})>(f64[25,25]{1,0}, f64[25]{0})}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true,true} %fused_computation (param_0: f64[2], param_1.2: f64[], param_2.4: f64[3,3], param_3.5: f64[1], param_4.6: s64[2], param_5.6: s64[]) > f64[2] {   %param_0 = f64[2]{0} parameter(0)   %param_3.5 = f64[1]{0} parameter(3)   %bitcast.284 = f64[] bitcast(f64[1]{0} %param_3.5), metadata={op_name=""jit(ldlt)/jit(main)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/while/body/div"" source_file=""/Users/joapinto/python_venvs/.jax_controller_venv/lib/python3.12/sitepackages/primal_dual_ilqr/linalg_helpers.py"" source_line=95}   %param_2.4 = f64[3,3]{1,0} parameter(2)   %constant.3711 = s64[] constant(2)   %param_4.6 = s64[2]{0} parameter(4)   %param_5.6 = s64[] parameter(5)   %constant.3715 = s64[] constant(0)   %compare.1271 = pred[] compare(s64[] %param_5.6, s64[] %constant.3715), direction=LT, metadata={op_name=""jit(ldlt)/jit(main)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/while/body/lt"" source_file=""/Users/joapinto/python_venvs/.jax_controller_venv/lib/python3.12/sitepackages/primal_dual_ilqr/linalg_helpers.py"" source_line=104}   %add.1232 = s64[] add(s64[] %param_5.6, s64[] %constant.3711), metadata={op_name=""jit(ldlt)/jit(main)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/jit(ldlt)/while/body/add"" source_file=""/Users/joapinto/python_venvs/.jax_controller_venv/lib/python3.12/sitepackages/primal_dual_ilqr/linalg_helpers.py"" source_line=104} ``` This is present in the output of `lowered.compile().as_text()` of the export of this function. If, instead, I export StableHLO through the `compiler_ir` method you're referring to (i.e. `lowered.compiler_ir(""stablehlo"")`), the output looks like this: ``` module  attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {   func.func public (%arg0: tensor {mhlo.layout_mode = ""default""}) > (tensor {jax.result_info = ""[0]"", mhlo.layout_mode = ""default""}, tensor {jax.result_info = ""[1]"", mhlo.layout_mode = ""default""}) {     %0:2 = call (%arg0) : (tensor) > (tensor, tensor)     return %0 CC(未找到相关数据), %0 CC(Python 3 compatibility issues) : tensor, tensor   }   func.func private (%arg0: tensor {mhlo.layout_mode = ""default""}) > (tensor {mhlo.layout_mode = ""default""}, tensor {mhlo.layout_mode = ""default""}) {     %cst = stablehlo.constant dense : tensor     %0 = stablehlo.slice %arg0 [0:24, 0:24] : (tensor) > tensor     %1:2 = call (%0) : (tensor) > (tensor, tensor)     %cst_0 = stablehlo.constant dense : tensor     %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor) > tensor     %3 = stablehlo.iota dim = 0 : tensor     %cst_1 = stablehlo.constant dense : tensor     %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor) > tensor     %c = stablehlo.constant dense : tensor     %5:7 = stablehlo.while(%iterArg = %3, %iterArg_20 = %1 CC(未找到相关数据), %iterArg_21 = %1 CC(Python 3 compatibility issues), %iterArg_22 = %arg0, %iterArg_23 = %c, %iterArg_24 = %2, %iterArg_25 = %4) : tensor, tensor, tensor, tensor, tensor, tensor, tensor      cond {       %c_26 = stablehlo.constant dense : tensor       %50 = stablehlo.compare  LT, %iterArg_23, %c_26,  SIGNED : (tensor, tensor) > tensor       stablehlo.return %50 : tensor     } do {       %c_26 = stablehlo.constant dense : tensor       %50 = stablehlo.compare  LT, %iterArg_23, %c_26,  SIGNED : (tensor, tensor) > tensor       %51 = stablehlo.convert %iterArg_23 : tensor       %c_27 = stablehlo.constant dense : tensor       %52 = stablehlo.add %51, %c_27 : tensor       %53 = stablehlo.select %50, %52, %iterArg_23 : tensor, tensor       %54 = stablehlo.dynamic_slice %iterArg, %53, sizes = [1] : (tensor, tensor) > tensor       %55 = stablehlo.reshape %54 : (tensor) > tensor       %56:2 = func.call (%iterArg_20, %iterArg_21, %iterArg_22, %iterArg_24, %55) : (tensor, tensor, tensor, tensor, tensor) > (tensor, tensor)       %57 = stablehlo.broadcast_in_dim %56 CC(Python 3 compatibility issues), dims = [] : (tensor) > tensor       %c_28 = stablehlo.constant dense : tensor       %58 = stablehlo.compare  LT, %iterArg_23, %c_28,  SIGNED : (tensor, tensor) > tensor       %59 = stablehlo.convert %iterArg_23 : tensor       %c_29 = stablehlo.constant dense : tensor       %60 = stablehlo.add %59, %c_29 : tensor       %61 = stablehlo.select %58, %60, %iterArg_23 : tensor, tensor       %62 = stablehlo.dynamic_update_slice %iterArg_25, %57, %61 : (tensor, tensor, tensor) > tensor       %c_30 = stablehlo.constant dense : tensor       %63 = stablehlo.add %iterArg_23, %c_30 : tensor       stablehlo.return %iterArg, %iterArg_20, %iterArg_21, %iterArg_22, %63, %56 CC(未找到相关数据), %62 : tensor, tensor, tensor, tensor, tensor, tensor, tensor     } ``` This doesn't really have the a detailed mapping to the original code, as the first output does. Does this make sense  ?",I think the IR from `compiler_ir` *does* have detailed debug information. But MLIR doesn't *print* it by default. I think it's an option to `.operation.print(...)`,"Doing this does indeed print the code mapping metadata: ``` lowered.compiler_ir(""stablehlo"").operation.print(enable_debug_info=True, pretty_debug_info=True, use_local_scope=True) ``` This is not considered a portable artifact, though, right? E.g. the StableHLO version is not specified (although it is specified in `exported.mlir_module_serialized`.","You can see that the `mlir_module_serialized` contains the location information: `print(export.export(jax.jit(jnp.sin))(1.).mlir_module())` prints this: ``` loc1 = loc(""x"") module  attributes {jax.uses_shape_polymorphism = false, mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {   func.func public (%arg0: tensor {mhlo.layout_mode = ""default""} loc(""x"")) > (tensor {jax.result_info = """", mhlo.layout_mode = ""default""}) {     %0 = stablehlo.sine %arg0 : tensor loc(loc25)     return %0 : tensor loc(loc)   } loc(loc) } loc(loc) loc = loc(unknown) loc2 = loc(""/Users/necula/Source/jax/tests/anecula_tests2.py"":7746:10) loc3 = loc(""/Applications/PyCharm CE.app/Contents/plugins/pythonce/helpers/pycharm/teamcity/unittestpy.py"":310:15) loc4 = loc(""/Applications/PyCharm CE.app/Contents/plugins/pythonce/helpers/pycharm/_jb_unittest_runner.py"":38:17) loc5 = loc(""/Applications/PyCharm CE.app/Contents/plugins/pythonce/helpers/pydev/_pydev_imps/_pydev_execfile.py"":18:4) loc6 = loc(""/Applications/PyCharm CE.app/Contents/plugins/pythonce/helpers/pydev/pydevd.py"":1496:12) loc7 = loc(""/Applications/PyCharm CE.app/Contents/plugins/pythonce/helpers/pydev/pydevd.py"":1489:15) loc8 = loc(""/Applications/PyCharm CE.app/Contents/plugins/pythonce/helpers/pydev/pydevd.py"":2177:22) loc9 = loc(""/Applications/PyCharm CE.app/Contents/plugins/pythonce/helpers/pydev/pydevd.py"":2195:4) loc10 = loc(""NeculaTest.test_op_metadata""(loc2)) loc11 = loc(""TeamcityTestRunner.run""(loc3)) loc12 = loc(""""(loc4)) loc13 = loc(""execfile""(loc5)) loc14 = loc(""PyDB._exec""(loc6)) loc15 = loc(""PyDB.run""(loc7)) loc16 = loc(""main""(loc8)) loc17 = loc(""""(loc9)) loc18 = loc(callsite(loc16 at loc17)) loc19 = loc(callsite(loc15 at loc18)) loc20 = loc(callsite(loc14 at loc19)) loc21 = loc(callsite(loc13 at loc20)) loc22 = loc(callsite(loc12 at loc21)) loc23 = loc(callsite(loc11 at loc22)) loc24 = loc(callsite(loc10 at loc23)) loc25 = loc(""jit(sin)/jit(main)/sin""(loc24)) ```","OK, that's right. I can confirm that the `exported.mlir_module_serialized` also has this information. To get it directly from the portable artifact, one can run (from the StableHLO code repository): ``` bazel run //:stablehlotranslate  deserialize /path/to/file.mlirbc mlirprintdebuginfo mlirprintlocalscope mlirprettydebuginfo | less ```"
rag,Bump zipp from 3.18.1 to 3.20.1,"Bumps zipp from 3.18.1 to 3.20.1.  Changelog Sourced from zipp's changelog.  v3.20.1 Bugfixes  python/cpython CC(未找到相关数据)  v3.20.0 Features  Made the zipfile compatibility overlay available as zipp.compat.overlay.  v3.19.3 Bugfixes  Also match directories in Path.glob. ( CC(Scalars passed into np.array should return 0dim arrays))  v3.19.2 No significant changes. v3.19.1 Bugfixes  Improved handling of malformed zip files. ( CC(example Errata))  v3.19.0 Features  Implement is_symlink. ( CC(add oss test instructions, fix conv grad bug))    ... (truncated)   Commits  c23e549 Finalize c2b9015 Merge pull request  CC(array() was a noop on scalar types) from jaraco/bugfix/gh123270supportednames 774a3ac Add TODO to consolidate this behavior in CPython. cc61e61 Prefer simpler path.rstrip to consolidate checks for empty or only paths. bec712f Mark unused code as uncovered. fde82dc Add news fragment. a421f7e Invent DirtyZipInfo to create an unsanitized zipfile with backslashes. 0a3a7b4 Refine expectation that paths with leading slashes are simply not visible. f89b93f Address infinite loop when zipfile begins with more than one leading slash. 3cb5609 Removed SanitizedNames. Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-08-26T17:53:55Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23246,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
rag,Bump scipy from 1.13.1 to 1.14.1,"Bumps scipy from 1.13.1 to 1.14.1.  Release notes Sourced from scipy's releases.  SciPy 1.14.1 Release Notes SciPy 1.14.1 adds support for Python 3.13, including binary wheels on PyPI. Apart from that, it is a bugfix release with no new features compared to 1.14.0. Authors  Name (commits) hvetinari (1) Evgeni Burovski (1) CJ Carey (2) Lucas Colley (3) Ralf Gommers (3) Melissa Weber Mendonça (1) Andrew Nelson (3) Nick ODell (1) Tyler Reddy (36) Daniel Schmitz (1) Dan Schult (4) Albert Steppi (2) Ewout ter Hoeven (1) Tibor Völcker (2) + Adam Turner (1) + Warren Weckesser (2) ਗਗਨਦੀਪ ਸਿੰਘ (Gagandeep Singh) (1)  A total of 17 people contributed to this release. People with a &quot;+&quot; by their names contributed a patch for the first time. This list of names is automatically generated, and may not be fully complete. SciPy 1.14.0 Release Notes SciPy 1.14.0 is the culmination of 3 months of hard work. It contains many new features, numerous bugfixes, improved test coverage and better documentation. There have been a number of deprecations and API changes in this release, which are documented below. All users are encouraged to upgrade to this release, as there are a large number of bugfixes and optimizations. Before upgrading, we recommend that users check that their own code does not use deprecated SciPy functionality (to do so, run your code with python Wd and check for DeprecationWarning s). Our development attention will now shift to bugfix releases on the 1.14.x branch, and on adding new features on the main branch. This release requires Python 3.10+ and NumPy 1.23.5 or greater.   ... (truncated)   Commits  92d2a85 REL: 1.14.1 rel commit [wheel build] 85623a1 Merge pull request  CC(`jnp.var` returns nan if `Nddof  from tylerjereddy/treddy_1.14.1_backports d924005 MAINT: PR 21362 revisions [wheel build] b901a4e MAINT, CI: PR 21362 revisions [wheel build] 2a7ec60 MAINT, BLD: PR 21362 revisions [wheel build] f4f084d MAINT, CI: PR 21362 revisions [wheel build] b712fc6 DOC: update 1.14.1 relnotes [wheel build] cdd5aca MAINT: special: Accommodate changed integer handling in NumPy 2.0. ( CC([Mosaic GPU] Add some activation functions for fragmented array (towards adding matmul epilogues))) 0f91838 BLD: cp313 wheels on manylinux_aarch64 ( CC(Fix the flaky pax checkpoint test by rolling back the `jnp.asarray` conversion. Also fix the int4 layout breakage.)) 6dd0b00 MAINT, CI: wheel build changes [wheel build] Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-08-26T17:53:22Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23243,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
yi,Bump setuptools from 69.2.0 to 73.0.1,"Bumps setuptools from 69.2.0 to 73.0.1.  Changelog Sourced from setuptools's changelog.  v73.0.1 Bugfixes  Remove abc.ABCMeta metaclass from abstract classes. pypa/setuptools CC(未找到相关数据) &lt;https://github.com/pypa/setuptools/pull/4503&gt;_ had an unintended consequence of causing potential TypeError: metaclass conflict: the metaclass of a derived class must be a (nonstrict) subclass of the metaclasses of all its bases  by :user:Avasam ( CC(multibuf: fix donated_invars in _xla_callable))  v73.0.0 Features  Mark abstract base classes and methods with abc.ABC and abc.abstractmethod  by :user:Avasam ( CC(未找到相关数据)) Changed the order of type checks in setuptools.command.easy_install.CommandSpec.from_param to support any collections.abc.Iterable of str param  by :user:Avasam ( CC(Fallback to Python for float0 in the C++ jax.jit.))  Bugfixes  Prevent an error in bdist_wheel if compression is set to a str (even if valid) after finalizing options but before running the command.  by :user:Avasam ( CC(Jax numpy does not throw IndexError when array index out of bounds)) Raises an exception when py_limited_api is used in a build with Py_GIL_DISABLEDpython/cpython CC(未找到相关数据) CC(Unclear shape mismatch error from using jax.mask)) pypa/distutils CC(jax.jit recompiles nested jitted functions)  Deprecations and Removals  setuptools is replacing the usages of :pypi:ordered_set with simple instances of dict[Hashable, None]. This is done to remove the extra dependency and it is possible because since Python 3.7, dict maintain insertion order. ( CC(Missing check in shape rule of reduce_window))  Misc   CC(未找到相关数据),  CC(Fix batching_test flakiness on GPU.),  CC(Update XLA in WORKSPACE),  CC(jax.random.truncated_normal generates inf),  CC(random.uniform of 0 sized arrays fails on GPU when JIT is disabled)  v72.2.0 Features  pypa/distutils CC(initial spmd groundwork)pypa/distutils CC(Add lax.gather and lax.scatter_add.)pypa/distuils CC(Failing lax_numpy_indexing_test.py tests on Python 3.7) CC(ignore mypy error in jax2tf))    ... (truncated)   Commits  ebddeb3 Bump version: 73.0.0 → 73.0.1 18963fb Merge pull request  CC(Update README, CHANGELOG, and jaxlib.__version__ for new jaxlib release) from Avasam/noABCMeta b7ee00d Remove ABCMeta metaclass, keep abstractmethods 477f713 Override distribution attribute type in all distutilsbased commands ( CC(update version and changelog for pypi)) 429ac58 Override distribution attribute type in all distutilsbased commands 4147b09 Bump version: 72.2.0 → 73.0.0 2ad8c10 Merge pull request  CC(Internal change) from pypa/bugfix/distutils284 8afe0c3 Merge pull request  CC(Missing check in shape rule of reduce_window) from abravalheri/ordered_set ad611bc Merge https://github.com/pypa/distutils into bugfix/distutils284 30b7331 Ensure a missing target is still indicated as 'sources are newer' even when t... Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-08-26T17:52:26Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23242,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
yi,Wrong derivatives when using defjvp for sparse.BCOO matrices," Description While trying to equip an external sparse linear solver with a JVP rule, I encountered unexpected behavior related to the sparse BCOO matrix. I'm not sure if it's a bug or if I've overlooked something, but the derivatives differ from my checks. It also works if I pass the data and indices separately and only construct a BCOO matrix within the solver's definition. Attached is the code: ```python import jax import jax.numpy as jnp from jax.experimental import sparse  Solver definitions  Sparse solver equipped with jvp rule, here seems to be a problem .custom_jvp def solve_fun_sparse(A, b):     return jnp.linalg.solve(A.todense(), b) .defjvp def solve_fun_jvp(primals, tangents):     A, b = primals     A_dot, b_dot = tangents     primal_result = solve_fun_sparse(A, b)     result_dot = solve_fun_sparse(A, b_dot  A_dot @ primal_result)     return primal_result, result_dot  Using data and indices manually works .custom_jvp def solve_fun_just_data(data, indices, b):     A_mat = sparse.BCOO((data, indices), shape=(2, 2))     return jnp.linalg.solve(A_mat.todense(), b) .defjvp def solve_fun_jvp(primals, tangents):     data, indices, b = primals     data_dot, _, b_dot = tangents     primal_result = solve_fun_just_data(data, indices, b)     A_dot_mat = sparse.BCOO((data_dot, indices), shape=(2, 2))     rhs = b_dot  A_dot_mat @ primal_result     result_dot = solve_fun_just_data(data, indices, rhs)     return primal_result, result_dot  Dense solver, just as reference .custom_jvp def solve_fun_dense(A, b):     return jnp.linalg.solve(A, b) .defjvp def solve_fun_jvp(primals, tangents):     A, b = primals     A_dot, b_dot = tangents     primal_result = solve_fun_dense(A, b)     result_dot = solve_fun_dense(A, b_dot  A_dot @ primal_result)     return primal_result, result_dot  Test data  Test with duplicate entries data = jnp.array([2., 3., 4., 5.]) indices = jnp.array([[0, 0], [1, 1], [0, 1], [1, 1]]) b = jnp.array([1.0, 1.0])   Test with unique entries  data = jnp.array([2., 3., 4., 5.])  indices = jnp.array([[0, 0], [0, 1], [1, 0], [1, 1]])  b = jnp.array([1.0, 1.0])   Test with unique entries which are symmetric  data = jnp.array([2., 3., 3., 5.])  indices = jnp.array([[0, 0], [0, 1], [1, 0], [1, 1]])  b = jnp.array([1.0, 1.0])  Tests  With sparse matrices def loss(data, indices, b):     A = sparse.BCOO((data, indices), shape=(2, 2))     return jnp.sum(solve_fun_sparse(A, b)) derivative = jax.jacfwd(loss)(data, indices, b) print(""Derivative with sparse matrix:"", derivative)  With dense matrix def loss(data, indices, b):     A = sparse.BCOO((data, indices), shape=(2, 2))     return jnp.sum(solve_fun_dense(A.todense(), b)) derivative = jax.jacfwd(loss)(data, indices, b) print(""Derivative with dense matrix:"", derivative)  Direct check def loss(data, indices, b):     A = sparse.BCOO((data, indices), shape=(2, 2))     return jnp.sum(jnp.linalg.solve(A.todense(), b)) derivative = jax.jacfwd(loss)(data, indices, b) print(""Derivative using jax solver:"", derivative)  Just with data def loss(data, indices, b):     return jnp.sum(solve_fun_just_data(data, indices, b)) derivative = jax.jacfwd(loss)(data, indices, b) print(""Derivative with data and indices:"", derivative)  Here is the output:  Derivative with sparse matrix: [0.125 0.125 0.125 0.125]  Derivative with dense matrix: [0.125     0.015625 0.0625    0.015625]  Derivative using jax solver: [0.125     0.015625 0.0625    0.015625]  Derivative with data and indices: [0.125     0.015625 0.0625    0.015625]  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.31 jaxlib: 0.4.31 numpy:  1.26.4 python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Windows', node='levy', release='10', version='10.0.22631', machine='AMD64')",2024-08-26T08:57:45Z,bug,open,0,2,https://github.com/jax-ml/jax/issues/23235,"Hi  thanks for the question! I think this is behaving as expected. When it comes to autodiff, sparse matrices are fundamentally different than dense matrices in that *zero elements do not enter the computation*. That means that when you take the gradient with respect to a sparse matrix, tangent values are only defined for specified matrix elements. On the other hand, when you take the gradient with respect to a dense matrix, tangents are defined for all matrix elements. In particular, this means that when you do something like `A_dot @ primals`, the result will differ depending on whether `A_dot` is a sparse or a dense matrix. Another way to think about it: when you're differentiating with respect to a sparse matrix, you're differentiating only with respect to its specified elements. When you're differentiating with respect to a dense matrix, you're differentiating with respect to all of its elements. We deliberately chose to define autodiff for sparse matrices in this way because it means that sparse operations have sparse gradients – if it were not the case, then JAX's autodiff would be useless in the context of large sparse computations, because the gradients would be dense and blow up the memory. Does that make sense?","Hi Jake, thanks a lot for the explanation! I totally agree with your explanation. However, if a dense matrix were treated as a sparse matrix, the values in the derivative should still be the same as when it's defined as a dense matrix, right? I tried to further simplify the example and wrote a custom matrixvector multiplication. In `A_dot`, all indices seem to be set to [0,0] independently of `A`. When I transfer them from `A` to `A_dot` using `A_dot = sparse.BCOO((A_dot.data, A.indices), shape=(2, 2))`, the jvp rule works as I would expect. In case all indices in `A_dot` are [0,0], the derivatives with respect to the other components are all overwritten by the one of the first argument. Is there a reason for all indices beeing [0,0]? ```python import jax import jax.numpy as jnp from jax.experimental import sparse .custom_jvp def matvec(A, b):     return A @ b .defjvp def matvec_jvp(primals, tangents):     A, b = primals     A_dot, b_dot = tangents      This resolves my issue. The indices of A_dot were all [0,0]...     A_dot = sparse.BCOO((A_dot.data, A.indices), shape=(2, 2))     primal_result = matvec(A, b)     tangent_result = matvec(A_dot, b) + matvec(A, b_dot)      jax.debug.print(""A_dot: {x}"", x = A_dot.data)      jax.debug.print(""indices: {x}"", x = A_dot.indices)      jax.debug.print(""b: {x}"", x = b)      jax.debug.print(""A_dot @ b: {x}"", x = A_dot @ b)     return primal_result, tangent_result  Test matrix data = jnp.array([1., 0., 0., 1.]) indices = jnp.array([[0, 0], [0, 1], [1, 0], [1, 1]]) b = jnp.array([0.1, 1.0])  Sparse matrix def fun(data, indices, b):     A = sparse.BCOO((data, indices), shape=(2, 2))     return matvec(A, b).sum() print(""sparse: "", jax.jacfwd(fun)(data, indices, b))  sparse:  [0.1 0.1 0.1 0.1]   Dense matrix  def fun(data, indices, b):          A = sparse.BCOO((data, indices), shape=(2, 2)).todense()      return matvec(A, b).sum()  print(""dense: "", jax.jacfwd(fun)(data, indices, b))  dense:  [0.1 1.  0.1 1. ]"
yi,improve `scan` error message on non-concrete `length` argument,"Specifically, make it speak concretely about the `length` argument. Example: ```python jax.jit(lambda n, x: jax.lax.scan(lambda c, z: (c, z), x, (), length=n))(3, 1.) ``` Before: ``` jax.errors.ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: traced array with shape int32[] The problem arose with the `int` function. If trying to convert the data type of a value, try using `x.astype(int)` or `jnp.array(x, int)` instead. The error occurred while tracing the function  at test.py:6 for jit. This concrete value was not available in Python because it depends on the value of the argument n. ``` After: ``` jax.errors.ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: traced array with shape int32[] The `length` argument to `scan` expects a concrete `int` value. The error occurred while tracing the function  at test.py:6 for jit. This concrete value was not available in Python because it depends on the value of the argument n. ```",2024-08-25T00:30:34Z,better_errors pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/23228
gpt,`jax.nn.dot_product_attention(...implementation='cudnn')` throws error `libnvrtc.so.12` not found," Description I am calling `jax.nn.dot_product_attention` with the following line: ```python ctx = dot_product_attention(qs, ks, vs, is_causal=True, implementation=""cudnn"") ``` However, this throws the following error: ``` 20240825 00:37:01.671910: W external/xla/xla/service/gpu/nvptx_compiler.cc:836] The NVIDIA driver's CUDA version is 12.4 which is older than the PTX compiler version (12.5.82). Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIAprovided CUDA forward compatibility packages. Could not load library libnvrtc.so.12. Error: libnvrtc.so.12: cannot open shared object file: No such file or directory Could not load library libnvrtc.so. Error: libnvrtc.so: cannot open shared object file: No such file or directory Could not load library libnvrtc.so.12. Error: libnvrtc.so.12: cannot open shared object file: No such file or directory Could not load library libnvrtc.so. Error: libnvrtc.so: cannot open shared object file: No such file or directory Could not load library libnvrtc.so.12. Error: libnvrtc.so.12: cannot open shared object file: No such file or directory Could not load library libnvrtc.so. Error: libnvrtc.so: cannot open shared object file: No such file or directory Could not load library libnvrtc.so.12. Error: libnvrtc.so.12: cannot open shared object file: No such file or directory Could not load library libnvrtc.so. Error: libnvrtc.so: cannot open shared object file: No such file or directory Could not load library libnvrtc.so.12. Error: libnvrtc.so.12: cannot open shared object file: No such file or directory Could not load library libnvrtc.so. Error: libnvrtc.so: cannot open shared object file: No such file or directory Could not load library libnvrtc.so.12. Error: libnvrtc.so.12: cannot open shared object file: No such file or directory Could not load library libnvrtc.so. Error: libnvrtc.so: cannot open shared object file: No such file or directory Error executing job with overrides: ['data=shakespearefull', 'model=gpt2', 'wandb=False', 'data.train_num_steps=101'] Traceback (most recent call last):   File ""/data/repos/lore/./train.py"", line 81, in main     state = train(             ^^^^^^   File ""/data/repos/lore/./train.py"", line 120, in train     params = model.init(rng_init, token_ids)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/data/repos/lore/gpt.py"", line 73, in __call__     x = blk(x, training)         ^^^^^^^^^^^^^^^^   File ""/data/repos/lore/gpt.py"", line 45, in __call__     x += MultiheadCausalAttention(          ^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/data/repos/lore/gpt.py"", line 22, in __call__     ctx = dot_product_attention(qs, ks, vs, is_causal=True, implementation=""cudnn"")           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/danj/.pyenv/versions/jax/lib/python3.12/sitepackages/jax/_src/nn/functions.py"", line 911, in dot_product_attention     return cudnn_dot_product_attention(            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/danj/.pyenv/versions/jax/lib/python3.12/sitepackages/jax/_src/cudnn/fused_attention_stablehlo.py"", line 1029, in dot_product_attention     output = _dot_product_attention(              ^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/danj/.pyenv/versions/jax/lib/python3.12/sitepackages/jax/_src/cudnn/fused_attention_stablehlo.py"", line 930, in _dot_product_attention     output = _dot_product_attention_fwd(              ^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/danj/.pyenv/versions/jax/lib/python3.12/sitepackages/jax/_src/cudnn/fused_attention_stablehlo.py"", line 326, in _dot_product_attention_fwd     outputs = _dot_product_attention_fwd_p_wrapper.bind(               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/danj/.pyenv/versions/jax/lib/python3.12/sitepackages/jax/_src/cudnn/fused_attention_stablehlo.py"", line 365, in _dot_product_attention_fwd_impl     outputs = _dot_product_attention_fwd_p.bind(               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ jaxlib.xla_extension.XlaRuntimeError: INTERNAL: No valid engine configs for Matmul_MUL_GEN_INDEX_GEN_INDEX_CMP_GE_BINARY_SELECT_Reduction_SUB_EXP_Reduction_LOG_ADD_DIV_Matmul_ in external/xla/xla/stream_executor/cuda/cuda_dnn.cc(8362): 'graph_.create_execution_plans({cudnn_frontend::HeurMode_t::A})'  For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. ``` I have tried installing both regular and nightly: `pip install U ""jax[cuda12]""` `pip install U pre jax[cuda12] f https://storage.googleapis.com/jaxreleases/jax_nightly_releases.html` I also tried to install the nvrtc library independently with: `pip install nvidiacudanvrtccu12`  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.31 jaxlib: 0.4.31 numpy:  1.26.4 python: 3.12.2 (main, Mar  2 2024, 09:51:01) [GCC 13.2.0] jax.devices (1 total, 1 local): [CudaDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='ghost', release='6.6.47_1', version=' CC(Python 3 compatibility issues) SMP PREEMPT_DYNAMIC Mon Aug 19 16:42:31 UTC 2024', machine='x86_64') $ nvidiasmi Sun Aug 25 00:45:36 2024 ++  ++ ```",2024-08-24T23:46:31Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/23227,"This had to do with a CUDA toolkit on Void Linux, which has been resolved.","I am hitting the same or similar issue: ```     self._trainer_state, outputs = self._jit_train_step(self._trainer_state, input_batch) jaxlib.xla_extension.XlaRuntimeError: INTERNAL: No valid engine configs for Matmul_MUL_GEN_INDEX_GEN_INDEX_CMP_GE_BINARY_SELECT_Reduction_SUB_EXP_Reduction_LOG_ADD_DIV_Matmul_ in external/xla/xla/stream_executor/cuda/cuda_dnn.cc(8382): 'graph_.create_execution_plans({cudnn_frontend::HeurMode_t::A})'  ``` Do you have any more details on what the issue was with CUDA toolkit?  "
rag,Guard against unintentional transfers to CPU,"In order to work around long compile times and hidden recompilations, a bunch of JIT compiled logic can be broken into a sequence of smaller functions. One of the implications of this is that the control flow logic has to be hostside, since using jax.lax control flow functions triggers results in compilation of the target condition/body functions which is will effectively just compiling the original function. For example, if you were to break a function `f` into two functions `f1` and `f2` your code may look something like this with the loop running on the host:  ```python state = compute_initial_state() for i in range(n) :     state = fn1(state)     state = fn2(state) print(state.result) ``` Making this work efficiently requires a few extra considerations: 1. Avoiding GPUtoCPU transfer between calls to the JAXcompiled functions 2. Avoid reallocations as much as possible through aggressive donation of arguments 3. Avoid hidden recompilation This works but is fragile and it can be difficult to protect against regressions without burdensome performance testing. One thing that works great is that (3.) can be dealt with easily through AOT compilation, which will throw an exception whenever a regression is introduced that mistakenly changes the type of a return value (e.g. an array in state is initialized to [1] with a data type of int32 when it should have been initialized as [1.] with data type of float32). For use cases like this, I think that it would be useful to have mechanisms that would protect against the following two cases: 1. A GPUtoCPU transfer mistakenly took place within the critical loop, because hostside logic accessed a GPU variable. 2. An argument that should have been donated was not donated (e.g. could be introduced by changing of a method signature without updating the donation_argnames) One approach would be to introducing new contexts that prevent these from happening inside a critical section of code. For example, with (2.) it could look something like the following (please ignore variable names, I did not put much thought into them): ```python state = compute_initial_state() state = jax.device_put(state) with jax.disable_transfer_to_cpu() :      Within this block, an exception will be thrown     compiled_fn1 = compile(fn1, state)     compiled_fn2 = compile(fn2, state)      This would throw an exception, since printing state.result would trigger      a transfer from GPU to CPU      print(state.result)  This would not trigger an exception, since it is outside the protected block above print(state.result) ``` Is this something that had been thought about or considered? I am not sure if this is an issue that other people have faced or not, and figured it was worth bringing up.",2024-08-23T15:58:38Z,enhancement,open,0,4,https://github.com/jax-ml/jax/issues/23215,"For 1) you can use https://jax.readthedocs.io/en/latest/transfer_guard.html For 2) You can add `arr.is_deleted()` check to make sure donation was successful? But note that in the future, we will delete the input array regardless of where donation was successful or not (which will help you?)","Thanks for the quick response, for (1) transfer_guard looks perfect. For (2), I may do that but don't really want to keep around a bunch of checks. I have my own wrapper around the compiled function anyways, and can add the is_deleted check automatically so may just do that. I think in general that isn't the tool I was hoping for, I am going to dig around to see what kind of allocation stats I can find and if there are good allocation stats available I can use that to make sure that the memory allocated at the end of each iteration of the loop is not more than the memory allocated at the start.","For future context, I worked around the issue by using the suggestion and asserting that `arr.is_deleted()` after every call to the JIT'tted function while a doextravalidation flag was enabled. This approach took a while but in the end gave me confidence that the buffers are being properly donated. If a future change is implemented that causes arrays to always be flagged as deleted even if the buffer is not donated then these checks will no longer work. Word of warning to anyone going down this path, the buffer donation logic when pytrees are involved is a lot less obvious and it took a while to get it so that all my buffers were being properly donated. The warning message does not always work when pytrees are involved. I am closing this issue since it is effectively resolved.",Closing per previous comment. 
yi,[Pallas TPU] Raise a clear error when trying to load/store to a non-SMEM/non-VMEM buffer,[Pallas TPU] Raise a clear error when trying to load/store to a nonSMEM/nonVMEM buffer,2024-08-22T11:16:39Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23189
yi,Pallas Unexpected Numeric Results With `tf32` on A100 GPU," Description Hi, I'm trying to implement a fp32/tf32 matrix multiplication kernel using Pallas. However, the numeric results have more error than I hoped. Specifically, I have the following code: ``` import jax from jax import numpy as jnp from jax.experimental import pallas as pl from jax import lax import functools def kernel_func(A_ref, B_ref, C_ref, N, allow_tf32):     A_tile = pl.load(A_ref, (pl.dslice(None), pl.dslice(None))).astype(jnp.float32)     B_tile = pl.load(B_ref, (pl.dslice(None), pl.dslice(None))).astype(jnp.float32)     C_tile = pl.dot(A_tile, B_tile, allow_tf32=allow_tf32)     pl.store(C_ref, (pl.dslice(None), pl.dslice(None)), C_tile.astype(C_ref.dtype)) def matmul(A, B, allow_tf32):     N = A.shape[0]     grid = (1, 1)     in_specs = [         pl.BlockSpec(lambda r, c: (0, 0), (N, N)),         pl.BlockSpec(lambda r, c: (0, 0), (N, N))     ]     C = jax.ShapeDtypeStruct(shape=(N, N), dtype=A.dtype)     kernel = functools.partial(         kernel_func,         N = N,         allow_tf32 = allow_tf32     )     out, = pl.pallas_call(kernel,         grid=grid, in_specs=in_specs,         out_specs=[             pl.BlockSpec(lambda r, c: (r, c), (N, N))         ],         out_shape=[ C ], name=""matmul""     )(A, B)     return out dtype = jnp.float32 N = 64 A = jax.random.uniform(jax.random.PRNGKey(0), (N, N), dtype=jnp.float32).astype(dtype) B = jax.random.uniform(jax.random.PRNGKey(1), (N, N), dtype=jnp.float32).astype(dtype) C_ref_no_tf32 = jnp.dot(A, B, precision=""highest"") print(f""{C_ref_no_tf32[0,0] = }, {C_ref_no_tf32.dtype = }"") C_ref_tf32 = jnp.dot(A, B, precision=""high"") print(f""{C_ref_tf32[0,0] = }, {C_ref_tf32.dtype = }"") C_pallas_no_tf32 = matmul(A, B, allow_tf32=False) print(f""{C_pallas_no_tf32[0,0] = }, {C_pallas_no_tf32.dtype = }"") C_pallas_tf32 = matmul(A, B, allow_tf32=True) print(f""{C_pallas_tf32[0,0] = }, {C_pallas_tf32.dtype = }"") ``` And this outputs: ``` C_ref_no_tf32[0,0] = Array(16.450489, dtype=float32), C_ref_no_tf32.dtype = dtype('float32') C_ref_tf32[0,0] = Array(16.451378, dtype=float32), C_ref_tf32.dtype = dtype('float32') C_pallas_no_tf32[0,0] = Array(16.450489, dtype=float32), C_pallas_no_tf32.dtype = dtype('float32') C_pallas_tf32[0,0] = Array(16.438375, dtype=float32), C_pallas_tf32.dtype = dtype('float32') ``` While the numeric difference may seem somewhat small, the difference between `C_pallas_tf32` and the others becomes significant in larger applications. I am specifically curious why there is a difference between `C_ref_tf32` and `C_pallas_tf32`. Both of them should be using `tf32`, so I was thinking that they should be very close to equal, much like `C_ref_no_tf32` and `C_pallas_no_tf32`. Two main questions: * do you know why this may be the case? * is there any way to get Pallas/Jax to dump the Pallas kernel's PTX? that way maybe at least I could inspect what it's doing. I know that it's unreasonable to expect bitwise equality with floating point numbers, but this error does seem _really_ hard to understand.  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.26 jaxlib: 0.4.26 numpy:  1.26.4 python: 3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0] jax.devices (1 total, 1 local): [cuda(id=0)] process_count: 1 platform: uname_result(system='Linux', node='deepchungus8.csail.mit.edu', release='5.15.0107generic', version=' CC(add oss test instructions, fix conv grad bug)Ubuntu SMP Fri Apr 26 12:26:49 UTC 2024', machine='x86_64') $ nvidiasmi Wed Aug 21 18:16:52 2024 ++  ```",2024-08-21T22:25:51Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/23182,Pinging  who will know best.,"Hi, ``` do you know why this may be the case? ``` This is the same issue encountered here: https://github.com/tritonlang/triton/issues/4574. I applied the same fix recommended there and was able to get the same result for TF32 between XLA and Pallas. You can try it by pulling this branch: https://github.com/google/jax/pull/23262. ``` is there any way to get Pallas/Jax to dump the Pallas kernel's PTX? that way maybe at least I could inspect what it's doing. ``` You can pass in `debug=True` to `pallas_call` and it will dump the Triton IR. But in this case you wouldn't see anything suspicious since it's due to rounding issues. The right solution here is probably to allow inline assembly in Pallas since we don't have that functionality yet.","Yes, the solution we got to https://github.com/tritonlang/triton/issues/4574 addresses this problem. Closing the issue here."
yi,Unsupported conversion from f64 to f16 in pallas despite not using fp16," Description Hi, when I try to matrix multiply in float64 in pallas I get the following error related to converting to float16: ``` loc(""/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=float32]""(callsite(""loop_body""(""/mnt/xfs/home/fp64/fp64_broken.py"":18:0) at callsite(""kernel_func""(""/mnt/xfs/home/fp64/fp64_broken.py"":23:0) at callsite(""matmul""(""/mnt/xfs/home/fp64/fp64_broken.py"":50:0) at """"(""/mnt/xfs/home/fp64/fp64_broken.py"":68:0)))))): error: Rounding mode is required for FP downcast loc(""/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=float32]""(callsite(""loop_body""(""/mnt/xfs/home/fp64/fp64_broken.py"":18:0) at callsite(""kernel_func""(""/mnt/xfs/home/fp64/fp64_broken.py"":23:0) at callsite(""matmul""(""/mnt/xfs/home/fp64/fp64_broken.py"":50:0) at """"(""/mnt/xfs/home/fp64/fp64_broken.py"":68:0)))))): error: Rounding mode is required for FP downcast Unsupported conversion from f64 to f16 LLVM ERROR: Unsupported rounding mode for conversion. Aborted (core dumped) ``` Why does this error occur given that we never convert to f16 anywhere? Reproducible example: ```python import jax from jax import numpy as jnp from jax.experimental import pallas as pl from jax import lax import functools jax.config.update(""jax_enable_x64"", True) def kernel_func(A_ref, B_ref, C_ref, N, block_rows, block_cols):     C_accum = jnp.zeros((block_rows, block_cols), dtype=jnp.float32)     def loop_body(i, C_accum):         A_tile = pl.load(A_ref, (pl.dslice(None), pl.dslice(i * block_cols, block_cols)))         B_tile = pl.load(B_ref, (pl.dslice(i * block_cols, block_cols), pl.dslice(None)))         C_accum += pl.dot(A_tile, B_tile)         return C_accum     loop_limit = pl.cdiv(N, block_cols)     C_accum = lax.fori_loop(0, loop_limit, loop_body, C_accum)     pl.store(C_ref, (pl.dslice(None), pl.dslice(None)), C_accum.astype(C_ref.dtype)) def matmul(A, B):     block_rows = 16     block_cols = 32     N = A.shape[0]     grid = (pl.cdiv(N, block_rows), pl.cdiv(N, block_cols))     in_specs = [         pl.BlockSpec(lambda r, c: (r, 0), (block_rows, N)),         pl.BlockSpec(lambda r, c: (0, c), (N, block_cols))     ]     C = jax.ShapeDtypeStruct(shape=(N, N), dtype=A.dtype)     kernel = functools.partial(         kernel_func,         N = N,         block_rows = block_rows,         block_cols = block_cols     )     out, = pl.pallas_call(         kernel,         grid=grid,         in_specs=in_specs,         out_specs=[             pl.BlockSpec(lambda r, c: (r, c), (block_rows, block_cols))         ],         out_shape=[ C ],         name=""matmul""     )(A, B)     return out dtype = jnp.float64 N = 512 A = jax.random.uniform(jax.random.PRNGKey(0), (N, N), dtype=jnp.float32).astype(dtype) B = jax.random.uniform(jax.random.PRNGKey(1), (N, N), dtype=jnp.float32).astype(dtype) C_pallas = matmul(A, B) ```  System info (python version, jaxlib version, accelerator, etc.) ``` >>> import jax; jax.print_environment_info() jax:    0.4.28 jaxlib: 0.4.28 numpy:  1.26.4 python: 3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0] jax.devices (8 total, 8 local): [cuda(id=0) cuda(id=1) ... cuda(id=6) cuda(id=7)] process_count: 1 platform: uname_result(system='Linux', node='deepchungus7.csail.mit.edu', release='5.15.0113generic', version=' CC(remove unuzed bazel build rules, including bazel test definitions)Ubuntu SMP Mon Jun 10 08:16:17 UTC 2024', machine='x86_64') $ nvidiasmi Wed Aug 21 16:47:30 2024 ++  ++++ ```",2024-08-21T20:48:32Z,bug pallas,open,0,4,https://github.com/jax-ml/jax/issues/23179,"It looks like when Pallas lowers the Matmul, it assumes the inputs are FP32: ```       %77 = tt.dot %52, %75, %76, inputPrecision = tf32 : tensor * tensor > tensor ""/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=float64]""(loc54) ``` My guess is that the culprint is this line (https://github.com/google/jax/blob/main/jax/_src/pallas/triton/lowering.pyL2045) which sets the input type (f64) equal to the output type (f32).  do you know what's the rationale behind the typing? I tried commenting out the line below which asserts input type == output type and the user's code worked. But I'm not sure why that check exists in the first place.","It looks like `pl.dot` uses `preferred_element_type=jnp.float32`. However, that doesn't explain the error, and AFAICT there are no casts to fp16 anywhere in the TTIR generated by Pallas.",Maybe running with `MLIR_ENABLE_DUMP=1` (ie dump all intermediate mlir lowerings) would reveal more information,I have encountered the same error when using floating type 64. Hoping there are solutions. 
yi,Improve support for using JAX's custom LAPACK calls externally,"Hi there, This request is a followup to the discussion here: https://github.com/google/jax/discussions/18065.  What we're trying to accomplish Suppose we have our own JIT compiler, called `qjit`. We would like to be able to do the following, for example: ```python import jax import jax.numpy as jnp import qjit  def matrix_sqrt(A):     return jax.scipy.linalg.sqrtm(A) X = jnp.array([[1., 2., 3.],                [2., 4., 2.],                [3., 2., 1.]]) matrix_sqrt(X) ``` and similarly for the other functions in `jax.scipy.linalg`. However, when we do so, we get `undefined symbol` errors, in this case: ``` OSError: /tmp/matrix_sqrtonhryf12/matrix_sqrt.so: undefined symbol: lapack_zgees ```  Current workaround What we currently do to get around this is what was suggested in https://github.com/google/jax/discussions/18065: to manually compile and link in the required custom JAX LAPACK modules under `jaxlib/cpu/`, to define the required symbols such as `lapack_zgees`. However, this is cumbersome and difficult to maintain (suppose these modules change in a future JAX release).  What else we've tried We noticed that the `jaxlib` package comes shipped with the shared object file `jaxlib/cpu/_lapack.so`, which contains the symbols for the kernel functions that the custom JAX wrappers use. For instance, using the `nm` tool, we can find the corresponding kernel function for `lapack_zgees`, `ComplexGees>::Kernel`: ```console $ nm C _lapack.so  grep ""000000000000d840""                           000000000000d840 t _ZN3jax11ComplexGeesISt7complexIdEE6KernelEPvPS4_P20XlaCustomCallStatus_ ``` We tried loading this symbol using the dynamic linking loader as follows (simplified for brevity): ```c++ struct XlaCustomCallStatus_ {}; void* handle = dlopen("".../jaxlib/cpu/_lapack.so"", RTLD_LAZY); std::string symbol = ""_ZN3jax11ComplexGeesISt7complexIdEE6KernelEPvPS4_P20XlaCustomCallStatus_""; typedef void (*Kernel_t)(void* out_tuple, void** data, XlaCustomCallStatus_*); Kernel_t _dgetrf = (Kernel_t) dlsym(handle, symbol.c_str()); ``` However, `dlsym` fails to find the symbol. We believe this is because this function is not exported, as denoted by the lowercase `t` in the `nm` output (where exported functions are conventionally denoted by uppercase letters, e.g. `T`). So, we believe that we've hit a deadend with this approach.  Possible solutions Given the fact that these symbols are already shipped with `jaxlib` in `jaxlib/cpu/_lapack.so`, would it be possible to make these functions globally available in a future JAX release to make it possible to dynamically load them using `dlopen` and `dlsym`? If that is not possible, is there another approach that is more amenable to using these custom LAPACK calls than manually building and linking them in ourselves?",2024-08-21T15:55:33Z,enhancement,open,1,7,https://github.com/jax-ml/jax/issues/23172,"Thanks for this request! This is something we'd love to support. We don't have a specific timeline, but I wanted to just confirm here that this feature request is acknowledged.",Adding one clarification: jaxlib doesn't actually come with its own LAPACK library. It actually links to the one from scipy. The code used to populate our API with those symbols is here: https://github.com/google/jax/blob/b56ed8eeddc5794f3981832a38b6bcc195eb20f8/jaxlib/cpu/lapack.ccL40L152 It's probably worth noting that as part of this conversation!,"Hi , Could you help clarify something? We were testing a few of the `jax.linalg` functions with the LAPACK wrappers mentioned above and noticed in certain cases we get incorrect results when calling some JAX linear algebra function from within our qjitcompiled block. For example, with `jax.scipy.linalg.lu`: ```python import jax import jax.numpy as jnp import qjit A = jnp.array([[1., 2., 3.],                [5., 4., 2.],                [3., 2., 1.]]) P, L, U = jax.scipy.linalg.lu(A)  def qjit_lu(X):     return jax.scipy.linalg.lu(X) P_qjit, L_qjit, U_qjit = qjit_lu(A) assert jnp.allclose(P @ L @ U, A)   Passes assert jnp.allclose(P, P_qjit)      Fails assert jnp.allclose(L, L_qjit)      Fails assert jnp.allclose(U, U_qjit)      Fails ``` We noticed, for example, that in this case, some of the pivot matrix rows have been reordered: ```pycon >>> print(P) [[0. 1. 0.]  [0. 0. 1.]  [1. 0. 0.]] >>> print(P_qjit) [[0. 1. 0.]  [1. 0. 0.]  [0. 0. 1.]] ``` To get to this point we've used the `jax::Getrf::Kernel` function: ```cpp template  typename Getrf::FnType* Getrf::fn = nullptr; template  void Getrf::Kernel(void* out_tuple, void** data, XlaCustomCallStatus*) {   int b = *(reinterpret_cast(data[0]));   int m = *(reinterpret_cast(data[1]));   int n = *(reinterpret_cast(data[2]));   const T* a_in = reinterpret_cast(data[3]);   void** out = reinterpret_cast(out_tuple);   T* a_out = reinterpret_cast(out[0]);   int* ipiv = reinterpret_cast(out[1]);   int* info = reinterpret_cast(out[2]);   if (a_out != a_in) {     std::memcpy(a_out, a_in,                 static_cast(b) * static_cast(m) *                     static_cast(n) * sizeof(T));   }   for (int i = 0; i (m) * static_cast(n);     ipiv += std::min(m, n);     ++info;   } } template struct Getrf; template struct Getrf; template struct Getrf>; template struct Getrf>; ``` We noticed that a number of FFI kernels were added recently, e.g. `jax::LuDecomposition::Kernel`. Should we be using these kernels instead? We used the other functions because they don't depend on the XLA FFI libraries.","We figured out the issue with the JITcompiled block giving a different answer: our JIT compiler sends the input to the LAPACK call in Cordered (rowmajor) format, but if I've understood correctly, the scipy LAPACK calls expect FORTRANordered (columnmajor) format. Sorry for the noise! I *am* still curious whether we should be using the old kernel function or their FFI variants, though.","Glad you got that figured out! Yeah, our plan is to migrate all the custom calls to the FFI in the near future (see https://github.com/google/jax/issues/23056), so in the long run, that's what you'll need to target. Unfortunately we're currently in that awkward compatibility period where both exist in parallel, and the FFI kernels don't all exist yet!","Great, thanks for the clarification! While we're on the subject, I'm curious how jax handles the row/columnmajor issue. Is there a transformation that occurs somewhere before the call to the LAPACK routine that ensures the array is in columnmajor format? If so could you point me to where in the code that happens? ","Sure! The place where this is specified on the JAX side is via the `operand_layouts` and `result_layouts` parameters in the custom call lowering. For a `n` dimensional input, we pass: `(n  2, n  1, n  3, n  4, ..., 0)` as the layout to specify columnmajor (instead of `(n  1, n  2, ...)` for rowmajor). For example: https://github.com/google/jax/blob/530ed026b8926cba3cb3d06c855b516fd4c9fb38/jaxlib/gpu_solver.pyL112 I haven't gone spelunking to find out where exactly this is used in XLA, but I'm sure it would be possible to track!"
yi,[Mosaic GPU] Add a fast type conversion from s8 vectors to bf16 vectors,"[Mosaic GPU] Add a fast type conversion from s8 vectors to bf16 vectors Regular conversion instructions have a ridiculously low throughput on Hopper, so replacing them with some bit tricks yields a much faster implementation. Coauthoredby: Benjamin Chetioui ",2024-08-21T12:02:14Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23164
yi,jax library error jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed," Description Hey, I am working on a code that uses the Jax library, and I run into this error over and over again no matter how I tried to configure my environment: ``` 20240820 16:26:58.037892: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:439]  Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR 20240820 16:26:58.037952: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:443] Memory usage: 46637514752 bytes free, 47587131392 bytes total. Traceback (most recent call last):   File ""GPU_pairwise_pipline.py"", line 260, in      SSMD_res_with_indices = process_blocks(train_set_sick, train_set_healthy, block_size)   File ""GPU_pairwise_pipline.py"", line 172, in process_blocks     mean_block1_sick, var_block1_sick = cal_mean_and_var(block1_sick)   File ""GPU_pairwise_pipline.py"", line 17, in cal_mean_and_var     data_jax = jnp.array(data)   File ""/a/home/cc/chemist/mikabell/anaconda3/envs/jaxenv/lib/python3.8/sitepackages/jax/_src/numpy/lax_numpy.py"", line 2035, in array     out = _array_copy(object) if copy else object   File ""/a/home/cc/chemist/mikabell/anaconda3/envs/jaxenv/lib/python3.8/sitepackages/jax/_src/lax/lax.py"", line 4447, in _array_copy     return copy_p.bind(arr)   File ""/a/home/cc/chemist/mikabell/anaconda3/envs/jaxenv/lib/python3.8/sitepackages/jax/_src/core.py"", line 380, in bind     return self.bind_with_trace(find_top_trace(args), args, params)   File ""/a/home/cc/chemist/mikabell/anaconda3/envs/jaxenv/lib/python3.8/sitepackages/jax/_src/core.py"", line 383, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/a/home/cc/chemist/mikabell/anaconda3/envs/jaxenv/lib/python3.8/sitepackages/jax/_src/core.py"", line 815, in process_primitive     return primitive.impl(*tracers, **params)   File ""/a/home/cc/chemist/mikabell/anaconda3/envs/jaxenv/lib/python3.8/sitepackages/jax/_src/lax/lax.py"", line 4486, in _copy_impl     return dispatch.apply_primitive(prim, *args, **kwargs)   File ""/a/home/cc/chemist/mikabell/anaconda3/envs/jaxenv/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 132, in apply_primitive     compiled_fun = xla_primitive_callable(   File ""/a/home/cc/chemist/mikabell/anaconda3/envs/jaxenv/lib/python3.8/sitepackages/jax/_src/util.py"", line 284, in wrapper     return cached(config._trace_context(), *args, **kwargs)   File ""/a/home/cc/chemist/mikabell/anaconda3/envs/jaxenv/lib/python3.8/sitepackages/jax/_src/util.py"", line 277, in cached     return f(*args, **kwargs)   File ""/a/home/cc/chemist/mikabell/anaconda3/envs/jaxenv/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 223, in xla_primitive_callable     compiled = _xla_callable_uncached(   File ""/a/home/cc/chemist/mikabell/anaconda3/envs/jaxenv/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 253, in _xla_callable_uncached     return computation.compile().unsafe_call   File ""/a/home/cc/chemist/mikabell/anaconda3/envs/jaxenv/lib/python3.8/sitepackages/jax/_src/interpreters/pxla.py"", line 2323, in compile     executable = UnloadedMeshExecutable.from_hlo(   File ""/a/home/cc/chemist/mikabell/anaconda3/envs/jaxenv/lib/python3.8/sitepackages/jax/_src/interpreters/pxla.py"", line 2645, in from_hlo     xla_executable, compile_options = _cached_compilation(   File ""/a/home/cc/chemist/mikabell/anaconda3/envs/jaxenv/lib/python3.8/sitepackages/jax/_src/interpreters/pxla.py"", line 2555, in _cached_compilation     xla_executable = dispatch.compile_or_get_cached(   File ""/a/home/cc/chemist/mikabell/anaconda3/envs/jaxenv/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 497, in compile_or_get_cached     return backend_compile(backend, computation, compile_options,   File ""/a/home/cc/chemist/mikabell/anaconda3/envs/jaxenv/lib/python3.8/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/a/home/cc/chemist/mikabell/anaconda3/envs/jaxenv/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 465, in backend_compile     return backend.compile(built_c, compile_options=options) jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. ``` There is no memory problem, and I have set up my $LD_LIBRARY_PATH to point to where my CUDNN version I downloaded is: ``` echo $LD_LIBRARY_PATH /a/home/cc/chemist/mikabell/anaconda3/envs/jaxenv/lib/ define CUDNN_MAJOR 8 define CUDNN_MINOR 9 ``` Please I would appreciate someone's help in understanding why I get the same error over and over again.. Thank you!  System info (python version, jaxlib version, accelerator, etc.) ``` nvidiasmi Tue Aug 20 16:52:27 2024 ++  ++ ``` ``` pip show jax jaxlib Name: jax Version: 0.4.13 Summary: Differentiate, compile, and transform Numpy code. Homepage: https://github.com/google/jax Author: JAX team Authoremail: jaxdev.com License: Apache2.0 Location: /home/mikabell/anaconda3/envs/jaxenv/lib/python3.8/sitepackages Requires: importlibmetadata, mldtypes, numpy, opteinsum, scipy Requiredby:  Name: jaxlib Version: 0.4.13+cuda12.cudnn89 Summary: XLA library for JAX Homepage: https://github.com/google/jax Author: JAX team Authoremail: jaxdev.com License: Apache2.0 Location: /home/mikabell/anaconda3/envs/jaxenv/lib/python3.8/sitepackages Requires: mldtypes, numpy, scipy Requiredby: ```",2024-08-20T14:35:14Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/23145,"It looks like you're running a really old version of JAX (0.4.13; the current version is 0.4.31). You'll probably need to update your Python version because 3.8 is no longer supported, and then install the most recent version of JAX (`pip install U ""jax[cuda12]""`) to see if that does the trick!"," Thank you for your response! > It looks like you're running a really old version of JAX (0.4.13; the current version is 0.4.31). You'll probably need to update your Python version because 3.8 is no longer supported, and then install the most recent version of JAX (`pip install U ""jax[cuda12]""`) to see if that does the trick! I tried to upgrade into a new version but I only get this after running  'pip install U ""jax[cuda12]""': `pip show jax jaxlib WARNING: Package(s) not found: jaxlib Name: jax Version: 0.4.13 Summary: Differentiate, compile, and transform Numpy code. Homepage: https://github.com/google/jax Author: JAX team Authoremail: jaxdev.com License: Apache2.0 Location: /home/mikabell/anaconda3/envs/jaxenv/lib/python3.8/sitepackages Requires: importlibmetadata, mldtypes, numpy, opteinsum, scipy Requiredby: ` for some reason, it doesn't upgrade",Did you upgrade your Python version? Python 3.8 is no longer supported!,"Given the 🎉  reaction, I'm going to hope that you got this sorted out on your end  and close this issue. Please feel free to comment or open a new issue if the problem persists!"
yi,Added `PyUnstable_Module_SetGIL` to `PyInit_cpu_feature_guard`,**This PR is based on https://github.com/google/jax/pull/23129** Description:  Added `PyUnstable_Module_SetGIL` to `PyInit_cpu_feature_guard` to explicitly indicate the extension supports running with the GIL disabled. Refs:  https://pyfreethreading.github.io/porting/__tabbed_1_1 Context:  https://github.com/google/jax/issues/23073,2024-08-19T22:16:20Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/23130
yi,Bump setuptools from 69.2.0 to 72.2.0,"Bumps setuptools from 69.2.0 to 72.2.0.  Changelog Sourced from setuptools's changelog.  v72.2.0 Features  pypa/distutils CC(initial spmd groundwork)pypa/distutils CC(Add lax.gather and lax.scatter_add.)pypa/distuils CC(Failing lax_numpy_indexing_test.py tests on Python 3.7) CC(ignore mypy error in jax2tf))  v72.1.0 Features  Restore the tests command and deprecate access to the module. ( CC(Internal change)) ( CC(Passing Haiku Module through custom_vjp lose transform))  v72.0.0 Deprecations and Removals  The test command has been removed. Users relying on 'setup.py test' will need to migrate to another test runner or pin setuptools before this version. ( CC(Question about stax batch dimension))  v71.1.0 Features   Added return types to typed public functions  by :user:Avasam Marked pkg_resources as py.typed  by :user:Avasam ( CC(Compile error on updating to JAX0.2.0))   Misc   CC(未找到相关数据)  v71.0.4 Bugfixes   ... (truncated)   Commits  76942cf Bump version: 72.1.0 → 72.2.0 780a782 Correct reference in news fragment. Ref  CC(Add jax.numpy.polyint) 64f10c5 Merge pull request  CC(Add jax.numpy.polyint) from pypa/feature/distutilsb7ee725f3 d4ad24b Add news fragment. 0ab156c Merge https://github.com/pypa/distutils b7ee725 Apply isort. Ref pypa/distutils CC(Work around breakage due to Numpy 1.16.0 version update.) 32e5fea Rely on monkeysession to monkeypatch. 5f79f22 Merge pull request pypa/distutils CC(MaxPool does not yet support jvp) from msys2contrib/cimsvcpythonmingw... 6748224 Merge pull request pypa/distutils CC(AvgPool does only work as global average pooling) from msys2contrib/testsfixvenv_insta... 1f999b9 Remove unused RangeMap Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-08-19T17:22:24Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23123,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
rag,Bump importlib-resources from 6.4.0 to 6.4.3,"Bumps importlibresources from 6.4.0 to 6.4.3.  Changelog Sourced from importlibresources's changelog.  v6.4.3 Bugfixes  When inferring the caller in files()python/cpython CC(未找到相关数据)  v6.4.2 Bugfixes  Merged fix for UTF16 BOM handling in functional tests. ( CC(generalize select batch rule (fixes 311)))  v6.4.1 Bugfixes  python/cpython CC(未找到相关数据)     Commits  d021417 Finalize 0ecbc3b Merge pull request  CC(make einsum deterministic, correct.) from python/gh123085/inferredcompiled 79fa62f Add docstring and reference to the issue. 90c0e42 Rely on resources.__name__ for easier portability. d618902 Add news fragment. ebc5b97 Extract the filename from the topmost frame of the stack. 4ea81bf Extract a function for computing 'this filename' once. cba8dce Adapt changes for new fixtures. 198adec gh121735: Fix inferring caller when resolving importlib.resources.files() 21afd61 Merge changes to syncronize the 6.4 release with downstream CPython changes. Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-08-19T17:20:49Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23120,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
yi,Rollback for: Implement initial vmap over pallas_call w/ ragged inputs (via jumbles),Rollback for: Implement initial vmap over pallas_call w/ ragged inputs (via jumbles) It can cause issues in x32 when trying to get the aval for array dimension sizes that are larger than i32. Reverts 24394a1b03f01138219013f4773104b834e498b7,2024-08-19T11:12:06Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23112
rag,Pallas TPU reduce kernel generalization gives wrong numerical results?," Description Credit to simveit for investigating implementing a generalization of the Pallas TPU reduce kernel here: The original (nongeneralized kernel): ```python def sum_kernel(x_ref, o_ref):   .when(pl.program_id(axis=0) == 0)   def _():     o_ref[...] = jnp.zeros_like(o_ref)   o_ref[...] += x_ref[...] def sum(x: jax.Array) > jax.Array:   grid, *out_shape = x.shape   return pl.pallas_call(       sum_kernel,       grid=grid,        None in `block_shape` means we pick a size of 1 and squeeze it away       in_specs=[pl.BlockSpec((None, *out_shape), lambda i: (i, 0, 0))],       out_specs=pl.BlockSpec(out_shape, lambda i: (0, 0)),       out_shape=jax.ShapeDtypeStruct(out_shape, x.dtype)   )(x) sum(x) ``` The user is not following the docs paragraph which say only last (minormost) dimensions should be reduced over. However, a transposition of the userkernel (according to what the docs suggest) also gives a numerically wrong result. Repro code: ```python import jax from jax.experimental import pallas as pl import jax.numpy as jnp import numpy as np import functools from jax import random   Reducing over the minormost axis   https://jax.readthedocs.io/en/latest/pallas/tpu/pipelining.html   > One last thing to note about reductions in Pallas are that they must be done  > in the minormost (rightmost) dimensions of our grid (our grid is 1dimensional  > in the above example so we are reducing over its minormost dimension).   > This is because the pipeline that Pallas generates using the BlockSpecs, grid  > and kernel function does not read outputs back from HBM. Once you’ve written an  > output value back to HBM you cannot revisit it.   > Therefore, you cannot do a reduction across a grid dimension that has any  > revisiting and therefore all reductions need to happen in the rightmost  > dimensions.  + id=""2pzgdMNivbdq"" def sum_kernel(x_ref, o_ref):     .when(pl.program_id(axis=0) == 0)     def _():         o_ref[...] = jnp.zeros_like(o_ref)     o_ref[...] += x_ref[...] .partial(jax.jit, static_argnames=[""b1"", ""b2"", ""strategy"", ""interpret""]) def sum_fn(x: jax.Array, b1: int, b2: int, strategy: str, interpret: bool = False) > jax.Array:     g0, *out_shape = x.shape   grid is the first dimension of x     g1 = out_shape[0] // b1     g2 = out_shape[1] // b2     print(f""g0: {g0}, g1: {g1}, g2: {g2}"")     if strategy == ""user_major_dimension"":         grid = (g0, g1, g2)         in_specs = [pl.BlockSpec(block_shape=(None, b1, b2), index_map=lambda i, j, k: (i, j, k))]         out_specs = pl.BlockSpec(block_shape=(b1, b2), index_map=lambda i, j, k: (j, k))     elif strategy == ""1d_minor_dimension"":         grid = (g0,)         in_specs = [pl.BlockSpec(block_shape=(None, *out_shape), index_map=lambda k: (k, 0, 0))]         out_specs = pl.BlockSpec(block_shape=tuple(out_shape), index_map=lambda k: (0, 0))     else:       grid = (g1, g2, g0)       in_specs = [pl.BlockSpec(block_shape=(None, b1, b2), index_map=lambda i, j, k: (k, i, j))]       out_specs = pl.BlockSpec(block_shape=(b1, b2), index_map=lambda i, j, k: (i, j))       if strategy == ""faux_3d_minor_dimension"":         assert g1 == g2 == 1, ""In faux 3d minor dimension reduction, dimensions g1, g2 must == 1""       elif strategy == ""full_3d_minor_dimension"":         pass       else:         raise ValueError(f""{strategy = } not supported."")     return pl.pallas_call(         sum_kernel,         grid=grid,         in_specs=in_specs,         out_specs=out_specs,         out_shape=jax.ShapeDtypeStruct(out_shape, x.dtype),         interpret=interpret,     )(x) n = 128  arr = jax.numpy.ones((11, 512, 512)) primes = [3, 5, 7, 11, 13] arr = jax.numpy.ones((len(primes), n, n)) * jnp.array(primes)[:, None, None] b1 = n // 4 b2 = n print(arr.devices()) strategies = [   ""user_major_dimension"",  user example, disregarding minordimensiononly rule, fails   ""1d_minor_dimension"",   suggested by the docs, works   ""faux_3d_minor_dimension"",   technically equivalent to the docs, but fails   ""full_3d_minor_dimension"",   a generalization of faux_3d, docs imply OK, but fails ]  CHOOSE THE STRATEGY HERE strategy = strategies[1] print(f""{strategy = }"") if strategy == ""faux_3d_minor_dimension"":   b1 = n pallas_val = sum_fn(arr, b1, b2, strategy=strategy) ref_val = jnp.sum(arr, 0) print(pallas_val) print(ref_val) np.testing.assert_array_equal(pallas_val, ref_val) ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.31 jaxlib: 0.4.31 numpy:  2.0.1 python: 3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0] jax.devices (4 total, 4 local): [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0) TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0) TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0) TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)] process_count: 1 platform: uname_result(system='Linux', node='t1vn8e5ad715w0', release='5.19.01022gcp', version=' CC(attempt to centerjustify the jax logo in readme)~22.04.1Ubuntu SMP Sun Apr 23 09:51:08 UTC 2023', machine='x86_64') ```",2024-08-17T00:48:56Z,pallas,closed,0,3,https://github.com/jax-ml/jax/issues/23099,Pinging  for visibility.,"If you change the grid rank (e.g. `(g0,) > (g1, g2, g0)`), you'll also need to change the `pl.program_id` axis in the kernel. The code works as expected when you modify the kernel to: ``` def sum_kernel(x_ref, o_ref):     .when(pl.program_id(axis=2) == 0)     def _():         o_ref[...] = jnp.zeros_like(o_ref)     o_ref[...] += x_ref[...] ``` for strategies 2 and 3.","That was indeed the problem! The modified example works for all strategies where the minormost axis is reduced: ```python def sum_kernel(x_ref, o_ref, last_axis):     .when(pl.program_id(axis=last_axis) == 0)     def _():         o_ref[...] = jnp.zeros_like(o_ref)     o_ref[...] += x_ref[...] .partial(jax.jit, static_argnames=[""b1"", ""b2"", ""strategy"", ""interpret""]) def sum_fn(x: jax.Array, b1: int, b2: int, strategy: str, interpret: bool = False) > jax.Array:     g0, *out_shape = x.shape   grid is the first dimension of x     g1 = out_shape[0] // b1     g2 = out_shape[1] // b2     print(f""g0: {g0}, g1: {g1}, g2: {g2}"")     if strategy == ""user_major_dimension"":         grid = (g0, g1, g2)         in_specs = [pl.BlockSpec(block_shape=(None, b1, b2), index_map=lambda i, j, k: (i, j, k))]         out_specs = pl.BlockSpec(block_shape=(b1, b2), index_map=lambda i, j, k: (j, k))     elif strategy == ""1d_minor_dimension"":         grid = (g0,)         in_specs = [pl.BlockSpec(block_shape=(None, *out_shape), index_map=lambda k: (k, 0, 0))]         out_specs = pl.BlockSpec(block_shape=tuple(out_shape), index_map=lambda k: (0, 0))     else:       grid = (g1, g2, g0)       in_specs = [pl.BlockSpec(block_shape=(None, b1, b2), index_map=lambda i, j, k: (k, i, j))]       out_specs = pl.BlockSpec(block_shape=(b1, b2), index_map=lambda i, j, k: (i, j))       if strategy == ""faux_3d_minor_dimension"":         assert g1 == g2 == 1, ""In faux 3d minor dimension reduction, dimensions g1, g2 must == 1""       elif strategy == ""full_3d_minor_dimension"":         pass       else:         raise ValueError(f""{strategy = } not supported."")     return pl.pallas_call(         functools.partial(sum_kernel, last_axis=len(grid)  1),         grid=grid,         in_specs=in_specs,         out_specs=out_specs,         out_shape=jax.ShapeDtypeStruct(out_shape, x.dtype),         interpret=interpret,     )(x) ```"
yi,CuDNN Initialization Error with Pre-existing CUDA Environment Fails with," Description Our a cluster node which already has Nvidia drivers and the CUDA toolkit installed (to maintain version compatibility with the underlying OS and the networking stack). Installing via jax[cuda12_local], would make sense. But, as mentioned in the installation guide, JAX requires CUDNN. This would entail that additional packages need to be installed (assuming JAX and the additional packages are installed in the Docker container). But it fails with an error. So, is the recommended practice to always install with jax[cuda12] even though CUDA is already installed? Note: I am setting the `LD_LIBRARY_PATH` to the location where cuDNN is installed using pip install nvidiacudnncu12. Error ```g CUDA libraries. GPU will not be used. : Error loading CUDA libraries. GPU will not be used. jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/workspace/jax_pingpong.py"", line 49, in      init_processes()   File ""/workspace/jax_pingpong.py"", line 44, in init_processes     run()   File ""/workspace/jax_pingpong.py"", line 24, in run     xs = jax.numpy.ones(jax.local_device_count())   File ""/usr/local/lib/python3.10/sitepackages/jax/_src/numpy/lax_numpy.py"", line 3883, in ones     return lax.full(shape, 1, _jnp_dtype(dtype), sharding=_normalize_to_sharding(device))   File ""/usr/local/lib/python3.10/sitepackages/jax/_src/lax/lax.py"", line 1302, in full     fill_value = _convert_element_type(fill_value, dtype, weak_type)   File ""/usr/local/lib/python3.10/sitepackages/jax/_src/lax/lax.py"", line 566, in _convert_element_type     return convert_element_type_p.bind(   File ""/usr/local/lib/python3.10/sitepackages/jax/_src/lax/lax.py"", line 2559, in _convert_element_type_bind     operand = core.Primitive.bind(convert_element_type_p, operand,   File ""/usr/local/lib/python3.10/sitepackages/jax/_src/core.py"", line 429, in bind     return self.bind_with_trace(find_top_trace(args), args, params)   File ""/usr/local/lib/python3.10/sitepackages/jax/_src/core.py"", line 433, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/usr/local/lib/python3.10/sitepackages/jax/_src/core.py"", line 939, in process_primitive     return primitive.impl(*tracers, **params)   File ""/usr/local/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 87, in apply_primitive     outs = fun(*args) jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors ``` Note: jax.devices() gives the expected output  System info (python version, jaxlib version, accelerator, etc.) 1. Accelerator: GPU 2. CUDA12 3. cuDNN > 9.0 ",2024-08-16T21:22:36Z,bug NVIDIA GPU,closed,0,5,https://github.com/jax-ml/jax/issues/23097,"If you have a CUDA already installed and you want to use that, but cudnn (or some other libs are missing). I think you should do: `jax[cuda12_local]` but also install the other missing packages. You could do that via pip to keep it simple. For cudnn: `nvidiacudnncu12` Here is others wheel that you could need: ```           ""nvidiacublascu12"",           ""nvidiacudanvcccu12"",           ""nvidiacudaruntimecu12"",           ""nvidiacufftcu12"",           ""nvidiacusolvercu12"",           ""nvidiacusparsecu12"", ```",", are the above packages not installed by the CUDA toolkit or during the cuDNN installation? Is there a document that mentions the list of required packages (either through pip or otherwise) for the endtoend flow to work?","You don't have a normal setup. There isn't doc for it.  Some of the packages above are provided by the cuda sdk. But cudnn and nccl isn't.  Can you try installing only cudnn and nccl packages?  If that don't work, report the error."," Installing the packages that you mentioned in the previous comment resolved the issue  ```  pip install U jax[cuda12_local]  pip install 'nvidiacudnncu12>=9.1.0,<10.0.0'  pip install ""nvidiacublascu12""  pip install ""nvidiacudanvcccu12""  pip install ""nvidiacudaruntimecu12""  pip install ""nvidiacufftcu12""  pip install ""nvidiacusolvercu12""  pip install ""nvidiacusparsecu12"" ```",You probably installed too many packages. I don't think this is an issue. Should we close this issue?
rag,"[Easy] Refactor ragged_dot transpose, combine ragged_to_dense","[Easy] Refactor ragged_dot transpose, combine ragged_to_dense",2024-08-14T16:48:16Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23061
yi,Import etils.epath lazily,Import etils.epath lazily This shaves off an extra 0.10.2s from JAX import times internally.,2024-08-13T20:15:14Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23037
yi,Errors from interaction between `shard_map` with `auto` and `lax.map`," Description I'm converting https://github.com/google/jax/discussions/23015 into an issue per 's request. Here's a minimal working example of what I'm trying to do. My goal in function `h` is to map a second function `g` over the first dimension `a` of a 2x2 array. Within `g`, I attempt to map a third function `f` over the array's second dimension `b`. Each map applies `shard_map` to `lax.map`; `vmap` isn't a good replacement because its batching rule for my code's actual `f` is quite expensive. ```python import os import jax print(jax.__version__)   0.4.31 jax.config.update('jax_platform_name', 'cpu') os.environ['XLA_FLAGS'] = 'xla_force_host_platform_device_count=4' from functools import partial from jax.sharding import Mesh, PartitionSpec as P from jax.experimental.shard_map import shard_map from jax.experimental.mesh_utils import create_device_mesh mesh = Mesh(create_device_mesh((2, 2)), ('a', 'b')) def f(x):     return x + 1 def g(x):     return shard_map(partial(jax.lax.map, f), mesh, P('b'), P('b'))(x) + 2 def h(x):     return shard_map(partial(jax.lax.map, g), mesh, P('a'), P('a'), check_rep=False, auto=frozenset({'b'}))(x) + 3 import numpy as np print(g(np.zeros(2)))   [3. 3.] print(h(np.zeros((2, 2))))   error ``` My understanding is that `auto` should make this work. But I get a `NotImplementedError` from the start of `jax.experimental.shard_map._shard_map_impl`. When I wrap `h` in a `jit` before calling it, I instead get the following:     F external/xla/xla/hlo/utils/hlo_sharding_util.cc:2806] Check failed: sharding.IsManualSubgroup()   System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.31 jaxlib: 0.4.31 numpy:  2.0.1 python: 3.10.13  (main, Dec 23 2023, 15:36:39) [GCC 12.3.0] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', release='5.10.16.3microsoftstandardWSL2', version='1 SMP Fri Apr 2 22:23:49 UTC 2021', machine='x86_64')",2024-08-12T19:11:58Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/23019
yi,Finite precision error calculations always 0 under JIT with bfloat16," Description I have some stochastic rounding code and uncovered a bug when trying to use the code like the following: ```py def _error(x: ndarray[*Shape, Float], y: ndarray[*Shape, Float], result: ndarray[*Shape, Float]):     y2 = result  x     x2 = result  y2     error_y = y  y2     error_x = x  x2     return error_x + error_y def add(x: ndarray[*Shape, Float], y: ndarray[*Shape, Float]):     result = x + y     return _error(x, y, result) ``` ```py dtype = bfloat16 op1 = jax.random.normal(jax.random.key(0), (1000, 4), dtype=dtype) op2 = jax.random.normal(jax.random.key(1), (1000, 4), dtype=dtype) print(jax.vmap(add)(op1, op2)) print(jnp.all(jax.jit(jax.vmap(add))(op1, op2) == 0)) ``` With bfloat16, the final line prints `True` even though it's clear from the preceding line that not all errors ought to be 0. `np.float32` does not have this behavior. Here are some lowering and compilation outputs, if that happens to be helpful. First bfloat16 and then float32: ```py dtype = bfloat16 args = (jnp.arange(4, 7, dtype=dtype), jnp.arange(3, dtype=dtype) / 1000) print(add(*args)) print(jax.jit(add)(*args)) print(jax.jit(add).lower(*args).as_text()) print(jax.jit(add).lower(*args).compile().as_text()) ``` ``` [0 0.000999451 0.0019989] [0 0 0] module  attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {   func.func public (%arg0: tensor {mhlo.layout_mode = ""default""}, %arg1: tensor {mhlo.layout_mode = ""default""}) > (tensor {jax.result_info = """", mhlo.layout_mode = ""default""}) {     %0 = stablehlo.add %arg0, %arg1 : tensor     %1 = stablehlo.subtract %0, %arg0 : tensor     %2 = stablehlo.subtract %0, %1 : tensor     %3 = stablehlo.subtract %arg1, %1 : tensor     %4 = stablehlo.subtract %arg0, %2 : tensor     %5 = stablehlo.add %4, %3 : tensor     return %5 : tensor   } } HloModule jit_add, is_scheduled=true, entry_computation_layout={(bf16[3]{0}, bf16[3]{0})>bf16[3]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}, frontend_attributes={fingerprint_before_lhs=""82694c3355091a0097f584dec86f3d57""} %fused_convert (param_0.3: bf16[3], param_1.5: bf16[3]) > bf16[3] {   %param_0.3 = bf16[3]{0} parameter(0)   %convert.9.1 = f32[3]{0} convert(bf16[3]{0} %param_0.3)   %param_1.5 = bf16[3]{0} parameter(1)   %convert.1.1 = f32[3]{0} convert(bf16[3]{0} %param_1.5)   %add.2.1 = f32[3]{0} add(f32[3]{0} %convert.9.1, f32[3]{0} %convert.1.1), metadata={op_name=""jit(add)/jit(main)/add"" source_file=""/tmp/ipykernel_47449/771070407.py"" source_line=787}   %subtract.8.1 = f32[3]{0} subtract(f32[3]{0} %add.2.1, f32[3]{0} %convert.9.1), metadata={op_name=""jit(add)/jit(main)/sub"" source_file=""/tmp/ipykernel_47449/771070407.py"" source_line=779}   %subtract.9.1 = f32[3]{0} subtract(f32[3]{0} %add.2.1, f32[3]{0} %subtract.8.1), metadata={op_name=""jit(add)/jit(main)/sub"" source_file=""/tmp/ipykernel_47449/771070407.py"" source_line=780}   %subtract.10.1 = f32[3]{0} subtract(f32[3]{0} %convert.9.1, f32[3]{0} %subtract.9.1), metadata={op_name=""jit(add)/jit(main)/sub"" source_file=""/tmp/ipykernel_47449/771070407.py"" source_line=782}   %subtract.11.1 = f32[3]{0} subtract(f32[3]{0} %convert.1.1, f32[3]{0} %subtract.8.1), metadata={op_name=""jit(add)/jit(main)/sub"" source_file=""/tmp/ipykernel_47449/771070407.py"" source_line=781}   %add.4.1 = f32[3]{0} add(f32[3]{0} %subtract.10.1, f32[3]{0} %subtract.11.1), metadata={op_name=""jit(add)/jit(main)/add"" source_file=""/tmp/ipykernel_47449/771070407.py"" source_line=783}   ROOT %convert.17.1 = bf16[3]{0} convert(f32[3]{0} %add.4.1) } ENTRY %main.9 (Arg_0.1.0: bf16[3], Arg_1.2.0: bf16[3]) > bf16[3] {   %Arg_1.2.0 = bf16[3]{0} parameter(1), metadata={op_name=""y""}   %Arg_0.1.0 = bf16[3]{0} parameter(0), metadata={op_name=""x""}   ROOT %loop_convert_fusion = bf16[3]{0} fusion(bf16[3]{0} %Arg_0.1.0, bf16[3]{0} %Arg_1.2.0), kind=kLoop, calls=%fused_convert } ``` ```py dtype = np.float32 args = (jnp.arange(4, 7, dtype=dtype), jnp.arange(3, dtype=dtype) / 1000) print(add(*args)) print(jax.jit(add)(*args)) print(jax.jit(add).lower(*args).as_text()) print(jax.jit(add).lower(*args).compile().as_text()) ``` ``` [0.0000000e+00 7.2526745e08 1.4505349e07] [0.0000000e+00 7.2526745e08 1.4505349e07] module  attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {   func.func public (%arg0: tensor {mhlo.layout_mode = ""default""}, %arg1: tensor {mhlo.layout_mode = ""default""}) > (tensor {jax.result_info = """", mhlo.layout_mode = ""default""}) {     %0 = stablehlo.add %arg0, %arg1 : tensor     %1 = stablehlo.subtract %0, %arg0 : tensor     %2 = stablehlo.subtract %0, %1 : tensor     %3 = stablehlo.subtract %arg1, %1 : tensor     %4 = stablehlo.subtract %arg0, %2 : tensor     %5 = stablehlo.add %4, %3 : tensor     return %5 : tensor   } } HloModule jit_add, is_scheduled=true, entry_computation_layout={(f32[3]{0}, f32[3]{0})>f32[3]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}, frontend_attributes={fingerprint_before_lhs=""9d06ea6507421c8754deb14690ff8cd9""} %fused_add (param_0.1: f32[3], param_1.3: f32[3]) > f32[3] {   %param_1.3 = f32[3]{0} parameter(1)   %param_0.1 = f32[3]{0} parameter(0)   %add.2.1 = f32[3]{0} add(f32[3]{0} %param_1.3, f32[3]{0} %param_0.1), metadata={op_name=""jit(add)/jit(main)/add"" source_file=""/tmp/ipykernel_47449/771070407.py"" source_line=787}   %subtract.8.1 = f32[3]{0} subtract(f32[3]{0} %add.2.1, f32[3]{0} %param_1.3), metadata={op_name=""jit(add)/jit(main)/sub"" source_file=""/tmp/ipykernel_47449/771070407.py"" source_line=779}   %subtract.9.1 = f32[3]{0} subtract(f32[3]{0} %add.2.1, f32[3]{0} %subtract.8.1), metadata={op_name=""jit(add)/jit(main)/sub"" source_file=""/tmp/ipykernel_47449/771070407.py"" source_line=780}   %subtract.10.1 = f32[3]{0} subtract(f32[3]{0} %param_1.3, f32[3]{0} %subtract.9.1), metadata={op_name=""jit(add)/jit(main)/sub"" source_file=""/tmp/ipykernel_47449/771070407.py"" source_line=782}   %subtract.11.1 = f32[3]{0} subtract(f32[3]{0} %param_0.1, f32[3]{0} %subtract.8.1), metadata={op_name=""jit(add)/jit(main)/sub"" source_file=""/tmp/ipykernel_47449/771070407.py"" source_line=781}   ROOT %add.4.1 = f32[3]{0} add(f32[3]{0} %subtract.10.1, f32[3]{0} %subtract.11.1), metadata={op_name=""jit(add)/jit(main)/add"" source_file=""/tmp/ipykernel_47449/771070407.py"" source_line=783} } ENTRY %main.9 (Arg_0.1.0: f32[3], Arg_1.2.0: f32[3]) > f32[3] {   %Arg_1.2.0 = f32[3]{0} parameter(1), metadata={op_name=""y""}   %Arg_0.1.0 = f32[3]{0} parameter(0), metadata={op_name=""x""}   ROOT %loop_add_fusion = f32[3]{0} fusion(f32[3]{0} %Arg_1.2.0, f32[3]{0} %Arg_0.1.0), kind=kLoop, calls=%fused_add, metadata={op_name=""jit(add)/jit(main)/add"" source_file=""/tmp/ipykernel_47449/771070407.py"" source_line=783} } ``` (Originally reported at: https://github.com/jaxml/ml_dtypes/issues/167)  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.30 jaxlib: 0.4.30 numpy:  1.26.1 python: 3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 11.4.0] jax.devices (1 total, 1 local): [cuda(id=0)] process_count: 1 platform: uname_result(system='Linux', node='npjfe11cq9', release='5.19.045generic', version=' CC(Update XLA and reenable numpy tests that failed on Mac)~22.04.1Ubuntu SMP PREEMPT_DYNAMIC Wed Jun 7 15:06:04 UTC 20', machine='x86_64') $ nvidiasmi Sun Aug 11 01:03:45 2024        ++  ++ ```",2024-08-12T16:03:04Z,bug,open,0,3,https://github.com/jax-ml/jax/issues/23007,"Hi  thanks for the question! I spent some time making a more concise reproduction here ```python import jax def check_err(x, y):   result = x + y   y2 = result  x   return y  y2 op1 = jax.random.normal(jax.random.key(0), (5,), dtype='bfloat16') op2 = jax.random.normal(jax.random.key(1), (5,), dtype='bfloat16') print(check_err(op1, op2))  [0 0.00244141 0 0.000488281 0.00390625] print(jax.jit(check_err)(op1, op2))  [0 0 0 0 0] ``` Since it looks like the compiler is doing something unexpected here, it will help to print the optimized HLO for the function: ```python print(jax.jit(check_err).lower(op1, op2).compile().as_text()) ``` ``` HloModule jit_check_err, entry_computation_layout={(bf16[5]{0}, bf16[5]{0})>bf16[5]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true} %fused_computation (param_0.2: bf16[5], param_1.4: bf16[5]) > bf16[5] {   %param_1.4 = bf16[5]{0} parameter(1)   %convert.11 = f32[5]{0} convert(bf16[5]{0} %param_1.4)   %param_0.2 = bf16[5]{0} parameter(0)   %convert.10 = f32[5]{0} convert(bf16[5]{0} %param_0.2)   %add.0 = f32[5]{0} add(f32[5]{0} %convert.10, f32[5]{0} %convert.11), metadata={op_name=""jit(check_err)/jit(main)/add"" source_file="""" source_line=4}   %subtract.1 = f32[5]{0} subtract(f32[5]{0} %add.0, f32[5]{0} %convert.10), metadata={op_name=""jit(check_err)/jit(main)/sub"" source_file="""" source_line=5}   %subtract.0 = f32[5]{0} subtract(f32[5]{0} %convert.11, f32[5]{0} %subtract.1), metadata={op_name=""jit(check_err)/jit(main)/sub"" source_file="""" source_line=6}   ROOT %convert.9 = bf16[5]{0} convert(f32[5]{0} %subtract.0) } ENTRY %main.6 (Arg_0.1: bf16[5], Arg_1.2: bf16[5]) > bf16[5] {   %Arg_0.1 = bf16[5]{0} parameter(0)   %Arg_1.2 = bf16[5]{0} parameter(1)   ROOT %fusion = bf16[5]{0} fusion(bf16[5]{0} %Arg_0.1, bf16[5]{0} %Arg_1.2), kind=kLoop, calls=%fused_computation } ``` and this shows what the problem is: the line `%convert.11 = f32[5]{0} convert(bf16[5]{0} %param_1.4)` is converting the input to `float32` before doing all the operations, and then `%convert.9 = bf16[5]{0} convert(f32[5]{0} %subtract.0)` converts this back to `bfloat16`. Thus the error is accumulating in float32 precision, and then when this small error is cast back to `bfloat16`, it is too small to be represented in bfloat16, and so we get zero. Essentially, the JITcompiled version is effectively doing this: ```python def check_err(x, y):   x, y = x.astype('float32'), y.astype('float32')   result = x + y   y2 = result  x   return (y  y2).astype('bfloat16') ``` I'm not aware of any way to prevent the compiler from doing this kind of casting – it's probably due to the fact that the hardware (CPU in my case) does not support native bfloat16 operations. I'll ask around to see if others have ideas.","Via , it seems the `xla_allow_excess_precision` flag controls this behavior. If you set it to False, then the compiler won't do this sort of internal upcasting: ```python import os os.environ['XLA_FLAGS'] = ""xla_allow_excess_precision=false"" import jax def check_err(x, y):   result = x + y   y2 = result  x   return y  y2 op1 = jax.random.normal(jax.random.key(0), (5,), dtype='bfloat16') op2 = jax.random.normal(jax.random.key(1), (5,), dtype='bfloat16') print(check_err(op1, op2))  [0 0.00244141 0 0.000488281 0.00390625] print(jax.jit(check_err)(op1, op2))  [0 0.00244141 0 0.000488281 0.00390625] ``` Note that XLA flag values are only read at the time the backend is initialized, so be sure to set them either as a system variable outside your script, or in your script via `os.environ` before running any `jax` commands.",That seems to work. Thanks!
yi,Import from ``mlir.dialects`` lazily,Import from ``mlir.dialects`` lazily These imports jointly account for ~0.3s of import time internally.,2024-08-12T12:19:04Z,,closed,0,0,https://github.com/jax-ml/jax/issues/22998
yi,CUDA `XlaRuntimeError` with MPI on `jax==0.4.31`," Description Hi, `jax.jit` on a function seems to fail when running in an OpenMPI environment. An MWE is shown below: ```python  error.py  Run as: mpirun n 8 python error.py import os from absl import logging import jax, jax.numpy as jnp logging.set_verbosity(""info"") os.environ[""no_proxy""] = ""x.x.x.x""   Internal use. jax.distributed.initialize() print(""Hello from process %d holding %d device(s)"" % (jax.process_index(), jax.local_device_count())) def dot_product_attention(     query: jnp.ndarray,     key: jnp.ndarray,     value: jnp.ndarray,     *,     dtype: jnp.dtype = jnp.float32) > jnp.ndarray:   depth = query.shape[1]   query = query / jnp.sqrt(depth).astype(dtype)   attn_weights = jnp.einsum('...qhd,...khd>...hqk', query, key)   attn_weights = jax.nn.softmax(attn_weights).astype(dtype)   return jnp.einsum('...hqk,...khd>...qhd', attn_weights, value) x = jnp.ones((1, 512, 8, 32), dtype=jnp.bfloat16) f = lambda x: dot_product_attention(x, x, x) print(jax.jit(f)(x)) ``` The error can be on select processes (in which case I see the output tensor) or all processes (it hangs/exits). I can confirm this error does not appear in `jax==0.4.30`.  System info (python version, jaxlib version, accelerator, etc.)    Error log   ```shell   JAX detected proxy variable(s) in the environment as distributed setup: no_proxy https_proxy HTTPS_PROXY HTTP_PROXY http_proxy. On some systems, this may cause a hang of distributed.initialize and you may need to unset these ENV variable(s)   JAX detected proxy variable(s) in the environment as distributed setup: no_proxy https_proxy HTTPS_PROXY HTTP_PROXY http_proxy. On some systems, this may cause a hang of distributed.initialize and you may need to unset these ENV variable(s)   JAX detected proxy variable(s) in the environment as distributed setup: no_proxy https_proxy HTTPS_PROXY HTTP_PROXY http_proxy. On some systems, this may cause a hang of distributed.initialize and you may need to unset these ENV variable(s)   JAX detected proxy variable(s) in the environment as distributed setup: no_proxy https_proxy HTTPS_PROXY HTTP_PROXY http_proxy. On some systems, this may cause a hang of distributed.initialize and you may need to unset these ENV variable(s)   JAX detected proxy variable(s) in the environment as distributed setup: no_proxy https_proxy HTTPS_PROXY HTTP_PROXY http_proxy. On some systems, this may cause a hang of distributed.initialize and you may need to unset these ENV variable(s)   JAX detected proxy variable(s) in the environment as distributed setup: no_proxy https_proxy HTTPS_PROXY HTTP_PROXY http_proxy. On some systems, this may cause a hang of distributed.initialize and you may need to unset these ENV variable(s)   JAX detected proxy variable(s) in the environment as distributed setup: no_proxy https_proxy HTTPS_PROXY HTTP_PROXY http_proxy. On some systems, this may cause a hang of distributed.initialize and you may need to unset these ENV variable(s)   JAX detected proxy variable(s) in the environment as distributed setup: no_proxy https_proxy HTTPS_PROXY HTTP_PROXY http_proxy. On some systems, this may cause a hang of distributed.initialize and you may need to unset these ENV variable(s)   Hello from process 3 holding 1 device(s)   Hello from process 5 holding 1 device(s)   Hello from process 1 holding 1 device(s)   Hello from process 7 holding 1 device(s)   Hello from process 0 holding 1 device(s)   Hello from process 4 holding 1 device(s)   Hello from process 6 holding 1 device(s)   Hello from process 2 holding 1 device(s)   jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.   The above exception was the direct cause of the following exception:   Traceback (most recent call last):     File ""/home/karan/workspace/jax_gpt2/error.py"", line 14, in        print(jax.jit(f)(x))   jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: GetKeyValue() timed out with key: cuda:gemm_fusion_autotuning_results_5_0 and duration: 1ms   jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.   The above exception was the direct cause of the following exception:   Traceback (most recent call last):     File ""/home/karan/workspace/jax_gpt2/error.py"", line 14, in        print(jax.jit(f)(x))   jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: GetKeyValue() timed out with key: cuda:gemm_fusion_autotuning_results_5_0 and duration: 1ms   jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.   The above exception was the direct cause of the following exception:   Traceback (most recent call last):     File ""/home/karan/workspace/jax_gpt2/error.py"", line 14, in        print(jax.jit(f)(x))   jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: GetKeyValue() timed out with key: cuda:gemm_fusion_autotuning_results_5_0 and duration: 1ms   jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.   The above exception was the direct cause of the following exception:   Traceback (most recent call last):     File ""/home/karan/workspace/jax_gpt2/error.py"", line 14, in    jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.   The above exception was the direct cause of the following exception:   Traceback (most recent call last):     File ""/home/karan/workspace/jax_gpt2/error.py"", line 14, in        print(jax.jit(f)(x))   jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: GetKeyValue() timed out with key: cuda:gemm_fusion_autotuning_results_5_0 and duration: 1ms       print(jax.jit(f)(x))   jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: GetKeyValue() timed out with key: cuda:gemm_fusion_autotuning_results_5_0 and duration: 1ms   jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.   The above exception was the direct cause of the following exception:   Traceback (most recent call last):     File ""/home/karan/workspace/jax_gpt2/error.py"", line 14, in        print(jax.jit(f)(x))   jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: GetKeyValue() timed out with key: cuda:gemm_fusion_autotuning_results_5_0 and duration: 1ms   jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.   The above exception was the direct cause of the following exception:   Traceback (most recent call last):     File ""/home/karan/workspace/jax_gpt2/error.py"", line 14, in        print(jax.jit(f)(x))   jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: GetKeyValue() timed out with key: cuda:gemm_fusion_autotuning_results_5_1 and duration: 1ms   jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.   The above exception was the direct cause of the following exception:   Traceback (most recent call last):     File ""/home/karan/workspace/jax_gpt2/error.py"", line 14, in        print(jax.jit(f)(x))   jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: GetKeyValue() timed out with key: cuda:gemm_fusion_autotuning_results_5_0 and duration: 1ms      Primary job  terminated normally, but 1 process returned   a nonzero exit code. Per userdirection, the job has been aborted.         mpirun detected that one or more processes exited with nonzero status, thus causing   the job to be terminated. The first process to do so was:     Process name: [[53590,1],2]     Exit code:    1      ```   System info:   ```shell   jax:    0.4.31   jaxlib: 0.4.31   numpy:  1.26.4   python: 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0]   jax.devices (8 total, 8 local): [CudaDevice(id=0) CudaDevice(id=1) ... CudaDevice(id=6) CudaDevice(id=7)]   process_count: 1   platform: uname_result(system='Linux', node='ubuntu', release='6.5.035generic', version=' CC(CUDA90 and py3 )~22.04.1Ubuntu SMP PREEMPT_DYNAMIC Tue May  7 09:00:52 UTC 2', machine='x86_64')   Truncated nvidiasmi info:    NVIDIASMI 555.42.06                 Driver Version: 555.42.06         CUDA Version: 12.5   GPU: RTX A6000   ``` ",2024-08-12T06:16:09Z,bug,open,0,5,https://github.com/jax-ml/jax/issues/22995," the error is related with getting `cuda:gemm_fusion_autotuning_results` on shards and maybe related to https://github.com/openxla/xla/pull/13108 (). To disable the autotuning and to make your MWE work, you could try to run it with: ```bash XLA_FLAGS=xla_gpu_shard_autotuning=false  mpirun n 8 python error.py ``` Let me know if this workaround helps","https://github.com/openxla/xla/pull/13108 was reverted. xla_gpu_shard_autotuning=false disables sharding of autotuning, not the autotuning itself.","I can reproduce with jax==0.4.31 and xla_gpu_shard_autotuning=false helps  looks like https://github.com/openxla/xla/pull/13108 got into this JAX release before it got reverted. Thank you for cc'ing me, I'll investigate why does it fail.","5 Your suggestion worked.   I observed that JAX was built against https://github.com/openxla/xla/commit/95e3eea8d2aebd55160ed4185a38345ae98ab500, which was before the revert","I sent a fix to XLA which makes the reproducer from this bug work. Independent of that, sharded autotuning got enabled yesterday again and it will likely get into the next JAX release."
yi,Accessing incoming gradient and activations in custom_vjp,"Hi! I am trying to write a custom backward pass for a linear layer where I can simultaneously compute the pairwise inner product of all the gradients in the batch. More specifically, suppose I have $a = W x$, where $x \in \mathbb{R}^{B\times N}$ is the input and let $\delta \in \mathbb{R}^{B\times M}$ be the incoming gradient. I want to compute two $B \times B$ matrices that is equal to $x x^\top$ and $\delta  \delta^\top$.  Both of these quantities are already computed for the backpass but as far as I can tell they are only available through the val fields of the Batchtraced shaped array. This becomes very inconvenient when the function has gone through several `vmap` which means I need to call `.val` several times to reach the full tensor and it's hard to know a priori how many levels of vmap it has gone through. Is it possible to make it easier to do something like this? Another related question is what is the best way to compute the gradient in custom_vjp in this case? Also is this generally a good idea for jit? ```python import jax jnp = jax.numpy .custom_vjp def linear_vjp(weight_MxN, x_N):     return weight_MxN @ x_N def linear_vjp_fwd(weight, x_N):     y = weight_MxN @ x_N     return y, (weight_MxN, x_N) def linear_vjp_bwd(res, grad):     weight_MxN, x_N = res     if type(grad) != type(x_N):         grad_weight = jnp.tile(grad[:, None], (1, x_N.val.shape[0])) @ x_N.val     else:         grad_weight = grad.val @ x_N.val     grad_x = weight_MxN.T @ grad     if type(grad) != type(x_N):         delta_inner = grad @ grad     elif grad.batch_dim == 0:         delta_inner = grad.val @ grad.val.T     else:         delta_inner = grad.val.T @ grad.val  what is the best way to do this?     activation_inner = x_N.val @ x_N.val.T      additional processing     return grad_weight, grad_x linear_vjp.defvjp(linear_vjp_fwd, linear_vjp_bwd) ```",2024-08-12T00:48:46Z,enhancement,open,0,4,https://github.com/jax-ml/jax/issues/22993,"I'm not sure I totally follow the goal here, but your intuition that you don't want to rely on the `.val` attribute is a good one! Do you think you could put together a simpler endtoend example of the expected behavior that I could run? At a high level, I expect that the key assumption here is that this custom_vjp will always be called inside a `vmap`? Perhaps it would be better to define the `linear_vjp` function to directly operate on batched inputs rather than calling it from within a `vmap`? I'm happy to try to give more concrete suggestions if you can put together a runnable demo!","Thank you! I can explain a bit more. The goal I am trying to achieve is to compute the pairwise inner product between the gradient of every batch element. Normally, in the backward pass, the gradient to the weight would be computed as the matrix multiplication $\delta^\top x$ where $\delta \in \mathbb{R}^{B\times M}, x\in \mathbb{R}^{B\times N}$ where $B$ is the batch dimension. Within the `bwd`, I wish to do $\delta \delta^\top \in \mathbb{R}^{B\times B}$ which computes the (part of the) pairwise inner product between the gradient of the different batch elements so the outcome is not really a ""batched"" object anymore.  I wish to integrate this into any arbitrary model I can define so the function may go through more than one `vmap` (e.g., $\delta \in \mathbb{R}^{B\times T\times M}$ in data with two batch axes)  it would be inconvenient to have to rewrite all existing models to explicitly account for batching (and during inference none of this matters so forcing the function to be batched is not ideal). Below is an endtoend example: ```python import numpy as np import jax jnp = jax.numpy vmap = jax.vmap .custom_vjp def linear_vjp(weight_MxN, x_N):     return weight_MxN @ x_N def linear_vjp_fwd(weight_MxN, x_N):     y = weight_MxN @ x_N     return y, (weight_MxN, x_N) def linear_vjp_bwd(res, grad):     weight_MxN, x_N = res     if type(grad) != type(x_N):         grad_weight = jnp.tile(grad[:, None], (1, x_N.val.shape[0])) @ x_N.val     else:         grad_weight = grad.val @ x_N.val     grad_x = weight_MxN.T @ grad     if type(grad) != type(x_N):         delta_inner = grad @ grad     elif grad.batch_dim == 0:         delta_inner = grad.val @ grad.val.T     else:         delta_inner = grad.val.T @ grad.val  This line computes the pairwise inner product     print(delta_inner)  This line prints out the pairwise inner product     return grad_weight, grad_x linear_vjp.defvjp(linear_vjp_fwd, linear_vjp_bwd) x_BxN = jnp.array(np.ones((2, 2)))  B=2, N=2 w_MxN = jnp.array(np.ones((3, 2)))  M=3 target_BxM = jnp.array(np.zeros((2, 3))) def loss_fn(w, x):     def batch_linear(x):         def partial(x):             return linear_vjp(w, x)         return vmap(partial)(x)     return jnp.sum(batch_linear(x)  target_BxM) value, grads = jax.value_and_grad(loss_fn, argnums=0)(w_MxN, x_BxN) ``` This script should print the $2\times 2$ pairwise inner product between the incoming gradients. ``` [[3. 3.]  [3. 3.]] ```","I see. And I guess you're using `delta_inner` for some sort of logging, or does it get used for something else? You can probably hack something like this by combining a batched implementation with `custom_vmap`, but it depends a bit on what you want to do with `delta_inner`...","I plan to use it to modify the gradient of the weight (not shown here for brevity) so it modifies the backpass. I also want to get it out of the grad by passing in a dummy $B \times B$ matrix into the function and returning this matrix as its gradient, but I think this part should be relatively easy (I think one way to think about it is that it's like an additional ""gradient meta data"" for that parameter). Could you elaborate on `custom_vmap`? Another related question is do you foresee this breaking anything if I am using distributed training across 4 TPU nodes with sharding?"
yi,Reconstructing a custom registered dataclass seems to give a segmentation fault," Description When trying to restore a PyTree node I get a segmentation fault, this is the repro: ```python import functools import dataclasses import jax .partial(jax.tree_util.register_dataclass, data_fields=[""a""], meta_fields=[""b""]) .dataclass class Custom:   a: int   b: str c = Custom(1, ""a"") c_leafs, c_tree = jax.tree.flatten(c) c_tree2 = c_tree.make_from_node_data_and_children(         jax.tree_util.default_registry, c_tree.node_data(), c_tree.children()) print(""Success so far..."", flush=True) print(c_tree2) ``` At least head and 0.4.31 are affected (on my CPU machine, linux x86 laptop). It seems to be an issue with xla's optimized implementation of registering dataclasses. This patch reverting to the old registration method seems to fix it: ```  tree_util.py        20240809 16:03:18.346194919 0700 +++ tree_util_fix.py    20240809 16:03:28.926294137 0700 @@ 1017,9 +1017,9 @@      data = tuple(getattr(x, name) for name in data_fields)      return data, meta   default_registry.register_dataclass_node(nodetype, list(data_fields), list(meta_fields))   none_leaf_registry.register_dataclass_node(nodetype, list(data_fields), list(meta_fields))   dispatch_registry.register_dataclass_node(nodetype, list(data_fields), list(meta_fields)) +  default_registry.register_node(nodetype, flatten_func, unflatten_func) +  none_leaf_registry.register_node(nodetype, flatten_func, unflatten_func) +  dispatch_registry.register_node(nodetype, flatten_func, unflatten_func)    _registry[nodetype] = _RegistryEntry(flatten_func, unflatten_func)    _registry_with_keypaths[nodetype] = _RegistryWithKeypathsEntry(        flatten_with_keys, unflatten_func ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.31 jaxlib: 0.4.31 numpy:  1.26.4 python: 3.11.9 (main, Jul 16 2024, 17:04:16) [GCC 13.2.0] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='rdyroglaptop2', release='6.7.121rodete1amd64', version=' CC(Python 3 compatibility issues) SMP PREEMPT_DYNAMIC Debian 6.7.121rodete1 (20240612)', machine='x86_64') ```",2024-08-09T23:12:17Z,bug,closed,0,0,https://github.com/jax-ml/jax/issues/22980
yi,[pallas] Move ops_test.py from jax_triton to jax/pallas,"[pallas] Move ops_test.py from jax_triton to jax/pallas The `jax_triton/ops_test.py` has over time accumulated many tests that are in fact platformindependent tests. Furthermore, those tests were only Googleinternal, and they can be external as well. This moves test coverage for Pallas from the jax_triton package to the Pallas core package. A small number of the tests were deleted, because they were already present in Pallas, e.g., tests in `jax_triton/ops_test.py:ControlFlowTest`, and tests for unary and binary ops in `jax_triton/ops_test.py:OpsTest`. The other tests were distributed to different files in the Pallas repo, according to their purpose:   * tests in `jax_triton/ops_test.py:PrettyPrintingTest` are moved to `tpu_pallas_test.py::PrettyPrintingTest`   * tests in `jax_triton/ops_test.py::IndexingTest` are appended to `indexing_test.py::IndexingTest`; some other indexing tests from `jax_triton/ops_test.py::LoadStoreTest` are also moved there.    * some tests in `jax_triton/ops_test.py:OpsTest` are moved to `ops_test.py::OpsTest`.    * some tests for TPU specific ops in `jax_triton/ops_test.py:OpsTest` are moved to a new test file `tpu_ops_tests.py` Some of this required adding sharding and hypothesis support to `ops_test.py`, and adding TPU versions of `indexing_test.py`.",2024-08-09T06:55:21Z,,closed,0,0,https://github.com/jax-ml/jax/issues/22959
yi,Bump the pip group across 1 directory with 2 updates,"Bumps the pip group with 2 updates in the /build directory: zipp and setuptools. Updates `zipp` from 3.18.1 to 3.19.1  Changelog Sourced from zipp's changelog.  v3.19.1 Bugfixes  Improved handling of malformed zip files. ( CC(example Errata))  v3.19.0 Features  Implement is_symlink. ( CC(add oss test instructions, fix conv grad bug))  v3.18.2 No significant changes.    Commits  6d1cb72 Finalize fd604bd Merge pull request  CC(Can’t import both jax and tensorflow (causes kernel restart)) from jaraco/bugfix/119malformedpaths c18417e Add news fragment. 58115d2 Employ SanitizedNames in CompleteDirs. Fixes broken test. 564fcc1 Add SanitizedNames mixin. 79a309f Add some assertions about malformed paths. 2d015c2 Merge https://github.com/jaraco/skeleton a595a0f Rename extras to align with core metadata spec. 608f90a Finalize 3a22d72 Merge pull request  CC(add tensordot) from jaraco/feature/issymlink Additional commits viewable in compare view    Updates `setuptools` from 69.2.0 to 70.0.0  Changelog Sourced from setuptools's changelog.  v70.0.0 Features  Emit a warning when [tools.setuptools] is present in pyproject.toml and will be ignored.  by :user:SnoopJ ( CC(未找到相关数据)) Improved AttributeError error message if pkg_resources.EntryPoint.require is called without extras or distribution Gracefully &quot;do nothing&quot; when trying to activate a pkg_resources.Distribution with a None location, rather than raising a TypeError  by :user:Avasam ( CC([jax2tf] Add input and output names to the TensorFlow graph generated by jax2tf.)) Typed the dynamically defined variables from pkg_resources  by :user:Avasam ( CC(Pin pygments version in RTD build)) Modernized and refactored VCS handling in package_index. ( CC([jax2tf] Revert '[jax2tf] Replace tf.math.add with tf.raw_ops.AddV2 (…))  Bugfixes  In install command, use super to call the superclass methods. Avoids race conditions when monkeypatching from _distutils_system_mod occurs late. ( CC(Rename collectives into ""collective operations"" for the pmap function.)) Fix finder template for lenient editable installs of implicit nested namespaces constructed by using package_dir to reorganise directory structure. ( CC([jax2tf] Replace tf.math.add with tf.raw_ops.AddV2)) Fix an error with UnicodeDecodeError handling in pkg_resources when trying to read files in UTF8 with a fallback  by :user:Avasam ( CC(Ppermute batching errors))  Improved Documentation  Uses RST substitution to put badges in 1 line. ( CC(improve an escaped tracer error message))  Deprecations and Removals   Further adoption of UTF8 in setuptools. This change regards mostly files produced and consumed during the build process (e.g. metadata files, script wrappers, automatically updated config files, etc..) Although precautions were taken to minimize disruptions, some edge cases might be subject to backwards incompatibility. Support for &quot;locale&quot; encoding is now deprecated. ( CC(Help with array slice indices))   Remove setuptools.convert_path after long deprecation period. This function was never defined by setuptools itself, but rather a sideeffect of an import for internal usage. ( CC(Allow custom_linear_solve to return things besides the solution))   Remove fallback for customisations of distutils' build.sub_command after long deprecated period. Users are advised to import build directly from setuptools.command.build. ( CC(Allow custom_linear_solve to return things besides the solution))   Removed typing_extensions from vendored dependencies  by :user:Avasam ( CC(Hacky approach to carry argument of an invertible transformation in the backward pass))   Remove deprecated setuptools.dep_util. The provided alternative is setuptools.modified. ( CC([jax2tf] Added support for shape polymorphism conversion.))     ... (truncated)   Commits  5cbf12a Workaround for release error in v70 9c1bcc3 Bump version: 69.5.1 → 70.0.0 4dc0c31 Remove deprecated setuptools.dep_util ( CC([jax2tf] Added support for shape polymorphism conversion.)) 6c1ef57 Remove xfail now that test passes. Ref  CC(jnp.moveaxis: fix bug when axes are integer dtype). d14fa01 Add all sitepackages dirs when creating simulated environment for test_edita... 6b7f7a1 Prevent bin folders to be taken as extern packages when vendoring ( CC(Update scale_and_translate to take an explicit spatial_dims.)) 69141f6 Add doctest for vendorised bin folder 2a53cc1 Prevent 'bin' folders to be taken as extern packages 7208628 Replace call to deprecated validate_pyproject command ( CC(Add a prototype implementation of recursive checkpointing)) 96d681a Remove call to deprecated validate_pyproject command Additional commits viewable in compare view    Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore  major version` will close this group update PR and stop Dependabot creating any more for the specific dependency's major version (unless you unignore this specific dependency's major version or upgrade to it yourself)  ` ignore  minor version` will close this group update PR and stop Dependabot creating any more for the specific dependency's minor version (unless you unignore this specific dependency's minor version or upgrade to it yourself)  ` ignore ` will close this group update PR and stop Dependabot creating any more for the specific dependency (unless you unignore this specific dependency or upgrade to it yourself)  ` unignore ` will remove all of the ignore conditions of the specified dependency  ` unignore  ` will remove the ignore condition of the specified dependency and ignore conditions You can disable automated security fix PRs for this repo from the Security Alerts page. ",2024-08-08T19:41:23Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/22950,"This pull request was built based on a group rule. Closing it will not ignore any of these versions in future pull requests. To ignore these dependencies, configure ignore rules in dependabot.yml"
yi,"Extending JAX with CUDA: cstdint: error: the global scope has no ""..."""," Description Hi there, I am trying to extend the capabilities of JAX with my own kernels and I have been following both the official documentation the the ̀dfm/extendingjax` repo. Here is my issue: When I run this (do I even need to have all of these parameters?): ``` nvcc threads 4 Xcompiler Wall ldl exptrelaxedconstexpr O3 DNDEBUG Xcompiler O3 generatecode=arch=compute_70,code=[compute_70,sm_70]  generatecode=arch=compute_75,code=[compute_75,sm_75] generatecode=arch=compute_80,code=[compute_80,sm_80] generatecode=arch=compute_86,code=[compute_86,sm_86] Xcompiler=fPIC Xcompiler=fvisibility=hidden x cu c lib/kernel.cu o build/dia_kernel.cu.o ``` I get the following error: ``` /usr/include/c++/11/cstdint(52): error: the global scope has no ""int_fast8_t"" /usr/include/c++/11/cstdint(53): error: the global scope has no ""int_fast16_t"" /usr/include/c++/11/cstdint(54): error: the global scope has no ""int_fast32_t"" /usr/include/c++/11/cstdint(55): error: the global scope has no ""int_fast64_t"" /usr/include/c++/11/cstdint(57): error: the global scope has no ""int_least8_t"" /usr/include/c++/11/cstdint(58): error: the global scope has no ""int_least16_t"" /usr/include/c++/11/cstdint(59): error: the global scope has no ""int_least32_t"" /usr/include/c++/11/cstdint(60): error: the global scope has no ""int_least64_t"" /usr/include/c++/11/cstdint(62): error: the global scope has no ""intmax_t"" /usr/include/c++/11/cstdint(63): error: the global scope has no ""intptr_t"" /usr/include/c++/11/cstdint(65): error: the global scope has no ""uint8_t"" /usr/include/c++/11/cstdint(66): error: the global scope has no ""uint16_t"" /usr/include/c++/11/cstdint(67): error: the global scope has no ""uint32_t"" /usr/include/c++/11/cstdint(68): error: the global scope has no ""uint64_t"" /usr/include/c++/11/cstdint(70): error: the global scope has no ""uint_fast8_t"" /usr/include/c++/11/cstdint(71): error: the global scope has no ""uint_fast16_t"" /usr/include/c++/11/cstdint(72): error: the global scope has no ""uint_fast32_t"" /usr/include/c++/11/cstdint(73): error: the global scope has no ""uint_fast64_t"" /usr/include/c++/11/cstdint(75): error: the global scope has no ""uint_least8_t"" /usr/include/c++/11/cstdint(76): error: the global scope has no ""uint_least16_t"" /usr/include/c++/11/cstdint(77): error: the global scope has no ""uint_least32_t"" /usr/include/c++/11/cstdint(78): error: the global scope has no ""uint_least64_t"" /usr/include/c++/11/cstdint(80): error: the global scope has no ""uintmax_t"" /usr/include/c++/11/cstdint(81): error: the global scope has no ""uintptr_t"" /usr/include/c++/11/exception(53): error: not a class or struct name /usr/include/c++/11/typeinfo(190): error: not a class or struct name /usr/include/c++/11/typeinfo(207): error: not a class or struct name /usr/include/c++/11/bits/nested_exception.h(99): error: identifier ""true_type"" is undefined /usr/include/c++/11/bits/nested_exception.h(108): error: identifier ""false_type"" is undefined /usr/include/c++/11/bits/nested_exception.h(108): error: function template ""dia_jax::std::__throw_with_nested_impl"" has already been defined /usr/include/c++/11/bits/nested_exception.h(134): error: enable_if is not a template /usr/include/c++/11/bits/nested_exception.h(135): error: identifier ""__and_"" is undefined /usr/include/c++/11/bits/nested_exception.h(135): error: identifier ""is_polymorphic"" is undefined /usr/include/c++/11/bits/nested_exception.h(135): error: type name is not allowed /usr/include/c++/11/bits/nested_exception.h(135): error: expected a "";"" /usr/include/c++/11/bits/stringfwd.h(79): error: basic_string is not a template /usr/include/c++/11/bits/stringfwd.h(83): error: basic_string is not a template /usr/include/c++/11/bits/stringfwd.h(93): error: basic_string is not a template /usr/include/c++/11/bits/stringfwd.h(96): error: basic_string is not a template /usr/include/c++/11/cwchar(64): error: the global scope has no ""mbstate_t"" /usr/include/c++/11/cwchar(141): error: the global scope has no ""wint_t"" /usr/include/c++/11/cwchar(143): error: the global scope has no ""btowc"" /usr/include/c++/11/cwchar(144): error: the global scope has no ""fgetwc"" /usr/include/c++/11/cwchar(145): error: the global scope has no ""fgetws"" /usr/include/c++/11/cwchar(146): error: the global scope has no ""fputwc"" /usr/include/c++/11/cwchar(147): error: the global scope has no ""fputws"" /usr/include/c++/11/cwchar(148): error: the global scope has no ""fwide"" /usr/include/c++/11/cwchar(149): error: the global scope has no ""fwprintf"" /usr/include/c++/11/cwchar(150): error: the global scope has no ""fwscanf"" /usr/include/c++/11/cwchar(151): error: the global scope has no ""getwc"" /usr/include/c++/11/cwchar(152): error: the global scope has no ""getwchar"" /usr/include/c++/11/cwchar(153): error: the global scope has no ""mbrlen"" /usr/include/c++/11/cwchar(154): error: the global scope has no ""mbrtowc"" /usr/include/c++/11/cwchar(155): error: the global scope has no ""mbsinit"" /usr/include/c++/11/cwchar(156): error: the global scope has no ""mbsrtowcs"" /usr/include/c++/11/cwchar(157): error: the global scope has no ""putwc"" /usr/include/c++/11/cwchar(158): error: the global scope has no ""putwchar"" /usr/include/c++/11/cwchar(160): error: the global scope has no ""swprintf"" /usr/include/c++/11/cwchar(162): error: the global scope has no ""swscanf"" /usr/include/c++/11/cwchar(163): error: the global scope has no ""ungetwc"" /usr/include/c++/11/cwchar(164): error: the global scope has no ""vfwprintf"" /usr/include/c++/11/cwchar(166): error: the global scope has no ""vfwscanf"" /usr/include/c++/11/cwchar(169): error: the global scope has no ""vswprintf"" /usr/include/c++/11/cwchar(172): error: the global scope has no ""vswscanf"" /usr/include/c++/11/cwchar(174): error: the global scope has no ""vwprintf"" /usr/include/c++/11/cwchar(176): error: the global scope has no ""vwscanf"" /usr/include/c++/11/cwchar(178): error: the global scope has no ""wcrtomb"" /usr/include/c++/11/cwchar(179): error: the global scope has no ""wcscat"" /usr/include/c++/11/cwchar(180): error: the global scope has no ""wcscmp"" /usr/include/c++/11/cwchar(181): error: the global scope has no ""wcscoll"" /usr/include/c++/11/cwchar(182): error: the global scope has no ""wcscpy"" /usr/include/c++/11/cwchar(183): error: the global scope has no ""wcscspn"" /usr/include/c++/11/cwchar(184): error: the global scope has no ""wcsftime"" /usr/include/c++/11/cwchar(185): error: the global scope has no ""wcslen"" /usr/include/c++/11/cwchar(186): error: the global scope has no ""wcsncat"" /usr/include/c++/11/cwchar(187): error: the global scope has no ""wcsncmp"" /usr/include/c++/11/cwchar(188): error: the global scope has no ""wcsncpy"" /usr/include/c++/11/cwchar(189): error: the global scope has no ""wcsrtombs"" /usr/include/c++/11/cwchar(190): error: the global scope has no ""wcsspn"" /usr/include/c++/11/cwchar(191): error: the global scope has no ""wcstod"" /usr/include/c++/11/cwchar(193): error: the global scope has no ""wcstof"" /usr/include/c++/11/cwchar(195): error: the global scope has no ""wcstok"" /usr/include/c++/11/cwchar(196): error: the global scope has no ""wcstol"" /usr/include/c++/11/cwchar(197): error: the global scope has no ""wcstoul"" /usr/include/c++/11/cwchar(198): error: the global scope has no ""wcsxfrm"" /usr/include/c++/11/cwchar(199): error: the global scope has no ""wctob"" /usr/include/c++/11/cwchar(200): error: the global scope has no ""wmemcmp"" /usr/include/c++/11/cwchar(201): error: the global scope has no ""wmemcpy"" /usr/include/c++/11/cwchar(202): error: the global scope has no ""wmemmove"" /usr/include/c++/11/cwchar(203): error: the global scope has no ""wmemset"" /usr/include/c++/11/cwchar(204): error: the global scope has no ""wprintf"" /usr/include/c++/11/cwchar(205): error: the global scope has no ""wscanf"" /usr/include/c++/11/cwchar(206): error: the global scope has no ""wcschr"" /usr/include/c++/11/cwchar(207): error: the global scope has no ""wcspbrk"" /usr/include/c++/11/cwchar(208): error: the global scope has no ""wcsrchr"" /usr/include/c++/11/cwchar(209): error: the global scope has no ""wcsstr"" /usr/include/c++/11/cwchar(210): error: the global scope has no ""wmemchr"" /usr/include/c++/11/cwchar(251): error: the global scope has no ""wcstold"" /usr/include/c++/11/cwchar(260): error: the global scope has no ""wcstoll"" /usr/include/c++/11/cwchar(261): error: the global scope has no ""wcstoull"" Error limit reached. 100 errors detected in the compilation of ""lib/kernel.cu"". Compilation terminated. ``` Any ideas ?  System info (python version, jaxlib version, accelerator, etc.) ``` import jax  jax.print_environment_info() jax:    0.4.25 jaxlib: 0.4.25 numpy:  1.26.4 python: 3.11.8 (main, Feb 22 2024, 16:59:22) [GCC 12.2.0] jax.devices (1 total, 1 local): [cuda(id=0)] process_count: 1 platform: uname_result(system='Linux', node='morbier', release='6.1.013amd64', version=' CC(Python 3 compatibility issues) SMP PREEMPT_DYNAMIC Debian 6.1.551 (20230929)', machine='x86_64') $ nvidiasmi Thu Aug  8 16:36:35 2024        ++  ++ ```",2024-08-08T14:37:01Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/22946,"We have recently updated the recommended approach for implementing custom calls like this and the new docs are here: https://jax.readthedocs.io/en/latest/ffi.html You'll need to update to the most recent version of JAX to get that working, but how about you give that a try first. If that doesn't work, please try to put together a minimal reproducible example that demonstrates the issue. At a high level this doesn't really seem like a JAX issue since the errors are raised when compiling your library, but I'm happy to try to help if you can isolate the minimal scope problem.","Thank for this information, this is really helpful. I am trying to reproduce the tutorial for my own kernel in CUDA and I have a conversion error. Here is the `.cu` file: ``` namespace ffi = xla::ffi; __global__ void matmul(int64_t N, int64_t diag_number, const float *A, const float *offsets, const float *B, float *C){     int row = blockIdx.y * blockDim.y + threadIdx.y;     int col = blockIdx.x * blockDim.x + threadIdx.x;     if (row (offsets[i]);             bool isOffsetPositive = (offset > 0);             if (isOffsetPositive){                 if (row  std::pair GetDims(const ffi::Buffer &buffer){     auto dims = buffer.dimensions();     if (dims.size() == 0){         return std::make_pair(0, 0);     }     return std::make_pair(buffer.element_count(), dims.back()); } ffi::Error MatmulDIAImpl(     cudaStream_t stream,      int64_t diag_number,     ffi::Buffer A,     ffi::Buffer offsets,     ffi::Buffer B,     ffi::Result> C ){     auto [totalSize, lastDim] = GetDims(A);     const int block_size = 32;     dim3 block(block_size * block_size);     dim3 grid(lastDim/block_size, lastDim/block_size);     matmul>>(lastDim, diag_number, A, offsets, B, C);     return ffi::Error::Success(); } ``` And when I run the command :  ̀cmake build ffi/_build`, I will get the following error: ``` error: no suitable conversion function from ""xla::ffi::Buffer"" to ""const float *"" exists ``` I don't know if this is the best minimal reproductible example but it is pretty minimal in what I am trying to do :)",It looks like you are passing an `ffi::Buffer` to `matmul` directly instead of its `.typed_data()`., — I'm going to close this issue because I think we got to the bottom of it for now. Please open new issues if you run into further problems!
yi,Installing jaxlib with CUDA Version: 11.7," Description I am trying to install CUDAenabled jaxlib on a system with `CUDA Version: 11.7`. From the displayed available versions, the lowes I see is `0.4.17+cuda11.cudnn86`. Is it possible to install jaxlib on a sytem with `CUDA Version: 11.7`?  System info (python version, jaxlib version, accelerator, etc.) I have jaxlib installe, but not a CUDAenabled version. ``` An NVIDIA GPU may be present on this machine, but a CUDAenabled jaxlib is not installed. Falling back to cpu. jax:    0.4.31 jaxlib: 0.4.31 numpy:  1.26.4 python: 3.12.3  NVIDIASMI 515.105.01   Driver Version: 515.105.01   CUDA Version: 11.7    ```",2024-08-08T10:00:30Z,question,closed,0,5,https://github.com/jax-ml/jax/issues/22932,"Looking at the changelog, JAX dropped support for CUDA versions older than 11.8 in v0.4.8. So for CUDA 11.7, you may have luck with JAX v0.4.7. You can find relevant installation instructions here: https://github.com/google/jax/tree/jaxv0.4.7?tab=readmeovfilepipinstallationgpucuda ``` pip install upgrade pip pip install upgrade ""jax[cuda]==0.4.7"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ```","Mmh, I tried a couple of versions, but always see the following message that indicates the lowest possible available version would be `0.4.17+cuda11.cudnn86` `ERROR: Could not find a version that satisfies the requirement jaxlib==0.4.7+cuda11.cudnn86; extra == ""cuda"" (from jax[cuda]) (from versions: 0.4.17, 0.4.17+cuda11.cudnn86, 0.4.17+cuda12.cudnn89, 0.4.18, 0.4.18+cuda11.cudnn86, 0.4.18+cuda12.cudnn89, 0.4.19, 0.4.19+cuda11.cudnn86, 0.4.19+cuda12.cudnn89, 0.4.20, 0.4.20+cuda11.cudnn86, 0.4.20+cuda12.cudnn89, 0.4.21, 0.4.21+cuda11.cudnn86, 0.4.21+cuda12.cudnn89, 0.4.22, 0.4.22+cuda11.cudnn86, 0.4.22+cuda12.cudnn89, 0.4.23, 0.4.23+cuda11.cudnn86, 0.4.23+cuda12.cudnn89, 0.4.24, 0.4.24+cuda11.cudnn86, 0.4.24+cuda12.cudnn89, 0.4.25, 0.4.25+cuda11.cudnn86, 0.4.25+cuda12.cudnn89, 0.4.26, 0.4.26+cuda12.cudnn89, 0.4.27, 0.4.27+cuda12.cudnn89, 0.4.28, 0.4.28+cuda12.cudnn89, 0.4.29, 0.4.29+cuda12.cudnn91, 0.4.30, 0.4.31) ERROR: No matching distribution found for jaxlib==0.4.7+cuda11.cudnn86; extra == ""cuda""`","It looks like you’re using Python 3.12, and Jax 0.4.7 wheels are only available for Python versions 3.8 through 3.11 (Python 3.12 had not yet been released when JAX 0.4.7 was released). You’ll either have to downgrade your Python version, or upgrade your CUDA version.","Thanks, that worked. However, when I run `python c ""import jax; print(f'Jax backend: {jax.default_backend()}')"" ` to check jax finds the CUDA backend, nothing happens and the process is not responding, so I'm forced to kill it. The same happens when I do `import jax; jax.print_environment_info()`.",It's hard to say what's causing that crash – is it possible you have crosstalk between multiple Python environments in your path?
yi,Getting some problems in the usage of  jnp.linalg.inv  ," Description When I use jnp.linalg.inv() to calculate the inverse of the matrix, it raise erros for the arguments Here is my testing code:  import jax.numpy as jnp B = jnp.array([[1,3],[2,4]]) inv_B = jnp.linalg.inv(B) > The raised error are like this: > E0808 15:38:55.241322  985717 pjrt_stream_executor_client.cc:3067] Execution of replica 0 failed: INVALID_ARGUMENT: Wrong number of attributes: expected 1 but got 3 > Traceback (most recent call last): >   File ""/home/huaze/yixian/test_jax.py"", line 8, in  >     inv_B = jnp.linalg.inv(B) >             ^^^^^^^^^^^^^^^^^ > jaxlib.xla_extension.XlaRuntimeError: INVALID_ARGUMENT: Wrong number of attributes: expected 1 but got 3 >  > For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. >   System info (python version, jaxlib version, accelerator, etc.) jax **0.4.30** has this problem, while I use jax **0.4.25** has not",2024-08-08T07:53:28Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/22928,"Thanks for raising this issue. I'm not able to reproduce this error on CPU or GPU with jax v0.4.31. Can you try reinstalling JAX (there could be some unexpected version skew between jax, jaxlib and the various plugins) and if that doesn't work please describe exactly how you installed JAX and we'll see what we can do.","Thank you for your suggestion. I reinstalled JAX, and it resolved the issue. I appreciate your help and the quick response. Your advice was invaluable. Thanks again!",Great! Glad to hear it worked.
rag,Add unit tests for features of FFI.,"Add unit tests for features of FFI. While XLA has tests for the C++ FFI API, and JAX already has high level tests for `ffi_call`, we were missing a mechanism for testing the integration of all of the FFI features into JAX's API. One option, which is what we have been using so far, is to just rely on the FFI calls provided by jaxlib, but those are being added slowly and won't necessarily leverage all of the relevant APIs. Furthermore, this makes the iteration speed on feature development slower. I'm still working through how to wire these tests into our infrastructure appropriately.",2024-08-07T12:56:39Z,,closed,0,0,https://github.com/jax-ml/jax/issues/22915
yi,PRNG Key in shard_map()," Description I'm not sure if I'm misunderstanding the best way to do something like this, but I'm trying to write a parallel optimizer across many GPUs/nodes and I'm having some issues. A random initial distribution is generated on a per device basis and independently optimized. This is a somewhat simplified example of how I'm trying to set up the initial distributions on a single node with 4 GPUs: ``` import jax import jax.numpy as jnp from jax.sharding import Mesh from jax.sharding import PartitionSpec from jax.sharding import NamedSharding from jax.experimental import mesh_utils from jax.experimental.shard_map import shard_map P = jax.sharding.PartitionSpec devices = mesh_utils.create_device_mesh((4,)) mesh = jax.sharding.Mesh(devices, ('x')) sharding = jax.sharding.NamedSharding(mesh, P('x')) rng_pre = jax.random.PRNGKey(0) rng = jax.random.split(rng_pre,4) arr = jnp.arange(4) arr_s = jax.device_put(arr, sharding) rng_s = jax.device_put(rng, sharding) .jit def f(rng, x):     print(""x shape"", x.shape)     print(""rng shape"", rng.shape)     return jax.random.uniform(rng) + x print(""f output"", f(rng_s, arr_s)) ``` From reading through the sharded computation tutorials (https://jax.readthedocs.io/en/latest/shardedcomputation.html), I would expect that creating a sharding across the 4 devices on my machine for both input arrays to a function would implicitly split the parameters along their leading axis. This also appears to be the case when visualizing the sharding of both arrays (both are distributed on all 4 devices). When running the code however, looking inside the elementwise function reveals that this isn't the case: ``` x shape (4,) rng shape (4, 2) ValueError: uniform accepts a single key, but was given a key array of shape (4, 2) != (). Use jax.vmap for batching. ``` This is easily solvable by defining an explicit sharding for the function instead with the third technique mentioned: ``` f_sh = shard_map(f, mesh=mesh, in_specs=P(""x""), out_specs=P(""x"")) print(""f_sh output"", f_sh(rng, arr)) ``` This approach runs into another issue but does imply that the sharding is dividing the computation across the intended axis: ``` x shape (1,) rng shape (1, 2) ValueError: uniform accepts a single key, but was given a key array of shape (1, 2) != (). Use jax.vmap for batching. ``` I have two questions that I'm trying to figure out. First, what is the issue with my attempt to automatically parallelize the function in this case? I would have expected this to raise the elementwise computation to the sharded dataset. Second, am I doing something obviously wrong in handling random numbers this way? It's desirable in this case that each device/optimizer run have a different initial random seed and this seems like the most obvious way to do it. I'm assuming this happens because the shard map isn't axis reducing but I'm not sure if I'm just thinking about this in a way that isn't idiomatic for distributed JAX.  System info (python version, jaxlib version, accelerator, etc.) ``` >>> jax.print_environment_info() jax:    0.4.30 jaxlib: 0.4.30 numpy:  1.26.4 python: 3.10.14  ++ ```",2024-08-04T06:13:34Z,question,closed,1,4,https://github.com/jax-ml/jax/issues/22860,"In the first case, the issue is that even though your input keys are sharded, the function must still semantically operate batchwise over your data. An easy way to automatically vectorize your funciton this way is with `vmap`: ```python .jit .vmap def f(rng, x):     print(""x shape"", x.shape)     print(""rng shape"", rng.shape)     return jax.random.uniform(rng) + x print(""f output"", f(rng_s, arr_s)) ``` ``` x shape () rng shape (2,) f output [0.6216618 1.0862398 2.9888763 3.7490206] ``` In the `shard_map` case, the issue is that `shard_map` passes batches of data to the underlying function, so with the input data you defined you end up with a size1 batch of keys in the function. As the error message says, `uniform` accepts a single key, not a batch of keys. For this particular case, the vmapped version solves the problem as well: ```python .jit .vmap def f(rng, x):     print(""x shape"", x.shape)     print(""rng shape"", rng.shape)     return jax.random.uniform(rng) + x f_sh = shard_map(f, mesh=mesh, in_specs=P(""x""), out_specs=P(""x"")) print(""f_sh output"", f_sh(rng, arr)) ``` ``` x shape () rng shape (2,) f_sh output [0.6216618 1.0862398 2.9888763 3.7490206] ```","This makes more sense. I was under the impression that vmap wasn't necessarily composable with SPMD/multi process applications but if I'm understanding what you're saying, as long as the data is appropriately sharded or a function is annotated with a shard_map or sharding constraints (options 1,2, or 3 in the SPMD tutorial), the computation will still follow the data on each device or node (in a multi process setting) as expected. Thank you so much for the help!","Yes, `vmap` and `shard_map` are composable (though they cannot be used together if you use collectives and are both vmapping and sharding the collective axis). One side note: you'll generally see better performance if you do jitofshardmap, rather than shardmapofjit.",There's a way that doesn't use vmap here: https://github.com/jaxml/jax/discussions/22862discussioncomment10779458
yi,improve while_loop carry pytree/type mismatch errors,"Now we call into the same error utility as we use in scan. ```python import jax import jax.numpy as jnp thing1 = {f'roy{i}': [1.] * i for i in range(20)} thing2 = dict(thing1, roy5=[1.] * 4) jax.lax.while_loop(lambda dct: False, lambda dct: thing2, thing1) ``` Before: ``` TypeError: body_fun output and input must have same type structure, got PyTreeDef({'roy0': [], 'roy1': [*], 'roy10': [*, *, *, *, *, *, *, *, *, *], 'roy11': [*, *, *, *, *, *, *, *, *, *, *], 'roy12': [*, *, *, *, *, *, *, *, *, *, *, *], 'roy13': [*, *, *, *, *, *, *, *, *, *, *, *, *], 'roy14': [*, *, *, *, *, *, *, *, *, *, *, *, *, *], 'roy15': [*, *, *, *, *, *, *, *, *, *, *, *, *, *, *], 'roy16': [*, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *], 'roy17': [*, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *], 'roy18': [*, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *], 'roy19': [*, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *], 'roy2': [*, *], 'roy3': [*, *, *], 'roy4': [*, *, *, *], 'roy5': [*, *, *, *], 'roy6': [*, *, *, *, *, *], 'roy7': [*, *, *, *, *, *, *], 'roy8': [*, *, *, *, *, *, *, *], 'roy9': [*, *, *, *, *, *, *, *, *]}) and PyTreeDef({'roy0': [], 'roy1': [*], 'roy10': [*, *, *, *, *, *, *, *, *, *], 'roy11': [*, *, *, *, *, *, *, *, *, *, *], 'roy12': [*, *, *, *, *, *, *, *, *, *, *, *], 'roy13': [*, *, *, *, *, *, *, *, *, *, *, *, *], 'roy14': [*, *, *, *, *, *, *, *, *, *, *, *, *, *], 'roy15': [*, *, *, *, *, *, *, *, *, *, *, *, *, *, *], 'roy16': [*, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *], 'roy17': [*, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *], 'roy18': [*, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *], 'roy19': [*, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *], 'roy2': [*, *], 'roy3': [*, *, *], 'roy4': [*, *, *, *], 'roy5': [*, *, *, *, *], 'roy6': [*, *, *, *, *, *], 'roy7': [*, *, *, *, *, *, *], 'roy8': [*, *, *, *, *, *, *, *], 'roy9': [*, *, *, *, *, *, *, *, *]}). ``` After: ``` TypeError: while_loop body function carry input and carry output must have the same pytree structure, but they differ: The input carry component dct['roy5'] is a list of length 5 but the corresponding component of the carry output is a list of length 4, so the lengths do not match. Revise the function so that the carry output has the same pytree structure as the carry input. ```",2024-08-03T21:53:27Z,pull ready,closed,1,0,https://github.com/jax-ml/jax/issues/22857
yi,Importing jax.numpy fails when building jaxlib from source," Description `AttributeError: partially initialized module 'jax._src.interpreters.pxla' has no attribute 'identity' (most likely due to a circular import)`  Reproduction: On a graviton3 instance (c7g), using Python 3.10.12 1. Checkout jax to jaxlibv0.4.31. 2. Build jaxlib from source using `python build/build.py` 3. Install jaxlib from the new wheel in `dist/` 4. Install jax using `pip install e .` 5. Run `import jax.numpy as jnp`    Stack Trace  ``` AttributeError                            Traceback (most recent call last) Cell In[6], line 1 > 1 import jax.numpy as jnp File ~/jax/jax/numpy/__init__.py:18       1  Copyright 2018 The JAX Authors.       2        3  Licensed under the Apache License, Version 2.0 (the ""License"");    (...)      15  Note: import  as  is required for names to be exported.      16  See PEP 484 & https://github.com/google/jax/issues/7570 > 18 from jax.numpy import fft as fft      19 from jax.numpy import linalg as linalg      21 from jax._src.basearray import Array as ndarray File ~/jax/jax/numpy/fft.py:18       1  Copyright 2020 The JAX Authors.       2        3  Licensed under the Apache License, Version 2.0 (the ""License"");    (...)      15  Note: import  as  is required for names to be exported.      16  See PEP 484 & https://github.com/google/jax/issues/7570 > 18 from jax._src.numpy.fft import (      19   ifft as ifft,      20   ifft2 as ifft2,      21   ifftn as ifftn,      22   ifftshift as ifftshift,      23   ihfft as ihfft,      24   irfft as irfft,      25   irfft2 as irfft2,      26   irfftn as irfftn,      27   fft as fft,      28   fft2 as fft2,      29   fftfreq as fftfreq,      30   fftn as fftn,      31   fftshift as fftshift,      32   hfft as hfft,      33   rfft as rfft,      34   rfft2 as rfft2,      35   rfftfreq as rfftfreq,      36   rfftn as rfftn,      37 ) File ~/jax/jax/_src/numpy/fft.py:22      19 import numpy as np      21 from jax import dtypes > 22 from jax import lax      23 from jax._src.lib import xla_client      24 from jax._src.util import safe_zip File ~/jax/jax/lax/__init__.py:18       1  Copyright 2019 The JAX Authors.       2        3  Licensed under the Apache License, Version 2.0 (the ""License"");    (...)      15  Note: import  as  is required for names to be exported.      16  See PEP 484 & https://github.com/google/jax/issues/7570 > 18 from jax._src.lax.lax import (      19   DotDimensionNumbers as DotDimensionNumbers,      20   Precision as Precision,      21   PrecisionLike as PrecisionLike,      22   RandomAlgorithm as RandomAlgorithm,      23   RoundingMethod as RoundingMethod,      24   abs as abs,      25   abs_p as abs_p,      26   acos as acos,      27   acos_p as acos_p,      28   acosh as acosh,      29   acosh_p as acosh_p,      30   add as add,      31   add_p as add_p,      32   after_all as after_all,      33   after_all_p as after_all_p,      34   and_p as and_p,      35   argmax as argmax,      36   argmax_p as argmax_p,      37   argmin as argmin,      38   argmin_p as argmin_p,      39   asin as asin,      40   asin_p as asin_p,      41   asinh as asinh,      42   asinh_p as asinh_p,      43   atan as atan,      44   atan_p as atan_p,      45   atan2 as atan2,      46   atan2_p as atan2_p,      47   atanh as atanh,      48   atanh_p as atanh_p,      49   batch_matmul as batch_matmul,      50   bitcast_convert_type as bitcast_convert_type,      51   bitcast_convert_type_p as bitcast_convert_type_p,      52   bitwise_and as bitwise_and,      53   bitwise_not as bitwise_not,      54   bitwise_or as bitwise_or,      55   bitwise_xor as bitwise_xor,      56   broadcast as broadcast,      57   broadcast_in_dim as broadcast_in_dim,      58   broadcast_in_dim_p as broadcast_in_dim_p,      59   broadcast_shapes as broadcast_shapes,      60   broadcast_to_rank as broadcast_to_rank,      61   broadcasted_iota as broadcasted_iota,      62   cbrt as cbrt,      63   cbrt_p as cbrt_p,      64   ceil as ceil,      65   ceil_p as ceil_p,      66   clamp as clamp,      67   clamp_p as clamp_p,      68   clz as clz,      69   clz_p as clz_p,      70   collapse as collapse,      71   complex as complex,      72   complex_p as complex_p,      73   concatenate as concatenate,      74   concatenate_p as concatenate_p,      75   conj as conj,      76   conj_p as conj_p,      77   convert_element_type as convert_element_type,      78   convert_element_type_p as convert_element_type_p,      79   copy_p as copy_p,      80   cos as cos,      81   cos_p as cos_p,      82   cosh as cosh,      83   cosh_p as cosh_p,      84   create_token as create_token,      85   create_token_p as create_token_p,      86   div as div,      87   div_p as div_p,      88   dot as dot,      89   dot_general as dot_general,      90   dot_general_p as dot_general_p,      91   dtype as dtype,      92   eq as eq,      93   eq_p as eq_p,      94   eq_to_p as eq_to_p,      95   exp as exp,      96   exp_p as exp_p,      97   exp2 as exp2,      98   exp2_p as exp2_p,      99   expand_dims as expand_dims,     100   expm1 as expm1,     101   expm1_p as expm1_p,     102   floor as floor,     103   floor_p as floor_p,     104   full as full,     105   full_like as full_like,     106   ge as ge,     107   ge_p as ge_p,     108   gt as gt,     109   gt_p as gt_p,     110   imag as imag,     111   imag_p as imag_p,     112   infeed as infeed,     113   infeed_p as infeed_p,     114   integer_pow as integer_pow,     115   integer_pow_p as integer_pow_p,     116   iota as iota,     117   iota_p as iota_p,     118   is_finite as is_finite,     119   is_finite_p as is_finite_p,     120   le as le,     121   le_p as le_p,     122   le_to_p as le_to_p,     123   log as log,     124   log1p as log1p,     125   log1p_p as log1p_p,     126   log_p as log_p,     127   logistic as logistic,     128   logistic_p as logistic_p,     129   lt as lt,     130   lt_p as lt_p,     131   lt_to_p as lt_to_p,     132   max as max,     133   max_p as max_p,     134   min as min,     135   min_p as min_p,     136   mul as mul,     137   mul_p as mul_p,     138   ne as ne,     139   ne_p as ne_p,     140   neg as neg,     141   neg_p as neg_p,     142   nextafter as nextafter,     143   nextafter_p as nextafter_p,     144   not_p as not_p,     145   or_p as or_p,     146   outfeed as outfeed,     147   outfeed_p as outfeed_p,     148   pad as pad,     149   pad_p as pad_p,     150   padtype_to_pads as padtype_to_pads,     151   population_count as population_count,     152   population_count_p as population_count_p,     153   pow as pow,     154   pow_p as pow_p,     155   ragged_dot as ragged_dot,     156   real as real,     157   real_p as real_p,     158   reciprocal as reciprocal,     159   reduce as reduce,     160   reduce_and_p as reduce_and_p,     161   reduce_max_p as reduce_max_p,     162   reduce_min_p as reduce_min_p,     163   reduce_or_p as reduce_or_p,     164   reduce_p as reduce_p,     165   reduce_precision as reduce_precision,     166   reduce_precision_p as reduce_precision_p,     167   reduce_prod_p as reduce_prod_p,     168   reduce_sum_p as reduce_sum_p,     169   reduce_xor_p as reduce_xor_p,     170   rem as rem,     171   rem_p as rem_p,     172   reshape as reshape,     173   reshape_p as reshape_p,     174   rev as rev,     175   rev_p as rev_p,     176   rng_bit_generator as rng_bit_generator,     177   rng_bit_generator_p as rng_bit_generator_p,     178   rng_uniform as rng_uniform,     179   rng_uniform_p as rng_uniform_p,     180   round as round,     181   round_p as round_p,     182   rsqrt as rsqrt,     183   rsqrt_p as rsqrt_p,     184   select as select,     185   select_n as select_n,     186   select_n_p as select_n_p,     187   shift_left as shift_left,     188   shift_left_p as shift_left_p,     189   shift_right_arithmetic as shift_right_arithmetic,     190   shift_right_arithmetic_p as shift_right_arithmetic_p,     191   shift_right_logical as shift_right_logical,     192   shift_right_logical_p as shift_right_logical_p,     193   sign as sign,     194   sign_p as sign_p,     195   sin as sin,     196   sin_p as sin_p,     197   sinh as sinh,     198   sinh_p as sinh_p,     199   sort as sort,     200   sort_key_val as sort_key_val,     201   sort_p as sort_p,     202   sqrt as sqrt,     203   sqrt_p as sqrt_p,     204   square as square,     205   squeeze as squeeze,     206   squeeze_p as squeeze_p,     207   stop_gradient as stop_gradient,     208   sub as sub,     209   sub_p as sub_p,     210   tan as tan,     211   tan_p as tan_p,     212   tanh as tanh,     213   tanh_p as tanh_p,     214   top_k as top_k,     215   top_k_p as top_k_p,     216   transpose as transpose,     217   transpose_p as transpose_p,     218   xor_p as xor_p,     219   zeros_like_array as zeros_like_array,     220 )     221 from jax._src.lax.special import (     222   bessel_i0e as bessel_i0e,     223   bessel_i0e_p as bessel_i0e_p,    (...)     249   zeta_p as zeta_p,     250 )     251 from jax._src.lax.slicing import (     252   GatherDimensionNumbers as GatherDimensionNumbers,     253   GatherScatterMode as GatherScatterMode,    (...)     280   slice_p as slice_p,     281 ) File ~/jax/jax/_src/lax/lax.py:32      30 import jax      31 from jax import tree_util > 32 from jax.sharding import Sharding      33 from jax.tree_util import tree_map      35 from jax._src import ad_util File ~/jax/jax/sharding.py:30      19 from jax._src.sharding_impls import (      20     XLACompatibleSharding as _deprecated_XLACompatibleSharding,      21     NamedSharding as NamedSharding,    (...)      25     PositionalSharding as PositionalSharding,      26 )      27 from jax._src.partition_spec import (      28     PartitionSpec as PartitionSpec,      29 ) > 30 from jax._src.interpreters.pxla import Mesh as Mesh      32 _deprecations = {      33      Added Jun 4, 2024.      34     ""XLACompatibleSharding"": (    (...)      40     )      41 }      43 import typing File ~/jax/jax/_src/interpreters/pxla.py:39      37 from jax._src import config      38 from jax._src import core > 39 from jax._src import dispatch      40 from jax._src import dtypes      41 from jax._src import effects File ~/jax/jax/_src/dispatch.py:35      33 from jax._src import config      34 from jax._src import core > 35 from jax._src import api      36 from jax._src import dtypes      37 from jax._src import source_info_util File ~/jax/jax/_src/api.py:49      47 from jax._src import dispatch      48 from jax._src import effects > 49 from jax._src import array      50 from jax._src import basearray      51 from jax._src import dtypes File ~/jax/jax/_src/array.py:1015    1013 core.pytype_aval_mappings[ArrayImpl] = abstract_arrays.canonical_concrete_aval    1014 xla.pytype_aval_mappings[ArrayImpl] = op.attrgetter('aval') > 1015 xla.canonicalize_dtype_handlers[ArrayImpl] = pxla.identity    1016 api_util._shaped_abstractify_handlers[ArrayImpl] = op.attrgetter('aval')    1017  TODO(jakevdp) replace this with true inheritance at the C++ level. AttributeError: partially initialized module 'jax._src.interpreters.pxla' has no attribute 'identity' (most likely due to a circular import) ```   Note that running `python benchmarks/math_benchmark.py` and other benchmarks work fine.  System info (python version, jaxlib version, accelerator, etc.) Python 3.10.12 `AttributeError: module 'jax' has no attribute 'print_environment_info'`",2024-08-02T16:42:57Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/22845,The script should not be in the same parent directory of the jax repo of course
yi,Data type precision and demote/promote preference config,"Please:  [x] Check for duplicate requests.      https://github.com/google/jax/issues/19033       https://github.com/google/jax/issues/16705      https://github.com/google/jax/issues/15580    [x] Describe your goal, and if possible provide a code snippet with a motivating example.  TLDR: to benefit the future of AI innovation, we could add config options to enhance JAX support for mixed and reduced precision numbers  I've hit this error message a billion times in getting rolling with `ml_dtypes`. !image _I wish I could set `jax.config` options to make it easier..._  Ideas:  A `default_precise_float_dtype` value set to `f32` (same default dtype as now)  A `default_imprecise_float_dtype` value set to `None` or `bf16`  A `float_precision_mismatch_behavior` enum with `raise`, `auto_demote`, and `auto_promote` variants which defaults to `raise` (same behavior as now)  (Optional): A `default_float_dtype_precision` enum with `precise` and `imprecise` variants, defaulting to `precise` Crucial point: these config options ought to have sensible defaults which yield no change in behavior unless a user changes them. Thus it would be a purely additive change and not negatively impact backward compatiblity. (ditto for integers possible)  Explanation:  We keep the default f32 dtype as the default_precise_float type If the accelerator can support reduced precision like `bf16` or `f8`, then they could specify the desired imprecise dtype as a config option and select `auto_demote` instead of `raise` Then the `auto_demote` behavior can be: if one of the dtypes is reduced precision and the other is higher precision, jax could know to reduce the precision of the more precise float to the `default_imprecise_float_dtype`.  Likewise, if the situation is mostly all clearly specified in the codebase as reduced precision, then a savvy jax user could set `float_precision_mismatch_behavior` to `auto_promote` and jax could know to convert imprecise floats to the more precise type, for use cases like batch norm or bigger number types (and surely I recognize this is default for some dtype pairings)  Minimal reproduction: ```py import jax import ml_dtypes precise_arr = jax.numpy.ones(3) imprecise_arr = jax.numpy.ones(3, dtype=ml_dtypes.float8_e5m2) jax.numpy.array_equal(precise_arr, imprecise_arr) ```  Error copypasta  (i'm sure you've seen this already if you're working with mixed precision) ```sh args = (Tracedwith, Tracedwith)     def promote_dtypes(*args: ArrayLike) > list[Array]:       """"""Convenience function to apply Numpy argument dtype promotion.""""""        TODO(dougalm,mattjj): This is a performance bottleneck. Consider memoizing.       if len(args)        to_dtype, weak_type = dtypes._lattice_result_type(*args) E       jax._src.dtypes.TypePromotionError: Input dtypes ('float8_e5m2', 'float32') have no available implicit dtype promotion path. To avoid unintended promotion, 8bit floats do not support implicit promotion. If you'd like your inputs to be promoted to another type, you can do so explicitly using e.g. x.astype('float32') E        ```  Final Disclaimer  I am no expert on jax config options so there might already be something like this (the `jax.config.update(""jax_enable_x64"", True)` one I sometimes use comes to mind)  There may exist XLA environment variables to achieve similar aims  There might be better ways to pull this off  There might be better names for these config options  There might be better data structures, imho enums are good, clear  I'm not really familiar enough with this codebase to make a PR for this.  I welcome valuable input from the JAX community, WDYT?",2024-08-02T15:38:31Z,enhancement,closed,0,2,https://github.com/jax-ml/jax/issues/22842,"Hi  we've discussed this before, and in general we've found that different users have different opinions when it comes to what the default promotion behavior for float8 should be. Given that, and given that float8 is generally used very deliberately within a very limited range of situations, we've come to the conculsion that it's best for promotion with float8 to be explicit, rather than implicit. What do you think?","f8 will see more use in the future to save memory, so my objective was to prepare to explicitly configure optional sensible defaults to **demote** to more efficient dtypes, not promote Sadly, I can't contribute to Google projects knowing the Gemini API and Alphafold Server customer noncompetes are unsafe, so I'm closing, no offense to you Jake, I know you're a 10x dev and I really admire all the jax team members, just can't contribute to JAX in good conscience knowing this clause are up.  !image my bad for opening this, I was excited because I hadn't noticed that, but that clause makes the set of things one can actually use gemini for without fear of legal risk an empty set, and anything I do to help JAX, just contributes make the ""closed output / customer noncompete"" problem even worse, so I must cease involvement in the project if the most critical documents in AI safety are written by corporate lawyers instead of AI engineers, humanity is truly ****ed here's a better license, not that anyone cares !image"
yi,pallas simple ``pl.program_id()`` example not working," Description Code executed: ```python from functools import partial import jax from jax.experimental import pallas as pl import jax.numpy as jnp import numpy as np def iota_kernel(o_ref):   i = pl.program_id(0)   o_ref[i] = i def iota(size: int):   return pl.pallas_call(iota_kernel,                         out_shape=jax.ShapeDtypeStruct((size,), jnp.int32),                         grid=(size,))() iota(8) ``` Stack Trace: ```  MLIRError                                 Traceback (most recent call last) google3/third_party/py/jax/_src/pallas/mosaic/lowering.py in lower_jaxpr_to_func(ctx, jaxpr, mosaic_grid_mapping, name, for_verification)     681   try: > 682     body.func_op.verify()     683   except Exception as e: MLIRError: Verification failed: error: ""/swap[tree=PyTreeDef((CustomNode(NDIndexer[(PyTreeDef((*,)), (8,), ())], [*]),))]""(callsite(""iota_kernel""("""":3:2) at callsite(""iota""("""":8:9) at callsite(""""("""":11:0) at callsite(""InteractiveShell.run_code""(""third_party/py/IPython/v3_2_3/core/interactiveshell.py"":3066:16) at callsite(""InteractiveShell.run_ast_nodes""(""third_party/py/IPython/v3_2_3/core/interactiveshell.py"":3012:19) at callsite(""InteractiveShell.run_cell""(""third_party/py/IPython/v3_2_3/core/interactiveshell.py"":2901:16) at callsite(""IPythonKernel.do_execute""(""third_party/py/IPython/v3_2_3/kernel/zmq/ipkernel.py"":181:12) at callsite(""Kernel.execute_request""(""third_party/py/IPython/v3_2_3/kernel/zmq/kernelbase.py"":361:24) at callsite(""ColabKernel.execute_request""(""research/colab/notebook/colab_kernel.py"":240:4) at ""Kernel.dispatch_shell""(""third_party/py/IPython/v3_2_3/kernel/zmq/kernelbase.py"":213:16))))))))))): 'vector.shape_cast' op operand CC(未找到相关数据) must be vector of any type values, but got 'i32'  note: ""/swap[tree=PyTreeDef((CustomNode(NDIndexer[(PyTreeDef((*,)), (8,), ())], [*]),))]""(callsite(""iota_kernel""("""":3:2) at callsite(""iota""("""":8:9) at callsite(""""("""":11:0) at callsite(""InteractiveShell.run_code""(""third_party/py/IPython/v3_2_3/core/interactiveshell.py"":3066:16) at callsite(""InteractiveShell.run_ast_nodes""(""third_party/py/IPython/v3_2_3/core/interactiveshell.py"":3012:19) at callsite(""InteractiveShell.run_cell""(""third_party/py/IPython/v3_2_3/core/interactiveshell.py"":2901:16) at callsite(""IPythonKernel.do_execute""(""third_party/py/IPython/v3_2_3/kernel/zmq/ipkernel.py"":181:12) at callsite(""Kernel.execute_request""(""third_party/py/IPython/v3_2_3/kernel/zmq/kernelbase.py"":361:24) at callsite(""ColabKernel.execute_request""(""research/colab/notebook/colab_kernel.py"":240:4) at ""Kernel.dispatch_shell""(""third_party/py/IPython/v3_2_3/kernel/zmq/kernelbase.py"":213:16))))))))))): see current operation: %3 = ""vector.shape_cast""(%arg0) : (i32) > vector The above exception was the direct cause of the following exception: JaxStackTraceBeforeTransformation         Traceback (most recent call last)  in run_filename_from_loader_as_main()  in _run_code_in_main() 31 frames google3/learning/deepmind/dm_python/dm_notebook3_tpu.py in ()      36   else: > 37     app.run(main, flags_parser=PrepareAppAndParseFlags) google3/third_party/py/absl/app.py in run()     483     try: > 484       _run_main(main, args)     485     except UsageError as error: google3/third_party/py/absl/app.py in _run_main()     403   else: > 404     sys.exit(main(argv))     405  google3/learning/deepmind/dm_python/dm_notebook3_tpu.py in main()      25 def main(argv): > 26   return notebook.main(argv)      27  google3/research/colab/notebook/notebook.py in main()     531   if len(sys.argv) > 1 and sys.argv[1] == 'kernel': > 532     return kernel_app.RunForever(kernel_autoload_modules)     533  google3/research/colab/notebook/kernel_app.py in RunForever()     238  > 239   kernelapp_instance.start()     240  google3/third_party/py/IPython/v3_2_3/kernel/zmq/kernelapp.py in start()     372         try: > 373             ioloop.IOLoop.instance().start()     374         except KeyboardInterrupt: google3/third_party/py/tornado/v4_5/ioloop.py in start()     887                         fd_obj, handler_func = self._handlers[fd] > 888                         handler_func(fd_obj, events)     889                     except (OSError, IOError) as e: google3/third_party/py/tornado/v4_5/stack_context.py in null_wrapper()     276                 _state.contexts = cap_contexts[0] > 277                 return fn(*args, **kwargs)     278             finally: google3/third_party/py/zmq/eventloop/zmqstream.py in _handle_events()     577             if zmq_events & zmq.POLLIN and self.receiving(): > 578                 self._handle_recv()     579                 if not self.socket: google3/third_party/py/zmq/eventloop/zmqstream.py in _handle_recv()     606                 callback = self._recv_callback > 607                 self._run_callback(callback, msg)     608  google3/third_party/py/zmq/eventloop/zmqstream.py in _run_callback()     556              inside our blanket exception handler rather than outside. > 557             callback(*args, **kwargs)     558         except Exception: google3/third_party/py/tornado/v4_5/stack_context.py in null_wrapper()     276                 _state.contexts = cap_contexts[0] > 277                 return fn(*args, **kwargs)     278             finally: google3/third_party/py/IPython/v3_2_3/kernel/zmq/kernelbase.py in dispatcher()     251             def dispatcher(msg): > 252                 return self.dispatch_shell(stream, msg)     253             return dispatcher google3/third_party/py/IPython/v3_2_3/kernel/zmq/kernelbase.py in dispatch_shell()     212             try: > 213                 handler(stream, idents, msg)     214             except Exception: google3/research/colab/notebook/colab_kernel.py in execute_request()     239         ) > 240     super().execute_request(stream, ident, parent)     241  google3/third_party/py/IPython/v3_2_3/kernel/zmq/kernelbase.py in execute_request()     360  > 361         reply_content = self.do_execute(code, silent, store_history,     362                                         user_expressions, allow_stdin) google3/third_party/py/IPython/v3_2_3/kernel/zmq/ipkernel.py in do_execute()     180         try: > 181             shell.run_cell(code, store_history=store_history, silent=silent)     182         except: google3/third_party/py/IPython/v3_2_3/core/interactiveshell.py in run_cell()    2900                 interactivity = ""none"" if silent else self.ast_node_interactivity > 2901                 self.run_ast_nodes(code_ast.body, cell_name,    2902                    interactivity=interactivity, compiler=compiler, result=result) google3/third_party/py/IPython/v3_2_3/core/interactiveshell.py in run_ast_nodes()    3011                 code = compiler(mod, cell_name, ""single"") > 3012                 if self.run_code(code, result):    3013                     return True google3/third_party/py/IPython/v3_2_3/core/interactiveshell.py in run_code()    3065                 rprint('Running code', repr(code_obj))  dbg > 3066                 exec(code_obj, self.user_global_ns, self.user_ns)    3067             finally: [](https://colab.corp.google.com/drive/1CWuh1m9k9fwdF3irJa1NGk8ttFKB1B22) in ()      10                         grid=(size,))() > 11 iota(8) [](https://colab.corp.google.com/drive/1CWuh1m9k9fwdF3irJa1NGk8ttFKB1B22) in iota()       7   y = jnp.arange(8) > 8   return pl.pallas_call(iota_kernel,       9                         out_shape=jax.ShapeDtypeStruct((size,), jnp.int32), google3/third_party/py/jax/_src/pallas/pallas_call.py in wrapped()    1105     index_args, rest_args = split_list(flat_args, [grid_mapping.num_index_operands]) > 1106     out_flat = pallas_call_p.bind(    1107         *dynamic_grid_bounds, *index_args, *rest_args, JaxStackTraceBeforeTransformation: jax._src.pallas.mosaic.lowering.LoweringException: Body failed to verify: ""func.func""() >) > (), sym_name = ""main""}> ({ ^bb0(%arg0: i32, %arg1: memref>):   %0 = ""arith.index_cast""(%arg0) : (i32) > index   %1 = ""vector.load""(%arg1, %0) : (memref>, index) > vector   %2 = ""vector.shape_cast""(%1) : (vector) > vector   %3 = ""vector.shape_cast""(%arg0) : (i32) > vector   ""vector.store""(%3, %arg1, %0) : (vector, memref>, index) > ()   ""func.return""() : () > () }) : () > () . This is an internal error. Please report a bug at: https://github.com/google/jax/issues/new?assignees=sharadmv. The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: LoweringException                         Traceback (most recent call last) [](https://colab.corp.google.com/drive/1CWuh1m9k9fwdF3irJa1NGk8ttFKB1B22) in ()       9                         out_shape=jax.ShapeDtypeStruct((size,), jnp.int32),      10                         grid=(size,))() > 11 iota(8) [](https://colab.corp.google.com/drive/1CWuh1m9k9fwdF3irJa1NGk8ttFKB1B22) in iota(size)       6   x = jnp.arange(8)       7   y = jnp.arange(8) > 8   return pl.pallas_call(iota_kernel,       9                         out_shape=jax.ShapeDtypeStruct((size,), jnp.int32),      10                         grid=(size,))() google3/third_party/py/jax/_src/pallas/pallas_call.py in _pallas_call_lowering(ctx, interpret, *in_nodes, **params)     929     )     930  > 931   return mlir.lower_per_platform(ctx, ""pallas_call"",     932                                  dict(cpu=cpu_lowering,     933                                       tpu=tpu_lowering, google3/third_party/py/jax/_src/pallas/pallas_call.py in tpu_lowering(ctx, *in_nodes, **params)     911     if mosaic_tpu_backend is None:     912       raise _unsupported_lowering_error(""tpu"") > 913     return mosaic_tpu_backend.pallas_call_tpu_lowering_rule(     914         ctx, *in_nodes, **params     915     ) google3/third_party/py/jax/_src/pallas/mosaic/pallas_call_registration.py in pallas_call_tpu_lowering_rule(***failed resolving arguments***)     108           dimension_semantics=dimension_semantics, mesh=mesh,     109           for_verification=for_verification) > 110   mosaic_module, extra_args = lower_module(for_verification=False)     111   if debug:     112     print(mosaic_module) google3/third_party/py/jax/_src/pallas/mosaic/pallas_call_registration.py in lower_module(for_verification)     104     with mlir_ctx, ir.Location.unknown(mlir_ctx):     105       dimension_semantics = mosaic_params.get(""dimension_semantics"", None) > 106       return lowering.lower_jaxpr_to_module(     107           ctx, mlir_ctx, grid_mapping, jaxpr,     108           dimension_semantics=dimension_semantics, mesh=mesh, google3/third_party/py/jax/_src/pallas/mosaic/lowering.py in lower_jaxpr_to_module(lowering_context, ctx, grid_mapping, jaxpr, dimension_semantics, mesh, for_verification)     501   m = ir.Module.create()     502   sym_tab = ir.SymbolTable(m.operation) > 503   func_op = lower_jaxpr_to_func(     504       ctx, jaxpr, mosaic_grid_mapping=mosaic_grid_mapping,     505       name=""main"", for_verification=for_verification, google3/third_party/py/jax/_src/pallas/mosaic/lowering.py in lower_jaxpr_to_func(ctx, jaxpr, mosaic_grid_mapping, name, for_verification)     682     body.func_op.verify()     683   except Exception as e: > 684     raise LoweringException(     685         f""Body failed to verify: {body.func_op}.\nThis is an internal error.""     686         "" Please report a bug at:"" LoweringException: Body failed to verify: ""func.func""() >) > (), sym_name = ""main""}> ({ ^bb0(%arg0: i32, %arg1: memref>):   %0 = ""arith.index_cast""(%arg0) : (i32) > index   %1 = ""vector.load""(%arg1, %0) : (memref>, index) > vector   %2 = ""vector.shape_cast""(%1) : (vector) > vector   %3 = ""vector.shape_cast""(%arg0) : (i32) > vector   ""vector.store""(%3, %arg1, %0) : (vector, memref>, index) > ()   ""func.return""() : () > () }) : () > () . This is an internal error. Please report a bug at: https://github.com/google/jax/issues/new?assignees=sharadmv. ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.32 jaxlib: 0.4.32 numpy:  1.26.3 python: 3.11.8 (stable, redacted, redacted) [Clang google3trunk (84658fb82b67fc22ecba1560d0cddd09f9104178)] jax.devices (8 total, 8 local): [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0) TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1) ... TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0) TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)] process_count: 1 platform: uname_result(system='Linux', node='d00ef52addb6d6c9526d6bea6d8.borgtask.google.com', release='5.10.0smp1102.57.0.0', version=' CC(Python 3 compatibility issues) [v5.10.01102.57.0.0] SMP ', machine='x86_64') ```",2024-08-01T13:38:57Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/22817," Explanation of the error The underlying error is the following message: `'vector.shape_cast' op operand CC(未找到相关数据) must be vector of any type values, but got 'i32'` This error is happening because `pl.program_id` returns a scalar which lives in a separate memory space from vectors (SMEM vs VMEM, see https://jax.readthedocs.io/en/latest/pallas/tpu/pipelining.htmltpuanditsmemoryspaces). Because `o_ref` is by default stored in VMEM, the compiler is trying to cast the `program_id` to a vector of length1 on the store operation, but Mosaic's shape cast operation `vector.shape_cast` is only designed to translate vectors to other vectors, not scalars to vectors. Ideally, Pallas ops should work gracefully regardless of whether the inputs are in SMEM/VMEM, but we don't have this implemented yet for all cases. We're also working on improving the error messages since these are quite difficult to parse currently and requires underlying knowledge of the Mosaic compiler.  Temporary Solutions There's a few ways you can work around this while waiting for an upstream fix. One solution is to place `o_ref` into SMEM as follows: ```python import jax from jax.experimental import pallas as pl from jax.experimental.pallas import tpu as pltpu import jax.numpy as jnp def iota_kernel(o_ref):   i = pl.program_id(0)   o_ref[i] = i def iota(size: int):   grid_spec = pltpu.PrefetchScalarGridSpec(             num_scalar_prefetch=0,             in_specs=[],             out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM),             grid=(size,)         )   return pl.pallas_call(iota_kernel,                         out_shape=jax.ShapeDtypeStruct((size,), jnp.int32),                         grid_spec=grid_spec)() iota(8) ``` You could also do the more awkward method of reshaping `o_ref` to (size, 1) and using a reshape. By explicitly reshaping `program_id` to a vector, this avoids having the store operation implicitly attempt the shape cast. ```python import jax from jax.experimental import pallas as pl import jax.numpy as jnp def iota_kernel(o_ref):   i = pl.program_id(0)   o_ref[i, :] = jnp.reshape(i, (1,)) def iota(size: int):   return pl.pallas_call(iota_kernel,                         out_shape=jax.ShapeDtypeStruct((size, 1), jnp.int32),                         grid=(size,), debug=True)() iota(8) ```",Thanks for the detailed comment  ! The explicit separation of SMEM seems nice for now. 
llm,SVD returns NaNs on a specific matrix but singular values may be computed (in JAX x64)," Description I use JAX's least squarez on many matrices I generate using some data. For all of them it is fine, except for one that returns NaNs. I have identified it is because the SVD fails, but the behavior is very surprising. Here is code to reproduce behavior, in which SVD fails, but singular value computations work I am attaching the specific matrix (as this is only time this happened out of hundreds of times) K_mat.npy.zip ```python import jax jax.config.update(""jax_enable_x64"", True) import jax.numpy as np K=np.load('K_mat.npy') print(     f""singular values not full matrices:{np.linalg.svd(K, full_matrices=False)[1]}"" ) print(     f""singular values compute uv false:{np.linalg.svd(K, compute_uv=False)}"" ) print(f""singular values full:{np.linalg.svd(K)[1]}"") ``` The output is  ``` singular values not full matrices:[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan .........   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan   nan nan nan nan nan nan nan nan nan nan]] singular values compute uv false:[[1.77636679e+03 5.86804028e+02 4.52113084e+02 3.47016320e+02   3.27916486e+02 2.88326633e+02 2.79452389e+02 2.64957839e+02   2.49505597e+02 2.39655682e+02 2.16787141e+02 2.06448680e+02   2.01588783e+02 1.98790750e+02 1.90476976e+02 1.89436389e+02   1.74878205e+02 1.67960666e+02 1.60346373e+02 1.56708675e+02   1.48685379e+02 1.43526663e+02 1.42038369e+02 1.36650560e+02   1.32831164e+02 1.26578000e+02 1.24067651e+02 1.22712841e+02   1.20575691e+02 1.15885351e+02 1.14298104e+02 1.08010003e+02   1.05989552e+02 1.04931340e+02 1.00606987e+02 9.81974821e+01   9.59902344e+01 9.41146066e+01 9.25373791e+01 8.88305596e+01   8.81519865e+01 8.75127180e+01 8.43495944e+01 8.23256340e+01   8.18829602e+01 8.08863536e+01 7.95487076e+01 7.85424800e+01   7.62397668e+01 7.31017178e+01 7.21903462e+01 7.16869933e+01   7.11341066e+01 6.84242997e+01 6.75223914e+01 6.64060017e+01   6.46809895e+01 6.42261756e+01 6.38468981e+01 6.15434835e+01   6.07184329e+01 5.84578061e+01 5.80368967e+01 5.70453336e+01   5.63152461e+01 5.56556690e+01 5.42673972e+01 5.33343327e+01   5.29308746e+01 5.21549972e+01 5.06259177e+01 4.98282291e+01   4.80953175e+01 4.76511391e+01 4.70111465e+01 4.64342843e+01   4.53122321e+01 4.42772297e+01 4.34423933e+01 4.26767886e+01   4.22644585e+01 4.14006432e+01 4.08855971e+01 4.05253527e+01   3.95273190e+01 3.85703014e+01 3.71255119e+01 3.68242359e+01   3.58700245e+01 3.55326354e+01 3.49463452e+01 3.46252549e+01   3.45144129e+01 3.36778366e+01 3.31220535e+01 3.27189507e+01   3.21282171e+01 3.17358328e+01 3.14448377e+01 3.10394389e+01   3.06737080e+01 2.99731284e+01 2.96530828e+01 2.86542123e+01   2.81791817e+01 2.77947846e+01 2.77196336e+01 2.68543771e+01   2.64670505e+01 2.58945667e+01 2.55076644e+01 2.52232276e+01   2.49026102e+01 2.45158622e+01 2.41283268e+01 2.37901942e+01   2.33190009e+01 2.27870939e+01 2.26364387e+01 2.21493721e+01   2.19293579e+01 2.17222421e+01 2.13012038e+01 2.09036480e+01   2.06434984e+01 2.05119683e+01 1.99803531e+01 1.93384667e+01   1.88141776e+01 1.85303185e+01 1.83399190e+01 1.80510505e+01   1.77461694e+01 1.73116334e+01 1.71376356e+01 1.67759950e+01   1.67100361e+01 1.63904285e+01 1.59975755e+01 1.56052556e+01   1.53877821e+01 1.53251807e+01 1.48501905e+01 1.46766440e+01   1.44519138e+01 1.37571656e+01 1.36804920e+01 1.35328510e+01   1.32680584e+01 1.27761474e+01 1.26013179e+01 1.22859637e+01   1.21266063e+01 1.17112422e+01 1.14375695e+01 1.10680215e+01   1.06873378e+01 1.06197370e+01 1.04174277e+01 1.02258492e+01   9.88907725e+00 9.82759084e+00 9.58713683e+00 9.29649819e+00   9.12915555e+00 8.76692588e+00 8.57611210e+00 8.34564131e+00   7.96542655e+00 7.78880395e+00 7.72094414e+00 7.53439047e+00   7.36342747e+00 6.98668449e+00 6.84716770e+00 6.76329943e+00   6.50033477e+00 6.27439996e+00 6.01691834e+00 5.87764286e+00   5.47188981e+00 5.20476732e+00 5.09501298e+00 4.79884372e+00   4.48872063e+00 4.18850720e+00 4.10030038e+00 3.84031641e+00   3.69980939e+00 3.59992874e+00 3.39488753e+00 3.18220174e+00   3.07418020e+00 3.00148303e+00 2.68682131e+00 2.50224673e+00   2.31985701e+00 2.20799312e+00 2.17530954e+00 2.03674788e+00   1.99235535e+00 1.73485213e+00 1.59249174e+00 1.55611220e+00   1.43569831e+00 1.32429179e+00 1.25912164e+00 1.16951772e+00   1.10911659e+00 1.03069487e+00 9.59890155e01 8.85633623e01   7.88792645e01 7.49541914e01 7.13003876e01 6.26281745e01   6.21165634e01 5.99808628e01 5.27640861e01 4.72742393e01   4.23622886e01 4.02383871e01 3.92730569e01 3.58188010e01   3.09882004e01 2.60503103e01 2.38032750e01 2.20850669e01   1.26257564e01 3.86180869e02 1.49413809e02 1.28070693e12   8.01205677e13 5.24945057e13 4.63600002e13 4.35355604e13 .........   2.08086828e17 6.45220618e31 3.63127219e31 3.97173855e32]] singular values full:[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan ......   nan nan nan nan nan nan nan nan nan nan]] ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.28 jaxlib: 0.4.28 numpy:  1.26.4 python: 3.12.3  (main, Apr 15 2024, 18:35:20) [Clang 16.0.6 ] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Darwin', node='MacBookProdeTheo2.local', release='23.4.0', version='Darwin Kernel Version 23.4.0: Fri Mar 15 00:12:37 PDT 2024; root:xnu10063.101.17~1/RELEASE_ARM64_T6031', machine='arm64') ```",2024-08-01T02:18:08Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/22809,It looks like some of the singular values for K are very close to zero: ``` max_singular_value = 1776.366793229257 min _singular_value = 3.971738548313294e32 ``` SVD with `compute_uv=False` and `compute_uv=True` have different implementations so the underlying numerics are slightly different. It could be a good idea to add a small identity regularizer to the matrix to improve the numerical stability of the solver: e.g. `K += np.eye(N) * np.finfo('float64').eps` After adding the regularizer I no longer see the `nan` values you are getting.,I see that is interesting. Thank you for finding the reason and fixing it. I didn't know high condition number could lead to failure in SVD computations. Should the svd code be modified? Otherwise I'm happy to close the issue,"JAX directly calls lapack on CPU (in this case, `dgesdd` for doubleprecision SVD), so the implementation is unlikely to be changed.",Indeed. Thanks for the help! 
yi,checkify: false positive in jnp.where,"```python import jax.numpy as jnp import jax.experimental.checkify as checkify x = jnp.arange(2) err, _ = checkify.checkify(jnp.where, errors=checkify.index_checks)(x > 0.5, size=1) err.throw() ``` ```pytb Traceback (most recent call last):   File ""/Users/vanderplas/github/google/jax/tmp.py"", line 5, in      err, _ = checkify.checkify(jnp.where, errors=checkify.index_checks)(x > 0.5, size=1)   File ""/Users/vanderplas/github/google/jax/jax/_src/numpy/lax_numpy.py"", line 2116, in where     return nonzero(condition, size=size, fill_value=fill_value)   File ""/Users/vanderplas/github/google/jax/jax/_src/numpy/lax_numpy.py"", line 2607, in nonzero     bincount(reductions.cumsum(mask), length=calculated_size))   File ""/Users/vanderplas/github/google/jax/jax/_src/numpy/lax_numpy.py"", line 2229, in bincount     return zeros(length, _dtype(weights)).at[clip(x, 0)].add(weights)   File ""/Users/vanderplas/github/google/jax/jax/_src/numpy/array_methods.py"", line 537, in add     return scatter._scatter_update(self.array, self.index, values,   File ""/Users/vanderplas/github/google/jax/jax/_src/ops/scatter.py"", line 76, in _scatter_update     return _scatter_impl(x, y, scatter_op, treedef, static_idx, dynamic_idx,   File ""/Users/vanderplas/github/google/jax/jax/_src/ops/scatter.py"", line 127, in _scatter_impl     out = scatter_op( jax._src.checkify.OOBError: outofbounds indexing for array of shape (1,): index 1 is out of bounds for axis 0 with size 1.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/Users/vanderplas/github/google/jax/tmp.py"", line 6, in      err.throw()   File ""/Users/vanderplas/github/google/jax/jax/_src/checkify.py"", line 245, in throw     _check_error(self)   File ""/Users/vanderplas/github/google/jax/jax/_src/checkify.py"", line 1194, in _check_error     return check_p.bind(*err_args, err_tree=tree_def, debug=debug)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Users/vanderplas/github/google/jax/jax/_src/core.py"", line 429, in bind     return self.bind_with_trace(find_top_trace(args), args, params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Users/vanderplas/github/google/jax/jax/_src/core.py"", line 433, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Users/vanderplas/github/google/jax/jax/_src/core.py"", line 939, in process_primitive     return primitive.impl(*tracers, **params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Users/vanderplas/github/google/jax/jax/_src/checkify.py"", line 494, in check_impl     raise JaxRuntimeError(str(exc)) from exc jax._src.checkify.JaxRuntimeError: outofbounds indexing for array of shape (1,): index 1 is out of bounds for axis 0 with size 1.  ```",2024-07-31T23:11:52Z,bug,open,0,3,https://github.com/jax-ml/jax/issues/22806,"I think the approach to fixing this is: 1. make checkify only error when `mode=PROMISE_IN_BOUNDS` 2. ensure that `gather` and `scatter` default to `mode=PROMISE_IN_BOUNDS` 3. explicitly set `mode=DROP` or `mode=FILL` in `jnp.where` implementation, as well as other library code, to avoid checkify errors (2) is the only thing here that I think would be controversial: `gather` already defaults to `PROMISE_IN_BOUNDS`, but `scatter` defaults to `FILL_OR_DROP`, and changing this behavior might change the default behavior in some implementations.","On discussion with , it seems that defaulting to `PROMISE_IN_BOUNDS` is not the right approach, because it has problematic semantics (particularly for autodiff). Given this, our plan now is to add a new `GatherScatterMode.DEFAULT` mode that is semantically equivalent to `FILL_OR_DROP`, except that `checkify` will raise for outofbound indices in DEFAULT mode.",I think I'll leave this until checkify has stabilized more.
rag,Implement initial vmap over pallas_call w/ ragged inputs (via jumbles),"Implement initial vmap over pallas_call w/ ragged inputs (via jumbles) The plan here is to load it up with invariants, and start with a really simple kernel. After that, we can slowly relax the various invariants and implement support for others. Note  the work saving here is compute only, not memory yet. A fastfollowup CL is adding memory savings via indexmap rewriting",2024-07-31T21:41:52Z,,closed,0,0,https://github.com/jax-ml/jax/issues/22799
rag,pip install jax[tpu] fails with libtpu inconsistent version error," Description As of today the pip install command for jax[tpu] fails: ``` > pip install jax[tpu]==0.4.31 f https://storage.googleapis.com/jaxreleases/libtpu_releases.html Looking in links: https://storage.googleapis.com/jaxreleases/libtpu_releases.html Collecting jax==0.4.31 (from jax[tpu]==0.4.31)   Using cached jax0.4.31py3noneany.whl.metadata (22 kB) Collecting jaxlib=0.4.30 (from jax==0.4.31>jax[tpu]==0.4.31)   Using cached jaxlib0.4.31cp310cp310manylinux2014_x86_64.whl.metadata (983 bytes) ... Collecting libtpunightly==0.1.dev20240729 (from jax[tpu]==0.4.31)   Using cached https://storage.googleapis.com/libtpunightlyreleases/wheels/libtpunightly/libtpu_nightly0.1.dev20240729%2Bnightlypy3noneany.whl (120.1 MB) Discarding https://storage.googleapis.com/libtpunightlyreleases/wheels/libtpunightly/libtpu_nightly0.1.dev20240729%2Bnightlypy3noneany.whl (from https://storage.googleapis.com/jaxreleases/libtpu_releases.html): Requested libtpunightly==0.1.dev20240729 from https://storage.googleapis.com/libtpunightlyreleases/wheels/libtpunightly/libtpu_nightly0.1.dev20240729%2Bnightlypy3noneany.whl (from jax==0.4.31) has inconsistent version: expected '0.1.dev20240729+nightly', but metadata has '0.1.dev20240729+default' ``` Even yesterday I was having no issues with it. The problem seems to be with the distribution of the libtpunightly specifically, I can reproduce the error by: ``` > pip install libtpunightly==0.1.dev20240729 f https://storage.googleapis.com/jaxreleases/libtpu_releases.html Looking in links: https://storage.googleapis.com/jaxreleases/libtpu_releases.html Collecting libtpunightly==0.1.dev20240729   Using cached https://storage.googleapis.com/libtpunightlyreleases/wheels/libtpunightly/libtpu_nightly0.1.dev20240729%2Bnightlypy3noneany.whl (120.1 MB) Discarding https://storage.googleapis.com/libtpunightlyreleases/wheels/libtpunightly/libtpu_nightly0.1.dev20240729%2Bnightlypy3noneany.whl (from https://storage.googleapis.com/jaxreleases/libtpu_releases.html): Requested libtpunightly==0.1.dev20240729 from https://storage.googleapis.com/libtpunightlyreleases/wheels/libtpunightly/libtpu_nightly0.1.dev20240729%2Bnightlypy3noneany.whl has inconsistent version: expected '0.1.dev20240729+nightly', but metadata has '0.1.dev20240729+default' ERROR: Ignored the following yanked versions: 0.0.1 ERROR: Could not find a version that satisfies the requirement libtpunightly==0.1.dev20240729 (from versions: 0.1.dev20210615+nightly, 0.1.dev20210709+nightly, 0.1.dev20210809+nightly, 0.1.dev20210916+nightly, 0.1.dev20210917+nightly, 0.1.dev20210920+nightly, 0.1.dev20210921+nightly, 0.1.dev20210922+nightly,  ... 0.1.dev20240726+nightly, 0.1.dev20240727+nightly, 0.1.dev20240728+nightly, 0.1.dev20240729+nightly, 0.1.dev20240730+nightly, 0.1.dev20240731+nightly) ERROR: No matching distribution found for libtpunightly==0.1.dev20240729 ``` The key issue seems to be this: **Requested libtpunightly==0.1.dev20240729 from https://storage.googleapis.com/libtpunightlyreleases/wheels/libtpunightly/libtpu_nightly0.1.dev20240729%2Bnightlypy3noneany.whl has inconsistent version: expected '0.1.dev20240729+nightly', but metadata has '0.1.dev20240729+default**  System info (python version, jaxlib version, accelerator, etc.)  Gcloud TPU Vm  Python 3.10.13  pip 24.2",2024-07-31T17:27:45Z,bug,closed,2,3,https://github.com/jax-ml/jax/issues/22793,"Here is a workaround: ```   pip install libtpunightly==0.1.dev20240731 f https://storage.googleapis.com/jaxreleases/libtpu_releases.html   pip install jaxlib[tpu]==0.4.31 f https://storage.googleapis.com/jaxreleases/libtpu_releases.html nodeps   pip install jax==0.4.31 f https://storage.googleapis.com/jaxreleases/libtpu_releases.html nodeps ``` it works because `libtpunightly==0.1.dev20240731`  the newest published libtpu  installs without errors, whereas the older versions fail.",Thanks for the report! We're aware of the issue and working on rollingout the fix.,"It should be fixed now. Thanks again for the report, and please let us know if there are any further issues!"
yi,"[Pallas] When mixing basic indexing and integer array indexing, the axis corresponding to integer array indexing is unnecessarily moved to the front"," Description I am testing in interpret mode. Repro: ```python import functools import jax import jax.numpy as jnp from jax.experimental import pallas as pl import numpy as np x_shape = (16, 3) x = jnp.arange(np.prod(x_shape), dtype=jnp.float32).reshape(x_shape) a = jnp.array([1, 1, 1, 1, 1], dtype=jnp.int32) y = x[::4, a] .partial(     pl.pallas_call,     out_shape=jax.ShapeDtypeStruct(y.shape, jnp.float32),     interpret=True, ) def kernel(x_ref, o_ref):     o_ref[...] = x_ref[::4, a] y_ = kernel(x) np.testing.assert_array_equal(y_, y) ``` Expected behavior: The line `y_ = kernel(x)` should run successfully, and yield the same value as `y`. Actual behavior: ``` Traceback (most recent call last):   File ""/home/ayx/development/jax/test.py"", line 23, in      y_ = kernel(x)          ^^^^^^^^^   File ""/home/ayx/development/jax/jax/_src/pallas/pallas_call.py"", line 1085, in wrapped     grid_mapping, jaxpr, consts = _trace_kernel_to_jaxpr(                                   ^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/development/jax/jax/_src/pallas/pallas_call.py"", line 857, in _trace_kernel_to_jaxpr     jaxpr, _, consts, () = pe.trace_to_jaxpr_dynamic(wrapped_kernel_fun,                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/development/jax/test.py"", line 21, in kernel     o_ref[...] = x_ref[::4, a]     ~~~~~^^^^^   File ""/home/ayx/development/jax/jax/_src/numpy/array_methods.py"", line 747, in op     return getattr(self.aval, f""_{name}"")(self, *args)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/development/jax/jax/_src/state/types.py"", line 187, in _setitem     return ref_set(tracer, idx, value)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/development/jax/jax/_src/state/primitives.py"", line 124, in ref_set     ref_swap(ref_or_view, idx, value, _function_name=""ref_set"")   File ""/home/ayx/development/jax/jax/_src/state/primitives.py"", line 120, in ref_swap     return swap_p.bind(ref, value, *flat_indexers, tree=tree)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/development/jax/jax/_src/state/primitives.py"", line 188, in _swap_abstract_eval     raise ValueError(""Invalid shape for `swap`. "" ValueError: Invalid shape for `swap`. Ref shape: (4, 5). Expected shape: (4, 5). Value shape: (5, 4). Indices: (NDIndexer(indices=(Slice(start=0, size=4, stride=1), Slice(start=0, size=5, stride=1)), shape=(4, 5), int_indexer_shape=(), validate=False),).   For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. ``` Explanation: The correct shape of the resulting array should be (4, 5), but in Pallas, the shape is incorrectly assumed to be (5, 4), thus resulting the error. I have tested various indexing and observed a pattern that when there is only 1 integer array indexing, the axis corresponding to it is always unnecessarily moved to the front. For example, in the above case, the axis with shape 5 is moved to the front, making Pallas to assume the shape to be (5, 4) instead of (4, 5). This may have to do with https://github.com/google/jax/blob/5c9bb612a775ca23d311eef1aeac03dfe0828a62/jax/_src/state/indexing.pyL256L257.  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.31.dev20240729+6a7822a73 jaxlib: 0.4.30 numpy:  1.26.4 python: 3.12.4 (main, Jun 12 2024, 19:06:53) [GCC 13.2.0] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='ayx1', release='6.6.152rodete2amd64', version=' CC(Python 3 compatibility issues) SMP PREEMPT_DYNAMIC Debian 6.6.152rodete2 (20240319)', machine='x86_64') ```",2024-07-31T11:47:00Z,bug,open,0,3,https://github.com/jax-ml/jax/issues/22783,"Better repro (without strided indexing): ```python import functools import jax import jax.numpy as jnp from jax.experimental import pallas as pl import numpy as np x_shape = (16, 3) x = jnp.arange(np.prod(x_shape), dtype=jnp.float32).reshape(x_shape) a = jnp.array([1, 1, 1, 1, 1], dtype=jnp.int32) y = x[:, a] .partial(     pl.pallas_call,     out_shape=jax.ShapeDtypeStruct(y.shape, jnp.float32),     interpret=True, ) def kernel(x_ref, o_ref):     o_ref[...] = x_ref[:, a] y_ = kernel(x) np.testing.assert_array_equal(y_, y) ``` Error: ``` Traceback (most recent call last):   File ""/home/ayx/development/jax/4.py"", line 23, in      y_ = kernel(x)          ^^^^^^^^^   File ""/home/ayx/development/jax/jax/_src/pallas/pallas_call.py"", line 1129, in wrapped     jaxpr = _trace_kernel_to_jaxpr(             ^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/development/jax/jax/_src/pallas/pallas_call.py"", line 901, in _trace_kernel_to_jaxpr     jaxpr, _, consts, () = pe.trace_to_jaxpr_dynamic(wrapped_kernel_fun,                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/development/jax/4.py"", line 21, in kernel     o_ref[...] = x_ref[:, a]     ~~~~~^^^^^   File ""/home/ayx/development/jax/jax/_src/numpy/array_methods.py"", line 749, in op     return getattr(self.aval, f""_{name}"")(self, *args)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/development/jax/jax/_src/state/types.py"", line 187, in _setitem     return ref_set(tracer, idx, value)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/development/jax/jax/_src/state/primitives.py"", line 114, in ref_set     ref_swap(ref_or_view, idx, value, _function_name=""ref_set"")   File ""/home/ayx/development/jax/jax/_src/state/primitives.py"", line 110, in ref_swap     return swap_p.bind(ref, value, *flat_indexers, tree=tree)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ayx/development/jax/jax/_src/state/primitives.py"", line 178, in _swap_abstract_eval     raise ValueError(""Invalid shape for `swap`. "" ValueError: Invalid shape for `swap`. Ref shape: (16, 5). Expected shape: (16, 5). Value shape: (5, 16). Indices: (NDIndexer(indices=(Slice(start=0, size=16, stride=1), Slice(start=0, size=5, stride=1)), shape=(16, 5), int_indexer_shape=(), validate=False),).   For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. ```","Is there a workaround for this issue, e.g. using `pallas.dslice` or `dynamic_slice` ?  ","> Is there a workaround for this issue, e.g. using `pallas.dslice` or `dynamic_slice` ? https://github.com/jaxml/jax/pull/23758 already works. It's just that the CI is not happy."
yi,infinite loop when trying to install jax with cuda support on windows," Description   Downloading jax0.4.12.tar.gz (1.3 MB)       1.3/1.3 MB 13.2 MB/s eta 0:00:00   Installing build dependencies ... done   Getting requirements to build wheel ... done   Preparing metadata (pyproject.toml) ... done   Downloading jax0.4.11.tar.gz (1.3 MB)       1.3/1.3 MB 13.1 MB/s eta 0:00:00   Installing build dependencies ... done   Getting requirements to build wheel ... done   Preparing metadata (pyproject.toml) ... done   Downloading jax0.4.10.tar.gz (1.3 MB)       1.3/1.3 MB 16.1 MB/s eta 0:00:00   Installing build dependencies ... done   Getting requirements to build wheel ... done   Preparing metadata (pyproject.toml) ... done   Downloading jax0.4.9.tar.gz (1.3 MB)       1.3/1.3 MB 21.5 MB/s eta 0:00:00   Installing build dependencies ... done   Getting requirements to build wheel ... done   Preparing metadata (pyproject.toml) ... done   Downloading jax0.4.8.tar.gz (1.2 MB)       1.2/1.2 MB 15.5 MB/s eta 0:00:00 infinite loop... when running pip install U r dreamerv3/requirements.txt f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html same with python m pip install jax[cuda]  System info (python version, jaxlib version, accelerator, etc.) python 3.10.11",2024-07-31T06:28:30Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/22776,"in particular, it works fine on wsl and linux, only windows has this problem..","Hi  As per the documentation, JAX cuda support is not available on Windows.   Please have a look at the documentation: https://jax.readthedocs.io/en/latest/installation.html It is also mentioned in the documentation that [`pip install upgrade ""jax[cuda12]""` do not work with Windows, and may fail silently](https://jax.readthedocs.io/en/latest/installation.htmlsupportedplatforms:~:text=These%20pip%20installations%20do%20not%20work%20with%20Windows%2C%20and%20may%20fail%20silently%3B%20refer%20to%20the%20table%20above.). Thank you.", what is the typical protocol for doing RL for windowsonly games then?," unfortunately, I have no idea. But I think it's best we close the issue since, as  pointed out, Windows + GPU is officially unsupported. (There may be some community projects though.)"
yi,"Hi all,","jaxlib.xla_extension.XlaRuntimeError Hi, all Currently I am running my program, I have a function called update, and I get error as follows: ``` Traceback (most recent call last):   File ""./hydrogenfiniteThfnew/src/main.py"", line 720, in      = update(params_flow, params_van, params_wfn, state_idx, mo_coeff, bands, s, x, keys, data_acc, grads_acc, flow_score_acc, van_score_acc, wfn_score_acc, flow_fisher_acc, van_fisher_acc, wfn_fisher_acc, wfn_score_mean_acc, final_step, mix_fisher, opt_state_wfn_e, opt_state_flow_p,opt_state_van) jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 3743729664 bytes. ``` It is clear that my gpu is out of memory, and I want to check which lines in function ""update"" caused this, however, I can even print the last line of function ""update"", the fig is in attached files.   Now I am confused about it, and I do not know which line caused this error, could you please give me some advice? Many thanks! _Originally posted by  in https://github.com/google/jax/discussions/22754_",2024-07-30T13:24:09Z,,closed,0,0,https://github.com/jax-ml/jax/issues/22755
yi,Bump setuptools from 69.2.0 to 72.1.0,"Bumps setuptools from 69.2.0 to 72.1.0.  Changelog Sourced from setuptools's changelog.  v72.1.0 Features  Restore the tests command and deprecate access to the module. ( CC(Internal change)) ( CC(Passing Haiku Module through custom_vjp lose transform))  v72.0.0 Deprecations and Removals  The test command has been removed. Users relying on 'setup.py test' will need to migrate to another test runner or pin setuptools before this version. ( CC(Question about stax batch dimension))  v71.1.0 Features   Added return types to typed public functions  by :user:Avasam Marked pkg_resources as py.typed  by :user:Avasam ( CC(Compile error on updating to JAX0.2.0))   Misc   CC(未找到相关数据)  v71.0.4 Bugfixes  Removed lingering unused code around Distribution._patched_dist. ( CC(test fixes in jax_jit_test))  v71.0.3 Bugfixes   ... (truncated)   Commits  441799f Bump version: 72.0.0 → 72.1.0 59aff44 Merge pull request  CC([jax2tf] Add paths that do not use XLA in conv_general_dilated.) from pypa/feature/gracefuldroptests c437aaa Restore the tests command and deprecate access to the module. a6726b9 Add celery and requests to the packages that test integration. Ref  CC(Passing Haiku Module through custom_vjp lose transform) 5e1b3c4 Bump version: 71.1.0 → 72.0.0 4c0b9f3 Merge pull request  CC(Update XLA.) from pypa/debt/removetestcommand be8e3a0 Merge pull request  CC(Update XLA.) from pypa/docs/4483installcoreextra 99d2c72 Add documentation clarifying how to reliably install setuptools with its depe... 63c89f9 👹 Feed the hobgoblins (delint). c405ac1 Merge branch 'main' into debt/removetestcommand Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-07-29T17:21:15Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/22732,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
yi,Bump numpy from 1.26.4 to 2.0.1,"Bumps numpy from 1.26.4 to 2.0.1.  Release notes Sourced from numpy's releases.  v2.0.1 NumPy 2.0.1 Release Notes NumPy 2.0.1 is a maintenance release that fixes bugs and regressions discovered after the 2.0.0 release. NumPy 2.0.1 is the last planned release in the 2.0.x series, 2.1.0rc1 should be out shortly. The Python versions supported by this release are 3.93.12. NOTE:  Do not use the GitHub generated &quot;Source code&quot; files listed in the &quot;Assets&quot;, they are garbage. Improvements np.quantile with method closest_observation chooses nearest even order statistic This changes the definition of nearest for border cases from the nearest odd order statistic to nearest even order statistic. The numpy implementation now matches other reference implementations. (gh26656) Contributors A total of 15 people contributed to this release. People with a &quot;+&quot; by their names contributed a patch for the first time.  @​vahidmech + Alex Herbert + Charles Harris Giovanni Del Monte + Leo Singer Lysandros Nikolaou Matti Picus Nathan Goldbaum Patrick J. Roddy + Raghuveer Devulapalli Ralf Gommers Rostan Tabet + Sebastian Berg Tyler Reddy Yannik Wicke +  Pull requests merged A total of 24 pull requests were merged for this release.   CC(未找到相关数据): MAINT: prepare 2.0.x for further development  CC(未找到相关数据): TYP: fix incorrect import in ma/extras.pyi stub  CC(未找到相关数据): DOC: Mention '1.25' legacy printing mode in set_printoptions  CC(未找到相关数据): DOC: Remove mention of NaN and NAN aliases from constants    ... (truncated)   Commits  4c9f431 Merge pull request  CC(未找到相关数据) from charris/prepare2.0.1 0e70e00 REL: Prepare for the NumPy 2.0.1 release [wheel build] 4d10ffc Merge pull request  CC(未找到相关数据) from charris/backport26985 764b667 BUG: Add object cast to avoid warning with limited API 9be6ad6 Merge pull request  CC(未找到相关数据) from charris/backport26935 6d950e9 BUG: fix f2py tests to work with v2 API 89630c0 Merge pull request  CC(未找到相关数据) from charris/backport26919 88fa840 TST: Apply test suggestion by Nathan for rlstrip fixes a9da01e BUG,MAINT: Fix utf8 character stripping memory access 6afbbf8 Merge pull request  CC(未找到相关数据) from charris/backport26930 Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-07-29T17:21:02Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/22731,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
yi,Equivalent of torch's retain_graph,"Hi all ! I've been having a blast learning JAX recently. However, coming from torch there is one type of operation that is very easy to do in torch but I can't wrap my head around how to do it in JAX's functional paradigm.  I basically want to differentiate twice through the same forward pass. More specifically, I want to be able to reuse computation of a forward pass for two distinct backward passes (and not take second order derivatives). This requirement is a dealbreaker for my research where the forward pass takes a very long time (I would have to go back to torch which I really don't want). With torch's computation graph and autograd's retain_graph this is very easy to do.  One motivation is for Truncated Backpropagation Through Time style scenarios where we might enlarge the forward pass' computation graph slightly at every step but we do not want to recompute the whole window. In torch the computation graph allows to organize the code in such a way that the two backward passes can be in completely different locations (just passing around the outputs). Here is a very simplified version of what I mean in torch. EDIT: In the simple example below and in the case of TBPTT this could be achieved by applying the chain rule by hand. But I'm wondering if there is an automatic way to do it for arbitrarily complex functions (multioutput, multiinput). This would be much less error prone and maybe more efficient (?) ``` python import torch  Define the forward function def forward(x):     return torch.sin(x) * torch.cos(x)  Compute the forward pass and store the result x = torch.ones(5, requires_grad=True) y = forward(x)  Take the derivative with respect to the input grad1 = torch.autograd.grad(outputs=y, inputs=x, grad_outputs=torch.ones_like(y), create_graph=True)[0] print(""Forward output:"", y) print(""Gradient 1:"", grad1)  Perform some modification on the output modified_y = y * 2  Take the derivative again with respect to the input using the modified output grad2 = torch.autograd.grad(outputs=modified_y, inputs=x, grad_outputs=torch.ones_like(modified_y))[0] print(""Modified forward output:"", modified_y) print(""Gradient 2:"", grad2) ``` I understand that this goes against the whole philosophy of the framework as I understand it because it would assume we maintain side effects.  I have two questions: 1. Is there in the API a function transform to wrap around the forward pass that would return something like a tuple of (the output, reusable computation representation) where the second output would be used like a _virtual_ function call (same output, same dependencies but not ran explicitly) ? 2. Is it simply doable in a different way that is less obvious for someone coming from torch ? Thank you very much in advance !",2024-07-29T15:34:27Z,question,closed,0,3,https://github.com/jax-ml/jax/issues/22725,https://jax.readthedocs.io/en/latest/_autosummary/jax.linearize.html,"> Is there in the API a function transform to wrap around the forward pass that would return something like a tuple of (the output, reusable computation representation) where the second output would be used like a virtual function call (same output, same dependencies but not ran explicitly) ? Thanks for the question! I think perhaps you just want `jax.vjp`: ```python import jax import jax.numpy as jnp  Define the forward function def forward(x):     return jnp.sin(x) * jnp.cos(x)  Compute the forward pass and store the result x = jnp.ones(5) y, f_vjp = jax.vjp(forward, x)  Take the derivative with respect to the input grad_outputs = jnp.ones_like(y) grad1, = f_vjp(grad_outputs) print(""Forward output:"", y) print(""Gradient 1:"", grad1)  Apply another function to the output modified_y, g_vjp = jax.vjp(lambda y: y * 2, y)  Evaluate the derivative of the function composition g . forward grad2, = f_vjp(g_vjp(jnp.ones_like(modified_y))[0]) print(""Modified forward output:"", modified_y) print(""Gradient 2:"", grad2) ``` Just think of the `vjp` functions as pulling back gradients from output space to input space. In other words, they're ""backwardpass functions"". For more, see the autodiff cookbook if you haven't already.  What do you think?",Sorry for the late reply. This lead me to rethink completely (or think for the first time maybe) how autodiff worked. As a follow up of this issue I would like to point to CC(未找到相关数据). Thank you for your help !
agent,FileNotFoundError when using cache in parallel," Description I've got a pytest test suite and I've recently started running it with `n 3`, using `pytestxdist`, so it'll run on 3 processes in parallel. I sometimes get a warning like this one: ``` test_polina/test_golden_runs.py::test_ipd_three_agent_golden_run   C:\Program Files\Python312\Lib\sitepackages\jax\_src\compiler.py:688: UserWarning: Error writing persistent compilation cache entry for 'jit_scan_outer_bigs': FileNotFoundError: [WinError 2] The system cannot find the file specified: 'J:\\jaxxy\\_temp_jit_scan_outer_bigs43cfe1699669be4ee80585eba899a893fea7eb948b6e707293e0235d7fdbffe0' > 'J:\\jaxxy\\jit_scan_outer_bigs43cfe1699669be4ee80585eba899a893fea7eb948b6e707293e0235d7fdbffe0'     warnings.warn(  Docs: https://docs.pytest.org/en/stable/howto/capturewarnings.html ``` When it does happen, the tests run a lot slower. I'm guessing sometimes the tests attempt to manipulate the cache at the same time and clash with each other, causing the cache to not be used and triggering compilation. I can't reliably reproduce this problem.  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.30 jaxlib: 0.4.30 numpy:  1.26.4 python: 3.12.1 (tags/v3.12.1:2305ca5, Dec  7 2023, 22:03:25) [MSC v.1937 64 bit (AMD64)] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Windows', node='Turing', release='10', version='10.0.19045', machine='AMD64') $ nvidiasmi Mon Jul 29 15:48:53 2024 ++  ++ ```",2024-07-29T12:52:58Z,bug,open,0,6,https://github.com/jax-ml/jax/issues/22718,Can you try jax 0.4.31 which is just released?,"I upgraded to `0.4.31` and now I'm getting different warnings: ```   C:\Users\Administrator\.venvs\polina_env\Lib\sitepackages\jax\_src\compiler.py:663: UserWarning: Error reading persistent compilation cache entry for 'jit_run_evaluation_rollout': PermissionError: [Errno 13] Permission denied: 'J:\\jaxxy\\jit_run_evaluation_rolloutae9e0a0f7ccd64b22e7749a155df1b10cd93374b9af8ff3154d62baafe614222atime'     warnings.warn( test_polina/test_dumb_on_dumb.py::test_dumb_on_dumb   C:\Users\Administrator\.venvs\polina_env\Lib\sitepackages\jax\_src\compiler.py:663: UserWarning: Error reading persistent compilation cache entry for 'jit_evaluate_population_vs_fixed_policies': PermissionError: [Errno 13] Permission denied: 'J:\\jaxxy\\jit_evaluate_population_vs_fixed_policiesf84b39d478dddc89b532239150355f5d699747470f65562e377b663be8e93bccatime'     warnings.warn( ``` There were about 10 more, and all the filenames end with `atime`, none end with `cache`. I enabled tracebacks and got this: ``` polina\cling.py:70: in train     for population, evaluation, outer_analytica_rows, outer_fuzzy_rows in iterator: polina\pola_algorithm.py:149: in train     yield population, self.env.evaluate(seed, population, i_epoch=0), (), () polina\enving\evaluating_enving.py:98: in evaluate     step_aux = self.run_evaluation_rollout(seed, population) ..\..\..\.venvs\polina_env\Lib\sitepackages\jax\_src\pjit.py:332: in cache_miss     outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked = _python_pjit_helper( ..\..\..\.venvs\polina_env\Lib\sitepackages\jax\_src\pjit.py:190: in _python_pjit_helper     out_flat = pjit_p.bind(*args_flat, **p.params) ..\..\..\.venvs\polina_env\Lib\sitepackages\jax\_src\core.py:2739: in bind     return self.bind_with_trace(top_trace, args, params) ..\..\..\.venvs\polina_env\Lib\sitepackages\jax\_src\core.py:433: in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params) ..\..\..\.venvs\polina_env\Lib\sitepackages\jax\_src\core.py:939: in process_primitive     return primitive.impl(*tracers, **params) ..\..\..\.venvs\polina_env\Lib\sitepackages\jax\_src\pjit.py:1730: in _pjit_call_impl     return xc._xla.pjit( ..\..\..\.venvs\polina_env\Lib\sitepackages\jax\_src\pjit.py:1712: in call_impl_cache_miss     out_flat, compiled = _pjit_call_impl_python( ..\..\..\.venvs\polina_env\Lib\sitepackages\jax\_src\pjit.py:1642: in _pjit_call_impl_python     ).compile(compile_options) ..\..\..\.venvs\polina_env\Lib\sitepackages\jax\_src\interpreters\pxla.py:2295: in compile     executable = UnloadedMeshExecutable.from_hlo( ..\..\..\.venvs\polina_env\Lib\sitepackages\jax\_src\interpreters\pxla.py:2807: in from_hlo     xla_executable = _cached_compilation( ..\..\..\.venvs\polina_env\Lib\sitepackages\jax\_src\interpreters\pxla.py:2621: in _cached_compilation     xla_executable = compiler.compile_or_get_cached( ..\..\..\.venvs\polina_env\Lib\sitepackages\jax\_src\compiler.py:345: in compile_or_get_cached     retrieved_executable, retrieved_compile_time = _cache_read( ..\..\..\.venvs\polina_env\Lib\sitepackages\jax\_src\compiler.py:658: in _cache_read     return compilation_cache.get_executable_and_time( ..\..\..\.venvs\polina_env\Lib\sitepackages\jax\_src\compilation_cache.py:215: in get_executable_and_time     executable_and_time = cache.get(cache_key) ..\..\..\.venvs\polina_env\Lib\sitepackages\jax\_src\lru_cache.py:112: in get     atime_path.write_bytes(timestamp) ..\..\..\.venvs\polina_env\Lib\sitepackages\etils\epath\abstract_path.py:187: in write_bytes     with self.open('wb') as f: ..\..\..\.venvs\polina_env\Lib\sitepackages\etils\epath\gpath.py:255: in open     gfile = self._backend.open(self._path_str, mode) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self =  path = 'J:\\jaxxy\\jit_run_evaluation_rolloutff90452405b81e57fa45c6153d6b90b6fd7afae25904fd1330a052f35df7f531atime' mode = 'wb'     def open(         self,         path: PathLike,         mode: str,     ) > typing.IO[Union[str, bytes]]:       if 'b' in mode:         encoding = None       else:         encoding = 'utf8' >     return open(path, mode, encoding=encoding) E     PermissionError: [Errno 13] Permission denied: 'J:\\jaxxy\\jit_run_evaluation_rolloutff90452405b81e57fa45c6153d6b90b6fd7afae25904fd1330a052f35df7f531atime' ```","Hi, I hit on a similar problem and I think I found the cause and solution. I think the key thing is that the temporary file the cache writes to in normal filesystems does not have a perprocess unique name: https://github.com/google/jax/blob/fed7efd73003988282744b2f649df493b808c781/jax/_src/gfile_cache.pyL37L55 All processes will attempt to open the same `tmp_path = self._path / f""_temp_{key}""`, and any but the first will error. A possible fix is suffixing the temporary file with the hostname of the machine and process id. I include the hostname, since the usecase that brought me here is reusing the same cache for processes in possibly the same node but different MPI groups, but also processes in different nodes. In which case it is also important to differentiate based on hostname.  In NFS, a more complex solution and possibly unnecessary operation is doing NFS locking like described  here (see D) , or done in flufl.lock.", The file you mentioned has been removed in recent version of JAX. Can you try again with the latest version?," I see, I mistakenly navigated to old code. Here’s the code in version 0.4.32, which works for a single machine if you have eviction enabled, but I’m not sure will work for NFS ( since SoftFileLock does not seem to follow all the steps described in the link above regarding NFS; compare with flufl.lock implementation) https://github.com/google/jax/blob/1594d2f30fdbfebf693aba4a2b264e4a3e52acc6/jax/_src/lru_cache.pyL120L160","Numba had to deal with the same issue. Instead of making a key that incorporates hostname/pid, they just open a temp file with a name built from a uuid: https://github.com/numba/numba/blob/301ba23116864ef66a2caccfab7c7123e7bcbcfc/numba/core/caching.pyL599L616"
yi,"jax.jit(function).lower(x, y).cost_analysis() gives NONE while running with GPU as the device"," Description I am training a model Owl_Vit. While trying to train it with GPU, at one stage, it gives None as output for `jax.jit(function).lower(x, y).cost_analysis()`. The thing is that when I use cpu version of the jax, it is working. I get this output: `{'bytes accessed0{}': 4058464512.0, 'utilization0{}': 2059.1572265625, 'bytes accessedout{}': 4285124864.0, 'bytes accessed2{}': 13349412.0, 'utilization2{}': 21.0, 'utilization1{}': 1139.0, 'bytes accessed1{}': 2781101056.0, 'flops': 237087424512.0, 'transcendentals': 111938816.0, 'utilization4{}': 1.0, 'bytes accessed': 11138055168.0, 'bytes accessed3{}': 4.0, 'utilization3{}': 2.0}` So there is no problem with the code. It's just the version of jax that is giving error I think. for version 0.4.23, and 0.4.30 it is giving same error. I tried this specific version because it gives error `jax.random has no attribute PRNGKeyArray`. While using version <0.4.23, it does not give this error. But for both these version, for GPU it is not working. I installed jax with `pip install U ""jax[cuda12]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` and `pip install U ""jax[cuda12]==0.4.23"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html`. I am using conda environment. I tried with CUDA=12.1 to CUDA 12.5, but it is giving same error. In my base OS I have CUDA=12.4. Can anyone tell me what can be the issue here. Thank you.  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.30 jaxlib: 0.4.30 numpy:  1.26.4 python: 3.10.0  ++ ```",2024-07-29T11:53:41Z,bug,open,0,4,https://github.com/jax-ml/jax/issues/22713,"v0.4.23 is fairly old; I'd suggest trying with a more recent JAX version, particularly if you are using more recent CUDA versions. > I tried this specific version because it gives error `jax.random has no attribute PRNGKeyArray` `jax.random.PRNGKeyArray` was deprecated in JAX v0.4.16 and removed in JAX v0.4.24 (see the Change log). You can replace it with `jax.Array` to address this error, and then hopefully use a more recent JAX version.","I changed the ""jax.random.PRNGKeyArray"" with ""jax.Array"", which solved the error of `jax.random has no attribute PRNGKeyArray.`  > I'd suggest trying with a more recent JAX version I tried with v0.4.31, but it is still giving NONE as output for `jax.jit(function).lower(x, y).cost_analysis()`.  UPDATE: I tried with all the versions from 0.4.23 to 0.4.30. Same results. Can there be any problem with cuda configuration?  This is my cuda configuration in conda environment: ``` cuda                      12.4.0                        0    nvidia cudacccl                 12.5.39                       0    nvidia cudacccl_linux64        12.5.39                       0    nvidia cudacommandlinetools   12.5.1                        0    nvidia cudacompiler             12.5.1                        0    nvidia cudacudart               12.5.82                       0    nvidia cudacudartdev           12.5.82                       0    nvidia cudacudartdev_linux64  12.5.82                       0    nvidia cudacudartstatic        12.5.82                       0    nvidia cudacudartstatic_linux64 12.5.82                       0    nvidia cudacudart_linux64      12.5.82                       0    nvidia cudacuobjdump            12.5.39                       0    nvidia cudacupti                12.5.82                       0    nvidia cudacuptidev            12.5.82                       0    nvidia cudacuxxfilt             12.5.82                       0    nvidia cudademosuite           12.4.127                      0    nvidia cudadriverdev           12.5.82                       0    nvidia cudadriverdev_linux64  12.5.82                       0    nvidia cudagdb                  12.5.82                       0    nvidia cudalibraries            12.5.1                        0    nvidia cudalibrariesdev        12.5.1                        0    nvidia cudansight               12.5.82                       0    nvidia cudanvcc                 12.4.131                      0    nvidia cudanvdisasm             12.5.39                       0    nvidia cudanvmldev             12.5.82                       0    nvidia cudanvprof               12.5.82                       0    nvidia cudanvprune              12.5.82                       0    nvidia cudanvrtc                12.5.82                       0    nvidia cudanvrtcdev            12.5.82                       0    nvidia cudanvtx                 12.5.82                       0    nvidia cudanvvp                 12.5.82                       0    nvidia cudaopencl               12.5.39                       0    nvidia cudaopencldev           12.5.39                       0    nvidia cudaprofilerapi         12.5.39                       0    nvidia cudaruntime              12.5.1                        0    nvidia cudasanitizerapi        12.5.81                       0    nvidia cudatoolkit              12.5.1                        0    nvidia cudatools                12.5.1                        0    nvidia cudaversion              12.5                          3    nvidia cudavisualtools         12.5.1                        0    nvidia jaxcuda12pjrt           0.4.31                   pypi_0    pypi jaxcuda12plugin         0.4.31                   pypi_0    pypi nvidiacudacupticu12    12.3.101                 pypi_0    pypi nvidiacudanvcccu12     12.3.107                 pypi_0    pypi nvidiacudanvrtccu12    12.3.107                 pypi_0    pypi nvidiacudaruntimecu12  12.3.101                 pypi_0    pypi ```"," you may try to run `jax.jit(function).lower(x, y).compile().cost_analysis()` to get the cost analysis on GPU. For example: ```python import jax import jax.numpy as jnp .jit def f(x):     return jnp.sin(jnp.cos(x)) c = f.lower(3.) print(""cost_analysis:"", c.compile().cost_analysis())  cost_analysis: [{'transcendentals': 2.0, 'utilization0{}': 1.0, 'bytes accessed0{}': 4.0, 'bytes accessedout{}': 4.0, 'bytes accessed': 8.0}] ```",Im also using jax:    0.4.30 jaxlib: 0.4.30 As suggested by 5 . I changed `analysis = jax.jit(flax_model_apply_fn).lower(*dummy_input).cost_analysis() ` to `analysis = jax.jit(flax_model_apply_fn).lower(*dummy_input).compile().cost_analysis()[0]` that fixed the problem for me. 
gemma,Setting XLA flags causes segmentation fault," Description As per the docs, these flags can help improve the performance on a A100 machine. I am running a jax program written in Equinox on a A100 machine with 2 GPUs, and setting any of these flags cause a segfault. Idk if these flags alone can accelerate the performance, but right now my JAX distributed training is extremely slow compared to torch (which is extremely surprising for me). Couple of reasons that I can think of: 1. torch using tf32 > not sure how to do this in jax 2. torch using flash attention: there is no equivalent of flash attention but I guess these two flags `xla_gpu_enable_triton_softmax_fusion=true xla_gpu_triton_gemm_any=True ` can help bridge the gap 3. torch is using fused adam > not sure optax uses fused implementation depending on the available hardware  System info (python version, jaxlib version, accelerator, etc.) ``` GPU: A100 40G  CC(Explicit tuples are not valid function parameters in Python 3) jax==0.4.30 jaxcuda12pjrt==0.4.30 jaxcuda12plugin==0.4.30 jaxlib==0.4.30 ```",2024-07-29T06:32:24Z,bug,closed,0,8,https://github.com/jax-ml/jax/issues/22705,I would recommend filing an issue about the segfault in openxla/xla. It would really help if you could include a reproducer for the XLA team. Re 2: Could you use `jax.nn.dot_product_attention`? ,"Thanks I can do that. re 2: Nice, but I don't think this is available in 0.4.30. I have the same version installed on my mac, and it isn't there",Can you try the nightly? https://jax.readthedocs.io/en/latest/installation.htmljaxnightlyinstallation,Thanks got it. I will close this and will open an issue for the flags in the XLA repo," Did you file that bug with XLA? I encountered this issue with simple repro steps. I am using a DGX system and this is running on an H100: ```python import os os.environ['XLA_FLAGS'] = 'xla_gpu_enable_triton_softmax_fusion=true' import jax.numpy as jnp data = jnp.array([10, 82, 79, 87, 83, 13, 10], dtype=jnp.uint8) jnp.where(data == 10) ``` The above snippet of code reliably crashes every time, and commenting out the softmax env flag causes the issue to go away.","No, I didn't find any solution. Re flags: Not only that flag, I think most flags are not tested properly. All the flags that are supposed to provide gains on GPU haven't provided any gains on A100 at least. ","I reopened the XLA bug. The crash is certainly something that shouldn't happen, although it would be reasonable for the XLA folks not to prioritize a nondefault configuration. The GPU perf tuning document is supposed to suggest things that may help if tweaked, although there's no guarantee that anything will help, or indeed work, for any given workload. The only truly supported configuration is the default one. Perhaps we should mark those flags as more explicitly experimental? For flash attention, I recommend you use the flash attention API in the most recent JAX, as Sergei suggests. The other two optimizations (TF32 and fused adam) should be happening by default, assuming your training is inside a `jit` for the fused adam.","Thanks  for the quick response. >Perhaps we should mark those flags as more explicitly experimental? Agreed, that will help clarify things for the end user"
yi,Remove custom partitioning pointer from pre-compiled HLO,"When hashing the precompiled IR during computation of the JAX compilation cache key, function that use custom_partitioning produce a different hash every time even for the same underlying computation. This is because of the backend_config pointer changing across runs. To make the compilation cache work for functions that implement custom_partitioning, this PR includes a flag, which when set will set that pointer to be a constant. ",2024-07-29T00:14:22Z,pull ready,closed,0,4,https://github.com/jax-ml/jax/issues/22702, sevcik ,We may want to consider just putting a counter on the MlirContext. That way we can make stable uuids.,> We may want to consider just putting a counter on the MlirContext. That way we can make stable uuids.  What would be the goal of that counter?, I think this is ready to be merged?
yi,[shape_poly] Export jax.typing.DimSize and jax.typing.Shape.,"Some users want to have more precise typing annotations for JAX dimensions sizes and shapes, even in presence of shape polymorphism. At the moment the `DimSize` is defined as `int | Any`. Defining it more precisely is a more involved affair that would involve specifying the symbolic dimensions duck type integers. This is future work.",2024-07-25T13:10:44Z,,closed,0,2,https://github.com/jax-ml/jax/issues/22658, I am wondering if I should actually go through the work to define `DimSize` more precisely and making it work internally before opening this up.,">  I am wondering if I should actually go through the work to define `DimSize` more precisely and making it work internally before opening this up. Yes, I think so. Currently `DimSize` is a more complicated way to spell `Any`. In general, tightening the definition of pubic type aliases is a *very* painful change, because there's no way to do a soft deprecation period for static types. Given that, we should avoid exporting a type name until we're happy with it's definition."
yi,JAX implementation of `scipy.integrate.cumulative_trapezoid`,"The  `scipy.integrate.cumulative_trapezoid`functionis a useful func. Unfortunately, there is no equivalent implementation in jax. Here is an example (adapted  from scipy): ``` x = jnp.linspace(2, 2, num=20) y = x y_int = jax.scipy.integrate.cumulative_trapezoid(y, x, initial=0) plt.plot(x, y_int, 'ro', x, y[0] + 0.5 * x**2, 'b') plt.show() ```",2024-07-25T09:10:55Z,enhancement,closed,0,2,https://github.com/jax-ml/jax/issues/22651,Hi  thanks for the request! We decided a while ago that `scipy.integrate` is outofscope for the JAX package; you can see the discussion and reasoning behind this at https://jax.readthedocs.io/en/latest/jep/18137numpyscipyscope.htmlscipyintegrate,This is implemented in `quadax` : https://quadax.readthedocs.io/en/latest/_api/quadax.cumulative_trapezoid.htmlquadax.cumulative_trapezoid
yi,ROCm build fails on Fedora 40," Description I'm trying to build jaxlib0.4.30 from source on Fedora 40 using ROCm 6.0.2 that comes in their standard repositories. Fedora dumps all ROCm libraries/headers directly into `/usr`, and these seem to be found correctly. However, the build fails because the ROCm device libraries are not found, which I suspect is the stuff installed in `/usr/lib/clang/17/amdgcn/bitcode`: ```bash rpm ql rocmdevicelibs /usr/lib/clang/17/amdgcn /usr/lib/clang/17/amdgcn/bitcode /usr/lib/clang/17/amdgcn/bitcode/asanrtl.bc /usr/lib/clang/17/amdgcn/bitcode/hip.bc /usr/lib/clang/17/amdgcn/bitcode/ockl.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_abi_version_400.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_abi_version_500.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_correctly_rounded_sqrt_off.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_correctly_rounded_sqrt_on.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_daz_opt_off.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_daz_opt_on.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_finite_only_off.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_finite_only_on.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_1010.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_1011.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_1012.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_1013.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_1030.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_1031.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_1032.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_1033.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_1034.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_1035.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_1036.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_1100.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_1101.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_1102.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_1103.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_1150.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_1151.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_600.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_601.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_602.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_700.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_701.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_702.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_703.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_704.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_705.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_801.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_802.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_803.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_805.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_810.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_900.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_902.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_904.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_906.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_908.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_909.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_90a.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_90c.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_940.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_941.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_isa_version_942.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_unsafe_math_off.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_unsafe_math_on.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_wavefrontsize64_off.bc /usr/lib/clang/17/amdgcn/bitcode/oclc_wavefrontsize64_on.bc /usr/lib/clang/17/amdgcn/bitcode/ocml.bc /usr/lib/clang/17/amdgcn/bitcode/opencl.bc /usr/lib64/cmake/AMDDeviceLibs /usr/lib64/cmake/AMDDeviceLibs/AMDDeviceLibsConfig.cmake /usr/share/doc/rocmdevicelibs /usr/share/doc/rocmdevicelibs/OCKL.md /usr/share/doc/rocmdevicelibs/OCML.md /usr/share/doc/rocmdevicelibs/README.md /usr/share/licenses/rocmdevicelibs /usr/share/licenses/rocmdevicelibs/LICENSE.TXT ``` Running ```bash python3 ./build/build.py enable_rocm rocm_amdgpu_targets=gfx1100,gfx1101 rocm_path=/usr python_version=3.11 ``` I get the following error: ``` [738 / 8,363] Executing genrule //rocm:rocmlib; 3s local ... (24 actions, 23 running) ERROR: /home/richard/.cache/bazel/_bazel_richard/2be5aaa14d23ec4c14ff72f51bd1b8a9/external/xla/xla/stream_executor/rocm/BUILD:206:13: Compiling xla/stream_executor/rocm/hip_conditional_kernels.cu.: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command (from target //xla/stream_executor/rocm:hip_conditional_kernels)    (cd /home/richard/.cache/bazel/_bazel_richard/2be5aaa14d23ec4c14ff72f51bd1b8a9/execroot/__main__ && \   exec env  \     PATH=/home/richard/.local/bin:/home/richard/.local/miniconda3/condabin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/sbin:/usr/sbin:/sbin:/var/lib/snapd/snap/bin \     PWD=/proc/self/cwd \     ROCM_PATH=/usr \     TF_ROCM_AMDGPU_TARGETS=gfx1100,gfx1101 \   external/local_config_rocm/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc U_FORTIFY_SOURCE fstackprotector Wall Wunusedbutsetparameter Wnofreenonheapobject fnoomitframepointer g0 O2 'D_FORTIFY_SOURCE=1' DNDEBUG ffunctionsections fdatasections 'std=c++14' MD MF bazelout/k8opt/bin/external/xla/xla/stream_executor/rocm/_objs/hip_conditional_kernels/hip_conditional_kernels.cu.pic.d 'frandomseed=bazelout/k8opt/bin/external/xla/xla/stream_executor/rocm/_objs/hip_conditional_kernels/hip_conditional_kernels.cu.pic.o' fPIC 'DBAZEL_CURRENT_REPOSITORY=""xla""' iquote external/xla iquote bazelout/k8opt/bin/external/xla iquote external/local_config_rocm iquote bazelout/k8opt/bin/external/local_config_rocm isystem external/local_config_rocm/rocm isystem bazelout/k8opt/bin/external/local_config_rocm/rocm isystem external/local_config_rocm/rocm/rocm/include isystem bazelout/k8opt/bin/external/local_config_rocm/rocm/rocm/include isystem external/local_config_rocm/rocm/rocm/include/rocrand isystem bazelout/k8opt/bin/external/local_config_rocm/rocm/rocm/include/rocrand isystem external/local_config_rocm/rocm/rocm/include/roctracer isystem bazelout/k8opt/bin/external/local_config_rocm/rocm/rocm/include/roctracer 'fvisibility=hidden' Wnosigncompare Wnounknownwarningoption Wnostringoptruncation Wnoarrayparameter 'DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir.' mavx 'std=c++17' x rocm 'amdgputarget=gfx1100' 'amdgputarget=gfx1101' fnocanonicalsystemheaders Wnobuiltinmacroredefined 'D__DATE__=""redacted""' 'D__TIMESTAMP__=""redacted""' 'D__TIME__=""redacted""' 'DTENSORFLOW_USE_ROCM=1' D__HIP_PLATFORM_AMD__ DEIGEN_USE_HIP DUSE_ROCM nocanonicalprefixes fnocanonicalsystemheaders c external/xla/xla/stream_executor/rocm/hip_conditional_kernels.cu./k8opt/bin/external/xla/xla/stream_executor/rocm/_objs/hip_conditional_kernels/hip_conditional_kernels.cu.pic.o)  Configuration: b3c10413bb73176f65c435545cfa027fac61a68a2dc64684a92b98fca12a27fa  Execution platform: //:platform /home/richard/.cache/bazel/_bazel_richard/2be5aaa14d23ec4c14ff72f51bd1b8a9/execroot/__main__/external/local_config_rocm/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc:162: SyntaxWarning: invalid escape sequence '\.'   re.search('\.cpp$\.C$', f)] /home/richard/.cache/bazel/_bazel_richard/2be5aaa14d23ec4c14ff72f51bd1b8a9/execroot/__main__/external/local_config_rocm/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc:23: DeprecationWarning: 'pipes' is deprecated and slated for removal in Python 3.13   import pipes clang: warning: argument unused during compilation: 'fcudaflushdenormalstozero' [Wunusedcommandlineargument] clang: error: cannot find ROCm device library; provide its path via 'rocmpath' or 'rocmdevicelibpath', or pass 'nogpulib' to build without ROCm device library ``` Is there some way to specify `rocmdevicelibpath` for the build? I am unfortunately completely unfamiliar with bazel and don't even know where to start looking. Thanks!  System info (python version, jaxlib version, accelerator, etc.) ``` jax: 0.4.30 Python: 3.11 OS: Fedora 40 ROCm: 6.0.2 GPU: 7800 XT ```",2024-07-25T07:16:40Z,bug AMD GPU,open,0,7,https://github.com/jax-ml/jax/issues/22650,"Sorry, I missed that this had been assigned to me. Is this still a problem?","Hi, sorry for taking a while to get back on this. I realized I can get around the particular issue of missing device files by creating the symlink ```bash ls la /usr/amdgcn lrwxrwxrwx. 1 root root 19 Sep 29 12:42 /usr/amdgcn > lib/clang/17/amdgcn ``` However, now the build fails at a later stage: ```bash BUILD_DIR=~/build/rocm mkdir p ${BUILD_DIR} cd ${BUILD_DIR} git clone b rocmjaxlibv0.4.31 https://github.com/ROCm/jax.git git clone b rocmjaxlibv0.4.31 https://github.com/ROCm/xla.git cd jax python3 ./build/build.py clang_path=/usr/bin/clang17  enable_rocm rocm_amdgpu_targets=gfx1100  build_gpu_plugin gpu_plugin_rocm_version=60  bazel_options=override_repository=xla=${BUILD_DIR}/xla rocm_path=/usr enable_mkl_dnn=false ``` buildrocmjax0.4.31.log I also tried with the main branch and the default XLA, but that causes a different error which seems to be related to building zlib. buildjaxmain.log Thanks!"," thanks for notifying us the issue. Actually, we are working on clang patch. Meanwhile, can you try compiling like > ``` rm rf dist; python3.11 m pip uninstall jax jaxlib jaxrocm60pjrt jaxrocm60plugin y; python3.11 ./build/build.py use_clang=false enable_rocm build_gpu_plugin gpu_plugin_rocm_version=60 rocm_amdgpu_targets=[gfxXXX] bazel_options=override_repository=xla=[xla_dir] rocm_path=/opt/rocm6.2.1/ && python3.11 setup.py develop user && python3.11 m pip install dist/*.whl ```","Hi  , It does not seem to make a difference whether I use clang or not. Running the command you suggested (adapted for the Fedora 40 setup), ```bash BUILD_DIR=~/build/rocm mkdir p ${BUILD_DIR} cd ${BUILD_DIR} git clone b rocmjaxlibv0.4.33 https://github.com/ROCm/jax.git git clone b rocmjaxlibv0.4.33 https://github.com/ROCm/xla.git cd jax ``` ```bash python3.11 ./build/build.py use_clang=false enable_rocm build_gpu_plugin gpu_plugin_rocm_version=60 rocm_amdgpu_targets=gfx1100 bazel_options=override_repository=xla=/home/richard/build/rocm/xla rocm_path=/usr ``` produces the same error, see jaxbuildnoclang.txt I know it works on the Ubuntubased Docker container that AMD/ROCm provide, so it's probably something specific to Fedora. As you probably know, Fedora ships ROCm6.1.2 directly in their repos and everything is installed right into `/usr` as opposed to  `/opt/rocm6.x.y`", hmm. I don't have Fedora container to reproduce this error. Can you add > lib/clang/17/include ^^ This above include here > https://github.com/openxla/xla/blob/9e28b002070276a852de6b5508224d35d2547d51/third_party/tsl/third_party/gpus/rocm_configure.bzlL210 And check if it compiles?,"Hi  , I can confirm that the build runs through on Fedora with the following change: ``` git diff diff git a/third_party/tsl/third_party/gpus/rocm_configure.bzl b/third_party/tsl/third_party/gpus/rocm_configure.bzl index 482b3eca70..7b3827c79c 100644  a/third_party/tsl/third_party/gpus/rocm_configure.bzl +++ b/third_party/tsl/third_party/gpus/rocm_configure.bzl @@ 207,6 +207,7 @@ def _rocm_include_path(repository_ctx, rocm_config, bash_bin):      inc_dirs.append(rocm_toolkit_path + ""/llvm/lib/clang/16.0.0/include"")      inc_dirs.append(rocm_toolkit_path + ""/llvm/lib/clang/17.0.0/include"")      inc_dirs.append(rocm_toolkit_path + ""/llvm/lib/clang/17/include"") +    inc_dirs.append(rocm_toolkit_path + ""/lib/clang/17/include"")      inc_dirs.append(rocm_toolkit_path + ""/llvm/lib/clang/18/include"")      if int(rocm_config.rocm_version_number) >= 60200:          inc_dirs.append(rocm_toolkit_path + ""/lib/llvm/lib/clang/18/include"") ``` I haven't had the time to check whether the binary actually works, I'll come back to that tomorrow. Thanks a lot!", thanks!  we can close this now please.
yi,Don't broadcast scalar conditions in the jnp.where implementation().,The underlying lax primitive is perfectly happy to accept scalar conditions with the other arguments being nonscalar.,2024-07-24T15:27:02Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/22629
yi,jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed," Description Hello im having an issue using jax and cuda. My error message looks like this: 20240719 15:49:01.341533: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:439] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR 20240719 15:49:01.341571: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:443] Memory usage: 2624061440 bytes free, 12524191744 bytes total. Traceback (most recent call last): File ""scripts/train_vqgan.py"", line 202, in main() File ""scripts/train_vqgan.py"", line 32, in main rng = jax.random.PRNGKey(config.seed) File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/random.py"", line 160, in PRNGKey key = prng.seed_with_impl(impl, seed) File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/prng.py"", line 406, in seed_with_impl return random_seed(seed, impl=impl) File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/prng.py"", line 690, in random_seed return random_seed_p.bind(seeds_arr, impl=impl) File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/prng.py"", line 702, in random_seed_impl base_arr = random_seed_impl_base(seeds, impl=impl) File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/prng.py"", line 707, in random_seed_impl_base return seed(seeds) File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/prng.py"", line 936, in threefry_seed return _threefry_seed(seed) File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback return fun(*args, **kwargs) File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/pjit.py"", line 250, in cache_miss outs, out_flat, out_tree, args_flat, jaxpr = _python_pjit_helper( File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/pjit.py"", line 163, in _python_pjit_helper out_flat = pjit_p.bind(*args_flat, **params) File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/core.py"", line 2677, in bind return self.bind_with_trace(top_trace, args, params) File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/core.py"", line 383, in bind_with_trace out = trace.process_primitive(self, map(trace.full_raise, args), params) File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/core.py"", line 815, in process_primitive return primitive.impl(*tracers, **params) File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/pjit.py"", line 1203, in _pjit_call_impl return xc._xla.pjit(name, f, call_impl_cache_miss, [], [], donated_argnums, File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/pjit.py"", line 1187, in call_impl_cache_miss out_flat, compiled = _pjit_call_impl_python( File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/pjit.py"", line 1120, in _pjit_call_impl_python compiled = _pjit_lower( File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/interpreters/pxla.py"", line 2323, in compile executable = UnloadedMeshExecutable.from_hlo( File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/interpreters/pxla.py"", line 2645, in from_hlo xla_executable, compile_options = _cached_compilation( File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/interpreters/pxla.py"", line 2555, in _cached_compilation xla_executable = dispatch.compile_or_get_cached( File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 497, in compile_or_get_cached return backend_compile(backend, computation, compile_options, File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/profiler.py"", line 314, in wrapper return func(*args, **kwargs) File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 465, in backend_compile return backend.compile(built_c, compile_options=options) jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified. The above exception was the direct cause of the following exception: Traceback (most recent call last): File ""scripts/train_vqgan.py"", line 202, in main() File ""scripts/train_vqgan.py"", line 32, in main rng = jax.random.PRNGKey(config.seed) File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/random.py"", line 160, in PRNGKey key = prng.seed_with_impl(impl, seed) File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/prng.py"", line 406, in seed_with_impl return random_seed(seed, impl=impl) File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/prng.py"", line 690, in random_seed return random_seed_p.bind(seeds_arr, impl=impl) File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/core.py"", line 380, in bind return self.bind_with_trace(find_top_trace(args), args, params) File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/core.py"", line 383, in bind_with_trace out = trace.process_primitive(self, map(trace.full_raise, args), params) File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/core.py"", line 815, in process_primitive return primitive.impl(*tracers, **params) File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/prng.py"", line 702, in random_seed_impl base_arr = random_seed_impl_base(seeds, impl=impl) File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/prng.py"", line 707, in random_seed_impl_base return seed(seeds) File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/prng.py"", line 936, in threefry_seed return _threefry_seed(seed) jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. I already saw an issue about this which was already closed but after trying anything that was mentioned there i still could not get it to run. I have tried the following things already: pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html pip install forcereinstall ""jax[cuda11_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html XLA_PYTHON_CLIENT_PREALLOCATE=false XLA_PYTHON_CLIENT_MEM_FRACTION=.05 XLA_PYTHON_CLIENT_ALLOCATOR=platform none of those changed the error message, I always got the error above no matter what i did unfortunately. For System Information and further setup information see below. I hope someone can help me because this is of really really high importance to me as I need to use Jax for an important project. I would appreciate any help i can get. The information on hardware below is the setup i have sudo access to and since i get the same error there i was hoping to be able to fix it there and see what i can do after that because the actual goal is to run this on a server i dont have sudo access to so things like reinstalling cuda stuff wont be possible.  System info (python version, jaxlib version, accelerator, etc.) System info (python version, jaxlib version, accelerator, etc.) Hardware GPU: RTX 3060 12GB CUDA version 12.5 RAM: 16GB Python Version: 3.8 Jax version: 0.4.13 jax.print_environment_info() jax: 0.4.13 jaxlib: 0.4.13 numpy: 1.24.4 python: 3.8.19  ++",2024-07-24T14:53:04Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/22626,"This error can come up when JAX is used with an incompatible CUDNN version. You report using jax 0.4.13 with CUDA 12.5 – that's a fairly old JAX version with a much more recent CUDA version, so I wouldn't be surprised if there are some incompatibilities. We've certainly never tested jax 0.4.13 with CUDA 12.5... I'd suggest updating to a more recent JAX version and see if that helps.",Thank you very much for pointing that out. I was sure i have tried to adjust the versions before but I probably messed up by having a different version support of jax installed when trying a different version from what I installed with jax. I tried it again and was extra cautious with having the same versions in place and now it works totally fine. With jax 0.4.13 there is no compatibility to the major version update of cudnn to 9.x I hope anyone who runs into the same issue will see this and be able to make it work too. I will now close this as it is solved.
chat,Bulid jaxlib from source code error:  ERROR: Build failed. Not running target! ," Description When I used the pip tool ( the command I used is:  pip install upgrade pip NVIDIA CUDA 12 installation Note: wheels only available on linux. pip install upgrade ""jax[cuda12]"" ）  to install jaxlib and jax, and then tried to import jax display : **This version of jaxlib was built using AVX instructions, which your CPU and/or operating system do not support. You may be able work around this issue by building jaxlib from source。** then Follow the prompts I try to install from the source code refer to**https://jax.readthedocs.io/en/latest/developer.htmlbuildingjaxlibfromsource** .The error is take place. the key information is as below: >ERROR: /home/chat/.cache/bazel/_bazel_chat/e07bb7f39ed7b030eaf9bab18b94dd76/external/llvmproject/llvm/BUILD.bazel:1296:7: Generating code from table: lib/Target/RISCV/RISCV.td project//llvm:RISCVTargetParserDefGen__gen_riscv_target_def_genrule [for tool] failed: (Illegal instruction): bash failed: error executing command (from target project//llvm:RISCVTargetParserDefGen__gen_riscv_target_def_genrule)   (cd /home/chat/.cache/bazel/_bazel_chat/e07bb7f39ed7b030eaf9bab18b94dd76/execroot/__main__ && \  exec env  \  LD_LIBRARY_PATH=/usr/local/cuda12.3/lib64:/usr/local/cuda12.3/lib64 \  PATH=/usr/local/cuda12.3/bin:/home/chat/home/chat/anaconda3/bin/bin:/home/chat/.vscodeserver/cli/servers/Stableea1445cc7016315d0f5728f8e8b12a45dc0a7286/server/bin/remotecli:/usr/local/cuda12.3/bin:/home/chat/home/chat/anaconda3/bin/bin:/home/chat/anaconda3/envs/xuyunlian_clone_cloud/bin:/home/chat/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \  /bin/bash c 'source external/bazel_tools/tools/genrule/genrulesetup.sh; bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/llvmmintblgen I external/llvmproject/llvm/include I external/llvmproject/clang/include I $(dirname external/llvmproject/llvm/lib/Target/RISCV/RISCV.td) genriscvtargetdef external/llvmproject/llvm/lib/Target/RISCV/RISCV.td  o bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/include/llvm/TargetParser/RISCVTargetParserDef.inc') Configuration: 96e81dc836f2097966e75983107cb61c476a61d38f22eded3fcd30b0d3cc1a49 Execution platform: //:platform Target //jaxlib/tools:build_wheel failed to build INFO: Elapsed time: 1.152s, Critical Path: 0.26s INFO: 22 processes: 21 internal, 1 local. FAILED: Build did NOT complete successfully ERROR: Build failed. Not running target  System info (python version, jaxlib version, accelerator, etc.) When I used the pip tool ( the command I used is:  pip install upgrade pip NVIDIA CUDA 12 installation Note: wheels only available on linux. pip install upgrade ""jax[cuda12]"" ）  to install jaxlib and jax, and then tried to import jax display : **This version of jaxlib was built using AVX instructions, which your CPU and/or operating system do not support. You may be able work around this issue by building jaxlib from source。** then Follow the prompts I try to install from the source code refer to**https://jax.readthedocs.io/en/latest/developer.htmlbuildingjaxlibfromsource** .The error is take place. the key information is as below: >ERROR: /home/chat/.cache/bazel/_bazel_chat/e07bb7f39ed7b030eaf9bab18b94dd76/external/llvmproject/llvm/BUILD.bazel:1296:7: Generating code from table: lib/Target/RISCV/RISCV.td project//llvm:RISCVTargetParserDefGen__gen_riscv_target_def_genrule [for tool] failed: (Illegal instruction): bash failed: error executing command (from target project//llvm:RISCVTargetParserDefGen__gen_riscv_target_def_genrule)   (cd /home/chat/.cache/bazel/_bazel_chat/e07bb7f39ed7b030eaf9bab18b94dd76/execroot/__main__ && \  exec env  \  LD_LIBRARY_PATH=/usr/local/cuda12.3/lib64:/usr/local/cuda12.3/lib64 \  PATH=/usr/local/cuda12.3/bin:/home/chat/home/chat/anaconda3/bin/bin:/home/chat/.vscodeserver/cli/servers/Stableea1445cc7016315d0f5728f8e8b12a45dc0a7286/server/bin/remotecli:/usr/local/cuda12.3/bin:/home/chat/home/chat/anaconda3/bin/bin:/home/chat/anaconda3/envs/xuyunlian_clone_cloud/bin:/home/chat/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \  /bin/bash c 'source external/bazel_tools/tools/genrule/genrulesetup.sh; bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/llvmmintblgen I external/llvmproject/llvm/include I external/llvmproject/clang/include I $(dirname external/llvmproject/llvm/lib/Target/RISCV/RISCV.td) genriscvtargetdef external/llvmproject/llvm/lib/Target/RISCV/RISCV.td  o bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/include/llvm/TargetParser/RISCVTargetParserDef.inc') Configuration: 96e81dc836f2097966e75983107cb61c476a61d38f22eded3fcd30b0d3cc1a49 Execution platform: //:platform Target //jaxlib/tools:build_wheel failed to build INFO: Elapsed time: 1.152s, Critical Path: 0.26s INFO: 22 processes: 21 internal, 1 local. FAILED: Build did NOT complete successfully ERROR: Build failed. Not running target The Complete information is： >(xuyunlian_clone_cloud) chatStandardPCi440FXPIIX1996:~/jax/jaxmain$ python build/build.py enable_cuda Bazel binary path: /usr/bin/bazel Bazel version: 6.5.0 Python binary path: /home/chat/anaconda3/envs/xuyunlian_clone_cloud/bin/python Python version: 3.11 Use clang: no MKLDNN enabled: yes Target CPU: x86_64 Target CPU features: release CUDA enabled: yes NCCL enabled: yes ROCm enabled: no Building XLA and installing it in the jaxlib source tree... /usr/bin/bazel run verbose_failures=true //jaxlib/tools:build_wheel  output_path=/home/chat/jax/jaxmain/dist jaxlib_git_hash= cpu=x86_64 INFO: Options provided by the client: Inherited 'common' options: isatty=0 terminal_columns=80 INFO: Reading rc options for 'run' from /home/chat/jax/jaxmain/.bazelrc: Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'run' from /home/chat/jax/jaxmain/.bazelrc: Inherited 'build' options: nocheck_visibility apple_platform_type=macos macos_minimum_os=10.14 announce_rc define open_source_build=true spawn_strategy=standalone enable_platform_specific_config experimental_cc_shared_library define=no_aws_support=true define=no_gcp_support=true define=no_hdfs_support=true define=no_kafka_support=true define=no_ignite_support=true define=grpc_no_ares=true define=tsl_link_protobuf=true c opt config=short_logs copt=DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir. //xla/python:enable_gpu=false INFO: Reading rc options for 'run' from /home/chat/jax/jaxmain/.jax_configure.bazelrc: Inherited 'build' options: strategy=Genrule=standalone config=avx_posix config=mkl_open_source_only config=cuda repo_env HERMETIC_PYTHON_VERSION=3.11 INFO: Found applicable config definition build:short_logs in file /home/chat/jax/jaxmain/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:avx_posix in file /home/chat/jax/jaxmain/.bazelrc: copt=mavx host_copt=mavx INFO: Found applicable config definition build:mkl_open_source_only in file /home/chat/jax/jaxmain/.bazelrc: define=tensorflow_mkldnn_contraction_kernel=1 INFO: Found applicable config definition build:cuda in file /home/chat/jax/jaxmain/.bazelrc: repo_env TF_NEED_CUDA=1 repo_env TF_NCCL_USE_STUB=1 action_env TF_CUDA_COMPUTE_CAPABILITIES=sm_50,sm_60,sm_70,sm_80,compute_90 crosstool_top=//crosstool:toolchain //:enable_cuda //xla/python:enable_gpu=true //xla/python:jax_cuda_pip_rpaths=true define=xla_python_enable_gpu=true linkopt=Wl,disablenewdtags INFO: Found applicable config definition build:linux in file /home/chat/jax/jaxmain/.bazelrc: config=posix copt=Wnounknownwarningoption copt=Wnostringoptruncation copt=Wnoarrayparameter INFO: Found applicable config definition build:posix in file /home/chat/jax/jaxmain/.bazelrc: copt=fvisibility=hidden copt=Wnosigncompare cxxopt=std=c++17 host_cxxopt=std=c++17 Loading: DEBUG: /home/chat/.cache/bazel/_bazel_chat/e07bb7f39ed7b030eaf9bab18b94dd76/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:10: AutoConfiguration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default DEBUG: /home/chat/.cache/bazel/_bazel_chat/e07bb7f39ed7b030eaf9bab18b94dd76/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:10: AutoConfiguration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default Loading: Loading: 2 packages loaded Analyzing: target //jaxlib/tools:build_wheel (3 packages loaded, 0 targets configured) Analyzing: target //jaxlib/tools:build_wheel (103 packages loaded, 3496 targets configured) Analyzing: target //jaxlib/tools:build_wheel (156 packages loaded, 6720 targets configured) Analyzing: target //jaxlib/tools:build_wheel (242 packages loaded, 23146 targets configured) Analyzing: target //jaxlib/tools:build_wheel (245 packages loaded, 23707 targets configured) Analyzing: target //jaxlib/tools:build_wheel (245 packages loaded, 23707 targets configured) INFO: Analyzed target //jaxlib/tools:build_wheel (246 packages loaded, 24402 targets configured). INFO: Found 1 target... [4 / 356] [Prepa] BazelWorkspaceStatusAction stablestatus.txt ... (7 actions, 4 running) [304 / 4,299] Executing genrule //cuda:cudalib; 0s local ... (11 actions, 9 running) [387 / 4,765] Executing genrule //cuda:cudalib; 2s local ... (14 actions, 13 running) [3,377 / 6,935] Executing genrule //cuda:cudalib; 3s local ... (17 actions, 14 running) [4,069 / 7,464] Executing genrule //cuda:cudalib; 4s local ... (19 actions, 18 running) [4,495 / 9,629] Executing genrule //cuda:cudalib; 5s local ... (20 actions, 19 running) [6,046 / 9,683] Executing genrule //cuda:cudalib; 6s local ... (19 actions, 18 running) [6,086 / 9,698] Executing genrule //cuda:cudalib; 7s local ... (20 actions, 19 running) [6,132 / 10,677] Executing genrule //cuda:cudalib; 8s local ... (19 actions, 18 running) [6,175 / 11,902] Executing genrule //cuda:cudalib; 9s local ... (19 actions, 18 running) [6,216 / 12,069] Executing genrule //cuda:cudalib; 10s local ... (19 actions, 18 running) [6,287 / 12,273] Executing genrule //cuda:cudalib; 11s local ... (20 actions, 19 running) [6,351 / 12,902] Compiling llvm/lib/TableGen/Record.cpp [for tool]; 3s local ... (20 actions, 19 running) [6,405 / 13,190] Compiling llvm/lib/Support/APFloat.cpp [for tool]; 2s local ... (20 actions, 19 running) [6,473 / 13,190] Compiling xla/service/gpu/buffer_comparator.cu.cc; 1s local ... (20 actions, 19 running) [6,532 / 13,190] Compiling xla/service/gpu/buffer_comparator.cu.cc; 2s local ... (20 actions, 19 running) [6,574 / 13,208] Compiling xla/service/gpu/buffer_comparator.cu.cc; 3s local ... (20 actions, 19 running) [6,626 / 13,282] Compiling xla/service/gpu/buffer_comparator.cu.cc; 4s local ... (20 actions, 19 running) [6,672 / 13,282] Compiling xla/service/gpu/buffer_comparator.cu.cc; 5s local ... (20 actions, 19 running) [6,731 / 13,440] Compiling xla/service/gpu/buffer_comparator.cu.cc; 6s local ... (20 actions, 19 running) [6,780 / 14,198] Compiling llvm/lib/Support/VirtualFileSystem.cpp [for tool]; 3s local ... (19 actions, 18 running) [6,811 / 14,711] Compiling llvm/lib/Support/VirtualFileSystem.cpp [for tool]; 4s local ... (20 actions, 19 running) [6,851 / 14,716] Compiling llvm/lib/Support/CommandLine.cpp [for tool]; 3s local ... (20 actions, 19 running) [6,883 / 14,716] Compiling src/google/protobuf/descriptor.cc; 3s local ... (20 actions, 19 running) [6,912 / 14,716] Compiling src/google/protobuf/descriptor.cc; 5s local ... (20 actions, 19 running) [6,961 / 14,940] Compiling src/google/protobuf/descriptor.cc; 6s local ... (20 actions, 19 running) [7,002 / 14,940] Compiling src/google/protobuf/descriptor.cc; 7s local ... (20 actions, 19 running) [7,038 / 14,940] Compiling src/google/protobuf/descriptor.cc; 8s local ... (20 actions running) [7,063 / 14,941] Compiling src/google/protobuf/descriptor.cc; 9s local ... (20 actions running) [7,093 / 15,035] Compiling src/cpu/x64/shuffle/jit_uni_shuffle_kernel.cpp; 3s local ... (19 actions running) [7,112 / 15,058] Compiling src/cpu/x64/shuffle/jit_uni_shuffle_kernel.cpp; 4s local ... (20 actions running) [7,129 / 15,111] Compiling src/cpu/x64/gemm/s8x8s32/jit_sse41_u8_copy_sum_at_kern_autogen.cpp; 3s local ... (20 actions running) [7,152 / 15,114] Compiling xla/service/cpu/runtime_single_threaded_matmul_s32.cc; 3s local ... (18 actions running) [7,173 / 15,130] Compiling xla/service/cpu/runtime_single_threaded_matmul_s32.cc; 4s local ... (19 actions, 18 running) [7,188 / 15,222] Compiling xla/service/cpu/runtime_single_threaded_matmul_s32.cc; 5s local ... (20 actions running) [7,215 / 15,222] Compiling src/google/protobuf/wire_format.cc [for tool]; 3s local ... (20 actions running) ERROR: /home/chat/.cache/bazel/_bazel_chat/e07bb7f39ed7b030eaf9bab18b94dd76/external/llvmproject/llvm/BUILD.bazel:783:7: Generating code from table: include/llvm/CodeGen/ValueTypes.td project//llvm:vt_gen__gen_vt_genrule failed: (Illegal instruction): bash failed: error executing command (from target project//llvm:vt_gen__gen_vt_genrule) (cd /home/chat/.cache/bazel/_bazel_chat/e07bb7f39ed7b030eaf9bab18b94dd76/execroot/**main** &&  exec env   LD_LIBRARY_PATH=/usr/local/cuda12.3/lib64:/usr/local/cuda12.3/lib64  PATH=/usr/local/cuda12.3/bin:/home/chat/home/chat/anaconda3/bin/bin:/home/chat/.vscodeserver/cli/servers/Stableea1445cc7016315d0f5728f8e8b12a45dc0a7286/server/bin/remotecli:/usr/local/cuda12.3/bin:/home/chat/home/chat/anaconda3/bin/bin:/home/chat/anaconda3/envs/xuyunlian_clone_cloud/bin:/home/chat/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin  TF_CUDA_COMPUTE_CAPABILITIES=sm_50,sm_60,sm_70,sm_80,compute_90  /bin/bash c 'source external/bazel_tools/tools/genrule/genrulesetup.sh; bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/llvmmintblgen I external/llvmproject/llvm/include I external/llvmproject/clang/include I $(dirname external/llvmproject/llvm/include/llvm/CodeGen/ValueTypes.td) genvt external/llvmproject/llvm/include/llvm/CodeGen/ValueTypes.td  o bazelout/k8opt/bin/external/llvmproject/llvm/include/llvm/CodeGen/GenVT.inc') Configuration: 833f5f4976e067f110730df7e3b0b8f20258e7d01a0245e165b3de4c2e3de8cc Execution platform: //:platform Target //jaxlib/tools:build_wheel failed to build INFO: Elapsed time: 44.051s, Critical Path: 11.96s INFO: 6941 processes: 5584 internal, 1357 local. FAILED: Build did NOT complete successfully ERROR: Build failed. Not running target Traceback (most recent call last): File ""/home/chat/jax/jaxmain/build/build.py"", line 750, in  main() File ""/home/chat/jax/jaxmain/build/build.py"", line 701, in main shell(build_cpu_wheel_command) File ""/home/chat/jax/jaxmain/build/build.py"", line 45, in shell output = subprocess.check_output(cmd) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""/home/chat/anaconda3/envs/xuyunlian_clone_cloud/lib/python3.11/subprocess.py"", line 465, in check_output return run(*popenargs, stdout=PIPE, timeout=timeout, check=True, ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""/home/chat/anaconda3/envs/xuyunlian_clone_cloud/lib/python3.11/subprocess.py"", line 569, in run raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command '['/usr/bin/bazel', 'run', 'verbose_failures=true', '//jaxlib/tools:build_wheel', '', 'output_path=/home/chat/jax/jaxmain/dist', 'jaxlib_git_hash=', 'cpu=x86_64']' returned nonzero exit status 1. (xuyunlian_clone_cloud) chatStandardPCi440FXPIIX1996:~/jax/jaxmain$/usr/bin/bazel run verbose_failures=true //jaxlib/tools:build_wheel  output_path=/home/chat/jax/jaxmain/dist jaxlib_git_hash= cpu=x86_64 INFO: Options provided by the client:   Inherited 'common' options: isatty=1 terminal_columns=157 INFO: Reading rc options for 'run' from /home/chat/jax/jaxmain/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'run' from /home/chat/jax/jaxmain/.bazelrc:   Inherited 'build' options: nocheck_visibility apple_platform_type=macos macos_minimum_os=10.14 announce_rc define open_source_build=true spawn_strategy=standalone enable_platform_specific_config experimental_cc_shared_library define=no_aws_support=true define=no_gcp_support=true define=no_hdfs_support=true define=no_kafka_support=true define=no_ignite_support=true define=grpc_no_ares=true define=tsl_link_protobuf=true c opt config=short_logs copt=DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir. //xla/python:enable_gpu=false INFO: Reading rc options for 'run' from /home/chat/jax/jaxmain/.jax_configure.bazelrc:   Inherited 'build' options: strategy=Genrule=standalone config=avx_posix config=mkl_open_source_only config=cuda repo_env HERMETIC_PYTHON_VERSION=3.11 INFO: Found applicable config definition build:short_logs in file /home/chat/jax/jaxmain/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:avx_posix in file /home/chat/jax/jaxmain/.bazelrc: copt=mavx host_copt=mavx INFO: Found applicable config definition build:mkl_open_source_only in file /home/chat/jax/jaxmain/.bazelrc: define=tensorflow_mkldnn_contraction_kernel=1 INFO: Found applicable config definition build:cuda in file /home/chat/jax/jaxmain/.bazelrc: repo_env TF_NEED_CUDA=1 repo_env TF_NCCL_USE_STUB=1 action_env TF_CUDA_COMPUTE_CAPABILITIES=sm_50,sm_60,sm_70,sm_80,compute_90 crosstool_top=//crosstool:toolchain //:enable_cuda //xla/python:enable_gpu=true //xla/python:jax_cuda_pip_rpaths=true define=xla_python_enable_gpu=true linkopt=Wl,disablenewdtags INFO: Found applicable config definition build:linux in file /home/chat/jax/jaxmain/.bazelrc: config=posix copt=Wnounknownwarningoption copt=Wnostringoptruncation copt=Wnoarrayparameter INFO: Found applicable config definition build:posix in file /home/chat/jax/jaxmain/.bazelrc: copt=fvisibility=hidden copt=Wnosigncompare cxxopt=std=c++17 host_cxxopt=std=c++17 INFO: Analyzed target //jaxlib/tools:build_wheel (0 packages loaded, 0 targets configured). INFO: Found 1 target... ERROR: /home/chat/.cache/bazel/_bazel_chat/e07bb7f39ed7b030eaf9bab18b94dd76/external/llvmproject/llvm/BUILD.bazel:1296:7: Generating code from table: lib/Target/RISCV/RISCV.td project//llvm:RISCVTargetParserDefGen__gen_riscv_target_def_genrule [for tool] failed: (Illegal instruction): bash failed: error executing command (from target project//llvm:RISCVTargetParserDefGen__gen_riscv_target_def_genrule)    (cd /home/chat/.cache/bazel/_bazel_chat/e07bb7f39ed7b030eaf9bab18b94dd76/execroot/__main__ && \   exec env  \     LD_LIBRARY_PATH=/usr/local/cuda12.3/lib64:/usr/local/cuda12.3/lib64 \     PATH=/usr/local/cuda12.3/bin:/home/chat/home/chat/anaconda3/bin/bin:/home/chat/.vscodeserver/cli/servers/Stableea1445cc7016315d0f5728f8e8b12a45dc0a7286/server/bin/remotecli:/usr/local/cuda12.3/bin:/home/chat/home/chat/anaconda3/bin/bin:/home/chat/anaconda3/envs/xuyunlian_clone_cloud/bin:/home/chat/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \   /bin/bash c 'source external/bazel_tools/tools/genrule/genrulesetup.sh; bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/llvmmintblgen I external/llvmproject/llvm/include I external/llvmproject/clang/include I$(dirname external/llvmproject/llvm/lib/Target/RISCV/RISCV.td) genriscvtargetdef external/llvmproject/llvm/lib/Target/RISCV/RISCV.td  o bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/include/llvm/TargetParser/RISCVTargetParserDef.inc') Configuration: 96e81dc836f2097966e75983107cb61c476a61d38f22eded3fcd30b0d3cc1a49 Execution platform: //:platform Target //jaxlib/tools:build_wheel failed to build INFO: Elapsed time: 1.152s, Critical Path: 0.26s INFO: 22 processes: 21 internal, 1 local. FAILED: Build did NOT complete successfully ERROR: Build failed. Not running target  ERROR: /home/chat/.cache/bazel/_bazel_chat/e07bb7f39ed7b030eaf9bab18b94dd76/external/llvmproject/llvm/BUILD.bazel:1296:7: Generating code from table: lib/Target/RISCV/RISCV.td project//llvm:RISCVTargetParserDefGen__gen_riscv_target_def_genrule [for tool] failed: (Illegal instruction): bash failed: error executing command (from target project//llvm:RISCVTargetParserDefGen__gen_riscv_target_def_genrule)  (cd /home/chat/.cache/bazel/_bazel_chat/e07bb7f39ed7b030eaf9bab18b94dd76/execroot/__main__ && \ exec env  \ LD_LIBRARY_PATH=/usr/local/cuda12.3/lib64:/usr/local/cuda12.3/lib64 \ PATH=/usr/local/cuda12.3/bin:/home/chat/home/chat/anaconda3/bin/bin:/home/chat/.vscodeserver/cli/servers/Stableea1445cc7016315d0f5728f8e8b12a45dc0a7286/server/bin/remotecli:/usr/local/cuda12.3/bin:/home/chat/home/chat/anaconda3/bin/bin:/home/chat/anaconda3/envs/xuyunlian_clone_cloud/bin:/home/chat/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \ /bin/bash c 'source external/bazel_tools/tools/genrule/genrulesetup.sh; bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/llvmmintblgen I external/llvmproject/llvm/include I external/llvmproject/clang/include I $(dirname external/llvmproject/llvm/lib/Target/RISCV/RISCV.td) genriscvtargetdef external/llvmproject/llvm/lib/Target/RISCV/RISCV.td  o bazelout/k8optexec50AE0418/bin/external/llvmproject/llvm/include/llvm/TargetParser/RISCVTargetParserDef.inc') Configuration: 96e81dc836f2097966e75983107cb61c476a61d38f22eded3fcd30b0d3cc1a49 Execution platform: //:platform Target //jaxlib/tools:build_wheel failed to build INFO: Elapsed time: 1.152s, Critical Path: 0.26s INFO: 22 processes: 21 internal, 1 local. FAILED: Build did NOT complete successfully ERROR: Build failed. Not running target! **Looking forward to a helpful reply！**",2024-07-24T07:29:07Z,bug type:support,open,0,1,https://github.com/jax-ml/jax/issues/22614,"Hi   Are you attempting to install JAX with CUDA support on Windows? If so, please note that JAX with CUDA support is not available on windows. Please let us know if you are using any other OS. Thank you."
rag,fix memory leak in cond jaxpr tracing,fixes CC(Memory leak when closing over large constants in compiled jax.lax.cond statement) We already have test coverage for getting cache hits in `LaxControlFlowTest.LaxControlFlowTest.test_cond_excessive_compilation`.,2024-07-23T23:39:34Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/22610
yi,[Mosaic GPU] Move barrier allocation to SMEM scratch specs,"[Mosaic GPU] Move barrier allocation to SMEM scratch specs This is slightly less convenient than our previous approach but it has two main upsides: 1. It lets us automatically emit necessary fences and barriers for use with block clusters 2. It lets us share the same block/cluster barrier for all initializations of mbarriers This change also moves away from the nvgpu dialect for barriers and allocates them in dynamic SMEM instead of relying on static SMEM. This should give us more control over SMEM layouts and alignments, and simplifies the lowering process.",2024-07-23T14:19:21Z,,closed,0,0,https://github.com/jax-ml/jax/issues/22596
yi,Remove the unaccelerate_deprecation utility,"I don't think `unaccelerate_getattr_deprecation` is a pattern we should use, even if only internally. Instead, we should handle accelerated deprecations directly in the tests.",2024-07-22T19:34:27Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/22574
rag,Bump pytest from 8.1.1 to 8.3.1,"Bumps pytest from 8.1.1 to 8.3.1.  Release notes Sourced from pytest's releases.  8.3.1 pytest 8.3.1 (20240720) The 8.3.0 release failed to include the change notes and docs for the release. This patch release remedies this. There are no other changes. 8.3.0 pytest 8.3.0 (20240720) New features    CC(Fix `Cmd.use_rawinput` type in debuggers): Added [xfailtb]{.titleref} flag, which turns on traceback output for XFAIL results.  If the [xfailtb]{.titleref} flag is not given, tracebacks for XFAIL results are NOT shown. The style of traceback for XFAIL is set with [tb]{.titleref}, and can be [autono]{.titleref}. Note: Even if you have [xfailtb]{.titleref} set, you won't see them if [tb=no]{.titleref}.  Some history: With pytest 8.0, [rx]{.titleref} or [ra]{.titleref} would not only turn on summary reports for xfail, but also report the tracebacks for xfail results. This caused issues with some projects that utilize xfail, but don't want to see all of the xfail tracebacks. This change detaches xfail tracebacks from [rx]{.titleref}, and now we turn on xfail tracebacks with [xfailtb]{.titleref}. With this, the default [rx]{.titleref}/ [ra]{.titleref} behavior is identical to pre8.0 with respect to xfail tracebacks. While this is a behavior change, it brings default behavior back to pre8.0.0 behavior, which ultimately was considered the better course of action.    CC(未找到相关数据): Added support for keyword matching in marker expressions. Now tests can be selected by marker keyword arguments. Supported values are int{.interpretedtext role=&quot;class&quot;}, (unescaped) str{.interpretedtext role=&quot;class&quot;}, bool{.interpretedtext role=&quot;class&quot;} &amp; None{.interpretedtext role=&quot;data&quot;}. See marker examples &lt;marker_keyword_expression_example&gt;{.interpretedtext role=&quot;ref&quot;} for more information.  by lovetheguitar{.interpretedtext role=&quot;user&quot;}    CC(Replace `jax.xla.DeviceArray` private type with the new public type `jax.Array`.): Added nofoldskipped command line option. If this option is set, then skipped tests in short summary are no longer grouped by reason but all tests are printed individually with their nodeid in the same way as other statuses.  by pbrezina{.interpretedtext role=&quot;user&quot;}   Improvements in existing functionality    CC(scatter_mul autodiff bug): The console output now uses the &quot;thirdparty plugins&quot; terminology, replacing the previously established but confusing and outdated reference to setuptools &lt;setuptools:index&gt;{.interpretedtext role=&quot;std:doc&quot;}  by webknjaz{.interpretedtext role=&quot;user&quot;}.    CC([PAX] Flip jax_array to True for pax in McJAX world.),  CC(Propagate `name` through ExecuteReplicated for `dispatch.check_special`): Python virtual environment detection was improved by checking for a pyvenv.cfg{.interpretedtext role=&quot;file&quot;} file, ensuring reliable detection on various platforms  by zachsnickers{.interpretedtext role=&quot;user&quot;}.     ... (truncated)   Commits  de98446 Prepare release version 8.3.1 bd0a042 Merge pull request  CC(未找到相关数据) from pytestdev/updatereleasenotes 664325b doc/changelog: update 8.3.0 notes 19d225d Merge pull request  CC(Create `Array`s from `__getitem__` and `__iter__`. This is done by `device_put`ting from the host to default device which is suboptimal. But there is a TODO to fix this!) from pytestdev/release8.3.0 bc33028 Prepare release version 8.3.0 a7d5a8e Merge pull request  CC(jax.pmap argument in_axes can lead to error when passed a list) from x612skm/maintainence/11771pypy3.9bump ced7072 Add a change note for PR  CC(Treat all shardings on a single device as equivalent.) d42b76d Adjust test_errors_in_xfail_skip_expressions for PyPy 9eee45a Bump PyPy runtime to v3.9 @ GHA d489247 Fix caching of parameterized fixtures ( CC(Add jax_array coverage to debug_nans_test)) Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-07-22T17:08:58Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/22570,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
yi,Bump fonttools from 4.51.0 to 4.53.1,"Bumps fonttools from 4.51.0 to 4.53.1.  Release notes Sourced from fonttools's releases.  4.53.1  [feaLib] Improve the sharing of inline chained lookups ( CC(jaxpr typechecking: custom rules and errors)) [otlLib] Correct the calculation of OS/2.usMaxContext with reversed chaining contextual single substitutions ( CC(Clarify docs on jax.lax.cond.)) [misc.visitor] Visitors search the inheritance chain of objects they are visiting ( CC([jax2tf] Add support for custom JVP/VJP))  4.53.0  [ttLib.removeOverlaps] Support CFF table to aid in downconverting CFF2 fonts (fonttools/fonttools CC(WIP: Print JAX arrays to full precision)) [avar] Fix crash when accessing notyetexisting attribute (fonttools/fonttools CC(Fix eigh JVP to ensure that both the primal and tangents of the eigen…)) [docs] Add buildMathTable to otlLib.builder documentation (fonttools/fonttools CC(Restrict .compress tests to arrays only.)) [feaLib] Allow UTF8 with BOM when reading features (fonttools/fonttools CC(Bump jaxlib version to 0.1.49 and update WORKSPACE)) [SVGPathPen] Revert rounding coordinates to two decimal places by default (fonttools/fonttools CC(Avoid direct type/dtype comparisons to fix NumPy 1.19 deprecation war…)) [varLib.instancer] Refix output filename decisionmaking  (fonttools/fonttools CC(Remove warning suppression for tuple and list arguments to reductions.), fonttools/fonttools CC(Add physical optimization example), fonttools/fonttools CC(Fix typos ""Pytrees"" page on readthedocs))  4.52.4  [varLib.cff] Restore and deprecate convertCFFtoCFF2 that was removed in 4.52.0 release as it is used by downstream projects ( CC(Repro lazy issue)).  4.52.3 Fixed a small syntax error in the reStructuredTextformatted NEWS.rst file which caused the upload to PyPI to fail for 4.52.2. No other code changes. 4.52.2  [varLib.interpolatable] Ensure that scipy/numpy output is JSONserializable ( CC(summary statistics of jaxpr equations),  CC(fix an issue with newer versions of pytype)) [housekeeping] Regenerate table lists, to fix pyinstaller packaging of the new VARC table ( CC(avoid valuebased error check in random.choice),  CC([testdocs branch] Update JAX quickstart, test conf.py for proper Markdown support)) [cffLib] Make CFFToCFF2 and CFF2ToCFF more robust ( CC(Add class wrapper for doubledouble arithmetic),  CC(Fix typos: np.bool > np.bool_))  4.52.1 Fixed a small syntax error in the reStructuredTextformatted NEWS.rst file which caused the upload to PyPI to fail for 4.52.0. No other code changes. 4.52.0  Added support for the new VARC (Variable Composite) table that is being proposed to OpenType spec ( CC(tweak t logpdf tolerance for float64)). For more info: https://github.com/harfbuzz/boringexpansionspec/blob/main/VARC.md [ttLib.main] Fixed decompiling all tables (90fed08). [feaLib] Don't reference the same lookup index multiple times within the same feature record, it is only applied once anyway ( CC(Update JAX Quickstart with parallelization (`pmap`), other changes)). [cffLib] Moved methods to desubroutinize, remove hints and unused subroutines from subset module to cffLib ( CC(cumsum inaccuracy)). [varLib.instancer] Added support for partialinstancing CFF2 tables! Also, added method to downconvert from CFF2 to CFF 1.0, and CLI entry points to convert CFFCFF2 ( CC(K)). [subset] Prune unused user name IDs even with nameIDs='*' ( CC(Question: 1D convolutional network with stax.GeneralConv())). [ttx] use GNUstyle getopt to intermix options and positional arguments ( CC(pin numpy version in setup.py to avoid warnings)). [feaLib.variableScalar] Fixed value_at_location() method ( CC(refactor call primitives, simpler param processing)) [psCharStrings] Shorten output of encodeFloat ( CC(Fix test failure in jax2tf due to conflicting merges.)). [bezierTools] Fix infiniterecursion in calcCubicArcLength ( CC(Another small fix of the link rendering in the Autodiff Cookbook  vmap transformation)). [avar2] Implement avar2 support in TTFont.getGlyphSet() ( CC(DeviceArray's method .copy() returns np.array)).     Changelog Sourced from fonttools's changelog.  4.53.1 (released 20240705)  [feaLib] Improve the sharing of inline chained lookups ( CC(jaxpr typechecking: custom rules and errors)) [otlLib] Correct the calculation of OS/2.usMaxContext with reversed chaining contextual single substitutions ( CC(Clarify docs on jax.lax.cond.)) [misc.visitor] Visitors search the inheritance chain of objects they are visiting ( CC([jax2tf] Add support for custom JVP/VJP))  4.53.0 (released 20240531)  [ttLib.removeOverlaps] Support CFF table to aid in downconverting CFF2 fonts ( CC(WIP: Print JAX arrays to full precision)) [avar] Fix crash when accessing notyetexisting attribute ( CC(Fix eigh JVP to ensure that both the primal and tangents of the eigen…)) [docs] Add buildMathTable to otlLib.builder documentation ( CC(Restrict .compress tests to arrays only.)) [feaLib] Allow UTF8 with BOM when reading features ( CC(Bump jaxlib version to 0.1.49 and update WORKSPACE)) [SVGPathPen] Revert rounding coordinates to two decimal places by default ( CC(Avoid direct type/dtype comparisons to fix NumPy 1.19 deprecation war…)) [varLib.instancer] Refix output filename decisionmaking  ( CC(Remove warning suppression for tuple and list arguments to reductions.),  CC(Add physical optimization example),  CC(Fix typos ""Pytrees"" page on readthedocs))  4.52.4 (released 20240527)  [varLib.cff] Restore and deprecate convertCFFtoCFF2 that was removed in 4.52.0 release as it is used by downstream projects ( CC(Repro lazy issue)).  4.52.3 (released 20240527)  Fixed a small syntax error in the reStructuredTextformatted NEWS.rst file which caused the upload to PyPI to fail for 4.52.2. No other code changes.  4.52.2 (released 20240527)  [varLib.interpolatable] Ensure that scipy/numpy output is JSONserializable ( CC(summary statistics of jaxpr equations),  CC(fix an issue with newer versions of pytype)). [housekeeping] Regenerate table lists, to fix pyinstaller packaging of the new VARC table ( CC(avoid valuebased error check in random.choice),  CC([testdocs branch] Update JAX quickstart, test conf.py for proper Markdown support)). [cffLib] Make CFFToCFF2 and CFF2ToCFF more robust ( CC(Add class wrapper for doubledouble arithmetic),  CC(Fix typos: np.bool > np.bool_)).  4.52.1 (released 20240524)  Fixed a small syntax error in the reStructuredTextformatted NEWS.rst file which caused the upload to PyPI to fail for 4.52.0. No other code changes.  4.52.0 (released 20240524)  Added support for the new VARC (Variable Composite) table that is being proposed to OpenType spec ( CC(tweak t logpdf tolerance for float64)). For more info: https://github.com/harfbuzz/boringexpansionspec/blob/main/VARC.md    ... (truncated)   Commits  d3e68f2 Release 4.53.1 f676472 Test for visitor inheritance 3546b9f Search the object's MRO for visitors ee56bb0 Merge pull request  CC(Correction of a typo in the period of the PRNG.) from fonttools/pyupscheduledupdate20240701 7f473e1 Update uharfbuzz from 0.39.1 to 0.39.3 1b87761 Update scipy from 1.13.1 to 1.14.0 cf44801 Update reportlab from 4.2.0 to 4.2.2 221c518 Merge pull request  CC(Update docker script for CUDA 11 and libcudnn8.) from fonttools/dependabot/github_actions/pypa/ghact... 68be9ae Merge pull request  CC(Improve jnp.sum's error messages for tuple inputs) from fonttools/pyupscheduledupdate20240624 00f85a4 Update sympy from 1.12 to 1.12.1 Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-07-22T17:08:36Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/22569,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
yi,Bump exceptiongroup from 1.2.1 to 1.2.2,"Bumps exceptiongroup from 1.2.1 to 1.2.2.  Release notes Sourced from exceptiongroup's releases.  1.2.2  Removed an assert in exceptiongroup._formatting that caused compatibility issues with Sentry ( CC(remove unuzed bazel build rules, including bazel test definitions))     Changelog Sourced from exceptiongroup's changelog.  Version history This library adheres to Semantic Versioning 2.0 &lt;http://semver.org/&gt;_. 1.2.2  Removed an assert in exceptiongroup._formatting that caused compatibility issues with Sentry ( CC(remove unuzed bazel build rules, including bazel test definitions) &lt;https://github.com/agronholm/exceptiongroup/issues/123&gt;_)  1.2.1  Updated the copying of __notes__ to match CPython behavior (PR by CF BolzTereick) Corrected the type annotation of the exception handler callback to accept a BaseExceptionGroup instead of BaseException Fixed type errors on Python &lt; 3.10 and the type annotation of suppress() (PR by John Litborn)  1.2.0  Added special monkeypatching if Apport &lt;https://github.com/canonical/apport&gt;_ has overridden sys.excepthook so it will format exception groups correctly (PR by John Litborn) Added a backport of contextlib.suppress() from Python 3.12.1 which also handles suppressing exceptions inside exception groups Fixed bare raise in a handler reraising the original naked exception rather than an exception group which is what is raised when you do a raise in an except* handler  1.1.3  catch() now raises a TypeError if passed an async exception handler instead of just giving a RuntimeWarning about the coroutine never being awaited. ( CC(Conda installations), PR by John Litborn) Fixed plain raise statement in an exception handler callback to work like a raise in an except* block Fixed new exception group not being chained to the original exception when raising an exception group from exceptions raised in handler callbacks Fixed type annotations of the derive(), subgroup() and split() methods to match the ones in typeshed  1.1.2  Changed handling of exceptions in exception group handler callbacks to not wrap a single exception in an exception group, as per CPython issue 103590 &lt;https://github.com/python/cpython/issues/103590&gt;_  1.1.1  Worked around    ... (truncated)   Commits  2399d54 Added the release version bec9651 Removed problematic assert that caused compatibility issues f3f0ff6 Updated Ruff configuration bb43ee0 Fixed formatting tests failing on Python 3.13 eb8fbbc [precommit.ci] precommit autoupdate ( CC(CUDA backend produces inconsistent results for jax.numpy.linalg.inv)) 6ff8300 [precommit.ci] precommit autoupdate ( CC(lax_linalg.triangular_solve gives RuntimeError for largeish inputs on CPU)) 761933f [precommit.ci] precommit autoupdate ( CC(Implement the JVP of the QR decomposition)) 1b43294 [precommit.ci] precommit autoupdate ( CC(Improper kwarg forwarding for np.mean)) dd87018 [precommit.ci] precommit autoupdate ( CC(array() was a noop on scalar types)) 54d8b8d [precommit.ci] precommit autoupdate ( CC(Scalars passed into np.array should return 0dim arrays)) Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-07-22T17:07:49Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/22564,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
yi,"When logging cache hit/miss, include cache_key","I want these log messages: ``` Persistent compilation cache hit for 'jit_scan_outer_bigs' ``` To look like this: ``` Persistent compilation cache hit for 'jit_scan_outer_bigs' with cache_key `b86d2a45745d2417b7a28b1c38aa39497bf611e8320238cabe31e99142be5f2d` ``` Optionally we can truncate the cache key. I implemented this feature, let me know if you want a pull request. ",2024-07-21T09:00:25Z,enhancement,closed,0,3,https://github.com/jax-ml/jax/issues/22548,Thank you! Please make a pull request,"I like your proposal  just let me note here that you can already log cache keys (including info about its calculation): ``` import logging logging.getLogger(""jax._src.cache_key"").setLevel(logging.DEBUG) ```",sevcik Thank you. However these log lines do not contain the module name which makes them a bit difficult to search for. 
yi,JAX unable to match NVIDIA driver version," Description Similar to this issue and this discussion, I'm on a HPC cluster where the NVIDIA GPU's have CUDA 12.4. When I try to install jax with `pip install jax[cuda12]` or with `pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html`, I get a warning suggesting that JAX is not heeding the 12.4 driver version, and is instead trying to use CUDA 12.5: ``` 20240719 09:58:22.022601: W external/xla/xla/service/gpu/nvptx_compiler.cc:765] The NVIDIA driver's CUDA  version is 12.4 which is older than the ptxas CUDA version (12.5.82). Because the driver is older than th e ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update  your NVIDIA driver or use the NVIDIAprovided CUDA forward compatibility packages. ``` I understand that this isn't blocking, but I don't relish the idea of randomly slowed compilation times. Is there a reason JAX is unable to match the NVIDIA driver? Note: I have been able to get JAX to work with my cluster's provided CUDA 12.1 drivers by using the following pip args: ``` f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html jax[cuda12_cudnn89]==0.4.14 jaxlib==0.4.14+cuda12.cudnn89 ``` But I anticipate that sooner or later I will want to use a JAX version that is newer than what my cluster provides. Thanks!  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.30 jaxlib: 0.4.30 numpy:  2.0.0 python: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] jax.devices (1 total, 1 local): [cuda(id=0)] process_count: 1 platform: uname_result(system='Linux', node='computeg17155.o2.rc.hms.harvard.edu', release='3.10.01160.118.1.el7.x86_64', version=' CC(Python 3 compatibility issues) SMP Wed Apr 24 16:01:50 UTC 2024', machine='x86_64') $ nvidiasmi Fri Jul 19 10:41:24 2024 ++  ++",2024-07-19T14:51:14Z,bug,open,8,2,https://github.com/jax-ml/jax/issues/22534,Having the same issues here. Any idea?,"It is my understanding that the cuda forward compatibility exist for such reasons installingcompatpackages So even though I haven't tried it, installing the necessary compat and then changing `LD_LIBRARY_PATH` before pip installing should do it."
yi,Could not create CUDNN handle: CUDNN_STATUS_INTERNAL_ERROR," Description Hello im having an issue using jax and cuda. My error message looks like this: 20240719 15:49:01.341533: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:439] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR 20240719 15:49:01.341571: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:443] Memory usage: 2624061440 bytes free, 12524191744 bytes total. Traceback (most recent call last):   File ""scripts/train_vqgan.py"", line 202, in      main()   File ""scripts/train_vqgan.py"", line 32, in main     rng = jax.random.PRNGKey(config.seed)   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/random.py"", line 160, in PRNGKey     key = prng.seed_with_impl(impl, seed)   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/prng.py"", line 406, in seed_with_impl     return random_seed(seed, impl=impl)   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/prng.py"", line 690, in random_seed     return random_seed_p.bind(seeds_arr, impl=impl)   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/prng.py"", line 702, in random_seed_impl     base_arr = random_seed_impl_base(seeds, impl=impl)   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/prng.py"", line 707, in random_seed_impl_base     return seed(seeds)   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/prng.py"", line 936, in threefry_seed     return _threefry_seed(seed)   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/pjit.py"", line 250, in cache_miss     outs, out_flat, out_tree, args_flat, jaxpr = _python_pjit_helper(   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/pjit.py"", line 163, in _python_pjit_helper     out_flat = pjit_p.bind(*args_flat, **params)   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/core.py"", line 2677, in bind     return self.bind_with_trace(top_trace, args, params)   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/core.py"", line 383, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/core.py"", line 815, in process_primitive     return primitive.impl(*tracers, **params)   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/pjit.py"", line 1203, in _pjit_call_impl     return xc._xla.pjit(name, f, call_impl_cache_miss, [], [], donated_argnums,   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/pjit.py"", line 1187, in call_impl_cache_miss     out_flat, compiled = _pjit_call_impl_python(   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/pjit.py"", line 1120, in _pjit_call_impl_python     compiled = _pjit_lower(   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/interpreters/pxla.py"", line 2323, in compile     executable = UnloadedMeshExecutable.from_hlo(   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/interpreters/pxla.py"", line 2645, in from_hlo     xla_executable, compile_options = _cached_compilation(   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/interpreters/pxla.py"", line 2555, in _cached_compilation     xla_executable = dispatch.compile_or_get_cached(   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 497, in compile_or_get_cached     return backend_compile(backend, computation, compile_options,   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 465, in backend_compile     return backend.compile(built_c, compile_options=options) jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""scripts/train_vqgan.py"", line 202, in      main()   File ""scripts/train_vqgan.py"", line 32, in main     rng = jax.random.PRNGKey(config.seed)   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/random.py"", line 160, in PRNGKey     key = prng.seed_with_impl(impl, seed)   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/prng.py"", line 406, in seed_with_impl     return random_seed(seed, impl=impl)   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/prng.py"", line 690, in random_seed     return random_seed_p.bind(seeds_arr, impl=impl)   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/core.py"", line 380, in bind     return self.bind_with_trace(find_top_trace(args), args, params)   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/core.py"", line 383, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/core.py"", line 815, in process_primitive     return primitive.impl(*tracers, **params)   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/prng.py"", line 702, in random_seed_impl     base_arr = random_seed_impl_base(seeds, impl=impl)   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/prng.py"", line 707, in random_seed_impl_base     return seed(seeds)   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/prng.py"", line 936, in threefry_seed     return _threefry_seed(seed) jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. I already saw an issue about this which was already closed but after trying anything that was mentioned there i still could not get it to run. I have tried the following things already:   pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html  pip install forcereinstall ""jax[cuda11_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html  XLA_PYTHON_CLIENT_PREALLOCATE=false  XLA_PYTHON_CLIENT_MEM_FRACTION=.05  XLA_PYTHON_CLIENT_ALLOCATOR=platform none of those changed the error message, I always got the error above no matter what i did unfortunately. For System Information and further setup information see below. I hope someone can help me because this is of really really high importance to me as I need to use Jax for an important project. I would appreciate any help i can get. The information on hardware below is the setup i have sudo access to and since i get the same error there i was hoping to be able to fix it there and see what i can do after that because the actual goal is to run this on a server i dont have sudo access to so things like reinstalling cuda stuff wont be possible.  System info (python version, jaxlib version, accelerator, etc.)  Hardware GPU: RTX 3060 12GB CUDA version 12.5 RAM: 16GB  Python Version: 3.8 Jax version: 0.4.13 >>> jax.print_environment_info() jax:    0.4.13 jaxlib: 0.4.13 numpy:  1.24.4 python: 3.8.19  ++",2024-07-19T14:07:38Z,bug,closed,0,0,https://github.com/jax-ml/jax/issues/22533
rag,[Mosaic GPU] Add reverse arithmetic functions for FragmentedArrary for convenience,[Mosaic GPU] Add reverse arithmetic functions for FragmentedArrary for convenience,2024-07-19T13:32:47Z,,closed,0,0,https://github.com/jax-ml/jax/issues/22531
yi,jaxlib is not matching with cuda version 12. ,"I am trying to use GPU to run scripts but consistently the errors is coming ""ERROR: Could not find a version that satisfies the requirement jaxlib==0.4.29+cuda12.cudnn88 (from versions: 0.1.75+cuda11.cudnn805, 0.1.75+cuda11.cudnn82, 0.1.76+cuda11.cudnn805, 0.1.76+cuda11.cudnn82, etc"" after pip install pip install upgrade jax jaxlib==0.4.30+cuda12.cudnn89 f https://github.com/google/jaxpipinstallationgpucuda  [ ] Check for duplicate requests.  [ ] Describe your goal, and if possible provide a code snippet with a motivating example.",2024-07-18T23:44:26Z,enhancement,closed,0,2,https://github.com/jax-ml/jax/issues/22522,"Please follow the installation instructions: https://jax.readthedocs.io/en/latest/installation.htmlinstallingjax ``` pip install jax[cuda12] ``` If that doesn't work, you'll need to provide more details of your Python installation. What Python version, for example?","Yes, it's worked, Thank you so much!!"
yi,`custom_jvp` functions are slow when used with `jit`," Description I want to define a function such that one of the input arguments is the derivative order that the user wants. My actual function is far more complex but here is an example that has the same issue. ```python import jax import jax.numpy as jnp from jax.lax import fori_loop .custom_jvp .jit def fun(x, n, dx):     """"""The dx th derivative of the function x^n.""""""     out = x**(ndx)     coef = fori_loop(0, dx, lambda i, x: x*(ni), 1)     return coef*out .defjvp def _fun_jvp(a, adot):     """"""Custom derivative rule for the function.     This is just the same function called with dx+1.     """"""     (x, n, dx) = a     (xdot, ndot, dxdot) = adot     f = fun(x, n, dx)     df = fun(x, n, dx+1)     return f, df .jit def fun2(x, n, dx):     """"""Same function without custom_jvp decorator.""""""     out = x**(ndx)     coef = fori_loop(0, dx, lambda i, x: x*(ni), 1)     return coef*out x = jnp.arange(1000) n = 10 dx = 0 assert fun(x, n, dx).all() == fun2(x, n, dx).all() %timeit fun(x, n, dx).block_until_ready() %timeit fun2(x, n, dx).block_until_ready() ``` ```bash 73 μs ± 1.05 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each) 8.6 μs ± 1.26 μs per loop (mean ± std. dev. of 7 runs, 100,000 loops each) ``` This function calculates `dx`th derivative of $x^n$. So, $$dx = 0: f(x)=x^n $$ $$dx = 1: f(x)=nx^{n1} $$ $$dx = 2: f(x)=n(n1)x^{n2}$$ I don't want to use `grad()` to take the derivative. However, when I define the custom derivative to be the same function called with `dx+1`, the execution becomes way slower (even if I call the original function `dx`=0). I assume this is due to `custom_jvp` being a class and this adds some extra stuff being done when that function is called, even if we call it multiple times.  The speed is the key for my application, but I also need to define a custom derivative rule for the function for jacobian calculations involving this function. Is there a more optimized way to do that?  System info (python version, jaxlib version, accelerator, etc.) ```bash jax:    0.4.28 jaxlib: 0.4.28 numpy:  1.26.4 python: 3.11.0 (main, Mar  1 2023, 18:26:19) [GCC 11.2.0] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='Yigit', release='6.5.01025oem', version=' CC(fix symbolic zero handling in concat transpose)Ubuntu SMP PREEMPT_DYNAMIC Tue Jun 18 12:35:22 UTC 2024', machine='x86_64') ```",2024-07-18T15:21:09Z,bug,closed,0,9,https://github.com/jax-ml/jax/issues/22513,"The issue here has to do with where the `jax.jit` is applied. When defining `fun`, the `jit` decorator is currently _inside_ the `custom_jvp`. This means that the full computation isn't being JIT compiled. To fix this issue, you can update the code as follows: ```python  JIT compile `fun` to remove all the overhead of `custom_jvp` fun = jax.jit(fun)  It is useful to warm up the cache fun(x, n, dx).block_until_ready() fun2(x, n, dx).block_until_ready() %timeit fun(x, n, dx).block_until_ready() %timeit fun2(x, n, dx).block_until_ready() ``` On my system I now see roughly the same performance. Note: For somewhat subtle reasons you can't actually just use: ```python .jit .custom_jvp def fun(x, n, dx):   ... ``` You need to apply the `jit` after defining the JVP function. Hope this helps!","Thank you very much ! I had also tried to put `jax.jit` before `custom_jvp` but I got the following error, ```bash  AttributeError                            Traceback (most recent call last) Cell In[2], line 13      10     coef = fori_loop(0, dx, lambda i, x: x*(ni), 1)      11     return coef*out > 13 .defjvp      14 def _fun_jvp(a, adot):      15     """"""Custom derivative rule for the function.      16           17     This is just the same function called with dx+1.      18     """"""      19     (x, n, dx) = a AttributeError: 'jaxlib.xla_extension.PjitFunction' object has no attribute 'defjvp' ``` I can apply `jax.jit` in my project probably as you suggest. But it would be perfect to be able to change the order and add jit as decorator instead of jitting manually.","Great! I'd say that supporting the other order of decorators is pretty low priority since `custom_jvp` functions typically don't live at the top level of a program. In other words, normally you call `custom_jvp` functions deep within some larger function. You'll typically get the most performance improvement by `jit`ting at the outermost scope. Therefore, I don't think we're likely to add support for calling `defjvp` on a `jit` function. For your use case, I don't think it adds too much overhead to add that one line!","I'm going to close this now because I think we solved the main issue. As I mentioned, I think the feature request is unlikely to be executed, but please feel free to open a new feature request with more details if it becomes a blocker.","  My actual function uses `static_argnums` with `.partial(jit, static_argnums=3)`. For some reason, that caused a problem with `jit`ing the function by ```python fun = jax.jit(fun, static_argnums=3) ``` Instead I had to, ```python fun = jax.jit(fun.fun, static_argnums=3) ``` Is there something I am missing?","What was the issue you were seeing? I believe that `jax.jit(fun, static_argnums=3)` should work fine, and `jax.jit(fun.fun, static_argnums=3)` definitely isn't what you want because then you'll lose the `custom_jvp`! Can you post a minimal reproducer and the specific error you're seeing?","Let's say I added a redundant `if` statement, and now I need to specify `static_argnums` (my actual code is a bit more complex, but have similar problem). ```python import jax import jax.numpy as jnp from jax.lax import fori_loop import functools .custom_jvp .partial(jax.jit, static_argnums=2) def fun(x, n, dx):     """"""The dx th derivative of the function x^n.""""""     if dx == 0:         return x**n     out = x**(ndx)     coef = fori_loop(0, dx, lambda i, x: x*(ni), 1)     return coef*out .defjvp def _fun_jvp(a, adot):     """"""Custom derivative rule for the function.     This is just the same function called with dx+1.     """"""     (x, n, dx) = a     (xdot, ndot, dxdot) = adot     f = fun(x, n, dx)     df = fun(x, n, dx+1)     return f, df .partial(jax.jit, static_argnums=2) def fun2(x, n, dx):     """"""Same function without custom_jvp decorator.""""""     if dx == 0:         return x**n     out = x**(ndx)     coef = fori_loop(0, dx, lambda i, x: x*(ni), 1)     return coef*out x = jnp.arange(1000) n = 10 dx = 2 fun = jax.jit(fun, static_argnums=2)  assert fun(x, n, dx).all() == fun2(x, n, dx).all() %timeit fun(x, n, dx).block_until_ready() %timeit fun2(x, n, dx).block_until_ready() ``` The error message now is ```bash [... skipping hidden 28 frame] Cell In[8], line 10       6 .custom_jvp       7 .partial(jax.jit, static_argnums=2)       8 def fun(x, n, dx):       9     """"""The dx th derivative of the function x^n."""""" > 10     if dx == 0:      11         return x**n      12     out = x**(ndx)     [... skipping hidden 1 frame] File ~/miniconda3/envs/descenv11/lib/python3.11/sitepackages/jax/_src/core.py:1475, in concretization_function_error..error(self, arg)    1474 def error(self, arg): > 1475   raise TracerBoolConversionError(arg) TracerBoolConversionError: Attempted boolean conversion of traced array with shape bool[].. The error occurred while tracing the function fun at /tmp/ipykernel_22677/578150583.py:6 for jit. This value became a tracer due to JAX operations on these lines:   operation a:bool[] = eq b c     from line /tmp/ipykernel_22677/578150583.py:10:7 (fun) ``` And, yes, I realized that I was losing `custom_jvp` right after I sent the comment :)","Ah so in this case the JIT isn't the problem. You need to label that parameter also as nondifferentiable: ```python .partial(jax.custom_jvp, nondiff_argnums=(2,)) def fun(x, n, dx):   ... ``` You could keep the inner jit, but I don't think it adds any value.","Oh thank you very much! It works now! For practicality, I still want to make a decorator that `jit`s and `custom_jvp`s the function. Here is my version, ```python import jax import jax.numpy as jnp from jax.lax import fori_loop import functools def custom_jvp_with_jit(func):     .partial(jax.custom_jvp, nondiff_argnums=(2,))     def dummy(x, n, dx):         return func(x, n, dx)     .defjvp     def _dummy_jvp(nondiff_dx, a, adot):         """"""Custom derivative rule for the function.         This is just the same function called with dx+1.         """"""         (x, n) = a         (xdot, ndot) = adot         f = dummy(x, n, nondiff_dx)         df = dummy(x, n, nondiff_dx+1)         return f, df     return jax.jit(dummy, static_argnums=2)  def fun(x, n, dx):     """"""The dx th derivative of the function x^n.""""""     if dx == 0:         return x**n     out = x**(ndx)     coef = fori_loop(0, dx, lambda i, x: x*(ni), 1)     return coef*out ``` I hope it doesn't have any more problems!!! EDIT: I had to change `.defjvp` to comply with this tutorial."
yi,[pallas] Request to add support for select/argmax primitive in Pallas TPU,"Currently, it's not possible to use jnp.select in pallas kernel or in BlockSpec. Trying to do so causes the follow error. `jax._src.source_info_util.JaxStackTraceBeforeTransformation: NotImplementedError: Unimplemented primitive in Pallas TPU lowering: argmax. Please file an issue on https://github.com/google/jax/issues.` Using select allows better conditional control within blockspec. There's certain use case for condition branch, especially when selecting index. ",2024-07-18T11:41:52Z,enhancement pallas,open,1,1,https://github.com/jax-ml/jax/issues/22508,"I'd like to add that using cond is an usable solution, but it's longer and uglier. "
rag,Add jvp and transpose rule for ragged dot.,"Add jvp and transpose rule for ragged dot. The numerical accuracy test is perfect against the reference implementation, and somewhat loose against the alt grad implementation used for testing.",2024-07-18T06:36:01Z,,closed,0,0,https://github.com/jax-ml/jax/issues/22504
yi,jax_enable_x64 environment variable not respected by jax.numpy.indices," Description Even when jax_enable_x64 is enabled, jnp.indices() continues to use int32. The issue is more relevant when the output is fed into (for instance) vmap, which does not promote to int64.  Here is a case where overflow occurs for a reason that is not obvious just looking at the code. ``` python jax.config.update(""jax_enable_x64"", True) indices = jnp.indices((32,)) powers = vmap(lambda x: 2**x)(indices) ``` Output: ``` python [[          1           2           4           8          16          32            64         128         256         512        1024        2048          4096        8192       16384       32768       65536      131072        262144      524288     1048576     2097152     4194304     8388608      16777216    33554432    67108864   134217728   268435456   536870912    1073741824 2147483648]] ``` It is still possible to force int64 by instead using ``` python indices = jnp.indices((32,), dtype=jnp.int64) ``` Output: ``` python [[         1          2          4          8         16         32           64        128        256        512       1024       2048         4096       8192      16384      32768      65536     131072       262144     524288    1048576    2097152    4194304    8388608     16777216   33554432   67108864  134217728  268435456  536870912   1073741824 2147483648]] ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.30 jaxlib: 0.4.30 numpy:  1.26.4 python: 3.11.7  (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Windows', node='LAPTOP0OR5UP2L', release='10', version='10.0.19045', machine='AMD64') ```",2024-07-17T23:05:44Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/22501,Thanks for the report! I do think this is a bug and I've implemented a possible fix in CC(Fix dtype canonicalization in `jnp.indices`). We'll see if any other members of the team know of a reason why this isn't right!
yi,Cannot execute custom operation with XLA typed FFI," Description I am attempting to add a custom operation using the typed (rather than untyped( XLA FFI api. However, I get a warning/error when trying to use it that that the symbol cannot be found: ``` simple_orc_jit.cc:433] Unable to resolve runtime symbol: 'my_func'. Hint: if the symbol a custom call target, make sure you've registered it with the JIT using XLA_CPU_REGISTER_CUSTOM_CALL_TARGET. JIT session error: Symbols not found: [ my_func ] ``` Unlike in the examples, my custom operation uses the typed interface like  ``` xla::ffi::Error my_func(    xla::ffi::Buffer buf0,    xla::ffi::Buffer buf1,     xla::ffi::Result> out) { ... } XLA_FFI_DEFINE_HANDLER_SYMBOL(     my_func_handler, my_func,     xla::ffi::Ffi::Bind()         .Arg>()         .Arg>()         .Ret>()); ``` In another C++ file: ``` PYBIND11_MODULE(my_func_mod, m) {   m.def(""my_func_registrations"", []() {     pybind11::dict dict;     dict[""my_func""] = pybind11::capsule(reinterpret_cast(my_func_handler));     return dict;   }); } ``` In python (some boilerplate hidden): ``` for _name, _value in my_func_mod.my_func_registrations().items():     xla_client.register_custom_call_target(_name, {""execute"": _value}, platform=""cpu"", api_version=1) def _my_func_lowering(ctx, a, b):     a_type = ir.RankedTensorType(a.type)     b_type = ir.RankedTensorType(b.type)     return custom_call(         'my_func',         result_types=[a_type],         operands=[a, b],         operand_layouts=default_layouts(a_type.shape, b_type.shape),         result_layouts=default_layouts(a_type.shape),         api_version=xla_client.ops.CustomCallApiVersion.API_VERSION_TYPED_FFI     ).results ``` Reading the XLA source suggests that `SimpleOrcJIT` looks up custom symbols in `xla::CustomCallTargetRegistry`, which is only populated for `api_version == 0` in `PyRegisterCustomCallTarget`.  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.30 jaxlib: 0.4.30 numpy:  2.0.0 ```",2024-07-17T20:23:25Z,bug,closed,0,8,https://github.com/jax-ml/jax/issues/22499,"Thanks for the report. This is absolutely expected to work! I'm currently writing up a tutorial about this (see https://github.com/google/jax/pull/22095) and I haven't had any issues running ~equivalent code on CPU with all the same versions of JAX that you're using. I'll see if I can dig into this a bit, but how about you take a look at the draft of my tutorial and see if you notice anything useful there. Everything you've included as sample code here looks right to me on a first pass!",Also I confirmed the untyped ffi works correctly with the same registration code and changing the api version. Do I need `XLA_FFI_REGISTER_HANDLER` as well? I took a look at the tutorial and it uses `jax.extend.ffi.ffi_call`. I can't find this in my distribution  is it new? ,"You shouldn't need to call `XLA_FFI_REGISTER_HANDLER`. The references to that macro in `jaxlib` are unused  they're only for users who use `jaxlib` without the Python frontend. The only way to register user FFI calls is via Python. > I took a look at the tutorial and it uses `jax.extend.ffi.ffi_call`. I can't find this in my distribution  is it new? I thought it got into 0.4.30, but it looks like I actually missed that cutoff (here's the PR: https://github.com/google/jax/pull/21925). It will be included in the next release!","I wanted to check in to confirm that you were able to get the code working, . Is that right? I'm still not totally sure what the problem was before, but if you have something that works for now, let's close the issue and you can open a new one if you run into more problems. These APIs might still have some rough edges and I'm keen to get them sorted out!",No I wasn't able to get it working. I switched to the untyped api which works fine. Once the next release is out I can test it again with `ffi_call`.,Oh I see. I don't expect that `ffi_call` will solve this issue though. Can you put together a complete minimal example (i.e. the full C++ and Python files and tell me how you're compiling it) so that I can reproduce the issue locally?, — I wanted to check back in here to see if you managed to get to the bottom of this?,"I'm going to close this as stale, but please comment if you continue to run into issues!"
yi,"look mum I can still can edit Pallas, AGI! Optimize pipeline emitter scheduler by omitting copies of accumulators during iteration in which they are going to be zeroed out.","look mum I can still can edit Pallas, AGI! Optimize pipeline emitter scheduler by omitting copies of accumulators during iteration in which they are going to be zeroed out. Also, add some clarifying comments and set fixed RHS schedules of matmul reduce scatter implementations.",2024-07-17T17:56:56Z,,closed,0,0,https://github.com/jax-ml/jax/issues/22496
rag,[MosaicGPU] Add a __repr__ to FragmentedArray so pallas:mosaic_gpu errors are more readable.,[MosaicGPU] Add a __repr__ to FragmentedArray so pallas:mosaic_gpu errors are more readable.,2024-07-17T16:44:26Z,,closed,0,0,https://github.com/jax-ml/jax/issues/22492
yi,Add Pallas helpers for verifying complicated kernels,"Add Pallas helpers for verifying complicated kernels This change adds two capabilities that should allow us to verify existing complicated kernels without having to modify their code significantly. 1. Kernels that contain complicated patterns such as datadependent controlflow    cannot currently be modeled. But, in many cases the controlflow does not    actually matter (e.g. it's only used to cut down on unnecessary work and    should not influence communication). Now, `pltpu.verification.assume(x, when_verifying=y)`    will normally evaluate to `x`, but will be replaced by `y` in the model.    This allows us to e.g. replace dynamic values with constants. 2. We might not be interested in exporting all the details of a kernel to the    model. For example, there is little point in verifying that the local DMA    pipeline generated by `pltpu.emit_pipeline` has no races. Now, using    `pltpu.verification.define_model` a function that contains a pipeline    can be replaced by a simplified implementation that e.g. specifies the    set of read and written memory references using `pltpu.verification.pretend`.",2024-07-17T14:49:33Z,,closed,0,0,https://github.com/jax-ml/jax/issues/22491
yi,DCT: add support for `orthogonalize=False`,"> On the topic of `norm=""ortho""`, please also be aware of https://github.com/scipy/scipy/issues/21198. Unlike, scipy, JAX doesn't provide the `orthogonalize=False` option. This makes me a little skeptical of using JAX's DCT (even for type 2 DCT). The two linked issues shown there only discuss the effect on the type 1 DCT, but I think the same issues should occur to types 1,2,3. > Ideally, I'd set `norm=""backward""` to avoid this issue, but given that JAX only implements `norm=""ortho""` I think JAX should implement an option to set `orthogonalize=False`. _Originally posted by  in https://github.com/google/jax/issues/22466issuecomment2231562983_",2024-07-16T18:57:31Z,enhancement,open,0,1,https://github.com/jax-ml/jax/issues/22476,"To summarize, for DCT and DST. the `ortho` option is presented as a normalization flag, `norm=""ortho""`, and this seems misleading because this option changes the transformation to not behave like the familiar FFT anymore. Scipy later added the the flag `orthogonalize=False` to try to fix this. For my work, I need `orthogonalize=False`, but JAX doesn't have that option implemented."
yi,Wrong normalization in FFT methods," Description Summary: The discrete cosine transformation libraries normalize the transforms incorrectly. I am aware that JAX's documentation states that only `norm=""ortho""` is supported, however, this still requires the user to explicitly pass `norm=ortho`, which the documentation does not state. To be clear, jax gives incorrect results unless `norm=""ortho""` is passed explicitly. Jax also doesn't seem to implement backward dct and forward idct correctly. If this is not supported yet, then JAX should fail early. The effect of these bugs is that interpolation with fft's without specifying additional flags gives incorrect results. Minimal reproductions. Bug 1. ```python import numpy as np from scipy.fftpack import dct as sdct from scipy.fftpack import idct as sidct from jax.scipy.fft import dct as jdct from jax.scipy.fft import idct as jidct import matplotlib.pyplot as plt m = np.linspace(0, 2 * np.pi, 100) n = np.linspace(0, 2 * np.pi, 5 * m.size) f = m normalize = np.sqrt(n.size / m.size) sf = sidct(sdct(f, norm=""ortho""), n=n.size, norm=""ortho"") * normalize jf = jidct(jdct(f, norm=""ortho""), n=n.size, norm=""ortho"") * normalize  Doesn't implement ortho correctly unless specifed. Documentation says only ortho is supported. Bug 1. jf_bug = jidct(jdct(f), n=n.size) * normalize plt.scatter(m, f, label=""truth"", color=""black"") plt.plot(n, sf, linestyle="""", linewidth=4, label=""ortho scipy"") plt.plot(n, jf, linestyle="""", linewidth=2.5, label=""ortho jax"") plt.plot(n, jf_bug, linestyle="""", linewidth=2.5, label=""ortho jax bug"") plt.legend() ``` !out Ok, so now the user wonders whether the documentation is out of date, and tries to find out what is the normalization that JAX implements by default. They may conclude that default is `backward` on `dct` and `forward` on `idct`. Scipy's default is `backward` for both. This departure from `scipy` seems confusing given that this methods are in `jax.scipy.fft`. More confusing, is that when we specify explicitly to do `backward` on `dct` and `forward` on `idct` for `jax`, the results change, so it seems JAX does not implement the options that is does by default correctly when they are specified explicitly. Bug 2. ```python import numpy as np from scipy.fftpack import dct as sdct from scipy.fftpack import idct as sidct from jax.scipy.fft import dct as jdct from jax.scipy.fft import idct as jidct import matplotlib.pyplot as plt m = np.linspace(0, 2 * np.pi, 100) n = np.linspace(0, 2 * np.pi, 5 * m.size) f = m normalize = n.size / m.size sf = sidct(sdct(f, norm=""backward""), n=n.size, norm=""forward"") * normalize jf_bug = jidct(jdct(f), n=n.size) * normalize   matches sf, so must be doing backward then forward  But this does not match the above 2. jf_bug_2 = jidct(jdct(f, norm=""backward""), n=n.size, norm=""forward"") * normalize plt.scatter(m, f, label=""truth"", color=""black"") plt.plot(n, sf, linestyle='', linewidth=4, label=""scipy backward forward"")  This plot matches the truth, but only because we multiplied by ""normalize"",  if it was doing ortho then it should only match if we multiply by sqrt(normalize),  hence the bug plt.plot(n, jf_bug, linestyle='', linewidth=2.5, label=""ortho jax bug"")  If jax doesn't support backward and forward, then this should fail early with an error. plt.plot(n, jf_bug_2, linestyle='', linewidth=2.5, label=""backward forward jax bug 2"") plt.legend() ``` !out2. Please let me know if I am missing something.  System info (python version, jaxlib version, accelerator, etc.) ``` An NVIDIA GPU may be present on this machine, but a CUDAenabled jaxlib is not installed. Falling back to cpu. jax:    0.4.28 jaxlib: 0.4.28 numpy:  1.26.4 python: 3.10.14  (main, Mar 20 2024, 12:45:18) [GCC 12.3.0] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='fedora', release='6.9.8200.fc40.x86_64', version=' CC(Python 3 compatibility issues) SMP PREEMPT_DYNAMIC Fri Jul  5 16:20:11 UTC 2024', machine='x86_64') ```",2024-07-15T22:15:24Z,bug,closed,2,11,https://github.com/jax-ml/jax/issues/22466,"For the second example, in the newer `scipy.fft` libraries, the equivalent of ```python  with scipy.fftpack normalize = n.size / m.size sf = sidct(sdct(f, norm=""backward""), n=n.size, norm=""forward"") * normalize ``` becomes ```python  with scipy.fft normalize = n.size / m.size sf = sidct(sdct(f, norm=""backward""), n=n.size, norm=""backward"") * normalize ``` Still, replacing ```python jf_bug_2 = jidct(jdct(f, norm=""backward""), n=n.size, norm=""forward"") * normalize ``` with  ```python jf_bug_2 = jidct(jdct(f, norm=""backward""), n=n.size, norm=""backward"") * normalize ``` does not change any issues raised in the original post.",Thanks for reporting this so clearly!,"It's unrelated to this issue, but on the topic of `norm=""ortho""`, please also be aware of https://github.com/scipy/scipy/issues/21198. Unlike, scipy, JAX doesn't provide the `orthogonalize=False` option. This makes me a little skeptical of using JAX's DCT (even for type 2 DCT). The two linked issues shown there only discuss the effect on the type 1 DCT, but I think the same issues should occur to types 1,2,3. Ideally, I'd set `norm=""backward""` to avoid this issue, but given that JAX only implements `norm=""ortho""` I think JAX should implement an option to set `orthogonalize=False`. Should this comment be referenced in a new issue?","> Should this comment be referenced in a new issue? Yes, new issue please!",Hello everyone. I have recently started using JAX and interested to contribute in this community.  can I start working on this issue? Is there anything you would suggest me before starting on it?,Thanks for the report. I've added validation that raises an error for unsupported `norm` values in CC(jax.scipy.fft: error for unsupported norm argument). As for the incorrect results with `norm=None`: this is strange because we have a suite of tests against scipy's implementation for the default `norm=None`; for example https://github.com/google/jax/blob/db05734041a6b6c26440b087a22c036825665dc2/tests/scipy_fft_test.pyL48L62 There must be some relevant case in that test parameterization that is not covered.,"Ah, it looks like our implementation does match that of `scipy.fft`, although it does not match `scipy.fftpack`: ```python import numpy as np from scipy.fft import dct as sdct from scipy.fft import idct as sidct from jax.scipy.fft import dct as jdct from jax.scipy.fft import idct as jidct import matplotlib.pyplot as plt m = np.linspace(0, 2 * np.pi, 100) n = np.linspace(0, 2 * np.pi, 5 * m.size) f = m normalize = np.sqrt(n.size / m.size) jf_none = jidct(jdct(f), n=n.size) * normalize sf_none = sidct(sdct(f), n=n.size) * normalize plt.plot(n, sf_none, linestyle="""", linewidth=4, label=""ortho scipy"") plt.plot(n, jf_none, linestyle="""", linewidth=2.5, label=""ortho jax"") plt.legend() ``` !download I believe that `scipy.fftpack` is deprecated, so you should be using the `scipy.fft` versions. With that in mind, I think the only bug here involves validation of unsupported `norm` arguments, which is fixed in CC(jax.scipy.fft: error for unsupported norm argument). If you can find any discrepancies between the outputs of `jax.scipy.fft` functions and `scipy.fft` functions, please let us know!"," CC(jax.scipy.fft.dct: implement & test norm='backward') implements and tests `norm='backward'` for all `dct` functions, and ensures that its behavior matches the corresponding implementations in `scipy.fft`."," Thanks for fixing this! It looks like CC(jax.scipy.fft: error for unsupported norm argument) and CC(jax.scipy.fft.dct: implement & test norm='backward') should resolve the issue. Perhaps it's useful to document: CC(jax.scipy.fft: error for unsupported norm argument) alone would not solve the issue: After the first PR, 1. The documentation states that JAX only supports `norm=""ortho""`. 2. It's good that JAX now fails if an unsupported argument is given. However, note that JAX will now fail for all options `norm not in [""ortho"", None]`. 3. 1 and 2 both imply that by default, `norm=""ortho""` will be performed. This differs from `scipy.fft`, which states that, by default, `norm=""backward""`. In that case matching plots you showed in your previous comment would indicate a bug, because the two plots should differ. We should instead expect the following two plots to match. ```python import numpy as np from scipy.fft import dct as sdct from scipy.fft import idct as sidct from jax.scipy.fft import dct as jdct from jax.scipy.fft import idct as jidct import matplotlib.pyplot as plt m = np.linspace(0, 2 * np.pi, 100) n = np.linspace(0, 2 * np.pi, 5 * m.size) f = m  scipy has backward is default sf = sidct(sdct(f), n=n.size) * (n.size / m.size)  jax uses ortho by default jf = jidct(jdct(f), n=n.size) * np.sqrt(n.size / m.size) plt.scatter(m, f, label=""truth"", color=""black"") plt.plot(n, sf, linestyle="""", linewidth=4, label=""scipy"") plt.plot(n, jf, linestyle="""", linewidth=2.5, label=""jax"") plt.legend() ``` !image The second PR updates the documentation to state `norm=""backward""` is supported and used by default. In that case, the correct usage becomes ```python sf = sidct(sdct(f), n=n.size) * (n.size / m.size) jf = jidct(jdct(f), n=n.size) * (n.size / m.size) ``` which produces matching plots as expected.",I think https://github.com/google/jax/pull/22571 addresses all your comments – please let me know if that is not the case.,"Yes it does, thanks."
yi,Bump pillow from 10.3.0 to 10.4.0,"Bumps pillow from 10.3.0 to 10.4.0.  Release notes Sourced from pillow's releases.  10.4.0 https://pillow.readthedocs.io/en/stable/releasenotes/10.4.0.html Changes  Raise FileNotFoundError if show_file() path does not exist  CC(Deprecate X64 flag) [@​radarhere] Improved reading 16bit TGA images with colour  CC(`vmap(grad(all_gather))` errors) [@​Yay295] Fixed processing multiple JPEG EXIF markers  CC(Numerical errors in `jax.numpy.einsum`) [@​radarhere] Do not preserve EXIFIFD tag by default when saving TIFF images  CC(Use all_gather+reduce_scatter HLOs on TPU) [@​radarhere] Added ImageFont.load_default_imagefont()  CC([JAX] Fix pylint errors.) [@​radarhere] Added Image.WARN_POSSIBLE_FORMATS  CC([sparse]: make repr safe for invalid BCOO objects) [@​radarhere] Do not presume &quot;xmp&quot; info simply because &quot;XML:com.adobe.xmp&quot; info exists  CC(JIT explicit pytree arguments) [@​radarhere] Remove zerobyte end padding when parsing any XMP data  CC(`jax.nn.sigmoid` custom jvp leak) [@​radarhere] Do not detect Ultra HDR images as MPO  CC(未找到相关数据) [@​radarhere] Raise SyntaxError specific to JP2  CC(未找到相关数据) [@​Yay295] Do not use first frame duration for other frames when saving APNG images  CC(Fix incorrect EllipsisType reference for Python 3.10) [@​radarhere] Consider I;16 pixel size when using a 1 mode mask  CC(未找到相关数据) [@​radarhere] When saving multiple PNG frames, convert to mode rather than raw mode  CC([JAX] Fix more pylint errors.) [@​radarhere] Added byte support to FreeTypeFont  CC(names of jitted functions not shown in __repr__) [@​radarhere] Allow float center for rotate operations  CC(Remove documentation for xla_hlo_profile.) [@​radarhere] Do not read layers immediately when opening PSD images  CC([xla] Skip loop full unroll pass in LLVM pipeline.) [@​radarhere] Restore original thread state  CC(DOC: add deprecation message to index_update and friends) [@​radarhere] Read IM and TIFF images as RGB, rather than RGBX  CC(aheadoftime lowering and compilation for jit) [@​radarhere] Only preserve TIFF IPTC_NAA_CHUNK tag if type is BYTE or UNDEFINED  CC([host_callback] Fix an assertion failure for grad(remat(host_callback))) [@​radarhere] Prevent extra EPS header validations  CC(jnp.setdiff1d: add optional size and fill_value arguments) [@​Yay295] Clarify ImageDraw2 error message when size is missing  CC(Add checkpoint policy to save dots w/o batch dim.) [@​radarhere] Support unpacking more rawmodes to RGBA palettes  CC(convolutions: use flip() to clean up reverseindexing) [@​radarhere] Removed support for Qt 5  CC(Rollback: breaks when layouts of variadic tuple elements differ) [@​radarhere] Improve ImageFont.freetype support for XDG directories on Linux  CC(config setting to control the default PRNG implementation) [@​mamg22] Improved consistency of XMP handling  CC(failed to build v0.2.21 on Windows) [@​radarhere] Use pkgconfig to help find libwebp and raqm  CC(Fix repr() of jitcompiled functions (Fixes 8141)) [@​radarhere] Renamed C transform2 to transform  CC(Add note to changelog about deprecation of jax.ops.index_...) [@​radarhere] Updated nasm to 2.16.03  CC(未找到相关数据) [@​radarhere] [precommit.ci] precommit autoupdate  CC(Enhancement: support normalization options in FFT) [@​precommitci] Updated ImageCms.createProfile colorTemp default and docstring  CC(Programmatic monitoring of memory usage) [@​radarhere] Added ImageDraw circle()  CC(未找到相关数据) [@​void4] Don't reuse variable name  CC(Support nontrivial strides when importing DLPack tensors ) [@​Yay295] Use functools.cached_property in GifImagePlugin  CC(Add support for CPU CustomCall functions that can fail.) [@​radarhere] Add mypy target to Makefile  CC(未找到相关数据) [@​Yay295] Added Python 3.13 beta wheels  CC(pjit custom prngkey test) [@​radarhere] Parse _version contents instead of using exec()  CC(未找到相关数据) [@​radarhere] Lint fixes  CC(jnp.power gives inaccurate results) [@​radarhere] Fix type errors  CC(Update jax.ops.index_* documentation to include current preferred index notation) [@​radarhere] Added MPEG accept function  CC(PRNGKeys can have dtype float0 if they are a tangent.) [@​radarhere] Added more modes to Image.MODES  CC(Import numpy explicitly in the jax namespace) [@​radarhere] [precommit.ci] precommit autoupdate  CC(bfloat16 pmap cuda11.1 results in error (v2)) [@​precommitci] Do not use percent format in strings  CC(Add jaxlib version check to pjit_test.py) [@​radarhere] Changed string formatting to fstrings  CC(DeviceArray.__iter__ returns DeviceArrays, without host sync) [@​mrKazzila] Removed direct invocation of setup.py  CC(random.multivariate_normal broadcasting error  ) [@​radarhere] Update ExifTags.py  CC([WIP] BCSR format) [@​CTimmerman]    ... (truncated)   Changelog Sourced from pillow's changelog.  10.4.0 (20240701)   Raise FileNotFoundError if show_file() path does not exist  CC(Deprecate X64 flag) [radarhere]   Improved reading 16bit TGA images with colour  CC(`vmap(grad(all_gather))` errors) [Yay295, radarhere]   Deprecate nonimage ImageCms modes  CC(未找到相关数据) [radarhere]   Fixed processing multiple JPEG EXIF markers  CC(Numerical errors in `jax.numpy.einsum`) [radarhere]   Do not preserve EXIFIFD tag by default when saving TIFF images  CC(Use all_gather+reduce_scatter HLOs on TPU) [radarhere]   Added ImageFont.load_default_imagefont()  CC([JAX] Fix pylint errors.) [radarhere]   Added Image.WARN_POSSIBLE_FORMATS  CC([sparse]: make repr safe for invalid BCOO objects) [radarhere]   Remove zerobyte end padding when parsing any XMP data  CC(`jax.nn.sigmoid` custom jvp leak) [radarhere]   Do not detect Ultra HDR images as MPO  CC(未找到相关数据) [radarhere]   Raise SyntaxError specific to JP2  CC(未找到相关数据) [Yay295, radarhere]   Do not use first frame duration for other frames when saving APNG images  CC(Fix incorrect EllipsisType reference for Python 3.10) [radarhere]   Consider I;16 pixel size when using a 1 mode mask  CC(未找到相关数据) [radarhere]   When saving multiple PNG frames, convert to mode rather than raw mode  CC([JAX] Fix more pylint errors.) [radarhere]   Added byte support to FreeTypeFont  CC(names of jitted functions not shown in __repr__) [radarhere]   Allow float center for rotate operations  CC(Remove documentation for xla_hlo_profile.) [radarhere]   Do not read layers immediately when opening PSD images  CC([xla] Skip loop full unroll pass in LLVM pipeline.) [radarhere]     ... (truncated)   Commits  9b4fae7 10.4.0 version bump b55d74b Update CHANGES.rst [ci skip] 8daf550 Merge pull request  CC(Deprecate X64 flag) from radarhere/imageshow c6d8c58 Merge pull request  CC(`vmap(grad(all_gather))` errors) from Yay295/patch3 c9ec76a Raise FileNotFoundError if show_file() path does not exist b48d175 Update CHANGES.rst [ci skip] 4d6dff3 Merge pull request  CC(未找到相关数据) from radarhere/imagingcms_modes 70b3815 Merge pull request  CC(Numerical errors in `jax.numpy.einsum`) from radarhere/multiple_exif_markers 88cd6d4 Rearranged comments 41426a6 Merge pull request  CC(Use all_gather+reduce_scatter HLOs on TPU) from radarhere/exififd Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-07-15T17:40:24Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/22455,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
yi,"Bump etils[epath,epy] from 1.7.0 to 1.9.2","Bumps [etils[epath,epy]](https://github.com/google/etils) from 1.7.0 to 1.9.2.  Release notes Sourced from etils[epath,epy]'s releases.  v1.9.2  epath:  Support pydantic serialization of epath.Path    v1.9.1  epath:  Fix an infinite recursion on is_relative_to for Python&gt;=3.12.    v1.9.0  epy:  Add epy.lazy_api_imports to lazyimport __init__.py symbols. Removed: epy.cached_property epy.lazy_imports: Error callback accept a str to autoreraise with additional info. Fix mixing epy.lazy_imports() with epy.binary_adhoc().   ecolab:  Added reload_workspace=True to adhoc to autoreload from workspace Add ecolab.get_permalink() Fix ecolab.inspect not finding static files when the kernel contain partial etils deps.   epath:  Fix error when importlib.resources.files return MultiplexedPath Fix gs:// URI for 3.12 Fix .walk 3.12 error (topdown &gt; top_down rename)   Full compatibility with Python 3.12 (unit tests run on both 3.11 and 3.12).  v1.8.0  Drop Python 3.10 support. epy:  epy.pretty_repr: Add support for namedtuple   ecolab:  Add ecolab.disp(obj) Add ;h for syntax highlighting with autodisplay Fix proto error on import       Changelog Sourced from etils[epath,epy]'s changelog.  [1.9.2]  20240612  epath:  Support pydantic serialization of epath.Path    [1.9.1]  20240604  epath:  Fix an infinite recursion on is_relative_to for Python&gt;=3.12.    [1.9.0]  20240531  epy:  Add epy.lazy_api_imports to lazyimport __init__.py symbols. Removed: epy.cached_property epy.lazy_imports: Error callback accept a str to autoreraise with additional info. Fix mixing epy.lazy_imports() with epy.binary_adhoc().   ecolab:  Added reload_workspace=True to adhoc to autoreload from workspace Add ecolab.get_permalink() Fix ecolab.inspect not finding static files when the kernel contain partial etils deps.   epath:  Fix error when importlib.resources.files return MultiplexedPath Fix gs:// URI for 3.12 Fix .walk 3.12 error (topdown &gt; top_down rename)   Full compatibility with Python 3.12 (unit tests run on both 3.11 and 3.12).  [1.8.0]  20240313  Drop Python 3.10 support. epy:  epy.pretty_repr: Add support for namedtuple   ecolab:  Add ecolab.disp(obj) Add ;h for syntax highlighting with autodisplay Fix proto error on import       Commits  cd3dc8e Wrap pydantic schema in no_info_after_validator_function f82afde Add support for pydantic in treating etils.epath.Path as a normal Path 9ef3958 Document that xnp should be passed as a keyword argument for check_and_normal... a599dcc Support for python3.12 d51bd37 Release etils==1.9.1 with a fix for python 3.12. cbceb9e Remove is_relative_to backport, as etils uses Python &gt;3.11 (Fix  CC(Test case improvements:)) a4b4c80 Publish etils==1.9.0. af1d018 Yield directly pathlib output. 937e865 Launch tests on both 3.11 and 3.12. 5fb74a4 Merge pull request  CC(Added np.array_equal) from Sibgatulin:adapt_gpath_to_py312 Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-07-15T17:40:05Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/22454,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
rag,Bump setuptools from 69.2.0 to 70.3.0,"Bumps setuptools from 69.2.0 to 70.3.0.  Changelog Sourced from setuptools's changelog.  v70.3.0 Features  Support for loading distutils from the standard library is now deprecated, including use of SETUPTOOLS_USE_DISTUTILS=stdlib and importing distutils before importing setuptools. ( CC(Do not call asarray() on inputs of jax.random.choice))  Bugfixes  pypa/distutils CC(Add multivariate_normal and pdf to jax.scipy.stats)  v70.2.0 Features  Updated distutils including significant changes to support Cygwin and mingw compilers. ( CC(Call check_user_dtype on all user dtypes))  Bugfixes  Fix distribution name normalisation (:pep:625) for valid versions that are not canonical (e.g. 1.02). ( CC(reduce testcase count of the numpydispatch CI check))  v70.1.1 Misc   CC(Add support for `all_to_all` in vmap)  v70.1.0 Features   Adopted the bdist_wheel command from the wheel project  by :user:agronholm ( CC(Nan Mean and Adds Nan Reducers to testing))   Improve error message when pkg_resources.ZipProvider tries to extract resources with a missing Egg  by :user:Avasam Added variables and parameter type annotations to pkg_resources to be nearly on par with typeshed.*  by :user:Avasam     ... (truncated)   Commits  356e9a0 Bump version: 70.2.0 → 70.3.0 822280b Merge pull request  CC(Revert ""[jax2tf] Added Getting Started notebook (4460)"") from pypa/bugfix/distutils34f9518ef c4e64c1 Add news fragment. b01183c Merge https://github.com/pypa/distutils into bugfix/distutils34f9518ef e221581 Merge pull request pypa/distutils CC(Update Jaxlib references to 0.1.6.) from msys2contrib/customize_compiler_m... 34f9518 Merge pull request  CC(add omnistaging doc) from pypa/debt/4137deprecatedistutilsstdlib bacd9c6 sysconfig: skip customize_compiler() with MSVC Python again 4a3406b CI: also set CC/CXX when pip installing with mingw+clang e9f0be9 Merge pull request  CC(lax.switch compilation time is superlinear in number of switch options) from pypa/dropgitignore 70cda3d Use '.yml' for consistency. Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-07-15T17:39:54Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/22453,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
yi,Bump scipy from 1.13.1 to 1.14.0,"Bumps scipy from 1.13.1 to 1.14.0.  Release notes Sourced from scipy's releases.  SciPy 1.14.0 Release Notes SciPy 1.14.0 is the culmination of 3 months of hard work. It contains many new features, numerous bugfixes, improved test coverage and better documentation. There have been a number of deprecations and API changes in this release, which are documented below. All users are encouraged to upgrade to this release, as there are a large number of bugfixes and optimizations. Before upgrading, we recommend that users check that their own code does not use deprecated SciPy functionality (to do so, run your code with python Wd and check for DeprecationWarning s). Our development attention will now shift to bugfix releases on the 1.14.x branch, and on adding new features on the main branch. This release requires Python 3.10+ and NumPy 1.23.5 or greater. For running on PyPy, PyPy3 6.0+ is required. Highlights of this release  SciPy now supports the new Accelerate library introduced in macOS 13.3, and has wheels built against Accelerate for macOS &gt;=14 resulting in significant performance improvements for many linear algebra operations. A new method, cobyqa, has been added to scipy.optimize.minimize  this is an interface for COBYQA (Constrained Optimization BY Quadratic Approximations), a derivativefree optimization solver, designed to supersede COBYLA, developed by the Department of Applied Mathematics, The Hong Kong Polytechnic University. scipy.sparse.linalg.spsolve_triangular is now more than an order of magnitude faster in many cases.  New features scipy.fft improvements  A new function, scipy.fft.prev_fast_len, has been added. This function finds the largest composite of FFT radices that is less than the target length. It is useful for discarding a minimal number of samples before FFT.  scipy.io improvements  wavfile now supports reading and writing of wav files in the RF64 format, allowing files greater than 4 GB in size to be handled.  scipy.constants improvements  Experimental support for the array API standard has been added.    ... (truncated)   Commits  87c4664 REL: SciPy 1.14.0 rel commit [wheel build] ac63c81 Merge pull request  CC(Finalize deprecation of lax.linalg positional args) from tylerjereddy/treddy_1.14.0_final_backports 541003f DOC: update 1.14 relnotes [wheel build] a08d1ff MAINT: stats.gstd: warn when an observation is &lt;= 0 a4f7119 DEP: special.perm: deprecate noninteger N and k with exact=True ( CC(Pallas jax.lax.fori_loop over long inputs slows down)) 73339fb TST: stats: fix use of np.testing to compare xparrays 0542df6 DOC: Update 1.14.0 release notes f8e530c STY: _lib._util: silence mypy e2cbda2 TST:sparse.linalg: Skip test due to sensitivity to numerical noise 4fb2e6a TST: robustify test_nnls_inner_loop_case1 Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-07-15T17:39:42Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/22452,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
yi,Bump the pip group across 1 directory with 2 updates,"Bumps the pip group with 2 updates in the /build directory: zipp and setuptools. Updates `zipp` from 3.18.1 to 3.19.1  Changelog Sourced from zipp's changelog.  v3.19.1 Bugfixes  Improved handling of malformed zip files. ( CC(example Errata))  v3.19.0 Features  Implement is_symlink. ( CC(add oss test instructions, fix conv grad bug))  v3.18.2 No significant changes.    Commits  6d1cb72 Finalize fd604bd Merge pull request  CC(Can’t import both jax and tensorflow (causes kernel restart)) from jaraco/bugfix/119malformedpaths c18417e Add news fragment. 58115d2 Employ SanitizedNames in CompleteDirs. Fixes broken test. 564fcc1 Add SanitizedNames mixin. 79a309f Add some assertions about malformed paths. 2d015c2 Merge https://github.com/jaraco/skeleton a595a0f Rename extras to align with core metadata spec. 608f90a Finalize 3a22d72 Merge pull request  CC(add tensordot) from jaraco/feature/issymlink Additional commits viewable in compare view    Updates `setuptools` from 69.2.0 to 70.0.0  Changelog Sourced from setuptools's changelog.  v70.0.0 Features  Emit a warning when [tools.setuptools] is present in pyproject.toml and will be ignored.  by :user:SnoopJ ( CC(未找到相关数据)) Improved AttributeError error message if pkg_resources.EntryPoint.require is called without extras or distribution Gracefully &quot;do nothing&quot; when trying to activate a pkg_resources.Distribution with a None location, rather than raising a TypeError  by :user:Avasam ( CC([jax2tf] Add input and output names to the TensorFlow graph generated by jax2tf.)) Typed the dynamically defined variables from pkg_resources  by :user:Avasam ( CC(Pin pygments version in RTD build)) Modernized and refactored VCS handling in package_index. ( CC([jax2tf] Revert '[jax2tf] Replace tf.math.add with tf.raw_ops.AddV2 (…))  Bugfixes  In install command, use super to call the superclass methods. Avoids race conditions when monkeypatching from _distutils_system_mod occurs late. ( CC(Rename collectives into ""collective operations"" for the pmap function.)) Fix finder template for lenient editable installs of implicit nested namespaces constructed by using package_dir to reorganise directory structure. ( CC([jax2tf] Replace tf.math.add with tf.raw_ops.AddV2)) Fix an error with UnicodeDecodeError handling in pkg_resources when trying to read files in UTF8 with a fallback  by :user:Avasam ( CC(Ppermute batching errors))  Improved Documentation  Uses RST substitution to put badges in 1 line. ( CC(improve an escaped tracer error message))  Deprecations and Removals   Further adoption of UTF8 in setuptools. This change regards mostly files produced and consumed during the build process (e.g. metadata files, script wrappers, automatically updated config files, etc..) Although precautions were taken to minimize disruptions, some edge cases might be subject to backwards incompatibility. Support for &quot;locale&quot; encoding is now deprecated. ( CC(Help with array slice indices))   Remove setuptools.convert_path after long deprecation period. This function was never defined by setuptools itself, but rather a sideeffect of an import for internal usage. ( CC(Allow custom_linear_solve to return things besides the solution))   Remove fallback for customisations of distutils' build.sub_command after long deprecated period. Users are advised to import build directly from setuptools.command.build. ( CC(Allow custom_linear_solve to return things besides the solution))   Removed typing_extensions from vendored dependencies  by :user:Avasam ( CC(Hacky approach to carry argument of an invertible transformation in the backward pass))   Remove deprecated setuptools.dep_util. The provided alternative is setuptools.modified. ( CC([jax2tf] Added support for shape polymorphism conversion.))     ... (truncated)   Commits  5cbf12a Workaround for release error in v70 9c1bcc3 Bump version: 69.5.1 → 70.0.0 4dc0c31 Remove deprecated setuptools.dep_util ( CC([jax2tf] Added support for shape polymorphism conversion.)) 6c1ef57 Remove xfail now that test passes. Ref  CC(jnp.moveaxis: fix bug when axes are integer dtype). d14fa01 Add all sitepackages dirs when creating simulated environment for test_edita... 6b7f7a1 Prevent bin folders to be taken as extern packages when vendoring ( CC(Update scale_and_translate to take an explicit spatial_dims.)) 69141f6 Add doctest for vendorised bin folder 2a53cc1 Prevent 'bin' folders to be taken as extern packages 7208628 Replace call to deprecated validate_pyproject command ( CC(Add a prototype implementation of recursive checkpointing)) 96d681a Remove call to deprecated validate_pyproject command Additional commits viewable in compare view    Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore  major version` will close this group update PR and stop Dependabot creating any more for the specific dependency's major version (unless you unignore this specific dependency's major version or upgrade to it yourself)  ` ignore  minor version` will close this group update PR and stop Dependabot creating any more for the specific dependency's minor version (unless you unignore this specific dependency's minor version or upgrade to it yourself)  ` ignore ` will close this group update PR and stop Dependabot creating any more for the specific dependency (unless you unignore this specific dependency or upgrade to it yourself)  ` unignore ` will remove all of the ignore conditions of the specified dependency  ` unignore  ` will remove the ignore condition of the specified dependency and ignore conditions You can disable automated security fix PRs for this repo from the Security Alerts page. ",2024-07-15T17:13:34Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/22449,"This pull request was built based on a group rule. Closing it will not ignore any of these versions in future pull requests. To ignore these dependencies, configure ignore rules in dependabot.yml"
yi,Fix compatibility of `jnp.unique` with numpy nightly,"In https://github.com/numpy/numpy/pull/26914, the behavior of the `return_inverse` argument to `np.unique` was partially reverted to the prev2.0 behavior. The PR brings JAX's implementation compatible with the `numpy>2.0.0` behavior.",2024-07-15T15:44:17Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/22445
llama,CTRL+C Broken When Running distributed.initialize() on one TPU Host by Accident," Issue Encountered a deadlock while running a JAXbased LLM training script on a TPUv432 pod. SSH'd into worker 0 and ran the script there directly, instead of using `worker all command ""...""`. CTRL+C should be able to kill the process, but it didn't in this case.  Command ```bash python m EasyLM.models.llama.llama_train \     mesh_dim='1,4,1' \     dtype='fp16' \     total_steps=250000 \     log_freq=50 \     save_model_freq=0 \     save_milestone_freq=2500 \     load_llama_config='7b' \     update_llama_config='' \     load_dataset_state='' \     load_checkpoint='' \     optimizer.type='adamw' \     optimizer.adamw_optimizer.weight_decay=0.1 \     optimizer.adamw_optimizer.lr=3e4 \     optimizer.adamw_optimizer.end_lr=3e5 \     optimizer.adamw_optimizer.lr_warmup_steps=2000 \     optimizer.adamw_optimizer.lr_decay_steps=250000 \     train_dataset.type='json' \     train_dataset.text_processor.fields='text' \     train_dataset.json_dataset.path='togethercomputer/RedPajamaData1T' \     train_dataset.json_dataset.seq_length=2048 \     train_dataset.json_dataset.batch_size=2048 \     train_dataset.json_dataset.tokenizer_processes=16 \     checkpointer.save_optimizer_state=True \     logger.online=True \     logger.prefix='EasyLM' \     logger.project=""open_llama_7b"" \     logger.output_dir=""."" \     logger.wandb_dir=""$HOME/experiment_output/open_llama_7b"" ```  Error Message ``` /home/air/.local/lib/python3.10/sitepackages/jax/_src/xla_bridge.py:152: UserWarning: TPU backend initialization is taking more than 60.0 seconds. Did you run your code on all TPU hosts? See https://jax.readthedocs.io/en/latest/multi_process.html for more information.   warnings.warn( ``` After this warning, the script hung indefinitely and CTRL+C did not kill it.  System info (python version, jaxlib version, accelerator, etc.)  Environment  JAX version: 0.4.28  Hardware: TPUv432 pod  Python version: 3.10  TPUUbuntubase22.04",2024-07-14T12:46:40Z,better_errors,open,0,4,https://github.com/jax-ml/jax/issues/22436,"Author of EasyLM here. This is expected as you are only running the training script on a single host out of 4 hosts in a v432 pod. JAX uses a SPMD mode of execution, which means that you need to run the same script (same command) on all hosts in a TPU pod. You can manually ssh into each host and run the same command, or use gcloud ssh to do that by specifying `worker=all`. If you are looking for something more user friendly, you can also checkout my recent TPU pod command package, which helps you launch jobs on TPU pods.","smits what do you think would be the best behavior here? We could eventually time out completely and exit with an error instead of hanging, but the timeout would have to be pretty long to make sure we don't accidentally quit too early on large deployments. I'm curious if you have more ideas!","Yes, that sounds very tricky indeed. A timeout is not needed in my opinion. Being able to CTRL+C to cancel the TPU processes initiated with distributed.initialize() would solve the problem for me. I'd reckon the nesting depth is quite large, with multiple layers of TPU processes, such that even repeated CTRL+C attempts fail to fully terminate the entire process tree, however this would be desirable. If this is an unique edge case and you've never seen such a request before, I don't mind closing the Issue without a solution.","Ah great point about CTRL+C not working! I've also run into this, although haven't investigated yet. I think it does have something to do with calling into the C++ TPU runtime. Let's keep this issue open to track handling CTRL+C correctly in this situation. I took the liberty of editing your initial issue a bit to include this, so people don't have to read the whole thread (feel free to edit my edits more). Thanks for this feedback!"
rag,INVALID_ARGUMENT: Mismatched PJRT plugin PJRT API version ," Description ```bash RuntimeError: Unable to initialize backend 'cuda': INVALID_ARGUMENT: Mismatched PJRT plugin PJRT API version (0.54) and framework PJRT API version 0.51). ```  System info (python version, jaxlib version, accelerator, etc.) ```bash pip install jaxlib==0.4.28+cuda12.cudnn89 f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html pip install ""jax[cpu]==0.4.28"" f https://storage.googleapis.com/jaxreleases/jax_releases.html ```",2024-07-12T06:19:55Z,bug,closed,0,5,https://github.com/jax-ml/jax/issues/22416,"Hi, It's possible that there was an issue with the release. As a temporary workaround you can set the environment flag `ENABLE_PJRT_COMPATIBILITY=true`. Does this problem still persist if you install the latest JAX version (v0.4.30)?",can not find v0.4.30 for cudnn89,"new error: ```bash E0715 18:02:17.586518  591934 cuda_dnn.cc:523] Loaded runtime CuDNN library: 9.0.0 but source was compiled with: 9.1.1.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration. ``` but i have install 0.4.28+cuda12.cudnn89, also raise the same error for 0.4.27+cuda12.cudnn89","What hardware are you running on? Could you print out the output of `nvidiasmi`, `pip list`, and `import jax; jax.print_environment_info()`? I could try to reproduce on my end.",This problem was solved after I decided not to use conda
glm,all inputs are nans in a next step iterationwhile using `jax.scipy.optimize.minimize`," Description Just cannot understand why while using `minimize`, the first iteration is good, `m_term` and `LL` are all reasonable values, then the next step, all terms in `m_term` becomes nan. ```python import jax import jax.numpy as jnp import jax.random as jnr from jax import lax from jax.scipy.stats import poisson from jax.scipy.optimize import minimize import time print(jax.__version__) key=jnr.PRNGKey(0) def h(H):     return jnp.where(H>40.0,H,jnp.log(1+jnp.exp(H))) def POIS_GLM_MLE(M,y,rank):      def LL(m_term):  FIXME: !!! m_term all becomes nans 2nd step of iterations         lambda_ = h(M)  M n_row*rank, m_term rank          poival=poisson.logpmf(y,lambda_)         ll=jnp.sum(jnp.where(jnp.isfinite(poival),poival,0.0))         jax.debug.print(""LL"")         jax.debug.breakpoint()         return ll     res=minimize(LL,jnr.uniform(key,(rank,)),method='BFGS')     jax.debug.print(""res"")     jax.debug.breakpoint()     return res.x def als_decomposition_column(Y, rank, tol=1e6, max_iter=100):     n, m = Y.shape     U = jnr.uniform(jnr.PRNGKey(2),(n, rank))     V = jnr.uniform(jnr.PRNGKey(3),(m, rank))     def mapM(y,M):         m = POIS_GLM_MLE(M, y, rank)             return m     def body_fn(carry,_):         U,V = carry         U = lax.map(lambda y:mapM(y, V),Y)         V = lax.map(lambda y:mapM(y, U),Y.T)         jax.debug.print(""body"")         jax.debug.breakpoint()          Check for convergence         poival=poisson.logpmf(Y,h(U @ V.T))         reconstruction_error = jnp.sum(jnp.where(jnp.isfinite(poival),poival,0.0))         return (U,V),reconstruction_error     (U,V),reconstruction_error=lax.scan(body_fn,(U,V),jnp.arange(max_iter))     return U,V,reconstruction_error if __name__=='__main__':     U=jnr.uniform(key,(600,30))     V=jnr.uniform(key,(50,30))     H=U.T     lambda_=h(H)     Y=jnr.poisson(key,lambda_).astype(float)     Y=Y.at[0,0].set(jnp.nan)     st=time.time()     nU,nV,reconstruction_error=als_decomposition_column(Y,30,max_iter=500)     print(lambda_h(nU.T))     print(reconstruction_error)     print(""time"",time.time()st) ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.28 jaxlib: 0.4.28 numpy:  1.23.5 python: 3.10.12  ++ ```",2024-07-12T04:26:42Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/22413,"I found it is my function property, sorry."
yi,Accuracy of `jax.experimental.sparse.spsolve`," Description Is there any reason why `spsolve` would return a solution that is close to the true one, but not accurate? I can't provide a working example but I'm trying to solve a system (40000, 40000) sparse system (255101 nonzero entries), and the solution I get has `jnp.max(jnp.abs(matrixb))=0.0045`, which is not ideal. Any reason why, or is any way I can do to improve this accuracy?  System info (python version, jaxlib version, accelerator, etc.) Python version 3.10.13 Jax version 0.4.30 Running on gpu",2024-07-11T14:21:23Z,question,open,0,5,https://github.com/jax-ml/jax/issues/22399,"Just a guess: JAX computes results in float32 by default; iterative solvers are often quite sensitive to float rounding errors. If you enable float64 computation, you will likely see more accurate results.","You are totally right, I had considered the 32 vs 64 but didn't think it would make such a difference. I just developed some rutines with Jax to solve Linear Complementarty Problems, both for sparse and non sparse matrices (Lemke and Fischer 1995). I'd be happy to collaborate with the package but I'm not sure I know how. Or I can do it as an independent package.","Following this discussin, it seems that sparse.eye causes problems when float64 is enabled.  ``` from jax.experimental import sparse import jax import jax.numpy as jnp jax.config.update(""jax_enable_x64"", True) key = jax.random.PRNGKey(0) n=10 M = sparse.random_bcoo(key,(n,n)) M.astype(jnp.float64)+sparse.eye(n, dtype=jnp.float64) ``` I get: ``` File ~/miniconda3/lib/python3.10/sitepackages/jax/experimental/sparse/transform.py:428, in eval_sparse(jaxpr, consts, spvalues, spenv)     426   if prim not in sparse_rules_bcoo:     427     _raise_unimplemented_primitive(prim) ...    5047   equiv = _JNP_FUNCTION_EQUIVALENTS[name]    5048   msg += f"" (Tip: jnp.{equiv} is a similar function that does automatic type promotion on inputs)."" > 5049 raise TypeError(msg.format(name, "", "".join(str(a.dtype) for a in avals))) TypeError: lax.concatenate requires arguments to have the same dtypes, got int64, int32. (Tip: jnp.concatenate is a similar function that does automatic type promotion on inputs). ``` However, defining my own eye function ``` def eye(n):     data = jnp.ones(n)     indices = jnp.column_stack((jnp.arange(n), jnp.arange(n)))     return sparse.BCOO((data, indices), shape=(n, n)) ``` works.",This looks like a problem in the `sparse.concatenate` implementation: it doesn't account for the fact that the indices may have different dtypes.,"Good catch, thank you. I understand the `jax `framework and sparsity are hard to combine, I hope someday it is no longer `experimental`. `spsolve` had issues to solve a system of equations when `jnp.float64` was used, it raises an error saying the matrix is singular when it is not. Using 32 bit floats, the correct solution is found. "
yi,Add jax.scipy.integrate.cumulative_trapezoid.,Add jax.scipy.integrate.cumulative_trapezoid.,2024-07-11T12:43:20Z,,closed,0,0,https://github.com/jax-ml/jax/issues/22396
llm,Local shards accessed using devices_indices_map are scaled by # of pods," Description Given an array sharded across `n` pods, the following code results in each `local_shard` having its elements scaled by exactly `n`: ```python         m = sharded_arr.sharding.devices_indices_map(sharded_arr.shape)         for device in jax.local_devices():             index = m[device]             local_shard = jax.device_get(sharded_arr[index]) ``` That is, each `local_shard` is exactly `n * sharded_arr[index]`. For comparison, this gives the correct list of shards: ```python         for shard in sharded_arr.addressable_shards:             index = shard.index             local_shard = shard.data ``` Full reproducer with some mesh/distributed setup code omitted: ```python import jax.numpy as jnp import jax.sharding from jax.experimental import multihost_utils def test_sharding():      ... init distributed JAX and mesh ...     with mesh:         arr = jnp.full(len(jax.devices()), 1e4, dtype=jnp.float32)         sharding = jax.sharding.NamedSharding(             mesh, jax.sharding.PartitionSpec(""name_of_first_axis_in_mesh"")         )         sharded_arr = jax.jit(lambda x: x, out_shardings=sharding)(arr)         assert sharded_arr.sharding == sharding         assert jnp.all(multihost_utils.process_allgather(sharded_arr) == arr)          Okay...         for shard in sharded_arr.addressable_shards:             index = shard.index             local_shard = shard.data             assert jnp.all(arr[index] == local_shard), (arr[index], local_shard)          Bugged...         m = sharded_arr.sharding.devices_indices_map(sharded_arr.shape)         for device in jax.local_devices():             index = m[device]             local_shard = sharded_arr[index]             assert jnp.all(arr[index] == local_shard), (arr[index], local_shard) ``` The last assert is the one that fails, with output: ``` AssertionError: (Array([10000.], dtype=float32), Array([640000.], dtype=float32)) ``` when using 64 pods. If using 4 pods then it is off by a factor of 4. And if using just 1 pod then it is fine.  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.31.dev20240628 jaxlib: 0.4.31.dev20240628 numpy:  1.24.4 python: 3.11.6 (main, Jul  7 2024, 03:20:08) [GCC 11.4.0] jax.devices (4 total, 4 local): [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0) TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0) TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0) TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)] process_count: 1 platform: uname_result(system='Linux', node='...', release='6.1.58+', version='...', machine='x86_64') ```",2024-07-11T01:29:57Z,bug,open,0,2,https://github.com/jax-ml/jax/issues/22387,Using `.addressable_shards` is the way to go. That's the API to fetch local shards.,"Thanks for confirming the fix. To clarify / help others who might run into this, the issue is the first approach works if you're on a single pod, and it _sort of_ seems to work at first for more than 1 pod too until you run into correctness issues that are difficult to pinpoint especially since it doesn't reproduce on a single pod setup. It's easy to be mislead into thinking `sharded_arr[index]` is an okay thing to do when the `index` corresponds to a locally addressable shard, and it even seems to work (until it doesn't)."
yi,Unifying persistent cache messages," Also moving persistent cache messages (miss & hit)  to WARNING logging level when `config.explain_cache_misses` is true; otherwise they remain in DEBUG. Message ""Persistent compilation cache hit for"" becomes ""PERSISTENT COMPILATION CACHE (HIT|MISS)"" to remain consistent with ""TRACING CACHE MISS"". The included tests test for correct presence of cache miss/hit messages in the logging level depending on the value of `config.explain_cache_misses`",2024-07-11T01:26:14Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/22386
yi,[jax.distributed] Allow setting local device ids via env var,"Makes `jax.distributed.initialize` aware of a new environmental variable, `JAX_LOCAL_DEVICE_IDS`, which allows overriding the local device ids set by default or configured by clusterdependent autoconfiguration. Note that `local_device_ids` passed explicitly to `jax.distributed.initialize` are still respected. We are currently evaluating the benefits of oneprocessperhost vs. oneprocessperGPU vs. oneprocessperNUMAnode and this override would be useful for local experiments with different configurations for multiprocessJAX. E.g. running an existing workload like PAXML with oneprocessperGPU currently requires reaching into the framework and modifying its setup code (e.g. https://github.com/google/paxml/blob/9863f276af5431d8f08542dba06ced1dccb51aa7/paxml/setup_jax.pyL69L73). Similarly, the autoconfiguration for clusters currently presumes a onetoone mapping of processes to devices (e.g. https://github.com/google/jax/blob/e4b606e38a18867c757c738dd16265ca03d2cf88/jax/_src/clusters/slurm_cluster.pyL65L66). The first commit contains the minimal amount of changes necessary. The second commit extends the existing special value `""all""` of config variables `jax_cuda_visible_devices` and `jax_rocm_visible_devices` to the `local_device_ids` parameter and `JAX_LOCAL_DEVICE_IDS`.",2024-07-10T11:36:56Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/22373," Rebased and dropped the second commit (the part supporting ""all""). PTAL"
yi,jax.numpy: better docstring for allclose and isclose functions,Better docstring added for jax.numpy.allclose and jax.numpy.isclose. Part of 21461,2024-07-09T22:46:48Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/22359,evaluation condition inserted and text fixed.
yi,Issue with shardmap when trying to use static arguments," Description Following from this discussion https://github.com/google/jax/discussions/22353 I made this MWE ```python from functools import partial import jax jax.distributed.initialize() from jax.experimental.shard_map import shard_map from jax.experimental import mesh_utils from jax.sharding import PartitionSpec as P , Mesh pdims = (2 , 2) mesh_shape = (8 , 8 , 8) devices = mesh_utils.create_device_mesh(pdims) mesh = Mesh(devices, axis_names=('y', 'z')) sharding = jax.sharding.NamedSharding(mesh, P('y', 'z')) local_mesh_shape = [mesh_shape[i] // pdims[i] for i in range(len(pdims))] master_key = jax.random.PRNGKey(0) key = jax.random.split(master_key)[jax.process_index()] (shard_map , mesh=mesh , in_specs = (None , P()) , out_specs = P('y', 'z')) def multi_gpu_impl(shape , key):     local_mesh_shape = [         shape[0] // pdims[1], shape[1] // pdims[0], shape[2]     ]     return jax.random.normal(key, local_mesh_shape) out = multi_gpu_impl(mesh_shape , master_key) print(out.shape) print(out.addressable_data(0).shape) ``` I have this error ```     out = multi_gpu_impl(mesh_shape , master_key) ValueError: safe_zip() argument 2 is longer than argument 1 ``` Following what  did in https://github.com/google/jax/pull/22049 I am not able to use a tuple static arguments   System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.30 jaxlib: 0.4.30 numpy:  2.0.0 python: 3.10.4  ++++ ```",2024-07-09T21:13:57Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/22356,Thanks for raising this! Definitely looks like a bug; any time you see a `safe_zip` error it's likely a JAXinternal bug.,"Thank you , I have tried a bunch of things, bools PRNGKeys work. It is only Sequences that seems to cause the issue.","Hmm, I tried to translate to a local CPU repro but this didn't crash: ```python import os os.environ['XLA_FLAGS'] = 'xla_force_host_platform_device_count=8' from functools import partial import jax from jax.experimental.shard_map import shard_map from jax.experimental import mesh_utils from jax.sharding import PartitionSpec as P , Mesh pdims = (2 , 4) devices = mesh_utils.create_device_mesh(pdims) mesh = Mesh(devices, axis_names=('y', 'z')) sharding = jax.sharding.NamedSharding(mesh, P('y', 'z')) master_key = jax.random.PRNGKey(0) key = jax.random.split(master_key)[jax.process_index()] (shard_map , mesh=mesh ,          in_specs = (None , P()) , out_specs = P('y', 'z')) def multi_gpu_impl(shape , key):     return jax.random.normal(key, shape) out = multi_gpu_impl((2, 2) , master_key) print(out.shape) print(out.addressable_data(0).shape) ``` Is that missing something? Could you set the `JAX_TRACEBACK_FILTERING=off` shell environment variable and rerun your repro, then paste the full traceback?","Ah, I wonder if it's just a version issue: https://github.com/google/jax/pull/22049 may only be available at HEAD, ie jax==0.4.31, not in jax==0.4.30. Only jax==0.4.30 has been pushed to pypi, so you'd have to install jax from GitHub.",Can you try running from github HEAD? You might be able to do `pip install upgrade git+https://github.com/google/jax.git`.,"Yeah with 0.4.31 I don't reproduce. I am going to close then thank you very much. One question, when to use P() and when to use None None is fully compatible with `static_argnums` if the parent has a jit decorater?","> One question, when to use P() and when to use None There's not much of a difference; `P()` only works for arraylikes, whereas `None` works more generally, and they produce the same result even for ararylike arguments."
rag,Bump zipp from 3.18.1 to 3.19.1 in /build in the pip group across 1 directory,"Bumps the pip group with 1 update in the /build directory: zipp. Updates `zipp` from 3.18.1 to 3.19.1  Changelog Sourced from zipp's changelog.  v3.19.1 Bugfixes  Improved handling of malformed zip files. ( CC(example Errata))  v3.19.0 Features  Implement is_symlink. ( CC(add oss test instructions, fix conv grad bug))  v3.18.2 No significant changes.    Commits  6d1cb72 Finalize fd604bd Merge pull request  CC(Can’t import both jax and tensorflow (causes kernel restart)) from jaraco/bugfix/119malformedpaths c18417e Add news fragment. 58115d2 Employ SanitizedNames in CompleteDirs. Fixes broken test. 564fcc1 Add SanitizedNames mixin. 79a309f Add some assertions about malformed paths. 2d015c2 Merge https://github.com/jaraco/skeleton a595a0f Rename extras to align with core metadata spec. 608f90a Finalize 3a22d72 Merge pull request  CC(add tensordot) from jaraco/feature/issymlink Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore  major version` will close this group update PR and stop Dependabot creating any more for the specific dependency's major version (unless you unignore this specific dependency's major version or upgrade to it yourself)  ` ignore  minor version` will close this group update PR and stop Dependabot creating any more for the specific dependency's minor version (unless you unignore this specific dependency's minor version or upgrade to it yourself)  ` ignore ` will close this group update PR and stop Dependabot creating any more for the specific dependency (unless you unignore this specific dependency or upgrade to it yourself)  ` unignore ` will remove all of the ignore conditions of the specified dependency  ` unignore  ` will remove the ignore condition of the specified dependency and ignore conditions You can disable automated security fix PRs for this repo from the Security Alerts page. ",2024-07-09T19:25:15Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/22352,"This pull request was built based on a group rule. Closing it will not ignore any of these versions in future pull requests. To ignore these dependencies, configure ignore rules in dependabot.yml"
yi,Bump psutil from 5.9.8 to 6.0.0,"Bumps psutil from 5.9.8 to 6.0.0.  Changelog Sourced from psutil's changelog.  6.0.0 20240618 Enhancements  2109_: maxfile and maxpath fields were removed from the namedtuple returned by disk_partitions()_. Reason: on network filesystems (NFS) this can potentially take a very long time to complete. 2366_, [Windows]: log debug message when using slower process APIs. 2375_, [macOS]: provide arm64 wheels.  (patch by Matthieu Darbois) 2396_: process_iter()_ no longer preemptively checks whether PIDs have been reused. This makes process_iter()_ around 20x times faster. 2396_: a new psutil.process_iter.cache_clear() API can be used the clear process_iter()_ internal cache. 2401_, Support building with freethreaded CPython 3.13. (patch by Sam Gross) 2407_: Process.connections()_ was renamed to Process.net_connections()_. The old name is still available, but it's deprecated (triggers a DeprecationWarning) and will be removed in the future. 2425_: [Linux]: provide aarch64 wheels.  (patch by Matthieu Darbois / Ben Raz)  Bug fixes  2250_, [NetBSD]: Process.cmdline()_ sometimes fail with EBUSY. It usually happens for long cmdlines with lots of arguments. In this case retry getting the cmdline for up to 50 times, and return an empty list as last resort. 2254_, [Linux]: offline cpus raise NotImplementedError in cpu_freq() (patch by Shade Gladden) 2272_: Add pickle support to psutil Exceptions. 2359_, [Windows], [CRITICAL]: pid_exists()_ disagrees with Process_ on whether a pid exists when ERROR_ACCESS_DENIED. 2360_, [macOS]: can't compile on macOS &lt; 10.13.  (patch by Ryan Schmidt) 2362_, [macOS]: can't compile on macOS 10.11.  (patch by Ryan Schmidt) 2365_, [macOS]: can't compile on macOS &lt; 10.9.  (patch by Ryan Schmidt) 2395_, [OpenBSD]: pid_exists()_ erroneously return True if the argument is a thread ID (TID) instead of a PID (process ID). 2412_, [macOS]: can't compile on macOS 10.4 PowerPC due to missing MNT_ constants.  Porting notes Version 6.0.0 introduces some changes which affect backward compatibility:  2109_: the namedtuple returned by disk_partitions()_' no longer has maxfile and maxpath fields. 2396_: process_iter()_ no longer preemptively checks whether PIDs have been reused. If you want to check for PID reusage you are supposed to use Process.is_running()_ against the yielded Process_ instances. That will also automatically remove reused PIDs from process_iter()_ internal cache.    ... (truncated)   Commits  3d5522a release 5b30ef4 Add aarch64 manylinux wheels ( CC(Improved argument checking for lax.broadcast_in_dim)) 1d092e7 test subprocesses: sleep() with an interval of 0.1 to make the test process m... 5f80c12 Fix  CC(jax.lax.{scan, map} break when supplied with an empty array), [macOS]: can't compile on macOS 10.4 PowerPC due to missing MNT_... 89b6096 process_iter(): use another global var to keep track of reused PIDs 9421bf8 openbsd: skip test if cmdline() returns [] due to EBUSY 4b1a054 Fix  CC(Port np.roots) / NetBSD / cmdline: retry on EBUSY. ( CC(Fix typo in README snippet)) 20be5ae ruff: enable and fix 'unused variable' rule 5530985 chore(ci): update actions ( CC(Clean up jax.numpy.__init__)) 1c7cb0a Don't build with limited API for 3.13 freethreaded build ( CC(add raisesexception notebook cell metadata)) Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-07-08T17:44:15Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/22328,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
rag,Bump packaging from 24.0 to 24.1,"Bumps packaging from 24.0 to 24.1.  Release notes Sourced from packaging's releases.  24.1 What's Changed  pyupgrade/black/isort/flake8 → ruff by @​DimitriPapadopoulos in pypa/packaging CC(fix travis by running `pytest n 1` instead of `n 2`) Add support for Python 3.13 and drop EOL 3.7 by @​hugovk in pypa/packaging CC(Fixes from pytype) Bump the githubactions group with 4 updates by @​dependabot in pypa/packaging CC(Improve behavior of a number of math functions for extreme inputs.) Fix typo in _parser docstring by @​pradyunsg in pypa/packaging CC(Cleaned up the GP regression example) Modernise type annotations using FA rules from ruff by @​pradyunsg in pypa/packaging CC(Check for failure of LAPACK calls and return NaNs on failure.) Document markers.default_environment() by @​edgarrmondragon in pypa/packaging CC(Incorrect results for reversemode differentiation of scan wrt sequence) Bump the githubactions group with 3 updates by @​dependabot in pypa/packaging CC(Make pmap lax.psum(1, 'i') and pxla.axis_index('i') work) Work around platform.python_version() returning non PEP 440 compliant version for nontagged CPython builds by @​sbidoul in pypa/packaging CC(REPL latency issue)  New Contributors  @​dependabot made their first contribution in pypa/packaging CC(Improve behavior of a number of math functions for extreme inputs.) @​edgarrmondragon made their first contribution in pypa/packaging CC(Incorrect results for reversemode differentiation of scan wrt sequence)  Full Changelog: https://github.com/pypa/packaging/compare/24.0...24.1    Changelog Sourced from packaging's changelog.  24.1  20240610  No unreleased changes.     Commits  85442b8 Bump for release 3e67fc7 Work around platform.python_version() returning non PEP 440 compliant versi... 32deafe Bump the githubactions group with 3 updates ( CC(Make pmap lax.psum(1, 'i') and pxla.axis_index('i') work)) e0dda88 Document markers.default_environment() ( CC(Incorrect results for reversemode differentiation of scan wrt sequence)) cc938f9 Modernise type annotations using FA rules from ruff ( CC(Check for failure of LAPACK calls and return NaNs on failure.)) 757f559 Fix typo in _parser docstring ( CC(Cleaned up the GP regression example)) ec9f203 Bump the githubactions group with 4 updates ( CC(Improve behavior of a number of math functions for extreme inputs.)) 5cbe1e4 Add support for Python 3.13 and drop EOL 3.7 ( CC(Fixes from pytype)) cb8fd38 pyupgrade/black/isort/flake8 → ruff ( CC(fix travis by running `pytest n 1` instead of `n 2`)) e8002b1 Bump for development See full diff in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-07-08T17:43:46Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/22326,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
yi,guidance on how to properly use lax.reduce on an multi-dimension array.,"I want to perform reduce operation on an array in a slightly different way than `jnp.argmax`. I have an array `A` with shape `(2,5)`. I wanted to reduce `A` in the below way: for a pair of index `(i, j)`, calculate `z = A[0][i]*A[1][j]A[0][j]*A[1][i]`, if `z>0`, then pick `i` otherwise pick `j`. Continue this computation until getting the final index.  What's the best way for me to perform this in JAX? Below is my code on trying to implement this reduce operation, which threw error `IndexError: Too many indices for array: 1 nonNone/Ellipsis indices for dim 0.`. ``` A = np.random.randn(2, 5) def my_compare(x0, x1):     y0 = x0[0]*x1[1]     y1 = x0[1]*x1[0]     z = y0 > y1     return jax.lax.select(z, x0, x1) res = jax.lax.reduce(A, A[0][0], my_compare, [0,1]) ```",2024-07-08T16:32:48Z,enhancement,closed,0,1,https://github.com/jax-ml/jax/issues/22320,This looks to be a duplicate of CC(未找到相关数据) – I'm going to close this one because the discussion format is probably better for this question!
transformer,Deprecate support for custom lowering rules that return tuple-wrapped ir.Values.,"Deprecate support for custom lowering rules that return tuplewrapped ir.Values. https://github.com/google/jax/pull/22211 forbade custom lowering rules from returning singleton tuples of ir.Value, but this appears to break downstream users, notably Transformer Engine. Instead, allow lowering rules to return singleton tuples and unwrap them if needed, but warn if this behavior is seen.",2024-07-08T14:16:58Z,,closed,0,0,https://github.com/jax-ml/jax/issues/22314
yi,FP8 XLA matmul fusion into `__cublas$lt$matmul$f8` not fully working ," Description To achieve optimal performance in FP8 training, one needs to fuse pre and postscaling as well as `amax` capture directly in the `cublasltmatmul` call. The XLA FP8 RFC describes the pseudo code corresponding, and to be best of my knowledge, these rules have been implemented in the OpenXLA Gemm rewriter pass. I am trying to get the codegen described in the XLA RFC directly from JAX, with the following FP8 matmul code: ```python def quantized_matmul(x_fp8, y_fp8, x_scale, y_scale, z_scale):      Dequantize x and y     f32_x = x_fp8.astype(np.float32) * x_scale     f32_y = y_fp8.astype(np.float32) * y_scale          Do the matmul     z = jax.lax.dot(f32_x, f32_y)      New z scale (delayed scaling)     new_z_scale = jax.lax.reduce_max_p.bind(jax.lax.abs(z), axes=(0, 1))      Quantize the matmul output with the old scale     return jax.lax.bitcast_convert_type(z / z_scale, jnp.float8_e4m3fn), new_z_scale ``` It generates the following stable HLO lowering: ```python module  attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {   func.func public (%arg0: tensor {mhlo.layout_mode = ""default""}, %arg1: tensor {mhlo.layout_mode = ""default""}, %arg2: tensor {mhlo.layout_mode = ""default""}, %arg3: tensor {mhlo.layout_mode = ""default""}, %arg4: tensor {mhlo.layout_mode = ""default""}) > (tensor {jax.result_info = ""[0]"", mhlo.layout_mode = ""default""}, tensor {jax.result_info = ""[1]"", mhlo.layout_mode = ""default""}) {     %0 = stablehlo.convert %arg0 : (tensor) > tensor     %1 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor) > tensor     %2 = stablehlo.multiply %0, %1 : tensor     %3 = stablehlo.convert %arg1 : (tensor) > tensor     %4 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor) > tensor     %5 = stablehlo.multiply %3, %4 : tensor     %6 = stablehlo.dot_general %2, %5, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor, tensor) > tensor     %7 = stablehlo.abs %6 : tensor     %cst = stablehlo.constant dense : tensor     %8 = stablehlo.reduce(%7 init: %cst) applies stablehlo.maximum across dimensions = [0, 1] : (tensor, tensor) > tensor     %9 = stablehlo.broadcast_in_dim %arg4, dims = [] : (tensor) > tensor     %10 = stablehlo.divide %6, %9 : tensor     %11 = stablehlo.bitcast_convert %10 : (tensor) > tensor     return %11, %8 : tensor, tensor   } } ``` which is (almost) equivalent to the HLO code describe in the RFC. Unfortunately, after compilation (i.e. `qn_compiled = jax.jit(quantized_matmul).lower(a, b, x_scale, y_scale, z_scale).compile()`), the HLO Module outputted has the following form: ```python HloModule jit_quantized_matmul, is_scheduled=true, entry_computation_layout={(f8e4m3fn[128,128]{1,0}, f8e4m3fn[128,128]{1,0}, f32[], f32[], f32[])>(f8e4m3fn[128,128,4]{2,1,0}, f32[])}, allow_spmd_sharding_propagation_to_parameters={true,true,true,true,true}, allow_spmd_sharding_propagation_to_output={true,true}, frontend_attributes={fingerprint_before_lhs=""e06ca42eb69ead880e64adfce609bb08""} %region_0.15 (Arg_0.16: f32[], Arg_1.17: f32[]) > f32[] {   %Arg_1.17 = f32[] parameter(1)   %Arg_0.16 = f32[] parameter(0)   ROOT %maximum.1 = f32[] maximum(f32[] %Arg_0.16, f32[] %Arg_1.17) } %fused_reduce (param_0.10: f32[128,128]) > f32[] {   %param_0.10 = f32[128,128]{1,0} parameter(0)   %abs.1.1 = f32[128,128]{1,0} abs(f32[128,128]{1,0} %param_0.10)   %bitcast.82.1 = f32[16384]{0} bitcast(f32[128,128]{1,0} %abs.1.1)   %constant_6_1 = f32[] constant(inf)   ROOT %reduce.19.1 = f32[] reduce(f32[16384]{0} %bitcast.82.1, f32[] %constant_6_1), dimensions={0}, to_apply=%region_0.15 } %fused_bitcast_convert (param_0.11: f32[128,128], param_1.15: f32[]) > f8e4m3fn[128,128,4] {   %param_0.11 = f32[128,128]{1,0} parameter(0)   %param_1.15 = f32[] parameter(1)   %broadcast.17.3 = f32[128,128]{1,0} broadcast(f32[] %param_1.15), dimensions={}   %divide.1.3 = f32[128,128]{1,0} divide(f32[128,128]{1,0} %param_0.11, f32[128,128]{1,0} %broadcast.17.3)   %bitcastconvert.8.3 = u32[128,128]{1,0} bitcastconvert(f32[128,128]{1,0} %divide.1.3)   %broadcast.18.3 = u32[128,128,4]{2,1,0} broadcast(u32[128,128]{1,0} %bitcastconvert.8.3), dimensions={0,1}   %constant_0_1 = u32[] constant(8)   %broadcast.19.3 = u32[128,128,4]{2,1,0} broadcast(u32[] %constant_0_1), dimensions={}   %iota.2.3 = u32[128,128,4]{2,1,0} iota(), iota_dimension=2   %multiply.4.3 = u32[128,128,4]{2,1,0} multiply(u32[128,128,4]{2,1,0} %broadcast.19.3, u32[128,128,4]{2,1,0} %iota.2.3)   %shiftrightlogical.2.3 = u32[128,128,4]{2,1,0} shiftrightlogical(u32[128,128,4]{2,1,0} %broadcast.18.3, u32[128,128,4]{2,1,0} %multiply.4.3)   %constant_1_1 = u32[] constant(255)   %broadcast.21.3 = u32[128,128,4]{2,1,0} broadcast(u32[] %constant_1_1), dimensions={}   %and.2.3 = u32[128,128,4]{2,1,0} and(u32[128,128,4]{2,1,0} %shiftrightlogical.2.3, u32[128,128,4]{2,1,0} %broadcast.21.3)   %convert.4.3 = u8[128,128,4]{2,1,0} convert(u32[128,128,4]{2,1,0} %and.2.3)   ROOT %bitcastconvert.9.1 = f8e4m3fn[128,128,4]{2,1,0} bitcastconvert(u8[128,128,4]{2,1,0} %convert.4.3) } %wrapped_transpose_computation (param_0.12: f8e4m3fn[128,128]) > f8e4m3fn[128,128] {   %param_0.12 = f8e4m3fn[128,128]{1,0} parameter(0)   ROOT %transpose.1.1 = f8e4m3fn[128,128]{1,0} transpose(f8e4m3fn[128,128]{1,0} %param_0.12), dimensions={1,0} } ENTRY %main.24 (Arg_0.1.0: f8e4m3fn[128,128], Arg_1.2.0: f8e4m3fn[128,128], Arg_2.3.0: f32[], Arg_3.4.0: f32[], Arg_4.5.0: f32[]) > (f8e4m3fn[128,128,4], f32[]) {   %constant_2_0 = f32[] constant(1)   %Arg_4.5.0 = f32[] parameter(4)   %Arg_3.4.0 = f32[] parameter(3)   %Arg_2.3.0 = f32[] parameter(2)   %Arg_1.2.0 = f8e4m3fn[128,128]{1,0} parameter(1)   %Arg_0.1.0 = f8e4m3fn[128,128]{1,0} parameter(0)   %wrapped_transpose = f8e4m3fn[128,128]{1,0} fusion(f8e4m3fn[128,128]{1,0} %Arg_1.2.0), kind=kInput, calls=%wrapped_transpose_computation   %cublasgemm.1.0 = (f32[128,128]{1,0}, s8[33554432]{0}) customcall(f8e4m3fn[128,128]{1,0} %Arg_0.1.0, f8e4m3fn[128,128]{1,0} %wrapped_transpose, f32[] %Arg_2.3.0, f32[] %Arg_3.4.0, f32[] %constant_2_0, /*index=5*/f32[] %constant_2_0), custom_call_target=""__cublas$lt$matmul$f8"" backend_cfg:  {     ""operation_queue_id"": ""0"",     ""wait_on_operation_queues"": [],     ""gemm_backend_config"": {         ""alpha_real"": 1,         ""alpha_imag"": 0,         ""beta"": 0,         ""dot_dimension_numbers"": {             ""lhs_contracting_dimensions"": [                 ""1""             ],             ""rhs_contracting_dimensions"": [                 ""1""             ],             ""lhs_batch_dimensions"": [],             ""rhs_batch_dimensions"": []         },         ""precision_config"": {             ""operand_precision"": [                 ""DEFAULT"",                 ""DEFAULT""             ],             ""algorithm"": ""ALG_UNSET""         },         ""epilogue"": ""DEFAULT"",         ""damax_output"": false,         ""selected_algorithm"": ""2"",         ""lhs_stride"": ""16384"",         ""rhs_stride"": ""16384"",         ""grad_x"": false,         ""grad_y"": false     },     ""force_earliest_schedule"": false }   %gettupleelement.1 = f32[128,128]{1,0} gettupleelement((f32[128,128]{1,0}, s8[33554432]{0}) %cublasgemm.1.0), index=0   %input_reduce_fusion = f32[] fusion(f32[128,128]{1,0} %gettupleelement.1), kind=kInput, calls=%fused_reduce   %loop_bitcast_convert_fusion = f8e4m3fn[128,128,4]{2,1,0} fusion(f32[128,128]{1,0} %gettupleelement.1, f32[] %Arg_4.5.0), kind=kLoop, calls=%fused_bitcast_convert   ROOT %tuple.23.0 = (f8e4m3fn[128,128,4]{2,1,0}, f32[]) tuple(f8e4m3fn[128,128,4]{2,1,0} %loop_bitcast_convert_fusion, f32[] %input_reduce_fusion) } ``` In short: the FP8 matmul with inputs scaling is recognized, generating a custom call to `__cublas$lt$matmul$f8`. But the HLO compilation fails to recognize the postmatmul pattern of amax catpure, rescaling and casting back to FP8. Note: the same happens when adding the `clamp` call described in the RFC `jax.lax.clamp(np.float32(448), z / z_scale, np.float32(448))`. Do you have an idea of why the full op fusing is not happening? And more generally, when the bug is fixed, I believe it would be great for user to have an advanced FP8 documentation describing this codegen (and a unit test making there is no regression!). Edit: * OpenXLA hash in Jaxlib 0.4.30: https://github.com/google/jax/blob/jaxlibv0.4.30/third_party/xla/workspace.bzl * XLA gemm rewriter for this hash: https://github.com/openxla/xla/blob/79fd5733f99b3c0948d7202bc1bbe1ee3980da5c/xla/service/gpu/gemm_rewriter.ccL1305  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.30 jaxlib: 0.4.30 numpy:  2.0.0 python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] jax.devices (1 total, 1 local): [cuda(id=0)] process_count: 1 platform: uname_result(system='Linux', node='20920159107', release='6.2.037generic', version=' CC(Require protobuf 3.6.0 or later)~22.04.1Ubuntu SMP PREEMPT_DYNAMIC Thu Nov  2 18:01:13 UTC 2', machine='x86_64') $ nvidiasmi Mon Jul  8 11:16:21 2024        ++  ++++ ```",2024-07-08T11:37:18Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/22313,This is probably a bug for openxla/xla.,"Apologies, the mistake was on my side. Wrongly used `bitcast_convert_type` instead of `convert_element_type`. For future reference, the following piece of code is working properly: ```python def quantized_matmul(x_fp8, y_fp8, x_scale, y_scale, z_scale):      Dequantize x and y     f32_x = x_fp8.astype(np.float32) * x_scale     f32_y = y_fp8.astype(np.float32) * y_scale          Do the matmul     z = jax.lax.dot(f32_x, f32_y)      New z scale (delayed scaling)     new_z_scale = jax.lax.reduce_max_p.bind(jax.lax.abs(z), axes=(0, 1))      Rescale & clamp.     z = jax.lax.clamp(np.float32(448), z / z_scale, np.float32(448))      Quantize the matmul output with the old scale     return jax.lax.convert_element_type(z, jnp.float8_e4m3fn), new_z_scale ```"
rag,Profiler fails on kaggle tpu v3-8 notebook," Description While on kaggle notebook, upgrading to newer version of jax[tpu] breaks profiler.  The follower command was used. (tested with 0.4.29, 0.4.30) `!pip install U optax jax[tpu]==0.4.30 f https://storage.googleapis.com/jaxreleases/libtpu_releases.html` Running a simple example code (as followed) caused an error ``` with jax.profiler.trace(""/tmp/jaxtrace"", create_perfetto_link=True):    Run the operations to be profiled   key = jax.random.key(0)   x = jax.random.normal(key, (5000, 5000))   y = x @ x   y.block_until_ready() ``` It caused the following error. ``` WARNING: Logging before InitGoogle() is written to STDERR E0000 00:00:1720206589.371430    9554 common_lib.cc:818] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=""local"" === Source Location Trace: === learning/45eac/tfrc/runtime/common_lib.cc:483 jax:    0.4.29 jaxlib: 0.4.29 numpy:  1.26.4 python: 3.10.14 (main, May 14 2024, 08:39:53) [GCC 12.2.0] jax.devices (8 total, 8 local): [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0) TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1) ... TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0) TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)] process_count: 1 platform: uname_result(system='Linux', node='88c1b3bf794b', release='6.1.42+', version=' CC(Python 3 compatibility issues) SMP PREEMPT_DYNAMIC Sun Oct  8 14:23:56 UTC 2023', machine='x86_64') 20240705 19:09:53.244284: F external/local_xla/xla/stream_executor/tpu/tpu_executor_init_fns.inc:25] TpuExecutor_AllocateStream not available in this library. https://symbolize.stripped_domain/r/?trace=7c7d8eec9e2c,7c7d8ee7b04f&map=  *** SIGABRT received by PID 9554 (TID 9554) on cpu 31 from PID 9554; stack trace: *** PC: @     0x7c7d8eec9e2c  (unknown)  (unknown)     @     0x7c7966e9c181        944  (unknown)     @     0x7c7d8ee7b050  (unknown)  (unknown)     @     0x7c7d8ed5f4f0  (unknown)  (unknown) https://symbolize.stripped_domain/r/?trace=7c7d8eec9e2c,7c7966e9c180,7c7d8ee7b04f,7c7d8ed5f4ef&map=  E0705 19:09:53.263036    9554 coredump_hook.cc:316] RAW: Remote crash data gathering hook invoked. E0705 19:09:53.263053    9554 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec. E0705 19:09:53.263058    9554 coredump_hook.cc:411] RAW: Sending fingerprint to remote end. E0705 19:09:53.263080    9554 coredump_hook.cc:420] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory E0705 19:09:53.263087    9554 coredump_hook.cc:472] RAW: Dumping core locally. E0705 19:10:28.370369    9554 process_state.cc:805] RAW: Raising signal 6 with default behavior Aborted (core dumped) ``` Also tested without actually running any jax computation, and still got same error. ``` import jax jax.print_environment_info() with jax.profiler.trace(""/tmp/jaxtrace""):     print('wtf') ```  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.29 jaxlib: 0.4.29 numpy:  1.26.4 python: 3.10.14 (main, May 14 2024, 08:39:53) [GCC 12.2.0] jax.devices (8 total, 8 local): [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0) TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1) ... TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0) TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)] process_count: 1 platform: uname_result(system='Linux', node='88c1b3bf794b', release='6.1.42+', version=' CC(Python 3 compatibility issues) SMP PREEMPT_DYNAMIC Sun Oct  8 14:23:56 UTC 2023', machine='x86_64')",2024-07-05T19:14:24Z,bug,closed,1,4,https://github.com/jax-ml/jax/issues/22294,Also tested with jax version 0.4.30. Same result,"Cakes Can you share public Kaggle notebook that fully repros? One other thing you can usually try is adding `!pip install tensorflowcpu` before anything else. Sometimes, having tensorflowtpu installed can break the other TPUenabled frameworks.","Also, are you by any chance updating jax + jaxlib without using the full jax[tpu] command? https://github.com/google/jax?tab=readmeovfileinstructions","> Cakes Can you share public Kaggle notebook that fully repros? >  > One other thing you can usually try is adding `!pip install tensorflowcpu` before anything else. Sometimes, having tensorflowtpu installed can break the other TPUenabled frameworks. Here is a public notebook showing it Further testing shows it only occurs when updating jax to 0.4.30 version. Testing with kaggle's native 0.4.23 doesn't trigger the error.  `!pip install tensorflowcpu` fixes the problem. So for anyone stumbling upon the same error, do `!pip install tensorflowcpu` I do not understand why tensorflowcpu isn't required for 0.4.23 and why pip doesn't install it as dependency."
rag,Persistent compilation cache hit but tracing cache miss?," Description For this simple test program: ```python import os os.environ[""JAX_DEBUG_LOG_MODULES""] = ""jax._src.compiler,jax._src.lru_cache"" os.environ[""JAX_PLATFORMS""] = ""cpu"" import jax jax.config.update(""jax_compilation_cache_dir"", ""jaxcache"") jax.config.update(""jax_persistent_cache_min_entry_size_bytes"", 1) jax.config.update(""jax_persistent_cache_min_compile_time_secs"", 0) jax.config.update(""jax_explain_cache_misses"", True) import jax.numpy as jnp x = jnp.ones((16000, 16000)) .jit def fn1(y):     return x + y with jax.log_compiles():     print(fn1(jnp.array(1.0))) ``` I get the following somewhat perplexing behavior: ``` ❯ rm r jaxcache/ ❯ time python m ml_scratch.compile TRACING CACHE MISS at /home/.../code/mlscratch/ml_scratch/compile.py:17:4 () because:   never seen function:     convert_element_type id=139717684748640 defined at /nix/store/769bv4hg2xakyd6fw81sg8h1i6sp3a1cpython33.11.9env/lib/python3.11/sitepackages/jax/_src/dispatch.py:94 DEBUG:20240704 16:05:36,312:jax._src.compiler:143: get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]] DEBUG:20240704 16:05:36,312:jax._src.compiler:202: get_compile_options XLAAutoFDO profile: using XLAAutoFDO profile version 1 DEBUG:20240704 16:05:36,312:jax._src.lru_cache:97: Cache miss for key: 'jit_convert_element_type5fc83c2910217f17488f9c7585413b9b9b02cedb2005cbc8fb49e3610626c23e' DEBUG:20240704 16:05:36,318:jax._src.compiler:678: 'jit_convert_element_type' took at least 0.00 seconds to compile (0.01s) TRACING CACHE MISS at /home/.../code/mlscratch/ml_scratch/compile.py:17:4 () because:   never seen function:     broadcast_in_dim id=139716578421600 defined at /nix/store/769bv4hg2xakyd6fw81sg8h1i6sp3a1cpython33.11.9env/lib/python3.11/sitepackages/jax/_src/dispatch.py:94   but seen another function defined on the same line; maybe the function is   being redefined repeatedly, preventing caching? DEBUG:20240704 16:05:36,322:jax._src.compiler:143: get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]] DEBUG:20240704 16:05:36,322:jax._src.compiler:202: get_compile_options XLAAutoFDO profile: using XLAAutoFDO profile version 1 DEBUG:20240704 16:05:36,322:jax._src.lru_cache:97: Cache miss for key: 'jit_broadcast_in_dim875c687931df48ac3248b1af85f83a6540dfea8a16b64dac46b4f5385f8cf24e' DEBUG:20240704 16:05:36,330:jax._src.compiler:678: 'jit_broadcast_in_dim' took at least 0.00 seconds to compile (0.01s) Finished tracing + transforming convert_element_type for pjit in 0.00023603439331054688 sec TRACING CACHE MISS at /home/.../code/mlscratch/ml_scratch/compile.py:26:14 () because:   never seen function:     convert_element_type id=139716580005568 defined at /nix/store/769bv4hg2xakyd6fw81sg8h1i6sp3a1cpython33.11.9env/lib/python3.11/sitepackages/jax/_src/dispatch.py:94 Compiling convert_element_type with global shapes and types [ShapedArray(float32[])]. Argument mapping: [UnspecifiedValue]. Finished jaxpr to MLIR module conversion jit(convert_element_type) in 0.001786947250366211 sec DEBUG:20240704 16:05:36,397:jax._src.compiler:143: get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]] DEBUG:20240704 16:05:36,398:jax._src.compiler:202: get_compile_options XLAAutoFDO profile: using XLAAutoFDO profile version 1 DEBUG:20240704 16:05:36,398:jax._src.lru_cache:97: Cache miss for key: 'jit_convert_element_type358792f083aa925ad9c17149144fa7d64e64aa8d6b205621a677bbbfb614c735' DEBUG:20240704 16:05:36,402:jax._src.compiler:678: 'jit_convert_element_type' took at least 0.00 seconds to compile (0.00s) Finished XLA compilation of jit(convert_element_type) in 0.00501251220703125 sec Finished tracing + transforming add for pjit in 0.00035691261291503906 sec Finished tracing + transforming fn1 for pjit in 0.0007784366607666016 sec TRACING CACHE MISS at /home/.../code/mlscratch/ml_scratch/compile.py:26:10 () because:   never seen function:     fn1 id=139716577389888 defined at /home/.../code/mlscratch/ml_scratch/compile.py:20 Compiling fn1 with global shapes and types [ShapedArray(float32[], weak_type=True)]. Argument mapping: [UnspecifiedValue]. Finished jaxpr to MLIR module conversion jit(fn1) in 0.368039608001709 sec DEBUG:20240704 16:05:36,773:jax._src.compiler:143: get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]] DEBUG:20240704 16:05:36,773:jax._src.compiler:202: get_compile_options XLAAutoFDO profile: using XLAAutoFDO profile version 1 DEBUG:20240704 16:05:36,773:jax._src.lru_cache:97: Cache miss for key: 'jit_fn14b667dc9233abd90ebf5e6c157ae7c3ab98be64c96bef95b332fcdae22d91b06' DEBUG:20240704 16:05:36,782:jax._src.compiler:678: 'jit_fn1' took at least 0.00 seconds to compile (0.01s) Finished XLA compilation of jit(fn1) in 0.00935673713684082 sec [[2. 2. 2. ... 2. 2. 2.]  [2. 2. 2. ... 2. 2. 2.]  [2. 2. 2. ... 2. 2. 2.]  ...  [2. 2. 2. ... 2. 2. 2.]  [2. 2. 2. ... 2. 2. 2.]  [2. 2. 2. ... 2. 2. 2.]] real	0m0.986s user	0m1.267s sys	0m1.904s ❯ ls jaxcache/ jit_broadcast_in_dim875c687931df48ac3248b1af85f83a6540dfea8a16b64dac46b4f5385f8cf24e      jit_convert_element_type5fc83c2910217f17488f9c7585413b9b9b02cedb2005cbc8fb49e3610626c23e jit_convert_element_type358792f083aa925ad9c17149144fa7d64e64aa8d6b205621a677bbbfb614c735  jit_fn14b667dc9233abd90ebf5e6c157ae7c3ab98be64c96bef95b332fcdae22d91b06 ❯ time python m ml_scratch.compile TRACING CACHE MISS at /home/.../code/mlscratch/ml_scratch/compile.py:17:4 () because:   never seen function:     convert_element_type id=140718745313632 defined at /nix/store/769bv4hg2xakyd6fw81sg8h1i6sp3a1cpython33.11.9env/lib/python3.11/sitepackages/jax/_src/dispatch.py:94 DEBUG:20240704 16:05:47,413:jax._src.compiler:143: get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]] DEBUG:20240704 16:05:47,413:jax._src.compiler:202: get_compile_options XLAAutoFDO profile: using XLAAutoFDO profile version 1 DEBUG:20240704 16:05:47,414:jax._src.lru_cache:100: Cache hit for key: 'jit_convert_element_type5fc83c2910217f17488f9c7585413b9b9b02cedb2005cbc8fb49e3610626c23e' DEBUG:20240704 16:05:47,415:jax._src.compiler:333: Persistent compilation cache hit for 'jit_convert_element_type' TRACING CACHE MISS at /home/.../code/mlscratch/ml_scratch/compile.py:17:4 () because:   never seen function:     broadcast_in_dim id=140717638216544 defined at /nix/store/769bv4hg2xakyd6fw81sg8h1i6sp3a1cpython33.11.9env/lib/python3.11/sitepackages/jax/_src/dispatch.py:94   but seen another function defined on the same line; maybe the function is   being redefined repeatedly, preventing caching? DEBUG:20240704 16:05:47,417:jax._src.compiler:143: get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]] DEBUG:20240704 16:05:47,418:jax._src.compiler:202: get_compile_options XLAAutoFDO profile: using XLAAutoFDO profile version 1 DEBUG:20240704 16:05:47,418:jax._src.lru_cache:100: Cache hit for key: 'jit_broadcast_in_dim875c687931df48ac3248b1af85f83a6540dfea8a16b64dac46b4f5385f8cf24e' DEBUG:20240704 16:05:47,418:jax._src.compiler:333: Persistent compilation cache hit for 'jit_broadcast_in_dim' Finished tracing + transforming convert_element_type for pjit in 0.0002205371856689453 sec TRACING CACHE MISS at /home/.../code/mlscratch/ml_scratch/compile.py:26:14 () because:   never seen function:     convert_element_type id=140717639800512 defined at /nix/store/769bv4hg2xakyd6fw81sg8h1i6sp3a1cpython33.11.9env/lib/python3.11/sitepackages/jax/_src/dispatch.py:94 Compiling convert_element_type with global shapes and types [ShapedArray(float32[])]. Argument mapping: [UnspecifiedValue]. Finished jaxpr to MLIR module conversion jit(convert_element_type) in 0.0018596649169921875 sec DEBUG:20240704 16:05:47,486:jax._src.compiler:143: get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]] DEBUG:20240704 16:05:47,486:jax._src.compiler:202: get_compile_options XLAAutoFDO profile: using XLAAutoFDO profile version 1 DEBUG:20240704 16:05:47,487:jax._src.lru_cache:100: Cache hit for key: 'jit_convert_element_type358792f083aa925ad9c17149144fa7d64e64aa8d6b205621a677bbbfb614c735' DEBUG:20240704 16:05:47,487:jax._src.compiler:333: Persistent compilation cache hit for 'jit_convert_element_type' Finished XLA compilation of jit(convert_element_type) in 0.0008900165557861328 sec Finished tracing + transforming add for pjit in 0.000370025634765625 sec Finished tracing + transforming fn1 for pjit in 0.0007677078247070312 sec TRACING CACHE MISS at /home/.../code/mlscratch/ml_scratch/compile.py:26:10 () because:   never seen function:     fn1 id=140717637184832 defined at /home/.../code/mlscratch/ml_scratch/compile.py:20 Compiling fn1 with global shapes and types [ShapedArray(float32[], weak_type=True)]. Argument mapping: [UnspecifiedValue]. Finished jaxpr to MLIR module conversion jit(fn1) in 0.37015557289123535 sec DEBUG:20240704 16:05:47,859:jax._src.compiler:143: get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]] DEBUG:20240704 16:05:47,860:jax._src.compiler:202: get_compile_options XLAAutoFDO profile: using XLAAutoFDO profile version 1 DEBUG:20240704 16:05:47,860:jax._src.lru_cache:100: Cache hit for key: 'jit_fn14b667dc9233abd90ebf5e6c157ae7c3ab98be64c96bef95b332fcdae22d91b06' DEBUG:20240704 16:05:47,861:jax._src.compiler:333: Persistent compilation cache hit for 'jit_fn1' Finished XLA compilation of jit(fn1) in 0.0010833740234375 sec [[2. 2. 2. ... 2. 2. 2.]  [2. 2. 2. ... 2. 2. 2.]  [2. 2. 2. ... 2. 2. 2.]  ...  [2. 2. 2. ... 2. 2. 2.]  [2. 2. 2. ... 2. 2. 2.]  [2. 2. 2. ... 2. 2. 2.]] real	0m0.985s user	0m1.174s sys	0m1.833s ``` Note that:  The cache does actually get filled  Compiler debug message mentions a cache hit  Explain cache misses reports a cache miss  The overall execution time doesn't actually decrease (closing over the large constant is just to encourage nonnegligible compilation times)  The `id` reported for each function is distinct on each run I get similar behavior running this code (without the `JAX_PLATFORMS=cpu` bit) on a colaboratory GPU: ``` WARNING:jax._src.pjit:TRACING CACHE MISS at /content/compile.py:14 () because:   never seen function:     convert_element_type id=136619115461456 defined at /usr/local/lib/python3.10/distpackages/jax/_src/dispatch.py:97 DEBUG:20240704 23:01:51,915:jax._src.compiler:145: get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[cuda(id=0)]] DEBUG:jax._src.compiler:get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[cuda(id=0)]] DEBUG:20240704 23:01:51,928:jax._src.compiler:204: get_compile_options XLAAutoFDO profile: using XLAAutoFDO profile version 1 DEBUG:jax._src.compiler:get_compile_options XLAAutoFDO profile: using XLAAutoFDO profile version 1 /usr/local/lib/python3.10/distpackages/jax/_src/compiler.py:522: UserWarning: Error reading persistent compilation cache entry for 'jit_convert_element_type': XlaRuntimeError: INTERNAL: Failed to parse serialized GpuThunkAotCompilationResult.   warnings.warn( DEBUG:20240704 23:01:51,997:jax._src.compiler:556: 'jit_convert_element_type' took at least 0.00 seconds to compile (0.06s) DEBUG:jax._src.compiler:'jit_convert_element_type' took at least 0.00 seconds to compile (0.06s) WARNING:jax._src.pjit:TRACING CACHE MISS at /content/compile.py:14 () because:   never seen function:     broadcast_in_dim id=136619115457424 defined at /usr/local/lib/python3.10/distpackages/jax/_src/dispatch.py:97   but seen another function defined on the same line; maybe the function is   being redefined repeatedly, preventing caching? DEBUG:20240704 23:01:52,320:jax._src.compiler:145: get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[cuda(id=0)]] DEBUG:jax._src.compiler:get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[cuda(id=0)]] DEBUG:20240704 23:01:52,331:jax._src.compiler:204: get_compile_options XLAAutoFDO profile: using XLAAutoFDO profile version 1 DEBUG:jax._src.compiler:get_compile_options XLAAutoFDO profile: using XLAAutoFDO profile version 1 DEBUG:20240704 23:01:52,337:jax._src.compiler:290: Persistent compilation cache hit for 'jit_broadcast_in_dim' DEBUG:jax._src.compiler:Persistent compilation cache hit for 'jit_broadcast_in_dim' WARNING:jax._src.dispatch:Finished tracing + transforming convert_element_type for pjit in 0.00041174888610839844 sec WARNING:jax._src.pjit:TRACING CACHE MISS at /content/compile.py:23 () because:   never seen function:     convert_element_type id=136618417824400 defined at /usr/local/lib/python3.10/distpackages/jax/_src/dispatch.py:97 WARNING:jax._src.interpreters.pxla:Compiling convert_element_type for with global shapes and types [ShapedArray(float32[])]. Argument mapping: [UnspecifiedValue]. WARNING:jax._src.dispatch:Finished jaxpr to MLIR module conversion jit(convert_element_type) in 0.0042226314544677734 sec DEBUG:20240704 23:01:52,355:jax._src.compiler:145: get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[cuda(id=0)]] DEBUG:jax._src.compiler:get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[cuda(id=0)]] DEBUG:20240704 23:01:52,361:jax._src.compiler:204: get_compile_options XLAAutoFDO profile: using XLAAutoFDO profile version 1 DEBUG:jax._src.compiler:get_compile_options XLAAutoFDO profile: using XLAAutoFDO profile version 1 DEBUG:20240704 23:01:52,373:jax._src.compiler:556: 'jit_convert_element_type' took at least 0.00 seconds to compile (0.01s) DEBUG:jax._src.compiler:'jit_convert_element_type' took at least 0.00 seconds to compile (0.01s) WARNING:jax._src.dispatch:Finished XLA compilation of jit(convert_element_type) in 0.021401643753051758 sec WARNING:jax._src.dispatch:Finished tracing + transforming fn for pjit in 0.000827789306640625 sec WARNING:jax._src.dispatch:Finished tracing + transforming fn1 for pjit in 0.006087303161621094 sec WARNING:jax._src.pjit:TRACING CACHE MISS at /content/compile.py:23 () because:   never seen function:     fn1 id=136618417825840 defined at /content/compile.py:17   but seen another function defined on the same line; maybe the function is   being redefined repeatedly, preventing caching? WARNING:jax._src.interpreters.pxla:Compiling fn1 for with global shapes and types [ShapedArray(float32[], weak_type=True)]. Argument mapping: [UnspecifiedValue]. WARNING:jax._src.dispatch:Finished jaxpr to MLIR module conversion jit(fn1) in 2.324035882949829 sec DEBUG:20240704 23:01:54,735:jax._src.compiler:145: get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[cuda(id=0)]] DEBUG:jax._src.compiler:get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[cuda(id=0)]] DEBUG:20240704 23:01:54,742:jax._src.compiler:204: get_compile_options XLAAutoFDO profile: using XLAAutoFDO profile version 1 DEBUG:jax._src.compiler:get_compile_options XLAAutoFDO profile: using XLAAutoFDO profile version 1 DEBUG:20240704 23:01:54,750:jax._src.compiler:290: Persistent compilation cache hit for 'jit_fn1' DEBUG:jax._src.compiler:Persistent compilation cache hit for 'jit_fn1' WARNING:jax._src.dispatch:Finished XLA compilation of jit(fn1) in 0.007249593734741211 sec [[2. 2. 2. ... 2. 2. 2.]  [2. 2. 2. ... 2. 2. 2.]  [2. 2. 2. ... 2. 2. 2.]  ...  [2. 2. 2. ... 2. 2. 2.]  [2. 2. 2. ... 2. 2. 2.]  [2. 2. 2. ... 2. 2. 2.]] ``` Though here we also get a parsing error when attempting to read from the cache.  System info (python version, jaxlib version, accelerator, etc.) Local device ``` jax:    0.4.30 jaxlib: 0.4.30 numpy:  1.26.1 python: 3.11.9 (main, Apr  2 2024, 08:25:04) [GCC 13.3.0] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='nixos', release='6.6.32', version=' CC(Python 3 compatibility issues)NixOS SMP PREEMPT_DYNAMIC Sat May 25 14:22:56 UTC 2024', machine='x86_64') ``` Colaboratory ``` jax:    0.4.26 jaxlib: 0.4.26 numpy:  1.25.2 python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] jax.devices (1 total, 1 local): [cuda(id=0)] process_count: 1 platform: uname_result(system='Linux', node='3c435d2eab1b', release='6.1.85+', version=' CC(Python 3 compatibility issues) SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024', machine='x86_64') $ nvidiasmi Thu Jul  4 23:12:42 2024        ++  ++ ```",2024-07-04T23:16:27Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/22281, ,"Tracing cache is a *nonpersistent* perfunction cache to make sure we do not recompile for each individual function call with same argument shapes. In each run, the first call to each function will always miss that cache. Also note that `id` here is the memory address of the closure. It is fully expected to be different from run to run. Note that the persistent cache did hit for your function: ``` DEBUG:20240704 16:05:47,861:jax._src.compiler:333: Persistent compilation cache hit for 'jit_fn1' ```","Ah, okay, I suspected I was missing something here (because of the `id` is the memory address fact, the cache hit log message, etc). And then the persistent cache is relevant for the ""Finished XLA compilation"" step (this message has slightly different meanings when there's a cache hit vs miss) but not the ""Finished jaxpr to MLIR module conversion"" (where most time is spent in my test case)?","That's right: to get a disk compilation cache hit in a new process, we first redo the work of tracing the Python function to a jaxpr and then lowering the jaxpr to an MLIR module. Only then do we fingerprint the MLIR module and notice we have a disk cache entry for it. I think this issue is resolved so let's close it, but please let me know if I'm mistaken."
yi,Fix compatibility with nightly numpy,"Numpy recently merged support for the 2023.12 revision of the Array API: https://github.com/numpy/numpy/pull/26724 This breaks two of our tests: 1. The first breakage was caused by differences in how numpy and JAX cast negative floats to `uint8`. Specifically `np.float32(1).astype(np.uint8)` returns `np.uint8(255)` whereas `jnp.float32(1).astype(jnp.uint8)` produces `Array(0, dtype=uint8)`. We don't make any promises about consistency with casting floats to ints, noting that this can even be backend dependent. ~~I don't believe this failure is identifying any unexpected behavior, and we test many other dtypes properly so I'm not concerned about skipping this test.~~ To fix our test, we now only generate positive inputs when the output dtype is unsigned. 2. The second failure was caused by the fact that the approach we took in CC(Update `jnp.clip` to Array API 2023 standard and introduces `jax.experimental.array_api.clip`) to support backwards compatibility and the Array API for `clip` differs from the one used in numpy/numpy CC(未找到相关数据). Again, the behavior is consistent, but it produces a different signature. I've skipped checking `clip`'s signature, but we should revisit it once the `a_min` and `a_max` parameters have been removed from JAX. Fixes CC(⚠️ Nightly upstreamdev CI failed ⚠️)",2024-07-03T14:46:16Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/22257
transformer,Fine-grained remat policy makes async/pipelined collectives execute in the main stream," Description Hi, I have following setup:  Transformer model with N layers scanned over input  fully sharded data parallel sharding  asynchronous communications (latencyhiding scheduler, pipelined allgather,allreduce,reducescatter) I'm using following flags: ```bash xla_gpu_graph_level=0  xla_gpu_enable_latency_hiding_scheduler=true  xla_gpu_enable_all_gather_combine_by_dim=false  xla_gpu_enable_reduce_scatter_combine_by_dim=false  xla_gpu_enable_pipelined_all_gather=true  xla_gpu_enable_pipelined_reduce_scatter=true  xla_gpu_enable_pipelined_all_reduce=true  xla_gpu_enable_pipelined_collectives=false  xla_gpu_enable_while_loop_double_buffering=true  xla_gpu_enable_highest_priority_async_stream=true  xla_gpu_all_reduce_combine_threshold_bytes=2147483648  xla_gpu_all_gather_combine_threshold_bytes=2147483648   xla_gpu_reduce_scatter_combine_threshold_bytes=2147483648 xla_gpu_disable_async_collectives=collectivebroadcast,alltoall,collectivepermute ``` To speedup backward by finegrained reduction of activations recomputation, I marked each dense layer's output in transformer block with specific name: ``` result = jax.lax.dot_general(     inputs,     kernel,     dimension_numbers=((axis, contract_ind), ((), ())),     precision=self.precision,     preferred_element_type=self.accumulator_dtype, ) result = jax.ad_checkpoint.checkpoint_name(result, self.activation_dot_name) ``` So, for example, in attention layer I have ""dot_attention_query"", ""dot_attention_key"", ""dot_attention_value"", ""dot_attention_out"".  And then I apply checkpoint policy on scanned function which accepts list of activation names to checkpoint: ``` def rematted_layer(layer):     return nn.remat(         layer,         policy=jax.checkpoint_policies.save_only_these_names(             *self.config.save_names_for_bwd         ),         prevent_cse=not self.config.scan,     ) ``` and then scan It over embeddings: ``` apply_block = rematted_layer(apply_block) apply_block = nn.scan(     apply_block,     length=self.config.num_layers,     variable_axes={         ""params"": 0,     },     variable_broadcast=False,     split_rngs={""params"": True},     metadata_params={nn.PARTITION_NAME: ""layers""}, ) block = TransformerBlock(     name=""scan"",     config=self.config.block, ) embeddings, _ = apply_block(block, embeddings, None) ``` If I set self.config.save_names_for_bwd to empty list (which is basically equivalent to ""nothing_saveable"" policy), then communications works correctly  allgather/reducescatters/allreduces are overlapped with computations, as can be seen on this perfetto trace:  nothing_saveable.tgz But as soon as I start to specify some names in self.config.save_names_for_bwd, for example,  ```yaml     save_names_for_bwd:        dot_mlp_out        dot_attention_value        dot_attention_query        dot_attention_key ``` While these activations is indeed not recomputed during backward pass, all communications are executed in main stream without any overlapping with computations:  save_only_these_names_trace.tgz  System info (python version, jaxlib version, accelerator, etc.) ``` >>> import jax; jax.print_environment_info() jax:    0.4.30 jaxlib: 0.4.30 numpy:  1.24.3 python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] jax.devices (8 total, 8 local): [cuda(id=0) cuda(id=1) ... cuda(id=6) cuda(id=7)] process_count: 1 platform: uname_result(system='Linux', node='ffisindev8gpu', release='5.4.0155generic', version=' CC(MNIST example runtime crash)Ubuntu SMP Fri Jul 7 16:10:02 UTC 2023', machine='x86_64') $ nvidiasmi Mon Jul  1 13:21:57 2024        ++  ++ ```",2024-07-03T12:38:56Z,bug,closed,4,3,https://github.com/jax-ml/jax/issues/22252,corresponding XLA issue: https://github.com/openxla/xla/issues/14397,"I _think_ this is an XLA bug, so let's watch openxla/xla CC(jax.grad computes incorrect derivative for polynomials) to see.",Looks like that XLA issue is resolved!
yi,Prefer __qualname__ as a pjit_p name,"If applying `jit` to a class method, it is often important to know the class name in the jaxpr.",2024-07-02T18:53:10Z,pull ready,closed,0,2,https://github.com/jax-ml/jax/issues/22237,Have you considered instead expanding `util.fun_name` that we use in a few places?,"> Have you considered instead expanding `util.fun_name` that we use in a few places? Yes, see my response to Sergei. Doing that broke tests and I'd rather make the minimal change that solves the problem I have (jaxpr readability)."
yi,"Feature Request: a way to opt out of ""hermetic python""","Hi! We're encountering issues concerning https://github.com/google/jax/pull/20469 with the jax{,lib} packages in Nixpkgs. It is crucial to our project to bootstrap the python interpreter from scratch and outside bazel, rather download a prebuilt one from the internet. We've been relying on bazel selecting the system python (setting `PYTHON_BIN_PATH`) for this. Right now we're looking into reverting the respective diff ad hoc, but we'd prefer a tighter integration Thanks Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example.",2024-07-01T17:44:18Z,enhancement,open,4,2,https://github.com/jax-ml/jax/issues/22216,"Hello , For the projects which want to provide their own Python interpreter (a Linux distribution would be a good example of that) there is a way to doe exactly that. For the other concerns, please check my response in the corresponding nixpkgs thread.  Specifying your own Python interpreter For a basic example of specifying your own interpreter please check Building with prerelease Python version. Note, you do not have to follow those instructions directly if you don't want. The only thing that you actually need is to be able to specify a path to your desired (hopefully working) python interpreter in the end. The instructions just serve the purpose of obtaining such working custom interpreter, if you already have one, you can skip directly to the last step: To use newly built Python interpreter add the following code snippet RIGHT AFTER `python_init_toolchains()` in your `WORKSPACE` file. ```bzl load(""//python:repositories.bzl"", ""python_register_toolchains"") python_register_toolchains(     name = ""python"",      By default assume the interpreter is on the local file system, replace      with proper URL if it is not the case.     base_url = ""file://"",     ignore_root_user_error = True,     python_version = ""3.13.0a6"",     tool_versions = {         ""3.13.0a6"": {              Path to .tar.gz with Python binary.             ""url"": ""/full/path/to/your/python_dev3.13.0a6.tgz"",             ""sha256"": {                  By default we assume Linux x86_64 architecture, eplace with                  proper architecture if you were building on a different platform.                 ""x86_64unknownlinuxgnu"": ""cd99233ccd2df426208be3d394e1b89bbb2511caf389cfa9df7bab624a6cdc24"",             },             ""strip_prefix"": ""python_dev3.13.0a6"",         },     }, ) ```  Mimic old nonhermetic behavior (very very not recommended) We do not support and do not recommend this use case, but it is still possible with a little bit of work on your side. The instructions above assume you have python packaged as a standalone .tgz archive. If you still want to just depend on whatever is installed locally on your system, you can go further but there is an **important thing to know before doing so (which may affect your decision):** Even in previous nonhermetic python setup, it was wrapping system python inside bazel rules and copying parts of your system python package inside bazel's cache to be abele to use it for the rest of the build. I.e.  nonhermetic python acted almost as it was still downloading a python from somewhere, it was just that ""somewhere"" happened to be your local environment. With that being said, you can mimic old nonhermetic python setup by having a custom repository rule which would search your local system, package it in a form of a standalone archive to match structure of the ones we currently depend on (the structure there matches default layout you would get when build vanill Python from official sources) and then provides the packaged archive to the value for `url` field in the code snipped above. Note, we do not provide such custom localfilesystemsearch rule ourseves and do not plan to, as it basically would reintroduce the nonhermetic python with all its issues, such as being very fragile and nonreproducible setup, but it is not very difficult to implement such on your side, especially if you do not need to make it generic (if it matches only NixOS structure, than it would be much easier to implement and maintain than something which should work on any linux system).","Hi! Thank you google, for a prompt and comprehensive reply. I hope my comment about ""random executables"" in the other issue wasn't too rude, otherwise I'm prepared to apologize:) We'll look into implementing a solution along the lines of the snippet you provided. I expect it won't be a ""smooth ride"". I think it's best I avoid ""explaining Nix"" in this thread (maybe we reserve the Nixpkgs issue for that) and focus on our integration with Bazel. Suffice it to say that Nix and Bazel overlap in scope, which is why (1) we're not concerned about reproducibility and correctness of ""system packages"", and (2) why having Bazel set up its own sandbox and copy these packages into new locations has been problematic. For now, there are two implementation details about the python we build I'm worried about:  Currently, we only consider our outputs ""correct"" if they're deployed at certain predefined locations: e.g. `/nix/store/python/bin/python` will have a `PT_INTERPTER` pointing at a specific `/nix/store/glibc/lib/ldlinux.so`, etc. Things would likely still work: since Bazel implements hermetic builds, I presume it would patchelf the executables it ""downloads""?  We only consider our outputs ""correct"" when referential integrity is maintained: if `/bin/python` references `/libz.so`, this `python` can only be deployed together with it. We have ways to inspect and export the full ""closure"" of such a package, but we've never explicitly provided Bazel's ""old nonhermetic python"" with this information. Does this mean that Bazel has been copying ""our python"" and relinking it to different versions of dependencies for the duration of the build? Perhaps these notions of ""correctness"" (=conditions under which we're ready to ""guarantee"" our software will work) also clarify why I believed it would be easiest for us if we could relax Bazel's sandbox and let it see the predeployed toolchain"
yi,Bump hypothesis from 6.100.1 to 6.104.2,"Bumps hypothesis from 6.100.1 to 6.104.2.  Commits  d8c1783 Bump hypothesispython version to 6.104.2 and update changelog c79e893 Merge pull request  CC(jax.numpy.clip has unexpected behavior which diverges from numpy.clip ) from tybug/posttestcasehookfix 976f559 update typing 1f86be4 better post_test_case_hook error message 39a8f72 raise flaky when replaying backend flakes 09b00a2 add release notes 90ad3e2 exit if replayed interesting backends are flaky c46da15 move post_test_case_hook higher up e51d473 Bump hypothesispython version to 6.104.1 and update changelog 0c885e7 Merge pull request  CC([jax2tf] Disable the CI tests for jax2tf.) from jobh/coverage_fixme Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-07-01T17:26:23Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/22215,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
rag,Bump setuptools from 69.2.0 to 70.2.0,"Bumps setuptools from 69.2.0 to 70.2.0.  Changelog Sourced from setuptools's changelog.  v70.2.0 Features  Updated distutils including significant changes to support Cygwin and mingw compilers. ( CC(Call check_user_dtype on all user dtypes))  Bugfixes  Fix distribution name normalisation (:pep:625) for valid versions that are not canonical (e.g. 1.02). ( CC(reduce testcase count of the numpydispatch CI check))  v70.1.1 Misc   CC(Add support for `all_to_all` in vmap)  v70.1.0 Features   Adopted the bdist_wheel command from the wheel project  by :user:agronholm ( CC(Nan Mean and Adds Nan Reducers to testing))   Improve error message when pkg_resources.ZipProvider tries to extract resources with a missing Egg  by :user:Avasam Added variables and parameter type annotations to pkg_resources to be nearly on par with typeshed.*  by :user:Avasam * Excluding TypeVar and overload. Return types are currently inferred. ( CC([jax2tf] Cleanup test_unary_elementwise.))   Migrated Setuptools' own config to pyproject.toml ( CC(Allow preallocation of GPU memory based on total memory))   Bugfixes  Prevent a TypeError: 'NoneType' object is not callable when shutil_rmtree is called without an onexc parameter on Python&lt;=3.11  by :user:Avasam ( CC(Enable fast TPU LU decomposition for complex types.)) Replace use of mktemp with can_symlink from the stdlib test suite. ( CC([jax2tf] Fix casting in translation of eigh for empty arrays.)) Improvement for attr: directives in configuration to handle more edge cases related to complex package_dir. ( CC(mark lax higherorder functions as entry points for stack trace filtering)) Fix accidental implicit string concatenation. ( CC(fix links in omnistaging doc (hopefully...)))  Misc   ... (truncated)   Commits  3accd5c Bump version: 70.1.1 → 70.2.0 719192a Merge pull request  CC(Fix dtype behavior with float0s in CustomVJP.) from pypa/infra/refreshskeleton e32f8df Merge pull request  CC(Call check_user_dtype on all user dtypes) from pypa/feature/distutilsf3b225449 343ac6f Merge https://github.com/pypa/distutils into feature/distutilsf3b225449 4e6d97d Prefer relative imports for better portability. 449021c Merge pull request  CC(Edit and fix rendering problems in lax.cond docstring.) from DimitriPapadopoulos/default 97e3c8f Merge pull request  CC(RuntimeError: Internal: libdevice not found at ./libdevice.10.bc) from pypa/docs/1648interpolation a9a5400 Add a section on interpolation. 31c8599 Remove 'normally supplied to setup()'. Declarative styles are normalized. ad8c7ba Merge pull request  CC(conda installation cuda incompatibility error invalid) from DimitriPapadopoulos/TRY Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-07-01T17:25:46Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/22214,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
transformer,Check failed in collective_pipeliner when using gradient accumulation with non-unrolled loop," Description Hi, I have following setup:  Transformer model with N layers scanned over input  fully sharded data parallel sharding  asynchronous communications (latencyhiding scheduler, pipelined allgather,allreduce,reducescatter) I'm using following flags: ```bash xla_gpu_graph_level=0  xla_gpu_enable_triton_gemm=false  xla_gpu_enable_command_buffer=  xla_gpu_enable_latency_hiding_scheduler=true  xla_gpu_enable_all_gather_combine_by_dim=false  xla_gpu_enable_reduce_scatter_combine_by_dim=false  xla_gpu_enable_pipelined_all_gather=true  xla_gpu_enable_pipelined_reduce_scatter=true  xla_gpu_enable_pipelined_all_reduce=true  xla_gpu_enable_pipelined_collectives=false  xla_gpu_enable_while_loop_double_buffering=true  xla_gpu_enable_highest_priority_async_stream=true  xla_gpu_disable_async_collectives=collectivebroadcast,alltoall,collectivepermute ``` This works correctly and indeed hide layers' weights allgather and gradient reducescatter behind computations. Problems are starting to arise when I try to use gradient accumulation in this setup. It is implemented like this: ```python     grads_sum = jax.tree_map(jnp.zeros_like, train_state.params)     train_state, grads_sum = jax.lax.fori_loop(         lower=0,         upper=num_minibatches_in_batch,         body_fun=_loop_body,         init_val=(train_state, grads_sum),         unroll=False,     )     mean_grads = jax.tree_map(lambda x: x / num_minibatches_in_batch, grads_sum) ``` When I set gradient accumulation factor (num_minibatches_in_batch in this snippet) to value greater than 1, I'm getting following error during compilation:  ``` 20240701 12:57:35.488299: F external/xla/xla/service/collective_pipeliner.cc:675] Check failed: last_cloned != nullptr (0 vs. nullptr) ``` Here is xla_dump_to result: xla_dump.tgz One important fact here is that if I set `unroll` in jax.lax.fori_loop to True, then there is no compilation error and everything works. But this obviously leads to additional memory usage proportional to gradient accumulation factor so this hack doesn't seem to be viable. My hypothesis is that when using `xla_gpu_enable_while_loop_double_buffering=true` with pipelined collectives and latency  hiding scheduler, XLA compiler tries to double buffer this fori_loop which is actually undesired behavior. Basically, there are two problems:  Bug in compiler that leads to hardtoparse source of error in JAX code  If my hypothesis is correct, I would like to have mechanism to disable while_loop_double_buffering for specific loops (like gradient accumulation loop) or enable only for specific loops (like layers loop) I've tested this on JAX 0.4.29 and 0.4.30.  System info (python version, jaxlib version, accelerator, etc.) ``` >>> import jax; jax.print_environment_info() jax:    0.4.30 jaxlib: 0.4.30 numpy:  1.24.3 python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] jax.devices (8 total, 8 local): [cuda(id=0) cuda(id=1) ... cuda(id=6) cuda(id=7)] process_count: 1 platform: uname_result(system='Linux', node='ffisindev8gpu', release='5.4.0155generic', version=' CC(MNIST example runtime crash)Ubuntu SMP Fri Jul 7 16:10:02 UTC 2023', machine='x86_64') $ nvidiasmi Mon Jul  1 13:21:57 2024        ++  ++ ```",2024-07-01T13:22:30Z,better_errors needs info,closed,6,3,https://github.com/jax-ml/jax/issues/22210,related XLA issue: https://github.com/openxla/xla/issues/14332,"Thanks for raising this. I think it's an XLA:GPU issue, and we don't have any way to fix it from JAX. That said, the hardtoparsae error may be something we can get traction on from JAX... can you say a bit more about what would've helped in the error message? We attach Python source information to the HLO program, but it's up to XLA to raise errors that reference it... from JAX we could've at least told you which jitted function raised the compiler error, but I'm not sure if we have other information to provide...",I think we should close this in favor of the XLA issue. Looks like it just got assigned yesterday!
yi,Efficient parallelization of tasks with highly variable difficulty and run times,"Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example. I am struggling to write **idiomatic and efficient** parallel code in jax for tasks with variable difficulty and run times. Jax parallelization is based on sharding and it is incapable of balancing workloads across devices. I suspect that introducing a task scheduler could bring considerable benefits. **Example**. Consider a dynamic programming problem with the following structure  For each parameter set, solve a dynamic programming problem. Parallelizable across parameter sets.      Sequential backwards time loop. It can use `jax.lax.scan`.          For each possible state from a state grid, solve an inner problem that makes a call to `quadax.quadgk` (only 1 adaptive step) with `jaxopt.ProjectedGradient`. Parallelized across states. I have a question about the inner loop. When applying `vmap` to hundreds of projected gradient descents, does a single hard problem force the rest of problems to keep evaluating even if they already found a solution? That is, could `vmap` be counterproductive if there are big differences in terms of iterations required by each problem? In general, what would be the most efficient and jax idiomatic way to parallelize such a problem? Regarding the outer loop, this is what I have tried so far **Jax tools**. Currently Jax provides two flavors of parallelization  `vmap` vectorizes operations within a device  Sharding via `jit` + `device_put` /  `shard_map` to distribute operations across devices `vmap` fails to take advantage of all resources to exploit parallelization. A single outer loop iteration (parameter set) takes from 4 to 10 minutes. After vmapping 500 parameter sets on the outer loop, CPU usage doesn't grow significantly (stays below 10% on a 24 core machine with 48 logical processors) and it takes more than 24 hours (killed at this time limit). Sharding the outer loop via `device_put` manages to execute tasks in parallel but it is incapable of balancing work across devices. At the beginning, CPU usage can grow to 100% when sharding with `xla_force_host_platform_device_count=48`. More precisely, I have to manually divide inputs into two arrays: the first one is composed of `n_devices` shards of size `n_parameter_sets // n_devices` each, and the second one of `n_parameter_sets % n_devices` shards of size `1` each. Apart from being slightly cumbersome and producing nonreusable code, I am afraid this strategy is not taking full advantage of all resources in the system. Some tasks are harder than others (but only learn about the difficulty after we have solved it) and some devices could run faster than others because of usage patterns from unrelated programs. Thus we can expect significant differences in completion times across Jax devices, but Jax seems to just wait until the last (and slowest) device finishes, instead of rescheduling tasks to devices that already completed all of their tasks and are currently idling. **Python tools**. Jax is compatible with applying `ThreadPoolExecutor.map()` and `threading` to the outer loop, with some caveats. Jax devices are pinned at the process level and by default all threads try to use the same jax device. Therefore I need to apply `device_put` on thread input arguments to make sure each thread uses a separate jax device. CPU usage can fluctuate between 10% and 100%. It can be implemented lazyly, meaning that a large number of tasks won't cause OOM errors. For many longrunning tasks and after a breakin period, it can achieve a sustained 100% CPU utilization and effectively balance workload across threads. `multiprocessing` is the most reliable tool that I have found to parallelize tasks efficiently. Applying `Pool.imap` to the outer loop manages to exploit all the resources from the system and balances work efficiently across cpu cores (consistent 100% CPU usage). It is also applied lazyly, meaning that a large number of tasks won't cause OOM errors. The downside is that it is very cumbersome to use. Users have to write code in a separate file, serialize and parse input/output using temporary files, struggle with debugging, etc. **Summary**. Devices are heterogeneous in terms of resources and tasks are heterogeneous in terms of operations required for completion (but I can't tell them apart until they finish). Existing high level sharding tools in jax force users to micromanage which task goes to which device which has several disadvantages: it is slightly cumbersome, it causes device layout lockin and this hinders code reusability. They also try to fit all tasks in memory at the same time, which can lead to OOM errors . Moreover, these sharding tools are incapable of dynamically balancing load from congested devices to idling devices. It would be nice if jax provided a high level utility to execute SPMD tasks across multiple devices, automatically sharding data along specified input axes, possibly batching tasks up to a specified limit and rebalancing queued tasks/batches across devices based on congestion.",2024-06-28T13:44:13Z,enhancement,open,5,0,https://github.com/jax-ml/jax/issues/22176
yi,[memories] Overlap shard transfers in async_serialize,"Builds upon https://github.com/google/jax/pull/22114, which introduces a fastpath for async checkpoint saving in which each array shard is copied through a single devicetopinnedhost transfer. So far, all of these transfers are serialized. The present PR allows overlapping the transfers of a single array's shards. This is achieved by inserting `asyncio.sleep(0)` right after the transfer has been started, thereby permitting the `_write_array(shard)` coroutine to yield. _Why do we need the sleep?_ Python's `await` doesn't actually guarantee that control will be yielded to the eventloop so that other coroutines can be scheduled. Python ties coroutine semantics to that of generators. As I understand it, the only thing that causes control to pass to the eventloop (and thus allows other coroutines to be scheduled) is a `yield`. What we therefore need to achieve overlap in our situation, is something like ```python import types .coroutine def noop():   yield async def transfer_shard_to_host(shard):   ...   await noop()  behaves like asyncio.sleep(0)   ... ``` This is, in fact, what `asyncio.sleep(0)` does. An alternative, more roundabout way to achieve overlap would be to split `_write_array(shard)` into a first phase that initiates all the transfers, and provides the resulting hostresident arrays to a second phase, in which we invoke `t.write(data)` as before.  ",2024-06-28T11:52:03Z,pull ready,closed,0,4,https://github.com/jax-ml/jax/issues/22169,"Your CL got reverted because of internal failures for layouts. I posted on the chat, but we would need to reland that before we can submit this right?",~That's right  the first JAX PR (https://github.com/google/jax/pull/22114) requires various XLA PRs to be landed including the one that got reverted (https://github.com/openxla/xla/pull/14090). Will take a look.~ This should actually work out of the box since we don't depend on XLA CC(CI: update deprecated uses of setoutput) anymore.,"The test seems to be failing on CPU:  Traceback (most recent call last):   File ""jax/experimental/array_serialization/serialization_test.py"", in test_transfer_shard_to_host     sharding = SingleDeviceSharding(jax.devices()[0], memory_kind=""device"")                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ValueError: Could not find memory addressable by device cpu. Device cpu can address the following memory kinds: unpinned_host. Got memory kind: device Can you just skip on CPU?"," Sg, I marked the new test in https://github.com/google/jax/pull/22114 with `.skip_on_devices('cpu')` and also rebased this PR."
yi,Explicitly disallow duplicated devices during array construction,"Explicitly disallow duplicated devices during array construction `jax.make_array_from_single_device_arrays` should not allow passing more than one array on the same device as that would lead to an invalid array. While some of this case is already detected by later checks (e.g., `ArrayImpl._check_and_rearrange`), this CL explicitly checks the device list before calling IFRT so that we don't create an invalid IFRT array to begin with.",2024-06-28T08:34:47Z,,closed,0,0,https://github.com/jax-ml/jax/issues/22165
yi,Mark jax typing Protocols as `@runtime_checkable`,"Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example. **Goal**: Ensure ""isinstance"" works to check if a type (not a value) is a jax Array type. **Problem**: When deserializing data containing jax arrays, one must check if the types are jax Array types. However, ""jaxlib.xla_extension.ArrayImpl is jax.Array"" evaluates False (normal, but annoying) **Opportunity**: Could we make these typing protocols runtime checkable to faciliate serialization/deserialization logic?  TLDR Fix: `from typing import runtime_checkable` & use `` decorator on Protocol subclasses  Reproduction ```py import jax, jax.numpy as jnp from jax._src.typing import DuckTypedArray from jaxlib.xla_extension import ArrayImpl def test_jaxlib__extensions_arraylike_is_array():     try:         assert ArrayImpl is jax.Array, f""{ArrayImpl=} is not {jax.Array=}""     except Exception as e:         print(f""{e=}"")     print(""This is normal."") def test_duck_typed_array_is_runtime_checkable():     arr = jnp.ones(3, int)     print(""arr"", arr)     assert isinstance(         arr,         DuckTypedArray,     ), ""jax._src.typing.DuckTypedArray is not runtime_checkable"" test_jaxlib__extensions_arraylike_is_array() test_duck_typed_array_is_runtime_checkable() ```  Result !image  System: ```sh ../jax (main) $ > python c 'import jax; jax.print_environment_info()' jax:    0.4.31.dev20240627+4c70a94c8 jaxlib: 0.4.30 numpy:  1.26.4 python: 3.12.0  ++ ```  References / Links (not strictly needed) [1] from a Python array API standard, but it's really long to paste here.  [2] Another resource for protocols is DLPack C API, seems clear on data type sizes of things  Here's the short notes on those, I'll add class attributes to the tree_plus eventually, but it can get noisy. !image  Workaround ```py from typing import TypeGuard from jaxlib.xla_extension import ArrayImpl import jax, jax.numpy as jnp  test_duck_typed_array_is_runtime_checkable() def is_jax_array_type(type_x: type, *, debug: bool = DEBUG) > bool:     if type_x is ArrayImpl:         decision = True     elif type_x is jax.Array:         decision = True     else:         decision = False     if debug:         print(f""is {type_x=} a jax.Array? {decision=}"")     return decision def is_jax_array_value(x: Any, *, debug: bool = DEBUG) > TypeGuard[jax.Array]:     type_x = type(x)     return is_jax_array_type(type_x, debug=debug) def test_typeguard_workaround():     arr = jnp.ones(3, int)     print(""arr"", arr)     arr_is_jax_array = is_jax_array_value(arr)     print(f""{arr_is_jax_array=}"")     assert (         arr_is_jax_array     ), ""the is_jax_array TypeGuard[jax.Array] failed to classify a jax Array""     assert not is_jax_array_value(1)     assert not is_jax_array_value(False)     assert not is_jax_array_value((1, 2, 3)) test_typeguard_workaround() ``` !image Cheers, love me some jax!",2024-06-27T16:23:54Z,enhancement,closed,0,5,https://github.com/jax-ml/jax/issues/22144,"Assigning  because he has thought a lot about this kind of typing issue, and may have good feedback!","I don't mind making `DuckTypedArray` runtimecheckable, but the guarantees these runtime checks give are pretty weak. In particular, they do not do any type checking: ``` >>> from typing import Protocol, runtime_checkable >>>  ... class P(Protocol): ...   def f(self) > int: ... ... >>> class A: ...   f = 42 ... >>> isinstance(A(), P) True ``` Note also, that `DuckTypedArray` only has two methods and thus the runtime check is equivalent to `hasattr(..., ""shape"") and hasattr(..., ""dtype"")`, which means that e.g. a NumPy array will be considered an instance of `DuckTypedArray`. Is that the behavior you expect?","Oh, you're right, that wouldn't work, since it would make code think it sees jax arrays when it sees numpy arrays. I just want to roundtrip serialize/deserialize all the things, and hit a situation where I needed to check if a type is jax.Array or not, but `T is jax.Array` didn't work for ArrayImpl. The typeguard function solved my problem, could be fine to just stick with that. Seems like a lot of python typing is still Work in Progress, if there's a solution which python's type system supports better than protocols, like dataclasses or abstract base classes, that's great.  Mainly my objective was to ensure we can support both type and value level checks, for the abstract case to check if type T is jax Array, and the concrete case to check if value V is an instance of a jax Array I'm agnostic about the implementation details, so whatever you think is best with the state of Python, I'm down to test it","`jax.Array` is a base class, so to check if `x` is a JAX array you can use `isinstance(x, jax.Array)`, or to check if `T = type(x)` is an array type, you can use `issubclass(T, jax.Array)`.","sounds good! only thing i'd add would be, make sure to use try/except with issubclass, it's pretty flaky, crashes with annotations (oof) !image given that, here's a working solution with issubclass, thank you ! closing ```py from typing import TypeGuard, Any from jaxlib.xla_extension import ArrayImpl import jax, jax.numpy as jnp def is_jax_array_subclass(x: type) > bool:     ""check if a type is a subclass of jax.Array""     try:         return issubclass(x, jax.Array)     except Exception as _e:         return False def is_jax_array(x: Any) > TypeGuard[jax.Array]:     ""check if a value is an instance of jax.Array""     try:         return is_jax_array_subclass(x.__class__)     except Exception as _e:         return False def test_is_jax_array_subclass():     assert is_jax_array_subclass(jax.Array)     assert is_jax_array_subclass(ArrayImpl)     assert not is_jax_array_subclass(tuple[int, ...])     assert not is_jax_array_subclass(jnp.ones(3)) def test_is_jax_array():     assert not is_jax_array(jax.Array)     assert not is_jax_array(ArrayImpl)     assert not is_jax_array(tuple[int, ...])     assert is_jax_array(jnp.ones(3)) test_is_jax_array_subclass() test_is_jax_array() ``` !image"
yi,"tweak dynamic trace state to only depend on level int, not MainTrace","This is a small step in simplifying some parts of the core tracing machinery. Our caches, like the C++ dispatch cache, must depend on JAX's global tracing state. The main part of the global state is the trace stack (aka `core.thread_local_state.trace_state.trace_stack`), and one component of that is which trace is the 'dynamic' trace, i.e. `core.thread_local_state.trace_state.trace_stack.dynamic`. That attribute is a reference to a `MainTrace` instance, but it could've just been an `int`: it's just pointing to object in`core.thread_local_state.trace_state.trace_stack.stack`, which is a `list[MainTrace]`. We should probably make it an int! Or revise it away completely. That's planned followup work. For now, this PR just changes the way we represent it to the C++ cache: we can only hit the fast path when the trace state is basically `TraceStack(stack=[EvalTrace(0)], dynamic=0)`, so we should include `(dynamic.level, dynamic.trace_type)` in the cache key (and we'll only ever populate a cache entry when it's `(0, EvalTrace)`).",2024-06-26T20:52:30Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/22123,"Hopefully we can delete much of this anyway. That is, we have a plan to delete the `dynamic` field entirely..."
yi,Run `pyupgrade --py310-plus` on .pyi files.,Manually fix import orders.,2024-06-26T19:25:10Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/22118
yi,[memories] Transfer to pinned_host fast path in async_serialize,"Adds a fast path to `jax.experimental.array_serialization.serialization.async_serialize` that avoids XLA's regular devicetohost transfer and instead uses a single devicetopinnedhost transfer per `_write_array(arr)` invocation. This allows us to achieve much closer to ideal transfer bandwidths in practice. For comparison, the existing approach stages copies through a fixed size intermediate 128MBbuffer and requires `sizeof(arr)/128MB` alternations between D2H and H2H copies. Note that the `np.array(data, copy=False)` is not strictly necessary as the tensorstore invocation `t.write(...)` immediately performs the CAPI equivalent of `np.array(data, copy=None)`. We expect all of these to be zerocopy, hence explicitly calling `np.array(data, copy=False)` provides some extra safety, since it would fail if jax.Array's implementation changed and no longer permitted zerocopying its private numpy array `_value`. Alas the latter check is not foolproof: for example, prior to XLA CC(Clarify the docstring for vjp.) the construction of the jax.Array from the device buffer also forced a copy. Depends on XLA CC(Expose fp8 types from jnp namespace.) XLA CC(Fix typo ""invalud"" > ""invalid"" in error message.) XLA CC(Clarify the docstring for vjp.) ~XLA CC(CI: update deprecated uses of setoutput)~",2024-06-26T18:32:18Z,pull ready,closed,0,7,https://github.com/jax-ml/jax/issues/22114,Do you have some benchmarks where this is super fast and helpful? (something that you ran locally or in your runs?),"  > Do you have some benchmarks where this is super fast and helpful? (something that you ran locally or in your runs?) Here's a selfcontained example that doesn't quite behave like the E2E workload mentioned before, but illustrates the effects and is a good candidate for profiling: https://gist.github.com/gspschmid/52a1062916c7030a513b0581bd56c5be The first improvement corresponds to this PR (along with its XLA dependencies), the second improvement corresponds to https://github.com/google/jax/pull/22169. Note that after applying the first improvement other overheads in tensorstore's `t.write(data)` begin to dominate. Attached below are some screenshots of nsys profiles corresponding to the last iteration for each variant. Baseline:  devicetopinnedhost transfer:  devicetopinnedhost transfer + overlap shard transfers: "," Not sure what CI you run for JAX contributions, but now that the remaining XLA PRs are in (https://github.com/openxla/xla/pull/14089 and https://github.com/openxla/xla/pull/14268) this should be ready to test."," , This PR should be good to go  since we were able to switch from `jax.jit` to using `device_put` it doesn't depend on XLA CC(CI: update deprecated uses of setoutput) any longer. I checked with a recent unpatched XLA build + these pending JAX patches and got the expected performance."," Seems like the test failed due to `memories_test.py` now depending on `tensorstore`, but that dependency being missing from the bazel target. Do you prefer adding something like ` + py_deps(""tensorstore"")` there or should I move the test to `jax/experimental/array_serialization/serialization_test.py`?",Yeah move it to serialization_test.py please :), Done! Let's give it another go :)
yi,jax.numpy: better docstring for isreal and isrealobj functions,Better docstring and JAXspecific code examples are added for jax.numpy.isreal and jax.numpy.isrealobj. Part of 21461,2024-06-25T23:04:07Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/22097,Thanks   Typo fixed.
yi,Ergonomics: Move calls to partial inside decorators,"Please:  [ x] Check for duplicate requests.  [ x] Describe your goal, and if possible provide a code snippet with a motivating example. It would be nice to eliminate the awkwardness with partials for decorators. Code like this: ```python from functools import partial import jax (jax.jit, kwarg=kwval, ...) def func(...): ``` Can be simplified to this: ```python import jax .jit(kwarg=kwval, ...) def func(...): ``` This may seem minor, but little ergonomic changes like this can have an outsized impact on both interest in adoption and postadoption development speed. Here's how to do it with `jit` in particular: ```python def jit(     fun: Callable = None,     in_shardings=sharding_impls.UNSPECIFIED,     out_shardings=sharding_impls.UNSPECIFIED,     static_argnums: int  partial:     """"""Sets up ``fun`` for justintime compilation with XLA.     Args:       fun: Function to be jitted. ``fun`` should be a pure function.         The arguments and return value of ``fun`` should be arrays, scalar, or         (nested) standard Python containers (tuple/list/dict) thereof. Positional         arguments indicated by ``static_argnums`` can be any hashable type. Static         arguments are included as part of a compilation cache key, which is why         hash and equality operators must be defined. JAX keeps a weak reference to         ``fun`` for use as a compilation cache key, so the object ``fun`` must be         weaklyreferenceable.       in_shardings: optional, a :py:class:`Sharding` or pytree with         :py:class:`Sharding` leaves and structure that is a tree prefix of the         positional arguments tuple to ``fun``. If provided, the positional         arguments passed to ``fun`` must have shardings that are compatible with         ``in_shardings`` or an error is raised, and the compiled computation has         input shardings corresponding to ``in_shardings``. If not provided, the         compiled computation's input shardings are inferred from argument         shardings.       out_shardings: optional, a :py:class:`Sharding` or pytree with         :py:class:`Sharding` leaves and structure that is a tree prefix of the         output of ``fun``. If provided, it has the same effect as applying         corresponding :py:func:`jax.lax.with_sharding_constraint`s to the output         of ``fun``.       static_argnums: optional, an int or collection of ints that specify which         positional arguments to treat as static (trace and compiletime         constant).         Static arguments should be hashable, meaning both ``__hash__`` and         ``__eq__`` are implemented, and immutable. Otherwise they can be arbitrary         Python objects. Calling the jitted function with different values for         these constants will trigger recompilation. Arguments that are not         arraylike or containers thereof must be marked as static.         If neither ``static_argnums`` nor ``static_argnames`` is provided, no         arguments are treated as static. If ``static_argnums`` is not provided but         ``static_argnames`` is, or vice versa, JAX uses         :code:`inspect.signature(fun)` to find any positional arguments that         correspond to ``static_argnames``         (or vice versa). If both ``static_argnums`` and ``static_argnames`` are         provided, ``inspect.signature`` is not used, and only actual         parameters listed in either ``static_argnums`` or ``static_argnames`` will         be treated as static.       static_argnames: optional, a string or collection of strings specifying         which named arguments to treat as static (compiletime constant). See the         comment on ``static_argnums`` for details. If not         provided but ``static_argnums`` is set, the default is based on calling         ``inspect.signature(fun)`` to find corresponding named arguments.       donate_argnums: optional, collection of integers to specify which positional         argument buffers can be overwritten by the computation and marked deleted         in the caller. It is safe to donate argument buffers if you no longer need         them once the computation has started. In some cases XLA can make use of         donated buffers to reduce the amount of memory needed to perform a         computation, for example recycling one of your input buffers to store a         result. You should not reuse buffers that you donate to a computation; JAX         will raise an error if you try to. By default, no argument buffers are         donated.         If neither ``donate_argnums`` nor ``donate_argnames`` is provided, no         arguments are donated. If ``donate_argnums`` is not provided but         ``donate_argnames`` is, or vice versa, JAX uses         :code:`inspect.signature(fun)` to find any positional arguments that         correspond to ``donate_argnames``         (or vice versa). If both ``donate_argnums`` and ``donate_argnames`` are         provided, ``inspect.signature`` is not used, and only actual         parameters listed in either ``donate_argnums`` or ``donate_argnames`` will         be donated.         For more details on buffer donation see the         `FAQ `_.       donate_argnames: optional, a string or collection of strings specifying         which named arguments are donated to the computation. See the         comment on ``donate_argnums`` for details. If not         provided but ``donate_argnums`` is set, the default is based on calling         ``inspect.signature(fun)`` to find corresponding named arguments.       keep_unused: optional boolean. If `False` (the default), arguments that JAX         determines to be unused by `fun` *may* be dropped from resulting compiled         XLA executables. Such arguments will not be transferred to the device nor         provided to the underlying executable. If `True`, unused arguments will         not be pruned.       device: This is an experimental feature and the API is likely to change.         Optional, the Device the jitted function will run on. (Available devices         can be retrieved via :py:func:`jax.devices`.) The default is inherited         from XLA's DeviceAssignment logic and is usually to use         ``jax.devices()[0]``.       backend: This is an experimental feature and the API is likely to change.         Optional, a string representing the XLA backend: ``'cpu'``, ``'gpu'``, or         ``'tpu'``.       inline: Optional boolean. Specify whether this function should be inlined         into enclosing jaxprs. Default False.     Returns:       A wrapped version of ``fun``, set up for justintime compilation.     Examples:       In the following example, ``selu`` can be compiled into a single fused kernel       by XLA:       >>> import jax       >>>       >>> .jit       ... def selu(x, alpha=1.67, lmbda=1.05):       ...   return lmbda * jax.numpy.where(x > 0, x, alpha * jax.numpy.exp(x)  alpha)       >>>       >>> key = jax.random.key(0)       >>> x = jax.random.normal(key, (10,))       >>> print(selu(x))   doctest: +SKIP       [0.54485  0.27744 0.29255 0.91421 0.62452 0.24748       0.85743 0.78232  0.76827  0.59566 ]       To pass arguments such as ``static_argnames`` when decorating a function:       >>> .jit(static_argnames=['n'])       ... def g(x, n):       ...   for i in range(n):       ...     x = x ** 2       ...   return x       >>>       >>> g(jnp.arange(4), 3)       Array([   0,    1,  256, 6561], dtype=int32)     """"""     if fun is None:         return partial(             jit,             in_shardings=in_shardings,             out_shardings=out_shardings,             static_argnums=static_argnums,             static_argnames=static_argnames,             donate_argnums=donate_argnums,             donate_argnames=donate_argnames,             keep_unused=keep_unused,             device=device,             backend=backend,             inline=inline,             abstracted_axes=abstracted_axes,         )     return pjit.make_jit(         fun, in_shardings, out_shardings, donate_argnums, donate_argnames,         static_argnums, static_argnames, device, backend, abstracted_axes,         keep_unused, inline, use_resource_env=False) ```",2024-06-24T15:20:00Z,enhancement,closed,4,1,https://github.com/jax-ml/jax/issues/22061,"Thanks for the request! This has been discussed before at CC(Decorator factory pattern), and has come up a number of times since then. I think the opinion of the maintainers is still the same, so I'm going to close this as a duplicate. Please feel free to comment on CC(Decorator factory pattern) if you disagree with the concensus reached there."
yi,scan of device_put carry raises TypeError during the backward pass," Description After jax 0.4.30, the following code raises ``` TypeError: body_fun output and input must have identical types, got ('ShapedArray(int32[], weak_type=True)', ['DIFFERENT ShapedArray(int32[], weak_type=True) vs. ShapedArray(float0[])', 'ShapedArray(float32[], weak_type=True)'], []). ``` ```python import jax def f(ix, t):     return ix, ix[1] + t def scan_f(x):     ix = jax.device_put((0, x))     _, xs = jax.lax.scan(f, ix, jax.numpy.arange(10))     return xs.sum() jax.grad(scan_f)(1.) ``` This does not happen if we define ``` ix = 0, jax.device_put(x) ``` or ``` ix = jax.device_put(0), jax.device_put(x) ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.30 jaxlib: 0.4.30 numpy:  1.26.0 python: 3.10.13 (main, Aug 24 2023, 12:59:26) [Clang 15.0.0 (clang1500.0.40.1)] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 ```",2024-06-22T01:33:14Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/22045,I'll take a look. Thanks for the minimal repro!,"I think this is a case of inconsistent dtypes.. the following works where everything is a float (instead of a combination of float and int). ``` In [4]: import jax    ...:    ...: def f(ix, t):    ...:     return ix, ix[1] + t    ...:    ...: def scan_f(x):    ...:     ix = jax.device_put((0., x))    ...:     _, xs = jax.lax.scan(f, ix, jax.numpy.arange(10, dtype=np.float32))    ...:     return xs.sum()    ...:    ...: jax.grad(scan_f)(1.) Out[4]: Array(10., dtype=float32, weak_type=True) ``` The error message still sucks though. I'll look into improving it.","Hi ,  Thanks for reporting this, could you try to execute the code in 0.4.34 and higher Jax version, it seems to be working fine.  Attaching gist for your reference. Thank you.",Let's close this issue then. Thanks!
yi,jaxlib wheel hardcoded to manylinux2014 platform," Description I am opening this issue to get a discussion started around manylinux support for JAX/jaxlib. I am currently working on trying to get manylinux_2_28 jaxlib wheels built with ROCm support for ROCm users. Currently jaxlib wheels are hardcoded to be tagged as manylinux2014. See https://github.com/google/jax/blob/main/jaxlib/tools/build_wheel.pyL167 and https://github.com/google/jax/blob/main/jax/tools/build_utils.pyL56L58 This hardcoding of platform name forces the wheel build process to tag the wheel filename and metadata (WHEEL file inside the zip) as manylinux2014 which is only correct if building/linking on a platform with glibc 2.17, which is rather old. I am not aware of how google is building these wheels internally, but I imagine this is going to result in an incorrect wheel build for almost all other linux users. I have tested a change which removes the platform name override, running a build on a Centos8 / Almalinux 8 system, and it results in a `linux_x86_64` wheel, which is expected. After running `auditwheel repair` on this wheel it correctly scans the glibc versioned symbols and creates a new manylinux_2_27 wheel (highest symbol is actually 2.27 in the output wheel). This seems more correct, although I have yet to try it with the new jax plugin wheel builds. Some open questions are: Is there a reason that the builds were hardcoded to manylinux2014? Is making the above change and using auditwheel repair a path forward that works for upstream JAX/jaxlib builds? What is the manylinux target that jaxlib needs to support going forward? manylinux2014 seems to be deprecated at the end of June 2024 from pypa/manylinux repo > (PEP 599 defines the following platform tags: manylinux2014_x86_64, manylinux2014_i686, manylinux2014_aarch64, manylinux2014_armv7l, manylinux2014_ppc64, manylinux2014_ppc64le and manylinux2014_s390x. Wheels are built on CentOS 7 which will reach End of Life (EOL) on June 30th, 2024.)  System info (python version, jaxlib version, accelerator, etc.) ``` ```",2024-06-21T17:06:23Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/22034,"In general our build script produces the tags that our release builds should produce; it doesn't try to adapt the tags to the environment. I'm a little reluctant to run `auditwheel repair` because I don't want auditwheel changing the wheel. One option would be for us to default to a relaxed tag like `linux_x86_64`, and change our release builds to override the tag to a `manylinux` tag.","As to ongoing support: we're not sure yet. One likely possibility is at least 2_28, but we may go newer. (The thing pushing us to newer is that we would like to build with C++20).","> One option would be for us to default to a relaxed tag like linux_x86_64, and change our release builds to override the tag to a manylinux tag. I think this might be easiest, because then it pushes the ""generalization"" step to the release process that comes after build. I am also a bit worried about `auditwheel repair` changing the wheel as well, since it tries to embed any third party SO files into the zip and changes the RPATHs to use them instead of any external ones. This can be worked around with its exclude options, but it seems too easy to miss something. My current plan is to probably write something that uses auditwheel's functionality to modify the wheel but do it in a more controlled manner. Basically, just doing the lddtree scan for all versioned symbols, collecting the max versions of libs, and then outputting the `manylinux_x_y` wheel, without any RPATH or SO file changes. > One likely possibility is at least 2_28, but we may go newer. (The thing pushing us to newer is that we would like to build with C++20). I am positive this would make a lot of folks that use JAX on older systems very upset... all the users I am supporting right now seem to be on ubuntu 20 or EL8 derivatives which require glibc 2.28 or less. I haven't looked at the glibcpp versions on those distros but I am almost certain they are not C++20 level universally."
yi,jax.numpy: better docstring for iscomplex function,New docstring and JAXspecific code examples are added for jax.numpy.iscomplex. Part of 21461,2024-06-20T22:53:40Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/22014
yi,Improved docs for jnp.polyint and jnp.polyder,Part of CC(Tracking issue: inline docstrings),2024-06-20T09:18:12Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/21991
yi,Improve compilation cache tests,"This PR improves the tests for the compilation cache.  Background The JAX compilation cache operates through a set of functions defined in `jax/_src/compilation_cache.py`. These functions implement the operations of the compilation cache. A global variable, `_cache`, is defined in this file as an instance of the abstract class `CacheInterface`. This instance can be either `GFileCache` or the new `LRUCache` added in https://github.com/google/jax/pull/21394. There are three levels of abstraction involved: 1. Compilation cache operations 2. The `CacheInterface` abstract class 3. Implementations of the `CacheInterface` abstract class  The Problem The current `compilation_cache_test.py` file is intended to test the compilation cache operations but relies heavily on the implementation details of the underlying cache. For instance, it assumes that the number of files in the cache directory matches the number of items in the cache.  Why is This a Problem? With the intended introduction of the LRU cache eviction policy for the JAX compilation cache, we now require two separate files for each cache item: one for data and one for metadata. Consequently, the assumption that one cache item corresponds to one file in the cache directory is no longer valid.  The Solution This PR addresses the problem by implementing two functions, `count_cache_items()` and `clear_cache()`, at each level of abstraction. This ensures that the tests for the compilation cache do not depend on the specific implementation details of the underlying cache.",2024-06-19T17:24:12Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/21982,Please squash your commits.
rag,Remove `jax.xla_computation` tests from jax2tf. `api_test.py` has enough coverage for `jax.xla_computation`,Remove `jax.xla_computation` tests from jax2tf. `api_test.py` has enough coverage for `jax.xla_computation`,2024-06-18T23:45:50Z,,closed,0,0,https://github.com/jax-ml/jax/issues/21970
yi,jax.numpy: better docstring for iscomplexobj function,New docstring and JAXspecific code examples are addd for jax.numpy.iscomplexobj. Part of 21461,2024-06-18T20:11:03Z,pull ready,closed,0,2,https://github.com/jax-ml/jax/issues/21966,Thanks   Long characters wrapped and commits squashed into one.,Looks like there are lint issues related to trailing whitespace
yi,Bad cast error in lax backend (OSX)," Description This is a fresh conda environment. Just trying to get jax working on my M2. ``` import jax import jax.numpy as jnp  jnp.array([1]) ``` **Error** ``` ValueError                                Traceback (most recent call last) Cell In[8], line 3       1 l = [q] > 3 jnp.array([1]) File ~/miniconda3/envs/vectorstore/lib/python3.10/sitepackages/jax/_src/numpy/lax_numpy.py:2197, in array(object, dtype, copy, order, ndmin)    2194 else:    2195   raise TypeError(f""Unexpected input type for array: {type(object)}"") > 2197 out_array: Array = lax_internal._convert_element_type(    2198     out, dtype, weak_type=weak_type)    2199 if ndmin > ndim(out_array):    2200   out_array = lax.expand_dims(out_array, range(ndmin  ndim(out_array))) File ~/miniconda3/envs/vectorstore/lib/python3.10/sitepackages/jax/_src/lax/lax.py:558, in _convert_element_type(operand, new_dtype, weak_type)     556   return type_cast(Array, operand)     557 else: > 558   return convert_element_type_p.bind(operand, new_dtype=new_dtype,     559                                      weak_type=bool(weak_type)) File ~/miniconda3/envs/vectorstore/lib/python3.10/sitepackages/jax/_src/core.py:422, in Primitive.bind(self, *args, **params)     419 def bind(self, *args, **params):     420   assert (not config.enable_checks.value or     421           all(isinstance(arg, Tracer) or valid_jaxtype(arg) for arg in args)), args > 422   return self.bind_with_trace(find_top_trace(args), args, params) ... > 87   outs = fun(*args)      88 finally:      89   lib.jax_jit.swap_thread_local_state_disable_jit(prev) ValueError: std::bad_cast ```  System info (python version, jaxlib version, accelerator, etc.) (I get the same error on 0.4.30) OSX Sonoma 14.5 jax:    0.4.26 jaxlib: 0.4.26 numpy:  1.26.4 python: 3.10.14 (main, May  6 2024, 14:42:37) [Clang 14.0.6 ] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Darwin', node='LeonsMacBookAir.local', release='23.5.0', version='Darwin Kernel Version 23.5.0: Wed May  1 20:19:05 PDT 2024; root:xnu10063.121.3~5/RELEASE_ARM64_T8112', machine='arm64')",2024-06-18T19:24:59Z,bug,closed,0,6,https://github.com/jax-ml/jax/issues/21964,Can you try with the latest release 0.4.30?,"Already did :/ >  System info (python version, jaxlib version, accelerator, etc.) > (I get the same error on 0.4.30) >  > OSX Sonoma 14.5 also made sure to update xcode","Hmm. I can't reproduce on my ARM Macbook, also running Sonoma. One thing I'm guessing might be related is that I'm using Python from `pyenv`. I wonder if this is `conda` specific?","I can't reproduce with a fresh condaforge install, either. I'm not allowed to use miniconda because of its licensing, so I'm not sure there's anything more I can do. One thing I might suggest trying: can you try with a fresh conda environment?","I run into the same problem. It started with a numpy incompatibility with pandas. After reinstalling numpy, `ValueError: std::bad_cast` is reported for similar codes. But creating a fresh conda environment does solve the issue.","Strange... as noted by  a fresh environment solves the problem. As did a fresh conda environment with installs from condaforge, or a pyenv. "
yi,RuntimeError: jaxlib/gpu/solver_kernels.cc:45: operation gpusolverDnCreate(&handle) failed: cuSolver internal error," Description  Description I am trying to use JAX version 0.4.29 with CUDA 12.4. When I computed a simple linear algebraic calculation, I got an error `RuntimeError: jaxlib/gpu/solver_kernels.cc:45: operation gpusolverDnCreate(&handle) failed: cuSolver internal error`.  Error When I did the following, I found the above error ``` >>> import jax >>> import jax.numpy as jnp >>> c = jnp.array([[ 0.,  0.,  0.,  1.],[0.,  0., 1.,  0.],[ 0.,  1.,  0.,  0.],[1.,  0., 0.,  0.]]) >>> jnp.linalg.det(c) Traceback (most recent call last):   File """", line 1, in    File ""/home/s.ono/.conda/envs/jax_cuda12/lib/python3.11/sitepackages/jax/_src/numpy/linalg.py"", line 692, in det     sign, logdet = slogdet(a)   File ""/home/s.ono/.conda/envs/jax_cuda12/lib/python3.11/sitepackages/jax/_src/numpy/linalg.py"", line 534, in slogdet     return SlogdetResult(*_slogdet_lu(a))   File ""/home/s.ono/.conda/envs/jax_cuda12/lib/python3.11/sitepackages/jax/_src/numpy/linalg.py"", line 455, in _slogdet_lu     lu, pivot, _ = lax_linalg.lu(a) jax._src.source_info_util.JaxStackTraceBeforeTransformation: RuntimeError: jaxlib/gpu/solver_kernels.cc:45: operation gpusolverDnCreate(&handle) failed: cuSolver internal error The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File """", line 1, in    File ""/home/s.ono/.conda/envs/jax_cuda12/lib/python3.11/sitepackages/jaxlib/gpu_solver.py"", line 102, in _getrf_hlo     lwork, opaque = gpu_solver.build_getrf_descriptor(                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ RuntimeError: jaxlib/gpu/solver_kernels.cc:45: operation gpusolverDnCreate(&handle) failed: cuSolver internal error ``` On the other hand, when I tried the following command, it works well. ``` >>> a = jnp.array([[1., 2.],[3., 4.]]) >>> jnp.linalg.det(a) Array(2., dtype=float32) ```  System info (python version, jaxlib version, accelerator, etc.)  System info (python version, jaxlib version, accelerator, etc.) ``` >>> jax.print_environment_info() jax:    0.4.29 jaxlib: 0.4.29 numpy:  1.26.4 python: 3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0] jax.devices (2 total, 2 local): [cuda(id=0) cuda(id=1)] process_count: 1 platform: uname_result(system='Linux', node='gnode05', release='3.10.01160.53.1.el7.x86_64', version=' CC(Python 3 compatibility issues) SMP Fri Jan 14 13:59:45 UTC 2022', machine='x86_64') $ nvidiasmi Tue Jun 18 21:26:33 2024        ++  ++ ```",2024-06-18T12:27:39Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/21950,"Can you describe exactly how you installed jax (and any relevant drivers, etc.) so that we can try to reproduce?","  Thanks for your message. I am using miniconda for the virtual environment. The following is what I did in a virtual enviornment.  ``` >>> conda create n jax_cuda12_test python=3.11.7 >>> conda activate jax_cuda12_test >>> python3 m pip install nvidiacudaccclcu12==12.4.127 nvidiacudacupticu12==12.4.127 nvidiacudanvcccu12==12.4.131 nvidiacudaopenclcu12==12.4.127 nvidiacudanvrtccu12==12.4.127 nvidiacublascu12==12.4.5.8 nvidiacudasanitizerapicu12==12.4.127 nvidiacufftcu12 nvidiacurandcu12 nvidiacusolvercu12==11.4.5.107 nvidiacusparsecu12==12.4.1.24 nvidianppcu12 nvidianvfatbincu12==12.4.127 nvidianvjitlinkcu12==12.4.127 nvidianvjpegcu12 nvidianvmldevcu12==12.4.127 nvidianvtxcu12==12.4.127 nvidiacudaruntimecu12==12.4.127 >>> pip install upgrade pip >>> pip install upgrade ""jax[cuda12]"" ``` The resulting `conda list` is as follows. ```  Name                    Version                   Build  Channel _libgcc_mutex             0.1                        main   _openmp_mutex             5.1                       1_gnu   bzip2                     1.0.8                h5eee18b_6   cacertificates           2024.3.11            h06a4308_0   jax                       0.4.29                   pypi_0    pypi jaxcuda12pjrt           0.4.29                   pypi_0    pypi jaxcuda12plugin         0.4.29                   pypi_0    pypi jaxlib                    0.4.29                   pypi_0    pypi ld_impl_linux64          2.38                 h1181459_1   libffi                    3.4.4                h6a678d5_1   libgccng                 11.2.0               h1234567_1   libgomp                   11.2.0               h1234567_1   libstdcxxng              11.2.0               h1234567_1   libuuid                   1.41.5               h5eee18b_0   mldtypes                 0.4.0                    pypi_0    pypi ncurses                   6.4                  h6a678d5_0   numpy                     1.26.4                   pypi_0    pypi nvidiacublascu12        12.4.5.8                 pypi_0    pypi nvidiacudaccclcu12     12.4.127                 pypi_0    pypi nvidiacudacupticu12    12.4.127                 pypi_0    pypi nvidiacudanvcccu12     12.4.131                 pypi_0    pypi nvidiacudanvrtccu12    12.4.127                 pypi_0    pypi nvidiacudaopenclcu12   12.4.127                 pypi_0    pypi nvidiacudaruntimecu12  12.4.127                 pypi_0    pypi nvidiacudasanitizerapicu12 12.4.127                 pypi_0    pypi nvidiacudnncu12         9.1.1.17                 pypi_0    pypi nvidiacufftcu12         11.2.3.18                pypi_0    pypi nvidiacurandcu12        10.3.6.39                pypi_0    pypi nvidiacusolvercu12      11.4.5.107               pypi_0    pypi nvidiacusparsecu12      12.4.1.24                pypi_0    pypi nvidiancclcu12          2.21.5                   pypi_0    pypi nvidianppcu12           12.3.0.116               pypi_0    pypi nvidianvfatbincu12      12.4.127                 pypi_0    pypi nvidianvjitlinkcu12     12.4.127                 pypi_0    pypi nvidianvjpegcu12        12.3.2.38                pypi_0    pypi nvidianvmldevcu12      12.4.127                 pypi_0    pypi nvidianvtxcu12          12.4.127                 pypi_0    pypi openssl                   3.0.13               h7f8727e_2   opteinsum                3.3.0                    pypi_0    pypi pip                       24.0                     pypi_0    pypi python                    3.11.7               h955ad1f_0   readline                  8.2                  h5eee18b_0   scipy                     1.13.1                   pypi_0    pypi setuptools                69.5.1                   pypi_0    pypi sqlite                    3.45.3               h5eee18b_0   tk                        8.6.14               h39e8969_0   tzdata                    2024a                h04d1e81_0   wheel                     0.43.0                   pypi_0    pypi xz                        5.4.6                h5eee18b_1   zlib                      1.2.13               h5eee18b_1 ```","I'll look into this a little later, but you shouldn't need to install all those `nvidia` pip packages manually. What happens if you just `pip install jax[cuda12]` in a fresh environment?"," Thanks for your message. In a new environment, I did `pip install upgrade ""jax[cuda12]""`. It looks like `pip install upgrade ""jax[cuda12]""` cannot install collect versions of `nvidia` packages. Actually, the reason why I installed `nvidia` myself is that I met the same warning.  ``` >>> import jax.numpy as jnp >>> a = jnp.array(1.) 20240619 10:47:17.191520: W external/xla/xla/service/gpu/nvptx_compiler.cc:765] The NVIDIA driver's CUDA version is 12.4 which is older than the ptxas CUDA version (12.5.40). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIAprovided CUDA forward compatibility packages. ```  On the other hand, the original error did not appear.  ``` >>> import jax.numpy as jnp >>> c = jnp.array([[ 0.,  0.,  0.,  1.],[0.,  0., 1.,  0.],[ 0.,  1.,  0.,  0.],[1.,  0., 0.,  0.]]) >>> jnp.linalg.det(c) Array(1., dtype=float32) ``` Just in case, I also show my `conda list`. ``` _libgcc_mutex             0.1                        main   _openmp_mutex             5.1                       1_gnu   bzip2                     1.0.8                h5eee18b_6   cacertificates           2024.3.11            h06a4308_0   jax                       0.4.30                   pypi_0    pypi jaxcuda12pjrt           0.4.30                   pypi_0    pypi jaxcuda12plugin         0.4.30                   pypi_0    pypi jaxlib                    0.4.30                   pypi_0    pypi ld_impl_linux64          2.38                 h1181459_1   libffi                    3.4.4                h6a678d5_1   libgccng                 11.2.0               h1234567_1   libgomp                   11.2.0               h1234567_1   libstdcxxng              11.2.0               h1234567_1   libuuid                   1.41.5               h5eee18b_0   mldtypes                 0.4.0                    pypi_0    pypi ncurses                   6.4                  h6a678d5_0   numpy                     2.0.0                    pypi_0    pypi nvidiacublascu12        12.5.2.13                pypi_0    pypi nvidiacudacupticu12    12.5.39                  pypi_0    pypi nvidiacudanvcccu12     12.5.40                  pypi_0    pypi nvidiacudaruntimecu12  12.5.39                  pypi_0    pypi nvidiacudnncu12         9.1.1.17                 pypi_0    pypi nvidiacufftcu12         11.2.3.18                pypi_0    pypi nvidiacusolvercu12      11.6.2.40                pypi_0    pypi nvidiacusparsecu12      12.4.1.24                pypi_0    pypi nvidiancclcu12          2.21.5                   pypi_0    pypi nvidianvjitlinkcu12     12.5.40                  pypi_0    pypi openssl                   3.0.14               h5eee18b_0   opteinsum                3.3.0                    pypi_0    pypi pip                       24.0                     pypi_0    pypi python                    3.11.7               h955ad1f_0   readline                  8.2                  h5eee18b_0   scipy                     1.13.1                   pypi_0    pypi setuptools                69.5.1                   pypi_0    pypi sqlite                    3.45.3               h5eee18b_0   tk                        8.6.14               h39e8969_0   tzdata                    2024a                h04d1e81_0   wheel                     0.43.0                   pypi_0    pypi xz                        5.4.6                h5eee18b_1   zlib                      1.2.13               h5eee18b_1 ``` P.S. Yesterday, JAX was updated, so the result of `jax.print_environment_info()` was also changed.  ``` >>> jax.print_environment_info() jax:    0.4.30 jaxlib: 0.4.30 numpy:  2.0.0 python: 3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0] jax.devices (2 total, 2 local): [cuda(id=0) cuda(id=1)] process_count: 1 platform: uname_result(system='Linux', node='gnode05', release='3.10.01160.53.1.el7.x86_64', version=' CC(Python 3 compatibility issues) SMP Fri Jan 14 13:59:45 UTC 2022', machine='x86_64') $ nvidiasmi Wed Jun 19 10:45:13 2024        ++  ++ ```","Thanks for the info. I can reproduce the warning that you're seeing about the ptxas version. I was able to work around this by simply downgrading the `nvidiacudanvcccu12` pip package. So, from a fresh virtual environment, I was able to get a working installation with: ```bash pip install ""jax[cuda12]"" ""nvidiacudanvcccu12<12.5"" ``` Want to see if that fixes the issue for you? Edited to add: I also wouldn't worry too much about the warning. It may make `jit` compilation a little bit slower, but I wouldn't expect it to be a major issue!", Thank you for your reply. The command you provided works for my environment. My program seems to run correctly. Let me check further if nothing happens. ,I'm going to close this as completed  please feel free to comment if there are other issues.
yi,Inconsistent behavior with pallas + jnp.power + fp16," Description I have the following 3 kernels that does y = x ** 1 , y= x ** 2 , y= x ** 3, and their behavior is inconsistent (one fails to compile, 2 succeeds) ``` import sys import jax import jax.numpy as jnp import jax.experimental.pallas as pl def main(kernel_num):     def kernel_1(x_ref, o_ref):         x_i = pl.load(x_ref, (slice(None), ))  read whole thing         o_i = jnp.power(x_i, 1)         pl.store(o_ref, (slice(None), ), o_i)     def kernel_2(x_ref, o_ref):         x_i = pl.load(x_ref, (slice(None), ))  read whole thing         o_i = jnp.power(x_i, 2)         pl.store(o_ref, (slice(None), ), o_i)     def kernel_3(x_ref, o_ref):         x_i = pl.load(x_ref, (slice(None), ))  read whole thing         o_i = jnp.power(x_i, 3)         pl.store(o_ref, (slice(None), ), o_i)     if kernel_num == 1:         kernel = kernel_1     elif kernel_num == 2:         kernel = kernel_2     elif kernel_num == 3:         kernel = kernel_3     x = jnp.array([1.0, 2.0, 3.0, 4.0], dtype=jnp.float16)     y = pl.pallas_call(kernel,                        grid=(1,),                        out_shape=jax.ShapeDtypeStruct((4,), jnp.float16))(x)     print(y) if __name__ == '__main__':     argv = sys.argv     kernel_num = int(argv[1])     main(kernel_num) ``` Running `kernel_2` and `kernel_3` succeeds ``` $ python jax_dtype_demo.py 2 [ 1.  4.  9. 16.] $ python jax_dtype_demo.py 3 [ 1.  8. 27. 64.] ``` But not for 1 (or any other integers that I tested, for that matter) ``` $ python jax_dtype_demo.py 1 $ python jax_dtype_demo.py 1 Traceback (most recent call last):   File ""/repo/jax_dtype_demo.py"", line 41, in      main(kernel_num)   File ""/repo/jax_dtype_demo.py"", line 32, in main     y = pl.pallas_call(kernel,   File ""/venv/lib/python3.11/sitepackages/jax/_src/pallas/pallas_call.py"", line 589, in wrapped     grid_mapping, jaxpr, consts, _ = _trace_to_jaxpr(   File ""/venv/lib/python3.11/sitepackages/jax/_src/pallas/pallas_call.py"", line 489, in _trace_to_jaxpr     jaxpr, _, consts, () = pe.trace_to_jaxpr_dynamic(wrapped_fun, jaxpr_flat_avals, debug)   File ""/repo/jax_dtype_demo.py"", line 12, in kernel_1     o_i = jnp.power(x_i, 1)   File ""/venv/lib/python3.11/sitepackages/jax/_src/numpy/ufuncs.py"", line 354, in power     return lax.integer_pow(x1, x2) jax._src.source_info_util.JaxStackTraceBeforeTransformation: NotImplementedError: unsupported types for pow: ('float16', 'int32') The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/venv/lib/python3.11/sitepackages/jax/_src/pallas/triton/lowering.py"", line 351, in lower_jaxpr_to_triton_ir     outvals = rule(rule_ctx, *invals, **eqn.params)               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/venv/lib/python3.11/sitepackages/jax/_src/pallas/triton/lowering.py"", line 574, in inner     raise NotImplementedError( NotImplementedError: unsupported types for pow: ('float16', 'int32') The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/repo/jax_dtype_demo.py"", line 41, in      main(kernel_num)   File ""/repo/jax_dtype_demo.py"", line 32, in main     y = pl.pallas_call(kernel,         ^^^^^^^^^^^^^^^^^^^^^^   File ""/venv/lib/python3.11/sitepackages/jax/_src/pallas/pallas_call.py"", line 531, in _pallas_call_lowering     return pallas_call_registration.pallas_call_lowering(            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/venv/lib/python3.11/sitepackages/jax/_src/pallas/triton/pallas_call_registration.py"", line 306, in pallas_call_lowering     return lowering_fn(            ^^^^^^^^^^^^   File ""/venv/lib/python3.11/sitepackages/jax/_src/pallas/triton/pallas_call_registration.py"", line 205, in _pallas_call_ttir_lowering     lowering_result = lowering.lower_jaxpr_to_triton_module(                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/venv/lib/python3.11/sitepackages/jax/_src/pallas/triton/lowering.py"", line 304, in lower_jaxpr_to_triton_module     () = lower_jaxpr_to_triton_ir(ctx, jaxpr, block_infos, *entry.arguments)          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/venv/lib/python3.11/sitepackages/jax/_src/pallas/triton/lowering.py"", line 351, in lower_jaxpr_to_triton_ir     outvals = rule(rule_ctx, *invals, **eqn.params)   ^^^^^   File ""/venv/lib/python3.11/sitepackages/jax/_src/pallas/triton/lowering.py"", line 1088, in f_lowered     out = _closed_call_lowering_rule(ctx, *args, call_jaxpr=jaxpr)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/venv/lib/python3.11/sitepackages/jax/_src/pallas/triton/lowering.py"", line 2095, in _closed_call_lowering_rule     return lower_jaxpr_to_triton_ir(ctx.context, jaxpr, ctx.block_infos, *args)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/venv/lib/python3.11/sitepackages/jax/_src/pallas/triton/lowering.py"", line 356, in lower_jaxpr_to_triton_ir     raise LoweringError( jax._src.pallas.triton.lowering.LoweringError: Exception while lowering eqn:   a:f16[4] = pow b 1 With context:   LoweringRuleContext(context=ModuleContext(name='kernel_1', grid_mapping=GridMapping(grid=(1,), block_mappings=(None, None), mapped_dims=(), num_index_operands=0, num_scratch_operands=0), program_ids=[]), avals_in=[ShapedArray(float16[4]), ShapedArray(int32[], weak_type=True)], avals_out=[ShapedArray(float16[4])], block_infos=[None, None]) With inval types=[RankedTensorType(tensor), None] In jaxpr: { lambda ; a:f16[4]. let b:f16[4] = pow a 1 in (b,) }  For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. ``` I have 2 questions: 1. In this simple example, why does jax/pallas handle `2`, `3` and other integers differently? 2. It seems to me that `2` and `3` works because an implicit type deduction (from python's Int to a fp16 compatible integer when lowering) is in place, and for other integers jax don't do that. Are users required to signify the type deduction in some way when using python constant in pallas? If so, how does one do that?  System info (python version, jaxlib version, accelerator, etc.) ``` Python 3.11.8 (main, Feb 25 2024, 16:39:33) [GCC 11.4.0] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import jax; jax.print_environment_info() jax:    0.4.26 jaxlib: 0.4.26 numpy:  1.26.4 python: 3.11.8 (main, Feb 25 2024, 16:39:33) [GCC 11.4.0] jax.devices (8 total, 8 local): [cuda(id=0) cuda(id=1) ... cuda(id=6) cuda(id=7)] process_count: 1 platform: uname_result(system='Linux', node='h10003', release='5.15.0100generic', version=' CC(Implement np.linalg.inv using a QR decomposition.)Ubuntu SMP Wed Feb 7 13:27:48 UTC 2024', machine='x86_64') $ nvidiasmi Mon Jun 17 20:20:37 2024        ++  ++ ```",2024-06-17T20:21:08Z,bug pallas,closed,0,1,https://github.com/jax-ml/jax/issues/21928,"We specialcase a few integer powers here and use libdevice functions for the rest. libdevice does not have a `pow` implementation for float16 and int32. I think we should probably loop instead in the general case, since the power is a statically known integer."
rag,[`LRUCache`] Storing the last access time of a cache entry in a separate file,"This PR is a followup of https://github.com/google/jax/pull/21394. It stores the last access time of a cache entry in a separate file for `LRUCache`. Previously, the information was stored in the file metadata (`mtime`). This PR is part of the process to add LRU cache eviction policy for the JAX persistent compilation cache. This change is necessary because we want to support GCS and potentially other cloud storage providers, where the `mtime` attribute may be either not available or very complicated to modify. I am planning to get this PR merged before https://github.com/google/jax/pull/21893, because this one is more complicated and merging this PR first will make it easier to resolve merge conflicts.  Details Previous behavior: If cache entries: `['a', 'b']` Then cache files: `['cache_dir/a', 'cache_dir/b']` Modified behavior: If cache entries: `['a', 'b']` Then cache files: `['cache_dir/acache.bin', 'cache_dir/bcache.bin', 'cache_dir/aatime.txt', 'cache_dir/batime.txt']`",2024-06-17T20:12:35Z,pull ready,closed,0,2,https://github.com/jax-ml/jax/issues/21926,Blocked by the failed `CompilationCacheTest` above,Need https://github.com/google/jax/pull/21982 to be merged first
yi,Add an `ffi_call` function with a similar signature to `pure_callback`,"Currently, JAX users who want to use XLA custom calls must interact with private APIs (e.g. `core.Primitive`) and MLIR. This doesn’t provide a great developer experience, and it would be useful to provide some sort of public API. This has been previously discussed in several different contexts (including CC([FFI]: Add JEP for FFI)), and this PR builds on ideas from this previous work to present a simple API that covers some core use cases. There are more advanced use cases which would require finergrained customization, and these would continue to rely on the private API. But, there do appear to be examples of use cases that would be satisfied by this simpler interface.  Example The general idea is to provide a function called (something like) `jax.extend.ffi.ffi_call` with a signature that is similar to `jax.pure_callback`, that lowers to a custom call. For example, the existing implementation of lu_pivots_to_permutation on GPU (the only FFI custom call currently in `jaxlib`), could (to first approximation) be written as: ```python def ffi_call_lu_pivots_to_permutation(pivots, permutation_size):   return jex.ffi.ffi_call(       ""cu_lu_pivots_to_permutation"",        Output types are specified without reference to MLIR       jax.ShapeDtypeStruct(           shape=dims[:1] + (permutation_size,),           dtype=pivots.dtype,       ),        Input arguments       pivots,        Keyword arguments are passed to the FFI custom call as attributes       permutation_size=np.int32(permutation_size),   Note: np not jnp   ) from jax.lib import xla_client xla_client.register_custom_call_target(     ""cu_lu_pivots_to_permutation"", ..., platform=""CUDA"", api_version=1) ``` Platformdependent behavior should be handled in user code with the help of `lax.platform_dependent`. (Currently this doesn't work, but  is looking into it.) Like `jax.pure_callback`, this could be combined with `custom_jvp` or `custom_vjp` to support autodiff. `vmap` is discussed below.  Batching This proofofconcept implementation includes a `vectorized` parameter which has the same behavior as the `vectorized` parameter to `jax.pure_callback` (in fact it uses exactly the same batching rule). The tl;dr is that when `vectorized is False`, the base custom call is executed in a while loop, but when `vectorized is True`, the `vmap`ped primitive calls the same custom call with an extra batch dimension on the left. This behavior could potentially work with the FFI interface since the input buffers include dimension metadata, but it’s a restrictive interface. Is there a better approach (don’t say custom_vmap! Or do...)?  Alternatives If we’re not totally wedded to aligning with the `jax.pure_callback` interface, it’s possible that a ""builder"" interface would be more future proof. For example, the syntax for the demo from above would be something like: ```python do_call = jex.ffi.make_ffi_call(""cu_lu_pivots_to_permutation"") do_call(     jax.ShapeDtypeStruct(         shape=dims[:1] + (permutation_size,),         dtype=pivots.dtype,     ),     pivots,     batch_size=np.int64(batch_size),     pivot_size=np.int32(pivot_size),     permutation_size=np.int32(permutation_size), ) ``` This has the potential benefit that do_call could include metadata like a reference to the underlying `core.Primitive` so that users could use that for further customization.",2024-06-17T19:50:28Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/21925
yi,sparse: bcoo_spdot_general have incorrect abstract_eval," Description ```python import jax from jax import lax from jax import numpy as jnp from jax.experimental import sparse as sp from jax.experimental.sparse.util import SparseInfo lhs = sp.BCOO(     (         jnp.ones((1, 1)),         lax.broadcasted_iota(jnp.int32, (10, 1, 1), 0),     ),     shape=(10, 10), ) rhs = sp.BCOO(     (jnp.array([1.0]), jnp.array([[3]])),     shape=(10,), )  return shape is wrong print(     sp.bcoo_spdot_general_p.abstract_eval(         lhs.data.aval,         lhs.indices.aval,         rhs.data.aval,         rhs.indices.aval,         dimension_numbers=(((1,), (0,)), ((), ())),         lhs_spinfo=SparseInfo(             shape=(10, 10), indices_sorted=False, unique_indices=False         ),         preferred_element_type=None,         rhs_spinfo=SparseInfo(shape=(10,), indices_sorted=False, unique_indices=False),     ) ) def inner(x, y):     return lax.dot_general(x, y, dimension_numbers=(((1,), (0,)), ((), ()))) print(sp.sparsify(inner)(lhs, rhs))   this works fine print(jax.jit(sp.sparsify(inner))(lhs, rhs)) ``` results in ``` ((ShapedArray(float32[1,1]), ShapedArray(int32[10,1,0])), frozenset()) BCOO(float32[10], nse=1, n_batch=1) Traceback (most recent call last):   File ""/nix/store/g2zlb8pq0rd2kyz7q6v1l2ivz96am8lpenv/lib/python3.12/sitepackages/jax/_src/interpreters/mlir.py"", line 994, in lower_jaxpr_to_module     if not ctx.module.operation.verify():            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ jaxlib.mlir._mlir_libs._site_initialize..MLIRError: Verification failed: error: unknown: type of return operand 0 ('tensor') doesn't match function result type ('tensor') in function   note: unknown: see current operation: ""func.return""(%104, %106) : (tensor, tensor) > () The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""***********************************.py"", line 42, in      print(jax.jit(sp.sparsify(inner))(lhs, rhs))           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ValueError: Cannot lower jaxpr with verifier errors: 	type of return operand 0 ('tensor') doesn't match function result type ('tensor') in function  		at loc(unknown) 	see current operation: ""func.return""(%104, %106) : (tensor, tensor) > () 		at loc(unknown)Define JAX_DUMP_IR_TO to dump the module.  For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.28 jaxlib: 0.4.28 numpy:  1.26.4 python: 3.12.2 (main, Feb  6 2024, 20:19:44) [GCC 13.2.0] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='DESKTOPNV274K6', release='5.15.153.1microsoftstandardWSL2', version=' CC(Python 3 compatibility issues) SMP Fri Mar 29 23:14:13 UTC 2024', machine='x86_64') $ nvidiasmi Mon Jun 17 13:59:12 2024        ++  ++ ```",2024-06-17T18:59:32Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/21921,Thanks for the report! CC(Fix bug in bcoo_spdot_general abstract_eval) should fix this.
rag,Use constant memory to pass in TMA descriptors to the kernel,"Use constant memory to pass in TMA descriptors to the kernel To work around another buggy part of the PTX documentation. While PTX explicitly says that TMA descriptors can be in global memory, the C++ programming guide heavily discurages this, because it can lead to incorrrect results. Which is also what we've sometimes observed as a cache coherency issue unless a TMA fence is explicitly inserted at the beginning of the kernel. Note that this approach has a big downside of making the kernel unsafe for concurrent use. I don't think that XLA:GPU will ever dispatch it concurrently so I didn't insert any extra synchronization for now, but we should seriously consider it. My hope at the moment is that we'll be able to start passing in TMA descs as kernel args soon (pending upstreaming LLVM changes...) and we won't have to deal with this again. For the programming guide, see: https://docs.nvidia.com/cuda/cudacprogrammingguide/index.htmlusingtmatotransfermultidimensionalarrays",2024-06-17T11:24:52Z,,closed,0,0,https://github.com/jax-ml/jax/issues/21910
yi,Build fails: Can't find amdhip64 and roctracer libs," Description Hello, When building v0.4.16 on Arch Linux, I ran into an issue where the amdhip64 and roctracer libs can't be found.  It seems like JAX is looking for them under `/opt/rocm/hip` and `/opt/rocm/roctracer`, respectively.  However, both libs only exist in `/opt/rocm/lib`.  Copying the libs to where JAX is looking for them resolves the issue, but I think that work should be handled by the build script. I've already reported this issue in the `pythonjaxrocm` Arch Linux package: https://github.com/brianrobt/pythonjaxrocm/issues/3 The exact command that is failing is this: ```bash   python build/build.py enable_rocm \     ""rocm_amdgpu_targets=gfx803,gfx900,gfx906,gfx908,gfx90a,gfx1030,gfx1100,gfx1101,gfx1102"" \     bazel_options=override_repository=xla=src/xlarocmjaxlibv0.4.16.tar.gz ``` Thank you, Brian  System info (python version, jaxlib version, accelerator, etc.) ``` OS: Arch Linux Python: 3.12.3 jaxlib: 0.4.16 ```",2024-06-14T15:55:31Z,bug needs info AMD GPU,closed,0,3,https://github.com/jax-ml/jax/issues/21886,v0.4.16 is quite old at this point. Does the issue still happen with a current version?,", Let me try early next week and I'll get back to you.  Thanks!",The issue doesn't happen on v0.4.29.  I'm closing out the issue.  Thanks !
yi,Add missing dependency,"Local check if for missing dependency. Proper fix is changing the upstream build target, verifying if this would fix locally.",2024-06-13T18:25:02Z,windows:force-run,closed,0,0,https://github.com/jax-ml/jax/issues/21858
yi,`jit-eval_shape-<callable PyTree with numpy __eq__ semantics>` crashes on JAX 0.4.29," Description First the MWE: ```python import equinox as eqx import jax class F(eqx.Module):     x: jax.Array     def __call__(self):         return 1. .jit def f(x):     jax.eval_shape(F(x)) x = jax.numpy.arange(3) f(x)  jax.errors.TracerBoolConversionError: Attempted boolean conversion of traced array with shape bool[]. ``` it looks like JAX is trying to determine cacheability of a callable using just `hash`: https://github.com/google/jax/blob/a0e5e0f411f3a4b2830e90fb6fdc914730f29ba4/jax/_src/api.pyL2807L2809 but in practice to be usable as a dictionary key, we must also require that its `__eq__` method return a boolean  and not a traced boolean! Equinox tries to follow NumPy semantics here by doing approximately `all(x == y for x, y in zip(leaves(self), leaves(other)))`, and this returns a tracer. FWIW the `hash` check *would* have worked for Equinox here  as Equinox computes its `__hash__` by hashing all the leaves  except that CC(Tracers are now hashable.) means that the `hash` worked!  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.29 jaxlib: 0.4.29 numpy:  1.26.4 python: 3.11.9 (main, Apr 19 2024, 11:43:47) [Clang 14.0.6 ] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Darwin', node='Air.localdomain', release='22.5.0', version='Darwin Kernel Version 22.5.0: Mon Apr 24 20:52:43 PDT 2023; root:xnu8796.121.2~5/RELEASE_ARM64_T8112', machine='arm64')",2024-06-12T13:06:04Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/21825,"Hi kidger, I couldn't reproduce the issue with JAX 0.4.36 (and later) and Equinox 0.11.7/0.11.11. The provided repro is executed without any error. Please find the gist for reference. Thank you."
yi,"[Mosaic] Handle adding singleton minor dimension that was already implicit for non-32-bit types, and do not force native tiling","[Mosaic] Handle adding singleton minor dimension that was already implicit for non32bit types, and do not force native tiling Also fix extra comma in apply_vector_layout_test which was being annoying with autoformatter",2024-06-11T00:14:22Z,,closed,0,0,https://github.com/jax-ml/jax/issues/21793
yi,jax.numpy.linalg.inv returns spurious results when called with array of matrices," Description I encountered unexpected behavior while using `jnp.linalg.inv` to invert an array of matrices (I wanted to get an array of their inverses). Specifically,  ```python import jax.numpy as jnp invertible = jnp.eye(2) not_invertible = jnp.array([[1,0], [1,0]]) array_of_matrices = jnp.array([invertible, not_invertible]) print(jnp.linalg.inv(not_invertible))   Expected result print(jnp.linalg.inv(array_of_matrices)[1])   Spurious inverse of not_invertible ``` This isn't mentioned in the documentation, which specifies that the return type is   Array of shape (..., N, N) containing the inverse of the input.  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.28 jaxlib: 0.4.28 numpy:  1.26.4 python: 3.11.8 (v3.11.8:db85d51d3e, Feb  6 2024, 18:02:37) [Clang 13.0.0 (clang1300.0.29.30)] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Darwin', node='bsmbpas0019', release='23.3.0', version='Darwin Kernel Version 23.3.0: Wed Dec 20 21:33:31 PST 2023; root:xnu10002.81.5~7/RELEASE_ARM64_T8112', machine='arm64')",2024-06-10T19:26:50Z,bug duplicate,closed,0,2,https://github.com/jax-ml/jax/issues/21785,"Thanks for the report! This is a known XLA CPU issue, first reported in CC(vmap behaving unexpected with function that inverts singular matrix) and tracked here: https://github.com/openxla/xla/issues/3891 Unfortunately there hasn't been much traction on the XLA side to get this fixed","Thank you for the lightning fast reply! Good to know, I'm working around it for now. And sorry for the duplicate, I hadn't spotted that."
rag,Bump zipp from 3.18.1 to 3.19.2,"Bumps zipp from 3.18.1 to 3.19.2.  Changelog Sourced from zipp's changelog.  v3.19.2 No significant changes. v3.19.1 Bugfixes  Improved handling of malformed zip files. ( CC(example Errata))  v3.19.0 Features  Implement is_symlink. ( CC(add oss test instructions, fix conv grad bug))  v3.18.2 No significant changes.    Commits  c6a3339 Move Python compatibility concerns to the appropriate modules. c24fc57 Finalize 294a462 Ignore coverage misses in tests.compat.py39 aab60f4 🧎‍♀️ Genuflect to the types. 59f852a Correct typo (incorrect letter used) when expacting alpharep root. 2a7a5bc Add test capturing expectation that a Path is a Traversable. 7d2b55b Update docstring to reference Traversable. 6d1cb72 Finalize fd604bd Merge pull request  CC(Can’t import both jax and tensorflow (causes kernel restart)) from jaraco/bugfix/119malformedpaths c18417e Add news fragment. Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-06-10T17:37:49Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/21778,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
rag,tree_all: add support for is_leaf,"Fixes CC(Allow for `is_leaf` in tree_all) It looks like all other APIs that are based on `tree_leaves` already support and forward the `is_leaf` argument. For consistency, `tree_all` probably should as well. I also more broadly improved test coverage of these parameters.",2024-06-10T16:46:44Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/21774
yi,Improve vmap out_axes error, Changes Shows the path of invalid leaves when trying to return vectorized arrays through outputs marked as `out_axes=None`.,2024-06-10T13:04:37Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/21766, I fixed the remaining tests. There's still the TODO of trying to refactor the code such that we don't flatten twice but maybe we can do this in the future? This basic implementation is significantly more useful than what we had before.
yi,TPU Initialization Failed," Description During TPU Research Program, I created a new TPUv48 VM and it's been working well.  This is not the first time I encountered this issue. Whenever I got this issue, I deleted the TPUvm and created a new TPU VM.  bash history ``` PJRT_DEVICE=TPU python3 xla/test/test_train_mp_imagenet.py fake_data batch_size=256 num_epochs=1  pip install tensorboard tensorboard logdir=test /bin/python /home/user_abc/DeepLearning/test/train.py  /bin/python /home/user_abc/.vscodeserver/extensions/mspython.python2024.8.0/python_files/shell_exec.py /bin/python m pip install U torchtbprofiler /tmp/tmp835907rdB3kupeorY7.log ```  Error message ``` concurrent.futures.process._RemoteTraceback:  """""" Traceback (most recent call last):   File ""/usr/lib/python3.10/concurrent/futures/process.py"", line 246, in _process_worker     r = call_item.fn(*call_item.args, **call_item.kwargs)   File ""/usr/lib/python3.10/concurrent/futures/process.py"", line 205, in _process_chunk     return [fn(*args) for args in chunk]   File ""/usr/lib/python3.10/concurrent/futures/process.py"", line 205, in      return [fn(*args) for args in chunk]   File ""/home/user_abc/.local/lib/python3.10/sitepackages/torch_xla/runtime.py"", line 95, in wrapper     return fn(*args, **kwargs)   File ""/home/user_abc/.local/lib/python3.10/sitepackages/torch_xla/_internal/pjrt.py"", line 59, in _run_thread_per_device     initializer_fn(local_rank, local_world_size)   File ""/home/user_abc/.local/lib/python3.10/sitepackages/torch_xla/runtime.py"", line 95, in wrapper     return fn(*args, **kwargs)   File ""/home/user_abc/.local/lib/python3.10/sitepackages/torch_xla/_internal/pjrt.py"", line 122, in initialize_multiprocess     devices = xm.get_xla_supported_devices()   File ""/home/user_abc/.local/lib/python3.10/sitepackages/torch_xla/core/xla_model.py"", line 99, in get_xla_supported_devices     devices = torch_xla._XLAC._xla_get_devices() RuntimeError: Bad StatusOr access: UNKNOWN: TPU initialization failed: open(/dev/accel0): Operation not permitted: Operation not permitted; Couldn't open device: /dev/accel0; [/dev/accel0]  """""" The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/home/user_abc/DeepLearning/test/test_linear_train.py"", line 174, in      xmp.spawn(_mp_fn, args=(FLAGS,), start_method='fork')   File ""/home/user_abc/.local/lib/python3.10/sitepackages/torch_xla/runtime.py"", line 95, in wrapper     return fn(*args, **kwargs)   File ""/home/user_abc/.local/lib/python3.10/sitepackages/torch_xla/distributed/xla_multiprocessing.py"", line 38, in spawn     return pjrt.spawn(fn, nprocs, start_method, args)   File ""/home/user_abc/.local/lib/python3.10/sitepackages/torch_xla/_internal/pjrt.py"", line 211, in spawn     run_multiprocess(spawn_fn, start_method=start_method)   File ""/home/user_abc/.local/lib/python3.10/sitepackages/torch_xla/runtime.py"", line 95, in wrapper     return fn(*args, **kwargs)   File ""/home/user_abc/.local/lib/python3.10/sitepackages/torch_xla/_internal/pjrt.py"", line 171, in run_multiprocess     replica_results = list(   File ""/home/user_abc/.local/lib/python3.10/sitepackages/torch_xla/_internal/pjrt.py"", line 172, in      itertools.chain.from_iterable(   File ""/usr/lib/python3.10/concurrent/futures/process.py"", line 575, in _chain_from_iterable_of_lists     for element in iterable:   File ""/usr/lib/python3.10/concurrent/futures/_base.py"", line 621, in result_iterator     yield _result_or_cancel(fs.pop())   File ""/usr/lib/python3.10/concurrent/futures/_base.py"", line 319, in _result_or_cancel     return fut.result(timeout)   File ""/usr/lib/python3.10/concurrent/futures/_base.py"", line 458, in result     return self.__get_result()   File ""/usr/lib/python3.10/concurrent/futures/_base.py"", line 403, in __get_result     raise self._exception RuntimeError: Bad StatusOr access: UNKNOWN: TPU initialization failed: open(/dev/accel0): Operation not permitted: Operation not permitted; Couldn't open device: /dev/accel0; [/dev/accel0]  ```   System info (python version, jaxlib version, accelerator, etc.) ``` python                     3.10.12 torch                      2.3.1 torchtbprofiler          0.4.3 torchxla                  2.3.0 torchvision                0.18.1 tensorboard                2.17.0 tensorboarddataserver    0.7.2 tensorboard_plugin_profile 2.15.1 ```",2024-06-10T08:56:47Z,bug,closed,0,0,https://github.com/jax-ml/jax/issues/21761
yi,Expand `device_put` benchmarks to run with different numbers of arrays and input types,"Expand `device_put` benchmarks to run with different numbers of arrays and input types For the upcoming batching changes for `device_put`, it is useful to benchmark `device_put` with varying numbers of arrays.",2024-06-09T18:21:49Z,,closed,0,0,https://github.com/jax-ml/jax/issues/21752
yi,Allow specifying initial forget gate bias for LSTMs.,"Initializing the forget gate bias, typically to positive values, is a relatively common tweak for better LSTM training, so it would be nice to support this out of the box.",2024-06-09T10:39:39Z,,open,0,0,https://github.com/jax-ml/jax/issues/21749
rag,Make `pxla.shard_arg` batch calls to `xc.copy_array_to_devices_with_sharding`,"Make `pxla.shard_arg` batch calls to `xc.copy_array_to_devices_with_sharding` This CL changes `shard_arg_handlers` to be batched, in that it now receives a list of objects and a list of shardings and returns a list of array. This makes it possible to batch backend calls whenever it's beneficial to do so. Based on the above, the batched shard arg for arrays leverages the newly added `xla::ifrt::Client::CopyArrays()` (https://github.com/tensorflow/tensorflow/pull/69096) to make bulk copy cheaper in some backend implementations. Since `Client::CopyArrays()` requires batched arrays to have the same set of source/destination devices, `PyArray::BatchedCopyToDeviceWithSharding()` internally groups arrays by their source/destination devices and memory kinds. The grouping is pushed all the way to C++ for performance in case we have lots of arrays.",2024-06-07T16:38:45Z,,closed,0,0,https://github.com/jax-ml/jax/issues/21736
yi,TracerBoolConversionError when jitting jax.numpy.linalg.norm," Description I am having some problems when trying to `jax.jit` the norm function in `jax.numpy`.  Ideally I would like to use the jitted version of norm for any ord that is a float and also for inf values. The minimal working example is: ``` >>> import jax >>> import jax.numpy as jnp >>> a_fun = jax.jit(jnp.linalg.norm) >>> a_fun(jnp.array([1.0,1.0]),ord=1) ``` results in  ``` Traceback (most recent call last):   File """", line 1, in    File ""/Users/matteovilucchio/Documents/GitHub/linearregression/.venv/lib/python3.12/sitepackages/jax/_src/numpy/linalg.py"", line 1106, in norm     if ord is None or ord == 2:                       ^^^^^^^^ jax.errors.TracerBoolConversionError: Attempted boolean conversion of traced array with shape bool[].. The error occurred while tracing the function norm at /Users/matteovilucchio/Documents/GitHub/linearregression/.venv/lib/python3.12/sitepackages/jax/_src/numpy/linalg.py:1020 for jit. This value became a tracer due to JAX operations on these lines:   operation a:bool[] = eq b c     from line :1 () See https://jax.readthedocs.io/en/latest/errors.htmljax.errors.TracerBoolConversionError  For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.28 jaxlib: 0.4.28 numpy:  1.26.4 python: 3.12.3 (main, Apr  9 2024, 08:09:14) [Clang 15.0.0 (clang1500.1.0.2.5)] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Darwin', node='tsf508wpa6217.epfl.ch', release='22.6.0', version='Darwin Kernel Version 22.6.0: Wed Jul  5 22:21:53 PDT 2023; root:xnu8796.141.3~6/RELEASE_ARM64_T6020', machine='arm64') ```",2024-06-07T12:18:06Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/21730,"`ord` must be static in the current implementation of `norm`. You can try this instead: ```python >>> import jax >>> import jax.numpy as jnp >>> a_fun = jax.jit(jnp.linalg.norm, static_argnames=['ord']) >>> a_fun(jnp.array([1.0,1.0]),ord=1) Array(2., dtype=float32) ``` JAX doesn't have any implementation of `norm` that accepts traced `ord` values, but it would be easy enough to create if you wish. Somethings like this should work for vector norms: ```python .jit def norm(x, ord):   return jnp.sum(abs(x) ** ord) ** (1 / ord) ```","It seems odd that some `jax.numpy` functions are decorated with`(jax.jit, static_argn{num,name}=...)` by default and some are not. Is there is a rule to decide which function is decorated by default? If there is no rule, then maybe new users first interacting with `jax.numpy` and `jax` transforms like `jit` should not be facing errors like this. I would say `jax.numpy` functions should be decorated by default. The downside of course is the _unexpected_ recompilation when changing some of the arguments. For this recompilation behavior, maybe it can be indicated in the docstring which arg{nums,names} are static.  WDYT?","> Is there is a rule to decide which function is decorated by default? Not a hard rule, but some NumPy APIs are incompatible with JIT, so we don't decorate those by default. For example `jnp.unique` cannot be JITcompiled by default, because it produces dynamicallyshaped outputs. > maybe it can be indicated in the docstring which arg{nums,names} are static. We're working on this as part of CC(Tracking issue: inline docstrings) ","> Not a hard rule, but some NumPy APIs are incompatible with JIT, To be more precise, I meant within the set of function that are compatible with jit, and can be decorated with `(jax.jit, static_argn{num,name}=...)`. IIUC, within `numpy` there are: 1.  Function that are jitcompatible and decorated with static argnum/argname. 2. Function that are jitcompatible and need to be decorated with static argnum/argname under `jit`.  3. Functions that are _not_ jitcompatible. My suggestion was to push 2) to be like 1). > For example jnp.unique cannot be JITcompiled by default, because it produces dynamicallyshaped outputs.  IIRC `numpy.unique` and single argument `numpy.where` among others have very clear docstring with a paragraph discussing `jit` and the dynamic shape issue. >We're working on this as part of https://github.com/google/jax/issues/21461 Looks great as always."
yi,Add function to calculate concrete jax2tf output shapes from polymorphic input/output shapes.,Add function to calculate concrete jax2tf output shapes from polymorphic input/output shapes. This is helpful when trying to load a serialized TF model with polymorphic shapes and embedding in a JAX computation (see https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.mdcallingtensorflowfunctionsfromjax for details).,2024-06-05T13:40:50Z,,closed,0,1,https://github.com/jax-ml/jax/issues/21665,Closing Copybara created PR due to inactivity
yi,Improve tensorstore I/O efficiency,"Improve tensorstore I/O efficiency Previously, when writing the OCDBT format, the manifest and root B+tree node could be redundantly written multiple times depending on timing. With this change, the manifest and root B+tree node are always written only once. Additionally, source data was previously redundantly copied into the TensorStore chunk cache.",2024-06-05T05:23:43Z,,closed,0,0,https://github.com/jax-ml/jax/issues/21654
yi,Stochastic pmap lowering behavior in tests," Description I'm working on debugging a failure that we're seeing in our daily open source build. This test is consistently failing at `main` when we run the full test suite with 4 GPUs using `pytest` with the error: ``` tests/pmap_test.py:2084: in test_grad_of_pmap_compilation_caching     self.assertEqual(count[0], 2)   one for fwd, one for bwd E   AssertionError: 3 != 2 ``` This failure can't be reproduced with a single test and it only appeared last week, although I think the underlying issue must be older than that. I thought it might be some shared state, but I've isolated the issue a little bit. When logging the compiles, I find that the three lowering operations are: 1. `pmap_f`, 2. `pmap_backwards_pass` (transpose of `f`), and 3. `jit__multi_slice` (defined here). When I run just this test, I find that `jit__multi_slice` should be lowered during the ""warmup"" call here: https://github.com/google/jax/blob/65063b7ac30635e76d2d692fb976a11f136e32bf/tests/pmap_test.pyL2079 but, when the test fails, it's not actually being compiled until here: https://github.com/google/jax/blob/65063b7ac30635e76d2d692fb976a11f136e32bf/tests/pmap_test.pyL2083 It seems like maybe there's some kind of async race condition or something here, but adding a `block_until_ready` didn't fix the issue. I wanted to raise this here to see if anyone had thoughts for where this might be coming from, but I know that it's annoying to repro, so no worries if not!   System info (python version, jaxlib version, accelerator, etc.) TBD",2024-06-04T19:20:05Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/21643,Maybe you need to do `jax.grad` as the warmup?," — Thanks and good question! Nominally the idea is that that first block is specifically testing the number of lowering operations required for the grad (or really vjp). But, I chatted with  and we decided that perhaps the best solution for now is to just skip this first check (this is `pmap` after all), effectively doing just as you suggest!","Thank you for raising this issue and providing a detailed description. Based on your findings, here’s a proposed solution to address the stochastic pmap lowering behavior: Synchronize Operations: Ensure that all JAX operations are properly synchronized using `block_until_ready`. This ensures that operations are completed before moving on to the next step. ```     def test_grad_of_pmap_compilation_caching():         x = jnp.ones((8, 8))         f(x).block_until_ready()   Warmup any dispatching compilations         count = [0]         def count_compiles(jaxpr):             count[0] += 1             return jaxpr          Inject compilation counting         jax.config.update(""jax_log_compiles"", True)         jax.xla.jit_compilation_counter = count_compiles         _ = f_bwd(x).block_until_ready()   Force compilation of the backward pass         assert count[0] == 2   one for fwd, one for bwd ``` 2. Simplify the Test: As per the discussion above, there are possibly 3 ways of simplifying the test. We could focus on core compilation checks, verify compilation count directly, and isolate and test forward and backward passes separately.  These solutions should help ensure that the configuration is correctly set within the threadlocal state and avoid the issues seen with asynchronous operations. If there are any further details or test cases needed, please let me know. I am happy to assist with the implementation and submit a PR."
rag,jaxlib.xla_extension.XlaRuntimeError: INTERNAL: ptxas exited with non-zero error code 11," Description I am consistently getting an error out of a compilcated code ```python jaxlib.xla_extension.XlaRuntimeError: INTERNAL: ptxas exited with nonzero error code 11, output: : If the error message indicates that a file could not be written, please verify that sufficient filesystem space is provided. ``` after having installed jax/jaxlib with on a clean environment.  ```python pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` I also made sure that in my `LD_LIBRARY_PATH` nothing is set. Is there some way to debug this in any way?  System info (python version, jaxlib version, accelerator, etc.) ``` >>> import jax; jax.print_environment_info() jax:    0.4.28 jaxlib: 0.4.28 numpy:  1.26.4 python: 3.11.9  ++ ``` (the Nvidia SMI that is being picked up is from the cluster installation, but cuda is not in my path ```python (myenv2) [filippo.vicentinigpu02 test2]$ which nvcc /usr/bin/which: no nvcc in (/mnt/beegfs/softs/opt/gcc_10.2.0/openmpi/4.1.4/bin:/mnt/beegfs/softs/opt/core/gcc/10.2.0/bin:/mnt/beegfs/workdir/filippo.vicentini/mambaforge/envs/myenv2/bin:/opt/MegaRAID/perccli:/mnt/beegfs/home/CPHT/filippo.vicentini/.vscodeserver/cli/servers/Stabledc96b837cf6bb4af9cd736aa3af08cf8279f7685/server/bin/remotecli:/mnt/beegfs/home/CPHT/filippo.vicentini/.cargo/bin:/mnt/beegfs/softs/opt/core/mambaforge/22.11.14/condabin:/usr/lib64/qt3.3/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/dell/srvadmin/bin:/mnt/beegfs/home/CPHT/filippo.vicentini/.local/bin:/mnt/beegfs/home/CPHT/filippo.vicentini/bin:/opt/ibutils/bin) (myenv2) [filippo.vicentinigpu02 test2]$ which ptxas /usr/bin/which: no ptxas in (/mnt/beegfs/softs/opt/gcc_10.2.0/openmpi/4.1.4/bin:/mnt/beegfs/softs/opt/core/gcc/10.2.0/bin:/mnt/beegfs/workdir/filippo.vicentini/mambaforge/envs/myenv2/bin:/opt/MegaRAID/perccli:/mnt/beegfs/home/CPHT/filippo.vicentini/.vscodeserver/cli/servers/Stabledc96b837cf6bb4af9cd736aa3af08cf8279f7685/server/bin/remotecli:/mnt/beegfs/home/CPHT/filippo.vicentini/.cargo/bin:/mnt/beegfs/softs/opt/core/mambaforge/22.11.14/condabin:/usr/lib64/qt3.3/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/dell/srvadmin/bin:/mnt/beegfs/home/CPHT/filippo.vicentini/.local/bin:/mnt/beegfs/home/CPHT/filippo.vicentini/bin:/opt/ibutils/bin) ```",2024-06-04T09:22:07Z,bug,open,1,5,https://github.com/jax-ml/jax/issues/21621,"This is the error I get. I can also share a reproducer if wanted. ```python Traceback (most recent call last):   File ""/mnt/beegfs/project/ndqm/test_luca/time_evolution.py"", line 570, in      obs_dict = solve_variational_evolution(                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/mnt/beegfs/project/ndqm/test_luca/time_evolution.py"", line 299, in solve_variational_evolution     step_function = integration_algorithm(dt, H, exp_x)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/mnt/beegfs/project/ndqm/test_luca/time_evolution.py"", line 363, in step_explicit_O2     exp_z = nkj.operations.get_apply_exp_diagH(Hd)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/mnt/beegfs/workdir/filippo.vicentini/mambaforge/envs/ENV_NAME/lib/python3.11/sitepackages/netket_pro/jumps/operations/exact_ops_on_FrozenExtendedNet.py"", line 41, in get_apply_exp_diagH     i, j = ij.T            ^^^^   File ""/mnt/beegfs/workdir/filippo.vicentini/mambaforge/envs/ENV_NAME/lib/python3.11/sitepackages/jax/_src/numpy/lax_numpy.py"", line 630, in transpose     return lax.transpose(a, axes_)            ^^^^^^^^^^^^^^^^^^^^^^^   File ""/mnt/beegfs/workdir/filippo.vicentini/mambaforge/envs/ENV_NAME/lib/python3.11/sitepackages/jax/_src/lax/lax.py"", line 986, in transpose     return transpose_p.bind(operand, permutation=permutation)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/mnt/beegfs/workdir/filippo.vicentini/mambaforge/envs/ENV_NAME/lib/python3.11/sitepackages/jax/_src/core.py"", line 387, in bind     return self.bind_with_trace(find_top_trace(args), args, params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/mnt/beegfs/workdir/filippo.vicentini/mambaforge/envs/ENV_NAME/lib/python3.11/sitepackages/jax/_src/core.py"", line 391, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/mnt/beegfs/workdir/filippo.vicentini/mambaforge/envs/ENV_NAME/lib/python3.11/sitepackages/jax/_src/core.py"", line 879, in process_primitive     return primitive.impl(*tracers, **params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/mnt/beegfs/workdir/filippo.vicentini/mambaforge/envs/ENV_NAME/lib/python3.11/sitepackages/jax/_src/dispatch.py"", line 86, in apply_primitive     outs = fun(*args)            ^^^^^^^^^^ jaxlib.xla_extension.XlaRuntimeError: INTERNAL: ptxas exited with nonzero error code 11, output: : If the error message indicates that a file could not be written, please verify that sufficient filesystem space is provided.  For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. /mnt/beegfs/project/ndqm/test_luca/time_evolution.py:542: UserWarning: Data has no positive values, and therefore cannot be logscaled.   ax.set_yscale(""log"") /mnt/beegfs/project/ndqm/test_luca/time_evolution.py:559: UserWarning: Data has no positive values, and therefore cannot be logscaled.   ax[0,i].set_yscale(""log"") ```","Thanks for raising this. Yes, can you share a reproducer? The error talks about filesystem issues (""If the error message indicates that a file could not be written, please verify that sufficient filesystem space is provided""). Could there be a permissions issue?","I also got this error, what is the fix? ```bash Traceback (most recent call last):   File ""/home/muhammad.zeeshan/.conda/envs/gwkenv/bin/sage_n_spls_m_sgs"", line 8, in      sys.exit(main())              ^^^^^^   File ""/home/muhammad.zeeshan/.conda/envs/gwkenv/lib/python3.11/sitepackages/kokab/n_spls_m_sgs/sage.py"", line 308, in main     handler.run()   File ""/home/muhammad.zeeshan/.conda/envs/gwkenv/lib/python3.11/sitepackages/gwkokab/inference/flowMChandler.py"", line 91, in run     sampler.sample(self.initial_position, self.data)   File ""/home/muhammad.zeeshan/.conda/envs/gwkenv/lib/python3.11/sitepackages/flowMC/Sampler.py"", line 204, in sample     ) = strategy(         ^^^^^^^^^   File ""/home/muhammad.zeeshan/.conda/envs/gwkenv/lib/python3.11/sitepackages/flowMC/strategy/global_tuning.py"", line 174, in __call__     ) = global_sampler.sample(         ^^^^^^^^^^^^^^^^^^^^^^   File ""/home/muhammad.zeeshan/.conda/envs/gwkenv/lib/python3.11/sitepackages/flowMC/proposal/NF_proposal.py"", line 130, in sample     proposal_position, log_prob_proposal, log_prob_nf_proposal = self.sample_flow(                                                                  ^^^^^^^^^^^^^^^^^   File ""/home/muhammad.zeeshan/.conda/envs/gwkenv/lib/python3.11/sitepackages/flowMC/proposal/NF_proposal.py"", line 210, in sample_flow     log_prob_proposal = self.logpdf_vmap(proposal_position, data)                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ jaxlib.xla_extension.XlaRuntimeError: INTERNAL: ptxas exited with nonzero error code 9, output: ptxas info    : Disabling default positionindependentcode(pic) compilation mode as program requires more resources than allowed. : If the error message indicates that a file could not be written, please verify that sufficient filesystem space is provided.  For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. real	63m49.430s user	63m50.481s ```",I have encountered the same issue like this and finally recognize that jax version is not compatible with cuda version in my laptop. Check again about cuda version and maybe you can find the solution.,"I have also encountered a similar issue on a cluster.  At first, I thought this might be due to file permissions since this was on a cluster and I did not have certain permissions. But I get a different error related to versioning. I am not sure if it is referring to some CUDA library (cuDNN?), or PTXAS (see versions below).  Is there any way to debug this? I had an older version of JAX running fine (0.4.35) on the default Python environment that the cluster provides by installing JAX in user mode.  The error message says the following and it not clear what the .version is supposed to be: ``` jaxlib.xla_extension.XlaRuntimeError: INTERNAL: ptxas exited with nonzero error code 65280, output: ptxas /tmp/tempfilenid2003298c70b8be0481bd69193730162ba6f7551a77, line 5; fatal   : Unsupported .version 8.3; current version is '8.2' ptxas fatal   : Ptx assembly aborted due to errors ``` ``` >> ptxas version ptxas: NVIDIA (R) Ptx optimizing assembler Copyright (c) 20052023 NVIDIA Corporation Built on Tue_Jun_13_19:13:58_PDT_2023 Cuda compilation tools, release 12.2, V12.2.91 Build cuda_12.2.r12.2/compiler.32965470_0 ``` The CUDA version:  ``` nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 20052023 NVIDIA Corporation Built on Tue_Jun_13_19:16:58_PDT_2023 Cuda compilation tools, release 12.2, V12.2.91 Build cuda_12.2.r12.2/compiler.32965470_0 ``` The JAX version is: ``` '0.4.38' ``` The Python version is 3.11.7. "
rag,Bump zipp from 3.18.1 to 3.19.1,"Bumps zipp from 3.18.1 to 3.19.1.  Changelog Sourced from zipp's changelog.  v3.19.1 Bugfixes  Improved handling of malformed zip files. ( CC(example Errata))  v3.19.0 Features  Implement is_symlink. ( CC(add oss test instructions, fix conv grad bug))  v3.18.2 No significant changes.    Commits  6d1cb72 Finalize fd604bd Merge pull request  CC(Can’t import both jax and tensorflow (causes kernel restart)) from jaraco/bugfix/119malformedpaths c18417e Add news fragment. 58115d2 Employ SanitizedNames in CompleteDirs. Fixes broken test. 564fcc1 Add SanitizedNames mixin. 79a309f Add some assertions about malformed paths. 2d015c2 Merge https://github.com/jaraco/skeleton a595a0f Rename extras to align with core metadata spec. 608f90a Finalize 3a22d72 Merge pull request  CC(add tensordot) from jaraco/feature/issymlink Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-06-03T17:08:40Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/21603,Superseded by CC(Bump zipp from 3.18.1 to 3.19.2).
yi,`input_shardings` returned from the compiled executable should match the in_tree pre DCE. Otherwise calling it raises an error saying that the length of pre dce in_tree and post dce input_shardings don't match,`input_shardings` returned from the compiled executable should match the in_tree pre DCE. Otherwise calling it raises an error saying that the length of pre dce in_tree and post dce input_shardings don't match,2024-06-03T17:08:40Z,,closed,0,0,https://github.com/jax-ml/jax/issues/21602
yi,[Mosaic GPU] Make the profiler warpgroup aware,"[Mosaic GPU] Make the profiler warpgroup aware Instead of creating one timeline per block, we now create one timeline per warpgroup. This is especially useful when warpgroups differ in their execution traces. Also, instead of specifying the total capacity, the profiler now accepts a number specifying a number of entries perblock. This makes it easier to find a good size.",2024-06-03T14:29:53Z,,closed,0,0,https://github.com/jax-ml/jax/issues/21599
yi,Custom partitioning error in fused_attention_stablehlo," Description I'm trying to use the recently added `jax._src.cudnn.fused_attention_stablehlo`, but I'm getting the following error from the `infer_sharding_from_operands` function: ``` AttributeError: 'PositionalSharding' object has no attribute 'spec' ``` Clearly, the function is expecting a `NamedSharding` but getting a `PositionalSharding` instead. This is strange, because I am only using `NamedSharding` in my code. I've even tried adding a `jax.lax.with_sharding_constraing(inputs, NamedSharding(...))` right before calling `fused_attention_stablehlo.dot_product_attention`, to no avail. I inserted a breakpoint into the `infer_sharding_from_operands` function and found that the `mesh` object being passed in is totally empty (`Mesh(device_ids=[], axis_names=())`) and all of the arguments have `PositionalSharding`s. I'm not sure this is the fault of `fused_attention_stablehlo`, since in the custom_partitioning docs, it does suggest calling `.spec` directly on the sharding, suggesting it's safe to assume it will always be a `NamedSharding`.  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.28 jaxlib: 0.4.28 numpy:  1.26.4 python: 3.11.9  (main, Apr 19 2024, 18:36:13) [GCC 12.3.0] jax.devices (8 total, 8 local): [cuda(id=0) cuda(id=1) ... cuda(id=6) cuda(id=7)] process_count: 1 platform: uname_result(system='Linux', node='computehpcnode294', release='6.5.01018oracle', version=' CC(Populate readme)~22.04.1Ubuntu SMP Sat Feb 17 21:41:13 UTC 2024', machine='x86_64') ```",2024-06-02T06:43:54Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/21584," can you have a look, please?",Can you try running your computation under `with mesh:` context manager? That should fix this.,"Thanks, that worked!",Can we keep that open as 1. It's not documented so I ran into the exact same problem 2. It's not a desirable long term solution (?)
yi,"str(PyTreeDef) identical for two PyTreeDefs, but assert with allclose fails"," Description I'm trying to understand a bug  in optimistix, and came across this behaviour that is unintuitive to me:   The PyTreeDef of two jacobians computed using `jax.linearize` is not the same, but the strings of the PyTreeDefs are the same.  Since the tree structure is what I want to examine for debugging purposes, this should not happen. Edit: fixed typo in comment. Here is an MWE that demonstrates the error: ```python import pytest import jax import jax.numpy as jnp import jax.tree_util as jtu import equinox as eqx def _no_nan(x):     """"""Compied from test/helpers.py in diffrax.""""""     if eqx.is_array(x):         return x.at[jnp.isnan(x)].set(8.9568)   arbitrary magic value     else:         return x def tree_allclose(x, y, *, rtol=1e5, atol=1e8, equal_nan=False):     """"""Copied from test/helpers.py in diffrax.""""""     if equal_nan:         x = jtu.tree_map(_no_nan, x)         y = jtu.tree_map(_no_nan, y)     return eqx.tree_equal(x, y, typematch=True, rtol=rtol, atol=atol) def fn(y):     def shifted_parabola(x0):         x = jnp.linspace(0, 10)         return (x  x0)**2     true = shifted_parabola(2.)   True value     fit = shifted_parabola(y)     return true  fit y0 = 1.   starting guess _, jac_of_fn = jax.linearize(fn, y0) _, jac_of_fn_with_lambda = jax.linearize(lambda _y: fn(_y), y0) assert tree_allclose(jax.jacfwd(fn)(y0), jac_of_fn(y0))  assert tree_allclose(jax.jacfwd(fn)(y0), jac_of_fn_with_lambda(y0))  Using lambda functions produces subtle difference in pytrees, not legible to human with pytest.raises(AssertionError):     assert jtu.tree_structure(jac_of_fn) == jtu.tree_structure(jac_of_fn_with_lambda) assert str(jtu.tree_structure(jac_of_fn)) == str(jtu.tree_structure(jac_of_fn_with_lambda)) ```  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.28 jaxlib: 0.4.28 numpy:  1.26.4 python: 3.11.8 (v3.11.8:db85d51d3e, Feb  6 2024, 18:02:37) [Clang 13.0.0 (clang1300.0.29.30)] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Darwin', node='bsmbpas0019', release='23.3.0', version='Darwin Kernel Version 23.3.0: Wed Dec 20 21:33:31 PST 2023; root:xnu10002.81.5~7/RELEASE_ARM64_T8112', machine='arm64')",2024-06-01T21:52:23Z,bug,closed,0,11,https://github.com/jax-ml/jax/issues/21581,"It looks like this is happening because we are using `functools.partial` internally, and `partial` objects do not implement `__eq__`: ```python >>> import functools >>> def f(): ... ... >>> functools.partial(f) == functools.partial(f) False ``` On top of that, one of the arguments is a `Jaxpr` which too does not implement `__eq__` and is thus comparable by reference.","I removed some unnecessary definitions/calls from the repro: ```python import jax import jax.numpy as jnp import jax.tree_util as jtu def fn(y):     def shifted_parabola(x0):         x = jnp.linspace(0, 10)         return (x  x0)**2     true = shifted_parabola(2.)   True value     fit = shifted_parabola(y)     return true  fit y0 = 1.   starting guess _, jac_of_fn = jax.linearize(fn, y0) _, jac_of_fn_with_lambda = jax.linearize(lambda _y: fn(_y), y0) assert jtu.tree_structure(jac_of_fn) == jtu.tree_structure(jac_of_fn_with_lambda) ```","You can simplify the repro even further: ```python import jax def fn(y):   return y _, jac_1 = jax.linearize(fn, 1.0) _, jac_2 = jax.linearize(fn, 1.0) assert jax.tree.structure(jac_1) == jax.tree.structure(jac_2) ``` This makes clear that the problem has nothing to do with the function being passed to `linearize`.","> You can simplify the repro even further: > This makes clear that the problem has nothing to do with the function being passed to `linearize`. Fascinating! And then I really am out of leads, I was sure it must have something to do with the function passed 😃 ","FWIW I think this behaviour is acceptable in JAX. A jaxpr is just JAX's internal representation of a function, and Python itself does the same thing: ```python def some_fun():     pass def higher_order_fun(fun):     def inner_fun():         return fun()     return inner_fun assert higher_order_fun(some_fun) == higher_order_fun(some_fun)   fails ``` Moreover I get a bit antsy about the idea of comparing jaxprs for equality. It's very common for jaxprs to become truly gigantic, and iterating over them to test for equality seems like a compiletime footgun.","Good point.  I went in this direction because I thought it might be at the root of why we can't batch in `GaussNewton`, but I'm not sure that is the case anymore, since I can batch over the output of `jax.linearize`.   Why does the difference now show up in the string of a PyTreeDef though? Those are equal. ",> Why does the difference now show up in the string of a PyTreeDef though? Those are equal. I think this is because `PyTreeDef.__str__` does not include the auxiliary data stored in the nodes.,What would that auxiliary data contain? Can I conclude that errors such as the one above do not come from the pytree definition if the strings are the same? I currently only use these as debugging tools.,"> What would that auxiliary data contain? It depends on the type and its flattening rule. > Can I conclude that errors such as the one above do not come from the pytree definition if the strings are the same? Sorry, I'm not sure I follow. Which errors are you referring to?","Thanks for the discussion, all! To summarize: equality of string prettyprints does not imply object equality (for treedefs, and likely for lots of Python classes).","> Thanks for the discussion, all! >  > To summarize: equality of string prettyprints does not imply object equality (for treedefs, and likely for lots of Python classes). This was the question I had :) "
yi,[export] Fix handling of a function with no name,"A user reported an error when trying to export a function that has a ""lower"" attribute (to impersonate a jitted function) but does not have a ""__name__"" attribute. The solution is to use the default name """". While I was at it I have added a `util.fun_name` to get the name of a Callable, and I use it in several places.",2024-06-01T02:56:21Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/21572,This has been submitted already as https://github.com/google/jax/commit/be1e40dc2e1777d83b870afa31e178123f2a1366 (due to a copybara glitch)
yi,pure_callback is not passing `np.ndarray` typed inputs," Description I updated JAX and am getting inputs to callback functions coming in as `jax.ArrayImpl` and not `np.ndarray`. ```python import jax import jax.numpy as jnp import numpy as np def fn(x: jax.Array) > jax.Array:      Define the expected shape & dtype of output.     result_shape_dtype = jax.ShapeDtypeStruct(         shape=np.shape(x),         dtype=x.dtype     )     return jax.pure_callback(_host_fn, result_shape_dtype, x) def _host_fn(x: np.ndarray):     print('x', type(x))     assert isinstance(x, np.ndarray)     return x if __name__ == '__main__':     fn(jnp.asarray([1.])) ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax==0.4.28 jaxlib==0.4.28 ```",2024-05-30T19:25:34Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/21526,"Yes, that was a deliberate change we made. Is there a problem or an issue? The current description does not seem to specify the issue.  ","The documentation says it should be `np.ndarray`. I noticed when it crashing all my C bindings which don't understand `ArrayImpl`. Easy fix though. I personally don't care, as long as CC(Provide output buffer for pure_callback result) gets implemented :)",Which documentation? I couldn't find any mention of it here: https://jax.readthedocs.io/en/latest/_autosummary/jax.pure_callback.html,"Where do the docs say `np.ndarray`? I couldn't track that down. The change was listed in the changelog here, and the current docs say ""JAX arrays"" AFAICT.",The source code: !Screenshot from 20240530 213500,"Maybe you are not looking at the main branch (https://github.com/google/jax/blob/51e743139b2bbe5146e257610eca31a05f277f83/jax/_src/callback.pyL272)? But anyways, this is WAI.  I am going to close the issue. Please reopen if this is causing major problems for you :)","I think the documentation update was done after we cut the release, unfortunately. So, the sourcelevel documentation is indeed misleading. Sorry about that."
rag,[mosaic:gpu] Minor cleanup in `FragmentedArray.transfer_tile`.,[mosaic:gpu] Minor cleanup in `FragmentedArray.transfer_tile`.  Remove redundant line.  Use `ConstantOp.create_index`.  Use `BoolAttr`.,2024-05-30T11:41:42Z,,closed,0,0,https://github.com/jax-ml/jax/issues/21510
yi,logging_test: avoid modifying global config state,Related to https://github.com/google/jax/pull/21489,2024-05-29T20:39:50Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/21497
yi,Cryptic error message when running pallas kernel on TPU ," Description I have a pallas kernel that's dying with a cryptic CHECK failure:  ``` Check failed: 4 <= bitwidth (4 vs. signed char value 1) ``` I don't have a useful stack trace here it's just the standard Google3 CHECK failure stuff that's pretty useless (see !screenshot). It happens in my call to pallas_call on a v5p8. It traces my kernel fine but then dies before returning from pallas_call, which I assume means this is some sort of compilation error? It happens when interpret is set to both true and false. The code runs without issue on CPU. The error does not come up when I remove the `custom_vjp` decorator and only run the forward pass. I have a repro on gist.  System info (python version, jaxlib version, accelerator, etc.) System info for CPU env (where the code succeeds): ``` jax:    0.4.26 jaxlib: 0.4.26 numpy:  1.26.4 python: 3.10.11 (main, May 23 2023, 13:58:30) [GCC 10.2.1 20210110] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='62b391e8e285', release='5.10.029cloudamd64', version=' CC(Python 3 compatibility issues) SMP Debian 5.10.2161 (20240503)', machine='x86_64') ``` System info for TPU env (where the code fails): ``` jax: 0.4.26 jaxlib: 0.4.26 numpy: 1.23.5 python: 3.10.11 (main, May 23 2023, 13:58:30) [GCC 10.2.1 20210110] jax.devices (4 total, 4 local): [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0) TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0) TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0) TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)] process_count: 1 platform: uname_result(system='Linux', node='gketpu306bac09cmh9', release='6.1.75+', version=' CC(Python 3 compatibility issues) SMP PREEMPT_DYNAMIC Sat Mar 30 14:38:17 UTC 2024', machine='x86_64') ```",2024-05-29T17:47:09Z,bug pallas,closed,0,7,https://github.com/jax-ml/jax/issues/21488,"My guess is that it's the `mask` operand that is causing the problem. If I understand correctly, it's an array of `jnp.bool`, but this type should not be supported at the kernel boundary. I think the best workaround would be to cast the mask to int8/int32 and then do `mask != 0` inside the kernel to recover it in the boolean form. Ultimately there are two things to fix here: (1) make Pallas more picky about input operand types and (2) add support for passing booleans to kernels.","Ah, yes, that fixed it. Thanks! Is it possible to have this be checked on CPU as well? I find it really confusing when there's such different behaviour between CPU and TPU. On Thu, May 30 2024 at 02:24, Adam Paszke ***@***.***> wrote: > My guess is that it's the mask operand that is causing the problem. If I > understand correctly, it's an array of jnp.bool, but this type should not > be supported at the kernel boundary. I think the best workaround would be > to cast the mask to int8/int32 and then do mask != 0 inside the kernel to > recover it in the boolean form. > > Ultimately there are two things to fix here: (1) make Pallas more picky > about input operand types and (2) add support for passing booleans to > kernels. > > — > Reply to this email directly, view it on GitHub > , or > unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >","I think on CPU we only simulate the custom call, which you can get on a TPU if you pass in `interpret=True`. But note that then you won't be actually generating a kernel but it will expand to a soup of loopy HLOs. Either way, we should fix bool support.","Ah, ty, I misunderstood and thought that the error was still happening with `interpret=True`, you're right, it doesn't. On Fri, May 31, 2024 at 3:15 AM, Adam Paszke ***@***.***> wrote: > I think on CPU we only simulate the custom call, which you can get on a > TPU if you pass in interpret=True. But note that then you won't be > actually generating a kernel but it will expand to a soup of loopy HLOs. > Either way, we should fix bool support. > > — > Reply to this email directly, view it on GitHub > , or > unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >",I'm currently working on a fix to cast bools memrefs to ints automatically so users don't have to worry about this detail.,Circling back on this: I added an updated so that this is supported for scalars (https://github.com/google/jax/pull/22464). However it seems like the vector case is a bit more tricky (due to vector layouts in Mosaic) and will take some more time.,Vector boolean loads/stores are now implemented: https://github.com/google/jax/pull/23044.
yi,Can´t get jax profiling to work," Description I'm trying to follow the documentation on profiling and am stuck when trying to evaluate the traces with tensorboard, running the following code:  ```python  import jax  jax.profiler.start_trace(""/tmp/tensorboard"")  Run the operations to be profiled key = jax.random.key(0) x = jax.random.normal(key, (5000, 5000)) y = x @ x y.block_until_ready() jax.profiler.stop_trace() ``` which is shown in the docs.  Now, trying to view the trace with tensorboard:  ```bash  tensorboard logdir /tmp/tensorboard/ ``` I always get the warning `No step marker observed and hence the step time is unknown. This may happen if (1) training steps are not instrumented (e.g., if you are not using Keras) or (2) the profiling duration is shorter than the step time. For (1), you need to add step instrumentation; for (2), you may try to profile longer.` No profiling data is available or shown. ` The code itself produces only the following warning:  ```bash  20240529 14:37:00.028767: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT ``` which I am not sure is relevant here.  What am I doing wrong? How could I go about troubleshooting this? thanks in advance   System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.28 jaxlib: 0.4.28 numpy:  1.26.4 python: 3.12.3 (main, Apr 27 2024, 19:00:21) [GCC 11.4.0] jax.devices (1 total, 1 local): [cuda(id=0)] process_count: 1 platform: uname_result(system='Linux', node='ssc10', release='6.8.10zabbly+', version='ubuntu22.04 SMP PREEMPT_DYNAMIC Sat May 18 13:41:36 UTC 2024', machine='x86_64') $ nvidiasmi Wed May 29 14:42:02 2024        ++  ++ ``` **accelerator:** GPU  EDIT: correct highlighting of error messages",2024-05-29T12:42:25Z,bug,open,3,5,https://github.com/jax-ml/jax/issues/21483,Do you see meaningful information under the trace viewer tool? I'd start there rather than the overview tool.,"Thank you for your answer. I do get output from the trace viewer, but I'm still confused by the warning message, since it seems to indicate that profiling didn´t work. Is there any documentation on this? is it intentional or unavoidable? ","Hello! I have met the same problem while using tensorboard profiling, have you resolved this issue?","what I ultimately did was using nvidia insight directly, which at least gives you some indication of what's going on. See here:  https://github.com/NVIDIA/JAXToolbox/blob/main/docs/profiling.md",The overview tool in tensorboard isn't really designed to work well with JAX. I'd recommend mostly ignoring it at the moment and looking at the trace viewer.
yi,Unexpected Assertion error when using lax.with_sharding_constraint," Description Hi, I was facing an error when trying to use data paralellism with equinox and opened an issue there: equinox issue CC(np.trace is broken). Turns out the issue is not related with equinox but with `lax.with_sharding_constraint` so I am opening an issue here. Here is the MWE: ``` import jax from jax.experimental import mesh_utils import jax.sharding as jshard num_devices = len(jax.devices()) print(f""Num GPUs: {num_devices}"") devices = mesh_utils.create_device_mesh((1,num_devices)) sharding = jshard.PositionalSharding(devices) replicated = sharding.replicate() x = jax.random.uniform(jax.random.PRNGKey(42), (256, 20, 1000)) x = jax.lax.with_sharding_constraint(x, sharding) ``` And here is the error being thrown: ``` Traceback (most recent call last):   File ""/home/ids/afreitas/test/mwe2.py"", line 13, in      x = jax.lax.with_sharding_constraint(x, sharding)   File ""/home/ids/afreitas/jax/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 2179, in with_sharding_constraint     outs = [sharding_constraint_p.bind(xf, sharding=s,   File ""/home/ids/afreitas/jax/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 2179, in      outs = [sharding_constraint_p.bind(xf, sharding=s,   File ""/home/ids/afreitas/jax/lib/python3.10/sitepackages/jax/_src/core.py"", line 422, in bind     return self.bind_with_trace(find_top_trace(args), args, params)   File ""/home/ids/afreitas/jax/lib/python3.10/sitepackages/jax/_src/core.py"", line 425, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/home/ids/afreitas/jax/lib/python3.10/sitepackages/jax/_src/core.py"", line 913, in process_primitive     return primitive.impl(*tracers, **params)   File ""/home/ids/afreitas/jax/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 2191, in _sharding_constraint_impl     return api.jit(_identity_fn, out_shardings=sharding)(x)   File ""/home/ids/afreitas/jax/lib/python3.10/sitepackages/jax/_src/sharding_impls.py"", line 122, in shard_shape     return _common_shard_shape(self, global_shape)   File ""/home/ids/afreitas/jax/lib/python3.10/sitepackages/jax/_src/sharding_impls.py"", line 68, in _common_shard_shape     assert len(partitions) == len(global_shape), (len(partitions), len(global_shape)) AssertionError: (1, 3) ```  System info (python version, jaxlib version, accelerator, etc.) ``` python                   3.10 jax                      0.4.26 ```",2024-05-29T09:29:15Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/21480,"You need to reshape your sharding to match the length of the shape's ndim: ``` In [7]: import jax    ...: from jax.experimental import mesh_utils    ...: import jax.sharding as jshard    ...:    ...: num_devices = len(jax.devices())    ...: print(f""Num GPUs: {num_devices}"")    ...: devices = mesh_utils.create_device_mesh((1, num_devices))    ...: sharding = jshard.PositionalSharding(devices).reshape(1, 1, 8)    ...: replicated = sharding.replicate()    ...:    ...: x = jax.random.uniform(jax.random.PRNGKey(42), (256, 20, 1000))    ...:    ...: x = jax.lax.with_sharding_constraint(x, sharding) ``` This might be a might inconvenient though, so I would suggest using NamedSharding instead: ``` In [10]: import jax     ...: from jax.experimental import mesh_utils     ...: import jax.sharding as jshard     ...: from jax.sharding import PartitionSpec as P     ...:     ...: num_devices = len(jax.devices())     ...: print(f""Num GPUs: {num_devices}"")     ...: devices = mesh_utils.create_device_mesh((1, num_devices))     ...: mesh = jax.sharding.Mesh(devices, ('x', 'y'))     ...: sharding = jshard.NamedSharding(mesh, P(None, None, 'x'))     ...:     ...: x = jax.random.uniform(jax.random.PRNGKey(42), (256, 20, 1000))     ...:     ...: x = jax.lax.with_sharding_constraint(x, sharding) Num GPUs: 8 ```  If you want to shard only the first dimension then your PartitionSpec can be `P('x')`.",We should probably raise a better error message when this happens. Wdyt ? ,"Sure, feel free to send a CL :) You would need to implement `is_compatible_aval` method on PositionalSharding to raise a better error message.",Thanks  !  Indeed a better error message would be useful :)
rag,"Remove warning logs for primary_host/remote_storage ""incompatibility"".","Remove warning logs for primary_host/remote_storage ""incompatibility"".",2024-05-28T20:49:54Z,,closed,0,0,https://github.com/jax-ml/jax/issues/21470
yi,example path," Description I am installing jax and running a simple example for the first time.  I am on and have installed the nightly JAX via pip3. (This error occurs if I also install stable jax via pip3.) I am running an uptodate Ubuntu 22.04.4 LTS using cuda12. ``` python3 ./examples/mnist_classifier_fromscratch.py Traceback (most recent call last):   File ""~/jax/./examples/mnist_classifier_fromscratch.py"", line 28, in      from examples import datasets ModuleNotFoundError: No module named 'examples' ``` Modifying the example appears to resolve the issue: ``` import os import sys sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))) from examples import datasets ``` The jax git repo I am working with is a fresh clone from this morning: ``` commit ff3db9b3a1bd19154769c2f72cf88593a52d8eb5 (HEAD > main, origin/main, origin/HEAD) Date:   Tue May 28 07:59:31 2024 0700 ```  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.29.dev20240528+ff3db9b3a jaxlib: 0.4.29.dev20240528 numpy:  1.26.4 python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] jax.devices (1 total, 1 local): [cuda(id=0)] process_count: 1 platform: uname_result(system='Linux', node='c', release='6.5.035generic', version=' CC(CUDA90 and py3 )~22.04.1Ubuntu SMP PREEMPT_DYNAMIC Tue May  7 09:00:52 UTC 2', machine='x86_64') $ nvidiasmi Tue May 28 09:08:46 2024        ++  ++",2024-05-28T16:09:29Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/21459,"This has to do with the relative position of the two files.  As they are right now `datasets.py` and `mnist_classifier_fromscratch.py` are under the same folder. I might be missing something here but if you move the `mnist_classifier_fromscratch.py` outside of the `examples` folder, it works as intended. If you want to keep the two files in the same folder then in `mnist_classifier_fromscratch.py` change; ```python from examples import datasets ``` to ```python from datasets ```  Something is telling me that I am missing something here since all examples that use the `datasets.py` are importing it with the first way and the author was able to run them just fine as they are."
yi,[export] Fix calling under pmap of exported computation with polymorphic shapes,If we call a computation with shape polymorphism under pmap we must refine the shapes before we compile. We follow the same pattern for `UnloadedPmapExecutable` as for `UnloadedMeshExecutable`: we store the `shape_poly_state` from the `LoweringResult` into the `compile_args` and we call `refine_polymorphic_shapes`. Without this fix we may end up trying to compile HLO with dynamic shapes.,2024-05-28T13:19:31Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/21451
yi,D2H (gpu -> cpu) transfer via `device_put` is very  slow," Description For D2H (gpu to cpu) transfer, `jax.device_put` has very low throughput. `device_put` yields ~2.7GB/s transfer speed; in contrast, a very simple CUDA program yields ~25GB/s. Is there an alternative approach in Jax here that I'm missing?  I tried the following two approaches as well, both performed at least as poorly as `jax.device_put`: * Calling XLA directly (via `xc.batched_device_put` as detailed in https://github.com/google/jax/issues/16905issue1829138543) * Calling a dummy JIT compiled function for CPU that returns identity Minimal Jax example: ```python import time import jax import jax.numpy as jnp cpu_dev = jax.devices('cpu')[0] GBs = 10 def big_tensor():     return jnp.ones((GBs * 1024**3 // 4,), dtype=jnp.float32, device=jax.devices('gpu')[0]) def test_transfer(x):     jax.block_until_ready(x)     s = time.time()     out = jax.device_put(x, cpu_dev)     jax.block_until_ready(out)     dur = time.time()  s     print(f""Time taken: {dur}; gbps: {GBs/dur}"") for i in range(5):     test_transfer(big_tensor()) ``` with output ``` Time taken: 3.7397873401641846; gbps: 2.6739488346310565 Time taken: 3.6008787155151367; gbps: 2.7770999220031807 Time taken: 3.677137613296509; gbps: 2.7195065976970936 Time taken: 3.594850778579712; gbps: 2.781756633567665 Time taken: 3.5868709087371826; gbps: 2.787945330187717 ``` And here is a simple CUDA program for copying: ```c include  include  include  const uint64_t num_bytes = 10ul * (1ul (end  start).count() / 1000.0);     cudaFreeHost(host_a);     cudaFree(device_a);     cudaStreamDestroy(stream0);     return 0; } ``` With output: ``` Time: 0.401000 ```  System info (python version, jaxlib version, accelerator, etc.) ```jax:    0.4.28 jaxlib: 0.4.28 numpy:  1.26.4 python: 3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0] jax.devices (8 total, 8 local): [cuda(id=0) cuda(id=1) ... cuda(id=6) cuda(id=7)] process_count: 1 platform: uname_result(system='Linux', node='deeph3.csail.mit.edu', release='5.15.0107generic', version=' CC(add oss test instructions, fix conv grad bug)Ubuntu SMP Fri Apr 26 12:26:49 UTC 2024', machine='x86_64') $ nvidiasmi Sun May 26 22:16:33 2024 ++  ```",2024-05-27T02:19:18Z,bug NVIDIA GPU,open,0,7,https://github.com/jax-ml/jax/issues/21438,"I suspect that the difference is that the destination of your transfer is in CUDA pinned host memory (`cudaMallocHost`), to which you can DMA directly from the device. JAX is transferring to unpinned memory. If you allocate the target buffer with `malloc` in your CUDA benchmark, how do the two compare? (We are actually working on adding support for pinned host memory allocations to JAX.)","Thanks for the quick response! With `malloc` I get 6.3 GB/s throughput vs 2.7 GB/s in Jax.  Even if there is not official support, is there an easy hack to get Jax to allocate CUDA pinned memory? This problem is very important in my application + I'm only using my CPU as a staging area for GPU operations, so I am happy to have Jax only use CUDA pinned memory.","Right now, you can't hack to get pinned_host working. The implementation is missing. We are working on it.","I might be able to get you the 6.3GB/s without much trouble, though, if that's helpful.","Another workaround for the moment would be to use DLPack to exchange the onGPU array with another library that already supports pinned host memory (e.g., cupy) and use that library to do the transfer.",Thank you for the suggestions  bridging to cupy worked!,"Any progress on the missing pinned host implementation? Is the following related: ""TODO(b/238441608): Use pinned memory here to speed up the transfer."" from py_client_gpu.cc? I see some code under gpu_transfer_manager referring to ""Check out pinned memory for each buffer we want to copy"" under GpuTransferManager::ReadDynamicShapes. Do you have a design about H2D D2H D2D memcpy ? Thank you."
yi,Low calculation performance compared to autograd elementwise_grad," Description I use autograd to calculate partial derivatives of functions of two variables (x, y). Due to the end of support for autograd, I'm trying to get the same results using jax. These functions have the form: $$\nabla^4 w = \cfrac{\partial^4 w}{\partial x^4} + 2\cfrac{\partial^4 w}{\partial x^2\partial y^2} + \cfrac{\partial^4 w}{\partial y^4}$$ where $w = w(x,y)$. I use similar functions obtained by automatic differentiation in other parts of the program as wrappers, and then to obtain the final results I substitute the values ​​of the NumPy arrays. I haven't found a way to port this type of twovariable functions from autograd to jax with similar performance.  Examples:  autograd (ex1.py) ```python import numpy as np from autograd import elementwise_grad as egrad dx, dy = 0, 1 def nabla4(w):     def fn(x, y):         return (             egrad(egrad(egrad(egrad(w, dx), dx), dx), dx)(x, y)             + 2 * egrad(egrad(egrad(egrad(w, dx), dx), dy), dy)(x, y)             + egrad(egrad(egrad(egrad(w, dy), dy), dy), dy)(x, y)         )     return fn def f(x, y):     return x**4 + 2 * x**2 * y**2 + y**4 x = np.arange(10_000, dtype=np.float64) y = np.arange(10_000, dtype=np.float64) w = [f] * 100   In a real program, the elements of the list are various functions. r = [nabla4(f)(x, y) for f in w] ``` ``` (idp) PS C:\Users\kryst\Projects\example> MeasureCommand { python ex1.py } Days              : 0 Hours             : 0 Minutes           : 0 Seconds           : 0 Milliseconds      : 813 Ticks             : 8130392 TotalDays         : 9,41017592592593E06 TotalHours        : 0,000225844222222222 TotalMinutes      : 0,0135506533333333 TotalSeconds      : 0,8130392 TotalMilliseconds : 813,0392 ```  jax (ex2.py) ```python import jax from jax import grad, vmap import jax.numpy as jnp jax.config.update(""jax_enable_x64"", True) dx, dy = 0, 1 def nabla4(w):     def fn(x, y):         return (             vmap(grad(grad(grad(grad(w, dx), dx), dx), dx))(x, y)             + 2 * vmap(grad(grad(grad(grad(w, dx), dx), dy), dy))(x, y)             + vmap(grad(grad(grad(grad(w, dy), dy), dy), dy))(x, y)         )     return fn def f(x, y):     return x**4 + 2 * x**2 * y**2 + y**4 x = jnp.arange(10_000, dtype=jnp.float64) y = jnp.arange(10_000, dtype=jnp.float64) w = [f] * 100   In a real program, the elements of the list are various functions. r = [nabla4(f)(x, y) for f in w] ``` ``` (idp) PS C:\Users\kryst\Projects\example> MeasureCommand { python ex2.py } Days              : 0 Hours             : 0 Minutes           : 0 Seconds           : 6 Milliseconds      : 906 Ticks             : 69064939 TotalDays         : 7,99362719907407E05 TotalHours        : 0,00191847052777778 TotalMinutes      : 0,115108231666667 TotalSeconds      : 6,9064939 TotalMilliseconds : 6906,4939 ``` The program using jax is almost 9x slower than the version using autograd. In more complicated programs the differences are much greater.  System info (python version, jaxlib version, accelerator, etc.) ```jax:    0.4.30 jaxlib: 0.4.30 numpy:  1.26.4 python: 3.10.13  (tags/v3.10.1325g07fbd8e9251dirty:07fbd8e9251, Dec 28 2023, 15:38:17) [MSC v.1929 64 bit (AMD64)] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Windows', node='Vero', release='10', version='10.0.22631', machine='AMD64') ```",2024-05-26T10:04:37Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/21436,(answered in CC(未找到相关数据))
yi,`unsafe_rbg` fold in function is insensitive to the order data is folded in.," Description The `unsafe_rbg` fold_in function is not sensitive to the order in which data is folded in. The underlying cause is that `unsafe_rbg` derives its key based on `key ^ rbg(data)`, but XOR is a commutative operation. So if a user folds in two values the order will not change the result since `key ^ rbg(1) ^ rbg(2) == key ^ rbg(2) ^ rbg(1)`. Reproducing example: ```python for impl in ['rbg', 'unsafe_rbg']:   print('impl:', impl)   key = jax.random.key(42, impl=impl)   print('Original key:', jax.random.key_data(key))   key_1 = jax.random.fold_in(key, 1)   key_12 = jax.random.fold_in(key_1, 2)   print('Foldin 1 > 2:', jax.random.key_data(key_12))   key_2 = jax.random.fold_in(key, 2)   key_21 = jax.random.fold_in(key_2, 1)   print('Foldin 2 > 1:', jax.random.key_data(key_21)) ``` Result: ``` impl: rbg Original key: [ 0 42  0 42] Foldin 1 > 2: [257214496 567757975 257214496 567757975] Foldin 2 > 1: [2853785955  313133857 2853785955  313133857] impl: unsafe_rbg Original key: [ 0 42  0 42] Foldin 1 > 2: [2393909057 2743418786 1382566513 2711092147] Foldin 2 > 1: [2393909057 2743418786 1382566513 2711092147] ``` Notice in the `unsafe_rbg` case the two derived keys are the same. Note that `rbg` uses the key derivation logic from threefry which does not have this issue.  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.29 jaxlib: 0.4.29 numpy:  1.26.3 python: 3.11.8 (stable, redacted, redacted) [Clang google3trunk (fc57f88f007497a4ead0ec8607ac66e1847b02d6)] jax.devices (1 total, 1 local): [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)] process_count: 1 platform: uname_result(system='Linux', node='8f069381872f0396766a8253c8.borgtask.google.com', release='5.10.0smp1101.34.0.0', version=' CC(Python 3 compatibility issues) [v5.10.01101.34.0.0] SMP ', machine='x86_64')",2024-05-23T20:44:59Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/21405,This is not an urgent issue to fix because it will change the reproducibility of bits produced by the PRNG. Just marking this as a fix we should bundle whenever we make a large breaking change to `unsafe_rbg` that would modify the bits anyways.
rag,[Mosaic GPU] Add some activation functions for fragmented array (towards adding matmul epilogues),[Mosaic GPU] Add some activation functions for fragmented array (towards adding matmul epilogues),2024-05-23T16:35:31Z,,closed,0,1,https://github.com/jax-ml/jax/issues/21401,Closing Copybara created PR due to inactivity
yi,Improve error message when trying to fetch value of non-addressable array.,Improve error message when trying to fetch value of nonaddressable array.,2024-05-23T15:47:31Z,,closed,0,0,https://github.com/jax-ml/jax/issues/21399
yi,jax-metal: top-k invalid behaviour with NaNs," Description ```python import jax import jax.numpy as jnp def f(x):   return jax.lax.top_k(x, k=3) x = jnp.array([""nan"", ""inf"", ""inf"", 0.0], dtype=jnp.float32)  Print lowered HLO print(jax.jit(f).lower(x).as_text()) print(jax.jit(f)(x)) ``` HLO ``` module  attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {   func.func public (%arg0: tensor {mhlo.layout_mode = ""default""}) > (tensor {jax.result_info = ""[0]"", mhlo.layout_mode = ""default""}, tensor {jax.result_info = ""[1]"", mhlo.layout_mode = ""default""}) {     %values, %indices = chlo.top_k(%arg0, k = 3) : tensor > (tensor, tensor)     return %values, %indices : tensor, tensor   } } ```  Returns: ```  CPU [Array([nan, inf,  0.], dtype=float32), Array([0, 2, 3], dtype=int32)]  Metal [Array([inf,  0.,  0.], dtype=float32), Array([2, 3, 0], dtype=int32)] ``` See that using jaxmetal the NaN is not leading, and interestingly it is replaced with 0 in the output (according to the returned indices).  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.26 jaxlib: 0.4.26 numpy:  1.26.4 python: 3.10.8 (main, Nov 16 2022, 12:45:33) [Clang 14.0.0 (clang1400.0.29.202)] jax.devices (1 total, 1 local): [METAL(id=0)] process_count: 1 platform: uname_result(system='Darwin', node='chonker', release='23.5.0', version='Darwin Kernel Version 23.5.0: Wed May  1 20:12:58 PDT 2024; root:xnu10063.121.3~5/RELEASE_ARM64_T6000', machine='arm64') ``` jaxmetal 0.0.7",2024-05-23T14:03:16Z,bug Apple GPU (Metal) plugin,open,0,0,https://github.com/jax-ml/jax/issues/21397
yi,Implement LRU cache eviction for persistent compilation cache,"This PR is part of the implementation of LRU cache eviction using the `mtime` attribute provided by the filesystem. The current PR does not support GCS, but this problem will be solved in a subsequent PR. More details in the design doc: https://docs.google.com/document/d/111YibwGXOFb_hMmlua1u63QooAzIBEHxfRPGmibis/edit?usp=sharing",2024-05-23T12:18:33Z,pull ready,closed,1,10,https://github.com/jax-ml/jax/issues/21394,"> Note, I saw some NFS server being configured to not update mtime to speed up the server. Maybe document that this can happen and in that case, this will revert to creation time? The first time I saw the behavior without knowing the reason, it took times to understand what was going on. Yes, I suspect there's a chance you might see stale `mtime` values if you stick the cache on NFS and you're accessing it concurrently from multiple clients (see `lookupcache` in the NFS docs). I'm not sure there's a lot we can do about that, though.","This is a first cut at the LRU eviction implementation, so it isn't expected to work well with network file systems yet (notably GCS, which many Cloud TPU users use for their cache storage). We'll iterate from here. I don't think we should publicly document this until it works well across filesystems, but absolutely agree this should eventually be in https://jax.readthedocs.io/en/latest/persistent_compilation_cache.html.","Test fails because the test utilises `filelock`, which is not installed. ``` =================================== FAILURES =================================== _______________________ LRUCacheTest.test_cache_eviction _______________________ [gw5] linux  Python 3.12.3 /opt/hostedtoolcache/Python/3.12.3/x64/bin/python tests/lru_cache_test.py:39: in test_cache_eviction     cache = Impl(path, max_size=884700) jax/_src/lru_cache.py:57: in __init__     raise RuntimeError(""Please install filelock package to set `jax_compilation_cache_max_size`"") E   RuntimeError: Please install filelock package to set `jax_compilation_cache_max_size` ```",Add filelock to build/testrequirements.txt and to the deps in tests/BUILD. Or skip the test for now if filelock is not importable.,"Just realised that JAX had a `FileSystemCache` that supports LRU cache eviction introduced in https://github.com/google/jax/pull/6869, but was subsequently removed in https://github.com/google/jax/pull/10771 to support GCS. This is exactly one of the challenges that I faced in this PR. Fortunately, I've devised potential solutions to simultaneously support LRU cache eviction and GCS compatibility. This is going to be completely solved in a subsequent PR.",All comments resolved,All review comments done,Please fix the failing lint_and_typecheck (looks like you need to fix a type annotation?),"Thank you for preparing this. Please squash the long chain of commits, or at least most of them.",> Can you link to the design doc?  I've just added the link to the first comment.
yi,[Mosaic GPU] Change row-warp assignment logic in matmul example epilogue.,"[Mosaic GPU] Change rowwarp assignment logic in matmul example epilogue. Previously we were assigning rows in a roundrobin fashion. Now, contiguous rows are assigned to the same warp for up to ```   vector_len * lanes_per_warp / min(n_out_tiling) = 4 * 32 / 32 = 4 rows. ``` This could theoretically help with small tile sizes, but in practice it doesn't seem to make a difference. Benchmarking with parameters `lhs_dtype=jnp.float32`, `rhs_dtype=jnp.float32`, `tile_m=128`, `rhs_transpose=True`, `stages=2`, and varying values for `tile_n`, gives us the following results. Before: ``` tile_n=32: 94.9 us = 93.4 TFLOPS tile_n=64: 74.2 us = 119.4 TFLOPS tile_n=128: 73.1 us = 121.3 TFLOPS ``` After: ``` tile_n=32: 96.1 us = 92.2 TFLOPS tile_n=64: 71.9 us = 123.1 TFLOPS tile_n=128: 73.1 us = 121.1 TFLOPS ```",2024-05-23T09:05:22Z,,closed,0,0,https://github.com/jax-ml/jax/issues/21385
yi,Simple primitive definition to manage speed/memory tradeoff,"I am running into an issue with JAX where after successive vmaps one gigantic matrix is created (or at least preallocated). In my case I am computing kernel matrices: ```python import jax import jax.numpy as np define the variables key=jax.random.PRNGKey(0) key,*subkeys=jax.random.split(key,4) X=jax.random.normal(subkeys[0],(1000,30)) active=jax.random.randint(subkeys[1],(30,30),0,2) y=jax.random.normal(subkeys[2],(30,1000)) def k1(x,y,which):   return np.prod(1+which*np.exp((xy)**2)) K1=jax.vmap(k1,in_axes=(0,None,None)) K1=jax.vmap(K1,in_axes=(None,0,None)) assert K1(X,X,active[0]).shape==(1000,1000) def activation(X, which_dim, yb):     mat = K1(X, X, which_dim)     return np.dot(yb, mat @ yb) activation_vmap = jax.vmap(activation, in_axes=(None, 0, 0)) assert activation_vmap(X,active,y).shape==(30,) print(jax.make_jaxpr(activation_vmap)(X,active,y)) ``` which prints ``` { lambda ; a:f64[1000,30] b:i64[30,30] c:f64[30,1000]. let     d:f64[1000,1,30] = broadcast_in_dim[       broadcast_dimensions=(0, 2)       shape=(1000, 1, 30)     ] a     e:f64[1,1000,30] = broadcast_in_dim[       broadcast_dimensions=(1, 2)       shape=(1, 1000, 30)     ] a     f:f64[1000,1000,30] = sub e d     g:f64[1000,1000,30] = integer_pow[y=2] f     h:f64[1000,1000,30] = neg g     i:f64[1000,1000,30] = exp h     j:f64[30,30] = convert_element_type[new_dtype=float64 weak_type=False] b     k:f64[30,1,30] = broadcast_in_dim[       broadcast_dimensions=(0, 2)       shape=(30, 1, 30)     ] j     l:f64[30,1,1,30] = broadcast_in_dim[       broadcast_dimensions=(0, 2, 3)       shape=(30, 1, 1, 30)     ] k     m:f64[1,1000,1000,30] = broadcast_in_dim[       broadcast_dimensions=(1, 2, 3)       shape=(1, 1000, 1000, 30)     ] i     n:f64[30,1000,1000,30] = mul l m     o:f64[30,1000,1000,30] = add 1.0 n     p:f64[30,1000,1000] = reduce_prod[axes=(3,)] o     q:f64[30,1000] = dot_general[       dimension_numbers=(([2], [1]), ([0], [0]))       preferred_element_type=float64     ] p c     r:f64[30] = dot_general[       dimension_numbers=(([1], [1]), ([0], [0]))       preferred_element_type=float64     ] c q   in (r,) } ``` This prints the jaxpr, and I see that one array is `n:f64[30,1000,1000,30] =...`. So what happens in the vmap is that jax pushes all the computations along the different axes into one very big matrix inside of the computations. This would prevent my code to scale well in memory.  However, if I use a slightly different function,  ```python def k2(x,y,which):   return (1 + jnp.dot(x * which, y)) ** 2 K2=jax.vmap(k2,in_axes=(0,None,None)) K2=jax.vmap(K2,in_axes=(None,0,None)) assert K2(X,X,active[0]).shape==(1000,1000) def activation(X, which_dim, yb):     mat = K2(X, X, which_dim)     return np.dot(yb, mat @ yb) activation_vmap = jax.vmap(activation, in_axes=(None, 0, 0)) assert activation_vmap(X,active,y).shape==(30,) print(jax.make_jaxpr(activation_vmap)(X,active,y)) ``` which prints ``` { lambda ; a:f64[1000,30] b:i64[30,30] c:f64[30,1000]. let     d:f64[30,30] = convert_element_type[new_dtype=float64 weak_type=False] b     e:f64[30,1,30] = broadcast_in_dim[       broadcast_dimensions=(0, 2)       shape=(30, 1, 30)     ] d     f:f64[1,1000,30] = broadcast_in_dim[       broadcast_dimensions=(1, 2)       shape=(1, 1000, 30)     ] a     g:f64[30,1000,30] = mul f e     h:f64[30,1000,1000] = dot_general[       dimension_numbers=(([2], [1]), ([], []))       preferred_element_type=float64     ] g a     i:f64[30,1000,1000] = add 1.0 h     j:f64[30,1000,1000] = integer_pow[y=2] i     k:f64[30,1000,1000] = transpose[permutation=(0, 2, 1)] j     l:f64[30,1000] = dot_general[       dimension_numbers=(([2], [1]), ([0], [0]))       preferred_element_type=float64     ] k c     m:f64[30] = dot_general[       dimension_numbers=(([1], [1]), ([0], [0]))       preferred_element_type=float64     ] c l   in (m,) } ``` I can see that the largest matrix created is of size `f64[30,1000,1000]`, and it can be attributed to the existence of the primitive dot general, which doesn't blow up the dimension of my matrix's intermediate computations.  My understanding is that I could make k1 a primitive so that its inside computations are not vectorized into a very large vector. While probably slower than what is shown above, this might allow for a more flexible management of the memory/speed tradeoff, by essentially blocking jax from going too deep into the computations and making a very large matrix in the process.  I have seen the tutorial for creating primitives here, but it seems like what I'm trying to accomplish may happen to others.  I was wondering if there could be a way to simply define primitives from JAX primitives (as is shown at the beginning of the tutorial) but tell JAX to not go lower than this level when doing vmap, jit etc...  In an ideal case this would look like ```python .as_primitive def k1(x,y,which):   return np.prod(1+which*np.exp((xy)**2)) ``` Additional arguments may be necessary to specify how the function should be vectorized, but this would allow to make a primitive without having to dig into XLA Compilation rules Then, the rest of the code would be the same and yield ```python K1=jax.vmap(k1,in_axes=(0,None,None)) K1=jax.vmap(K1,in_axes=(None,0,None)) assert K1(X,X,active[0]).shape==(1000,1000) def activation(X, which_dim, yb):     mat = K1(X, X, which_dim)     return np.dot(yb, mat @ yb) activation_vmap = jax.vmap(activation, in_axes=(None, 0, 0)) assert activation_vmap(X,active,y).shape==(30,) print(jax.make_jaxpr(activation_vmap)(X,active,y)) ``` ``` { lambda ; a:f64[1000,30] b:i64[30,30] c:f64[30,1000]. let     d:f64[30,30] = convert_element_type[new_dtype=float64 weak_type=False] b     e:f64[30,1,30] = broadcast_in_dim[       broadcast_dimensions=(0, 2)       shape=(30, 1, 30)     ] d     f:f64[1,1000,30] = broadcast_in_dim[       broadcast_dimensions=(1, 2)       shape=(1, 1000, 30)     ] a     g:f64[30,1000,30] = mul f e     h:f64[30,1000,1000] = custom_k1[       dimension_numbers=(([2], [1]), ([], []))       preferred_element_type=float64     ] g a  here is the change in the jaxpr      i:f64[30,1000,1000] = add 1.0 h     j:f64[30,1000,1000] = integer_pow[y=2] i     k:f64[30,1000,1000] = transpose[permutation=(0, 2, 1)] j     l:f64[30,1000] = dot_general[       dimension_numbers=(([2], [1]), ([0], [0]))       preferred_element_type=float64     ] k c     m:f64[30] = dot_general[       dimension_numbers=(([1], [1]), ([0], [0]))       preferred_element_type=float64     ] c l   in (m,) } ```",2024-05-23T02:49:10Z,enhancement,closed,0,2,https://github.com/jax-ml/jax/issues/21377,"No, there's no simple way to define primitives at the moment. That said, I don't think in practice you need this here. When wrapped in JIT, your computation will not materialize arrays of size `[30,1000,1000,30]`: the jaxpr you printed is a logical representation of the operation defined by your Python code; it is converted to HLO and then the compiler chooses how to execute it. In this case, if you inspect the compiled HLO, you see that several operations in your jaxpr are fused: essentially the compiler can recognize that the mapped sum of an elementwise product is equivalent to a `dot_general`, and it automatically calls `dot_general` instead. You can see this compiled HLO using the ahead of time compilation tools: ```python print(jax.jit(activation_vmap).lower(X, active, y).compile().as_text()) ``` ``` HloModule jit_activation, entry_computation_layout={(f32[1000,30]{1,0}, s32[30,30]{1,0}, f32[30,1000]{1,0})>f32[30]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true} %region_0.30 (Arg_0.31: f32[], Arg_1.32: f32[]) > f32[] {   %Arg_0.31 = f32[] parameter(0)   %Arg_1.32 = f32[] parameter(1)   ROOT %multiply.33 = f32[] multiply(f32[] %Arg_0.31, f32[] %Arg_1.32), metadata={op_name=""jit(activation)/jit(main)/reduce_prod[axes=(3,)]"" source_file="""" source_line=12} } %fused_computation.clone (param_0.10: f32[1000,1000,30], param_1.8: s32[30,30]) > f32[30,1000,1000] {   %param_1.8 = s32[30,30]{1,0} parameter(1)   %convert.1 = f32[30,30]{1,0} convert(s32[30,30]{1,0} %param_1.8), metadata={op_name=""jit(activation)/jit(main)/convert_element_type[new_dtype=float32 weak_type=False]"" source_file="""" source_line=12}   %broadcast.16 = f32[30,1000,1000,30]{3,2,1,0} broadcast(f32[30,30]{1,0} %convert.1), dimensions={0,3}, metadata={op_name=""jit(activation)/jit(main)/mul"" source_file="""" source_line=12}   %param_0.10 = f32[1000,1000,30]{2,1,0} parameter(0)   %broadcast.15 = f32[30,1000,1000,30]{3,2,1,0} broadcast(f32[1000,1000,30]{2,1,0} %param_0.10), dimensions={1,2,3}, metadata={op_name=""jit(activation)/jit(main)/mul"" source_file="""" source_line=12}   %multiply.2 = f32[30,1000,1000,30]{3,2,1,0} multiply(f32[30,1000,1000,30]{3,2,1,0} %broadcast.16, f32[30,1000,1000,30]{3,2,1,0} %broadcast.15), metadata={op_name=""jit(activation)/jit(main)/mul"" source_file="""" source_line=12}   %constant.1 = f32[] constant(1)   %broadcast.13 = f32[30,1000,1000,30]{3,2,1,0} broadcast(f32[] %constant.1), dimensions={}   %add.1 = f32[30,1000,1000,30]{3,2,1,0} add(f32[30,1000,1000,30]{3,2,1,0} %multiply.2, f32[30,1000,1000,30]{3,2,1,0} %broadcast.13), metadata={op_name=""jit(activation)/jit(main)/add"" source_file="""" source_line=12}   ROOT %reduce.1 = f32[30,1000,1000]{2,1,0} reduce(f32[30,1000,1000,30]{3,2,1,0} %add.1, f32[] %constant.1), dimensions={3}, to_apply=%region_0.30, metadata={op_name=""jit(activation)/jit(main)/reduce_prod[axes=(3,)]"" source_file="""" source_line=12} } %parallel_fusion (p: f32[1000,1000,30], p.1: s32[30,30]) > f32[30,1000,1000] {   %p = f32[1000,1000,30]{2,1,0} parameter(0)   %p.1 = s32[30,30]{1,0} parameter(1)   ROOT %fusion.clone = f32[30,1000,1000]{2,1,0} fusion(f32[1000,1000,30]{2,1,0} %p, s32[30,30]{1,0} %p.1), kind=kLoop, calls=%fused_computation.clone, metadata={op_name=""jit(activation)/jit(main)/reduce_prod[axes=(3,)]"" source_file="""" source_line=12}, backend_config={""outer_dimension_partitions"":[""2""]} } %fused_computation.1.clone (param_0.11: f32[1000,30]) > f32[1000,1000,30] {   %param_0.11 = f32[1000,30]{1,0} parameter(0)   %broadcast.18 = f32[1000,1000,30]{2,1,0} broadcast(f32[1000,30]{1,0} %param_0.11), dimensions={1,2}, metadata={op_name=""jit(activation)/jit(main)/sub"" source_file="""" source_line=12}   %broadcast.17 = f32[1000,1000,30]{2,1,0} broadcast(f32[1000,30]{1,0} %param_0.11), dimensions={0,2}, metadata={op_name=""jit(activation)/jit(main)/sub"" source_file="""" source_line=12}   %subtract.1 = f32[1000,1000,30]{2,1,0} subtract(f32[1000,1000,30]{2,1,0} %broadcast.18, f32[1000,1000,30]{2,1,0} %broadcast.17), metadata={op_name=""jit(activation)/jit(main)/sub"" source_file="""" source_line=12}   %multiply.3 = f32[1000,1000,30]{2,1,0} multiply(f32[1000,1000,30]{2,1,0} %subtract.1, f32[1000,1000,30]{2,1,0} %subtract.1), metadata={op_name=""jit(activation)/jit(main)/integer_pow[y=2]"" source_file="""" source_line=12}   %negate.1 = f32[1000,1000,30]{2,1,0} negate(f32[1000,1000,30]{2,1,0} %multiply.3), metadata={op_name=""jit(activation)/jit(main)/neg"" source_file="""" source_line=12}   ROOT %exponential.1 = f32[1000,1000,30]{2,1,0} exponential(f32[1000,1000,30]{2,1,0} %negate.1), metadata={op_name=""jit(activation)/jit(main)/exp"" source_file="""" source_line=12} } %parallel_fusion.1 (p.2: f32[1000,30]) > f32[1000,1000,30] {   %p.2 = f32[1000,30]{1,0} parameter(0)   ROOT %fusion.1.clone = f32[1000,1000,30]{2,1,0} fusion(f32[1000,30]{1,0} %p.2), kind=kLoop, calls=%fused_computation.1.clone, metadata={op_name=""jit(activation)/jit(main)/exp"" source_file="""" source_line=12}, backend_config={""outer_dimension_partitions"":[""2""]} } ENTRY %main.37 (Arg_0.1: f32[1000,30], Arg_1.2: s32[30,30], Arg_2.3: f32[30,1000]) > f32[30] {   %Arg_2.3 = f32[30,1000]{1,0} parameter(2)   %Arg_0.1 = f32[1000,30]{1,0} parameter(0)   %call.1 = f32[1000,1000,30]{2,1,0} call(f32[1000,30]{1,0} %Arg_0.1), to_apply=%parallel_fusion.1   %Arg_1.2 = s32[30,30]{1,0} parameter(1)   %call = f32[30,1000,1000]{2,1,0} call(f32[1000,1000,30]{2,1,0} %call.1, s32[30,30]{1,0} %Arg_1.2), to_apply=%parallel_fusion   %bitcast = f32[30,1000,1]{2,1,0} bitcast(f32[30,1000]{1,0} %Arg_2.3)   %customcall = f32[30,1000,1]{2,1,0} customcall(f32[30,1000,1000]{2,1,0} %call, f32[30,1000,1]{2,1,0} %bitcast), custom_call_target=""__onednn$matmul"", metadata={op_name=""jit(activation)/jit(main)/dot_general[dimension_numbers=(((2,), (1,)), ((0,), (0,))) precision=None preferred_element_type=float32]"" source_file="""" source_line=21}, backend_config={""outer_dimension_partitions"":[],""onednn_matmul_config"":{""transpose_a"":false,""transpose_b"":false,""fused_ops"":[],""bias_broadcast"":false,""alpha_typecast"":0}}   %bitcast.1 = f32[30,1000]{1,0} bitcast(f32[30,1000,1]{2,1,0} %customcall), metadata={op_name=""jit(activation)/jit(main)/dot_general[dimension_numbers=(((2,), (1,)), ((0,), (0,))) precision=None preferred_element_type=float32]"" source_file="""" source_line=21}   ROOT %dot.36 = f32[30]{0} dot(f32[30,1000]{1,0} %Arg_2.3, f32[30,1000]{1,0} %bitcast.1), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_name=""jit(activation)/jit(main)/dot_general[dimension_numbers=(((1,), (1,)), ((0,), (0,))) precision=None preferred_element_type=float32]"" source_file="""" source_line=21} } ``` A good way to read this is that the arrays within the `main` block will be actually materialized, and the arrays in the several `fusion` operations are logical shapes that will not be actually materialized in the course of the computation. Long story short: if you use `jit`, you don't need to worry about memory allocation at this level.",I see thank you for the clarification.  In that case should I compile in advance to avoid errors of preallocation? When one matrix gets too big I get errors that seem related to the size of the jaxpr matrices 
large language model,[NVIDIA] Add new SDPA API to jax.nn,"Attention plays a crucial role in modern transformerbased models. While there exist various variants, they generally follow the same workflow. Examples include the typical multihead attention (MHA), global query attention (GQA), and multiquery attention (MQA). Additionally, new implementations like the Flash Attention algorithm aim to enhance the utilization of accelerator devices. For instance, NVIDIA cuDNN supports Flash Attention and, through its API, can result in a 1.3x endtoend speedup for training large language models based on GPT alone. This PR proposes introducing a new API in the `jax.nn` module to handle attention. It will first try to use the cudnn flash attention execution path when the config is compatible. Otherwise it falls back to a jax implementation.  cc.    ",2024-05-22T20:34:19Z,pull ready,closed,0,21,https://github.com/jax-ml/jax/issues/21371, Can you help find reviewers?,Pushed a new commit to remove the use of `is_training` for the cudnn flash attention. This is a followup of this merged PR., Any updates?,"The API should have an `implementation` option, taking values like `""xla""`, `""cudnn""`, and `None` (the default, which selects the best algorithm). This list will grow with alternative kernel implementations (Pallas, etc). It is important to be able to select the implementation type:  `""cudnn""` will fail immediately if there is some unsupported shape, which prevents silent reversions to slow code paths.  Generating serialized models to do inference with on a different device type (eg train on GPU and test on TPU).  Regarding the names: does cuDNN expose both FlashAttention and nonFlashAttention? Perhaps this should be `""cudnn_flash""`? Note that XLA also has different implementations: we could support the lowmemory chunked implementation given here (https://arxiv.org/abs/2112.05682) that inspired FlashAttention, and which is closer numerically to FlashAttention than standard attention and has the same memory complexity (maybe `""xla_chunked""`? `""xla_low_memory""`?).  Are there any configuration options a user might want to pass to the cuDNN implementation? If so, it could be a string or a cuDNN config dataclass. Eg. in the lowmemory XLA case, the chunk size is something a user might want to configure.","> The API should have an `implementation` option, taking values like `""xla""`, `""cudnn""`, and `None` (the default, which selects the best algorithm). This list will grow with alternative kernel implementations (Pallas, etc). It is important to be able to select the implementation type: >  > * `""cudnn""` will fail immediately if there is some unsupported shape, which prevents silent reversions to slow code paths. > * Generating serialized models to do inference with on a different device type (eg train on GPU and test on TPU). >  > Regarding the names: does cuDNN expose both FlashAttention and nonFlashAttention? Perhaps this should be `""cudnn_flash""`? Note that XLA also has different implementations: we could support the lowmemory chunked implementation given here (https://arxiv.org/abs/2112.05682) that inspired FlashAttention, and which is closer numerically to FlashAttention than standard attention and has the same memory complexity (maybe `""xla_chunked""`? `""xla_low_memory""`?). >  > Are there any configuration options a user might want to pass to the cuDNN implementation? If so, it could be a string or a cuDNN config dataclass. Eg. in the lowmemory XLA case, the chunk size is something a user might want to configure. Sorry, I think I missed this comment. Do you mean sth like: ```python def sdpa(..., implementation=None):   if implementation == 'cudnn':     cudnn_sdpa()  users expect to fail on error   elif implementation == 'pallas':     pallas_sdpa()  this is for the future.   elif implementation is None:      current path of tryexcept. and will always fall back to `_dot_product_attention_xla`. ``` Re cudnn flash attentions: (1)  cuDNN used to expose both flash and nonflash attention kernel, but we choose not to use the nonflash anymore. So, the cudnn attention means cudnn flash attention now. And I am ok with the `cudnn`. (2) We don't need to pass config to cudnn calls and we are trying to hide it from users.","> Sorry, I think I missed this comment. Do you mean sth like: That looks correct. We have two options here: 1. Have multiple SDPA functions, one per backend/implementation. 2. Have a single API with the `implementation` option.  There are pros and cons of each, and some tricky questions. For example:  How closely do numerics need to match in the superfunction to be considered 'the same'? As found in this review, cuDNN with bf16 inputs does not cast the first matmul to BF16 before doing softmax, whilst XLA does. If we choose the cuDNN convention, the XLA implementation will be incredibly memoryinefficient. This might be a significant difference in certain applications (eg. training with one but doing inference with the other on a different devicetype). With future Pallas kernels, we can match the numerics. But this might be harder for thirdparty libraries like cuDNN. We might also do autotuning and choose the best kernel with the `None` option, which becomes problematic with these numerical differences. This is an argument to have separate functions for thirdparty kernels that JAX has no control over and are largely opaque (hard to see what numerical choices are being made), and only have a superfunction for implementations under JAXcontrol.   Another argument for separate functions is that the API can be restricted to only the supported features, rather than the most general function imaginable. The current design is makes it hard for users to see what is supported, and limits documentation opportunities. In addition, there are cuDNN specific options (like the philox dropout) unsupported by any other backend, further complicating the API.",I think the name should be `dot_product_attention` rather than `scaled_dot_product_attention`. Its also more consistent with Flax naming (https://flax.readthedocs.io/en/v0.8.0/api_reference/flax.linen/_autosummary/flax.linen.dot_product_attention.html).,"As discussed offline: lets land the simplest version first, without dropout or other complications. Then progressively add features. ","Just pushed some new commits for the simplified sdpa.  PTAL. Also talked to  and he will try to implement the combination of bias and mask in the cudnn dot_product_attention API (as described here in (1)). When that is in, our logic of preparing bias will be much simpler.",Pushed a few more changes. PTAL.  ,Please squash the commits. This will be mergeable as soon as Chris clarifies his comments.,> Please squash the commits. This will be mergeable as soon as Chris clarifies his comments. Sure. Rebased. PTAL.  ,: I'm happy with the state of it now. Think we can merge.,"Pushed new commits to resolved some failed python lint tests. Btw, can we have the access to add `kokoro:forcerun` label to trigger the tests?",Please squash the commits and we can merge.,Done. PTAL.  ,"I still saw this lint error: ```jax/_src/nn/functions.py:924: error: Argument 4 to ""dot_product_attention"" has incompatible type ""Array  None""  [argtype]``` But I am a bit confused. I think it refers to the `mask` which I have already converted to Array by `jnp.asarray(mask)` at the beginning in the function. Do you have any advice on this?   ","No worries, I'll resolve this internally.","> As discussed offline: lets land the simplest version first, without dropout or other complications. Then progressively add features. Thanks for adding FA! Is there a timeline to add `dropout` support in the SDPA API? I understand it is on hold due to differences in PRNG implementation. Would it be OK if we expose `dropout_rate` to the API while warning the user on reproducibility if `cudnn` is selected? https://github.com/google/jax/blob/417fcd574b9f33410ea8eb78ffdea825ad343eee/jax/_src/cudnn/fused_attention_stablehlo.pyL954L956","> > As discussed offline: lets land the simplest version first, without dropout or other complications. Then progressively add features. >  > Thanks for adding FA! Is there a timeline to add `dropout` support in the SDPA API? I understand it is on hold due to differences in PRNG implementation. Would it be OK if we expose `dropout_rate` to the API while warning the user on reproducibility if `cudnn` is selected? >  > https://github.com/google/jax/blob/417fcd574b9f33410ea8eb78ffdea825ad343eee/jax/_src/cudnn/fused_attention_stablehlo.pyL954L956 Yes, this is on our radar to be implemented. Can we know what types of model you are working on that needs the dropout?","> Yes, this is on our radar to be implemented. Can we know what types of model you are working on that needs the dropout? Attention dropout would help for almost all lowdata training regimes. Detection Transformers are one wellknown example.  Torch supports FA dropout (possibly nondeterministic) in their functional API."
yi,Bump setuptools from 69.2.0 to 70.0.0,"Bumps setuptools from 69.2.0 to 70.0.0.  Changelog Sourced from setuptools's changelog.  v70.0.0 Features  Emit a warning when [tools.setuptools] is present in pyproject.toml and will be ignored.  by :user:SnoopJ ( CC(未找到相关数据)) Improved AttributeError error message if pkg_resources.EntryPoint.require is called without extras or distribution Gracefully &quot;do nothing&quot; when trying to activate a pkg_resources.Distribution with a None location, rather than raising a TypeError  by :user:Avasam ( CC([jax2tf] Add input and output names to the TensorFlow graph generated by jax2tf.)) Typed the dynamically defined variables from pkg_resources  by :user:Avasam ( CC(Pin pygments version in RTD build)) Modernized and refactored VCS handling in package_index. ( CC([jax2tf] Revert '[jax2tf] Replace tf.math.add with tf.raw_ops.AddV2 (…))  Bugfixes  In install command, use super to call the superclass methods. Avoids race conditions when monkeypatching from _distutils_system_mod occurs late. ( CC(Rename collectives into ""collective operations"" for the pmap function.)) Fix finder template for lenient editable installs of implicit nested namespaces constructed by using package_dir to reorganise directory structure. ( CC([jax2tf] Replace tf.math.add with tf.raw_ops.AddV2)) Fix an error with UnicodeDecodeError handling in pkg_resources when trying to read files in UTF8 with a fallback  by :user:Avasam ( CC(Ppermute batching errors))  Improved Documentation  Uses RST substitution to put badges in 1 line. ( CC(improve an escaped tracer error message))  Deprecations and Removals   Further adoption of UTF8 in setuptools. This change regards mostly files produced and consumed during the build process (e.g. metadata files, script wrappers, automatically updated config files, etc..) Although precautions were taken to minimize disruptions, some edge cases might be subject to backwards incompatibility. Support for &quot;locale&quot; encoding is now deprecated. ( CC(Help with array slice indices))   Remove setuptools.convert_path after long deprecation period. This function was never defined by setuptools itself, but rather a sideeffect of an import for internal usage. ( CC(Allow custom_linear_solve to return things besides the solution))   Remove fallback for customisations of distutils' build.sub_command after long deprecated period. Users are advised to import build directly from setuptools.command.build. ( CC(Allow custom_linear_solve to return things besides the solution))   Removed typing_extensions from vendored dependencies  by :user:Avasam ( CC(Hacky approach to carry argument of an invertible transformation in the backward pass))   Remove deprecated setuptools.dep_util. The provided alternative is setuptools.modified. ( CC([jax2tf] Added support for shape polymorphism conversion.))     ... (truncated)   Commits  5cbf12a Workaround for release error in v70 9c1bcc3 Bump version: 69.5.1 → 70.0.0 4dc0c31 Remove deprecated setuptools.dep_util ( CC([jax2tf] Added support for shape polymorphism conversion.)) 6c1ef57 Remove xfail now that test passes. Ref  CC(jnp.moveaxis: fix bug when axes are integer dtype). d14fa01 Add all sitepackages dirs when creating simulated environment for test_edita... 6b7f7a1 Prevent bin folders to be taken as extern packages when vendoring ( CC(Update scale_and_translate to take an explicit spatial_dims.)) 69141f6 Add doctest for vendorised bin folder 2a53cc1 Prevent 'bin' folders to be taken as extern packages 7208628 Replace call to deprecated validate_pyproject command ( CC(Add a prototype implementation of recursive checkpointing)) 96d681a Remove call to deprecated validate_pyproject command Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-05-22T13:37:16Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/21355,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
rag,Segfault during initialization due to local cuda version mismatch," Description Running `python c ""import jax; jax.numpy.array(0)""` fails with segfault with the following call stack ``` CC(未找到相关数据)  __strlen_avx2 () at ../sysdeps/x86_64/multiarch/strlenavx2.S:65 CC(Python 3 compatibility issues)  0x00007fff1be8278f in std::__cxx11::basic_string, std::allocator > jax::cuda::(anonymous namespace)::ErrorString(cusparseStatus_t, char const*, long, char const*) () from ~/miniconda3/envs/testjax/lib/python3.11/sitepackages/jaxlib/cuda/_versions.so CC(Explicit tuples are not valid function parameters in Python 3)  0x00007fff1be826e6 in jax::cuda::AsStatus(cusparseStatus_t, char const*, long, char const*) () from ~/miniconda3/envs/testjax/lib/python3.11/sitepackages/jaxlib/cuda/_versions.so CC(Undefined name: from ..core import JaxTuple)  0x00007fff1be81633 in jax::cuda::CusparseGetVersion() () from ~/miniconda3/envs/testjax/lib/python3.11/sitepackages/jaxlib/cuda/_versions.so ``` Jax is installed with ```bash conda create n testjax python=3.11 numpy scipy conda activate activate testjax pip install upgrade ""jax[cuda12_local]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` and the system has only CUDA 11.7 locally. Installing jax with `pip install U ""jax[cuda12]""` fixes this problem as expected. But is it avoidable to fail with segfault, and print a reasonable error message instead?  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.28 jaxlib: 0.4.28 numpy:  1.26.4 python: 3.11.9  ++ ```",2024-05-22T06:45:37Z,bug,closed,0,6,https://github.com/jax-ml/jax/issues/21349,"Hi   Thanks for reporting. JAX has dropped support for cuda11 from jaxlib version 0.4.26 and supports only cuda 12.1 or newer.  Please take a look at the official documentation for reference.: https://jax.readthedocs.io/en/latest/changelog.html:~:text=jaxlib%200.4.26%20(April,supports%20NumPy%202.0.","Yes, I'm fully aware of the CUDA version requirement, and I'm discussing that whether the UX could improve. Also, it should be noted that running on a system without CUDA (tested with a fresh `python:3.11slim` container) also segfaults. In my opinion, segfaults should be avoided in all cases.",Ironically that's segfaulting in the code that's trying to check the cuda version and report a nice error. I'll take a look.,Can you please clarify what *CPU* you have? i.e. the output of `lscpu` or similar?,"Never mind, I figured out the bug. Oddly enough it only seems to reproduce for me under a condaforge environment, but it's a bug in the stubs we use to find cusparse. The error path triggered some invalid behavior, and openxla/xla CC(tflite converter segfaulting with an updated jax) fixes it by adding correct stubs for error string code.",I verified that this seems fixed to me in the most recent nightly release. Please reopen if you can still see it with a nightly (or the next release).
yi,Restoring and using int4 arrays on XLA CPU is broken after #20610.," Description Casting `int4` arrays to `int8` after restoring them from a checkpoint was broken in CC(Add `Layout` support to `jax.jit`.) (confirmed via CL sweep) with the following error: ```python     deserialized_arr = deserialized_arr.astype(jax.numpy.int8)                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""jax/_src/numpy/array_methods.py"", line 68, in _astype     return lax_numpy.astype(arr, dtype, copy=copy, device=device)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""jax/_src/numpy/lax_numpy.py"", line 2857, in astype     lax.convert_element_type(x_arr, dtype),     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""jax/_src/lax/lax.py"", line 517, in convert_element_type     return _convert_element_type(operand, new_dtype, weak_type=False)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""jax/_src/lax/lax.py"", line 559, in _convert_element_type     return convert_element_type_p.bind(operand, new_dtype=new_dtype,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""jax/_src/core.py"", line 408, in bind     return self.bind_with_trace(find_top_trace(args), args, params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""jax/_src/core.py"", line 412, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""jax/_src/core.py"", line 901, in process_primitive     return primitive.impl(*tracers, **params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""jax/_src/dispatch.py"", line 86, in apply_primitive     outs = fun(*args)            ^^^^^^^^^^   File ""jax/_src/traceback_util.py"", line 179, in reraise_with_filtered_traceback     return fun(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^   File ""jax/_src/pjit.py"", line 305, in cache_miss     outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked = _python_pjit_helper(                                                                 ^^^^^^^^^^^^^^^^^^^^   File ""jax/_src/pjit.py"", line 182, in _python_pjit_helper     out_flat = pjit_p.bind(*args_flat, **params)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""jax/_src/core.py"", line 2811, in bind     return self.bind_with_trace(top_trace, args, params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""jax/_src/core.py"", line 412, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""jax/_src/core.py"", line 901, in process_primitive     return primitive.impl(*tracers, **params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""jax/_src/pjit.py"", line 1525, in _pjit_call_impl     return xc._xla.pjit(            ^^^^^^^^^^^^^   File ""jax/_src/pjit.py"", line 1508, in call_impl_cache_miss     out_flat, compiled = _pjit_call_impl_python(                          ^^^^^^^^^^^^^^^^^^^^^^^   File ""jax/_src/pjit.py"", line 1438, in _pjit_call_impl_python     inline=inline, lowering_parameters=mlir.LoweringParameters()).compile()                                                                   ^^^^^^^^^   File ""jax/_src/interpreters/pxla.py"", line 2407, in compile     executable = UnloadedMeshExecutable.from_hlo(                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""jax/_src/interpreters/pxla.py"", line 2932, in from_hlo     in_layouts, out_layouts = _get_layouts_from_executable(                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""jax/_src/interpreters/pxla.py"", line 2633, in _get_layouts_from_executable     raise AssertionError( AssertionError: Unexpected XLA layout override: (XLA) DeviceLocalLayout({0:E(4)}) != DeviceLocalLayout({0}) (User input layout) ``` This can be tested locally using the following code: ```python import jax from jax.experimental.array_serialization import serialization import numpy as np dtype = jax.numpy.int4 shape = (8, 2) arr = jax.numpy.arange(np.prod(shape)).reshape(shape).astype(dtype)  Run serialization. sharding = jax.sharding.GSPMDSharding.get_replicated(jax.devices()) tspecs = jax.tree_util.tree_map(     serialization.get_tensorstore_spec, ['/tmp/test_ckpt'] ) manager = serialization.GlobalAsyncCheckpointManager() manager.serialize(     [arr],     tspecs,     on_commit_callback=lambda: None, ) manager.wait_until_finished()  Run deserialization. (deserialized_arr,) = serialization.run_deserialization(     shardings=[sharding],     tensorstore_specs=tspecs,     global_shapes=[shape],     dtypes=[dtype], )  Test usage. deserialized_arr = deserialized_arr.astype(jax.numpy.int8) assert deserialized_arr + deserialized_arr == 2 * deserialized_arr ``` and tested in the CI by adding the following to the end of `test_checkpointing_with_int4` in `jax/experimental/array_serialization/serialization_test.py`: ```python    Cast to int8 to test deserialized execution.    m2 = m2.astype(jax.numpy.int8)   self.assertArraysEqual(m2 + m2 == 2 * m2) ``` This also fails when casting is added to Orbax's `single_host_test.py`.  System info (python version, jaxlib version, accelerator, etc.) Google internal HEAD: ``` jax:    0.4.29 jaxlib: 0.4.29 numpy:  1.26.3 python: 3.11.8 (stable, redacted, redacted) [Clang (fc57f88f007497a4ead0ec8607ac66e1847b02d6)] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='ill.redact.this.too', release='', version='', machine='x86_64') ```",2024-05-21T22:57:38Z,bug,closed,0,6,https://github.com/jax-ml/jax/issues/21339,"I ran this function internally and it doesn't seem to error: ``` def test_astype(self):     x = jnp.arange(4, dtype=jnp.int4)     x.astype(jnp.int8) ``` Does this error for you?",Note that you need to run on TPU and not CPU. Do you see the same error on TPU?, Isn't the issue as stated by the reporter that int4 specifically regressed on *CPU*?,"Uh oh, I skipped that part lol.",https://github.com/google/jax/pull/21372 should fix it,"Thank you! My team uses XLA CPU when exporting to MLIR for mobile deployment, so this is quite helpful!"
yi,"Counterintuitive Running time on GPU, compiled differences on profiler for different shapes"," Description I am running some sort of stochastic optimisation on GPU and depending on the shape of my samples I get very different running times. For example, the following code with shapes of 100x200 yields a running time of 28.8s while for 1000x2000 I have 5.8s. I have trouble understanding this **increase** in performance on GPU as the number of samples is x100. I expected low sample size to be somewhat inefficient on GPU but not to have its performance reduce by a factor 6 compared to 100 times more samples. Is there something I am missing ?  A minimum reproducible example is: ```python import jax from jaxtyping import PRNGKeyArray, Array from functools import partial import jax.numpy as jnp import optax import time import numpyro.distributions as dist max_signal = 1.0 base_signal = 0.1 noise_var = 0.1 jax.print_environment_info() def forward(xi, theta):     inv_value = max_signal + jnp.power(theta  xi, 2).sum(1)     arr = base_signal + jnp.power(inv_value, 1).sum(1)     return arr def log_prob(thetas, y, xi):     f_values = jax.vmap(forward, in_axes=(0, None), out_axes=1)(xi, thetas)     return (         dist.Normal(0, noise_var)         .log_prob(jnp.log(y)  jnp.log(f_values))         .sum(1)     ) def sample(args, rng_key, xi):     f_values = jax.vmap(forward, in_axes=(0, None), out_axes=1)(xi, args)     return jnp.exp(         jnp.log(f_values)         + dist.Normal(0, noise_var).sample(             rng_key, sample_shape=f_values.shape         )     ) def energy(     design: Array,     rng_key: PRNGKeyArray,     thetas: Array):      sample N y_n     N_samples = jax.tree_map(lambda leaf: leaf[:, 0], thetas)     N, Lpp = jax.tree_util.tree_leaves(thetas)[0].shape[:2]     sample_keys = jax.random.split(rng_key, N)     sampler = partial(sample, xi=design)     N_y = jax.vmap(sampler, (0, 0))(N_samples, sample_keys)      corresponding NxL log p(y_n ++",2024-05-21T22:05:53Z,bug,open,0,4,https://github.com/jax-ml/jax/issues/21336,"Hi  thanks for the question! Your benchmarks may not be measuring what you think they are here, because you're not accounting for JIT compilation time or for asynchronous dispatch. I'd suggest applying some of the tips at FAQ: Benchmarking JAX code and rerunning your benchmarks.","Hi, thanks for your response. I believe this difference is neither due to compilation time nor asynchronous dispatch as I get same results with jax.block_until_ready() and compilation time seems to be <1s.","Your updated script doesn't seem to isolate compilation time – even if you're not using `jax.jit` yourself, it is used internally by APIs you are calling.","Yes  compilation happens inside scan. To asses compilation time I compared two consecutive calls to timer block: ```python start = time.time() keys = jax.random.split(rng_key, 5000) design, _ = jax.lax.scan(step, (design, optimizer_state), keys) jax.block_until_ready(design) end = time.time() print(end  start)  compilation happens 6s on GPU start = time.time() keys = jax.random.split(rng_key, 5000) design, _ = jax.lax.scan(step, (design, optimizer_state), keys) jax.block_until_ready(design) end = time.time() print(end  start)  no compilation 5.2s on GPU ``` which suggest that compilation takes <1s. My question was regarding the potential differences between XLA reduce_sum and add_any and why the same code with different shapes gets compiled in two more or less efficient ways. The most efficient way being the one compiled for the largest shapes. On top of empirical total running times, the difference between the two can be assessed with a tensorboard profiler for example where we actually see the difference between add_any / reduce_sum. The function step() takes longer to run in the add_any case even though there are less computations to do.  This behaviour is not observed on CPU where running time is proportional to shapes (2.5s for 100x200 and 126s for 1000x2000)."
yi,WIP `jax.numpy` array API compliance finalization,"Towards https://github.com/google/jax/issues/21088  Changes  Adds the following attributes to `ArrayImpl`:      `__array_namespace__` property      `to_device` method      `device` property  Adds the following to `jax.numpy`:      `__array_namespace_info__`      `__array_api_version__`  Notes This PR is a draft right now since we should include these changes _last_ so as to publicly support `jax.numpy` as an array API compliant namespace. 5 can take over this PR later once the remainder of the work is completed. This does not need to wait on _all_ the ongoing array API related deprecations to be completed, since some of them are only required for the 2023 standard, hence we can likely adopt the 2022 standard first. It may make sense break off the `to_device` and `device` changes for `ArrayImpl` into a small separate PR, since they don't imply explicit compliance by themselves, but I wanted to keep them together in this PR in case there were any caveats wrt to `ArrayImpl` vs `Tracer` behaviors that we should discuss first (based on old `TODO` note).",2024-05-21T01:03:41Z,,closed,0,2,https://github.com/jax-ml/jax/issues/21323,"We only finally removed the `arr.device()` method in JAX v0.4.27 – to avoid confusion for users I think we should wait for one more release (0.4.29) before we add the `arr.device` property, so that it will be part of 0.4.30. What do you think?", let me close this PR as Array API compliance is already finalized on `main`.
yi,Compilation time on GPU is proportional to batch size for grad of vmapped Cholesky solve," Description The problem is with the grad of the mean of a vmapped Cholesky solution . If I define ``` def func(pars):   ftf = fmat @ jax.numpy.diag(pars**2) @ fmat.T + one   cf = jax.scipy.linalg.cho_factor(ftf)   b = jax.scipy.linalg.cho_solve(cf, ones)   return b.mean() ``` and then transform/compile ``` jvg = jax.value_and_grad(lambda pars: jax.vmap(func)(pars).mean()) pars = jax.random.normal(jax.random.PRNGKey(0), (nbatch,2*ngp,)) jjvg = jax.jit(jvg).lower(pars).compile() ``` I find that the compilation time grows with `nbatch`. For instance `nbatch, time(s) = [16,0.532], [32,0.507], [64,0.516], [128,0.580], [256,0.652], [512,0.822], [1024,1.7], [2048,2.75]` for the example matrices listed below. What's happening here? To run this example you need matrices such as ``` nobs, ngp = 256, 64 t = np.linspace(0, 1, nobs) f = np.arange(1, ngp + 1, dtype=np.float64) fmat = np.zeros((nobs, 2*ngp), dtype=np.float64) fmat[:,  ::2] = np.sin(2.0 * jnp.pi * f * t[:,np.newaxis]) fmat[:, 1::2] = np.cos(2.0 * jnp.pi * f * t[:,np.newaxis]) one, ones = jax.numpy.identity(nobs, dtype=np.float64), jax.numpy.ones(nobs, dtype=np.float64) ```  System info (python version, jaxlib version, accelerator, etc.) ``` JAX 0.4.26, CUDA 12.2 and driver 535.104.05, Nvidia V100. Python 3.10.12 on Linux (Colab) ```",2024-05-20T19:41:32Z,bug,open,0,5,https://github.com/jax-ml/jax/issues/21313,"Thanks for the report. I'm not sure what's going on, but it seems others are also noticing this: https://stackoverflow.com/questions/78486071/whydoesjaxcompilationtimegrowwithvmapbatchsize","Thanks ; both queries are from me :) In this case, note also that the Cholesky without the grad compiles in constant time, so it must be something about the highlevel gradient algorithm for Cholesky.",I can repro on a Colab A100; thought it somehow might have to do with constant folding but even passing `fmat` as an argument and defining `one` and `ones` in function I still see the batchdependent compile time,"Interestingly, using your `make_hlo` (which I just found in https://github.com/google/jax/issues/7949) shows that the XLA code for, say, `nbatch = 64` and `nbatch = 512` is essentially the same, except for 64 > 512. Would this mean that the problem is at the LLVM level? (Or another Nvidia representation?)",Another clue is that the linear compilation time happens also for a function that's already written with the extra batch dimension instead of being `vmap`ped. So the problem must be the batched grad Cholesky.
rag,Bump setuptools from 69.2.0 to 69.5.1,"Bumps setuptools from 69.2.0 to 69.5.1.  Changelog Sourced from setuptools's changelog.  v69.5.1 No significant changes. v69.5.0 Features  Refresh unpinned vendored dependencies. ( CC(implement jnp.apply_along_axis)) Updated vendored packaging to version 24.0. ( CC(Support polynomial division for mask))  v69.4.2 Bugfixes  Merged bugfix for pypa/distutils CC(Batching rule for 'gather' not implemented).  v69.4.1 No significant changes. v69.4.0 Features  pypa/distutils CC(Implement the `mode='constant'` case of `np.pad`.) CC(Fix device_put_sharded() for concrete values))  v69.3.1 Bugfixes  Remove attempt to canonicalize the version. It's already canonical enough. ( CC(Fix code quality issues))  v69.3.0   ... (truncated)   Commits  ff58075 Bump version: 69.5.0 → 69.5.1 d46727f Merge tag 'v69.4.2' into main. 5de8e14 Bump version: 69.4.1 → 69.4.2 f07b037 Add news fragment. 608de82 Merge https://github.com/pypa/distutils into v69.4.1 e5e3cc1 Merge pull request  CC(Add Sphinxgenerated reference documentation for JAX.) from pypa/hotfix/246linkerargslist ef297f2 Extend the retention of the compatibility. 98eee7f Exclude compat package from coverage. d2581bf Add 'consolidate_linker_args' wrapper to protect the old behavior for now. a04913a Add type declaration for runtime_library_dir_option, making explicit the diff... Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-05-20T17:41:46Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/21312,Superseded by CC(Bump setuptools from 69.2.0 to 70.0.0).
yi,Bump pytest-xdist from 3.5.0 to 3.6.1,"Bumps pytestxdist from 3.5.0 to 3.6.1.  Changelog Sourced from pytestxdist's changelog.  pytestxdist 3.6.1 (20240428) Bug Fixes   CC(Implementation of np.corrcoef) &lt;https://github.com/pytestdev/pytestxdist/issues/1071&gt;_: Add backward compatibility for deadlock issue with the execnet new main_thread_only &quot;execmodel&quot; triggered when pytestcov accesses rinfo.  pytestxdist 3.6.0 (20240419) This release was YANKED due to a regression fixed in 3.6.1. Features   CC(`jax.jit` should report an error immediately for uncallable arguments) &lt;https://github.com/pytestdev/pytestxdist/pull/1027&gt;_:pytestxdist workers now always execute the tests in the main thread. Previously some tests might end up executing in a separate thread other than main in the workers, due to some internal execnet`` details. This can cause problems specially with async frameworks where the event loop is running in the ``main`` thread (for example  CC(Indexing numpy array with DeviceArray: index interpreted as tuple) pytestdev/pytestxdist CC(Indexing numpy array with DeviceArray: index interpreted as tuple)`__).  Bug Fixes    CC(FFT Hessian broken) &lt;https://github.com/pytestdev/pytestxdist/issues/1024&gt;_: Added proper handling of shouldstop (such as set by maxfail) and shouldfail conditions in workers. Previously, a worker might have continued executing further tests before the controller could terminate the session.    CC(`ravel_pytree` does not work with int input) &lt;https://github.com/pytestdev/pytestxdist/issues/1028&gt;_: Fixed compatibility issue between looponfail and editable installs.    CC(Indexing numpy array with DeviceArray: index interpreted as tuple) &lt;https://github.com/pytestdev/pytestxdist/issues/620&gt;_: Use the new main_thread_only execnet &quot;execmodel&quot; so that code which expects to only run in the main thread will now work as expected.    CC(Handle closure variables consistently in custom_transforms) &lt;https://github.com/pytestdev/pytestxdist/issues/937&gt;_: Fixed a bug where plugin would raise an incompatibility error with pdb despite using n0.   Removals    CC(Update core.call_impl to use call_wrapped) &lt;https://github.com/pytestdev/pytestxdist/issues/1053&gt;_: Dropped support for Python 3.7.    CC(Add a common lax._canonicalize_shape method, use on methods that acce…) &lt;https://github.com/pytestdev/pytestxdist/issues/1057&gt;_: pytest&gt;=7.0.0 is now required. execnet&gt;=2.1.0 is now required.   Trivial Changes    CC(Support mixed basic and advanced indexing in ops.index_update) &lt;https://github.com/pytestdev/pytestxdist/issues/1020&gt;_: pytestxdist's setup.py file is removed. If you relied on this file, e.g. to install pytest using setup.py install, please see Why you shouldn't invoke setup.py directly &lt;https://blog.ganssle.io/articles/2021/10/setuppydeprecated.htmlsummary&gt;_ for alternatives.     ... (truncated)   Commits  4dd2978 Release 3.6.1 b397288 Merge pull request  CC(Clipping of a NaN matrix) from zmedico/gatewaycacherinfo 12b3cce Cache execnet gateway rinfo during WorkerController setup c93a106 build(deps): bump hynek/buildandinspectpythonpackage ( CC(speed up pmap axissize getting)) 52e2022 [precommit.ci] precommit autoupdate ( CC(Avoid building an identity computation in jax.device_get().)) 699f939 Merge pull request  CC(Return DeviceArray from ShardedDeviceArray.__getitem__ instead of copying to host) from pytestdev/release3.6.0 80bc0b8 Release 3.6.0 20e3ac7 Use execnet main_thread_only execmodel ( CC(`jax.jit` should report an error immediately for uncallable arguments)) 0a4238f Merge pull request  CC(pmap error: ""No canonicalize handler registered for type: "") from pytestdev/precommitciupdateconfig 0686279 [precommit.ci] precommit autoupdate Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-05-20T17:41:38Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/21311,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
yi,Bump pluggy from 1.4.0 to 1.5.0,"Bumps pluggy from 1.4.0 to 1.5.0.  Changelog Sourced from pluggy's changelog.  pluggy 1.5.0 (20240419) Features    CC(add np.append and np.polyval) &lt;https://github.com/pytestdev/pluggy/issues/178&gt;_: Add support for deprecating specific hook parameters, or more generally, for issuing a warning whenever a hook implementation requests certain parameters. See :ref:warn_on_impl for details.   Bug Fixes   CC(Initial work on autospmd) &lt;https://github.com/pytestdev/pluggy/issues/481&gt;_: PluginManager.get_plugins() no longer returns None for blocked plugins.     Commits  f8aa4a0 Preparing release 1.5.0 b4a8c92 Merge pull request  CC(while_loop vmap init_val with mixed map/unmap dims) from bluetech/warnonimplargs 6f6ea68 Add support deprecating hook parameters 91f88d2 Merge pull request  CC(Add nanmean to lax_numpy) from bluetech/codecovaction 89ce829 ci: replace uploadcoverage script with codecov github action 29f104d Lift pluggy ( CC(Prefix functions in lax.py that shouldn't be visible in documentation.)) c2b36b4 Merge pull request  CC(Add batching rule for triangular solve.) from pytestdev/precommitciupdateconfig 2b533c9 [precommit.ci] precommit autoupdate 04d1bcd [precommit.ci] precommit autoupdate ( CC(vmap support / batching rule for lax.cond)) f74e94b [precommit.ci] precommit autoupdate ( CC(while_loop vmap error)) Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) You can trigger a rebase of this PR by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)  > **Note** > Automatic rebases have been disabled on this pull request as it has been open for over 30 days.",2024-05-20T17:41:33Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/21310,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
yi,Bump matplotlib from 3.8.3 to 3.9.0,"Bumps matplotlib from 3.8.3 to 3.9.0.  Release notes Sourced from matplotlib's releases.  REL: 3.9.0 Highlights of this release include:  Plotting and Annotation improvements  Axes.inset_axes is no longer experimental Legend support for Boxplot Percent sign in pie labels autoescaped with usetex=True hatch parameter for stackplot Add option to plot only one half of violin plot axhline and axhspan on polar axes Subplot titles can now be automatically aligned axisartist can now be used together with standard Formatters Toggle minorticks on Axis StrMethodFormatter now respects axes.unicode_minus   Figure, Axes, and Legend Layout  Subfigures now have controllable zorders Getters for xmargin, ymargin and zmargin   Mathtext improvements  mathtext documentation improvements mathtext spacing corrections   Widget Improvements  Check and Radio Button widgets support clearing   3D plotting improvements  Setting 3D axis limits now set the limits exactly   Other improvements  New BackendRegistry for plotting backends Add widths, heights and angles setter to EllipseCollection image.interpolation_stage rcParam Arrow patch position is now modifiable NonUniformImage now has mouseover support    REL: v3.9.0rc2 This is the second release candidate for the meso release 3.9.0. REL: v3.8.4 This is the fourth micro release of the 3.8 series. Highlights of the 3.8.4 release include:  Enable building against numpy 2.0; released wheels are built against numpy 2 macosx: Clean up singleshot timers correctly Add a draw during show for macos backend Fix color sequence data for Set2 and Set3 gtk: Ensure pending draws are done before GTK draw Update &quot;Created with&quot; url in hand.svg Avoid modifying user input to Axes.bar fix quiver3d incorrect arrow colors     Commits  be56634 REL: v3.9.0 846ce8a DOC: Finish documentation for 3.9.0 8604f67 Merge pull request  CC(未找到相关数据) from meeseeksmachine/autobackportofpr28205onv... 196c8db TST: Followup corrections to  CC(未找到相关数据) d8f3016 Backport PR  CC(未找到相关数据): TST: Fix tests with older versions of ipython 4db5ac9 Merge pull request  CC(未找到相关数据) from QuLogic/autobackportofpr28164onv3.9.x ef1a2db Merge pull request  CC(未找到相关数据) from meeseeksmachine/autobackportofpr28195onv... e4384b8 Merge pull request  CC(未找到相关数据) from QuLogic/nodevtheme 3b65546 Backport PR  CC(未找到相关数据): TST: Prepare for pytest 9 1b526c3 Backport PR  CC(未找到相关数据): CI: Ensure code coverage is always uploaded Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) You can trigger a rebase of this PR by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)  > **Note** > Automatic rebases have been disabled on this pull request as it has been open for over 30 days.",2024-05-20T17:41:25Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/21308,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
yi,JIT constant folding," Description Hi, I was hoping that someone could help me with this. Sometimes, when using constants in jitted functions, I get warnings like this one: ``` 20240519 20:16:26.694439: E external/xla/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:   %reduce.8 = f64[200000,10]{1,0} reduce(f64[200000,10,10]{2,1,0} %broadcast.2, f64[] %constant.3), dimensions={2}, to_apply=%region_0.4, metadata={op_name=""jit(f)/jit(main)/reduce_sum[axes=(2,)]"" source_file=""..."" source_line=13} This isn't necessarily a bug; constantfolding is inherently a tradeoff between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time. If you'd like to file a bug, run with envvar XLA_FLAGS=xla_dump_to=/tmp/foo and attach the results. ``` These warnings appear seemingly random, for example with the following code: ```python from functools import wraps import jax import jax.numpy as jnp import jax.core jax.config.update(""jax_enable_x64"", True) jax.config.update(""jax_platforms"", ""cpu"") v = jnp.zeros((200000, 10, 10)) def f():     return jax.vmap(jax.vmap(jnp.sum))(v) jax.jit(f)() ``` This code produces ""constant folding"" warnings on windows and on linux. Maybe / probably this is dependend on OS version, CPU type, ... When playing around with array shapes and number of nested vmaps, these messages appear or not appear without any clear (atleast not clear to me) pattern. For exampe, this is fast: ```python v = jnp.zeros((1000000, 10, 10)) def f():     return jax.vmap(jnp.sum)(v) jax.jit(f)() ``` While this is slow and produces the warning: ```python v = jnp.zeros((1000000, 10, 2)) def f():     return jax.vmap(jnp.sum)(v) jax.jit(f)() ``` Constant folding only happens when compiling with `jax.jit`  making jaxprs is not affected. Since jaxprs are perfectly able to catch constants, it is possible to compile them while treating constants as variables. The following function demonstrates this: ```python def other_jit(f):     (f)     def wrapper(*args):         jaxpr = jax.make_jaxpr(f)(*args)         return jax.jit(lambda c, *a: jax.core.eval_jaxpr(jaxpr.jaxpr, c, *a))(jaxpr.consts, *args)     return wrapper ``` Now, using `other_jit(f)()` instead of `jax.jit(f)()` prevents the issue. I was wondering if this is intended behavior. Wouldn't it be a better solution in most cases to always treat constants as variables while compiling, to prevent constant folding from slowing down compilations? In realworld scenarios, using (a generalized version of) the `other_jit` function I presented here can significantly reduce compilation times from a few minutes to just seconds. What's your opinion on this? I would appreciate any help or suggestions.  System info (python version, jaxlib version, accelerator, etc.) cpu jax 0.4.28 jaxlib 0.4.28",2024-05-19T20:36:43Z,bug,open,1,17,https://github.com/jax-ml/jax/issues/21300,I'm aware that `other_jit` recompiles the function with every call  in realword scenarios it would be better to save and reuse compiled functions.,"Some explanation why it depends on the shape: We have a heuristic to not apply constant folding if the operand shape is too large. The cutoff is 45 * 1000 * 1000 elements. In the ""fast"" cases we don't apply constant folding.","Thanks for the reply! It also seems to depend on the operation itself. For examle, with a double `vmap` (i.e. sum over last axis), it happens, but it doesn't happen when using only one vmap (i.e. sum over last two axes): ```python import jax import jax.numpy as jnp import jax.core jax.config.update(""jax_enable_x64"", True) jax.config.update(""jax_platforms"", ""cpu"") v = jnp.zeros((200000, 10, 10)) def f():     return jax.vmap(jax.vmap(jnp.sum))(v) def g():     return jax.vmap(jnp.sum)(v) print(""f"") jax.jit(f)() print(""g"") jax.jit(g)() ``` Maybe it would help to clarify what constant folding is used for / where it makes sense to apply it. As far as I know, it basically means that the compiler evaluates some operations at compile time (to save runtime), **if** all inputs for these operations are known in advance (i.e. if they are ""constants""). I'm wondering why this is so slow  intuitively, I would think that constant folding happens approximately at the speed of `numpy` or uncompiled `jax.numpy`. But it seems to be much slower than that!","For constant folding, the HloEvaluator is used. It is not optimized for speed, but for correctness, as it is used as reference backend in tests. You can see the rules that we have for the ConstantFolding pass here: https://github.com/openxla/xla/blob/main/xla/service/hlo_constant_folding.'t know what the nested jax.vmap would translate to, but I think you can safely assume that fast runtime means that constant folding is not applied. Constant folding only makes sense if what is being constant folded would run several times. If it is run only a single time, then you would be better off without constant folding.","Thanks for clarifying! Would it be a useful addition to `jax.jit` to make it possible to turn this behavior off? Instead, constants could be treated as regular variables (that then get passed to the compiled function), preventing constant folding from ever happening. For example, you could force this with the current API using `jax.make_jaxpr` and `partial_eval`  basically first extracting all constants (i.e. known values) with `make_jaxpr`, then computing as many values as possible using `partial_eval`, and then compiling the remaining jaxpr, using the precomputed values whenever it's called. Maybe this would be a nice addition for those users (like me) who use many and large static arrays (i.e. constants in the context of jit) but don't want constant folding to slow the compilation down.","I am not familiar with the JAX side of things. On XLA side we have a flag that could be used to turn off constant folding: xla_disable_hlo_passes=constant_folding This can be set via the XLA_FLAGS environment variable. So something like os.environ['XLA_FLAGS'] = ""xla_disable_hlo_passes=constant_folding"" from python",Thanks for helping! It would be nice to also have an option like this in `jax.jit` to control this behavior  something like `constant_folding: bool` maybe.,You can do this via `jax.jit(f).lower(*args).compile(compiler_options={'xla_disable_hlo_passes': True})`. We are looking into supporting this as an option to jit but you can do it via the AOT API for now.,"That was a fast comment! When trying this, i get the following error: `jaxlib.xla_extension.XlaRuntimeError: INVALID_ARGUMENT: While setting option xla_disable_hlo_passes, '1' is not a valid string value.`",Ohh sorry you need `'xla_disable_hlo_passes': 'constant_folding'`,"``` RuntimeError: Protocol Buffer reflection usage error:   Method      : google::protobuf::Reflection::SetString   Message type: xla.DebugOptions   Field       : xla.DebugOptions.xla_disable_hlo_passes   Problem     : Field is repeated; the method requires a singular field. ``` The code I used: ```python v = jnp.zeros((200000, 10, 10)) def f():     return jax.vmap(jax.vmap(jnp.sum))(v) jax.jit(f).lower().compile(compiler_options={'xla_disable_hlo_passes': 'constant_folding'}) ```","Hmm, this might require some fixes in the jax code. I'll take a look.","Was there any solution to this issue? I am also getting these warnings when scanning over large tensors. I'd also be happy with some way to silence the warnings and just accept the long compile time but without the terminal spam, but I couldn't find any logger that corresponded to the errors being issued (I tried setting all jax loggers to `logging.ERROR` using `logging.root.manager.loggingDict`).","I just ended up first converting my main functions to jaxpr and then compiling them while treating their constants as dynamic variables. Additionally, this enables you to use partialeval to precompute the constant part of your jaxpr (which can be a lot of saved time if you call it often enough). Sadly, jax does not seem to have any api for automating these processes.", Do you have code snippets to perform this? I am having similar troubles,"my_jit.txt This is (up to some minor differences) what I currently use. I don't exactly know how well the current implementation of jax.jit handles constant folding, but the ""jit"" function in this file will always treat constants that appear during jaxpr tracing as dynamic inputs to jax.jit and thus prevent any constants from appearing during compile time. It should be straightforward to see what this code does. If you enable ""precompute"", your code will run faster (if you have lots of constants in your main function) but may need considerably more memory. Note: As far as I know, jaxprs are considered to be private/semipublic api.","Thank you very much, I finally tested it and it works as intended. I still wish for better workaround in the future ahah"
yi,Can't build jaxlib in GH200 ," Description I'm trying to run some code utilizing my GH200 without success. Unable to build jaxlib for my GPU.   System info (python version, jaxlib version, accelerator, etc.) root:~/jax nvidiasmi Sun May 19 12:13:00 2024        ++  ++ root:~/jax nvcc version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 20052024 NVIDIA Corporation Built on Thu_Mar_28_02:24:28_PDT_2024 Cuda compilation tools, release 12.4, V12.4.131 Build cuda_12.4.r12.4/compiler.34097967_0  the error i get: Error limit reached. 100 errors detected in the compilation of ""external/xla/xla/stream_executor/gpu/redzone_allocator_kernel_cuda.cc"". Compilation terminated. Target //jaxlib/tools:build_gpu_plugin_wheel failed to build INFO: Elapsed time: 7.262s, Critical Path: 4.88s INFO: 73 processes: 73 internal. FAILED: Build did NOT complete successfully ERROR: Build failed. Not running target Traceback (most recent call last):   File ""/root/jax/build/build.py"", line 733, in      main()   File ""/root/jax/build/build.py"", line 727, in main     shell(build_pjrt_plugin_command)   File ""/root/jax/build/build.py"", line 45, in shell     output = subprocess.check_output(cmd)   File ""/usr/lib/python3.10/subprocess.py"", line 421, in check_output     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,   File ""/usr/lib/python3.10/subprocess.py"", line 526, in run     raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command '['/usr/local/bin/bazel', 'run', 'verbose_failures=true', '//jaxlib/tools:build_gpu_plugin_wheel', '', 'output_path=/root/jax/dist', 'jaxlib_git_hash=45a7c22e932fee257016bf0da1022be146ed6095', 'cpu=aarch64', 'cuda_version=12']' returned nonzero exit status 1.",2024-05-19T12:15:48Z,bug NVIDIA GPU,open,0,27,https://github.com/jax-ml/jax/issues/21299,Can you share the compilation errors you're getting in external/xla/xla/stream_executor/gpu/redzone_allocator_kernel_cuda.cc?,"Thanks for the quick reply...  include ""absl/base/call_once.h"" include ""absl/base/const_init.h"" include ""absl/base/thread_annotations.h"" include ""absl/container/node_hash_map.h"" include ""absl/log/check.h"" include ""absl/log/log.h"" include ""absl/status/statusor.h"" include ""absl/strings/string_view.h"" include ""absl/synchronization/mutex.h"" include ""absl/types/span.h"" include ""third_party/gpus/cuda/include/cuda.h"" include ""xla/stream_executor/cuda/cuda_asm_compiler.h"" include ""xla/stream_executor/cuda/cuda_driver.h"" include ""xla/stream_executor/device_memory.h"" include ""xla/stream_executor/gpu/redzone_allocator_kernel.h"" include ""xla/stream_executor/kernel.h"" include ""xla/stream_executor/stream_executor_pimpl.h"" include ""xla/stream_executor/typed_kernel_factory.h"" include ""tsl/platform/statusor.h"" namespace stream_executor { // Maintains a cache of pointers to loaded kernels template  static absl::StatusOr*> LoadKernelOrGetPtr(     StreamExecutor* executor, absl::string_view kernel_name,     absl::string_view ptx, absl::Span cubin_data) {   using KernelPtrCacheKey =       std::tuple;   static absl::Mutex kernel_ptr_cache_mutex(absl::kConstInit);   static auto& kernel_ptr_cache ABSL_GUARDED_BY(kernel_ptr_cache_mutex) =       *new absl::node_hash_map>();   CUcontext current_context = cuda::CurrentContextOrDie();   KernelPtrCacheKey kernel_ptr_cache_key{current_context, kernel_name, ptx};   absl::MutexLock lock(&kernel_ptr_cache_mutex);   auto it = kernel_ptr_cache.find(kernel_ptr_cache_key);   if (it == kernel_ptr_cache.end()) {     TF_ASSIGN_OR_RETURN(TypedKernel loaded,                         (TypedKernelFactory::Create(                             executor, kernel_name, ptx, cubin_data)));     it =         kernel_ptr_cache.emplace(kernel_ptr_cache_key, std::move(loaded)).first;   }   CHECK(it != kernel_ptr_cache.end());   return &it>second; } // PTX blob for the function which checks that every byte in // input_buffer (length is buffer_length) is equal to redzone_pattern. // // On mismatch, increment the counter pointed to by out_mismatch_cnt_ptr. // // Generated from: // __global__ void redzone_checker(unsigned char* input_buffer, //                                 unsigned char redzone_pattern, //                                 unsigned long long buffer_length, //                                 int* out_mismatched_ptr) { //   unsigned long long idx = threadIdx.x + blockIdx.x * blockDim.x; //   if (idx >= buffer_length) return; //   if (input_buffer[idx] != redzone_pattern) atomicAdd(out_mismatched_ptr, 1); // } // // Code must compile for the oldest GPU XLA may be compiled for. static const char* redzone_checker_ptx = R""( .version 4.2 .target sm_30 .address_size 64 .visible .entry redzone_checker(   .param .u64 input_buffer,   .param .u8 redzone_pattern,   .param .u64 buffer_length,   .param .u64 out_mismatch_cnt_ptr ) {   .reg .pred   %p;   .reg .b16   %rs;   .reg .b32   %r;   .reg .b64   %rd;   ld.param.u64   %rd6, [buffer_length];   ld.param.u64   %rd4, [input_buffer];   cvta.to.global.u64   %rd2, %rd4;   add.s64   %rd7, %rd2, %rd3;   ld.global.u8   %rs2, [%rd7];   setp.eq.s16   %p2, %rs2, %rs1;   @%p2 bra   LBB6_3;   ld.param.u64   %rd5, [out_mismatch_cnt_ptr];   ld.param.u8   %rs1, [redzone_pattern];   ld.param.u64   %rd4, [input_buffer];   cvta.to.global.u64   %rd2, %rd4;   add.s64   %rd7, %rd2, %rd3;   ld.global.u8   %rs2, [%rd7];   setp.eq.s16   %p2, %rs2, %rs1;   @%p2 bra   LBB6_3;   ld.param.u64   %rd5, [out_mismatch_cnt_ptr];   cvta.to.global.u64   %rd1, %rd5;   atom.global.add.u32   %r5, [%rd1], 1; LBB6_3:   ret; } )""; absl::StatusOr GetComparisonKernel(     StreamExecutor* executor, GpuAsmOpts gpu_asm_opts) {   absl::Span compiled_ptx = {};   absl::StatusOr> compiled_ptx_or =       CompileGpuAsmOrGetCached(executor>device_ordinal(), redzone_checker_ptx,                                gpu_asm_opts);   if (compiled_ptx_or.ok()) {     compiled_ptx = compiled_ptx_or.value();   } else {     static absl::once_flag ptxas_not_found_logged;     absl::call_once(ptxas_not_found_logged, [&]() {       LOG(WARNING) , uint8_t, uint64_t,                             DeviceMemory>(       executor, ""redzone_checker"", redzone_checker_ptx, compiled_ptx); } }  // namespace stream_executor   .reg .b16   %rs;                         ",This snippet doesn't contain any compilation errors AFAICT. Can you upload the output of the compiler to a gist?,I'm sorry but I don't understand the request. Can you be more specific and include the linux terminal commands you want me to run?,"The message you posted initially ``` Error limit reached. 100 errors detected in the compilation of ""external/xla/xla/stream_executor/gpu/redzone_allocator_kernel_cuda.cc"". ``` is usually preceded by compilation error messages, describing what went wrong while compiling jaxlib. If you upload the full output of `build.py`, that would include the error messages as well.",cc :  ,"This is what I get now (I'm in a different docker imae now): (base) root:~/jax python3 build/build.py enable_cuda build_gpu_plugin gpu_plugin_cuda_version=12      _   _  __  __      / ___ \/  \  \___/_/   \/_/\_\ Bazel binary path: ./bazel6.5.0linuxarm64 Bazel version: 6.5.0 Python binary path: /root/miniconda3/bin/python3 Python version: 3.12 Use clang: no MKLDNN enabled: yes Target CPU: aarch64 Target CPU features: release CUDA enabled: yes NCCL enabled: yes ROCm enabled: no Building XLA and installing it in the jaxlib source tree... ./bazel6.5.0linuxarm64 run verbose_failures=true //jaxlib/tools:build_wheel  output_path=/root/jax/dist jaxlib_git_hash=ffdb9bb0b0755e66f55995cafa2cf0946ed66598 cpu=aarch64 skip_gpu_kernels INFO: Options provided by the client:   Inherited 'common' options: isatty=0 terminal_columns=80 INFO: Reading rc options for 'run' from /root/jax/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'run' from /root/jax/.bazelrc:   Inherited 'build' options: nocheck_visibility apple_platform_type=macos macos_minimum_os=10.14 announce_rc define open_source_build=true spawn_strategy=standalone enable_platform_specific_config experimental_cc_shared_library define=no_aws_support=true define=no_gcp_support=true define=no_hdfs_support=true define=no_kafka_support=true define=no_ignite_support=true define=grpc_no_ares=true define=tsl_link_protobuf=true c opt config=short_logs copt=DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir. //xla/python:enable_gpu=false INFO: Reading rc options for 'run' from /root/jax/.jax_configure.bazelrc:   Inherited 'build' options: strategy=Genrule=standalone config=mkl_open_source_only config=cuda config=cuda_plugin repo_env HERMETIC_PYTHON_VERSION=3.12 INFO: Found applicable config definition build:short_logs in file /root/jax/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:mkl_open_source_only in file /root/jax/.bazelrc: define=tensorflow_mkldnn_contraction_kernel=1 INFO: Found applicable config definition build:cuda in file /root/jax/.bazelrc: repo_env TF_NEED_CUDA=1 repo_env TF_NCCL_USE_STUB=1 action_env TF_CUDA_COMPUTE_CAPABILITIES=sm_50,sm_60,sm_70,sm_80,compute_90 crosstool_top=//crosstool:toolchain //:enable_cuda //xla/python:enable_gpu=true //xla/python:jax_cuda_pip_rpaths=true define=xla_python_enable_gpu=true linkopt=Wl,disablenewdtags INFO: Found applicable config definition build:cuda_plugin in file /root/jax/.bazelrc: //xla/python:enable_gpu=false define=xla_python_enable_gpu=false INFO: Found applicable config definition build:linux in file /root/jax/.bazelrc: config=posix copt=Wnounknownwarningoption copt=Wnostringoptruncation copt=Wnoarrayparameter INFO: Found applicable config definition build:posix in file /root/jax/.bazelrc: copt=fvisibility=hidden copt=Wnosigncompare cxxopt=std=c++17 host_cxxopt=std=c++17 Loading:  INFO: Repository local_config_cuda instantiated at:   /root/jax/WORKSPACE:45:15: in    /root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/xla/workspace2.bzl:121:19: in workspace   /root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/tsl/workspace2.bzl:601:19: in workspace   /root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/tsl/workspace2.bzl:72:19: in _tf_toolchains Repository rule cuda_configure defined at:   /root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/tsl/third_party/gpus/cuda_configure.bzl:1542:33: in  ERROR: An error occurred during the fetch of repository 'local_config_cuda':    Traceback (most recent call last):         File ""/root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/tsl/third_party/gpus/cuda_configure.bzl"", line 1491, column 38, in _cuda_autoconf_impl                 _create_local_cuda_repository(repository_ctx)         File ""/root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/tsl/third_party/gpus/cuda_configure.bzl"", line 1040, column 35, in _create_local_cuda_repository                 cuda_config = _get_cuda_config(repository_ctx)         File ""/root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/tsl/third_party/gpus/cuda_configure.bzl"", line 716, column 30, in _get_cuda_config                 config = find_cuda_config(repository_ctx, [""cuda"", ""cudnn""])         File ""/root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/tsl/third_party/gpus/cuda_configure.bzl"", line 693, column 26, in find_cuda_config                 exec_result = execute(repository_ctx, [python_bin, repository_ctx.attr._find_cuda_config] + cuda_libraries)         File ""/root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/tsl/third_party/remote_config/common.bzl"", line 230, column 13, in execute                 fail( Error in fail: Repository command failed Could not find any cuda.h matching version '' in any subdirectory:         ''         'include'         'include/cuda'         'include/*linuxgnu'         'extras/CUPTI/include'         'include/cuda/CUPTI'         'local/cuda/extras/CUPTI/include'         'targets/x86_64linux/include' of:         '/lib'         '/usr'         '/usr/lib/aarch64linuxgnu'         '/usr/lib/aarch64linuxgnu/libfakeroot' ERROR: /root/jax/WORKSPACE:45:15: fetching cuda_configure rule //external:local_config_cuda: Traceback (most recent call last):         File ""/root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/tsl/third_party/gpus/cuda_configure.bzl"", line 1491, column 38, in _cuda_autoconf_impl                 _create_local_cuda_repository(repository_ctx)         File ""/root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/tsl/third_party/gpus/cuda_configure.bzl"", line 1040, column 35, in _create_local_cuda_repository                 cuda_config = _get_cuda_config(repository_ctx)         File ""/root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/tsl/third_party/gpus/cuda_configure.bzl"", line 716, column 30, in _get_cuda_config                 config = find_cuda_config(repository_ctx, [""cuda"", ""cudnn""])         File ""/root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/tsl/third_party/gpus/cuda_configure.bzl"", line 693, column 26, in find_cuda_config                 exec_result = execute(repository_ctx, [python_bin, repository_ctx.attr._find_cuda_config] + cuda_libraries)         File ""/root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/tsl/third_party/remote_config/common.bzl"", line 230, column 13, in execute                 fail( Error in fail: Repository command failed Could not find any cuda.h matching version '' in any subdirectory:         ''         'include'         'include/cuda'         'include/*linuxgnu'         'extras/CUPTI/include'         'include/cuda/CUPTI'         'local/cuda/extras/CUPTI/include'         'targets/x86_64linux/include' of:         '/lib'         '/usr'         '/usr/lib/aarch64linuxgnu'         '/usr/lib/aarch64linuxgnu/libfakeroot' INFO: Repository rules_cc instantiated at:   /root/jax/WORKSPACE:48:15: in    /root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/xla/workspace1.bzl:12:19: in workspace   /root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/tsl/workspace1.bzl:30:14: in workspace   /root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/com_github_grpc_grpc/bazel/grpc_deps.bzl:158:21: in grpc_deps Repository rule http_archive defined at:   /root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/bazel_tools/tools/build_defs/repo/http.bzl:372:31: in  ERROR: Skipping '//xla/python:enable_gpu': no such package '//cuda': Repository command failed Could not find any cuda.h matching version '' in any subdirectory:         ''         'include'         'include/cuda'         'include/*linuxgnu'         'extras/CUPTI/include'         'include/cuda/CUPTI'         'local/cuda/extras/CUPTI/include'         'targets/x86_64linux/include' of:         '/lib'         '/usr'         '/usr/lib/aarch64linuxgnu'         '/usr/lib/aarch64linuxgnu/libfakeroot' WARNING: Target pattern parsing failed. ERROR: //xla/python:enable_gpu :: Error loading option //xla/python:enable_gpu: no such package '//cuda': Repository command failed Could not find any cuda.h matching version '' in any subdirectory:         ''         'include'         'include/cuda'         'include/*linuxgnu'         'extras/CUPTI/include'         'include/cuda/CUPTI'         'local/cuda/extras/CUPTI/include'         'targets/x86_64linux/include' of:         '/lib'         '/usr'         '/usr/lib/aarch64linuxgnu'         '/usr/lib/aarch64linuxgnu/libfakeroot' Traceback (most recent call last):   File ""/root/jax/build/build.py"", line 733, in      main()   File ""/root/jax/build/build.py"", line 699, in main     shell(build_cpu_wheel_command)   File ""/root/jax/build/build.py"", line 45, in shell     output = subprocess.check_output(cmd)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/root/miniconda3/lib/python3.12/subprocess.py"", line 466, in check_output     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/root/miniconda3/lib/python3.12/subprocess.py"", line 571, in run     raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command '['./bazel6.5.0linuxarm64', 'run', 'verbose_failures=true', '//jaxlib/tools:build_wheel', '', 'output_path=/root/jax/dist', 'jaxlib_git_hash=ffdb9bb0b0755e66f55995cafa2cf0946ed66598', 'cpu=aarch64', 'skip_gpu_kernels']' returned nonzero exit status 2.","Okay, from this it looks like your CUDA installation is missing development headers: ``` Could not find any cuda.h matching version '' in any subdirectory: ```","JAXToolbox has nightly JAX container for ARM: https://github.com/NVIDIA/JAXToolbox For example: `ghcr.io/nvidia/jax:jax` for the latest nightly. If you want to build JAX yourself, this container already contain cuda: docker pull nvidia/cuda:12.4.1cudnndevelubuntu22.04 I'm mostly always using those 2 containers for development in JAX."," thanks, I tried those two options without success. I'm using a GH200 and I'm trying to use jax with the GPU, but it always fails. ","I'll also note that we (JAX) release CUDA arm wheels on pypi which should just work on GH200. Try: ``` pip install jax jaxlib jaxcuda12plugin jaxcuda12pjrt ``` (The more usual `pip install jax[cuda12]` won't work because NVIDIA doesn't release ARM wheels of CUDA, last I checked.)","We released ARM wheel last week. But it isn't tested. So let's try jax[cuda12] ``` docker run it gpus all ubuntu aptget update; aptget install y python3pip python3.12venv python3 m venv path/to/venv source path/to/venv/bin/activate pip install jax[cuda12]  works python3 c ""import jax; jax.numpy.zeros(3)""  fail with cudnn init error. ``` This installed cudnn 9.1.1. cudnn 8 isn't supported on GraceHopper to my knowledge.  Does the JAX wheel for ARM are also build with cudnn 8? Any idea when the cudnn 9 version can be created?","Thanks for the update, I'll check it right away"," After executing  `pip install jax jaxlib jaxcuda12plugin jaxcuda12pjrt ` I'm trying to run the following code: ``` import os os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""   Specify the index of the GPU you want to use import jax import jax.numpy as jnp def main():      Explicitly place arrays on GPU using jax.device_put     gpu_device = jax.devices(""gpu"")[0]   Use the first GPU     a = jax.random.normal(jax.random.PRNGKey(0), (size, size))     b = jax.random.normal(jax.random.PRNGKey(1), (size, size))     a_gpu = jax.device_put(a, device=gpu_device)     b_gpu = jax.device_put(b, device=gpu_device)      Run matrix multiplication on GPU     result = jnp.dot(a_gpu, b_gpu)      Print the result     print(""Result of matrix multiplication:"")     print(result) if __name__ == ""__main__"":     main() ``` but get `RuntimeError: Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices. (you may need to uninstall the failing plugin package, or set JAX_PLATFORMS=cpu to skip this backend.)`", does this mean I can't yet run jax with GPU acceleration on GH200?,It is possible. Can you give the exact command line you use to start the docker container? What is the output of nvidiasmi in it? Can you try the jax container we provide? It should work and won't ask you to compile JAX.,Last week I used the docker jax:jax here: https://github.com/NVIDIA/JAXToolbox I don't mind trying it again.  or do u mean nvcr.io/nvidia/jax:24.04maxtextpy3 (from here: https://catalog.ngc.nvidia.com/orgs/nvidia/containers/jax)?,> Last week I used the docker jax:jax here: https://github.com/NVIDIA/JAXToolbox I don't mind trying it again. or do u mean nvcr.io/nvidia/jax:24.04maxtextpy3 (from here: https://catalog.ngc.nvidia.com/orgs/nvidia/containers/jax)?  ,``` $ docker pull nvcr.io/nvidia/jax:24.04maxtextpy3 24.04maxtextpy3: Pulling from nvidia/jax no matching manifest for linux/arm64/v8 in the manifest list entries ```  ,``` gilad:~$ docker pull ghcr.io/nvidia/jax:jax  jax: Pulling from nvidia/jax ``` works  ,"> ``` > gilad:~$ docker pull ghcr.io/nvidia/jax:jax  > jax: Pulling from nvidia/jax > ``` >  > works  gilad:~$ docker run it gpus all ghcr.io/nvidia/jax:jax ========== == CUDA == ========== CUDA Version 12.4.1 Container image Copyright (c) 20162023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license: https://developer.nvidia.com/ngc/nvidiadeeplearningcontainerlicense A copy of this license is made available in this container at /NGCDLCONTAINERLICENSE for your convenience. WARNING: Your shm is currenly less than 1GB. This may cause SIGBUS errors. To avoid this problem, you can manually set the shm size in docker with: docker run ... shmsize=1g ... root:/ nvcc version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 20052024 NVIDIA Corporation Built on Thu_Mar_28_02:24:28_PDT_2024 Cuda compilation tools, release 12.4, V12.4.131 Build cuda_12.4.r12.4/compiler.34097967_0 root:/ nvidiasmi Tue May 21 17:27:31 2024        ++  ++ root:/ "," I still get: root:~ python jax_program.py  20240521 18:06:49.257055: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_SYSTEM_NOT_READY: system not yet initialized Traceback (most recent call last):   File ""/usr/local/lib/python3.10/distpackages/jax/_src/xla_bridge.py"", line 679, in backends     backend = _init_backend(platform)   File ""/usr/local/lib/python3.10/distpackages/jax/_src/xla_bridge.py"", line 761, in _init_backend     backend = registration.factory()   File ""/usr/local/lib/python3.10/distpackages/jax/_src/xla_bridge.py"", line 509, in factory     return xla_client.make_c_api_client(plugin_name, options, None)   File ""/usr/local/lib/python3.10/distpackages/jaxlib/xla_client.py"", line 190, in make_c_api_client     return _xla.get_c_api_client(plugin_name, options, distributed_client) jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: No visible GPU devices. During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""/root/jax_program.py"", line 23, in      main()   File ""/root/jax_program.py"", line 9, in main     gpu_device = jax.devices(""gpu"")[0]   Use the first GPU   File ""/usr/local/lib/python3.10/distpackages/jax/_src/xla_bridge.py"", line 872, in devices     return get_backend(backend).devices()   File ""/usr/local/lib/python3.10/distpackages/jax/_src/xla_bridge.py"", line 806, in get_backend     return _get_backend_uncached(platform)   File ""/usr/local/lib/python3.10/distpackages/jax/_src/xla_bridge.py"", line 786, in _get_backend_uncached     bs = backends()   File ""/usr/local/lib/python3.10/distpackages/jax/_src/xla_bridge.py"", line 695, in backends     raise RuntimeError(err_msg) RuntimeError: Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices. (you may need to uninstall the failing plugin package, or set JAX_PLATFORMS=cpu to skip this backend.)","From the output of nvidiasmi, the issues seem to be that MIG is enabled, but no MIG ""instance"" is created. If exact, that would make all software fail on that node. Can you ask your admins how they setup MIG and how to have a MIG instance created?"," u we're right! My colleague fixed the issue: ``` (base) nikola:~$  sudo nvidiasmi mig lgi ++  ++ (base) nikola:~$ echo $CUDA_VISIBLE_DEVICES (base) nikola:~$ nvidiasmi L GPU 0: NVIDIA GH200 480GB (UUID: GPUd8731c65c898919e74c9286b27400dac)   MIG 7g.96gb     Device  0: (UUID: MIG7baedeb1c0d753ba99262e341a42b470) ``` But now when I run the following code: ``` import os os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""   Specify the index of the GPU you want to use import jax import jax.numpy as jnp  Let's define a simple matrix multiplication function def matmul_on_gpu(a, b):     return jnp.dot(a, b)  Main function to demonstrate GPU acceleration def main():      Create some random matrices     size = 1000     a = jax.random.normal(jax.random.PRNGKey(0), (size, size))     b = jax.random.normal(jax.random.PRNGKey(1), (size, size))      Run matrix multiplication on GPU     result = matmul_on_gpu(a, b)      Print the result     print(""Result of matrix multiplication:"")     print(result) if __name__ == ""__main__"":     main() ``` I get the following error: ``` root:~ python jax_program.py  20240522 19:00:07.301687: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:474] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR 20240522 19:00:07.301822: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:478] Memory usage: 100780867584 bytes free, 101468602368 bytes total. 20240522 19:00:07.302174: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:474] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR 20240522 19:00:07.302277: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:478] Memory usage: 100780867584 bytes free, 101468602368 bytes total. jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/root/jax_program.py"", line 26, in      main()   File ""/root/jax_program.py"", line 15, in main     a = jax.random.normal(jax.random.PRNGKey(0), (size, size))   File ""/usr/local/lib/python3.10/distpackages/jax/_src/random.py"", line 240, in PRNGKey     return _return_prng_keys(True, _key('PRNGKey', seed, impl))   File ""/usr/local/lib/python3.10/distpackages/jax/_src/random.py"", line 202, in _key     return prng.random_seed(seed, impl=impl)   File ""/usr/local/lib/python3.10/distpackages/jax/_src/prng.py"", line 595, in random_seed     seeds_arr = jnp.asarray(np.int64(seeds))   File ""/usr/local/lib/python3.10/distpackages/jax/_src/numpy/lax_numpy.py"", line 2217, in asarray     return array(a, dtype=dtype, copy=bool(copy), order=order)   type: ignore   File ""/usr/local/lib/python3.10/distpackages/jax/_src/numpy/lax_numpy.py"", line 2172, in array     out_array: Array = lax_internal._convert_element_type(   File ""/usr/local/lib/python3.10/distpackages/jax/_src/lax/lax.py"", line 560, in _convert_element_type     return convert_element_type_p.bind(operand, new_dtype=new_dtype,   File ""/usr/local/lib/python3.10/distpackages/jax/_src/core.py"", line 444, in bind     return self.bind_with_trace(find_top_trace(args), args, params)   File ""/usr/local/lib/python3.10/distpackages/jax/_src/core.py"", line 447, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/usr/local/lib/python3.10/distpackages/jax/_src/core.py"", line 935, in process_primitive     return primitive.impl(*tracers, **params)   File ""/usr/local/lib/python3.10/distpackages/jax/_src/dispatch.py"", line 87, in apply_primitive     outs = fun(*args) jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. ```  is this the cuDNN problem you mentioned? what can I do to check it?",Yes I have cuDNN version 9.1.0.,"Yes, you need to downgrade to CUDNN 8.9 for now. JAX doesn't yet release with CUDNN 9.","Before downgrading, which container do you use and how JAX was installed? If you use the JAXToolbox jax container, you have a good combination of JAX (nightly), cudnn (9.1.1), and CUDA 12.4.1. Did you try with the JAXtoolbox container without setting CUDA_VISIBLE_DEVICES? Why do you try to set it? If there is only 1 GPU, normally JAX will just find and use it. So you don't need to set it. The MIG listing isn't the same as normal GPU. Also, you can't do multigpu across MIGs."
yi,[pallas] align interpreter load/store with masked behaviour," (and adds stride support)  Fixes https://github.com/google/jax/issues/21143 Implements a jittable masked gather/scatter where for load/store/swap any masked indexing does not occur. For load it sets any masked indexing to index to the first element in the array instead. For swap(/store) it also sets masked indexing to the first element (and then deals with special rules to make sure the first element is dealt with correctly)    The currently used dynamic_slices are replaced with explicit index materialisation and gathers/scatters. The advantage of doing it this way is that you can combine it with checkify(f, errors=checkify.index_checks) in interpreter mode to check for any unmasked OOB indexing which is (I think, and believe should be) undefined behaviour. [apologies this is a reopening of a previous request I'd done badly having not checked contributing.md]",2024-05-18T21:10:12Z,pull ready,closed,0,3,https://github.com/jax-ml/jax/issues/21298,"This corrects similar issues to https://github.com/google/jax/pull/21180 , though relating to indexing into MemRefs rather than non evenlydivisible block shapes for chunking arrays",This PR is now solely aimed at fixing dynamic slice store/load operations in the interpreter identified in https://github.com/google/jax/issues/21143. In the interpreter it pads the arrays with uninitialised values so dynamic slices are never pushed back into the 'true' array. Tests for both load and store are added. I've confirmed they error pre this commit and are fixed by this change (as is the original bug report). Separate PRs may be made to solve separate issues:  Special rules for OOB and NaN checking in checkify for masked store/load  Slice strides not equal to 1,Comments implemented and now should be ready
yi,Add RegularGridInterpolator to generated API docs,"In responding to CC(未找到相关数据), I noticed that `RegularGridInterpolator` isn't currently listed in the API docs. I know that `scipy.interpolate` is out of scope, but since we do currently provide this wrapper, it seems like it makes sense to include it in the docs! _Edit:_ I note that this docstring could also probably use some work. Happy to flesh it out as part of this PR if anyone thinks that's worthwhile.",2024-05-17T15:25:07Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/21283
yi,simple pallas kernel hangs when input size exceeds some threshold," Description The following simple pallas kernel that copies an array hangs indefinitely: ```python import jax import jax.numpy as jnp from jax.experimental import pallas as pl def copy_kernel(     src,     dst ):     def body(i, carry):         dst[i] = src[i]         return carry     _ = jax.lax.fori_loop(         lower=0,         upper=src.shape[0],         body_fun=body,         init_val=None     ) .jit .vmap def copy_func(src):     func = pl.pallas_call(         f=copy_kernel,         out_shape=jax.ShapeDtypeStruct(src.shape, src.dtype)     )     dst = func(src)     return dst if __name__ == '__main__':     batch_size = 2 ** 16     seq_length = 2 ** 16     dtype = jnp.float32      dtype = jnp.bfloat16     src = jnp.zeros((batch_size, seq_length), dtype=dtype)     print(f'Array elements: {src.size}')     print(f'Array size: {src.nbytes / 1e9:.4f}GB')     dst = copy_func(src)     dst.block_until_ready() ``` Program output: ``` 20240517 10:18:51.191896: W external/xla/xla/service/gpu/nvptx_compiler.cc:760] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.4.131). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIAprovided CUDA forward compatibility packages. Array elements: 4294967296 Array size: 17.1799GB ``` I remember reading somewhere that the above warning can be ignored so I think it this is unlikely related to the issue I'm seeing. It looks like as long as `batch_size * seq_length <= 2 ** 31` then the program will not get stuck. For example, if I change either `batch size` or `seq_length` from `2 ** 16` to `2 ** 15` then it works fine. However, changing `dtype` from `float32` to `bfloat16` does not fix the problem. Plus I'm using A100 80GB, with `batch_size = seq_length = 2 ** 16, dtype=float32` the array only takes roughly 17GB. So it perhaps has nothing to do with memory.  Also when it hangs both GPU and CPU utilization is zero.  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.28 jaxlib: 0.4.28 numpy:  1.26.4 python: 3.10.4 (main, Mar 31 2022, 08:41:55) [GCC 7.5.0] jax.devices (1 total, 1 local): [cuda(id=0)] process_count: 1 platform: uname_result(system='Linux', node='cng020.server.mila.quebec', release='5.15.0101generic', version=' CC(add travis ci support)Ubuntu SMP Tue Mar 5 20:16:58 UTC 2024', machine='x86_64') $ nvidiasmi Fri May 17 10:13:08 2024 ++  ++ ```",2024-05-17T14:17:51Z,bug pallas,open,0,3,https://github.com/jax-ml/jax/issues/21282,"I also tested `jax==0.4.25` with cuda 11 with which I don't see the `ptxas` version warning (but the kernel still hangs indefinitely), so it likely has nothing to do with that","It looks like something overflows and the loop iterates forever, but I'm not sure where the overflow actually happens.","Hi lin, Testing the provided repro on a GCP VM with A100 40GB GPU with JAX 0.5.0 resulted in a `RESOURCE EXHAUSTED` error. !Image Setting the `XLA_PYTHON_CLIENT_PREALLOCATE` flag to `'false'` and `XLA_PYTHON_CLIENT_ALLOCATOR` flag set to `'platform'` resolved the issue. !Image For `batch_size=2**16` and `seq_length=2**16`, the GPU memory usage appears to be around `33GB` (see screenshot below). !Image Thank you."
yi,Add more informative error message for invalid `unroll` parameter in `lax.scan`,"As reported in CC(`jax.lax.scan` function yield `ZeroDivisionError: integer division or modulo by zero` error when using unroll=0), setting `unroll=0` in `lax.scan` resulted in an uninformative `ZeroDivisionError`. This PR adds a check which raises a `ValueError` for `unroll<=0`. One interesting (to me!) technical point is that a more informative error is raised when `jax_enable_checks` is on (ref), but since this is off by default, I think it is sensible to have a better error message. That being said, I'm not sure if the way I'm turning `jax_enable_checks` off in the test is appropriate.",2024-05-15T18:36:06Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/21247
yi,Jax not recognizing GPU. ," Description I am trying to reproduce the study of this work from Google DeepMind by running Jax on  NVIDIA GPU (Driver: 550.67) and CUDA (12.4), but it returns ""No GPU/TPU found, falling back to CPU."" I tried bumping up the jax and jaxlib versiosn to 0.4.28 (the latest version) from 0.4.16 (the version listed in requirements.txt) and also upgraded flax to 0.8.3 from 0.7.4. These changes eliminate the warning and Jax seems to recognize the GPU devices but the computation is still very slow. How do I fix this?  System info (python version, jaxlib version, accelerator, etc.) $ python3 c ""import jax; jax.print_environment_info()"" jax:    0.4.28 jaxlib: 0.4.28 numpy:  1.26.0 python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] jax.devices (10 total, 10 local): [cuda(id=0) cuda(id=1) ... cuda(id=8) cuda(id=9)] process_count: 1 platform: uname_result(system='Linux', node='nebula', release='5.15.0107generic', version=' CC(add oss test instructions, fix conv grad bug)Ubuntu SMP Fri Apr 26 12:26:49 UTC 2024', machine='x86_64') $ nvidiasmi Wed May 15 09:53:43 2024 ++  ++",2024-05-15T14:54:29Z,bug,open,0,3,https://github.com/jax-ml/jax/issues/21240,"Hmm. I'm not sure this is an actionable report. You upgraded and the original problem was fixed, it seems? You say the model is running slowly. Can you say more? Knowing nothing about that particular model are you seeing different performance characteristics to the original model authors? How so?",I resolved this issue. ,"Hi guan  Please feel free to close the issue, if resolved. Thank you."
rag,Numerical differences between shardings in random algorithm," Description We are seeing numerical differences between shardings in random number initialization on GPUs. For example, if I have a mesh of DP, FSDP, TP , based on what no of devices I allocate to each of these axes the numerical output of my initialization changes drastically. As a result of this when we are using TP we are seeing divergences in the network.  System info (python version, jaxlib version, accelerator, etc.) ``` `jax:    0.4.27.dev20240514 jaxlib: 0.4.27.dev20240420 numpy:  1.26.4 python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] jax.devices (8 total, 8 local): [cuda(id=0) cuda(id=1) ... cuda(id=6) cuda(id=7)] process_count: 1 platform: uname_result(system='Linux', node='ipp12023.nvidia.com', release='5.15.088generic', version=' CC(make it easy to print jaxprs)Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023', machine='x86_64') $ nvidiasmi Tue May 14 23:37:29 2024 ++  ++ `` The reproduce unittest is as below, it is required to run on a node with 8GPUs `import jax.numpy as jnp import jax from jax.experimental import mesh_utils as jax_mesh_utils from jax.experimental.pjit import pjit from jax.sharding import PartitionSpec as P from jax.sharding import Mesh MESH_DATA_AXIS = 'data' MESH_TENSOR_AXIS = 'tensor' MESH_FSDP_AXIS=""pipeline""  create an FSDP mesh ici_mesh = (2, 4, 1)   DP, FSDP, TP dcn_mesh = (1, 1, 1)   DP, FSDP, TP devices = jax_mesh_utils.create_hybrid_device_mesh(ici_mesh, dcn_mesh) fsdp_mesh = Mesh(devices, (MESH_DATA_AXIS, MESH_FSDP_AXIS, MESH_TENSOR_AXIS)) print(fsdp_mesh.shape)   (2, 8, 1)  create an FSDP, TP mesh ici_mesh = (1, 4, 2)   DP, FSDP, TP dcn_mesh = (1, 1, 1)   DP, FSDP, TP devices = jax_mesh_utils.create_hybrid_device_mesh(ici_mesh, dcn_mesh) fsdp_tp_mesh = Mesh(devices, (MESH_DATA_AXIS, MESH_FSDP_AXIS, MESH_TENSOR_AXIS)) print(fsdp_tp_mesh.shape)   (1, 4, 4)  create an FSDP, TP, DP mesh ici_mesh = (2, 2, 2)   DP, FSDP, TP dcn_mesh = (1, 1, 1)   DP, FSDP, TP devices = jax_mesh_utils.create_hybrid_device_mesh(ici_mesh, dcn_mesh) fsdp_tp_dp_mesh = Mesh(devices, (MESH_DATA_AXIS, MESH_FSDP_AXIS, MESH_TENSOR_AXIS)) print(fsdp_tp_dp_mesh.shape)   (2, 2, 4)  generate the data batch_size = 32 seq_len = 8192 n_heads = 32 head_dim = 128 emb_dim = 4096 DATA_SUBMESH = (MESH_DATA_AXIS, MESH_FSDP_AXIS) def gen_data_fn():     key = jax.random.PRNGKey(43)     scale = 0.05     activations = scale * jax.random.normal(key, shape=(batch_size, seq_len, emb_dim), dtype=jnp.bfloat16)     weights = scale * jax.random.normal(key, shape=(emb_dim, n_heads, head_dim), dtype=jnp.bfloat16)     return activations, weights data_fn = pjit(     gen_data_fn,     out_shardings=(P(DATA_SUBMESH, None, MESH_TENSOR_AXIS), P(MESH_FSDP_AXIS, MESH_TENSOR_AXIS, None)), )  fsdp utputs with fsdp_mesh:     act1, weights1 = data_fn() with fsdp_tp_mesh:     act2, weights2 = data_fn() with fsdp_tp_dp_mesh:     act3, weights3 = data_fn()  diff b/w fsdp and fsdp,tp def get_diffs(x, y):     abs_diff = jnp.abs(x  y)     max_difference = round(jnp.max(abs_diff), 5)     min_difference = round(jnp.min(abs_diff), 5)     avg_difference = round(jnp.mean(abs_diff), 5)     return max_difference, min_difference, avg_difference max_diff, min_diff, avg_diff = jax.jit(get_diffs)(act1, act2) print(f""Differences b/w FSDP and FSDP,TP: Max  {max_diff}, Min  {min_diff}, Average  {avg_diff}"") max_diff, min_diff, avg_diff = jax.jit(get_diffs)(act1, act3) print(f""Differences b/w FSDP and FSDP,TP,DP: Max  {max_diff}, Min  {min_diff}, Average  {avg_diff}"") `",2024-05-14T23:43:38Z,bug,open,0,3,https://github.com/jax-ml/jax/issues/21232,"This is fixed by upgrading to partitionable threefry, e.g. by adding the following line to the top of the file (after imports): ```python jax.config.update('jax_threefry_partitionable', True) ``` See https://github.com/google/jax/discussions/18480 for more on the upgrade (which was delayed a bit, but is still planned).","IIUC this is a bug (unintended behavior) even with jax_threefry_partitionable=False, and also we don't yet know what's causing this bug. Good to know that setting jax_threefry_partitionable=True fixes it though!","Yes, I consider it a bug as well, but still undiagnosed."
rag,[jax:mosaic-gpu] Test type conversion for tiled fragment array,[jax:mosaicgpu] Test type conversion for tiled fragment array,2024-05-13T01:06:57Z,,closed,0,1,https://github.com/jax-ml/jax/issues/21199,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
yi,XLA errors with splash attention mask functions that use modulo," Description I am trying to use splash attention with a custom ComputableMask that uses the modulo operation. My first attempt is ``` class ModuloMask(splash_attention_mask._ComputableMask):   def __init__(       self,       input_size:int,       shard_count: int = 1,   ):     def mask_function(q_ids, kv_ids):         q_ids = q_ids % 48         kv_ids = kv_ids % 48         return q_ids <= kv_ids      super().__init__(         shape=(input_size, input_size),         mask_function=mask_function,         shard_count=shard_count,     ) ``` but this gives me a LoweringException: `LoweringException: Exception while lowering eqn:  a:bool[128,128] = ne b c` and `NotImplementedError: Mixed dtype operands in cmp` I modified this to ``` class ModuloMask(splash_attention_mask._ComputableMask):   def __init__(       self,       input_size:int,       shard_count: int = 1,   ):     width_np = cast(48, np.uint32)     def mask_function(q_ids, kv_ids):       if isinstance(q_ids, np.ndarray):         q_ids = q_ids % 48         kv_ids = kv_ids % 48         return q_ids <= kv_ids       else:         q_ids = jnp.mod(q_ids, width_np)         kv_ids = jnp.mod(kv_ids, width_np)         return q_ids <= kv_ids     super().__init__(         shape=(input_size, input_size),         mask_function=mask_function,         shard_count=shard_count,     ) ``` but then I get `ValueError: safe_map() argument 3 is shorter than argument 1` in `~/.venv/lib/python3.11/sitepackages/jax/_src/pallas/pallas_call.py:418` (corresponding to `~/.venv/lib/python3.11/sitepackages/jax/_src/pallas/pallas_call.py:560`). If I get rid of the modulo and just return `return q_ids <= kv_ids` in the mask function, it works. The full source to reproduce is: ``` import jax import jax.numpy as jnp from jax.experimental.pallas.ops.tpu.splash_attention import splash_attention_mask from jax.experimental.pallas.ops.tpu.splash_attention import splash_attention_kernel import numpy as np def cast(x, dtype):   return (x * jnp.ones([])).astype(dtype) class ModuloMask(splash_attention_mask._ComputableMask):   def __init__(       self,       input_size:int,       shard_count: int = 1,   ):     width_np = cast(48, np.uint32)     def mask_function(q_ids, kv_ids):       if isinstance(q_ids, np.ndarray):         q_ids = q_ids % 48         kv_ids = kv_ids % 48         return q_ids <= kv_ids       else:         q_ids = jnp.mod(q_ids, width_np)         kv_ids = jnp.mod(kv_ids, width_np)         return q_ids <= kv_ids     super().__init__(         shape=(input_size, input_size),         mask_function=mask_function,         shard_count=shard_count,     )   def __eq__(self, other: object):     if not isinstance(other, type(self)):         return NotImplemented     return (         self.shape == other.shape         and np.array_equal(self.q_sequence, other.q_sequence)     )   def __hash__(self):     return hash(         (             type(self),             self.shape,             self.q_sequence.tobytes() if self.q_sequence is not None else None,         )     ) B, H, N, D = 4, 4, 768, 256 q = k = v = jnp.zeros([B, H, N, D], jnp.bfloat16) mask = ModuloMask(768) masks = splash_attention_mask.MultiHeadMask(masks=[mask for i in range(q.shape[1])]) splash_kernel = splash_attention_kernel.make_splash_mha_single_device(     mask=masks ) splash_attention_fn = jax.vmap(splash_kernel) splash_attention_fn(     q, k, v ) ``` This bug occurs on `jax==0.4.25` and also `jax==0.4.28`  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.25 jaxlib: 0.4.25 numpy:  1.26.0 python: 3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 9.4.0] jax.devices (8 total, 8 local): [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0) TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1) ... TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0) TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)] process_count: 1 platform: uname_result(system='Linux', node='t1vnd8febb2aw0', release='5.13.01027gcp', version=' CC(Fix the bug in classifier example, batching_test and README)~20.04.1Ubuntu SMP Thu May 26 10:53:08 UTC 2022', machine='x86_64') ```",2024-05-12T21:06:43Z,bug pallas,closed,0,2,https://github.com/jax-ml/jax/issues/21196,Can you try using jax.lax.rem?,"Yes this works, thanks!"
yi,"Raise a runtime error when trying to convert the `jax.Array` wrapped by `jax.core.Token` to a numpy array, as it is an internal implementation detail and the buffer has XLA token shape.","Raise a runtime error when trying to convert the `jax.Array` wrapped by `jax.core.Token` to a numpy array, as it is an internal implementation detail and the buffer has XLA token shape.",2024-05-10T22:55:32Z,,closed,0,0,https://github.com/jax-ml/jax/issues/21182
yi,Add Distributed data loading in a multi-host/multi-process environment doc, PTAL Rendered preview: https://jax21181.org.readthedocs.build/en/21181/distributed_data_loading.html Preview 1: !image Preview 2: !image  fyi https://github.com/google/jax/issues/18585,2024-05-10T22:51:20Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/21181,"Is this notebook meant to be executable in a singleCPU context? If not please add it to `nb_execution_excludepatterns` in `docs/conf.py`, then the readthedocs build will pass."
yi,Register TPU profiler plugin when get_topology_desc is called with tpu platform.,"Register TPU profiler plugin when get_topology_desc is called with tpu platform. This allows the TPU profiler to work with other plugin backends. Tested on a GPU VM: $ pip install U ""jax[tpu]"" f https://storage.googleapis.com/jaxreleases/libtpu_releases.html $ pip install e . $ TPU_SKIP_MDS_QUERY=1 python tests/cross_aot_test.py Running tests under Python 3.10.12: /usr/bin/python [ RUN      ] JaxAotTest.test_tpu_profiler_registered_get_topology_from_devices NOT_FOUND: WARNING: could not determine TPU accelerator type. Set env var `TPU_ACCELERATOR_TYPE` to set manually. TPU runtime may not be properly initialized. === Source Location Trace: === learning/45eac/tfrc/runtime/common_lib.cc:285 NOT_FOUND: WARNING: could not determine TPU worker number. Set env var `TPU_WORKER_ID` to set manually. TPU runtime may not be properly initialized. === Source Location Trace: === learning/45eac/tfrc/runtime/common_lib.cc:285 NOT_FOUND: WARNING: could not determine TPU worker hostnames or internal IP addresses. Set env var `TPU_WORKER_HOSTNAMES` to set manually. TPU runtime may not be properly initialized. === Source Location Trace: === learning/45eac/tfrc/runtime/common_lib.cc:285 learning/45eac/tfrc/runtime/common_lib.cc:341 I0510 00:32:03.063246 130900437979136 cross_aot_test.py:58] Expected to fail to get topology I0510 00:32:03.079923 130900437979136 xla_bridge.py:884] Unable to initialize backend 'cuda':  I0510 00:32:03.080080 130900437979136 xla_bridge.py:884] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig' I0510 00:32:03.089399 130900437979136 xla_bridge.py:884] Unable to initialize backend 'tpu': UNKNOWN: TPU initialization failed: No ba16c7433 device found. W0510 00:32:03.089633 130900437979136 xla_bridge.py:931] An NVIDIA GPU may be present on this machine, but a CUDAenabled jaxlib is not installed. Falling back to cpu. /home/jieying/.local/lib/python3.10/sitepackages/tensorflow/__init__.py:30: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives   import distutils as _distutils 20240510 00:32:03.359597: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 20240510 00:32:03.359652: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 20240510 00:32:03.361368: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20240510 00:32:04.562557: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT [       OK ] JaxAotTest.test_tpu_profiler_registered_get_topology_from_devices  Ran 1 test in 2.549s OK In tests/cross_aot_test.py class JaxAotTest(jtu.JaxTestCase):   def test_tpu_profiler_registered_get_topology_from_devices(self):     try:       _ = topologies.get_topology_desc(           topology_name='fake_topology',           platform='tpu',       )     except xla_extension.XlaRuntimeError:       logging.info('Expected to fail to get topology')     with tempfile.TemporaryDirectory() as tmpdir:       try:         jax.profiler.start_trace(tmpdir)         jax.pmap(lambda x: jax.lax.psum(x + 1, 'i'), axis_name='i')(             jnp.ones(jax.local_device_count())         )       finally:         jax.profiler.stop_trace()       proto_path = glob.glob(           os.path.join(tmpdir, '**/*.xplane.pb'), recursive=True       )       self.assertLen(proto_path, 1)       with open(proto_path[0], 'rb') as f:         proto = f.read()        Sanity check that serialized proto contains host, and Python traces        without deserializing.       self.assertIn(b'/host:metadata', proto)       if jtu.test_device_matches(['tpu']):         self.assertNotIn(b'/device:TPU', proto)       self.assertIn(b'pxla.py', proto)",2024-05-09T23:57:12Z,,closed,0,0,https://github.com/jax-ml/jax/issues/21155
yi,[pallas] align interpreter load/store with masked behaviour (and adds stride support),"Matches behaviour in Triton where for load/store/swap any masked indexing does not occur.  For load it sets any masked indexing to index to the first element in the array instead. For swap(/store) it also sets masked indexing to the first element (and then deals with special rules to make sure the first element is dealt with correctly) The currently used dynamic_slices are replaced with explicit index materialisation and gathers/scatters. The advantage of doing it this way is that you can combine it with `checkify(f, errors=checkify.index_checks)` in interpreter mode to check for any unmasked OOB indexing which is (I think, and believe should be) undefined behaviour.",2024-05-09T09:06:46Z,pallas,closed,0,0,https://github.com/jax-ml/jax/issues/21144
llm,[pallas] Interpreter mismatch for masked OOB indexing," Description For triton (if I have read this correctly) masked load/stores do not occur. So you can request to load/store to an index OOB for ref if that is masked. The current interpreter uses dynamic_slices/dynamic_slice_updates where masked updates are applied. In line with the 'always be in bounds' design in JAX if you index a slice that overruns the edge of the array it will be shifted to be valid (if possible). This leads to a disconnect in interpreter and Pallas outputs. I know Triton is not Pallas, have you changed the desired behaviour for these cases in Pallas?  in which case this isn't a bug but needs documenting. I've added a pull request fixing this with some tests https://github.com/google/jax/pull/21298 Here is a colab minimal reproduction with shifts in load indices. ```python from functools import partial import jax from jax import numpy as jnp, jit from jax.experimental import pallas as pl def masked_load_pallas_kernel(x_ref, o_ref):   i = jnp.array(3)   mask = jnp.arange(x_ref.shape[0]) + i < x_ref.shape[0]   x = pl.load(x_ref, pl.dslice(i, mask.shape[0]), mask=mask, other=1)   o_ref[:] = x (jit, static_argnames=('interpret',)) def masked_load(x: jax.Array, interpret: bool=True):   return pl.pallas_call(masked_load_pallas_kernel,                         out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype),                         interpret = interpret,                         )(x) x = jnp.arange(16) print(f'Input:\nx:\n{x}\n\nOutput:') for interpret in (True, False):   print(f'Interpret: {interpret}\n{masked_load(x, interpret=interpret)}') ``` ``` Input: x: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15] Output: Interpret: True [ 0  1  2  3  4  5  6  7  8  9 10 11 12 1 1 1] Interpret: False [ 3  4  5  6  7  8  9 10 11 12 13 14 15 1 1 1] ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.26 jaxlib: 0.4.26 numpy:  1.25.2 python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] jax.devices (1 total, 1 local): [cuda(id=0)] process_count: 1 platform: uname_result(system='Linux', node='b70fe499e42d', release='6.1.58+', version=' CC(Python 3 compatibility issues) SMP PREEMPT_DYNAMIC Sat Nov 18 15:31:17 UTC 2023', machine='x86_64') $ nvidiasmi Thu May  9 08:55:06 2024        ++  ++ ``` (Problem persists in 0.4.28)",2024-05-09T09:01:44Z,bug pallas,closed,0,0,https://github.com/jax-ml/jax/issues/21143
yi,[XLA:Python] Fix a memory corruption bug in the tp_name attribute of ArrayImpl and PjitFunction for Python 3.10 or earlier.,"[XLA:Python] Fix a memory corruption bug in the tp_name attribute of ArrayImpl and PjitFunction for Python 3.10 or earlier. This works around https://github.com/python/cpython/issues/89478, which was fixed in Python 3.11.",2024-05-09T00:28:21Z,,closed,0,0,https://github.com/jax-ml/jax/issues/21141
yi,"[XLA:Mosaic] Fix infer/apply vector layout rule for terminators (scf::yieldOp, scf::conditionOp).","[XLA:Mosaic] Fix infer/apply vector layout rule for terminators (scf::yieldOp, scf::conditionOp). We should infer layout for each terminator inside its own region and find a compatible layout for a final result if the result is based on terminators from multiple regions like scf::ifOp, scf::whileOp, scf::forOp. If no compatible layout is found, we will fall back to a normalized layout. Finally we also need to ensure the layouts in input, terminator and output are consistent across loops.",2024-05-07T19:08:10Z,,closed,0,0,https://github.com/jax-ml/jax/issues/21113
yi,Fix failing smem_hbm_dma test,"Fixes copy error on test_smem_hbm_dma on v4 chips. This change initializes y_ref before copying it to avoid copying uninitialized values. Link to failing test: https://github.com/google/jax/actions/runs/8958798888/job/24603466593 ``` tests/pallas/pallas_call_tpu_test.py:917: in test_smem_hbm_dma     np.testing.assert_allclose(y, expected) /usr/lib/python3.10/contextlib.py:79: in inner     return func(*args, **kwds) E   AssertionError:  E   Not equal to tolerance rtol=1e07, atol=0 E    E   Mismatched elements: 47 / 1024 (4.59%) E   Max absolute difference: 1.2172749e36 E   Max relative difference: 0. E    x: array([[5.16e+02, 0.00e+00, 0.00e+00, ..., 0.00e+00, 0.00e+00, 0.00e+00], E          [0.00e+00, 0.00e+00, 4.20e45, ..., 0.00e+00, 0.00e+00, 0.00e+00], E          [0.00e+00, 0.00e+00, 0.00e+00, ..., 0.00e+00, 0.00e+00, 0.00e+00],... E    y: array([[516.,   0.,   0., ...,   0.,   0.,   0.], E          [  0.,   0.,   0., ...,   0.,   0.,   0.], E          [  0.,   0.,   0., ...,   0.,   0.,   0.],... ```",2024-05-07T16:56:30Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/21107
yi,Remaining API changes for array API compliance,"This issue tracks the deprecations that must still be started to ensure that the `jax.numpy` namespace is array API compliant. This covers some issues remaining which are not explicitly tracked in https://github.com/google/jax/issues/18353, with some overlap between both.   Core  [x] https://github.com/google/jax/pull/21130  [x] Add `device` kwarg      `jnp.arange`      `jnp.array`      `jnp.eye`      `jnp.linspace`  [x] Wait on final decision on https://github.com/dataapis/arrayapi/issues/405  [ ] Preserve integral `dtype`      `jnp.ceil`      `jnp.floor`      `jnp.trunc`  [x] https://github.com/google/jax/pull/21262      `jnp.std`      `jnp.var`  Compliance Finalization  [ ] https://github.com/google/jax/pull/21323      Add `__array_namespace__` to `ArrayImpl`      Add `to_device` to `ArrayImpl`      Add `device` to `ArrayImpl`      Add `__array_namespace_info__` to `jax.numpy`      Add `__array_api_version__` version information  FFT  [x] Add `device` kwarg CC([array API] move api metadata into jax.numpy namespace)      `jnp.fft.fftfreq`      `jnp.fft.rfftfreq`   Linalg  [x] https://github.com/google/jax/pull/21442  [x] https://github.com/google/jax/pull/21226      `jnp.linalg.matrix_rank`: Change `tol` arg to `rtol`      `jnp.linalg.pinv`: Change `rcond` arg to `rtol`",2024-05-06T18:29:04Z,enhancement,closed,0,2,https://github.com/jax-ml/jax/issues/21088,Task  > Add info namespace containing the same functions as https://github.com/google/jax/pull/20294 Nothing to be done in jax as arrayapitests should be fixed: https://github.com/google/jax/pull/21506,Closed by CC(Finalize array API in jax.numpy & deprecate jax.experimental.array_api)
yi,fix vmap-grad-remat-shmap bug with spmd_axis_name,"The fix in CC(fix vmapgradshmap bug with spmd_axis_name) was not correct because it assumed that the set of all mesh axis names appearing in in_specs was an upper bound on the set of mesh axes over which residuals could be devicevarying. But collectives can introduce device variance! So it's not an upper bound. We track device variance when check_rep=True, but often people set check_rep=False (e.g. when using pallas_call in a shard_map). So relying on our device variance tracking would be limiting. That may be a decent long term solution, if we can make it easy to annotate pallas_calls with device variance information. But it's not a great short term one to unblock things. So instead I temporarily went with context sensitivity: instead of making residuals sharded over all mesh.axis_names (as we did before these patches), we make them sharded over all mesh axis names _excluding_ any spmd_axis_names in our dynamic context (by looking at the traces in our trace stack). It's illegal to mention any spmd_axis_names in collectives (indeed anywhere in the body of the function being vmapped), but I don't think we check it. TODO(mattjj): add more testing (maybe in followups)",2024-05-03T16:13:54Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/21056
yi,spsolve exits with error when inverting matrix sum," Description I am trying to solve a linear system (A1 + A2) x = b using sparse matrices A1, A2 and `jax.experimental.sparse.linalg.spsolve`. Since spsolve requires BCSR format which does not yet support addition, I am storing A1 and A2 in BCOO format, and then converting the sum in BCSR format. However this is causing an error in spsolve. I can reproduce the problem with the following simplified code: **Runs Fine** ``` import jax import jax.experimental.sparse I_boo = jax.experimental.sparse.eye(2) I_csr = jax.experimental.sparse.BCSR.from_bcoo(I_boo) b = jax.numpy.ones(2) jax.experimental.sparse.linalg.spsolve(I_csr.data, I_csr.indices, I_csr.indptr, b) ``` **Exits in Error** ``` import jax import jax.experimental.sparse I_boo = jax.experimental.sparse.eye(2) I_boo = I_boo + I_boo  runs fine however causes spsolve to fail I_csr = jax.experimental.sparse.BCSR.from_bcoo(I_boo) b = jax.numpy.ones(2) jax.experimental.sparse.linalg.spsolve(I_csr.data, I_csr.indices, I_csr.indptr, b) ``` The error is copied below. ``` XlaRuntimeErrorTraceback (most recent call last)  in        7        8 b = jax.numpy.ones(2) > 9 jax.experimental.sparse.linalg.spsolve(I_csr.data, I_csr.indices, I_csr.indptr, b) ~/.local/lib/python3.9/sitepackages/jax/experimental/sparse/linalg.py in spsolve(data, indices, indptr, b, tol, reorder)     619     the sparse linear system.     620   """""" > 621   return spsolve_p.bind(data, indices, indptr, b, tol=tol, reorder=reorder) ~/.local/lib/python3.9/sitepackages/jax/_src/core.py in bind(self, *args, **params)     420     assert (not config.enable_checks.value or     421             all(isinstance(arg, Tracer) or valid_jaxtype(arg) for arg in args)), args > 422     return self.bind_with_trace(find_top_trace(args), args, params)     423      424   def bind_with_trace(self, trace, args, params): ~/.local/lib/python3.9/sitepackages/jax/_src/core.py in bind_with_trace(self, trace, args, params)     423      424   def bind_with_trace(self, trace, args, params): > 425     out = trace.process_primitive(self, map(trace.full_raise, args), params)     426     return map(full_lower, out) if self.multiple_results else full_lower(out)     427  ~/.local/lib/python3.9/sitepackages/jax/_src/core.py in process_primitive(self, primitive, tracers, params)     911      912   def process_primitive(self, primitive, tracers, params): > 913     return primitive.impl(*tracers, **params)     914      915   def process_call(self, primitive, f, tracers, params): ~/.local/lib/python3.9/sitepackages/jax/_src/dispatch.py in apply_primitive(prim, *args, **params)      85     prev = lib.jax_jit.swap_thread_local_state_disable_jit(False)      86     try: > 87       outs = fun(*args)      88     finally:      89       lib.jax_jit.swap_thread_local_state_disable_jit(prev)     [... skipping hidden 10 frame] ~/.local/lib/python3.9/sitepackages/jax/_src/interpreters/pxla.py in __call__(self, *args)    1203         or self.has_host_callbacks):    1204       input_bufs = self._add_tokens_to_inputs(input_bufs) > 1205       results = self.xla_executable.execute_sharded(    1206           input_bufs, with_tokens=True    1207       ) XlaRuntimeError: INTERNAL: Generated function failed: CpuCallback error: Traceback (most recent call last):   File ""/usr/lib64/python3.9/runpy.py"", line 197, in _run_module_as_main   File ""/usr/lib64/python3.9/runpy.py"", line 87, in _run_code   File ""/usr/lib/python3.9/sitepackages/ipykernel_launcher.py"", line 16, in    File ""/usr/lib/python3.9/sitepackages/traitlets/config/application.py"", line 845, in launch_instance   File ""/usr/lib/python3.9/sitepackages/ipykernel/kernelapp.py"", line 612, in start   File ""/usr/lib64/python3.9/sitepackages/tornado/platform/asyncio.py"", line 199, in start   File ""/usr/lib64/python3.9/asyncio/base_events.py"", line 596, in run_forever   File ""/usr/lib64/python3.9/asyncio/base_events.py"", line 1890, in _run_once   File ""/usr/lib64/python3.9/asyncio/events.py"", line 80, in _run   File ""/usr/lib64/python3.9/sitepackages/tornado/ioloop.py"", line 688, in    File ""/usr/lib64/python3.9/sitepackages/tornado/ioloop.py"", line 741, in _run_callback   File ""/usr/lib64/python3.9/sitepackages/tornado/gen.py"", line 814, in inner   File ""/usr/lib64/python3.9/sitepackages/tornado/gen.py"", line 775, in run   File ""/usr/lib/python3.9/sitepackages/ipykernel/kernelbase.py"", line 362, in process_one   File ""/usr/lib64/python3.9/sitepackages/tornado/gen.py"", line 234, in wrapper   File ""/usr/lib/python3.9/sitepackages/ipykernel/kernelbase.py"", line 265, in dispatch_shell   File ""/usr/lib64/python3.9/sitepackages/tornado/gen.py"", line 234, in wrapper   File ""/usr/lib/python3.9/sitepackages/ipykernel/kernelbase.py"", line 540, in execute_request   File ""/usr/lib64/python3.9/sitepackages/tornado/gen.py"", line 234, in wrapper   File ""/usr/lib/python3.9/sitepackages/ipykernel/ipkernel.py"", line 302, in do_execute   File ""/usr/lib/python3.9/sitepackages/ipykernel/zmqshell.py"", line 539, in run_cell   File ""/usr/lib/python3.9/sitepackages/IPython/core/interactiveshell.py"", line 2886, in run_cell   File ""/usr/lib/python3.9/sitepackages/IPython/core/interactiveshell.py"", line 2932, in _run_cell   File ""/usr/lib/python3.9/sitepackages/IPython/core/async_helpers.py"", line 68, in _pseudo_sync_runner   File ""/usr/lib/python3.9/sitepackages/IPython/core/interactiveshell.py"", line 3155, in run_cell_async   File ""/usr/lib/python3.9/sitepackages/IPython/core/interactiveshell.py"", line 3347, in run_ast_nodes   File ""/usr/lib/python3.9/sitepackages/IPython/core/interactiveshell.py"", line 3427, in run_code   File """", line 9, in    File ""/home/user/.local/lib/python3.9/sitepackages/jax/experimental/sparse/linalg.py"", line 621, in spsolve   File ""/home/user/.local/lib/python3.9/sitepackages/jax/_src/core.py"", line 422, in bind   File ""/home/user/.local/lib/python3.9/sitepackages/jax/_src/core.py"", line 425, in bind_with_trace   File ""/home/user/.local/lib/python3.9/sitepackages/jax/_src/core.py"", line 913, in process_primitive   File ""/home/user/.local/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 87, in apply_primitive   File ""/home/user/.local/lib/python3.9/sitepackages/jax/_src/traceback_util.py"", line 179, in reraise_with_filtered_traceback   File ""/home/user/.local/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 298, in cache_miss   File ""/home/user/.local/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 176, in _python_pjit_helper   File ""/home/user/.local/lib/python3.9/sitepackages/jax/_src/core.py"", line 2788, in bind   File ""/home/user/.local/lib/python3.9/sitepackages/jax/_src/core.py"", line 425, in bind_with_trace   File ""/home/user/.local/lib/python3.9/sitepackages/jax/_src/core.py"", line 913, in process_primitive   File ""/home/user/.local/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 1488, in _pjit_call_impl   File ""/home/user/.local/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 1471, in call_impl_cache_miss   File ""/home/user/.local/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 1427, in _pjit_call_impl_python   File ""/home/user/.local/lib/python3.9/sitepackages/jax/_src/profiler.py"", line 335, in wrapper   File ""/home/user/.local/lib/python3.9/sitepackages/jax/_src/interpreters/pxla.py"", line 1205, in __call__   File ""/home/user/.local/lib/python3.9/sitepackages/jax/_src/interpreters/mlir.py"", line 2466, in _wrapped_callback   File ""/home/user/.local/lib/python3.9/sitepackages/jax/experimental/sparse/linalg.py"", line 547, in _callback   File ""/home/user/.local/lib/python3.9/sitepackages/scipy/sparse/linalg/_dsolve/linsolve.py"", line 242, in spsolve   File ""/home/user/.local/lib/python3.9/sitepackages/scipy/sparse/_compressed.py"", line 1125, in sum_duplicates ValueError: WRITEBACKIFCOPY base is readonly ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.26 jaxlib: 0.4.26 numpy:  1.26.4 python: 3.9.7 (default, Aug 30 2021, 00:00:00)  [GCC 11.2.1 20210728 (Red Hat 11.2.11)] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='959df832f7d6', release='5.10.104linuxkit', version=' CC(Python 3 compatibility issues) SMP Thu Mar 17 17:08:06 UTC 2022', machine='x86_64') ```",2024-05-01T20:03:08Z,bug,open,0,4,https://github.com/jax-ml/jax/issues/21031,Thanks for the report! It looks like something in `spsolve` fails in the presence of duplicate indices. You can fix the issue in your case by doing this before passing the `I_csr` buffers to `spsolve`: ```python I_csr = I_csr.sum_duplicates() ```,"I suspect this is working as intended, since `spsolve` is a lowerlevel function, though we should do a better job of documenting its requirements.","Thank you, adding `sum_duplicates` resolved the issue! It would be great if this eventually gets automatically taken care of by Jax so that sparse matrices behaves in the same way as scipy.","I don't think we'll ever automatically take care of this, because deduplication is a relatively expensive operation (even detecting whether deduplication is necessary is expensive!), and people calling a lowlevel routine like `spsolve` generally care about performance."
yi,Bundle MLIR .pyi files with jaxlib,This allows mypy and pyright to type check the code which uses MLIR Python APIs.,2024-05-01T18:38:28Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/21029
yi,Add `sharding` on `ShapedArray` and in the type system of jaxpr. This can be enabled by setting the `jax_enable_sharding_in_avals` config option to `True`.,"Add `sharding` on `ShapedArray` and in the type system of jaxpr. This can be enabled by setting the `jax_enable_sharding_in_avals` config option to `True`. This CL only adds sharding to avals in `with_sharding_constraint`'s abstract_eval rule and `device_put`'s abstract eval rule. In the following CLs, I'll make `jit`, `eval_shape`, `shard_map`, `arguments` and other parts of JAX play nicely with this! But what problem are you trying to solve right now? This CL solves a very specific memories problem where you want to offload an output to host/device via `device_put` inside `jit` but with no `out_shardings` annotation. `device_put` or `wsc` on outputs should lower to the same HLO as `out_shardings`. But this change is what we want to do in the future and follow up CLs would make other parts of jax also work well with this. Jaxpr with sharding inside it (currently only works with NamedSharding) where the sharding type is printed as a mapping from `{axis_name: dimension}`: ``` { lambda ; a:i64[8,2]. let     b:i64[8,2]{x: 0} = sharding_constraint[       resource_env=ResourceEnv(mesh=Mesh(), ())       sharding=NamedSharding(mesh=Mesh('x': 2, 'y': 2), spec=PartitionSpec('x',))       unconstrained_dims=set()     ] a     c:i64[8,2] = pjit[       name=g       jaxpr={ lambda ; d:i64[8,2]{x: 0}. let e:i64[8,2] = mul d 2 in (e,) }     ] b     f:i64[8,2]{} = sharding_constraint[       resource_env=ResourceEnv(mesh=Mesh(), ())       sharding=NamedSharding(mesh=Mesh('x': 2, 'y': 2), spec=PartitionSpec())       unconstrained_dims=set()     ] c     g:i64[8,2] = pjit[       name=g       jaxpr={ lambda ; h:i64[8,2]{}. let i:i64[8,2] = mul h 2 in (i,) }     ] f     j:i64[8,2]{y: 1} = sharding_constraint[       resource_env=ResourceEnv(mesh=Mesh(), ())       sharding=NamedSharding(mesh=Mesh('x': 2, 'y': 2), spec=PartitionSpec(None, 'y'))       unconstrained_dims=set()     ] g   in (j,) } ```",2024-05-01T00:25:11Z,,closed,0,0,https://github.com/jax-ml/jax/issues/21022
yi,"Refactor `array_api` namespace, relying more directly on `jax.numpy`","This PR refactors the `jax.experimental.array_api` namespace by removing unnecessary wrappers around alreadycompliant functions in the `jax.numpy` namespace, and structuring the `array_api` namespace to pull directly from `jax.numpy` whenever possible. After this PR, the `array_api` submodule contain only: 1. Wrapper functions to insulate `jax.numpy` from breaking changes, which will be removed when the corresponding `jax.numpy` behavior is deprecated and made array API compliant 2. Additional features/utilities/API that is not yet present in `jax.numpy` and needs inclusion (e.g. introducing `jax.numpy.matmul`, which already exists in `jax.numpy.linalg`). This PR also adds several `TODO` items describing what is required to cull that portion of the `array_api` submodule, with the understanding that once it is empty, `jax.numpy` will be fully compliant. I figured it would be a bit neater to keep the `TODO` notes dense in this submodule, rather than spreading them across the `jax.numpy` submodule on their corresponding functions. It's also consistent with the `TODO`s for _new_ functionality or namespace elements. This PR also modifies `jax.numpy.isdtype` to accept `_ScalarMeta` and other dtypeinterpretable inputs. Note that the `arrayapitests` issues many `UserWarnings` for the special cases test, as well as for their reporting utilities due to not understanding what `` wrapped functions are in JAX, so this PR suppresses them in the `jaxarrayapi` workflow. This PR has been validated against the `arrayapitests` suite for version `2023.12`, using `jax/experimental/array_api/skips.txt`  although it is worth noting that the test suite does **not** cover everything, e.g. is still missing support for `copy` and `device` keyword tests. cc:  ",2024-04-30T20:41:27Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/21013,"The current failure is due to needing to add some signature test skips for `logical_{and, or, xor}`  that'll be fixed in the next push"
yi,Adds rewrite patterns for `arith` and `math` operations with `bf16` operands/results that are not supported by the underlying hardware.,Adds rewrite patterns for `arith` and `math` operations with `bf16` operands/results that are not supported by the underlying hardware.,2024-04-30T16:04:51Z,,closed,0,1,https://github.com/jax-ml/jax/issues/21007,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
llm,[Mosaic GPU] Only call kernel initializer from inside a custom call,"[Mosaic GPU] Only call kernel initializer from inside a custom call XLA:GPU custom call design is far from ideal, as there's apparently no way to figure out the CUDA context that will be used to run an HLO module before the custom call is first called. So, we can't preload the kernel onto the GPU, or else we'll get invalid handle errors due to the load and launch happening in different CUDA contexts... Also fix up build_wheel.py to match the rename of the runtime lib.",2024-04-30T10:07:58Z,,closed,0,0,https://github.com/jax-ml/jax/issues/20995
yi,DOC: improve docs for reshape() and ravel(),"Similar to CC(DOC: improve docs of transpose & matrix_transpose), these are functions where the view semantics differ from those of NumPy, so documenting the correct features from scratch is better than relying on a JAXspecific note above inaccurate docs. Rendered preview:  `reshape`  `ravel` Part of CC(Tracking issue: inline docstrings).",2024-04-29T15:36:08Z,documentation pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/20980
yi,Sharding is much slower than pmap for while loops of varying length while loops," Description As the title indicated, with a double while loop, where the inner while loop may change in length over outer while loop steps, `pmap` is substantially faster than sharding. This may sound contrived, but is exactly what happens in other packages, such as diffrax where I first identified this issue: https://github.com/patrickkidger/diffrax/issues/407. I believe there are two possibilities, 1) I am using sharding wrong and that is why it is slow (very possible, I am new to sharding), 2) something else is going on in sharding.  I have included a MVC below. I ran on both CPU and GPU and the results on GPU are even more noticeable.  ```python import jax import jax.numpy as jnp import jax.experimental.mesh_utils as mesh_utils def f(t, y, theta):     return jnp.abs(jnp.sin(t)) + theta * y def solve(init, key):     def inner_loop_cond(state):         t, y, _ = state         return y.squeeze() < 10     def inner_loop_body(state):         t, y, theta = state         dy = f(t, y, theta)         return (t + 0.1, y + 0.1 * dy, theta)     def outer_loop_cond(state):         _, _, _, count = state         return count < 5     def outer_loop_body(state):         t, y, theta, count = state         y = jax.random.uniform(jax.random.PRNGKey(count), shape=(1,))         new_t, new_y, _ = inner_while_loop(inner_loop_cond, inner_loop_body, (t, y, theta))         return (new_t, new_y, theta, count + 1)     inner_while_loop = jax.lax.while_loop     outer_while_loop = jax.lax.while_loop     theta = 5.0     t_initial = 0.0     y_initial = init     count_initial = jax.random.randint(key, minval=2, maxval=2, shape=())     final_state = outer_while_loop(outer_loop_cond, outer_loop_body, (t_initial, y_initial, theta, count_initial))     return final_state[1] batch_size = 30 inits = 0.1 * jnp.ones((batch_size, 1)) keys = jax.random.split(jax.random.PRNGKey(0), batch_size) num_devices = len(jax.devices()) devices = mesh_utils.create_device_mesh((num_devices, 1)) sharding = jax.sharding.PositionalSharding(devices) replicated = sharding.replicate() inits_pmap = inits.reshape(num_devices, batch_size // num_devices, *inits.shape[1:]) keys_pmap = keys.reshape(num_devices, batch_size // num_devices, *keys.shape[1:]) x, y = jax.device_put((inits, keys), sharding) fn = jax.jit(jax.vmap(solve)) pmap_fn = jax.pmap(fn)  Ignore compilation time _ = fn(x, y).block_until_ready() _ = pmap_fn(inits_pmap, keys_pmap).block_until_ready() ``` ```python %%timeit _ = fn(x, y).block_until_ready() ``` CPU: `5.11 ms ± 53.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)` GPU: `1.18 s ± 48.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)` ```python %%timeit _ = pmap_fn(inits_pmap, keys_pmap).block_until_ready() ``` CPU: `251 µs ± 1.14 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)` GPU: `3.93 ms ± 225 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)`  System info (python version, jaxlib version, accelerator, etc.) CPU: ``` jax:    0.4.26 jaxlib: 0.4.26 numpy:  1.26.4 python: 3.10.14  ++ ```",2024-04-27T21:09:38Z,bug,open,0,4,https://github.com/jax-ml/jax/issues/20968,"To be extra clear, if you make `count_initial = 0` you see the same speed for sharding and for pmap. Only when this varies per shard does this slow down","Also, if you replace the body and do  ```python new_t, new_y, theta = inner_loop_body((t, y, theta)) new_t, new_y, _ = inner_while_loop(inner_loop_cond, inner_loop_body, (t, y, theta)) return (new_t, new_y, theta, count + 1) ``` you see the same speed, which indicates the second while loop is important to the slowdown","kidger you mentioned in https://github.com/patrickkidger/diffrax/issues/407 that you suspect this is within XLA, do you have any advice on how to approach that? I haven't investigated an XLA system this complex before. Even my reduced complexity example (shown below) yields XLA's that are not exceedingly readable (shown further below). Is there a goto issue/piece of XLA/jax documentation on identifying whether a bug is in jax vs XLA and how to spot it? ```python import jax import jax.numpy as jnp import jax.experimental.mesh_utils as mesh_utils def solve(init, key):     def inner_loop_cond(state):         t, y, _ = state         return y.squeeze()   Full HLO   ``` HloModule xla_computation_solve, entry_computation_layout={(f32[10,1]{1,0}, u32[10,2]{1,0})>(f32[10,1]{1,0})} clip.3 {   Arg_2.6 = s32[] parameter(2)   Arg_1.5 = s32[] parameter(1)   Arg_0.4 = s32[] parameter(0)   maximum.7 = s32[] maximum(Arg_1.5, Arg_0.4)   ROOT minimum.8 = s32[] minimum(Arg_2.6, maximum.7) } clip_0.9 {   Arg_2.12 = s32[] parameter(2)   Arg_1.11 = s32[] parameter(1)   Arg_0.10 = s32[] parameter(0)   maximum.13 = s32[] maximum(Arg_1.11, Arg_0.10)   ROOT minimum.14 = s32[] minimum(Arg_2.12, maximum.13) } clip_0.15 {   Arg_2.18 = s32[] parameter(2)   Arg_1.17 = s32[] parameter(1)   Arg_0.16 = s32[] parameter(0)   maximum.19 = s32[] maximum(Arg_1.17, Arg_0.16)   ROOT minimum.20 = s32[] minimum(Arg_2.18, maximum.19) } region_0.21 {   arg_tuple.22 = (s32[], s32[], u32[10,2]{1,0}, u32[10,2]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) parameter(0)   gettupleelement.23 = s32[] gettupleelement(arg_tuple.22), index=0   constant.33 = s32[] constant(1)   add.87 = s32[] add(gettupleelement.23, constant.33)   gettupleelement.24 = s32[] gettupleelement(arg_tuple.22), index=1   add.34 = s32[] add(gettupleelement.24, constant.33)   gettupleelement.25 = u32[10,2]{1,0} gettupleelement(arg_tuple.22), index=2   gettupleelement.26 = u32[10,2]{1,0} gettupleelement(arg_tuple.22), index=3   add.37 = u32[10,2]{1,0} add(gettupleelement.25, gettupleelement.26)   gettupleelement.30 = u32[4]{0} gettupleelement(arg_tuple.22), index=7   slice.35 = u32[1]{0} slice(gettupleelement.30), slice={[0:1]}   reshape.36 = u32[] reshape(slice.35)   broadcast.38 = u32[10,2]{1,0} broadcast(reshape.36), dimensions={}   shiftleft.39 = u32[10,2]{1,0} shiftleft(gettupleelement.26, broadcast.38)   constant.32 = u32[] constant(32)   subtract.40 = u32[] subtract(constant.32, reshape.36)   broadcast.41 = u32[10,2]{1,0} broadcast(subtract.40), dimensions={}   shiftrightlogical.42 = u32[10,2]{1,0} shiftrightlogical(gettupleelement.26, broadcast.41)   or.43 = u32[10,2]{1,0} or(shiftleft.39, shiftrightlogical.42)   xor.44 = u32[10,2]{1,0} xor(add.37, or.43)   add.47 = u32[10,2]{1,0} add(add.37, xor.44)   slice.45 = u32[1]{0} slice(gettupleelement.30), slice={[1:2]}   reshape.46 = u32[] reshape(slice.45)   broadcast.48 = u32[10,2]{1,0} broadcast(reshape.46), dimensions={}   shiftleft.49 = u32[10,2]{1,0} shiftleft(xor.44, broadcast.48)   subtract.50 = u32[] subtract(constant.32, reshape.46)   broadcast.51 = u32[10,2]{1,0} broadcast(subtract.50), dimensions={}   shiftrightlogical.52 = u32[10,2]{1,0} shiftrightlogical(xor.44, broadcast.51)   or.53 = u32[10,2]{1,0} or(shiftleft.49, shiftrightlogical.52)   xor.54 = u32[10,2]{1,0} xor(add.47, or.53)   add.57 = u32[10,2]{1,0} add(add.47, xor.54)   slice.55 = u32[1]{0} slice(gettupleelement.30), slice={[2:3]}   reshape.56 = u32[] reshape(slice.55)   broadcast.58 = u32[10,2]{1,0} broadcast(reshape.56), dimensions={}   shiftleft.59 = u32[10,2]{1,0} shiftleft(xor.54, broadcast.58)   subtract.60 = u32[] subtract(constant.32, reshape.56)   broadcast.61 = u32[10,2]{1,0} broadcast(subtract.60), dimensions={}   shiftrightlogical.62 = u32[10,2]{1,0} shiftrightlogical(xor.54, broadcast.61)   or.63 = u32[10,2]{1,0} or(shiftleft.59, shiftrightlogical.62)   xor.64 = u32[10,2]{1,0} xor(add.57, or.63)   add.67 = u32[10,2]{1,0} add(add.57, xor.64)   gettupleelement.27 = u32[10,1]{1,0} gettupleelement(arg_tuple.22), index=4   broadcast.75 = u32[10,1]{1,0} broadcast(gettupleelement.27), dimensions={0,1}   reshape.76 = u32[10]{0} reshape(broadcast.75)   broadcast.77 = u32[10,2]{1,0} broadcast(reshape.76), dimensions={0}   add.78 = u32[10,2]{1,0} add(add.67, broadcast.77)   slice.65 = u32[1]{0} slice(gettupleelement.30), slice={[3:4]}   reshape.66 = u32[] reshape(slice.65)   broadcast.68 = u32[10,2]{1,0} broadcast(reshape.66), dimensions={}   shiftleft.69 = u32[10,2]{1,0} shiftleft(xor.64, broadcast.68)   subtract.70 = u32[] subtract(constant.32, reshape.66)   broadcast.71 = u32[10,2]{1,0} broadcast(subtract.70), dimensions={}   shiftrightlogical.72 = u32[10,2]{1,0} shiftrightlogical(xor.64, broadcast.71)   or.73 = u32[10,2]{1,0} or(shiftleft.69, shiftrightlogical.72)   xor.74 = u32[10,2]{1,0} xor(add.67, or.73)   gettupleelement.28 = u32[10,1]{1,0} gettupleelement(arg_tuple.22), index=5   broadcast.79 = u32[10,1]{1,0} broadcast(gettupleelement.28), dimensions={0,1}   reshape.80 = u32[10]{0} reshape(broadcast.79)   broadcast.81 = u32[10,2]{1,0} broadcast(reshape.80), dimensions={0}   add.82 = u32[10,2]{1,0} add(xor.74, broadcast.81)   add.83 = s32[] add(gettupleelement.24, constant.33)   convert.84 = u32[] convert(add.83)   broadcast.85 = u32[10,2]{1,0} broadcast(convert.84), dimensions={}   add.86 = u32[10,2]{1,0} add(add.82, broadcast.85)   gettupleelement.29 = u32[10,1]{1,0} gettupleelement(arg_tuple.22), index=6   gettupleelement.31 = u32[4]{0} gettupleelement(arg_tuple.22), index=8   ROOT tuple.88 = (s32[], s32[], u32[10,2]{1,0}, u32[10,2]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) tuple(add.87, add.34, add.78, add.86, gettupleelement.28, gettupleelement.29, gettupleelement.27, gettupleelement.31, gettupleelement.30) } region_1.89 {   arg_tuple.90 = (s32[], s32[], u32[10,2]{1,0}, u32[10,2]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) parameter(0)   gettupleelement.92 = s32[] gettupleelement(arg_tuple.90), index=1   gettupleelement.93 = u32[10,2]{1,0} gettupleelement(arg_tuple.90), index=2   gettupleelement.94 = u32[10,2]{1,0} gettupleelement(arg_tuple.90), index=3   gettupleelement.95 = u32[10,1]{1,0} gettupleelement(arg_tuple.90), index=4   gettupleelement.96 = u32[10,1]{1,0} gettupleelement(arg_tuple.90), index=5   gettupleelement.97 = u32[10,1]{1,0} gettupleelement(arg_tuple.90), index=6   gettupleelement.98 = u32[4]{0} gettupleelement(arg_tuple.90), index=7   gettupleelement.99 = u32[4]{0} gettupleelement(arg_tuple.90), index=8   gettupleelement.91 = s32[] gettupleelement(arg_tuple.90), index=0   constant.100 = s32[] constant(5)   ROOT compare.101 = pred[] compare(gettupleelement.91, constant.100), direction=LT } _threefry_split.102 {   constant.106 = s32[] constant(0)   iota.109 = u32[4]{0} iota(), iota_dimension=0   slice.112 = u32[2]{0} slice(iota.109), slice={[0:2]}   reshape.114 = u32[1,2]{1,0} reshape(slice.112)   broadcast.118 = u32[1,2]{1,0} broadcast(reshape.114), dimensions={0,1}   reshape.119 = u32[2]{0} reshape(broadcast.118)   broadcast.120 = u32[10,2]{1,0} broadcast(reshape.119), dimensions={1}   Arg_0.103 = u32[10,2]{1,0} parameter(0)   slice.110 = u32[10,1]{1,0} slice(Arg_0.103), slice={[0:10], [0:1]}   broadcast.121 = u32[10,1]{1,0} broadcast(slice.110), dimensions={0,1}   reshape.122 = u32[10]{0} reshape(broadcast.121)   broadcast.123 = u32[10,2]{1,0} broadcast(reshape.122), dimensions={0}   add.124 = u32[10,2]{1,0} add(broadcast.120, broadcast.123)   slice.113 = u32[2]{0} slice(iota.109), slice={[2:4]}   reshape.115 = u32[1,2]{1,0} reshape(slice.113)   broadcast.125 = u32[1,2]{1,0} broadcast(reshape.115), dimensions={0,1}   reshape.126 = u32[2]{0} reshape(broadcast.125)   broadcast.127 = u32[10,2]{1,0} broadcast(reshape.126), dimensions={1}   slice.111 = u32[10,1]{1,0} slice(Arg_0.103), slice={[0:10], [1:2]}   broadcast.128 = u32[10,1]{1,0} broadcast(slice.111), dimensions={0,1}   reshape.129 = u32[10]{0} reshape(broadcast.128)   broadcast.130 = u32[10,2]{1,0} broadcast(reshape.129), dimensions={0}   add.131 = u32[10,2]{1,0} add(broadcast.127, broadcast.130)   xor.116 = u32[10,1]{1,0} xor(slice.110, slice.111)   constant.104 = u32[] constant(466688986)   broadcast.105 = u32[10,1]{1,0} broadcast(constant.104), dimensions={}   xor.117 = u32[10,1]{1,0} xor(xor.116, broadcast.105)   constant.108 = u32[4]{0} constant({13, 15, 26, 6})   constant.107 = u32[4]{0} constant({17, 29, 16, 24})   tuple.132 = (s32[], s32[], u32[10,2]{1,0}, u32[10,2]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) tuple(constant.106, constant.106, add.124, add.131, slice.111, xor.117, slice.110, constant.108, constant.107)   while.133 = (s32[], s32[], u32[10,2]{1,0}, u32[10,2]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) while(tuple.132), condition=region_1.89, body=region_0.21   gettupleelement.134 = s32[] gettupleelement(while.133), index=0   gettupleelement.135 = s32[] gettupleelement(while.133), index=1   gettupleelement.138 = u32[10,1]{1,0} gettupleelement(while.133), index=4   gettupleelement.139 = u32[10,1]{1,0} gettupleelement(while.133), index=5   gettupleelement.140 = u32[10,1]{1,0} gettupleelement(while.133), index=6   gettupleelement.141 = u32[4]{0} gettupleelement(while.133), index=7   gettupleelement.142 = u32[4]{0} gettupleelement(while.133), index=8   gettupleelement.136 = u32[10,2]{1,0} gettupleelement(while.133), index=2   gettupleelement.137 = u32[10,2]{1,0} gettupleelement(while.133), index=3   concatenate.143 = u32[10,4]{1,0} concatenate(gettupleelement.136, gettupleelement.137), dimensions={1}   ROOT reshape.144 = u32[10,2,2]{2,1,0} reshape(concatenate.143) } region_2.145 {   arg_tuple.146 = (s32[], s32[], u32[10,1]{1,0}, u32[10,1]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) parameter(0)   gettupleelement.147 = s32[] gettupleelement(arg_tuple.146), index=0   constant.157 = s32[] constant(1)   add.205 = s32[] add(gettupleelement.147, constant.157)   gettupleelement.148 = s32[] gettupleelement(arg_tuple.146), index=1   add.158 = s32[] add(gettupleelement.148, constant.157)   gettupleelement.149 = u32[10,1]{1,0} gettupleelement(arg_tuple.146), index=2   gettupleelement.150 = u32[10,1]{1,0} gettupleelement(arg_tuple.146), index=3   add.161 = u32[10,1]{1,0} add(gettupleelement.149, gettupleelement.150)   gettupleelement.154 = u32[4]{0} gettupleelement(arg_tuple.146), index=7   slice.159 = u32[1]{0} slice(gettupleelement.154), slice={[0:1]}   reshape.160 = u32[] reshape(slice.159)   broadcast.162 = u32[10,1]{1,0} broadcast(reshape.160), dimensions={}   shiftleft.163 = u32[10,1]{1,0} shiftleft(gettupleelement.150, broadcast.162)   constant.156 = u32[] constant(32)   subtract.164 = u32[] subtract(constant.156, reshape.160)   broadcast.165 = u32[10,1]{1,0} broadcast(subtract.164), dimensions={}   shiftrightlogical.166 = u32[10,1]{1,0} shiftrightlogical(gettupleelement.150, broadcast.165)   or.167 = u32[10,1]{1,0} or(shiftleft.163, shiftrightlogical.166)   xor.168 = u32[10,1]{1,0} xor(add.161, or.167)   add.171 = u32[10,1]{1,0} add(add.161, xor.168)   slice.169 = u32[1]{0} slice(gettupleelement.154), slice={[1:2]}   reshape.170 = u32[] reshape(slice.169)   broadcast.172 = u32[10,1]{1,0} broadcast(reshape.170), dimensions={}   shiftleft.173 = u32[10,1]{1,0} shiftleft(xor.168, broadcast.172)   subtract.174 = u32[] subtract(constant.156, reshape.170)   broadcast.175 = u32[10,1]{1,0} broadcast(subtract.174), dimensions={}   shiftrightlogical.176 = u32[10,1]{1,0} shiftrightlogical(xor.168, broadcast.175)   or.177 = u32[10,1]{1,0} or(shiftleft.173, shiftrightlogical.176)   xor.178 = u32[10,1]{1,0} xor(add.171, or.177)   add.181 = u32[10,1]{1,0} add(add.171, xor.178)   slice.179 = u32[1]{0} slice(gettupleelement.154), slice={[2:3]}   reshape.180 = u32[] reshape(slice.179)   broadcast.182 = u32[10,1]{1,0} broadcast(reshape.180), dimensions={}   shiftleft.183 = u32[10,1]{1,0} shiftleft(xor.178, broadcast.182)   subtract.184 = u32[] subtract(constant.156, reshape.180)   broadcast.185 = u32[10,1]{1,0} broadcast(subtract.184), dimensions={}   shiftrightlogical.186 = u32[10,1]{1,0} shiftrightlogical(xor.178, broadcast.185)   or.187 = u32[10,1]{1,0} or(shiftleft.183, shiftrightlogical.186)   xor.188 = u32[10,1]{1,0} xor(add.181, or.187)   add.191 = u32[10,1]{1,0} add(add.181, xor.188)   gettupleelement.151 = u32[10,1]{1,0} gettupleelement(arg_tuple.146), index=4   add.199 = u32[10,1]{1,0} add(add.191, gettupleelement.151)   slice.189 = u32[1]{0} slice(gettupleelement.154), slice={[3:4]}   reshape.190 = u32[] reshape(slice.189)   broadcast.192 = u32[10,1]{1,0} broadcast(reshape.190), dimensions={}   shiftleft.193 = u32[10,1]{1,0} shiftleft(xor.188, broadcast.192)   subtract.194 = u32[] subtract(constant.156, reshape.190)   broadcast.195 = u32[10,1]{1,0} broadcast(subtract.194), dimensions={}   shiftrightlogical.196 = u32[10,1]{1,0} shiftrightlogical(xor.188, broadcast.195)   or.197 = u32[10,1]{1,0} or(shiftleft.193, shiftrightlogical.196)   xor.198 = u32[10,1]{1,0} xor(add.191, or.197)   gettupleelement.152 = u32[10,1]{1,0} gettupleelement(arg_tuple.146), index=5   add.200 = u32[10,1]{1,0} add(xor.198, gettupleelement.152)   add.201 = s32[] add(gettupleelement.148, constant.157)   convert.202 = u32[] convert(add.201)   broadcast.203 = u32[10,1]{1,0} broadcast(convert.202), dimensions={}   add.204 = u32[10,1]{1,0} add(add.200, broadcast.203)   gettupleelement.153 = u32[10,1]{1,0} gettupleelement(arg_tuple.146), index=6   gettupleelement.155 = u32[4]{0} gettupleelement(arg_tuple.146), index=8   ROOT tuple.206 = (s32[], s32[], u32[10,1]{1,0}, u32[10,1]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) tuple(add.205, add.158, add.199, add.204, gettupleelement.152, gettupleelement.153, gettupleelement.151, gettupleelement.155, gettupleelement.154) } region_3.207 {   arg_tuple.208 = (s32[], s32[], u32[10,1]{1,0}, u32[10,1]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) parameter(0)   gettupleelement.210 = s32[] gettupleelement(arg_tuple.208), index=1   gettupleelement.211 = u32[10,1]{1,0} gettupleelement(arg_tuple.208), index=2   gettupleelement.212 = u32[10,1]{1,0} gettupleelement(arg_tuple.208), index=3   gettupleelement.213 = u32[10,1]{1,0} gettupleelement(arg_tuple.208), index=4   gettupleelement.214 = u32[10,1]{1,0} gettupleelement(arg_tuple.208), index=5   gettupleelement.215 = u32[10,1]{1,0} gettupleelement(arg_tuple.208), index=6   gettupleelement.216 = u32[4]{0} gettupleelement(arg_tuple.208), index=7   gettupleelement.217 = u32[4]{0} gettupleelement(arg_tuple.208), index=8   gettupleelement.209 = s32[] gettupleelement(arg_tuple.208), index=0   constant.218 = s32[] constant(5)   ROOT compare.219 = pred[] compare(gettupleelement.209, constant.218), direction=LT } region_4.220 {   arg_tuple.221 = (s32[], s32[], u32[10,1]{1,0}, u32[10,1]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) parameter(0)   gettupleelement.222 = s32[] gettupleelement(arg_tuple.221), index=0   constant.232 = s32[] constant(1)   add.280 = s32[] add(gettupleelement.222, constant.232)   gettupleelement.223 = s32[] gettupleelement(arg_tuple.221), index=1   add.233 = s32[] add(gettupleelement.223, constant.232)   gettupleelement.224 = u32[10,1]{1,0} gettupleelement(arg_tuple.221), index=2   gettupleelement.225 = u32[10,1]{1,0} gettupleelement(arg_tuple.221), index=3   add.236 = u32[10,1]{1,0} add(gettupleelement.224, gettupleelement.225)   gettupleelement.229 = u32[4]{0} gettupleelement(arg_tuple.221), index=7   slice.234 = u32[1]{0} slice(gettupleelement.229), slice={[0:1]}   reshape.235 = u32[] reshape(slice.234)   broadcast.237 = u32[10,1]{1,0} broadcast(reshape.235), dimensions={}   shiftleft.238 = u32[10,1]{1,0} shiftleft(gettupleelement.225, broadcast.237)   constant.231 = u32[] constant(32)   subtract.239 = u32[] subtract(constant.231, reshape.235)   broadcast.240 = u32[10,1]{1,0} broadcast(subtract.239), dimensions={}   shiftrightlogical.241 = u32[10,1]{1,0} shiftrightlogical(gettupleelement.225, broadcast.240)   or.242 = u32[10,1]{1,0} or(shiftleft.238, shiftrightlogical.241)   xor.243 = u32[10,1]{1,0} xor(add.236, or.242)   add.246 = u32[10,1]{1,0} add(add.236, xor.243)   slice.244 = u32[1]{0} slice(gettupleelement.229), slice={[1:2]}   reshape.245 = u32[] reshape(slice.244)   broadcast.247 = u32[10,1]{1,0} broadcast(reshape.245), dimensions={}   shiftleft.248 = u32[10,1]{1,0} shiftleft(xor.243, broadcast.247)   subtract.249 = u32[] subtract(constant.231, reshape.245)   broadcast.250 = u32[10,1]{1,0} broadcast(subtract.249), dimensions={}   shiftrightlogical.251 = u32[10,1]{1,0} shiftrightlogical(xor.243, broadcast.250)   or.252 = u32[10,1]{1,0} or(shiftleft.248, shiftrightlogical.251)   xor.253 = u32[10,1]{1,0} xor(add.246, or.252)   add.256 = u32[10,1]{1,0} add(add.246, xor.253)   slice.254 = u32[1]{0} slice(gettupleelement.229), slice={[2:3]}   reshape.255 = u32[] reshape(slice.254)   broadcast.257 = u32[10,1]{1,0} broadcast(reshape.255), dimensions={}   shiftleft.258 = u32[10,1]{1,0} shiftleft(xor.253, broadcast.257)   subtract.259 = u32[] subtract(constant.231, reshape.255)   broadcast.260 = u32[10,1]{1,0} broadcast(subtract.259), dimensions={}   shiftrightlogical.261 = u32[10,1]{1,0} shiftrightlogical(xor.253, broadcast.260)   or.262 = u32[10,1]{1,0} or(shiftleft.258, shiftrightlogical.261)   xor.263 = u32[10,1]{1,0} xor(add.256, or.262)   add.266 = u32[10,1]{1,0} add(add.256, xor.263)   gettupleelement.226 = u32[10,1]{1,0} gettupleelement(arg_tuple.221), index=4   add.274 = u32[10,1]{1,0} add(add.266, gettupleelement.226)   slice.264 = u32[1]{0} slice(gettupleelement.229), slice={[3:4]}   reshape.265 = u32[] reshape(slice.264)   broadcast.267 = u32[10,1]{1,0} broadcast(reshape.265), dimensions={}   shiftleft.268 = u32[10,1]{1,0} shiftleft(xor.263, broadcast.267)   subtract.269 = u32[] subtract(constant.231, reshape.265)   broadcast.270 = u32[10,1]{1,0} broadcast(subtract.269), dimensions={}   shiftrightlogical.271 = u32[10,1]{1,0} shiftrightlogical(xor.263, broadcast.270)   or.272 = u32[10,1]{1,0} or(shiftleft.268, shiftrightlogical.271)   xor.273 = u32[10,1]{1,0} xor(add.266, or.272)   gettupleelement.227 = u32[10,1]{1,0} gettupleelement(arg_tuple.221), index=5   add.275 = u32[10,1]{1,0} add(xor.273, gettupleelement.227)   add.276 = s32[] add(gettupleelement.223, constant.232)   convert.277 = u32[] convert(add.276)   broadcast.278 = u32[10,1]{1,0} broadcast(convert.277), dimensions={}   add.279 = u32[10,1]{1,0} add(add.275, broadcast.278)   gettupleelement.228 = u32[10,1]{1,0} gettupleelement(arg_tuple.221), index=6   gettupleelement.230 = u32[4]{0} gettupleelement(arg_tuple.221), index=8   ROOT tuple.281 = (s32[], s32[], u32[10,1]{1,0}, u32[10,1]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) tuple(add.280, add.233, add.274, add.279, gettupleelement.227, gettupleelement.228, gettupleelement.226, gettupleelement.230, gettupleelement.229) } region_5.282 {   arg_tuple.283 = (s32[], s32[], u32[10,1]{1,0}, u32[10,1]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) parameter(0)   gettupleelement.285 = s32[] gettupleelement(arg_tuple.283), index=1   gettupleelement.286 = u32[10,1]{1,0} gettupleelement(arg_tuple.283), index=2   gettupleelement.287 = u32[10,1]{1,0} gettupleelement(arg_tuple.283), index=3   gettupleelement.288 = u32[10,1]{1,0} gettupleelement(arg_tuple.283), index=4   gettupleelement.289 = u32[10,1]{1,0} gettupleelement(arg_tuple.283), index=5   gettupleelement.290 = u32[10,1]{1,0} gettupleelement(arg_tuple.283), index=6   gettupleelement.291 = u32[4]{0} gettupleelement(arg_tuple.283), index=7   gettupleelement.292 = u32[4]{0} gettupleelement(arg_tuple.283), index=8   gettupleelement.284 = s32[] gettupleelement(arg_tuple.283), index=0   constant.293 = s32[] constant(5)   ROOT compare.294 = pred[] compare(gettupleelement.284, constant.293), direction=LT } _randint.295 {   constant.303 = s32[] constant(0)   Arg_0.296 = u32[10,2]{1,0} parameter(0)   call.312 = u32[10,2,2]{2,1,0} call(Arg_0.296), to_apply=_threefry_split.102   slice.313 = u32[10,1,2]{2,1,0} slice(call.312), slice={[0:10], [0:1], [0:2]}   reshape.314 = u32[10,2]{1,0} reshape(slice.313)   slice.317 = u32[10,1]{1,0} slice(reshape.314), slice={[0:10], [0:1]}   slice.318 = u32[10,1]{1,0} slice(reshape.314), slice={[0:10], [1:2]}   xor.319 = u32[10,1]{1,0} xor(slice.317, slice.318)   constant.299 = u32[] constant(466688986)   broadcast.300 = u32[10,1]{1,0} broadcast(constant.299), dimensions={}   xor.320 = u32[10,1]{1,0} xor(xor.319, broadcast.300)   constant.305 = u32[4]{0} constant({13, 15, 26, 6})   constant.304 = u32[4]{0} constant({17, 29, 16, 24})   tuple.321 = (s32[], s32[], u32[10,1]{1,0}, u32[10,1]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) tuple(constant.303, constant.303, slice.317, slice.318, slice.318, xor.320, slice.317, constant.305, constant.304)   while.322 = (s32[], s32[], u32[10,1]{1,0}, u32[10,1]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) while(tuple.321), condition=region_3.207, body=region_2.145   gettupleelement.323 = s32[] gettupleelement(while.322), index=0   gettupleelement.324 = s32[] gettupleelement(while.322), index=1   gettupleelement.326 = u32[10,1]{1,0} gettupleelement(while.322), index=3   gettupleelement.327 = u32[10,1]{1,0} gettupleelement(while.322), index=4   gettupleelement.328 = u32[10,1]{1,0} gettupleelement(while.322), index=5   gettupleelement.329 = u32[10,1]{1,0} gettupleelement(while.322), index=6   gettupleelement.330 = u32[4]{0} gettupleelement(while.322), index=7   gettupleelement.331 = u32[4]{0} gettupleelement(while.322), index=8   slice.315 = u32[10,1,2]{2,1,0} slice(call.312), slice={[0:10], [1:2], [0:2]}   reshape.316 = u32[10,2]{1,0} reshape(slice.315)   slice.333 = u32[10,1]{1,0} slice(reshape.316), slice={[0:10], [0:1]}   slice.334 = u32[10,1]{1,0} slice(reshape.316), slice={[0:10], [1:2]}   xor.335 = u32[10,1]{1,0} xor(slice.333, slice.334)   xor.336 = u32[10,1]{1,0} xor(xor.335, broadcast.300)   tuple.337 = (s32[], s32[], u32[10,1]{1,0}, u32[10,1]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) tuple(constant.303, constant.303, slice.333, slice.334, slice.334, xor.336, slice.333, constant.305, constant.304)   while.338 = (s32[], s32[], u32[10,1]{1,0}, u32[10,1]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) while(tuple.337), condition=region_5.282, body=region_4.220   gettupleelement.339 = s32[] gettupleelement(while.338), index=0   gettupleelement.340 = s32[] gettupleelement(while.338), index=1   gettupleelement.342 = u32[10,1]{1,0} gettupleelement(while.338), index=3   gettupleelement.343 = u32[10,1]{1,0} gettupleelement(while.338), index=4   gettupleelement.344 = u32[10,1]{1,0} gettupleelement(while.338), index=5   gettupleelement.345 = u32[10,1]{1,0} gettupleelement(while.338), index=6   gettupleelement.346 = u32[4]{0} gettupleelement(while.338), index=7   gettupleelement.347 = u32[4]{0} gettupleelement(while.338), index=8   Arg_1.297 = s32[] parameter(1)   constant.307 = s32[] constant(2147483648)   constant.306 = s32[] constant(2147483647)   call.310 = s32[] call(Arg_1.297, constant.307, constant.306), to_apply=clip_0.9   broadcast.370 = s32[10]{0} broadcast(call.310), dimensions={}   gettupleelement.325 = u32[10,1]{1,0} gettupleelement(while.322), index=2   reshape.332 = u32[10]{0} reshape(gettupleelement.325)   Arg_2.298 = s32[] parameter(2)   call.308 = s32[] call(constant.306, constant.307, constant.306), to_apply=clip.3   compare.309 = pred[] compare(Arg_2.298, call.308), direction=GT   call.311 = s32[] call(Arg_2.298, constant.307, constant.306), to_apply=clip_0.15   compare.353 = pred[] compare(call.311, call.310), direction=GT   and.354 = pred[] and(compare.309, compare.353)   compare.351 = pred[] compare(call.311, call.310), direction=LE   constant.302 = u32[] constant(1)   subtract.349 = s32[] subtract(call.311, call.310)   convert.350 = u32[] convert(subtract.349)   select.352 = u32[] select(compare.351, constant.302, convert.350)   add.355 = u32[] add(select.352, constant.302)   select.356 = u32[] select(and.354, add.355, select.352)   broadcast.360 = u32[10]{0} broadcast(select.356), dimensions={}   remainder.361 = u32[10]{0} remainder(reshape.332, broadcast.360)   constant.301 = u32[] constant(65536)   remainder.357 = u32[] remainder(constant.301, select.356)   multiply.358 = u32[] multiply(remainder.357, remainder.357)   remainder.359 = u32[] remainder(multiply.358, select.356)   broadcast.362 = u32[10]{0} broadcast(remainder.359), dimensions={}   multiply.363 = u32[10]{0} multiply(remainder.361, broadcast.362)   gettupleelement.341 = u32[10,1]{1,0} gettupleelement(while.338), index=2   reshape.348 = u32[10]{0} reshape(gettupleelement.341)   broadcast.364 = u32[10]{0} broadcast(select.356), dimensions={}   remainder.365 = u32[10]{0} remainder(reshape.348, broadcast.364)   add.366 = u32[10]{0} add(multiply.363, remainder.365)   broadcast.367 = u32[10]{0} broadcast(select.356), dimensions={}   remainder.368 = u32[10]{0} remainder(add.366, broadcast.367)   convert.369 = s32[10]{0} convert(remainder.368)   ROOT add.371 = s32[10]{0} add(broadcast.370, convert.369) } region_7.372 {   arg_tuple.373 = (s32[], s32[], u32[10,1]{1,0}, u32[10,1]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) parameter(0)   gettupleelement.374 = s32[] gettupleelement(arg_tuple.373), index=0   constant.384 = s32[] constant(1)   add.432 = s32[] add(gettupleelement.374, constant.384)   gettupleelement.375 = s32[] gettupleelement(arg_tuple.373), index=1   add.385 = s32[] add(gettupleelement.375, constant.384)   gettupleelement.376 = u32[10,1]{1,0} gettupleelement(arg_tuple.373), index=2   gettupleelement.377 = u32[10,1]{1,0} gettupleelement(arg_tuple.373), index=3   add.388 = u32[10,1]{1,0} add(gettupleelement.376, gettupleelement.377)   gettupleelement.381 = u32[4]{0} gettupleelement(arg_tuple.373), index=7   slice.386 = u32[1]{0} slice(gettupleelement.381), slice={[0:1]}   reshape.387 = u32[] reshape(slice.386)   broadcast.389 = u32[10,1]{1,0} broadcast(reshape.387), dimensions={}   shiftleft.390 = u32[10,1]{1,0} shiftleft(gettupleelement.377, broadcast.389)   constant.383 = u32[] constant(32)   subtract.391 = u32[] subtract(constant.383, reshape.387)   broadcast.392 = u32[10,1]{1,0} broadcast(subtract.391), dimensions={}   shiftrightlogical.393 = u32[10,1]{1,0} shiftrightlogical(gettupleelement.377, broadcast.392)   or.394 = u32[10,1]{1,0} or(shiftleft.390, shiftrightlogical.393)   xor.395 = u32[10,1]{1,0} xor(add.388, or.394)   add.398 = u32[10,1]{1,0} add(add.388, xor.395)   slice.396 = u32[1]{0} slice(gettupleelement.381), slice={[1:2]}   reshape.397 = u32[] reshape(slice.396)   broadcast.399 = u32[10,1]{1,0} broadcast(reshape.397), dimensions={}   shiftleft.400 = u32[10,1]{1,0} shiftleft(xor.395, broadcast.399)   subtract.401 = u32[] subtract(constant.383, reshape.397)   broadcast.402 = u32[10,1]{1,0} broadcast(subtract.401), dimensions={}   shiftrightlogical.403 = u32[10,1]{1,0} shiftrightlogical(xor.395, broadcast.402)   or.404 = u32[10,1]{1,0} or(shiftleft.400, shiftrightlogical.403)   xor.405 = u32[10,1]{1,0} xor(add.398, or.404)   add.408 = u32[10,1]{1,0} add(add.398, xor.405)   slice.406 = u32[1]{0} slice(gettupleelement.381), slice={[2:3]}   reshape.407 = u32[] reshape(slice.406)   broadcast.409 = u32[10,1]{1,0} broadcast(reshape.407), dimensions={}   shiftleft.410 = u32[10,1]{1,0} shiftleft(xor.405, broadcast.409)   subtract.411 = u32[] subtract(constant.383, reshape.407)   broadcast.412 = u32[10,1]{1,0} broadcast(subtract.411), dimensions={}   shiftrightlogical.413 = u32[10,1]{1,0} shiftrightlogical(xor.405, broadcast.412)   or.414 = u32[10,1]{1,0} or(shiftleft.410, shiftrightlogical.413)   xor.415 = u32[10,1]{1,0} xor(add.408, or.414)   add.418 = u32[10,1]{1,0} add(add.408, xor.415)   gettupleelement.378 = u32[10,1]{1,0} gettupleelement(arg_tuple.373), index=4   add.426 = u32[10,1]{1,0} add(add.418, gettupleelement.378)   slice.416 = u32[1]{0} slice(gettupleelement.381), slice={[3:4]}   reshape.417 = u32[] reshape(slice.416)   broadcast.419 = u32[10,1]{1,0} broadcast(reshape.417), dimensions={}   shiftleft.420 = u32[10,1]{1,0} shiftleft(xor.415, broadcast.419)   subtract.421 = u32[] subtract(constant.383, reshape.417)   broadcast.422 = u32[10,1]{1,0} broadcast(subtract.421), dimensions={}   shiftrightlogical.423 = u32[10,1]{1,0} shiftrightlogical(xor.415, broadcast.422)   or.424 = u32[10,1]{1,0} or(shiftleft.420, shiftrightlogical.423)   xor.425 = u32[10,1]{1,0} xor(add.418, or.424)   gettupleelement.379 = u32[10,1]{1,0} gettupleelement(arg_tuple.373), index=5   add.427 = u32[10,1]{1,0} add(xor.425, gettupleelement.379)   add.428 = s32[] add(gettupleelement.375, constant.384)   convert.429 = u32[] convert(add.428)   broadcast.430 = u32[10,1]{1,0} broadcast(convert.429), dimensions={}   add.431 = u32[10,1]{1,0} add(add.427, broadcast.430)   gettupleelement.380 = u32[10,1]{1,0} gettupleelement(arg_tuple.373), index=6   gettupleelement.382 = u32[4]{0} gettupleelement(arg_tuple.373), index=8   ROOT tuple.433 = (s32[], s32[], u32[10,1]{1,0}, u32[10,1]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) tuple(add.432, add.385, add.426, add.431, gettupleelement.379, gettupleelement.380, gettupleelement.378, gettupleelement.382, gettupleelement.381) } region_8.434 {   arg_tuple.435 = (s32[], s32[], u32[10,1]{1,0}, u32[10,1]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) parameter(0)   gettupleelement.437 = s32[] gettupleelement(arg_tuple.435), index=1   gettupleelement.438 = u32[10,1]{1,0} gettupleelement(arg_tuple.435), index=2   gettupleelement.439 = u32[10,1]{1,0} gettupleelement(arg_tuple.435), index=3   gettupleelement.440 = u32[10,1]{1,0} gettupleelement(arg_tuple.435), index=4   gettupleelement.441 = u32[10,1]{1,0} gettupleelement(arg_tuple.435), index=5   gettupleelement.442 = u32[10,1]{1,0} gettupleelement(arg_tuple.435), index=6   gettupleelement.443 = u32[4]{0} gettupleelement(arg_tuple.435), index=7   gettupleelement.444 = u32[4]{0} gettupleelement(arg_tuple.435), index=8   gettupleelement.436 = s32[] gettupleelement(arg_tuple.435), index=0   constant.445 = s32[] constant(5)   ROOT compare.446 = pred[] compare(gettupleelement.436, constant.445), direction=LT } _uniform.447 {   constant.459 = s32[] constant(0)   Arg_0.448 = u32[10,2]{1,0} parameter(0)   slice.464 = u32[10,1]{1,0} slice(Arg_0.448), slice={[0:10], [0:1]}   slice.465 = u32[10,1]{1,0} slice(Arg_0.448), slice={[0:10], [1:2]}   xor.466 = u32[10,1]{1,0} xor(slice.464, slice.465)   constant.457 = u32[] constant(466688986)   broadcast.458 = u32[10,1]{1,0} broadcast(constant.457), dimensions={}   xor.467 = u32[10,1]{1,0} xor(xor.466, broadcast.458)   constant.461 = u32[4]{0} constant({13, 15, 26, 6})   constant.460 = u32[4]{0} constant({17, 29, 16, 24})   tuple.468 = (s32[], s32[], u32[10,1]{1,0}, u32[10,1]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) tuple(constant.459, constant.459, slice.464, slice.465, slice.465, xor.467, slice.464, constant.461, constant.460)   while.469 = (s32[], s32[], u32[10,1]{1,0}, u32[10,1]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) while(tuple.468), condition=region_8.434, body=region_7.372   gettupleelement.470 = s32[] gettupleelement(while.469), index=0   gettupleelement.471 = s32[] gettupleelement(while.469), index=1   gettupleelement.473 = u32[10,1]{1,0} gettupleelement(while.469), index=3   gettupleelement.474 = u32[10,1]{1,0} gettupleelement(while.469), index=4   gettupleelement.475 = u32[10,1]{1,0} gettupleelement(while.469), index=5   gettupleelement.476 = u32[10,1]{1,0} gettupleelement(while.469), index=6   gettupleelement.477 = u32[4]{0} gettupleelement(while.469), index=7   gettupleelement.478 = u32[4]{0} gettupleelement(while.469), index=8   Arg_1.449 = f32[] parameter(1)   reshape.494 = f32[1,1]{1,0} reshape(Arg_1.449)   broadcast.495 = f32[1,1]{1,0} broadcast(reshape.494), dimensions={0,1}   reshape.496 = f32[1]{0} reshape(broadcast.495)   broadcast.497 = f32[10,1]{1,0} broadcast(reshape.496), dimensions={1}   gettupleelement.472 = u32[10,1]{1,0} gettupleelement(while.469), index=2   constant.455 = u32[] constant(9)   broadcast.456 = u32[10,1]{1,0} broadcast(constant.455), dimensions={}   shiftrightlogical.479 = u32[10,1]{1,0} shiftrightlogical(gettupleelement.472, broadcast.456)   constant.453 = u32[] constant(1065353216)   broadcast.454 = u32[10,1]{1,0} broadcast(constant.453), dimensions={}   or.480 = u32[10,1]{1,0} or(shiftrightlogical.479, broadcast.454)   bitcastconvert.481 = f32[10,1]{1,0} bitcastconvert(or.480)   constant.451 = f32[] constant(1)   broadcast.452 = f32[10,1]{1,0} broadcast(constant.451), dimensions={}   subtract.482 = f32[10,1]{1,0} subtract(bitcastconvert.481, broadcast.452)   Arg_2.450 = f32[] parameter(2)   reshape.463 = f32[1]{0} reshape(Arg_2.450)   reshape.462 = f32[1]{0} reshape(Arg_1.449)   subtract.483 = f32[1]{0} subtract(reshape.463, reshape.462)   reshape.484 = f32[1,1]{1,0} reshape(subtract.483)   broadcast.485 = f32[1,1]{1,0} broadcast(reshape.484), dimensions={0,1}   reshape.486 = f32[1]{0} reshape(broadcast.485)   broadcast.487 = f32[10,1]{1,0} broadcast(reshape.486), dimensions={1}   multiply.488 = f32[10,1]{1,0} multiply(subtract.482, broadcast.487)   reshape.489 = f32[1,1]{1,0} reshape(Arg_1.449)   broadcast.490 = f32[1,1]{1,0} broadcast(reshape.489), dimensions={0,1}   reshape.491 = f32[1]{0} reshape(broadcast.490)   broadcast.492 = f32[10,1]{1,0} broadcast(reshape.491), dimensions={1}   add.493 = f32[10,1]{1,0} add(multiply.488, broadcast.492)   ROOT maximum.498 = f32[10,1]{1,0} maximum(broadcast.497, add.493) } region_9.499 {   arg_tuple.500 = (f32[10]{0}, f32[10,1]{1,0}) parameter(0)   gettupleelement.502 = f32[10,1]{1,0} gettupleelement(arg_tuple.500), index=1   reshape.511 = f32[10]{0} reshape(gettupleelement.502)   constant.503 = f32[] constant(2)   broadcast.504 = f32[10]{0} broadcast(constant.503), dimensions={}   compare.512 = pred[10]{0} compare(reshape.511, broadcast.504), direction=LT   gettupleelement.501 = f32[10]{0} gettupleelement(arg_tuple.500), index=0   constant.507 = f32[] constant(0.1)   broadcast.508 = f32[10]{0} broadcast(constant.507), dimensions={}   add.509 = f32[10]{0} add(gettupleelement.501, broadcast.508)   select.513 = f32[10]{0} select(compare.512, add.509, gettupleelement.501)   reshape.514 = pred[10,1]{1,0} reshape(compare.512)   constant.505 = f32[] constant(0.1)   broadcast.506 = f32[10,1]{1,0} broadcast(constant.505), dimensions={}   add.510 = f32[10,1]{1,0} add(gettupleelement.502, broadcast.506)   select.515 = f32[10,1]{1,0} select(reshape.514, add.510, gettupleelement.502)   ROOT tuple.516 = (f32[10]{0}, f32[10,1]{1,0}) tuple(select.513, select.515) } region_11.517 {   Arg_0.518 = pred[] parameter(0)   Arg_1.519 = pred[] parameter(1)   ROOT or.520 = pred[] or(Arg_0.518, Arg_1.519) } region_10.521 {   arg_tuple.522 = (f32[10]{0}, f32[10,1]{1,0}) parameter(0)   gettupleelement.523 = f32[10]{0} gettupleelement(arg_tuple.522), index=0   gettupleelement.524 = f32[10,1]{1,0} gettupleelement(arg_tuple.522), index=1   reshape.528 = f32[10]{0} reshape(gettupleelement.524)   constant.526 = f32[] constant(2)   broadcast.527 = f32[10]{0} broadcast(constant.526), dimensions={}   compare.529 = pred[10]{0} compare(reshape.528, broadcast.527), direction=LT   constant.525 = pred[] constant(false)   ROOT reduce.530 = pred[] reduce(compare.529, constant.525), dimensions={0}, to_apply=region_11.517 } region_6.531 {   arg_tuple.532 = (f32[10]{0}, f32[10,1]{1,0}, s32[10]{0}) parameter(0)   gettupleelement.535 = s32[10]{0} gettupleelement(arg_tuple.532), index=2   constant.536 = s32[] constant(5)   broadcast.537 = s32[10]{0} broadcast(constant.536), dimensions={}   compare.556 = pred[10]{0} compare(gettupleelement.535, broadcast.537), direction=LT   gettupleelement.533 = f32[10]{0} gettupleelement(arg_tuple.532), index=0   constant.542 = s32[] constant(32)   broadcast.543 = s32[10]{0} broadcast(constant.542), dimensions={}   shiftrightlogical.544 = s32[10]{0} shiftrightlogical(gettupleelement.535, broadcast.543)   convert.545 = u32[10]{0} convert(shiftrightlogical.544)   reshape.546 = u32[10,1]{1,0} reshape(convert.545)   convert.547 = u32[10]{0} convert(gettupleelement.535)   reshape.548 = u32[10,1]{1,0} reshape(convert.547)   concatenate.549 = u32[10,2]{1,0} concatenate(reshape.546, reshape.548), dimensions={1}   constant.541 = f32[] constant(0)   constant.540 = f32[] constant(1)   call.550 = f32[10,1]{1,0} call(concatenate.549, constant.541, constant.540), to_apply=_uniform.447   tuple.551 = (f32[10]{0}, f32[10,1]{1,0}) tuple(gettupleelement.533, call.550)   while.552 = (f32[10]{0}, f32[10,1]{1,0}) while(tuple.551), condition=region_10.521, body=region_9.499   gettupleelement.553 = f32[10]{0} gettupleelement(while.552), index=0   select.557 = f32[10]{0} select(compare.556, gettupleelement.553, gettupleelement.533)   reshape.558 = pred[10,1]{1,0} reshape(compare.556)   gettupleelement.554 = f32[10,1]{1,0} gettupleelement(while.552), index=1   gettupleelement.534 = f32[10,1]{1,0} gettupleelement(arg_tuple.532), index=1   select.559 = f32[10,1]{1,0} select(reshape.558, gettupleelement.554, gettupleelement.534)   constant.538 = s32[] constant(1)   broadcast.539 = s32[10]{0} broadcast(constant.538), dimensions={}   add.555 = s32[10]{0} add(gettupleelement.535, broadcast.539)   select.560 = s32[10]{0} select(compare.556, add.555, gettupleelement.535)   ROOT tuple.561 = (f32[10]{0}, f32[10,1]{1,0}, s32[10]{0}) tuple(select.557, select.559, select.560) } region_13.562 {   Arg_0.563 = pred[] parameter(0)   Arg_1.564 = pred[] parameter(1)   ROOT or.565 = pred[] or(Arg_0.563, Arg_1.564) } region_12.566 {   arg_tuple.567 = (f32[10]{0}, f32[10,1]{1,0}, s32[10]{0}) parameter(0)   gettupleelement.568 = f32[10]{0} gettupleelement(arg_tuple.567), index=0   gettupleelement.569 = f32[10,1]{1,0} gettupleelement(arg_tuple.567), index=1   gettupleelement.570 = s32[10]{0} gettupleelement(arg_tuple.567), index=2   constant.572 = s32[] constant(5)   broadcast.573 = s32[10]{0} broadcast(constant.572), dimensions={}   compare.574 = pred[10]{0} compare(gettupleelement.570, broadcast.573), direction=LT   constant.571 = pred[] constant(false)   ROOT reduce.575 = pred[] reduce(compare.574, constant.571), dimensions={0}, to_apply=region_13.562 } solve.576 {   constant.579 = f32[] constant(0)   broadcast.580 = f32[10]{0} broadcast(constant.579), dimensions={}   Arg_0.577 = f32[10,1]{1,0} parameter(0)   Arg_1.578 = u32[10,2]{1,0} parameter(1)   constant.581 = s32[] constant(2)   constant.582 = s32[] constant(2)   call.583 = s32[10]{0} call(Arg_1.578, constant.581, constant.582), to_apply=_randint.295   tuple.584 = (f32[10]{0}, f32[10,1]{1,0}, s32[10]{0}) tuple(broadcast.580, Arg_0.577, call.583)   while.585 = (f32[10]{0}, f32[10,1]{1,0}, s32[10]{0}) while(tuple.584), condition=region_12.566, body=region_6.531   gettupleelement.586 = f32[10]{0} gettupleelement(while.585), index=0   ROOT gettupleelement.587 = f32[10,1]{1,0} gettupleelement(while.585), index=1   gettupleelement.588 = s32[10]{0} gettupleelement(while.585), index=2 } ENTRY main.591 {   Arg_0.1 = f32[10,1]{1,0} parameter(0)   Arg_1.2 = u32[10,2]{1,0} parameter(1)   call.589 = f32[10,1]{1,0} call(Arg_0.1, Arg_1.2), to_apply=solve.576   ROOT tuple.590 = (f32[10,1]{1,0}) tuple(call.589) } ``` ",For performancerelated things like this it is usually in XLA. JAX is mostly at the mercy of whatever code XLA generates. Unfortunately the parallelism part of this isn't something I'm familiar with at all. I think  might know more? This one is out of my wheelhouse I'm afraid.
yi,scipy.linalg.tril and .triu changed to sparse," Description When using jax._src.scipy.linalg I get an error saying: `AttributeError: module 'scipy.linalg' has no attribute 'tril'` `AttributeError: module 'scipy.linalg' has no attribute 'triu'` It seems that scipy changed the function namespaces to `scipy.sparse.tril` and `scipy.sparse.triu`.  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.17 jaxlib: 0.4.17 numpy:  1.26.4 python: 3.11.3 (v3.11.3:f3909b8bc8, Apr  4 2023, 20:12:10) [Clang 13.0.0 (clang1300.0.29.30)] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 ```",2024-04-27T09:13:30Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/20965,Resolved when installing latest release 0.4.16.
yi,Performance Issue Report: JAX Slower Than Autograd on GPU and CPU Setups," Description **Introduction:** This report outlines a performance issue observed with JAX on both GPU and CPU hardware setups. The purpose of this report is to provide detailed feedback to the JAX development team to aid in identifying potential areas for optimization. **Observed Performance Issues:**  **GPU Performance:**    JAX is significantly slower than expected when compared to Autograd on identical tasks, showing a minimum of 5x slower performance on NVIDIA GPUs.  **CPU Performance:**    Similar underperformance is observed on Intel Core i7 CPUs, where JAX operations are markedly slower than those performed with Autograd. **Steps to Reproduce:** 1. Set up the environment with specified hardware and software versions. 2. Run benchmark tests including matrix operations, gradient calculations (ADAM). 3. Compare execution times of JAX and Autograd. **Expected Behavior:** JAX should exhibit comparable or better performance than Autograd given its design for highperformance machine learning tasks, especially on platforms supporting GPU acceleration. **Actual Behavior:** JAX underperforms significantly compared to Autograd across all tested hardware setups. **Attachments:**  **Benchmarking Scripts:**  ADAM (beta1: 0.95, beta2:0.99, epsilon: 0.001), BFGS, NewtonCG, CG (standard scipy.optimize.minimize configuration) on synthetic function: ``` def f(x):   term1 = 0.5 * (x[0]**2 + (x[1]  0.5)**2)   Central parabolic valley    Nested valley 1 (deep, narrow)   term2_x = 4 * anp.exp((x[0] + 0.75)**2  10 * (x[1]  0.3)**2)   term2_y = 8 * anp.exp((x[0]  0.75)**2  10 * (x[1]  0.3)**2)   term2 = term2_x + term2_y    Nested valley 2 (wide, shallow)   term3_x = 2 * anp.sin(5 * anp.pi * (x[0]  1.25)) * anp.sin(5 * anp.pi * (x[1]  1.75))   term3_y = 3 * anp.sin(7 * anp.pi * (x[0]  1.25)) * anp.sin(7 * anp.pi * (x[1]  1.75))   term3 = 0.2 * (term3_x + term3_y)   Adjust coefficient for shallower valley   term4 = 3 * anp.sin(3 * anp.pi * x[0]) * anp.sin(3 * anp.pi * x[1])   Oscillating term   term5 = 5 * anp.exp((x[0] + 1)**2  (x[1] + 1)**2)   Deeper global minimum   term6 = anp.exp((x[0]  1.5)**2  (x[1]  1.5)**2)   Local minimum   term7 = 2 * anp.exp((x[0] + 2)**2  (x[1]  2)**2)   Local minimum   return term1 + term2 + term3 + term4 + term5 + term6 + term7 ``` and ``` def f(x):   term1 = 0.5 * (x[0]**2 + (x[1]  0.5)**2)   Central parabolic valley    Nested valley 1 (deep, narrow)   term2_x = 4 * jnp.exp((x[0] + 0.75)**2  10 * (x[1]  0.3)**2)   term2_y = 8 * jnp.exp((x[0]  0.75)**2  10 * (x[1]  0.3)**2)   term2 = term2_x + term2_y    Nested valley 2 (wide, shallow)   term3_x = 2 * jnp.sin(5 * jnp.pi * (x[0]  1.25)) * jnp.sin(5 * jnp.pi * (x[1]  1.75))   term3_y = 3 * jnp.sin(7 * jnp.pi * (x[0]  1.25)) * jnp.sin(7 * jnp.pi * (x[1]  1.75))   term3 = 0.2 * (term3_x + term3_y)   Adjust coefficient for shallower valley   term4 = 3 * jnp.sin(3 * jnp.pi * x[0]) * jnp.sin(3 * jnp.pi * x[1])   Oscillating term   term5 = 5 * jnp.exp((x[0] + 1)**2  (x[1] + 1)**2)   Deeper global minimum   term6 = jnp.exp((x[0]  1.5)**2  (x[1]  1.5)**2)   Local minimum   term7 = 2 * jnp.exp((x[0] + 2)**2  (x[1]  2)**2)   Local minimum   return term1 + term2 + term3 + term4 + term5 + term6 + term7 ``` **Conclusion:** JAX is rich in features, but is slower than Autograd.  **Recommendations:**  Conduct a thorough investigation into the causes of the observed performance bottlenecks. **Acknowledgments:** Thank you to the developers of JAX for their ongoing efforts and contributions to the opensource community.  System info (python version, jaxlib version, accelerator, etc.)    **Hardware:** Intel Core i79750H CPU, 16GB DDR4 RAM, NVIDIA GTX 1650 GPU, NVIDIA Tesla L4 GPU (used in Colab Pro)    **Software:** JAX version 0.4.26, Jaxlib version 0.4.26, Python version 3.9.15, Jupyter notebook 5.7.2.    **Comparison Reference:** Autograd version 1.6.2   **Additional info:**  > JAX Available devices: [cuda(id=0)] > Torch CUDA Available: True > Torch CUDA Device Name: NVIDIA GeForce GTX 1650 (on Colab Nvidia L4) > Torch Current CUDA Device ID: 0 > Torch Number of GPUs: 1",2024-04-26T10:38:30Z,bug performance,open,0,4,https://github.com/jax-ml/jax/issues/20948,"What happens if you wrap the jax function in `jax.jit`? Also, can you include details on how you ran the benchmarks? Keep in mind these tips to make sure you're measuring what you think you're measuring when running benchmarks of JAX code: https://jax.readthedocs.io/en/latest/faq.htmlbenchmarkingjaxcode","Hi Jake,  I read the documentation you mentioned, I believe I haven't miss anything important, since my code is simple and trivial. 1. I wrapped the function using one of two ways:     The decorator: ``     Directly on the function: `jax.jit(f)` 2. I moved `x0` to the GPU as follows:    ```    x = jnp.array([10., 80])    x0 = device_put(x, jax.devices('gpu')[0])    ``` 3. I ran identical code samples using `jnp` and `anp`. The `anp` version completed in under a second, while the `jnp` version has been running for over 10 minutes (I reduced the number of iteration to a minimum number to finish the test and take the screenshots, unlike `anp` which broke the loop upon meeting convergence criteria). 4. Here is ADAM with `time `: ``` def adam(grad_func, x0, alpha=0.01, beta1=0.95, beta2=0.99, epsilon=1e3):     start_time = time.time()     max_iter=500     initial_function_value = f(x0)     initial_function_value = f(x0)     m = jnp.zeros_like(x0)     v = jnp.zeros_like(x0)     t = 0     x = x0     path = [x0]     while True:     for i in range(max_iter):         grad = grad_func(x)         t += 1         m = beta1 * m + (1  beta1) * grad         v = beta2 * v + (1  beta2) * grad ** 2         m_hat = m / (1  beta1 ** t)         v_hat = v / (1  beta2 ** t)         x = x  alpha * m_hat / (jnp.sqrt(v_hat) + epsilon)         path.append(x)         if jnp.linalg.norm(grad) < epsilon or abs(f(x)  initial_function_value) <= epsilon:             break     initial_function_value = f(x)     end_time = time.time()   End timing     execution_time = end_time  start_time   Calculate total execution time     return x, path, execution_time ``` I am using the `time` library for rough performance measurement. The function in question is simple, as described. !1 !2","When I try benchmarking your original function using `jax.jit`, I find that JAX is 4x faster than autograd on both CPU and GPU for inputs of size 1000 ```python import autograd.numpy as anp import jax import jax.numpy as jnp def f_autograd(x):   term1 = 0.5 * (x[0]**2 + (x[1]  0.5)**2)   Central parabolic valley    Nested valley 1 (deep, narrow)   term2_x = 4 * anp.exp((x[0] + 0.75)**2  10 * (x[1]  0.3)**2)   term2_y = 8 * anp.exp((x[0]  0.75)**2  10 * (x[1]  0.3)**2)   term2 = term2_x + term2_y    Nested valley 2 (wide, shallow)   term3_x = 2 * anp.sin(5 * anp.pi * (x[0]  1.25)) * anp.sin(5 * anp.pi * (x[1]  1.75))   term3_y = 3 * anp.sin(7 * anp.pi * (x[0]  1.25)) * anp.sin(7 * anp.pi * (x[1]  1.75))   term3 = 0.2 * (term3_x + term3_y)   Adjust coefficient for shallower valley   term4 = 3 * anp.sin(3 * anp.pi * x[0]) * anp.sin(3 * anp.pi * x[1])   Oscillating term   term5 = 5 * anp.exp((x[0] + 1)**2  (x[1] + 1)**2)   Deeper global minimum   term6 = anp.exp((x[0]  1.5)**2  (x[1]  1.5)**2)   Local minimum   term7 = 2 * anp.exp((x[0] + 2)**2  (x[1]  2)**2)   Local minimum   return term1 + term2 + term3 + term4 + term5 + term6 + term7 .jit def f_jax(x):   term1 = 0.5 * (x[0]**2 + (x[1]  0.5)**2)   Central parabolic valley    Nested valley 1 (deep, narrow)   term2_x = 4 * jnp.exp((x[0] + 0.75)**2  10 * (x[1]  0.3)**2)   term2_y = 8 * jnp.exp((x[0]  0.75)**2  10 * (x[1]  0.3)**2)   term2 = term2_x + term2_y    Nested valley 2 (wide, shallow)   term3_x = 2 * jnp.sin(5 * jnp.pi * (x[0]  1.25)) * jnp.sin(5 * jnp.pi * (x[1]  1.75))   term3_y = 3 * jnp.sin(7 * jnp.pi * (x[0]  1.25)) * jnp.sin(7 * jnp.pi * (x[1]  1.75))   term3 = 0.2 * (term3_x + term3_y)   Adjust coefficient for shallower valley   term4 = 3 * jnp.sin(3 * jnp.pi * x[0]) * jnp.sin(3 * jnp.pi * x[1])   Oscillating term   term5 = 5 * jnp.exp((x[0] + 1)**2  (x[1] + 1)**2)   Deeper global minimum   term6 = jnp.exp((x[0]  1.5)**2  (x[1]  1.5)**2)   Local minimum   term7 = 2 * jnp.exp((x[0] + 2)**2  (x[1]  2)**2)   Local minimum   return term1 + term2 + term3 + term4 + term5 + term6 + term7 shape = (2, 1000) x_autograd = anp.ones(shape) %timeit f_autograd(x_autograd)  797 µs ± 440 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) x_jax = jnp.ones(shape) _ = f_jax(x_jax)   trigger compilation %timeit f_jax(x_jax).block_until_ready()  141 µs ± 17.1 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each) ``` This is on a Colab CPU runtime, using the builtin `%timeit` magic function. On a Colab T4 GPU, the timings I get are: ``` 374 µs ± 73.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) 92.9 µs ± 3.18 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each) ``` If you could include your full endtoend benchmark script, including all imports, array definitions, function definitions, and function calls, I may be able to comment on why you are seeing different results.","Ah I think now I see, when I run your snippet I got this warning: ``` CUDA backend failed to initialize: Unable to use CUDA because of the following issues with CUDA components: Outdated cuSPARSE installation found. Version JAX was built against: 12200 Minimum supported: 12100 Installed version: 12002 The local installation version must be no lower than 12100. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) 221 µs ± 3.81 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each) 118 µs ± 3.85 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each) ``` And got : `JAX Available devices: [CpuDevice(id=0)] ` When running: ``` devices = jax.devices() print(""JAX Available devices:"", devices) ``` But when I import `jaxlib` this warning disappears, and the performance drops to become almost equal to autograd (btw, never seen this warning before, maybe because I was importing jaxlib, but why really?) ``` 227 µs ± 19.1 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)  autograd 182 µs ± 49.8 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)  Jax ``` and still :  `JAX Available devices: [CpuDevice(id=0)] ` This is my `nvcc version`: ``` nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 20052023 NVIDIA Corporation Built on Tue_Feb__7_19:32:13_PST_2023 Cuda compilation tools, release 12.1, V12.1.66 Build cuda_12.1.r12.1/compiler.32415258_0 ``` And my `jax` and `jaxlib` versions are `0.4.26`"
rag,Implements Ragged Dot API," Background Ragged Dot is a specialized matrix multiplication operation that is commonly used in the context of Mixture of Experts (MoE) models. MoE models are a type of neural network architecture that consists of a collection of independent expert networks, each of which is responsible for processing a specific subset of the input data. In order to determine which expert should process a given input, a routing mechanism is employed. Ragged Dot plays a crucial role in this routing process. At the linear algebra level, Ragged Dot can be defined as follows: Given matrices, A (m x k), B (g x k x n), and G (g), where m is the number of input samples, k is the dimensionality of the input features, and g is the number of experts, the Ragged Dot operation produces a matrix C (m x n), where each row of C corresponds to the weighted sum of the corresponding row of A and the columns of B associated with the expert assigned to that input sample. More formally, the (i, j)th element of C is computed as follows: ``` C[i, j] = \sum_{k=1}^{k} A[i, k] * B[g,k, j] ``` where k ranges over the columns of B associated with the expert assigned to the ith input sample. The key characteristic of Ragged Dot is that the rows of A and slices of B in the 0th dimension (of size g) are grouped into disjoint sets, with each set corresponding to an expert. This grouping structure allows the routing mechanism to efficiently assign input samples to the appropriate experts.  Requirements Arguments: + lhs: (m, k) shaped array with integer or floating point element type. (REQUIRED) + rhs: (g1, k, n) shaped array with integer or floating point element type, where g1 denotes the number of local groups. (REQUIRED) + group_sizes: (g2,) shaped array with integer element type, where g2 denotes number of groups. The ith element indicates the size of ith group. (REQUIRED) + precision: consistent with precision argument for jax.lax.dot. (OPTIONAL). + preferred_element_type: the element type (jnp.dtype) for the output array. Consistent with preferred_element_type argument for jax.lax.dot. (OPTIONAL). + group_offset: (1,) shaped array. Indicates the group in group_sizes to start computing from. (OPTIONAL) If not specified, defaults to [0]. + existing_output: (m, n) shaped array with elements of type preferred_element_type, where the output should we written to. (OPTIONAL) defaults to None, in which case the output should be written to a newly allocated array. Results: + (m, n) shaped array with preferred_element_type element type. Preconditions: 1. group_sizes = [s_1, ..., s_g2], where s_1 + ... + s_g2 <= m 2. g1 <= g2 (number of local groups <= number of groups). If g1 == g2, group_offsets must be either unspecified or explicitly set to [0]. If g1 < g2, group_offsets must contain a single value in [0, g2) where group_offsets[0] + g1 <= g2. In this case, we perform g1 dots, where irrelevant slices of the results remain unchanged.",2024-04-25T20:46:17Z,pull ready,closed,0,9,https://github.com/jax-ml/jax/issues/20940,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Assigning , who has thought a bit about how to support ragged operations more broadly in JAX.", Removed uses of numpy functions from lax.ragged_dot  Added _CompileAndCheck tests in addition to self._CheckAgainstNumpy.,PTAL.,Implemented the ragged_dot as JAX primitive.,Addressed all the comments and replaced _reduce_sum() by contracting dimensions in the dot_general() call. PTAL.,Applied Matt's suggestion. Thank you all for the feedback!,"Addressed Jake's feedback, thank you.",Looks great  the last thing we need before getting this submitted is to squash the changes into a single commit.
yi,Enable async dispatch on JAX CPU by setting 'jax_cpu_enable_async_dispatch' to be `True` by default.,"Enable async dispatch on JAX CPU by setting 'jax_cpu_enable_async_dispatch' to be `True` by default. To prevent too much parallelism for nonparallel computations, we add a enqueue event to make sure next computation won't be enqueued until last one is done. In `~PyArray_Storage()`, we now release the python GIL then destroy the underlying buffer to prevent deadlock caused by interactions between argument donations and host callbacks on CPU backend.",2024-04-25T19:27:49Z,,closed,0,0,https://github.com/jax-ml/jax/issues/20939
llama,Adding `tree_util.stack_leaves()` and `tree_util.unstack_leaves()`, `stack_leaves`: Stack the leaves of one or more PyTrees along a new axis.  `unstack_leaves`: Unstack the leaves of a PyTree. References:  https://docs.lieselproject.org/en/v0.1.4/_modules/liesel/goose/pytree.htmlstack_leaves  https://gist.github.com/willwhitney/dd89cac6a5b771ccff18b06b33372c75?permalink_comment_id=4634557gistcomment4634557  https://github.com/ayaka14732/llama2jax/blob/ab33e1f15489daa8b9040389c77e486cd450e461/lib/tree_utils/__init__.py,2024-04-25T13:36:30Z,enhancement,open,0,5,https://github.com/jax-ml/jax/issues/20934,"To be clear, are these the semantics you have in mind? ```python def stack_leaves(pytrees, axis):   return jax.tree.map(lambda *xs: jnp.stack(xs, axis), pytrees) ```","> To be clear, are these the semantics you have in mind? Yes","For something like this, I'd probably lean toward recommending users implement what they need via existing API composability, rather than providing a new API for something that can already be pretty succinctly expressed. What do you think?",Maybe adding tree util cookbook would be useful?  ,"A pytree cookbook would be an interesting idea! This idea also came up in CC(Provide utilities for creating pytrees filled with random samples). , is that something you'd be interested in thinking about?"
yi,jax.numpy.insert returning incorrect results wen jitted on Metal," Description (pasting from Apple Developer forum) The `jax.numpy.insert()` function returns an incorrect result (zeropadding the array) when compiled with `jax.jit`. When not `jit`ted, the results are correct.  MWE ```py import jax import jax.numpy as jnp x = jnp.arange(20).reshape(5, 4) print(f""{x=}\n"") def return_arr_with_ins(arr, ins):     return jnp.insert(arr, 2, ins, axis=1) x2 = return_arr_with_ins(x, 99) print(f""{x2=}\n"") return_arr_with_ins_jit = jax.jit(return_arr_with_ins) x3 = return_arr_with_ins_jit(x, 99) print(f""{x3=}\n"") ```  Output * x2 (computed with the nonjitted function) is correct; x3 just has zeropadding instead of a column of 99 ``` x=Array([[ 0,  1,  2,  3],        [ 4,  5,  6,  7],        [ 8,  9, 10, 11],        [12, 13, 14, 15],        [16, 17, 18, 19]], dtype=int32) x2=Array([[ 0,  1, 99,  2,  3],        [ 4,  5, 99,  6,  7],        [ 8,  9, 99, 10, 11],        [12, 13, 99, 14, 15],        [16, 17, 99, 18, 19]], dtype=int32) x3=Array([[ 0,  1,  2,  3,  0],        [ 4,  5,  6,  7,  0],        [ 8,  9, 10, 11,  0],        [12, 13, 14, 15,  0],        [16, 17, 18, 19,  0]], dtype=int32) ``` * The same code run on a nonmetal machine gives the correct results: ``` x=Array([[ 0,  1,  2,  3],        [ 4,  5,  6,  7],        [ 8,  9, 10, 11],        [12, 13, 14, 15],        [16, 17, 18, 19]], dtype=int32) x2=Array([[ 0,  1, 99,  2,  3],        [ 4,  5, 99,  6,  7],        [ 8,  9, 99, 10, 11],        [12, 13, 99, 14, 15],        [16, 17, 99, 18, 19]], dtype=int32) x3=Array([[ 0,  1, 99,  2,  3],        [ 4,  5, 99,  6,  7],        [ 8,  9, 99, 10, 11],        [12, 13, 99, 14, 15],        [16, 17, 99, 18, 19]], dtype=int32) ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.26 jaxlib: 0.4.23 numpy:  1.26.2 python: 3.12.3  (main, Apr 19 2024, 11:44:52) [Clang 14.0.6 ] jax.devices (1 total, 1 local): [METAL(id=0)] process_count: 1 platform: uname_result(system='Darwin', release='23.4.0', version='Darwin Kernel Version 23.4.0: Fri Mar 15 00:10:42 PDT 2024; root:xnu10063.101.17~1/RELEASE_ARM64_T6000', machine='arm64') ```",2024-04-24T16:53:56Z,bug Apple GPU (Metal) plugin,open,0,1,https://github.com/jax-ml/jax/issues/20918,It is reproducible. The jitted module is incorrectly optimized and we will look into the fix. 
yi,Pallas jax.lax.fori_loop over long inputs slows down," Description Inside Pallas kernels, we often want a loop, and to speed up compilation, we typically use a scan function such as jax.lax.fori_loop. (For example, in the attention kernel example here.) As the length of the loop grows, fori_loop slows down execution substantially (relative to using a Python forloop). I put together a minimal script to isolate the issue, and running it on an A6000, saw a **23x slowdown** on long loops: ``` T=256 python forloop: compile = 177ms, execution ms_per_kernel_call = 0.317ms jax.lax.fori_loop: compile = 193ms, execution ms_per_kernel_call = 0.318ms T=2048 python forloop: compile = 242ms, execution ms_per_kernel_call = 2.247ms jax.lax.fori_loop: compile = 200ms, execution ms_per_kernel_call = 2.255ms T=8192 python forloop: compile = 473ms, execution ms_per_kernel_call = 8.946ms jax.lax.fori_loop: compile = 194ms, execution ms_per_kernel_call = 9.281ms T=16384 python forloop: compile = 776ms, execution ms_per_kernel_call = 18.177ms jax.lax.fori_loop: compile = 198ms, execution ms_per_kernel_call = 22.288ms T=32768 python forloop: compile = 1460ms, execution ms_per_kernel_call = 36.009ms jax.lax.fori_loop: compile = 200ms, execution ms_per_kernel_call = 58.552ms T=65536 python forloop: compile = 2978ms, execution ms_per_kernel_call = 71.313ms jax.lax.fori_loop: compile = 195ms, execution ms_per_kernel_call = 172.925ms ``` Here is the script that generated these results: ```python import time import jax import jax.experimental.pallas as pl import jax.numpy as jnp   PALLAS  class JaxKernel:     fwd_blk_i = 128     fwd_blk_j = 64     def __init__(self, use_scan):         self.use_scan = use_scan     def __call__(self, X):         t, d = X.shape         grid = (t // self.fwd_blk_i,)         Y = pl.pallas_call(             self.fwd_kernel,             grid=grid,             out_shape=jax.ShapeDtypeStruct(X.shape, X.dtype),         )(X)         return Y     def fwd_kernel(self, X_ref, Y_ref):         i = pl.program_id(0)         t, d = X_ref.shape         X_i = pl.load(X_ref, pl.ds(start=i * self.fwd_blk_i, size=self.fwd_blk_i))         Y_i_acc = jnp.zeros([self.fwd_blk_i, d], dtype=X_i.dtype)         def body(j, carry):             B_ij = X_i.sum()             carry += B_ij + j  crashes if loop variable not involved             return carry         if self.use_scan:             Y_i = jax.lax.fori_loop(0, t // self.fwd_blk_j, body, Y_i_acc)         else:             for j in range(0, t // self.fwd_blk_j):                 Y_i_acc = body(j, Y_i_acc)             Y_i = Y_i_acc         pl.store(Y_ref, pl.ds(start=i*self.fwd_blk_i, size=self.fwd_blk_i), Y_i)   BENCHMARK UTILS  def prepare_data(b, t, d, dtype):     """"""Creates the data for a forward pass.""""""     return jax.random.normal(jax.random.PRNGKey(0), shape=(b, t, d), dtype=dtype) def heavyweight(f, n, b, t, d, dtype):     """"""Given a kernel for a single batch, transforms it to repeat on many batches.     Returns output to make sure no computation gets compiled away.""""""     .jit     def heavy_f():         def batch_f(X):             out = f(X).mean()             return out         def scanner(_, __):             X = prepare_data(b, t, d, dtype)             return None, batch_f(X)         _, Y = jax.lax.scan(scanner, None, jnp.arange(n))         return Y     return heavy_f   RUN  def main():      set up pallas att     pallas_scanless = jax.vmap(JaxKernel(use_scan=False))     pallas_scanner = jax.vmap(JaxKernel(use_scan=True))      confirm output is correct     X = prepare_data(1, 2048, 16, jnp.float32)     pallas_scanless_Y = pallas_scanless(X)     pallas_scanner_Y = pallas_scanner(X)     assert jnp.allclose(pallas_scanner_Y, pallas_scanless_Y, atol=.001)      choose hyperparameters     N = 16       number of times to repeat execution     B = 512      batch size     T = 8192    context size     D = 64       feature size     dtype = jnp.float16     print(f'{N=} {B=} {T=} {D=}')      jit functions     heavy_scanless = heavyweight(pallas_scanless, N, B, T, D, dtype)     heavy_scanner = heavyweight(pallas_scanner, N, B, T, D, dtype)      compile     _t = time.time()     jax.block_until_ready(heavy_scanless())     scanless_time_to_compile_and_execute = time.time()  _t     _t = time.time()     jax.block_until_ready(heavy_scanner())     scanner_time_to_compile_and_execute = time.time()  _t      time the main execution     _t = time.time()     jax.block_until_ready(heavy_scanless())     scanless_time_to_execute = time.time()  _t     ms_per_kernel_call = 1000 * scanless_time_to_execute / N     scanless_time_to_compile = (             scanless_time_to_compile_and_execute  scanless_time_to_execute)     print(f'{1000*scanless_time_to_compile = :.3f}ms, '           f'execution {ms_per_kernel_call = :.3f}ms')     _t = time.time()     jax.block_until_ready(heavy_scanner())     scanner_time_to_execute = time.time()  _t     ms_per_kernel_call = 1000 * scanner_time_to_execute / N     scanner_time_to_compile = (             scanner_time_to_compile_and_execute  scanner_time_to_execute)     print(f'{1000*scanner_time_to_compile = :.3f}ms, '           f'execution {ms_per_kernel_call = :.3f}ms') if __name__ == '__main__':     main() ```  System info (python version, jaxlib version, accelerator, etc.) ``` >>> import jax; jax.print_environment_info() jax:    0.4.25 jaxlib: 0.4.25 numpy:  1.26.4 python: 3.11.4 (main, Dec  7 2023, 15:43:41) [GCC 12.3.0] jax.devices (2 total, 2 local): [cuda(id=0) cuda(id=1)] process_count: 1 platform: uname_result(system='Linux', node='jacobmanifestai', release='6.2.039generic', version=' CC(Invalid proto descriptor for file ""tensorflow/compiler/xla/xla_data.proto"")Ubuntu SMP PREEMPT_DYNAMIC Tue Nov 14 14:18:00 UTC 2023', machine='x86_64') $ nvidiasmi Wed Apr 24 01:15:09 2024 ++  ++ ```",2024-04-24T05:16:28Z,bug pallas,open,0,1,https://github.com/jax-ml/jax/issues/20909,"I think the best explanation I found online is the following: > To elaborate on this, the reason GPU is so fast for vectorized operations is not that individual floating point operations are particularly fast (they're actually often slower than similar operations on a CPU!), but rather that it can very efficiently run many operations in parallel. For an operation like scan in which each step depends on the output of the previous, the sequence of operations as a whole cannot be parallelized. So you end up not taking advantage of any of the GPU's inherent parallelism, and the result is slow execution. > Contrast this to CPU, where individual floating point operations are relatively fast, but there is no so much inbuilt parallelism available. Because of this, scan does not incur as much of a performance penalty. I think this falls into the same bucket. Like my example, where the input of each layer depended on the output of the previous one (the carry), this is a pure sequential loop. There is probably a sweet spot with the unroll parameter where compilation times and loop times are optimal."
yi,Fix `jax.scipy.stats.poisson.logpmf` to emulate `scipy.stats.poisson.logpmf` for non-integer values of `k`,"Inconsistency exists between `jax.scipy.stats.poisson.logpmf` and `scipy.stats.poisson.logpmf` for noninteger `k`. This pull request rectifies this behavior, enabling both functions `jax.scipy.stats.poisson.logpmf` and `jax.scipy.stats.poisson.pmf`  to emulate their corresponding `scipy` counterparts for any `k` value. Partly addresses CC(jax.scipy.stats has different behavior than scipy.stats when arguments are out of the support and when parameters lead to undefined densities) Current Behavior: ```python >>> import jax.scipy.stats as lsp >>> import scipy.stats as osp >>> osp.poisson.logpmf(k=1.2, mu=1) inf >>> lsp.poisson.logpmf(k=1.2, mu=1) Array(1.0969486, dtype=float32, weak_type=True) >>> osp.poisson.pmf(k=1.2, mu=1) 0.0 >>> lsp.poisson.pmf(k=1.2, mu=1) Array(0.33388835, dtype=float32, weak_type=True) ``` With this change: ```python >>> import jax.scipy.stats as lsp >>> import scipy.stats as osp >>> osp.poisson.logpmf(k=1.2, mu=1) inf >>> lsp.poisson.logpmf(k=1.2, mu=1) Array(inf, dtype=float32, weak_type=True) >>> osp.poisson.pmf(k=1.2, mu=1) 0.0 >>> lsp.poisson.pmf(k=1.2, mu=1) Array(0., dtype=float32, weak_type=True) ```",2024-04-23T16:56:57Z,pull ready,closed,0,3,https://github.com/jax-ml/jax/issues/20891,Please fix the trailing whitespace (see the CI failures),Please also squash your changes into a single commit; see https://jax.readthedocs.io/en/latest/contributing.htmlsinglechangecommitsandpullrequests. Thanks!,Squashed all commits into single commit
yi,"In QDWH, eliminate one of the two triangular solves in the steps using Cholesky.","In QDWH, eliminate one of the two triangular solves in the steps using Cholesky.",2024-04-23T01:06:33Z,,closed,0,1,https://github.com/jax-ml/jax/issues/20878,Closing Copybara created PR due to inactivity
yi,Tensorflow Building from Source Code," Description I am trying to build Tensorflow from source because i want to run tensorflow using c++ and i am trying to download the GPU version So I ran ./Configure command, and choose the option to enable GPU Support I ran the bazel command given on the tensorflow Build From source on windows  https://www.tensorflow.org/install/source_windows and ran into the following error, What does it imply and how can i Solve it ERROR: An error occurred during the fetch of repository 'local_config_cuda':    Traceback (most recent call last):         File ""D:/tensorflow/tensorflow2.9.0/third_party/gpus/cuda_configure.bzl"", line 1401, column 38, in _cuda_autoconf_impl                 _create_local_cuda_repository(repository_ctx)         File ""D:/tensorflow/tensorflow2.9.0/third_party/gpus/cuda_configure.bzl"", line 1179, column 17, in _create_local_cuda_repository                 cc = find_cc(repository_ctx)         File ""D:/tensorflow/tensorflow2.9.0/third_party/gpus/cuda_configure.bzl"", line 216, column 34, in find_cc                 return _get_msvc_compiler(repository_ctx)         File ""D:/tensorflow/tensorflow2.9.0/third_party/gpus/cuda_configure.bzl"", line 133, column 26, in _get_msvc_compiler                 return find_msvc_tool(repository_ctx, vc_path, ""cl.exe"").replace(""\\"", ""/"")         File ""C:/users/paart/_bazel_paart/3pmtytha/external/bazel_tools/tools/cpp/windows_cc_configure.bzl"", line 447, column 27, in find_msvc_tool                 if _is_vs_2017_or_2019(repository_ctx, vc_path) or _is_msbuildtools(vc_path):         File ""C:/users/paart/_bazel_paart/3pmtytha/external/bazel_tools/tools/cpp/windows_cc_configure.bzl"", line 265, column 72, in _is_vs_2017_or_2019                 vc_path_contents = [d.basename.lower() for d in repository_ctx.path(vc_path).readdir()] Error in path: in call to path(), parameter 'path' got value of type 'NoneType', want 'string, Label, or path' ERROR: D:/tensorflow/tensorflow2.9.0/WORKSPACE:15:14: fetching cuda_configure rule //external:local_config_cuda: Traceback (most recent call last):         File ""D:/tensorflow/tensorflow2.9.0/third_party/gpus/cuda_configure.bzl"", line 1401, column 38, in _cuda_autoconf_impl                 _create_local_cuda_repository(repository_ctx)         File ""D:/tensorflow/tensorflow2.9.0/third_party/gpus/cuda_configure.bzl"", line 1179, column 17, in _create_local_cuda_repository                 cc = find_cc(repository_ctx)         File ""D:/tensorflow/tensorflow2.9.0/third_party/gpus/cuda_configure.bzl"", line 216, column 34, in find_cc                 return _get_msvc_compiler(repository_ctx)         File ""D:/tensorflow/tensorflow2.9.0/third_party/gpus/cuda_configure.bzl"", line 133, column 26, in _get_msvc_compiler                 return find_msvc_tool(repository_ctx, vc_path, ""cl.exe"").replace(""\\"", ""/"")         File ""C:/users/paart/_bazel_paart/3pmtytha/external/bazel_tools/tools/cpp/windows_cc_configure.bzl"", line 447, column 27, in find_msvc_tool                 if _is_vs_2017_or_2019(repository_ctx, vc_path) or _is_msbuildtools(vc_path):         File ""C:/users/paart/_bazel_paart/3pmtytha/external/bazel_tools/tools/cpp/windows_cc_configure.bzl"", line 265, column 72, in _is_vs_2017_or_2019                 vc_path_contents = [d.basename.lower() for d in repository_ctx.path(vc_path).readdir()] Error in path: in call to path(), parameter 'path' got value of type 'NoneType', want 'string, Label, or path' INFO: Found applicable config definition build:cuda in file d:\tensorflow\tensorflow2.9.0\.bazelrc: repo_env TF_NEED_CUDA=1 crosstool_top=//crosstool:toolchain //:enable_cuda ERROR: //:enable_cuda :: Error loading option //:enable_cuda: in call to path(), parameter 'path' got value of type 'NoneType', want 'string, Label, or path'  System info (python version, jaxlib version, accelerator, etc.) I am using python version 3.8 CUDA version 11.2 CuDNN version 8.1 Bazel Version 5.0.0 MSVC 2019 This was the specifications that was given on the tensorflow website whose link I have Shared Above",2024-04-22T21:54:38Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/20871,"This repository is for JAX, not TensorFlow. You'd need to file this issue on the TensorFlow github project."
yi,Unbound axis names within scan within custom partitioning," Description I encountered this problem while implementing a ring attention algorithm using custom_partitioning. A custom partitioning of jnp.sum using the following contrived partitioning implementation (see full self contained code at the end of this report) throws an error, saying the axis name cannot be found: ```py     def part_func(x, axis_name):         def f(carry, part):             carry += jax.lax.psum(jnp.sum(part), axis_name=axis_name)             return carry, None         return jax.lax.scan(f, 0, x)[0]  NameError: unbound axis name: x. The following axis names (e.g. defined by pmap) are available to collective operations: [] ``` However, it works fine if the part_func does not contain a scan, such as: ```py     def part_func_without_scan(x, axis_name):         return jax.lax.psum(jnp.sum(x), axis_name=axis_name) ``` This also works if shard_map is used to directly call part_func instead of via custom partitioning, but only if check_rep=False (check_rep=True throws this error, which seems like a different issue but I am unsure: `Scan carry input and output got mismatched replication types [None] and [{'x'}]`). Full code: ```py import os from functools import partial, reduce os.environ[""XLA_FLAGS""] = 'xla_force_host_platform_device_count=2' import jax import jax.numpy as jnp import numpy as np from jax.experimental.custom_partitioning import custom_partitioning from jax.sharding import PartitionSpec as P from jax.sharding import Mesh from jax.sharding import NamedSharding from jax.sharding import PositionalSharding from jax.tree_util import tree_map import pytest def make_custom_partitioning(part_func):          def my_func(x):         return jnp.sum(x)     def partition(mesh, arg_shapes, result_shape):         result_shardings = tree_map(lambda x: x.sharding, result_shape)         arg_shardings = tree_map(lambda x: x.sharding, arg_shapes)         assert isinstance(arg_shardings[0], NamedSharding)         assert (None, 'x') == arg_shardings[0].spec         return mesh, partial(part_func, axis_name='x'), result_shardings, arg_shardings     def infer_sharding(mesh, arg_shapes, result_shape):         return NamedSharding(mesh, P())     def propagate_user_sharding(mesh, user_shape):         return user_shape.sharding     my_func.def_partition(partition, infer_sharding, propagate_user_sharding=propagate_user_sharding)     return my_func  This works fine: def test_simple():     def part_func(x, axis_name):         return jax.lax.psum(jnp.sum(x), axis_name=axis_name)     my_func = jax.jit(make_custom_partitioning(part_func))     with Mesh(jax.devices(backend='cpu'), axis_names=('x',)) as mesh:         array = jnp.ones([4,4])         assert int(my_func(array)) == 16         array = jax.device_put(array, NamedSharding(mesh, P(None,'x')))         assert int(my_func(array)) == 16  This doesn't: def test_scan():     def part_func(x, axis_name):         def f(carry, part):             carry += jax.lax.psum(jnp.sum(part), axis_name=axis_name)             return carry, None         return jax.lax.scan(f, 0, x)[0]     my_func = jax.jit(make_custom_partitioning(part_func))     with Mesh(jax.devices(backend='cpu'), axis_names=('x',)) as mesh:         array = jnp.ones([4,4])         assert int(my_func(array)) == 16         array = jax.device_put(array, NamedSharding(mesh, P(None,'x')))          Crashes here with NameError: unbound axis name: x. The following axis names (e.g. defined by pmap) are available to collective operations: []         assert int(my_func(array)) == 16  It works under shard_map, as long as check_rep is False. def test_shard_map():     def part_func(x, axis_name):         def f(carry, part):             carry += jax.lax.psum(jnp.sum(part), axis_name=axis_name)             return carry, None         return jax.lax.scan(f, 0, x)[0]     with Mesh(jax.devices(backend='cpu'), axis_names=('x',)) as mesh:         array = jnp.ones([4,4])         array = jax.device_put(array, NamedSharding(mesh, P(None,'x')))         sharded = jax.jit(jax.experimental.shard_map.shard_map(partial(part_func, axis_name='x'), mesh, in_specs=(P(None,'x'),), out_specs=P(), check_rep=True))         assert int(sharded(array)) == 16 ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.26 jaxlib: 0.4.26 numpy:  1.26.2 python: 3.11.8 (main, Feb 12 2024, 14:50:05) [GCC 13.2.1 20230801] jax.devices (1 total, 1 local): [cuda(id=0)] process_count: 1 platform: uname_result(system='Linux', node='phenex', release='6.8.2arch21', version=' CC(Python 3 compatibility issues) SMP PREEMPT_DYNAMIC Thu, 28 Mar 2024 17:06:35 +0000', machine='x86_64') $ nvidiasmi Tue Apr 23 00:02:35 2024 ++  ++++ ```",2024-04-22T14:05:20Z,bug,closed,0,0,https://github.com/jax-ml/jax/issues/20864
rag,[jax:mosaic-gpu] FragmentedArray can do tiled load.,[jax:mosaicgpu] FragmentedArray can do tiled load.,2024-04-18T19:01:52Z,,closed,0,1,https://github.com/jax-ml/jax/issues/20820,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
rag,Begin deprecation of implicit input conversion in FFT module,"Towards CC(Update `jax.experimental.array_api` to v2023.12 API) Array API 2023 changelog.  Updates the `jax.numpy.fft` namespace to begin a deprecation of implicitly converted inputs  explicitly sets `fft` function domains to either `real floating` or `complex`, warning on implicit conversion (i.e. `fft(x: float32)`, `rfft(x: int32)`, etc.). Updates the `jax.experimental.array_api.fft` namespace with corresponding changes which directly raise `ValueError`, marked with deprecation once `jax.numpy.fft` is array API compliant. Adds corresponding tests for both modules. Updates dtype coverage in `tests/fft_test.py` to reflect new restricted domains.",2024-04-18T17:29:44Z,,closed,0,4,https://github.com/jax-ml/jax/issues/20818,"I think this is a case where we should follow numpy rather than following the Array API to the letter. There are many places where the array API is more restrictive than NumPy, which we've been taking our cue from up until this point. Deprecating previously supported behavior when NumPy still supports it will lead to undue downstream churn, so I think we should avoid it. What do you think?","> I think this is a case where we should follow numpy rather than following the Array API to the letter. There are many places where the array API is more restrictive than NumPy, which we've been taking our cue from up until this point. Deprecating previously supported behavior when NumPy still supports it will lead to undue downstream churn, so I think we should avoid it. What do you think? I would generally agree, however there are a few benefits for this that make me prefer the domain restrictions. The domain restriction itself forces users to make an explicit decision regarding the transform they choose to use, pushing for using the most specific transform possible as opposed e.g. `fft` everything. Looking towards the scientific computing community, I think this by itself is a valuable aid, especially for folks that aren't familiar with the computational implications of `fft` vs say `rfft` or `hfft`. Also, pulling from  who provided a helpful summary of some of the discussion that went into the decision from the consortium, both in meetings and in PRs (e.g. this comment thread): > in the spec, we've sided against implicit upcasting from realtocomplex, as users should be clear about what/how they want to cast. While a realvalued array can be interpreted as a complex array having omitted imaginary components which are all zero, this doesn't have to be the case. The array could be imaginary components or interleaved real and imaginary, etc. Particularly for the FFT APIs, certain transforms are very explicitly C2C or C2R transforms. In which case, providing a realvalued array is not the intent. And ultimately afaik NumPy 2.0 _will_ be implementing the domain restrictions, as `numpy.array_api.fft` already does and they intend for the general `numpy` namespace to be properly array API compatible, so we're just getting ahead of them on this one. We don't need to complete the deprecation until they've fully adopted it anyways, so we won't be fully diverging. cc:  in case you have some opinions on this too","> And ultimately afaik NumPy 2.0 will be implementing the domain restrictions, Do you have a link to a NEP or other forum that discusses this?","Okay it looks like I had misinterpreted the word ""**should**"" in the array API spec to imply room for backwards compatibility _with the intent of deprecation_, given the changelog wording > `fft.fft`: **require** the input array to have a complexvalued floatingpoint data type however, that is not the case. As such, we are already compliant and this PR is not needed. Thanks for helping me catch that "
rag,Added int4 and uint4 to dtype-specific tests,"I probably missed some cases, so this PR is really just the first step in making sure we have good *int4 coverage.",2024-04-18T14:22:43Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/20816
yi,Import etils.epath lazily.,Import etils.epath lazily. Reduces jax import time.,2024-04-16T19:55:29Z,,closed,0,0,https://github.com/jax-ml/jax/issues/20786
yi,Update JAX official doc: point out that the device numbers are not in numerical order because of the underlying torus hardware topology.,Update JAX official doc: point out that the device numbers are not in numerical order because of the underlying torus hardware topology.,2024-04-16T18:08:28Z,,closed,0,0,https://github.com/jax-ml/jax/issues/20783
transformer,Latency Hiding Scheduler leads to x5 memory usage if used without jax.lax.scan," Description Hi, we're training large (300B, 60 layers) mixture of experts transformer on a 1000+ GPU. We have some nonuniformity in layers so we can't use jax.lax.scan directly to stack layers together  instead, we just call each layer independently.  Model doesn't have completely random structure, it is like (3 layers with same structure, 1 with another) repeated 15 times (to achieve 60 layers in total) We would benefit a LOT from overlapping computations & communications but when we try to enable latency hiding scheduler `xla_gpu_enable_latency_hiding_scheduler`, this leads to increased in memory usage by a factor of 45 (from 50Gb per GPU to 200250Gb per GPU, which is completely unusable).  My guess is that compiler doesn't reuse buffers for async comms in this case for different layers. We've tested also variant with jax.lax.scan and uniform layers, it seemed to work okay from memory usage point of view  only 2025% overhead from latency hiding scheduler. Is this a known problem? Is these any workaround?  System info (python version, jaxlib version, accelerator, etc.) tested on 0.4.25/0.4.26, 1000+ H100 GPU",2024-04-15T13:26:26Z,bug NVIDIA GPU,open,0,4,https://github.com/jax-ml/jax/issues/20763,"Here is some toy repro tested on JAX 0.4.34   ``` import flax.linen as nn import jax import jax.ad_checkpoint import jax.numpy as jnp import numpy as np from flax.linen.linear import default_kernel_init EMB_DIM = 8192 HID_DIM = 8192 BS = 32 SEQ_LEN = 4096 N_LAYERS = 32 SCAN = False CHECKPOINT_POLICY = jax.checkpoint_policies.save_and_offload_only_these_names(     names_which_can_be_saved=[],     names_which_can_be_offloaded=[],     offload_src=""device"",     offload_dst=""pinned_host"", ) mesh = jax.sharding.Mesh(np.array(jax.devices()).reshape(8, 1), (""data"", ""model"")) input_sharding = jax.sharding.NamedSharding(     mesh, jax.sharding.PartitionSpec(""data"", None) ) target_sharding = jax.sharding.NamedSharding(     mesh,     jax.sharding.PartitionSpec(         ""data"",     ), ) rules = (     (""batch"", ""data""),     (""embedding"", None),     (""hidden"", ""model""),     (""q_sequence"", ""model""), ) class MLP(nn.Module):     .compact     def __call__(self, x):         x_residual = x         h = nn.Dense(             HID_DIM,             kernel_init=nn.with_logical_partitioning(                 default_kernel_init,                 (""embedding"", ""hidden""),             ),             use_bias=False,         )(x)         h = nn.relu(h)         x = nn.Dense(             EMB_DIM,             kernel_init=nn.with_logical_partitioning(                 default_kernel_init,                 (""hidden"", ""embedding""),             ),             use_bias=False,         )(h)         x = x_residual + x          Sequence parallelism         x = nn.with_logical_constraint(x, (""batch"", ""q_sequence"", None))         return x class Output(nn.Module):     .compact     def __call__(self, x):         x = nn.Dense(             features=1,             kernel_init=nn.with_logical_partitioning(                 default_kernel_init,                 (""hidden"", None),             ),             use_bias=False,         )(x)[..., 0]         x = jnp.mean(x, axis=1)         return x class Model(nn.Module):     .compact     def __call__(self, x):         def apply_module(block, block_input, _):             block_output = block(block_input)             return block_output, None         apply_module = nn.remat(             apply_module,             policy=CHECKPOINT_POLICY,             prevent_cse=False,         )         if SCAN:             x, _ = nn.scan(                 apply_module,                 variable_axes={""params"": 0},                 split_rngs={""params"": True},                 length=N_LAYERS,                 metadata_params={nn.PARTITION_NAME: ""layers""},             )(MLP(), x, None)         else:             for i in range(N_LAYERS):                 x = MLP(name=f""block_{i}"")(x)         preds = Output()(x)         return preds def loss_fn(preds, target):     return jnp.mean((preds  target) ** 2) def calc_loss(params, inputs, target):     preds = Model().apply(params, inputs)     loss = loss_fn(preds, target)     return loss def train_step(params, inputs, target):     loss, grads = jax.value_and_grad(calc_loss)(params, inputs, target)     params = jax.tree_util.tree_map(lambda p, g: p  1e8 * g, params, grads)     return params, loss def unbox_logically_partioned(tree, apply_constraint: bool = True):     return jax.tree_util.tree_map(         lambda leaf: (             leaf.unbox(apply_constraint=apply_constraint)             if isinstance(leaf, nn.LogicallyPartitioned)             else leaf         ),         tree,         is_leaf=lambda node: isinstance(node, nn.LogicallyPartitioned),     ) def get_gpu_memory_usage() > dict[str, float]:     if jax.default_backend() != ""gpu"":         return {}     num_devices = jax.local_device_count(""gpu"")     gpu_memory_usage = []     for i in range(num_devices):         memory_stats = jax.local_devices()[i].memory_stats()         gpu_memory_usage.append(             memory_stats[""peak_bytes_in_use""] / memory_stats[""bytes_limit""] * 100         )     return {f""GPU{i}"": val for i, val in enumerate(gpu_memory_usage)} with mesh, nn.logical_axis_rules(rules):     fake_inputs = jnp.empty((BS, SEQ_LEN, EMB_DIM))     fake_inputs = jax.device_put(fake_inputs, input_sharding)     fake_target = jnp.empty((BS,))     fake_target = jax.device_put(fake_target, target_sharding)     params = Model().init(jax.random.PRNGKey(0), fake_inputs)     params = unbox_logically_partioned(params)     train_step_fn = (         jax.jit(             train_step,             in_shardings=(                 jax.tree_util.tree_map(lambda x: x.sharding, params),                 input_sharding,                 target_sharding,             ),             out_shardings=(                 jax.tree_util.tree_map(lambda x: x.sharding, params),                 jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec()),             ),             donate_argnums=(0,),         )         .lower(params, fake_inputs, fake_target)         .compile()     )     with open(""compiled.txt"", ""w"") as f:         f.write(train_step_fn.as_text())     memory_analysis = train_step_fn.memory_analysis()     print(         f""Total size device = {memory_analysis.temp_size_in_bytes / 1024 / 1024 / 1024} GB, ""   noqa E501         f""weights = {memory_analysis.argument_size_in_bytes / 1024 / 1024 / 1024} GB, ""         f""total: {(memory_analysis.argument_size_in_bytes + memory_analysis.temp_size_in_bytes) / 1024 / 1024 / 1024} GB""     )     for i in range(10):         inputs = jax.random.normal(jax.random.PRNGKey(i), (BS, SEQ_LEN, EMB_DIM))         inputs = jax.device_put(inputs, input_sharding)         target = jax.random.normal(jax.random.PRNGKey(0), (BS,))         target = jax.device_put(target, target_sharding)         if i == 3:             jax.tree_map(lambda x: x.block_until_ready(), params)             jax.profiler.start_trace(""./profile"", create_perfetto_trace=True)         params, loss = train_step_fn(params, inputs, target)         if i == 3:             jax.tree_map(lambda x: x.block_until_ready(), params)             jax.profiler.stop_trace()         print(loss)         print(get_gpu_memory_usage()) ```  Latency hiding scheduler enabled, XLA_FLAGS: `xla_gpu_graph_level=0 xla_gpu_enable_triton_gemm=false xla_gpu_enable_command_buffer= xla_gpu_enable_latency_hiding_scheduler=true` SCAN=False (hits OOM): ``` 20241017 10:47:03.275923: W external/xla/xla/service/hlo_rematerialization.cc:3005] Can't reduce memory use below 53.46GiB (57397316392 bytes) by rematerialization; only reduced to 70.25GiB (75430461976 bytes), down from 75.50GiB (81067606600 bytes) originally Total size device = 54.500054121017456 GB, weights = 16.500030532479286 GB, total: 71.00008465349674 GB ``` SCAN=True: ``` Total size device = 34.5000324845314 GB, weights = 16.500030532479286 GB, total: 51.00006301701069 GB ``` Latency hiding scheduler disabled, XLA_FLAGS=`xla_gpu_graph_level=0 xla_gpu_enable_triton_gemm=false xla_gpu_enable_command_buffer=` SCAN=False: ``` Total size device = 37.50002360343933 GB, weights = 16.500030532479286 GB, total: 54.00005413591862 GB ``` SCAN=True ``` Total size device = 35.00000220537186 GB, weights = 16.500030532479286 GB, total: 51.50003273785114 GB ```"," Using JAX 0.4.35 `XLA_FLAGS=""xla_gpu_graph_level=0 xla_gpu_enable_triton_gemm=false xla_gpu_enable_command_buffer= ""` and `SCAN=False`, I'm seeing a failure.  ```Out of memory while trying to allocate 35701941112 bytes. *** Check failure stack trace: ***     @     0x7f26b3b96dc4  absl::lts_20230802::log_internal::LogMessage::SendToLog()     @     0x7f26b3b96c34  absl::lts_20230802::log_internal::LogMessage::Flush()     @     0x7f26b3b971e9  absl::lts_20230802::log_internal::LogMessageFatal::~LogMessageFatal()     @     0x7f26ac056141  xla::PjRtStreamExecutorLoadedExecutable::Execute()     @     0x7f26abfa5d71  pjrt::PJRT_LoadedExecutable_Execute()     @     0x7f26bbd699fc  xla::PjRtCApiLoadedExecutable::Execute()     @     0x7f26c1ab0a25  xla::ifrt::PjRtLoadedExecutable::Execute()     @     0x7f26c1250c49  xla::(anonymous namespace)::ExecuteShardedOnLocalDevicesInternal()     @     0x7f26c12528ee  xla::PyLoadedExecutable::ExecuteSharded()     @     0x7f26bbc2fc55  xla::ValueOrThrowWrapper::operator()()     @     0x7f26bbc2fabd  nanobind::detail::func_create()::{lambda() CC(Python 3 compatibility issues)}::__invoke()     @     0x7f26c1a86eb8  nanobind::detail::nb_func_vectorcall_complex()     @     0x56227ff6aabb  _PyEval_EvalFrameDefault Aborted (core dumped) ``` Any chance you have other flags or env variables set? ", Can you please set `XLA_CLIENT_MEM_FRACTION=0.95` and use `xla_gpu_copy_insertion_use_region_analysis` in addition to your existing flags and report back if it resolves the issue?," `xla_gpu_memory_limit_slop_factor` flag could also help in this case. The default value is 95, so you can experiment with lower values (90, 80, 70, etc.). You can find more info about this flag at https://github.com/jaxml/jax/blob/main/docs/gpu_performance_tips.md. Let me know if you see any issues."
yi,Optimize `_create_copy_plan` in array.py,"Optimize `_create_copy_plan` in array.py * `_get_device` is called from many tight loops, so it's worth avoiding unnecessary work as much as possible. * `_create_copy_plan` now uses sharding's `_internal_device_list` instead of querying the device of every shard in a loop.",2024-04-12T19:36:48Z,,closed,0,0,https://github.com/jax-ml/jax/issues/20735
yi,[callbacks] io_callback while_loop batching rule fix   ~(^.~!)z,"~**Note to reviewer**: this PR currently contains the commit from CC([callbacks] io_callback batching rule accidentally called pure_callback), but once that goes in I'll rebase it away.~ When we batch a `while_loop(cond_fun, body_fun, init_val)`, two things can happen: If the predicate **is not** batched (because only some elements of `batched_init_val` are batched, and the predicate value doesn't depend on any of them), then we do something like this: ```python def batched_loop(batched_init_val):   return while_loop(cond_fun, vmap(body_fun), batched_init_val) ``` If the predicate **is** batched, we do something like this: ```python def batched_loop(batched_init_val):   def batched_cond(carry):     return vmap(cond_fun)(carry).any()   if any batch elts need more iters, we keep going   def batched_body(carry):     run_step = vmap(cond_fun)(carry)     new_carry = vmap(body_fun)(carry)     return jnp.where(run_step, new_carry, carry)   return while_loop(batched_cond, batched_body, batched_init_val) ``` The latter poses a problem when side effects are present in `body_fun`, even if they are unordered: we're relying on our ability to run extra iterations of the loop body for elements of the batch that have already finished, and then just mask those updates off. That works great when body_fun is functionally pure, but not so great when someone might notice our side effects! The former actually has no issue for unordered effects. Before this PR, while_loop's batching rule would blow up whenever there were ordered or unordered IO effects in cond_fun or body_fun. This PR relaxes the check and allows the case where the body contains unordered effects and the predicate is unbatched. This came up in CC(Support IO effect in vmapofwhile.).",2024-04-12T04:15:30Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/20726
yi,Let caller switch implementation of reduction after import,"Let caller switch implementation of reduction after import Thank you to gnecula@ for adding the jax2tf_associative_scan_reductions flag and context: https://github.com/google/jax/pull/9189/commits/5bfe1852a4626680f3612beee7d1a8b7a66fc79c   For GPU, the specific implementation of `cumsum` can make the whopping difference between a latency in microseconds versus milliseconds! Before this change, adjusting the method of lowering `cumsum` via this scope has no effect: ```py with jax.jax2tf_associative_scan_reductions(True):   ... ``` ... because the cumsum method (and other reduce methods) have their implementations set when the `jax2tf` library is imported, ie when this line is called: ```py from jax.experimental import jax2tf ``` Thus, any future switches of the implementation (to, say associative scanning), even if they happen before the `jax2tf.convert` method executes, had no effect because methods such as `cumsum` had already been curried at import time. This change fixes that by varying the implementation based on the current value of `config.jax2tf_associative_scan_reductions`. We use existing tests to verify the continued correctness of this CL that affects latency. We add TPU to the list of devices to apply some limitations  One TPU unit test had suddenly failed because the scope now works: Even though TPUs use a different path to lower by default, the context above explicitly sets to associative scanning.",2024-04-12T02:08:26Z,,closed,0,0,https://github.com/jax-ml/jax/issues/20724
yi,jax.distributed.initialize() fails on jax==0.4.26," Description The following example, works on jax==0.4.25, but yields a CUDA error on jax==0.4.26. It is being run on 4 processes with 1 GPU each (2 nodes x 2 GPUs), on a SLURM cluster. ```python import jax import jax.distributed if __name__ == ""__main__"":     jax.distributed.initialize()     print(f""Process {jax.process_index()}: Global devices: {jax.devices()}, Local devices: {jax.local_devices()}"") ``` **Failure with jax.distributed.initialize() for jax==0.4.26** (output appears 4 times because all process write to the same stdout file): ``` CUDA backend failed to initialize: jaxlib/cuda/versions_helpers.cc:90: operation gpuDeviceGetAttribute( &major, GPU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR, device) failed: CUDA_ERROR_NOT_INITIALIZED (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) CUDA backend failed to initialize: jaxlib/cuda/versions_helpers.cc:90: operation gpuDeviceGetAttribute( &major, GPU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR, device) failed: CUDA_ERROR_NOT_INITIALIZED (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) Process 2: Global devices: [CpuDevice(id=0), CpuDevice(id=1), CpuDevice(id=2), CpuDevice(id=3)], Local devices: [CpuDevice(id=2)] Process 3: Global devices: [CpuDevice(id=0), CpuDevice(id=1), CpuDevice(id=2), CpuDevice(id=3)], Local devices: [CpuDevice(id=3)] CUDA backend failed to initialize: jaxlib/cuda/versions_helpers.cc:90: operation gpuDeviceGetAttribute( &major, GPU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR, device) failed: CUDA_ERROR_NOT_INITIALIZED (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) Process 0: Global devices: [CpuDevice(id=0), CpuDevice(id=1), CpuDevice(id=2), CpuDevice(id=3)], Local devices: [CpuDevice(id=0)] CUDA backend failed to initialize: jaxlib/cuda/versions_helpers.cc:90: operation gpuDeviceGetAttribute( &major, GPU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR, device) failed: CUDA_ERROR_NOT_INITIALIZED (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) Process 1: Global devices: [CpuDevice(id=0), CpuDevice(id=1), CpuDevice(id=2), CpuDevice(id=3)], Local devices: [CpuDevice(id=1)] ``` **Working/expected output on jax==0.4.24**: ``` Process 2: Global devices: [cuda(id=0), cuda(id=1), cuda(id=2), cuda(id=3)], Local devices: [cuda(id=2)] Process 0: Global devices: [cuda(id=0), cuda(id=1), cuda(id=2), cuda(id=3)], Local devices: [cuda(id=0)] Process 3: Global devices: [cuda(id=0), cuda(id=1), cuda(id=2), cuda(id=3)], Local devices: [cuda(id=3)] Process 1: Global devices: [cuda(id=0), cuda(id=1), cuda(id=2), cuda(id=3)], Local devices: [cuda(id=1)] ``` **The SLURM jobfile:** ```bash !/bin/bash SBATCH J test_distributed SBATCH N 2 SBATCH partition zen3_0512_a100x2 SBATCH qos zen3_0512_a100x2 SBATCH output stdout_test_distributed.txt SBATCH time 5 SBATCH taskspernode=2 SBATCH gres=gpu:2 module purge source /gpfs/opt/sw/spack0.17.1/opt/spack/linuxalmalinux8zen3/gcc11.2.0/miniconda34.12.0ap65vga66z2rvfcfmbqopba6y543nnws/etc/profile.d/conda.sh conda activate sparse_wf srun python test_distributed.py ```  System info (python version, jaxlib version, accelerator, etc.) Output of jax.print_environment_info() **without calling jax.distributed.initialize()** ``` jax:    0.4.26 jaxlib: 0.4.26 numpy:  1.26.4 python: 3.12.2  ++ ```",2024-04-11T12:23:46Z,bug,closed,1,3,https://github.com/jax-ml/jax/issues/20711,"I think this bug has already been fixed at head, but it made it into the last release because we don't have any way to test SLURM clusters in CI. Can you try with a nightly jaxlib and jax?","You are right, it is indeed already fixed on the nightly build. It works as expected using ``` jax==0.4.27.dev20240410 jaxlib==0.4.27.dev20240410+cuda12.cudnn89 ``` **Thanks!**","Ran into the same issue without using SLURM, but multinode GPUs. Same fix worked with these too: > jax==0.4.27.dev20240415 > jaxlib==0.4.27.dev20240415+cuda12.cudnn89 Just dropping the install lines here in case anyone needed it like I did (got it from MaxText) ``` pip install pre U jax f https://storage.googleapis.com/jaxreleases/jax_nightly_releases.html pip install U pre jaxlib f https://storage.googleapis.com/jaxreleases/jaxlib_nightly_cuda12_releases.html  Maybe Transformer engine for H100+? pip install ""transformerengine==1.5.0+297459b"" \           extraindexurl https://uspython.pkg.dev/gceaiinfra/maxtextbuildsupportpackages/simple/ ```"
dspy,Automatic sharded parallelization of per-sample gradients fails on GPUs," Description The following minimal example works when run on 2 CPUs (CUDA_VISBLE_DEVICES=""""), but fails when run on GPUs due to **Unexpected XLA sharding override**. ```python import os  os.environ[""CUDA_VISIBLE_DEVICES""] = """"  Uncomment to run on CPU, in which case the code works os.environ[""XLA_FLAGS""] = ""xla_force_host_platform_device_count=2"" import jax from jax.experimental import mesh_utils from jax.sharding import PositionalSharding import jax.numpy as jnp import jax.distributed def model(params, x):     return x @ params if __name__ == ""__main__"":     jax.print_environment_info()     print(""Devices: "", jax.devices())     n_devices = jax.device_count()   2     devices = mesh_utils.create_device_mesh((n_devices,))     sharding = PositionalSharding(devices)     feature_dim = 3     batch_size_total = 8      Get example data     x = jnp.ones((batch_size_total,  feature_dim))     params = jnp.ones(feature_dim)      Shard data, replicate params     x = jax.device_put(x, sharding.reshape(n_devices, 1))     params = jax.device_put(params, sharding.replicate(axis=0))     y = model(params, x)     print(""Forward pass (with vectorizable function) works"")     y = jax.vmap(model, in_axes=(None, 0))(params, x)     print(""Forward pass with explicit vmap also works"")     grad = jax.grad(lambda p: model(p, x).sum())(params)     print(""Gradient of global function works"")     per_sample_grads = jax.vmap(jax.grad(model), in_axes=(None, 0))(params, x)     print(""This is never reached: persample gradients combining vmap and grad fails when run on GPUs"") ``` It appears to be specific to the combination of vmap and grad, and does not appear when using only vmap or only grad. In case of failure the output is: ``` Devices:  [cuda(id=0), cuda(id=1)] Forward pass (with vectorizable function) works Forward pass with explicity vmap also works Gradient of global function works Traceback (most recent call last):   File ""/home/scherbelam20/develop/sparse_wf/sparse_wf/preliminary_experiments/parallelization/per_sample_gradients/per_sample_gradients.py"", line 41, in      per_sample_grads = jax.vmap(jax.grad(model), in_axes=(None, 0))(params, x)   File ""/home/scherbelam20/develop/sparse_wf/sparse_wf/preliminary_experiments/parallelization/per_sample_gradients/per_sample_gradients.py"", line 12, in model     return x @ params   File ""/home/scherbelam20/.conda/envs/sparse_wf/lib/python3.12/sitepackages/jax/_src/numpy/array_methods.py"", line 736, in op     return getattr(self.aval, f""_{name}"")(self, *args)   File ""/home/scherbelam20/.conda/envs/sparse_wf/lib/python3.12/sitepackages/jax/_src/numpy/array_methods.py"", line 264, in deferring_binary_op     return binary_op(*args) jax._src.source_info_util.JaxStackTraceBeforeTransformation: AssertionError: Unexpected XLA sharding override: (XLA) GSPMDSharding({replicated}) != PositionalSharding([[{GPU 0}]                     [{GPU 1}]], shape=(2, 1)) (User sharding) ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.26 jaxlib: 0.4.26 numpy:  1.26.4 python: 3.12.2  ++ ```",2024-04-11T11:35:48Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/20710,I was able to repro this and have filed an internal bug. It should be fixed soon :),https://github.com/openxla/xla/pull/11483 should fix it!
yi,Provide output buffer for pure_callback result,"Is there any value in supplying a pure_callback (one with a return value), a buffer into which to place it's output? Something like the code below. I have some science applications where I need to use bindings to underlying C++ code that require taking an output buffer into which to place the results. I currently need to create the array inside the callback function, but I suspect that JAX might already have some preallocated buffer waiting for the result anyways, and could provide that to the function. ```python def _fn(x, _output_buffer):      Instead of _output_buffer=np.zeros_like(x)     np.square(x, out=_output_buffer)     return _output_buffer result_shape_dtype = jax.ShapeDtypeStruct(     shape=np.shape(x),     dtype=x.dtype ) return jax.pure_callback(_fn, result_shape_dtype, x, vectorized=True) ```",2024-04-10T20:50:08Z,enhancement,open,1,12,https://github.com/jax-ml/jax/issues/20701,"Also, is it safe to reuse the buffer of input values, e.g. like ```python def _fn(x):     np.square(x, out=x)     return x result_shape_dtype = jax.ShapeDtypeStruct(     shape=np.shape(x),     dtype=x.dtype ) return jax.pure_callback(_fn, result_shape_dtype, x, vectorized=True) ``` EDIT: That would be a nope as it gives `jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Generated function failed: CpuCallback error: ValueError: output array is readonly`","There is a preallocated buffer on the XLA side, but it is not currently passed to the `pure_callback`.   should we have an API for this, wdyt?","Also, the main reason this would be helpful is because the output buffers for the science applications I'm working on are really large so if there is already one allocated by XLA it would save lots of memory to use that one.",  any update available for this?,"Hey , sorry for the silence. Here is a quick update *  is working on a proposal to allow ""write once"" mutation for `jax.Array`s which are currently immutable (and thus cannot be used for output buffers); * I'm exploring a parallel idea  `dlpack.callback`, a new callback API using DLPack capsules for both inputs and outputs.","Another quick update: I prototyped `dlpack.callback`, but after discussing it with a few JAX team members, I decided not to move forward with it as JAX has too many callback APIs already. Instead, the plan is to change existing callback APIs to support `mutable_results=`. I am waiting on a few changes in XLA FFI, but once they land, it should be fairly straightforward to implement this.","Hi , any news?","No news yet in a sense that none of my attempts landed. Hopefully, in the coming weeks :)", can you check out JAXbind which offers a way to specify both JVP and VJP for external callbacks? I argue that it should be also possible within JAX to specify both.," — JAX doesn't currently offer a public API for this (customizing multiple transforms for a single callable), but it's on our radar. I'd say that this is off topic for this issue thread, but feel free to open another with more info about your use cases for using both JVP and VJP for one callback!","Re original topic:  suggested passing the output buffer as an argument and donating it to the callback, i.e. ``` jax.jit(partial(jax.pure_callback, ...), donate_argnums=...) ``` this will ensure that the runtime doesn't allocate a separate buffer for the output. Unfortunately, though, there isn't a great way for creating a mutable view of a `jax.Array`. I was hoping we can do `jax.Array` > DLPack > NumPy, but NumPy imports DLPack capsules as readonly.","Would donating the input avoid double allocating memory? I can imagine if it's something like: invoke pure callback > free input > allocate inside the pure callback > use that output array in JAX without reallocation. We're memory limited in this use case, `ducc.wgridder`."
yi,[key reuse] refactor & unify key reuse rule registration,This refactoring will pave the way for making key reuse checking part of the core dispatch path. There are two goals: 1. Move from having two tables of key reuse rules (static and dynamic) to having a single table: this will let us begin easily specifying key reuse signatures at the location where primitives are defined. 2. Move shared code into single utility functions that can be referenced from the main dispatch path: this will let us move away from the current approach which involves monkeypatching the impl rules.,2024-04-10T17:52:46Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/20692
yi,"On jax-metal, updating multidimensional boolean arrays sometimes fails"," Description I ran into a rather surprising case involving 3D boolean arrays, which only seems to fail on jaxmetal. Correct behavior (CPU): ```py >>> jnp.zeros((2,2,2), dtype=jnp.bool).at[:, :, 0].set(True)[:, :, 0] Array([[ True,  True],        [ True,  True]], dtype=bool) ``` But on jaxmetal, I get: ```py >>> jnp.zeros((2,2,2), dtype=jnp.bool).at[:, :, 0].set(True)[:, :, 0] Array([[False, False],        [False, False]], dtype=bool) ``` After playing around with some inputs, the problem seems to occur for `.at[:, i]` and `.at[:, :, i]`, but `.at[i]` works fine. So, any dimension higher than 0 seems to have a bug in the scatter update algorithm for booleans. Is there some way I can help debug this? Is the jaxmetal code open source? If it is, then if you point me to build instructions, I can try to track down the bug.  System info (python version, jaxlib version, accelerator, etc.) On jaxmetal: ``` >>> import jax; jax.print_environment_info() jax:    0.4.25 jaxlib: 0.4.23 numpy:  1.26.2 python: 3.10.13 (main, Aug 24 2023, 22:36:46) [Clang 14.0.3 (clang1403.0.22.14.1)] jax.devices (1 total, 1 local): [METAL(id=0)] process_count: 1 platform: uname_result(system='Darwin', node='shawn.local', release='23.4.0', version='Darwin Kernel Version 23.4.0: Fri Mar 15 00:12:49 PDT 2024; root:xnu10063.101.17~1/RELEASE_ARM64_T6020', machine='arm64') ``` On CPU (correct behavior): ``` >>> import jax; jax.print_environment_info() jax:    0.4.26 jaxlib: 0.4.26 numpy:  1.26.2 python: 3.10.13 (main, Aug 24 2023, 22:36:46) [Clang 14.0.3 (clang1403.0.22.14.1)] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Darwin', node='shawn.local', release='23.4.0', version='Darwin Kernel Version 23.4.0: Fri Mar 15 00:12:49 PDT 2024; root:xnu10063.101.17~1/RELEASE_ARM64_T6020', machine='arm64') ```",2024-04-10T01:04:00Z,bug Apple GPU (Metal) plugin,open,0,2,https://github.com/jax-ml/jax/issues/20675,(Note that integers and floats seem to work fine; only `dtype=jnp.bool_` seems affected.),jaxmetal is not open sourced as of the time.  We'll look into the issue and update any change here. 
yi,better unsupported indexing handling in lax_numpy.py,In replacement of CC(unsupported indexer reporting in lax_numpy)   ,2024-04-09T18:09:56Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/20666
yi,Bump actions/upload-artifact from 3 to 4,"Bumps actions/uploadartifact from 3 to 4.  Release notes Sourced from actions/uploadartifact's releases.  v4.0.0 What's Changed The release of uploadartifact and downloadartifact are major changes to the backend architecture of Artifacts. They have numerous performance and behavioral improvements. ℹ️ However, this is a major update that includes breaking changes. Artifacts created with versions v3 and below are not compatible with the v4 actions. Uploads and downloads must use the same major actions versions. There are also key differences from previous versions that may require updates to your workflows. For more information, please see:  The changelog post. The README. The migration documentation. As well as the underlying npm package, @​actions/artifact documentation.  New Contributors  @​vmjoseph made their first contribution in actions/uploadartifact CC(Implement a scatterupdate operator.)  Full Changelog: https://github.com/actions/uploadartifact/compare/v3...v4.0.0 v3.1.3 What's Changed  chore(github): remove trailing whitespaces by @​ljmf00 in actions/uploadartifact CC(skip some cases to satisfy internal tests) Bump @​actions/artifact version to v1.1.2 by @​bethanyj28 in actions/uploadartifact CC(Move masking of lower triangle of cholesky into Python code.)  Full Changelog: https://github.com/actions/uploadartifact/compare/v3...v3.1.3 v3.1.2  Update all /* NPM packages to their latest versions  CC(add version attribute) Update all dev dependencies to their most recent versions   CC(Cython compile error when `python build/build.py`)  v3.1.1  Update actions/core package to latest version to remove setoutput deprecation warning  CC(Gather batching rule)  v3.1.0 What's Changed  Bump @​actions/artifact to v1.1.0 (actions/uploadartifact CC(add batching rule for cholesky PAIR=phawkins))  Adds checksum headers on artifact upload (actions/toolkit CC(Fix test failures due to Numpy 1.17.)) (actions/toolkit CC(add ShardedDeviceTuple constant handler))       Commits  5d5d22a Merge pull request  CC(Fix convert_element_type bug when converting from a complex dtype.) from actions/eggyhead/updateartifactv2.1.1 f1e993d update artifact license 4881bfd updating dist: a30777e @​eggyhead 3a80482 Merge pull request  CC(Negative padding in pad_transpose) from actions/robherley/migrationdocstypo 9d63e3f Merge branch 'main' into robherley/migrationdocstypo dfa1ab2 fix typo with v3 artifact downloads in migration guide d00351b Merge pull request  CC(Support nondefault axis values in batch norm.) from markmssd/patch1 707f5a7 Update limitation of 10 artifacts upload to 500 26f96df Merge pull request  CC(numpy fft not yet implemented) from actions/robherley/mergeartifacts Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-04-08T17:42:57Z,dependencies github_actions,closed,0,2,https://github.com/jax-ml/jax/issues/20639,"Closing because the commit hash looks wrong. We already point to the v4.3.1 commit hash (https://github.com/actions/uploadartifact/commit/5d5d22a31266ced268874388b861e4b58bb5c2f3) it's unclear why the bot has chosen `ef09cdac3e2d3e60d8ccadda691f4f1cec5035cb`, as it doesn't correspond to any release tag.","OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
yi,Bump actions/setup-python from 4.7.1 to 5.1.0,"Bumps actions/setuppython from 4.7.1 to 5.1.0.  Release notes Sourced from actions/setuppython's releases.  v5.1.0 What's Changed  Leveraging the raw API to retrieve the versionmanifest, as it does not impose a rate limit and hence facilitates unrestricted consumption without the need for a token for Github Enterprise Servers by @​Shegox in actions/setuppython CC(Generic support for JIT compilation with custom NumPy ops). Dependency updates by @​dependabot and @​HarithaVattikuti in actions/setuppython CC(JAX slow compared to numba for certain calculations) Documentation changes for version in README by @​basnijholt in actions/setuppython CC(Scatter transpose rule doesn’t handle symbolic zeros) Documentation changes for link in README by @​ukd1 in actions/setuppython CC(vmap error with emtpy tuple) Documentation changes for link in Advanced Usage by @​Jamim in actions/setuppython CC(Improve behavior of a number of math functions for extreme inputs.) Documentation changes for avoiding rate limit issues on GHES by @​priyakinthali in actions/setuppython CC(IndexError: Indexing mode not yet supported)  New Contributors  @​basnijholt made their first contribution in actions/setuppython CC(Scatter transpose rule doesn’t handle symbolic zeros) @​ukd1 made their first contribution in actions/setuppython CC(vmap error with emtpy tuple) @​Jamim made their first contribution in actions/setuppython CC(Improve behavior of a number of math functions for extreme inputs.) @​Shegox made their first contribution in actions/setuppython CC(Generic support for JIT compilation with custom NumPy ops) @​priyakinthali made their first contribution in actions/setuppython CC(IndexError: Indexing mode not yet supported)  Full Changelog: https://github.com/actions/setuppython/compare/v5.0.0...v5.1.0 v5.0.0 What's Changed In scope of this release, we update node version runtime from node16 to node20 (actions/setuppython CC(backward pass of scan is very slow to compile in CPU)). Besides, we update dependencies to the latest versions. Full Changelog: https://github.com/actions/setuppython/compare/v4.8.0...v5.0.0 v4.8.0 What's Changed In scope of this release we added support for GraalPy (actions/setuppython CC(Higher order derivatives of norm.logcdf seem to have numerical problems at low input values)). You can use this snippet to set up GraalPy: steps:  uses: actions/checkout  uses: actions/setuppython    with:     pythonversion: 'graalpy22.3'   run: python my_script.py  Besides, the release contains such changes as:  Trim python version when reading from file by @​FerranPares in actions/setuppython CC(Forwardmode differentiation rule for 'select_and_scatter_add' not implemented) Use nondeprecated versions in examples by @​jeffwidman in actions/setuppython CC(Build error on MacOS) Change deprecation comment to past tense by @​jeffwidman in actions/setuppython CC(Support for GPU SVD) Bump @​babel/traverse from 7.9.0 to 7.23.2 by @​dependabot in actions/setuppython CC(Limit jax multithreading) advancedusage.md: Encourage the use actions/checkout by @​cclauss in actions/setuppython CC(improve documetnation of lax parallel operators) Examples now use checkout by @​simonw in actions/setuppython CC(np.trace is broken) Update actions/checkout to v4 by @​dmitryshibanov in actions/setuppython CC(make jax.random default dtypes 64bit)  New Contributors  @​FerranPares made their first contribution in actions/setuppython CC(Forwardmode differentiation rule for 'select_and_scatter_add' not implemented) @​timfel made their first contribution in actions/setuppython CC(Higher order derivatives of norm.logcdf seem to have numerical problems at low input values)    ... (truncated)   Commits  82c7e63 Documentation changes for avoiding rate limit issues on GHES ( CC(IndexError: Indexing mode not yet supported)) 10aa35a feat: fallback to raw endpoint for manifest when rate limit is reached ( CC(Generic support for JIT compilation with custom NumPy ops)) 9a7ac94 Bump undici from 5.27.2 to 5.28.3 ( CC(JAX slow compared to numba for certain calculations)) 871daa9 Fix the &quot;Specifying multiple Python/PyPy versions&quot; link ( CC(Improve behavior of a number of math functions for extreme inputs.)) 2f07895 Fix broken README.md link ( CC(vmap error with emtpy tuple)) e9d6f99 Replace setuppython by setuppython in README ( CC(Scatter transpose rule doesn’t handle symbolic zeros)) 0a5c615 Update action to node20 ( CC(backward pass of scan is very slow to compile in CPU)) 0ae5836 Add example of GraalPy to docs ( CC(Build for Ubuntu (ARMbased machine))) b64ffca update actions/checkout to v4 ( CC(make jax.random default dtypes 64bit)) 8d28961 Examples now use checkout ( CC(np.trace is broken)) Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-04-08T17:42:51Z,dependencies github_actions,closed,0,2,https://github.com/jax-ml/jax/issues/20638,Closing because the ratchet tag is wrong; I'll fix this manually.,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
yi,jax.debug.callback changes array type," Description ```python import jax.numpy as jnp from jax import Array, debug, jit def callback(x) > None:     assert isinstance(x, Array)   Fail!  def f(x):     assert isinstance(x, Array)   Good.     debug.callback(callback, x) f(jnp.ones(1)) ``` For some reason, `debug.callback` rematerializes the array as a numpy array instead of `jaxlib.xla_extension.ArrayImpl` (the thing created by `jnp.asarray`).  This can cause problems if the array in the callback is mixed with jax arrays when passed to libraries that implement the Array API.  System info (python version, jaxlib version, accelerator, etc.) An NVIDIA GPU may be present on this machine, but a CUDAenabled jaxlib is not installed. Falling back to cpu. jax: 0.4.26 jaxlib: 0.4.26 numpy: 1.26.4 python: 3.11.8 (main, Feb 22 2024, 17:25:49) [GCC 11.4.0]",2024-04-08T02:13:34Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/20627,"Thanks for the report! This is working as intended: callback functions will convert JAX arrays to NumPy arrays, however this will change soon (see CC(jax.pure_callback and jax.experimental.io_callback now use jax.Arrays))"," Great to hear!   So, in an upcoming release then?   Feel free to close this if so :smile: ",Is `debug.callback` going to turn numpy arrays into Jax arrays?,"Yeah, `debug.callback` is now always called with `jax.Array`s. So, if you call it with a NumPy array, JAX will rewrap the buffer into its own array type.", That may cause array API problems if you push a numpy array in and expect it to work with other numpy arrays.  But this is a better default than before!,"True, but you can always do `np.asarray` in the callback and recover the NumPy array, right?","Yes, but I'd have to store the type somewhere.  It's not a problem for me now though.  If it becomes a problem, maybe I'll check back in :smile:"
yi,Segmentation Fault on JAX GPU," Description Steps to reproduce: ```bash python m venv .venv && source .venv/bin/activate git clone https://github.com/atomicarchitects/symphony pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` and then running: ``` python m tests.train_test ``` gives: ```bash 20240406 23:20:38.196235: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT Running tests under Python 3.10.14: /home/ameyad/sphericalharmonicnet/.venv/bin/python [ RUN      ] TrainTest.test_equivariance0 (config_name='qm9_test', rng=0) Fatal Python error: Segmentation fault Current thread 0x00007fba32f6c180 (most recent call first):   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/jax/_src/xla_bridge.py"", line 322 in _version_check   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/jax/_src/xla_bridge.py"", line 387 in _check_cuda_versions   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/jax/_src/xla_bridge.py"", line 428 in make_gpu_client   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/jax/_src/xla_bridge.py"", line 935 in _init_backend   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/jax/_src/xla_bridge.py"", line 853 in backends   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/jax/_src/xla_bridge.py"", line 960 in _get_backend_uncached   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/jax/_src/xla_bridge.py"", line 981 in get_backend   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/jax/_src/xla_bridge.py"", line 1093 in local_devices   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/jax/_src/interpreters/pxla.py"", line 1698 in _get_default_device   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/jax/_src/interpreters/pxla.py"", line 1743 in _get_and_check_device_assignment   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/jax/_src/interpreters/pxla.py"", line 1707 in __call__   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 1322 in _resolve_in_shardings   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 1399 in _pjit_call_impl_python   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 1471 in call_impl_cache_miss   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 1488 in _pjit_call_impl   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/jax/_src/core.py"", line 913 in process_primitive   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/jax/_src/core.py"", line 425 in bind_with_trace   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/jax/_src/core.py"", line 2788 in bind   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 176 in _python_pjit_helper   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 298 in cache_miss   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/jax/_src/traceback_util.py"", line 179 in reraise_with_filtered_traceback   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 87 in apply_primitive   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/jax/_src/core.py"", line 913 in process_primitive   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/jax/_src/core.py"", line 425 in bind_with_trace   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/jax/_src/core.py"", line 422 in bind   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/jax/_src/lax/lax.py"", line 558 in _convert_element_type   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/jax/_src/numpy/lax_numpy.py"", line 2197 in array   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/jax/_src/numpy/lax_numpy.py"", line 2242 in asarray   File ""/home/ameyad/sphericalharmonicnet/tests/loss_test.py"", line 21 in create_dummy_data   File ""/home/ameyad/sphericalharmonicnet/tests/train_test.py"", line 32 in setUp   File ""/home/ameyad/miniconda3/envs/symphony/lib/python3.10/unittest/case.py"", line 546 in _callSetUp   File ""/home/ameyad/miniconda3/envs/symphony/lib/python3.10/unittest/case.py"", line 587 in run   File ""/home/ameyad/miniconda3/envs/symphony/lib/python3.10/unittest/case.py"", line 650 in __call__   File ""/home/ameyad/miniconda3/envs/symphony/lib/python3.10/unittest/suite.py"", line 122 in run   File ""/home/ameyad/miniconda3/envs/symphony/lib/python3.10/unittest/suite.py"", line 84 in __call__   File ""/home/ameyad/miniconda3/envs/symphony/lib/python3.10/unittest/suite.py"", line 122 in run   File ""/home/ameyad/miniconda3/envs/symphony/lib/python3.10/unittest/suite.py"", line 84 in __call__   File ""/home/ameyad/miniconda3/envs/symphony/lib/python3.10/unittest/runner.py"", line 184 in run   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/absl/testing/_pretty_print_reporter.py"", line 82 in run   File ""/home/ameyad/miniconda3/envs/symphony/lib/python3.10/unittest/main.py"", line 271 in runTests   File ""/home/ameyad/miniconda3/envs/symphony/lib/python3.10/unittest/main.py"", line 101 in __init__   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/absl/testing/absltest.py"", line 2653 in _run_and_get_tests_result   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/absl/testing/absltest.py"", line 2689 in run_tests   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/absl/testing/absltest.py"", line 2234 in main_function   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/absl/app.py"", line 254 in _run_main   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/absl/app.py"", line 308 in run   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/absl/testing/absltest.py"", line 2236 in _run_in_app   File ""/home/ameyad/sphericalharmonicnet/.venv/lib/python3.10/sitepackages/absl/testing/absltest.py"", line 2131 in main   File ""/home/ameyad/sphericalharmonicnet/tests/train_test.py"", line 85 in    File ""/home/ameyad/miniconda3/envs/symphony/lib/python3.10/runpy.py"", line 86 in _run_code   File ""/home/ameyad/miniconda3/envs/symphony/lib/python3.10/runpy.py"", line 196 in _run_module_as_main Extension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, jaxlib.cpu_feature_guard, scipy._lib._ccallback_c, scipy.special._ufuncs_cxx, scipy.special._cdflib, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.special._ellip_harm_2, scipy.signal._sigtools, scipy._lib._uarray._uarray, scipy.signal._max_len_seq_inner, scipy.signal._upfirdn_apply, scipy.signal._spline, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.spatial.transform._rotation, scipy.interpolate._fitpack, scipy.interpolate.dfitpack, scipy.optimize._minpack2, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._highs.cython.src._highs_wrapper, scipy.optimize._highs._highs_wrapper, scipy.optimize._highs.cython.src._highs_constants, scipy.optimize._highs._highs_constants, scipy.linalg._interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.interpolate._bspl, scipy.interpolate._ppoly, scipy.interpolate.interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.ndimage._nd_image, _ni_label, scipy.ndimage._ni_label, scipy.signal._sosfilt, scipy.signal._spectral, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.special.cython_special, scipy.stats._stats, scipy.stats.beta_ufunc, scipy.stats._boost.beta_ufunc, scipy.stats.binom_ufunc, scipy.stats._boost.binom_ufunc, scipy.stats.nbinom_ufunc, scipy.stats._boost.nbinom_ufunc, scipy.stats.hypergeom_ufunc, scipy.stats._boost.hypergeom_ufunc, scipy.stats.ncf_ufunc, scipy.stats._boost.ncf_ufunc, scipy.stats.ncx2_ufunc, scipy.stats._boost.ncx2_ufunc, scipy.stats.nct_ufunc, scipy.stats._boost.nct_ufunc, scipy.stats.skewnorm_ufunc, scipy.stats._boost.skewnorm_ufunc, scipy.stats.invgauss_ufunc, scipy.stats._boost.invgauss_ufunc, scipy.stats._biasedurn, scipy.stats._levy_stable.levyst, scipy.stats._stats_pythran, scipy.stats._ansari_swilk_statistics, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._mvn, scipy.stats._rcont.rcont, scipy.stats._unuran.unuran_wrapper, scipy.signal._peak_finding_utils, msgpack._cmsgpack, yaml._yaml, matscipy._matscipy, matplotlib._c_internal_utils, PIL._imaging, matplotlib._path, kiwisolver._cext, matplotlib._image, google._upb._message, tensorflow.python.framework.fast_tensor_util, charset_normalizer.md, h5py._errors, h5py.defs, h5py._objects, h5py.h5, h5py.utils, h5py.h5t, h5py.h5s, h5py.h5ac, h5py.h5p, h5py.h5r, h5py._proxy, h5py._conv, h5py.h5z, h5py.h5a, h5py.h5d, h5py.h5ds, h5py.h5g, h5py.h5i, h5py.h5f, h5py.h5fd, h5py.h5pl, h5py.h5o, h5py.h5l, h5py._selector, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, rdkit.rdBase, rdkit.DataStructs.cDataStructs, rdkit.Chem.rdchem, rdkit.Geometry.rdGeometry, rdkit.Chem.rdinchi, rdkit.Chem.rdCIPLabeler, rdkit.Chem.rdmolfiles, rdkit.Chem.rdmolops, rdkit.Chem.rdMolInterchange, rdkit.Chem.rdCoordGen, psutil._psutil_linux, psutil._psutil_posix, rdkit.Chem.rdDetermineBonds, rdkit.Chem.rdDistGeom, rdkit.ForceField.rdForceField, rdkit.Chem.rdForceFieldHelpers, rdkit.Chem.rdChemicalFeatures, rdkit.Chem.rdMolChemicalFeatures, rdkit.Chem.rdChemReactions, rdkit.Chem.rdDepictor, rdkit.Chem.rdFingerprintGenerator, rdkit.Chem.rdMolAlign, rdkit.Chem.rdMolDescriptors, rdkit.Chem.rdMolEnumerator, rdkit.Chem.rdMolTransforms, rdkit.Chem.rdPartialCharges, rdkit.Chem.rdqueries, rdkit.Chem.rdReducedGraphs, rdkit.Chem.rdShapeHelpers, rdkit.Chem.rdSLNParse, rdkit.Chem.MolStandardize.rdMolStandardize (total: 231) Segmentation fault (core dumped) ``` However, if I install the CPU version of JAX above: ```bash pip install ""jax[cpu]"" ``` everything is good. Any help would be greatly appreciated.  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.26 jaxlib: 0.4.26 numpy:  1.24.3 python: 3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0] jax.devices (8 total, 8 local): [cuda(id=0) cuda(id=1) ... cuda(id=6) cuda(id=7)] process_count: 1 platform: uname_result(system='Linux', node='potato.mit.edu', release='5.4.0139generic', version=' CC(Implement np.linalg.slogdet.)Ubuntu SMP Fri Jan 20 17:27:18 UTC 2023', machine='x86_64') $ nvidiasmi Sat Apr  6 23:24:08 2024 ++  ++ ```",2024-04-07T03:24:52Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/20619,"I tried an older version: ```bash pip install upgrade ""jax[cuda12_pip]==0.4.24"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` and it seems to work, but with the following warning: ``` W0406 23:27:00.085962 140583803240832 xla_bridge.py:724] CUDA backend failed to initialize: Found cuDNN version 8800, but JAX was built against version 8906, which is newer. The copy of cuDNN that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) ```"
yi,Performance issue with 64bit on CPU," Description Hi, I encountered a performance issue when enabling 64bit computations, and I would be glad if someone could help me here  is this a known issue, how do i fix it, etc. Below the example. ``` import time from contextlib import contextmanager from functools import partial from typing import Iterator import jax import jax.numpy as jnp jax.config.update(""jax_enable_x64"", True)   every option takes approximately the same time, as expected, when this is left out  def measure_time(name: str) > Iterator[None]:     start = time.time()     yield     stop = time.time()     print(f""Task [{name}] took {stop  start:.2f} s."") def g(x, a, b, c, inner_func):     d = jax.vmap(jax.vmap(         jax.vmap(             jax.vmap(                 inner_func,                 in_axes=(None, None, 0),                 out_axes=0             ),             in_axes=(None, 0, None),             out_axes=0         )     ))(x, a, b)     return jnp.einsum(""ij,ij...>i..."", c, d) f1 = lambda x: 1 + jnp.sum(jnp.sin(jnp.pi * x)**2)   f1 evaluates to 1.0 for x = 0.0, same as f2 f2 = lambda x: jnp.asarray(1.0) inner_func1 = lambda x, u, v: f1(x) * u @ v inner_func2 = lambda x, u, v: f1(x) * (u @ v) inner_func3 = lambda x, u, v: f2(x) * u @ v inner_func4 = lambda x, u, v: f2(x) * (u @ v) g1 = partial(g, inner_func=inner_func1) g2 = partial(g, inner_func=inner_func2) g3 = partial(g, inner_func=inner_func3) g4 = partial(g, inner_func=inner_func4) x = jnp.zeros((13720, 57, 3)) a = jnp.zeros((13720, 57, 35, 3)) b = jnp.zeros((13720, 57, 35, 3)) c = jnp.zeros((13720, 57)) with measure_time(""g1""):     jax.block_until_ready(jax.jit(g1)(x, a, b, c)) with measure_time(""g2""):     jax.block_until_ready(jax.jit(g2)(x, a, b, c))   much much slower than all other options when 64bit is enabled with measure_time(""g3""):     jax.block_until_ready(jax.jit(g3)(x, a, b, c)) with measure_time(""g4""):     jax.block_until_ready(jax.jit(g4)(x, a, b, c)) ``` All 4 functions g1g4 produce the same result (for `x = 0.0`), but g2 takes significantly longer than all other options. Of course, in real use cases, the inputs are not zero. g1 / g2 and g3 / g4 are mathematically equivalent. I tried to keep this code chunk as small as possible without making it hard to understand what's going on. This code is used for finite element computation, so this seemingly random function g has actual meaning to it. I would appreciate any help!  System info (python version, jaxlib version, accelerator, etc.) python 3.12 jax 0.4.26 jaxlib 0.4.26 CPU (AMD)",2024-04-06T19:04:24Z,performance XLA,closed,0,9,https://github.com/jax-ml/jax/issues/20616,"Is it possible that this is a RAM issue? It almost seems like it  with smaller arrays, even with x64 everything looks ok. But then again, I do not see (even with looking at the jaxpr) how the second option should be more expensive in terms of RAM than the others.","I think I tracked down the issue, and it looks like this even happens with 32bit numbers. Consider the following example, where the second function almost uses twice as much memory as the first one. The only thing different is the order of multiplication and dot product  as far as I know, this should not affect memory at all. Is there any way to check this? ```` import time from contextlib import contextmanager from typing import Iterator import jax import jax.numpy as jnp  def measure_time(name: str) > Iterator[None]:     start = time.time()     yield     stop = time.time()     print(f""Task [{name}] took {stop  start:.2f} s."") key = jax.random.PRNGKey(0) (subkey1, subkey2, subkey3, subkey4) = jax.random.split(key, 4) x = jax.random.uniform(subkey1, (15000, 50, 1, 1)) a = jax.random.uniform(subkey2, (15000, 50, 35, 5)) b = jax.random.uniform(subkey3, (15000, 50, 35, 5)) c = jax.random.uniform(subkey4, (15000, 50)) def h1(x, a, b, c):     a = x * a     d = jnp.einsum(""...ji,...ki>...jk"", a, b)     return d def h2(x, a, b, c):     d = jnp.einsum(""...ji,...ki>...jk"", a, b)     d = x * d     return d with measure_time(""h1""):     y = jax.jit(h1)(x, a, b, c)     jax.block_until_ready(y) with measure_time(""h2""):     y = jax.jit(h2)(x, a, b, c)     jax.block_until_ready(y) ````","Thanks for the report. It looks like the compiler is able to recognize the constant expression in one case, but not in the other. I don't think I'd consider this a bug: we can't expect the compiler to be perfect, and a longenough sequence of operations may evade its rewrite rules. But we might raise this issue in http://github.com/openxla/xla: perhaps there's a way to update the compiler's heuristics to catch this case.","Thanks for the answer. I don't have any experience with XLA per se, so I wouldn't know where to start there. Also, I don't really know how to observe RAM usage of a compiled function reliably. Currently I think the problem is that in this function ``` def f(x, a, b, c):     d = jnp.einsum(""...ji,...ki>...jk"", a, b)     d = x * d     return d ``` the multiplication `x * d` is not performed inplace even though it would technically be possible. Do you know how it would be possible to confirm this?","I mean, it's just a suspicion. But why else would it need so much more memory than ``` def f(x, a, b, c):     a = x * a     d = jnp.einsum(""...ji,...ki>...jk"", a, b)     return d ``` But then again, I don't even know how to veryify that this memory problem is ""real"" and not just a ""on my machine"" problem.","We can see what the compiler is doing with these functions using ahead of time lowering. For example: ``` h1_lowered = jax.jit(h1).lower(x, a, b, c).compile() h2_lowered = jax.jit(h2).lower(x, a, b, c).compile()  print compiled HLO print(h1_lowered.as_text()) print(h2_lowered.as_text())  print compiled HLO print(h1_lowered.cost_analysis()) print(h2_lowered.cost_analysis()) ``` If you inspect the outputs, you see that the second function has a much larger ""bytes accessed"" in the cost analysis. I believe this is due to the compiler not having any heuristic to fuse the operations when they are performed in this order, but I'm not certain of that interpretation.","I think I understand the difference: the expensive allocation is the output of the `einsum`. In `h1`, the input to the einsum is an internal buffer (the output of `x * a`) and therefore can be overwritten. In `h2`, both inputs are userlevel buffers, and so neither are overwritable by default and the compiler must allocate a new large buffer for the output. You could probably address this by using `donate_argnums` to tell the compiler that one or more of the input buffers is overwriteable (but of course you would not be able to use those donated buffers in subsequent computations). If this operation is done as part of a larger program, then it's likely that the inputs will already be temporary buffers, and this microbenchmark will not be reflective of the overall performance of the larger program. Hope that's clear!","Closing, as I think this is working as expected.","Thanks for helping . I agree, _IF_ this is an issue, it's definitely not JAX, but whatever the compiler does. In the meantime I played around with this a bit and noticed two important things (Your comments have been very helpful): 1. In the code I posted in this comment https://github.com/google/jax/issues/20616issuecomment2041276158, there is no problem at all. Both functions need exactly the same amount of memory (I think, not entirely sure though). If you simply put a `del y` in between the function calls, everything works as expected. That's on me, sorry for that. 2. The original issue still persists, I'm having huge performance drops (which I still think has to do with memory usage) in my actual code, which is still reproducable with the original code I posted in this issue (in which the function `g2` needs almost twice as much memory as the other options). If I find the actual reason for my problem (and if it has something to do with jax), I'll probably just open a new issue, if that is ok."
rag,Efficient diag(JtJ),"As best as I can tell, the most efficient way to get diag(JtJ) is to do jnp.square(primals).T @ jnp.square(cotangents), but this seems to be difficult to cleanly implement in JAX. I have an utterly terrible way of doing this by using a custom VJP: ```python .custom_vjp def matmul(a, b):  return a @ b def matmul_fwd(a, b):  return a @ b, (a, b) def matmul_bwd(res, y_bar):     a, b = res       da = y_bar @ b.T    db = a.T @ y_bar  What would actually compute the gradient.   db = jnp.square(a.T) @ jnp.square(y_bar)  Instead compute JtJ and return that as the gradient.   return da, db matmul.defvjp(matmul_fwd, matmul_bwd) class Test(nn.Module):   .compact   def __call__(self, x):     w1 = self.param('w1', nn.initializers.normal(), (x.shape[1], 3))     x = matmul(x, w1)  x @ w1     x = jax.nn.relu(x)     w2 = self.param('w2', nn.initializers.normal(), (x.shape[1], 1))         x = matmul(x, w2)  x @ w2         return x.sum() test = Test() x = jnp.ones((15,32)) params = params.init(jax.random.key(1), x) grad = jax.grad(test.apply)(params, x)   Actually computes diag(JtJ) rather than J (!) ``` Needless to say, this is rather ugly and fragile. So the question: Is there a good way to implement this with comparable efficiency? I did look at doing perexample gradients and then squaring and summing them, but that seems to be much less efficient: It builds the full outer product in memory before reducing. There doesn't seem to be an optimizer pass that will promote the square operation to the other side of the matrix multiply :(",2024-04-05T02:43:52Z,enhancement,open,0,14,https://github.com/jax-ml/jax/issues/20600,"Thanks for the question! I'm not sure, but I wouldn't be surprised if we can't do the maximally efficient thing as a composition of just `jax.jvp` and `jax.vjp` (and `jax.vmap`), and instead we need to write a special structureexploiting recursion. By that I mean something like in CC(laplacian experiment) (and the libraries discussed in the comments which do a much better job with that, though I think the fundamental math is about the same), or like what people have done for computing log det Jacobians efficiently (like Oryx does), or what I believe this 'fast NTK' stuff does. Actually, that latter may be computing something similar to what you want, though they're usually after full JJ' matrices rather than diag(J'J) matrices. Indeed I think your rule for matmul is representing exactly such a structureexploiting rule: because the Jacobian of v > Av is simply A, and because diag(A'A) can be cleverly computed using a Hadamard product A.^2 (and summing), we can avoid ever having to do a matmul to compute the coefficients of diag(A'A).  I want to think about this more when I have more time, but wanted to plant that initial guess. I'm starting to think that there are several such special autodiff recursions of interest, and we should factor the JAX autodiff machinery to make those things easy to write (i.e. you just write the rules, then the library takes care of all the JAXinternals boilerplate). Then we could build more such things into JAX, and also make it easy to stand up user libraries. WDYT?","As you point out, we could play that trick with the whole Jacobian, or rather all the perexample gradients, and maybe we can't get better asymptotic FLOP costs than that. But with a special recursion we'll be doing the computation in the right order, and thus not OOMing for example, rather than relying on the compiler to clean up everything.","The special recursion absolutely sounds like something that would be very useful. I'll how to think about exactly what that might look like in the more general case. For the optimization side, it did occur to me that it's fairly natural. Computing the JtJ as: ```python   def compute_jtj(params, x):     per_ex_grads = jax.vmap(jax.grad(test.apply), in_axes=(None, 0))(params, x)     jtj = jax.tree_util.tree_map(lambda x : jnp.square(x).sum(axis=0), per_ex_grads)     grads = jax.tree_util.tree_map(lambda x : x.sum(axis=0), per_ex_grads)     return jtj ``` This should compile to something like: ```   x = jnp.einsum('b u, b j > b i j', primals, cotangents)   x = jnp.square(x)   x = x.sum() ``` The matrix multiplication there has no contracting dimension, and the inputs are much smaller in size than the output, so lifting the square op to before the matrix mul should be a very easy optimization opportunity (lower FLOPS, lower memory BW, no precision issues as there's no additions). After that, it becomes: ```   a = jnp.square(primals)   b = jnp.square(cotangents)   x = jnp.einsum('b u, b j > b i j', a, b)   x = x.sum() ``` At which point, removing the sum() is another zerorisk optimization: It's clearly just adding the contracting dimension to the dotgeneral. It saves much memory BW and has no impact on precision. So it might be the easier thing in the near term is to enable these optimizations? I'm not sure if they'd go in JAX or LAX (probably LAX?).  I absolutely agree it would be better not to rely on compiler magic, but these may be such clear wins that not doing them is a bug! :)",Good point about moving the square through the outer product. I see now what you meant in your first message. Is that the only optimization you need here?,"Is the conversion of the sum() into an additional contracting dimension for the preceeding dotgeneral an already existing optimization? If so then yes, lifting the square is the only one remaining.","Just checked, looks like merging the sum into the dotgeneral does happen: ```python def f(x, w):   x = jnp.einsum('b i, b j > b i j', x, w)   x = x.sum(axis=0)   return x print(jax.jit(f).lower(jnp.ones((64,64)), jnp.ones((64, 64))).compile().as_text()) ``` gives ``` ENTRY %main.10 (Arg_0.1: f32[64,64], Arg_1.2: f32[64,64]) > f32[64,64] {   %Arg_1.2 = f32[64,64]{1,0} parameter(1), sharding={replicated}   %Arg_0.1 = f32[64,64]{1,0} parameter(0), sharding={replicated}   ROOT %customcall = f32[64,64]{1,0} customcall(f32[64,64]{1,0} %Arg_0.1, f32[64,64]{1,0} %Arg_1.2), custom_call_target=""__cublas$gemm"", frontend_attributes={fingerprint_before_lhs=""f45c017d4cbc60a6d36d764a48114f68""}, metadata={op_name=""jit(f)/jit(main)/b i, b j > b i j/dot_general[dimension_numbers=(((), ()), ((0,), (0,))) precision=None preferred_element_type=float32]""}, backend_config={""alpha_real"":1,""alpha_imag"":0,""beta"":0,""dot_dimension_numbers"":{""lhs_contracting_dimensions"":[""0""],""rhs_contracting_dimensions"":[""0""],""lhs_batch_dimensions"":[],""rhs_batch_dimensions"":[]},""precision_config"":{""operand_precision"":[""DEFAULT"",""DEFAULT""]},""epilogue"":""DEFAULT""} } ``` which looks like it has folded the sum into a contracting dimension. So yes, just lifting the square is enough to make the perexample gradient avenue efficient.","Hmm. Before I say that, the compile code for the current perexample computation is a little weird: ```   ...   %select.1 = f32[128,256]{1,0} select(pred[128,256]{1,0} %compare.1, f32[128,256]{1,0} %broadcast.22, f32[128,256]{1,0} %broadcast.23), metadata={op_name=""jit(jtj)/jit(main)/vmap(transpose(jvp(Test)))/select_n""}   %broadcast.21 = f32[512,128,256]{2,1,0} broadcast(f32[128,256]{1,0} %select.1), dimensions={1,2}, metadata={op_name=""jit(jtj)/jit(main)/vmap(transpose(vmap(jvp(Test))))/dot_general[dimension_numbers=(((), ()), ((0,), (0,))) precision=None preferred_element_type=float32]""}   %param_0.5 = f32[512,256]{1,0} parameter(0)   %broadcast.20 = f32[512,128,256]{2,1,0} broadcast(f32[512,256]{1,0} %param_0.5), dimensions={0,2}, metadata={op_name=""jit(jtj)/jit(main)/vmap(transpose(vmap(jvp(Test))))/dot_general[dimension_numbers=(((), ()), ((0,), (0,))) precision=None preferred_element_type=float32]""}   %multiply.14 = f32[512,128,256]{2,1,0} multiply(f32[512,128,256]{2,1,0} %broadcast.21, f32[512,128,256]{2,1,0} %broadcast.20), metadata={op_name=""jit(jtj)/jit(main)/vmap(transpose(vmap(jvp(Test))))/dot_general[dimension_numbers=(((), ()), ((0,), (0,))) precision=None preferred_element_type=float32]""}   %multiply.13 = f32[512,128,256]{2,1,0} multiply(f32[512,128,256]{2,1,0} %multiply.14, f32[512,128,256]{2,1,0} %multiply.14), metadata={op_name=""jit(jtj)/jit(main)/mul""}   ROOT %reduce.7 = f32[512,128]{1,0} reduce(f32[512,128,256]{2,1,0} %multiply.13, f32[] %constant_9), dimensions={2}, to_apply=%region_0.24 ``` It's actually broadcasting out the primals and tangents before doing an elementwise multiplication, not doing a dotgeneral at all. That seems ... odd? Unless there's some downstream optimization, that's going to use more memory and more memory BW than doing the dotgeneral?","It would be good to check the jaxpr, but I think you can see from the name metadata that those ops started from dot_generals in the jaxpr. I'd want to know that the perexample gradient computation looks right in the jaxpr; then it's just a question of how XLA is deciding to lower it (which would be platformdependent, sizedependent, etc).","Yes, the jaxpr looks sane. ``` module  attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {   func.func public (%arg0: tensor {mhlo.layout_mode = ""default"", mhlo.sharding = ""{replicated}""}, %arg1: tensor {mhlo.layout_mode = ""default"", mhlo.sharding = ""{replicated}""}, %arg2: tensor {mhlo.layout_mode = ""default"", mhlo.sharding = ""{replicated}""}) > (tensor {jax.result_info = ""['params']['w1']"", mhlo.layout_mode = ""default""}, tensor {jax.result_info = ""['params']['w2']"", mhlo.layout_mode = ""default""}) {     %0 = stablehlo.dot_general %arg2, %arg0, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor, tensor) > tensor     %1 = call (%0) : (tensor) > tensor     %2 = stablehlo.constant dense : tensor     %3 = stablehlo.broadcast_in_dim %2, dims = [] : (tensor) > tensor     %4 = stablehlo.compare  GT, %0, %3,  FLOAT : (tensor, tensor) > tensor     %5 = stablehlo.constant dense : tensor     %6 = stablehlo.broadcast_in_dim %5, dims = [] : (tensor) > tensor     %7 = stablehlo.dot_general %6, %1, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor, tensor) > tensor     %8 = stablehlo.transpose %7, dims = [1, 2, 0] : (tensor) > tensor     %9 = stablehlo.dot_general %6, %arg1, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor, tensor) > tensor     %10 = stablehlo.constant dense : tensor     %11 = stablehlo.broadcast_in_dim %10, dims = [] : (tensor) > tensor     %12 = stablehlo.constant dense : tensor     %13 = stablehlo.broadcast_in_dim %12, dims = [] : (tensor) > tensor     %14 = stablehlo.compare  EQ, %4, %13,  UNSIGNED : (tensor, tensor) > tensor     %15 = stablehlo.broadcast_in_dim %11, dims = [1] : (tensor) > tensor     %16 = stablehlo.broadcast_in_dim %9, dims = [1] : (tensor) > tensor     %17 = stablehlo.select %14, %16, %15 : tensor, tensor     %18 = stablehlo.dot_general %17, %arg2, batching_dims = [0] x [0], contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor, tensor) > tensor     %19 = stablehlo.transpose %18, dims = [0, 2, 1] : (tensor) > tensor     %20 = stablehlo.multiply %19, %19 : tensor     %21 = stablehlo.constant dense : tensor     %22 = stablehlo.reduce(%20 init: %21) applies stablehlo.add across dimensions = [0] : (tensor, tensor) > tensor     %23 = stablehlo.multiply %8, %8 : tensor     %24 = stablehlo.constant dense : tensor     %25 = stablehlo.reduce(%23 init: %24) applies stablehlo.add across dimensions = [0] : (tensor, tensor) > tensor     return %22, %25 : tensor, tensor   }   func.func private (%arg0: tensor) > tensor {     %0 = stablehlo.constant dense : tensor     %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor) > tensor     %2 = stablehlo.maximum %arg0, %1 : tensor     return %2 : tensor   } } ``` The square() at %20 needs to lift through the transpose at %19 as well as the dotgeneral at %18","I asked Blake the compiler guru and he said: > Seems like it is equivalent on reals but definitely not numerically identical so xla probably would not do it > It isn't unstable just different numerically > We don't know what the user is intending in general > So we avoid elementwise reassociation I'm following up a bit more, but it leaves us in an interesting spot: how would we solve this on the JAX side? Do we try to provide some mechanism to perform this optimization? Or instead of casting this as an optimization problem, do we instead work out some way to express the right computation well in the first place (along the lines of some 'special autodiff' recursion)?","I must admit it seems a little weird not to optimize this on the compiler side. This optimization reduces FLOP count and memory bandwidth by a factor equal to the batch size (e.g. 1/64th of the memBW for a batch size of 64). Maybe I'm misunderstanding the XLA goals? I take the general point about the elementwise reassociation, but in this case reassociating the square unlocks a huge decrease in the dotgeneral cost. That aside, it seems easy enough to optimize at the jaxpr level.  As far as I know, there's no hooks to add optimizations here? Ideally, this would be a callback from somewhere that takes and returns a Jaxpr. Maybe mlir.jaxpr_subcomp()? I just spent _way_ too long reading through chunks of the stages framework to see if I could mutate the Lowered() object before calling compile() on it, but it seems ... extremely fraught. It seems I can use jax.jit(...).lower(...)._hlo to get an ir.Module, and then get the assembly language from that as text, and then reparse it back into a new ir.Module and assign it to _hlo, and then call compile()! This _seems_ to work, but ... ??? ","I realized I'm actually confused about what you want to compute... When you write diag(JtJ), by J do you mean the Jacobian of scalarvalued function, like `apply` in your example? In that case, you'd be taking the diagonal elements of an outer product, i.e. you'd just be getting the elementwise square of gradients... Can you define J?","> Maybe I'm misunderstanding the XLA goals? Yeah, essentially XLA doesn't perform optimizations that would change numerical semantics too much, because that may be changing something the user wrote intentionally. You could imagine a compiler with a different philosophy, like ""optimize as much as possible, numerics be damned"", or a compiler that is even more conservative about numerics, or anything in between.","You're right, I'm saying diag(JtJ),  but it's a little illdefined here. What I want is $\mathbb{E}[\text{diag}(J^T J)]$, which is approximated by the perexample diag(JtJ) averaged over the batch. This is analogous to `jax.grad` returning $\mathbb{E}[J]$, being the perexample gradient averaged over the batch. NB: $\mathbb{E}[\text{diag}(J^T J)]$ is obviously distinct from $\text{diag}(\mathbb{E}[J]^T \mathbb{E}[J])$, which is what taking `diag(g.T @ g)` for `g = jax.grad(...)(...)` would give. Does that make sense?"
yi,Call cuInit before querying compute capability,"Since https://github.com/google/jax/commit/9fff9aeb69f586d36bdb6422f40b548aa41bd1e3 `jax.distributed.initialize` fails with the error ``` CUDA backend failed to initialize: jaxlib/cuda/versions_helpers.cc:90: operation gpuDeviceGetAttribute( &major, GPU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR, device) failed: CUDA_ERROR_NOT_INITIALIZED (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) ``` in some situations. Outside special cluster environments, JAX queries CUDA to find out how many GPUs are visible. Querying the device count correctly initialises CUDA, and all is well. However, at least on a Slurm clusters, the choice of which GPU a given distributed JAX process should use is based on Slurm environment variables, and in those environments JAX does **not** query the device count.  https://github.com/google/jax/commit/9fff9aeb69f586d36bdb6422f40b548aa41bd1e3 added a check that the selected GPUs have a minimum compute capability. This check assumes that CUDA is already initialised, which it is not in the Slurm case described above. This PR avoids the issue by calling cuInit before querying the compute capability.",2024-04-04T15:36:36Z,pull ready,closed,0,2,https://github.com/jax-ml/jax/issues/20588,Is `cuInit` idempotent? The docs do not say: https://docs.nvidia.com/cuda/cudadriverapi/group__CUDA__INITIALIZE.htmlgroup__CUDA__INITIALIZE,> Is `cuInit` idempotent? >  > The docs do not say: https://docs.nvidia.com/cuda/cudadriverapi/group__CUDA__INITIALIZE.htmlgroup__CUDA__INITIALIZE Yes. I made a request to document that.
yi,nan in hessian of linear function," Description I am trying to vectorize a procedure, where I need to do the elementwise power of a large vector before getting its Hessian. If I manually compute the elementwise power of the vector in order to define the function of interest, I can use this code which gives the expected output: ``` def fun(x):     x = jnp.array([x[0]])     return x hess = jacfwd(jacfwd(fun)) x = jnp.array([0.]) hess_val = hess(x) print(hess_val)  output: [[[0.]]] ``` However, if my function of interest takes the elementwise power as part of the definition, I get nan: ``` def fun(x):     x = x ** jnp.array([1])     return x hess = jacfwd(jacfwd(fun)) x = jnp.array([0.]) hess_val = hess(x) print(hess_val)   output: [[[nan]]] ``` This is related to issue CC(Error in Automatic Differenciation for Functions with Powers of Zero) but different as the problem only arises with the Hessian (the Jacobian gives the expected output using both function definitions). Would you have any hint why this happens? Thank you!  System info (python version, jaxlib version, accelerator, etc.) jax version: 0.4.25 jaxlib version: 0.4.23",2024-04-03T21:53:02Z,bug,open,0,4,https://github.com/jax-ml/jax/issues/20574,"Thanks for the report! I think you're corrrect that this is related to CC(Error in Automatic Differenciation for Functions with Powers of Zero). The reason it only affects the hessian and not the jacobian is because the hessian is a secondorder differentiation. Your function looks like this: $$ f(x) = x^1 $$ Differentiating once, you get this: $$ f^\prime(x) = x^0 $$ And differentiating a second time, you are differentiating $0^0$, which is illdefined for floating point exponent, and so we return NaN. Note that if you use a scalar integer exponent, the derivative is welldefined and the fix referenced in CC(Error in Automatic Differenciation for Functions with Powers of Zero) will apply to your hessian as well: ```python jax.hessian(lambda x: x ** 1)(x)  Array([[[0.]]], dtype=float32) ``` But for nonscalar, noninteger exponentiation, `nan` is the correct output because the value of the derivative is ambiguous (see the discussion in https://github.com/google/jax/issues/14397issuecomment1426386290).","Thank you for your reply, . I am afraid I do not understand why, in the second code snippet above, ```jnp.array([1])``` is treated as a floating point exponent. I just checked and ```jnp.array([1])``` has ```dtype=int32``` so I would have expected JAX to treat it as an integer exponent accordingly. As the endgoal is to take the elementwise power of a large vector: I could of course take the scalar, integer exponent of each element in the vector instead of using ```**``` but I would expect the looping through all the elements in the vector to make the process rather inefficient. Is there no better way? Thank you again.","`x ** jnp.array([1])` lowers to `lax.pow`, which has floatpower semantics, while `x ** 1` lowers to `lax.integer_pow`, which has integerpower semantics. We could maybe specialize the autodiff rules of `pow_p` to be dtypedependent, but we haven't done that. , what do you think?","Thank you for following up, . : I would be happy to make it a pull request but I would most likely need some guidance given that I am not familiar with all the details of JAX yet. What would you suggest?"
yi,test: work around issue with kstest in scipy>1.12,Works around the issue reported in https://github.com/scipy/scipy/issues/20386 Tested locally against scipy 1.13.0.,2024-04-03T18:18:30Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/20569
yi,jnp.geomspace: make complex behavior consistent with NumPy 2.0,"Tested locally against NumPy 2.0.0rc1: ``` $ python c ""import numpy; print(numpy.__version__)"" 2.0.0rc1 $ JAX_ENABLE_X64=1 JAX_NUM_GENERATED_CASES=90 pytest n auto tests/lax_numpy_test.py k Geomspace ============================= test session starts ============================== platform darwin  Python 3.9.6, pytest7.4.4, pluggy1.3.0 rootdir: /Users/vanderplas/jax_numpy_upstream/jax configfile: pyproject.toml plugins: xdist3.5.0, hypothesis6.92.5 8 workers [90 items]     ........................................................................ [ 80%] ..................                                                       [100%] ============================== 90 passed in 6.06s ============================== ``` Related to https://github.com/numpy/numpy/issues/26195.",2024-04-02T23:09:17Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/20552
yi,test: skip complex geomspace test under numpy 2.0,We can revert this once https://github.com/numpy/numpy/issues/26195 is fixed.,2024-04-02T19:29:30Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/20543
yi,[callback] Allow external callbacks to return 64-bit values in 32-bit mode,"Previously, prior to CC([callback] Fix io_callback for callbacks that return Python literals.), if the Python callback returned a Python literal (which is natively a 64bit value), and the `result_shape_dtypes` specified a 32bit expected returned value, we would just get garbage results. In CC([callback] Fix io_callback for callbacks that return Python literals.), I introduced an error in this situation. However, when trying to port the internal code that uses host_callback to `io_callback`, I am getting many instances of this error. The common scenario is a Python callback function that returns a Python scalar: ``` def f_host():   return 42. io_callback(f_host, jax.ShapeDtypeStruct((), np.float32)) ``` However, if the `f_host` were called directly JAX would canonicalize the value `42.` to a `float32` (when `jax_enable_x64` is not set). I do not think that it makes sense for `io_callback` to have stricter behavior that a direct call. In this PR we add a canonicalization step on the returned values of Python callbacks, which casts the values to 32bits if JAX is running in 32bit mode. Note that the above example should return an error in 64bit mode, because the actual returned value is a 64bit value but the declared expected value is `np.float32`. To avoid the error in both 64bit and 32bit mode, the python callback should return `np.float32(42.)`. In some sense this is replacing the change in CC([callback] Fix io_callback for callbacks that return Python literals.) to add a canonicalization step instead of an error.",2024-04-02T14:12:43Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/20534
yi,Bump actions/setup-python from 5.0.0 to 5.1.0,"Bumps actions/setuppython from 5.0.0 to 5.1.0.  Release notes Sourced from actions/setuppython's releases.  v5.1.0 What's Changed  Leveraging the raw API to retrieve the versionmanifest, as it does not impose a rate limit and hence facilitates unrestricted consumption without the need for a token for Github Enterprise Servers by @​Shegox in actions/setuppython CC(Generic support for JIT compilation with custom NumPy ops). Dependency updates by @​dependabot and @​HarithaVattikuti in actions/setuppython CC(JAX slow compared to numba for certain calculations) Documentation changes for version in README by @​basnijholt in actions/setuppython CC(Scatter transpose rule doesn’t handle symbolic zeros) Documentation changes for link in README by @​ukd1 in actions/setuppython CC(vmap error with emtpy tuple) Documentation changes for link in Advanced Usage by @​Jamim in actions/setuppython CC(Improve behavior of a number of math functions for extreme inputs.) Documentation changes for avoiding rate limit issues on GHES by @​priyakinthali in actions/setuppython CC(IndexError: Indexing mode not yet supported)  New Contributors  @​basnijholt made their first contribution in actions/setuppython CC(Scatter transpose rule doesn’t handle symbolic zeros) @​ukd1 made their first contribution in actions/setuppython CC(vmap error with emtpy tuple) @​Jamim made their first contribution in actions/setuppython CC(Improve behavior of a number of math functions for extreme inputs.) @​Shegox made their first contribution in actions/setuppython CC(Generic support for JIT compilation with custom NumPy ops) @​priyakinthali made their first contribution in actions/setuppython CC(IndexError: Indexing mode not yet supported)  Full Changelog: https://github.com/actions/setuppython/compare/v5.0.0...v5.1.0    Commits  82c7e63 Documentation changes for avoiding rate limit issues on GHES ( CC(IndexError: Indexing mode not yet supported)) 10aa35a feat: fallback to raw endpoint for manifest when rate limit is reached ( CC(Generic support for JIT compilation with custom NumPy ops)) 9a7ac94 Bump undici from 5.27.2 to 5.28.3 ( CC(JAX slow compared to numba for certain calculations)) 871daa9 Fix the &quot;Specifying multiple Python/PyPy versions&quot; link ( CC(Improve behavior of a number of math functions for extreme inputs.)) 2f07895 Fix broken README.md link ( CC(vmap error with emtpy tuple)) e9d6f99 Replace setuppython by setuppython in README ( CC(Scatter transpose rule doesn’t handle symbolic zeros)) See full diff in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-04-01T17:31:34Z,pull ready dependencies github_actions,closed,0,0,https://github.com/jax-ml/jax/issues/20520
rag,Issue installing JAX with Nvidia GPU: CUDA backend failed to initialize: Unable to load cuDNN.," Description After installing JAX with Nvidia GPU using the recommended method here, essentially running: ``` pip install upgrade pip  CUDA 12 installation  Note: wheels only available on linux. pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` doing something as simple as  ``` import jax.numpy as jnp jnp.arange(180, 180, 10, dtype=jnp.float32) ``` will trigger an error like this, although I have a GPU on my node. ``` CUDA backend failed to initialize: Unable to load cuDNN. Is it installed? (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) ``` I was able to fix this problem by doing as suggested here: https://github.com/google/jax/issues/18027issuecomment2028488429 Although I am not sure, but I think there might be a bug in detecting the correct version of cuDNN when installing JAX using the recommended method.   System info (python version, jaxlib version, accelerator, etc.) Before the fix, here is the system info after jax installation ``` jax:    0.4.25 jaxlib: 0.4.25 numpy:  1.26.4 python: 3.11.8 (main, Feb 26 2024, 21:39:34) [GCC 11.2.0] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='node100', release='4.18.0372.9.1.el8.x86_64', version=' CC(Python 3 compatibility issues) SMP Tue May 10 14:48:47 UTC 2022', machine='x86_64') $ nvidiasmi Mon Apr  1 12:44:54 2024        ++  ++ ```",2024-04-01T16:46:51Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/20519,"Yes, this is because CuDNN 9.0 was released and we didn't have a maximum version guard. We're making a new release soon that will fix the problem. Closing as duplicate of CC(Unable to correct CUDA vs. JAX version mismatch)."
yi,"Optimized code returns NaN, de-optimized does not"," Description I stumbled upon an error in JAX while trying to compute the gradient of a custom loss function. The error output states that jitoptimized code returns a `nan`, while a deoptimized function does not. There was a similar issue CC(jit(grad(f))(x) returns NaN, but grad(f)(x) evaluates fine), though occurring in a more complicated setting, so perhaps the underlying cause is not the same. Using `jax.config.update(""jax_enable_x64"", True)` does not fix it. A small reproducing sample: ```python import jax jax.config.update(""jax_debug_nans"", True) jax.config.update(""jax_enable_x64"", True) from jax import numpy as jnp from jax import jit, vmap, grad import numpy as np def kernel(x, y):     return jnp.exp( jnp.linalg.norm(x  y)) def kernel_matrix(matr, kernel_func):     return vmap(lambda x: vmap(lambda y: kernel_func(x, y))(matr))(matr) ndim = 2 num = 4 rng = np.random.default_rng(65) xs = rng.standard_normal((num, ndim, 1)) w_true = jnp.array([[2.], [1.]]) rs = vmap(lambda x: jnp.dot(x.T, w_true))(xs) ys = rng.standard_normal((num, ndim, 1)) def appl_for_w(w):     fs = vmap(lambda x: jnp.dot(x.T, w))(xs)     residuals = (rs  fs).reshape((1, 1))     k_residuals = kernel_matrix(residuals, kernel)     k_ys = kernel_matrix(ys, kernel)     return jnp.trace(k_residuals @ k_ys) w_est = jnp.array([[1.3], [0.2]]) appl_for_w(w_est)  all good grad(appl_for_w)(w_est)  error :( ``` Running the above results in ``` FloatingPointError: invalid value (nan) encountered in jit(mul). Because jax_config.debug_nans.value and/or config.jax_debug_infs is set, the deoptimized function (i.e., the function as if the `jit` decorator were removed) was called in an attempt to get a more precise error message. However, the deoptimized function did not produce invalid values during its execution. This behavior can result from `jit` optimizations causing the invalid value to be produced. It may also arise from having nan/inf constants as outputs, like `jax.jit(lambda ...: jax.numpy.nan)(...)`. It may be possible to avoid the invalid value by removing the `jit` decorator, at the cost of losing optimizations. If you see this error, consider opening a bug report at https://github.com/google/jax. ``` I wasn't able to make the error go away by using `jax.disable_jit`. Running ```python with jax.disable_jit():     grad(appl_for_w)(w_est) ``` gives a warning ``` Invalid nan value encountered in the output of a C++jit/pmap function. Calling the deoptimized version. ``` and then again the same error as before.  System info (python version, jaxlib version, accelerator, etc.) I'm running the above on a CPU. `import jax; jax.print_environment_info()` gives ``` jax:    0.4.25 jaxlib: 0.4.25 numpy:  1.26.4 python: 3.11.8 (main, Feb 12 2024, 14:50:05) [GCC 13.2.1 20230801] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='splorge', release='6.8.2arch21', version=' CC(Python 3 compatibility issues) SMP PREEMPT_DYNAMIC Thu, 28 Mar 2024 17:06:35 +0000', machine='x86_64') ```",2024-04-01T16:37:50Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/20518,"I think it's likely due to taking the gradient of the norm when the argument is zero, see https://stackoverflow.com/questions/74864427/whydoesjaxgradlambdavjnplinalgnormvvjnpones2producenans The solution is usually to use the ""double where"" trick: https://jax.readthedocs.io/en/latest/faq.htmlgradientscontainnanwhereusingwhere","Good call :) It works if I replace `kernel` with the function below, using a ""double where"" trick. ```python def safe_norm(x):      trick following https://github.com/google/jax/issues/3058issuecomment628059073     is_zero = jnp.allclose(x, 0)     l = jnp.linalg.norm(jnp.where(is_zero, jnp.ones_like(x), x))     return jnp.where(is_zero, 0., l) def kernel(x, y):     return jnp.exp( safe_norm(x  y)) ```"
yi,[callback] Improve caching effectiveness in presence of callbacks.,"Previously, the userprovided Python callback function was first flattened and then the result passed as a primitive parameter to the callback primitives. This means that two separate io_callback invocations with the same Python callable will generate different Jaxprs. To prevent this we defer the flattening to lowering time. I discovered this problem by trying to ensure that io_callback passes the host_callback_test.py.",2024-04-01T09:31:54Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/20514
yi,Bazel not installed to executable path location by running python build/build.py --configure_only (as suggested in build instructions)," Description Hey, I'm  getting the tests up and running on a colab instance following the documentation presently published at https://jax.readthedocs.io/en/latest/developer.html It turns out that running ``` python build/build.py configure_only   ``` will output ```    _   _  __  __      / ___ \/  \  \___/_/   \/_/\_\ Bazel binary path: ./bazel6.1.2linuxx86_64 Bazel version: 6.1.2 Python binary path: /usr/bin/python3 Python version: 3.10 NumPy version: 1.25.2 Use clang: no ... ``` and download bazel into the build/ directory of the source distribution but not put it on the path, so if you are using a system that doesn't have bazel installed already the remainder of the tutorial will not quite work unless you use the full path to the bazel executable or add it to the path manually.  Presumably some explicit instructions to install bazel should be added at this location.  System info (python version, jaxlib version, accelerator, etc.) Well I am actually seeing some sort of weird colab.research.google.com hiccup where the Jax module is being loaded successfully for introspection by the Jupyter/colab notebook interface but is somehow not being made available to the underlying ipython. This is completely unrelated to bazel not getting installed to the executable path by following along with the build instructions so I am filing this issue anyways.  If anybody can point me at colab's GitHub I'll post this part over there.  AttributeError                            Traceback (most recent call last) [](https://localhost:8080/) in ()       1 import jax > 2 jax.print_environment_info() AttributeError: module 'jax' has no attribute 'print_environment_info' !image",2024-03-31T22:53:51Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/20512,"The documentation change to either mention this or provide a quick ""you need to install bazel with pip"" goes  https://github.com/google/jax/blob/main/docs/developer.md?plain=1L205 right here. I opened this issue by clicking through to here from the exact line source in GitHub, but as far as I can tell the actual line number information to make resolving it very easy disappears into the nether regions of the internet somewhere.  Presumably there's a place to open that as an issue on GitHub somewhere but I don't know where it is."
yi,`jax.lax.scan` function yield `ZeroDivisionError: integer division or modulo by zero` error when using unroll=0," Description Here is the how to recover the error: ```python import jax def fn(carry, x):   return carry, x carry = 3 xs = jnp.arange(5) jax.lax.scan(fn, carry, xs, unroll=0) ``` and here is the output error: ``` 20240328 14:50:41.870755: W external/xla/xla/service/gpu/nvptx_compiler.cc:673] The NVIDIA driver's CUDA version is 11.4 which is older than the ptxas CUDA version (11.8.89). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIAprovided CUDA forward compatibility packages. Traceback (most recent call last):   File ""/home/vn1747/Desktop/somai/experimental/ninjax/scan_error.py"", line 7, in      jax.lax.scan(fn, carry, xs, unroll=0) jax._src.source_info_util.JaxStackTraceBeforeTransformation: ZeroDivisionError: integer division or modulo by zero The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/.../.../.../.../.../.../scan_error.py"", line 7, in      jax.lax.scan(fn, carry, xs, unroll=0) ZeroDivisionError: integer division or modulo by zero ```  NOTE * Setting `unroll` to `1` works fine  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.19 jaxlib: 0.4.19 numpy:  1.24.3 python: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] jax.devices (1 total, 1 local): [cuda(id=0)] process_count: 1 $ nvidiasmi Thu Mar 28 14:46:00 2024        ++  ++ ```",2024-03-28T18:52:02Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/20481,Thanks for the find! Did you intend on some behavior with unroll=0? Or will a better error work here?,I assume that 0 is False in this case so that the jax scan is completely unrolled.,"From the docs: > **unroll** (int | bool) – optional positive int or bool specifying, in the underlying operation of the scan primitive, how many scan iterations to unroll within a single iteration of a loop. If an integer is provided, it determines how many unrolled loop iterations to run within a single rolled iteration of the loop. If a boolean is provided, it will determine if the loop is competely unrolled (i.e. unroll=True) or left completely unrolled (i.e. unroll=False). `0` is not `False` in this context, because the argument accepts both integer and boolean arguments with different documented behavior. If you want `False`, you should pass `False`. We should probably do more validation in the integer case, and raise a more informative error rather than assuming valid input.",This was fixed over in CC(Add more informative error message for invalid `unroll` parameter in `lax.scan`). Thanks for reporting!
yi,[xla] hlo_computation: compact instructions' vector on Cleanup(),"[xla] hlo_computation: compact instructions' vector on Cleanup() tl;dr: this gives a 1.26x compilation time speedup for a large, dense model in XLA:GPU. The largest perf leaf seen in profiles of a large, dense model is related to computing the post order. Surprisingly, it is not the DFS itself what's most expensive; rather, most of the time is spent on scanning through HloComputation::Instructions() to identify DFS roots. The reason this scan becomes expensive as instructions are removed is that the vector holding HloInstructionInfo (introduced in cl/600130708  https://github.com/openxla/xla/commit/247280ab727) is not shrunk as it flows through the pipeline, making us having to walk through many deleted ""tombstone"" entries. Here is the histogram of  of tombstones encountered during post order computations for this model: ``` [        1  1,536,345) ****************************** (1,300,248) [1,536,345  3,072,690)  (2) [3,072,690  4,609,034)  (364) [4,609,034  6,145,378)  (10,443) ``` To ameliorate this, this CL shrinks the vector periodically, so far only between passes. This is done by running compaction on the vector during HloComputation::Cleanup(), which is called after every pass. The cost of compaction is made proportional to the number of deleted entries by swappingif neededeach tombstone with the rightmost (within the vector) nondeleted entry. This brings the number of seen tombstones down significantly: ``` [        1    327,699) ****************************** (937,541) [  327,699    655,396)  (308) [  655,396    983,094)  (0) [  983,094  1,310,792)  (1) ``` Note: we could further improve compaction by calling Cleanup() from some passes, instead of just between passes. However, that would not yield a significant gain; at least for this model, scanning the instructions' vector now takes ~1% of total time (vs. ~17% before).",2024-03-28T15:04:51Z,,closed,0,0,https://github.com/jax-ml/jax/issues/20477
yi,"jax.config.update(""jax_enable_x64"", True) does not work for joblib."," Description Briefly, even I set jax.config.update(""jax_enable_x64"", True) ahead , joblib will work under jax.config.update(""jax_enable_x64"", False).   ``` from jax.config import config import jax import jax.numpy as jnp import joblib jax.config.update(""jax_enable_x64"", True)  key = jax.random.PRNGKey(0) subkeys = jax.random.split(key, num=100) N = 5 def norm_gen(key_id):     return(jax.random.normal(subkeys[key_id],shape = (N,))) norm_gen(0)  Array([2.1415603 , 1.38104703, 0.91724264, 0.14136964,  2.0047816 ],      dtype=float64) norm_gen(1)  Array([ 0.81512986, 0.17840127,  1.00610674,  1.57187942,  0.14101336],      dtype=float64) ``` Joblib gives the float32 results, indicating jax.config.update(""jax_enable_x64"", True) not working. ```  joblib numbers = range(2) results = joblib.Parallel(n_jobs=5)(joblib.delayed(norm_gen)(num) for num in (numbers)) results [Array([0.90704215, 1.548955  ,  0.79437876, 0.4385296 ,  1.5684816 ],      dtype=float32),  Array([0.47438177,  1.4778413 , 1.123923  ,  1.6738842 , 2.7033079 ],      dtype=float32)] ``` However, if I set jax.config.update(""jax_enable_x64"",False), then it gives the same results as to joblib. ``` jax.config.update(""jax_enable_x64"",False) norm_gen(0)  Array([0.90704215, 1.548955  ,  0.79437876, 0.4385296 ,  1.5684816 ],      dtype=float32) norm_gen(1)  Array([0.47438177,  1.4778413 , 1.123923  ,  1.6738842 , 2.7033079 ],      dtype=float32) ```  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.22 jaxlib: 0.4.22 numpy:  1.25.0 python: 3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 $ nvidiasmi Thu Mar 28 02:00:11 2024        ++  ++",2024-03-28T06:00:30Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/20473,"Well, this isn't really a bug in JAX: we didn't promise anything about `joblib`. That said, you can think of the `config` option like a global variable: it apparently isn't being set in the subprocesses `joblib` is using. Perhaps there's a way to do some extra initialization as part of the `joblib` subprocesses so you can set that config option? Or you could presumably set it as part of the function you are parallelizing, as the first thing you do. What do you think?","> Well, this isn't really a bug in JAX: we didn't promise anything about `joblib`. >  > That said, you can think of the `config` option like a global variable: it apparently isn't being set in the subprocesses `joblib` is using. Perhaps there's a way to do some extra initialization as part of the `joblib` subprocesses so you can set that config option? Or you could presumably set it as part of the function you are parallelizing, as the first thing you do. >  > What do you think? Yes, you are right. I just found that setting ``` def norm_gen(key_id):     jax.config.update(""jax_enable_x64"", True)      return(jax.random.normal(subkeys[key_id],shape = (N,))) ``` can solve this issue. Thanks for pointing out!"
yi,Error: 'apple_common' value has no field or method 'multi_arch_split'," Description Hello, I am getting following error while building jax with cuda support.   apple_common.multi_arch_split seems to have been deleted with bazel 7. Is there anyway to build with bazel 7 https://www.buildbuddy.io/blog/whatsnewinbazel70/ ``` ERROR: Traceback (most recent call last): 	File ""/var/tmp/pamacbuildbakamotokatas/pythonjaxlibcuda/src/bazel/ec1b63115889792c942ec63928e6e90a/external/build_bazel_rules_apple/apple/internal/rule_support.bzl"", line 221, column 36, in  		deps_cfg = apple_common.multi_arch_split, Error: 'apple_common' value has no field or method 'multi_arch_split' ERROR: error loading package '@//': at /var/tmp/pamacbuildbakamotokatas/pythonjaxlibcuda/src/bazel/ec1b63115889792c942ec63928e6e90a/external/com_github_grpc_grpc/bazel/grpc_build_system.bzl:28:6: at /var/tmp/pamacbuildbakamotokatas/pythonjaxlibcuda/src/bazel/ec1b63115889792c942ec63928e6e90a/external/build_bazel_rules_apple/apple/ios.bzl:33:5: at /var/tmp/pamacbuildbakamotokatas/pythonjaxlibcuda/src/bazel/ec1b63115889792c942ec63928e6e90a/external/build_bazel_rules_apple/apple/internal/ios_rules.bzl:71:5: initialization of module 'apple/internal/rule_support.bzl' failed ERROR: /var/tmp/pamacbuildbakamotokatas/pythonjaxlibcuda/src/bazel/ec1b63115889792c942ec63928e6e90a/external/tsl/tsl/BUILD:460:11: error loading package '@//': at /var/tmp/pamacbuildbakamotokatas/pythonjaxlibcuda/src/bazel/ec1b63115889792c942ec63928e6e90a/external/com_github_grpc_grpc/bazel/grpc_build_system.bzl:28:6: at /var/tmp/pamacbuildbakamotokatas/pythonjaxlibcuda/src/bazel/ec1b63115889792c942ec63928e6e90a/external/build_bazel_rules_apple/apple/ios.bzl:33:5: at /var/tmp/pamacbuildbakamotokatas/pythonjaxlibcuda/src/bazel/ec1b63115889792c942ec63928e6e90a/external/build_bazel_rules_apple/apple/internal/ios_rules.bzl:71:5: initialization of module 'apple/internal/rule_support.bzl' failed and referenced by '@//tsl:grpc++' ERROR: Analysis of target '//jaxlib/tools:build_wheel' failed; build aborted: Analysis failed INFO: Elapsed time: 64.652s, Critical Path: 0.06s INFO: 1 process: 1 internal. ERROR: Build did NOT complete successfully FAILED:  ERROR: Build failed. Not running target ==> ERROR: A failure occurred in build().     Aborting... ```  System info (python version, jaxlib version, accelerator, etc.) endeavouros Python 3.11.8 bazel 7.1.1 cuda 12.4 cudnn 8.9.7.29 nccl 2.19.4",2024-03-27T11:35:40Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/20453,"We don't yet support building with Bazel 7. So this is mostly working as intended. You must use a compatible bazel version (6.1.1 IIRC right now, see `.bazelversion`)."
yi,`jax.scipy.special.ndtri` edge case behavior," Description `jax.scipy.special.ndtri` returns positive infinity when the argument exceeds `1.0` and negative infinity when the argument is less than `0.0`. ```python3 from jax import numpy as jnp from jax.scipy.special import ndtri ndtri(jnp.asarray(1.))   Array(inf, dtype=float32) ndtri(jnp.asarray(2.))   Array(inf, dtype=float32) ``` It seems that it would be more appropriate to return NaN in these cases (as `scipy.special.ndtri`, `torch.special.ndtri`, etc. do).  This came up when adding support for JAX as an array API backend in SciPy. For more information, please see scipy/scipy CC([XLA:Python] Port several more modules to nanobind.).  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.23 jaxlib: 0.4.23 numpy:  1.25.2 python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] jax.devices (1 total, 1 local): [cuda(id=0)] process_count: 1 $ nvidiasmi Tue Mar 26 04:33:34 2024        ++  ++ ```",2024-03-26T04:42:29Z,bug,closed,0,0,https://github.com/jax-ml/jax/issues/20430
yi,`numpy.memmap.flush()` in `jax`,"Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example. I'm implementing Stochastic Variational Inference algorithm of my model, during which, to avoid OOM, I try to use `jnp.load('*.npy', mmap_mode='r+')` load a huge batch parameter array from disk. However, when I attempt to do `flush()` on the loaded array I got `'ArrayImpl' object has no attribute 'flush'`.  Does `flush()` not be implemented in `jax`?",2024-03-25T06:30:13Z,enhancement,closed,0,4,https://github.com/jax-ml/jax/issues/20418,"I found that using `numpy.memmap` load parameters then convert it to `jax.Array` seems feasible and efficient acceptable(My parameters are saved on a M.2 solid disk). After some update steps, I convert it back to `ndarray` and do `flush()`.","Yes. Certainly on CPU JAX can exchange buffers with NumPy zerocopy, so you can save an array by calling `np.asarray` on it and then using NumPy's facilities to do it. `jnp.load` and `jnp.save` are thin wrappers around the NumPy features. I'm not completely sure it makes sense for us to implement `flush` since our arrays are immutable.","> Yes. Certainly on CPU JAX can exchange buffers with NumPy zerocopy, so you can save an array by calling `np.asarray` on it and then using NumPy's facilities to do it. `jnp.load` and `jnp.save` are thin wrappers around the NumPy features. >  > I'm not completely sure it makes sense for us to implement `flush` since our arrays are immutable. Thanks for your reply. I mean is there some kind of `np.memmap` in `jax`?","> > Yes. Certainly on CPU JAX can exchange buffers with NumPy zerocopy, so you can save an array by calling `np.asarray` on it and then using NumPy's facilities to do it. `jnp.load` and `jnp.save` are thin wrappers around the NumPy features. > > I'm not completely sure it makes sense for us to implement `flush` since our arrays are immutable. >  > Thanks for your reply. I mean is there some kind of `np.memmap` in `jax`? I just finished the SVI algorithm and ran some experiments, in which I found there is no need to implement the socalled `jnp.memmap` because using `np.asarray` to convert parameters and then update the corresponding `np.memmap` object is efficient enough. It can save a lot of GPU memory while completing each step very fast. I think I can close this issue. Thank you!  "
yi,[Pallas] Static argument yields `ValueError: safe_zip() argument 2 is shorter than argument 1`," Description I am trying to write a very basic Pallas kernel with an additional static argument as input. However, I run into a ValueError: safe_zip() argument 2 is shorter than argument 1 whenever I actually use my static argument inside the dynamic slice. Here is the MWE: ```python from functools import partial import jax import jax.numpy as jnp from jax.experimental import pallas as pl def dia_kernel(x_ref, y_ref, z_ref, *, k):     z_ref[:] = jnp.zeros_like(y_ref)     pl.store(         z_ref,         (pl.ds(start=k, size=y_ref.shape[0]), slice(None)),         x_ref[:] * y_ref[:],     ) .jit def dia(x: jax.Array, y: jax.Array, k: int) > jax.Array:     return pl.pallas_call(         partial(dia_kernel, k=k),         out_shape=jax.ShapeDtypeStruct(y.shape, y.dtype),     )(x, y)  prepare inputs x = jnp.expand_dims(jnp.arange(4), axis=1) y = jnp.arange(1, 17).reshape(4, 4) k = 1  run res = dia(x, y, k) print(res)  [0 1 0 0] [ 1  2  3  4] = [ 5  6  7  8]  [0 0 2 0] [ 5  6  7  8] = [18 20 22 24]  [0 0 0 3] [ 9 10 11 12] = [39 42 45 48]  [0 0 0 0] [13 14 15 16] = [ 0  0  0  0] ``` Changing line 12 from ```python (pl.ds(start=k, size=y_ref.shape[0]), slice(None)), ``` to ```python (pl.ds(start=1, size=y_ref.shape[0]), slice(None)), ``` works as intended.  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.25 jaxlib: 0.4.25 numpy:  1.26.4 python: 3.11.8 (main, Feb 25 2024, 16:01:26) [GCC 12.2.0] jax.devices (1 total, 1 local): [cuda(id=0)] process_count: 1 platform: uname_result(system='Linux', node='morbier', release='6.1.013amd64', version=' CC(Python 3 compatibility issues) SMP PREEMPT_DYNAMIC Debian 6.1.551 (20230929)', machine='x86_64') $ nvidiasmi Sat Mar 23 10:05:38 2024        ++  ++ ```",2024-03-23T09:06:11Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/20406,Was a simple mistake with the `jax.jit`.
dspy,jax.lax.scan tries to access empty array," Description Apologies if I'm just very confused. When `jax.lax.scan(f, init, xs)` is fed an empty array as `xs` and `init`, it will still start the loop, and try to apply the scanning function`f` to the empty carry `init` and nonexistent `xs`. Since the carry itself is also empty, this will result in a crash. ``` import jax import jax.numpy as jnp def foo(carry, j):     c=carry.at[j].set(j)     return c, None myarray = jnp.array([])                                                                                                                      carry, _ = jax.lax.scan( foo, myarray, jnp.arange(myarray.shape[0]) ) print(carry) ``` Result: ``` Traceback (most recent call last):   File ""/home/thomas/craftax/Craftax/test.py"", line 10, in      carry, _ = jax.lax.scan( foo, myarray, jnp.arange(myarray.shape[0]) )   File ""/home/thomas/craftax/Craftax/test.py"", line 5, in foo     c=carry.at[j].set(j)   File ""/home/thomas/craftax/craftax/lib/python3.10/sitepackages/jax/_src/numpy/array_methods.py"", line 490, in set     return scatter._scatter_update(self.array, self.index, values, lax.scatter,   File ""/home/thomas/craftax/craftax/lib/python3.10/sitepackages/jax/_src/ops/scatter.py"", line 80, in _scatter_update     return _scatter_impl(x, y, scatter_op, treedef, static_idx, dynamic_idx,   File ""/home/thomas/craftax/craftax/lib/python3.10/sitepackages/jax/_src/ops/scatter.py"", line 104, in _scatter_impl     indexer = jnp._index_to_gather(jnp.shape(x), idx,   File ""/home/thomas/craftax/craftax/lib/python3.10/sitepackages/jax/_src/numpy/lax_numpy.py"", line 4817, in _index_to_gather     raise IndexError(f""index is out of bounds for axis {x_axis} with size 0"") IndexError: index is out of bounds for axis 0 with size 0 ``` The expected behavior is that scanning on an empty array `xs` should do nothing at all. It should not start the loop, should not try to apply the function, and should return the unmodified `init` as carry.  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.25 jaxlib: 0.4.25 numpy:  1.26.4 python: 3.10.13  ++ ```",2024-03-22T19:13:46Z,better_errors,closed,0,11,https://github.com/jax-ml/jax/issues/20399,What would you expect from the code snippet?,"Since `myarray` is empty in this case, I would expect it to print `[]`, i.e. the final carry should be identical to `init` (because the scan call would do nothing). Most importantly I would expect it not to cause an error by trying to start a loop that should not be started. If the array is empty, I would expect it to behave exactly like `for _ in range(0)`. Note that if `myarray` is any nonempty array, then the snippet currently does the right thing, i.e. print `[0, 1, 2...]` with the same length as `myarray`.",Thanks for raising this and fo the thoughtful analysis! I agree.,"Ah, I just looked more carefully at the code in the OP, and I see the issue: we need to produce some scannedover output, and to figure out its type we need to be able to stage out the body function. But we can't stage out this body function because there's statically an outofbounds error, namely we know statically that `carry.at[j].set(j)` must fail. (Indeed we can see from reading the Python source that the scannedover output is just `None`, but JAX doesn't read source code; it just gets an opaque Python callable, and can do nothing but run it...) I think this is working as intended, and there's not much else we can do here other than raise a better error message.","I think the best thing to do is to recommend that you write the original code with a fori_loop, since you aren't using the scannedover inputs or outputs much: ```python import jax import jax.numpy as jnp def foo(j, carry):   return carry.at[j].set(j) myarray = jnp.array([]) carry = jax.lax.fori_loop(0, 0, foo, myarray) print(carry) ``` That would've also crashed from running the body, but CC(if fori_loop statically takes zero trips, don't run body) makes it not run anymore. I realize that your example may have been a distilled toy example, and perhaps in your real one you do want to use scannedover inputs, or something. But still I think this is our best option, for the reasons mentioned above.","FWIW, the reason for this post is Craftax, a 2D Minecraftlike game implemented entirely in Jax. It has a lot of passages that use `jax.lax.scan` to loop over all enemies / mobs / plants, etc., like this: ``` def _move_zombies(rng_and_state, zombie_index):     random_move_proposed_position = (         zombies.position[zombie_index] + random_move_direction     )     ... rng, _rng = jax.random.split(rng) (rng, new_state), _ = jax.lax.scan(     _move_zombie, (rng, state), jnp.arange(static_params.max_zombies) ) ``` If you try to set `max_zombies` to 0, this crashes (because `zombies.position` is an empty array but it still tries to access it). I've managed to make it work with a lot of `if` statements.  If this behaviour is not going to change, maybe it should be specified in the documentation that `scan` should not be applied to empty arrays, at least if the function accesses them?","How is checking if `length == 0` in a `fori_loop` different than checking if `len(xs)==0` in a `scan`? Is it the fact that a compiled `scan` does not recompile for different lengths of `xs`?  Sorry if it is a stupid question, but I was just wondering..."," super cool project! > How is checking if length == 0 in a fori_loop different than checking if len(xs)==0 in a scan? Is it the fact that a compiled scan does not recompile for different lengths of xs?  `scan` has to produce empty versions of the scannedover outputs, whereas a zerotripcount `fori_loop` just needs to return the initial carry you provided. That is, if you write `carry, ys = scan(f, carry, xs)` and `xs` have length 0, we still need to produce something for `ys`. To know what to produce, we need to know the output type of `f`: for example, if we had `f = lambda c, x: (c, x)` then we need `ys` to be an empty array that looks just like `xs`, but if we had `f = lambda c, x: (c, (x, x))` then we need `ys` to be a pair like `(xs, xs)`, or if we had `f = lambda c, x: (c, jnp.zeros(3, 'int32'))` then we need `ys` to be something like `jnp.zeros((0, 3), 'int32')`. (In general, 'the output type of `f`' means knowing its pytree structure and the shapes/dtypes of its leaves.) But `scan` is just given `f`, and has to infer its output type. The only thing we can do (robustly) with a Python callable is run it, so we run it and build a jaxpr for `f`, in the process discovering its output type. But what if `f` can't be run, e.g. ```python def f(c, x):   assert False   ... ``` Then we can't infer its output type! That's the issue here: we have a loop body that can never run, for _any_ length of `xs` (basically like the `assert False` example), because the carry is (statically) empty and can never be indexed. If we didn't have an empty carry, but still had length0 xs, everything would've worked fine: ```python def foo(carry, j):   c=carry.at[j].set(j)   return c, None myarray = jnp.array([1])   made carry not have zero length carry, _ = jax.lax.scan( foo, myarray, jnp.arange(0))   changed jnp.arange argument to keep zero length print(carry) ``` > If this behaviour is not going to change, maybe it should be specified in the documentation that scan should not be applied to empty arrays, at least if the function accesses them?   Well, the issue isn't exactly empty arrays or accessing them, as the example above shows; it's that the scan body can't always (i.e. statically) error, given the shapes of the arguments passed to scan, because we run the body for output type inference. That's the issue in the body function in the OP, but we'd have the same issue with a body function that just has an `assert False` at the top (and has nothing to do with empty arrays). We could indeed document that the body has to be runnable without error, if it's not already documented. > It has a lot of passages that use jax.lax.scan to loop over all enemies / mobs / plants, etc., like this: ```python def _move_zombies(rng_and_state, zombie_index):     random_move_proposed_position = (         zombies.position[zombie_index] + random_move_direction     )     ... rng, _rng = jax.random.split(rng) (rng, new_state), _ = jax.lax.scan(     _move_zombie, (rng, state), jnp.arange(static_params.max_zombies) ) ``` For code that fits this pattern, I suggest `fori_loop` instead of scanning over a `jnp.arange`. That is, wherever you have ```python carry, _ = scan(f, carry, jnp.arange(N))   e.g. f = _move_zombie, carry = (rng, state) ``` where you ignore the scannedover output anyway, you can instead write ```python carry = fori_loop(0, N, lambda i, carry: f(carry, i), carry) ``` and you'll get the behavior you want.  Another option we have is we could add an optional `out_type` argument to `scan`: if you provide the output type of the body, then we won't have to run it to infer the output type. Then you'd also get the behavior you want, i.e. you can write ```python carry, ys = scan(lambda c, x: assert_false(), carry, xs, out_type=...) ``` and when xs has length 0 that would not error. But if your examples all fit the `fori_loop` pattern, it seems reasonable just to use that... WDYT?","Thanks for the explanation ! And just to be sure, if during compilation you where to check `len(xs)` and just set `ys` to `None` when it is 0, that would not work because that would create a representation of `f` that has different output pytrees depending on `xs`?  which is a no go  since e.g. `ys` shape does depend on `xs` length, but not its pytree structure which is found out by running `f`. Is that correct? Does recompilation happen when changing the length of `xs`? Thank you for your work and feel free to ignore this! :) ","Late comment, but this has come up several times in the past and we've always answered that it's working as intended; see e.g. https://github.com/google/jax/issues/3285 and in particular https://github.com/google/jax/issues/3285issuecomment638120124 We also have places in the code where we account for this fact; for example here https://github.com/google/jax/blob/81c3d51deff4e947f9c708ab4191540711f4f80c/jax/_src/lax/linalg.pyL1213L1214","> And just to be sure, if during compilation you where to check len(xs) and just set ys to None when it is 0, that would not work because that would create a representation of f that has different output pytrees depending on xs? That's right. > Does recompilation happen when changing the length of xs? Yes, any given program we generate and compile is for fixed shapes. I think we resolved things here well enough to close this issue. Please correct me if I'm mistaken! Thanks, all."
yi,Propagate sharding of inputs to full_like that are capable of carrying sharding as an attribute.,Propagate sharding of inputs to full_like that are capable of carrying sharding as an attribute. Fixes https://github.com/google/jax/issues/20390,2024-03-22T15:47:38Z,,closed,0,0,https://github.com/jax-ml/jax/issues/20394
yi,Deprecate jax.experimental.host_callback in favor of JAX external callbacks,"We have marked the host_callback APIs deprecated on March 21, 2024 (JAX version 0.4.26). They will be removed in October 2024. Users should use instead the new JAX external callbacks.  Quick temporary migration As of October 1st, 2024 (JAX version 0.4.34) if you use the `jax.experimental.host_callback` APIs they will be implemented in terms of jax.experimental.io_callback. This is controlled by the configuration variable `jax_host_callback_legacy=False` (or the environment variable `JAX_HOST_CALLBACK_LEGACY=False`. For a very limited time, you can obtain the old behavior by setting the configuration variable to `True`. Very soon this configuration flag will be removed, so it is best to take the time to do the migration as explained below.  Real migration It is best to study the different flavors of JAX external callbacks to pick the right one for your use case. In general `io_callback(ordered=True)` will have more similar support to the existing `host_callback`. In general, you should replace calls to `id_tap` and `call` with `io_callback`, except when you need these calls to work under `vmap`, `grad`, `jvp`, `scan`, or `cond`, in which case you should use `jax.debug.callback`. Note that `jax.debug.callback` does not support returning values from the callback, so it can be used only in lieu of `.id_print` or `host_callback.id_tap` or in lieu of `host_callback.call` when the `result_shape=None`.  Known migration issues   * the `tap_with_device` option for `id_tap` and the `call_with_device` option for `call` are not supported. You must change the callbacks to not need the `device` argument. If you use `JAX_HOST_CALLBACK_LEGACY=False` you will get an error.   * the `transforms` argument to the callback called from `id_tap` is not supported. If you use `JAX_HOST_CALLBACK_LEGACY=False` the callback will be passed the empty tuple (no transforms).   * the old `host_callback` APIs passed `np.ndarray` objects to the callback. The new JAX external callbacks pass `jax.Array`. This should be Ok, except that it may lead to a deadlock if the code making the call is already running on CPU, because the callback will try to invoke JAX functions on the arguments and will find the device busy. The solution is to add `input = np.array(input)` at the start of your callback function.   * If you attempt to use `io_callback(ordered=True)` with `jax.grad`, you will get an error that `io_callback` does not support JVP. Try to use `debug_callback`.   * If you attempt to use `io_callback(ordered=True)` with `jax.pmap` you will get an error that ordered effects are not supported under `jax.pmap`. Try to use `ordered=True`.  Using `io_callback` in place of `host_callback.call` For example, ``` from jax.experimental import host_callback res = host_callback.call(fn, arg, result_shape=result_shape_dtypes) ``` should be replaced with ``` from jax.experimental import io_callback res = io_callback(fn, result_shape_dtypes, arg) ```  Using `io_callback` in place of `host_callback.id_tap` Similarly, `id_tap` can be replaced with a `io_callback` with `result_shape_dtypes=None`: ```  callback = lambda x, transforms: do_something(x)  res = host_callback.id_tap(callback, x_in) ``` should be replaced with ```   callback = lambda x: do_something(x)   io_callback(callback, None, x_in)   res = x_in   Simulates the return value of `id_tap` ``` Note that we have removed the `transforms` callback argument (this is not supported by the new callbacks). If you use the `result` parameter with `id_tap` then you can replace: ``` results = id_tap(     lambda arg, transform: done_callback(arg),     arg,     result=the_results, ) ``` with  ``` io_callback(     lambda arg: done_callback(arg),     None,     arg ) results = the_results ```  Using `jax.debug.print` in place of `host_callback.id_print` For `id_print` you should use instead `jax.debug.print`. E.g., `id_print(x)` can be replaced by `debug.print('{}', x)`. If you use the `name` parameter, you can replace `id_print(x, name=""my_x"")` with `jax.debug_print('name: my_x\n{}', x)`. If you use the `output_stream` parameter, you can replace: `id_print(x, output_stream=s)` by `jax.experimental.io_callback(lambda x: s.write(str(x)), None, x)`.  Using `jax.effects_barrier` in place of `host_callback.barrier_wait` Finally, `host_callback.barrier_wait` should be replaced with `jax.effects_barrier()`.  Callbacks and `jax.vmap` Under `vmap` the new callbacks behave differently than the host_callback. The latter will make a single call with a vector value, while the new callbacks will behave like a loop, and will make separate calls for each element in the vmap. For example, the code ``` def host_fn(x):   print(x) def fn(x):   res = 2 * x   id_tap(host_fn, res)   return res jax.vmap(fn)(np.arange(3)) ``` makes one call to `host_fn` with the vector [0, 2, 4], and if we replace `id_tap(host_fn, res)` with `jax.debug.callback(host_fn, res)` we will get 3 separate calls with `0`, `2`, and `4`, respectively.",2024-03-22T06:28:08Z,enhancement,open,0,2,https://github.com/jax-ml/jax/issues/20385,"Could you give or reference an example of io_callback `result_shape_dtypes`?  res = io_callback(fn, result_shape_dtypes, arg). I just can't manage to get it properly. ","There's an example in the docs here: https://jax.readthedocs.io/en/latest/notebooks/external_callbacks.htmlexploringjaxexperimentaliocallback In this case, since the function returns a value that is the same shape and dtype as the input, we pass the input to `result_shape_dtypes`."
yi,"Unexplained all-gather in batched array shuffling, crashes with shardmap"," Description I have an array, which is fully sharded along the first dimension. I'm trying to shuffle each row of the array using the code below. From what I understand, it should be an embarrassingly parallel operation, but XLA decides to insert an allgather in the end of it for some reason that I cannot understand. I also observe the same behavior if I replace `jax.random.permutation` with `jax.lax.top_k` Two questions: * What is this allgather doing there? * Is there a way to achieve what I want without any communications? ```python import jax import jax.numpy as jnp import numpy as np def func(rng, inputs):     batch_size, _, _ = inputs.shape     def _f(rng, input):         return jax.random.permutation(rng, input)     rng = jax.random.split(rng, batch_size)     return jax.vmap(_f)(rng, inputs) def main():     mesh = jax.sharding.Mesh(np.array(jax.devices()), ('dp'))     rng_sharding = jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec())     input_sharding = jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec('dp', None, None))     f = jax.jit(         func,         in_shardings=(rng_sharding, input_sharding),         out_shardings=(input_sharding),     )     inputs = jnp.zeros((24, 4096, 16))     inputs = jax.device_put(inputs, input_sharding)     rng = jax.random.PRNGKey(0)     rng = jax.device_put(rng, rng_sharding)     f = f.lower(rng, inputs).compile()     text = f.as_text()     print(text)     if ""allgatherstart"" in text:         print(""All gather found"")     else:         print(""All gather not found"") if __name__ == '__main__':     main() ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.25 jaxlib: 0.4.25 numpy:  1.24.3 python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] jax.devices (8 total, 8 local): [cuda(id=0) cuda(id=1) ... cuda(id=6) cuda(id=7)] process_count: 1 platform: uname_result(system='Linux', node='hr0nixdev8gpu', release='5.4.0155generic', version=' CC(MNIST example runtime crash)Ubuntu SMP Fri Jul 7 16:10:02 UTC 2023', machine='x86_64') ```",2024-03-22T00:43:35Z,bug,open,0,2,https://github.com/jax-ml/jax/issues/20381,"Ok, I was able to work around it with shard_map: ```python def main():     mesh = jax.sharding.Mesh(np.array(jax.devices()), ('dp'))     rng_pspec = jax.sharding.PartitionSpec()     rng_sharding = jax.sharding.NamedSharding(mesh, rng_pspec)     input_pspec = jax.sharding.PartitionSpec('dp', None, None)     input_sharding = jax.sharding.NamedSharding(mesh, input_pspec)     f = shard_map(         func,         mesh=mesh,         in_specs=(rng_pspec, input_pspec),         out_specs=input_pspec     )     f = jax.jit(         f,         in_shardings=(rng_sharding, input_sharding),         out_shardings=input_sharding,     )     inputs = jnp.zeros((24, 4096, 16))     inputs = jax.device_put(inputs, input_sharding)     rng = jax.random.PRNGKey(0)     rng = jax.device_put(rng, rng_sharding)     f = f.lower(rng, inputs).compile()     text = f.as_text()     print(text)     if ""allgatherstart"" in text:         print(""All gather found"")     else:         print(""All gather not found"")     print(f(rng, inputs).shape) ```","Ok, here is a fun fact: if I replace `jax.random.permutation` with `jax.lax.top_k` and leave `shard_map`, this code crashes with `Floating point exception` from native code (presumably XLA). Without shardmap it works fine except for undesired allgather."
gemma,Difference in output between jitted and non-jitted call," Description Originally I posted this to the Flax repo(https://github.com/google/flax/issues/3777), but I think this is jax issue now. I have found that the results of the forward pass, differ considerably if the **apply** function is jitted or not.  System information  Ubuntu 23.10 x86_64  Flax, jax, jaxlib versions (obtain with `pip show flax jax jaxlib`: flax 0.8.2, jax 0.4.25, jaxlib 0.4.25+cuda12.cudnn89  Python version: v3.10.13  GPU/TPU model and memory:  NVIDIA GeForce RTX 3090  CUDA version (if applicable): 12.4  Problem you have encountered: When using the forward pass for a simple MLP the results are different with the jitted version.  What you expected to happen: That the results are the same.  Logs, error messages, etc:  Steps to reproduce: ```python import flax.linen as nn import jax.numpy as jnp import jax from typing import Sequence class MLP(nn.Module):     hidden_channels: Sequence[int]     activation: nn.Module = nn.relu     .compact     def __call__(self, x: jnp.ndarray):         for i, channels in enumerate(self.hidden_channels):             x = nn.Dense(features=channels)(x)             if i != len(self.hidden_channels)  1:                 x = self.activation(x)         return x d_hidden = 64 d_input = 64 d_batch = 3 proj = MLP(     hidden_channels=[d_hidden] * 2,     activation=nn.selu, ) proj_vars = proj.init(jax.random.PRNGKey(546543), jnp.ones((d_batch, d_input))) x = jnp.ones((d_batch, d_input))  out = proj.apply(proj_vars, x) jitted_out = jax.jit(proj.apply)(proj_vars, x) diff = jnp.abs(out  jitted_out).mean() print(f""diff={diff}"") ``` this gives an error of 0.0005452320910990238 in my computer.  I found out that from `jaxlib==0.4.21` to `jaxlib==0.4.25` I get this error but with `jaxlib==0.4.20` there is no error. Interestingly if I use  'xla_gpu_triton_gemm_any=True ' there is no error as well.  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.25 jaxlib: 0.4.25 numpy:  1.26.4 python: 3.10.13  (main, Dec 23 2023, 15:36:39) [GCC 12.3.0] jax.devices (1 total, 1 local): [cuda(id=0)] process_count: 1 platform: uname_result(system='Linux', node='home', release='6.5.026generic', version=' CC(fix symbolic zero handling in concat transpose)Ubuntu SMP PREEMPT_DYNAMIC Tue Mar  5 21:19:28 UTC 2024', machine='x86_64') ```",2024-03-21T18:17:01Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/20371,"Thanks for the question! I think this is working as intended: especially on GPU, floating point numerics may not be exactly preserved under compilation. Indeed, floating point numerics may not even be preserved runtorun on GPU, unless you set some flags that significantly degrade performance! See https://github.com/google/jax/discussions/10674discussioncomment7214817 about nondeterminism. I'm not sure exactly why that flag would affect things, other than it's just changing the generated code. > differ considerably 0.0005452320910990238 does not seem surprising to me for f32, but if it's too much for your use case, maybe try a higher floating point precision (since presumably the compilerinduced numerical noise scales down with increased precision)? Or it may be that one can identify XLA:GPU flags that are best for ensuring jit invariance, though I'm not sure if those will be stable over time... WDYT?","Thanks for the prompt answer! I think that makes sense, but what I don't understand is why I get a different behaviour with `jaxlib==0.4.20` vs `jaxlib==0.4.21+`  or ~why the 'xla_gpu_triton_gemm_any=True ' seems to fix the problem as well~ (maybe related to the deterministic flag, however xla_gpu_deterministic_ops=true does not seem to fix the problem). Also if I run this in a collab notebook with 0.4.23 then I get no numerical error", is it possible that devicespecific behavior for `default_matmul_precision` could account for the difference across GPU types?," good point! Yeah I'm not sure, but different GPU types would also affect codegen in other ways.",Hi  The repro now produces a difference of 0.0 between jitted and nonjitted call when run on NVIDIA GeForce RTX 2060 in WSL with JAX 0.4.37. Please find the attached screenshots for reference. !image jax.print_environment_info(): !image Could you please verify if the issue still persists in your system and let us know. Thank you.,"Hi,  With versions >=0.4.37, the issue seems to be fixed. Thanks!"," Hi, with version 0.4.38 (also 0.5.0) there is still a difference `diff=0.0005837870994582772` on my system. Either `xla_gpu_triton_gemm_any=True` or `jax.config.update('jax_enable_x64', True)` resolves it, while `jax.default_matmul_precision('float32')` doesn't seem to work. It can be detrimental for scicomp when the error accumulates in a complex module. Strangely enough, `jax.config.update('jax_enable_x64', True)` seems to improve the inference speed while decreasing the training speed. System info: ``` jax:    0.4.38 jaxlib: 0.4.38 numpy:  2.2.1 python: 3.10.16  ++++ ```"
yi,"Use XLA operators instead of custom threefry2x32 kernel to generate random number, due to high memory costs of threefry2x32 kernel."," Description It seems that Jax's default random number generation algorithm threefry2x32 consumes large amout of memory. From search results, the requirement of threefry2x32 is: To generate random values using threefry2x32 directly, you need to provide a counter array and a PRNG key. The counter array determines the shape of the output, while the PRNG key determines the specific random values generated The counter and key are arrays of unsigned 32bit integers. For threefry2x32, both the counter and key have 2 elements This means to generate a random tensor, custom kernel threefry2x32 requires 2X memory size of the output tensor. This will result in big issue about memory consumption, especially for embedding tensor, as embedding tensor is already very large, 2X memory consumption is very likely to cause OOM. An example of HLO dumps is attached.  module_0001.jit_init.sm_9.0_gpu_after_optimizations.txt Another solution is  to comment out this line of code in JAX: https://github.com/google/jax/blob/b6e985ffe7af6054a289613eb70ce31c6360cd76/jax/_src/prng.pyL1045 If you comment it out, JAX will use an HLOonly implementation of the prng, with no CUDA. The only reason the custom kernel exists is because of compile time, and I'm not even sure it's needed any more. The HLOonly implementation can be fused with producers and consumers, so XLA may do something better in that case.  For our reproduce code, the initialization module’s memory consumption reduced from 41GB to 19.9GB.  System info (python version, jaxlib version, accelerator, etc.) import jax import flax.linen as nn import numpy as np from jax.experimental import mesh_utils from jax.sharding import Mesh, NamedSharding, PartitionSpec import os from jax.experimental import pjit emb_dim = 12288 vocab_size = 256000 batch_size = 1 max_seq_len = 8192 class Model(nn.Module):     emb_dim: int     vocab_size: int     .compact     def __call__(self, x: jax.Array):         layer = nn.Embed(num_embeddings=self.vocab_size, features=self.emb_dim, name=""input_embed"")         x = layer(x)         x = nn.LayerNorm(use_bias=False, name=""layernorm"")(x)         x = layer.attend(x)         return x def gen_inputs():     return np.random.randint(0, high=vocab_size  1, size=(batch_size, max_seq_len)) devices = mesh_utils.create_hybrid_device_mesh((1,1), (1,1))  (8,1) also works MESH_DATA_AXIS = 'data' MESH_TENSOR_AXIS = 'tensor' mesh = Mesh(devices, axis_names=(MESH_DATA_AXIS, MESH_TENSOR_AXIS)) def P(*specs):     return NamedSharding(mesh, PartitionSpec(*specs)) dummy_inputs = jax.jit(gen_inputs, out_shardings=P(MESH_DATA_AXIS, None))() model = Model(emb_dim, vocab_size) def get_shardings(param):     if param.shape == (vocab_size, emb_dim):         return P(None, MESH_TENSOR_AXIS)     else:         return P(None) def init(inputs):     key = jax.random.key(0)     return model.init(key, inputs) def forward(inputs, params):     return model.apply(params, inputs) abstract_vars = jax.eval_shape(init, dummy_inputs) shardings = jax.tree_util.tree_map(lambda x: get_shardings(x), abstract_vars) weights = jax.jit(init, out_shardings=shardings)(dummy_inputs) for i in range(11):     import ctypes     libcudart = ctypes.cdll.LoadLibrary('libcudart.so')     if i == 8 and (jax.process_index() == 0):         libcudart.cudaProfilerStart()     outputs = jax.jit(forward, out_shardings=P(MESH_DATA_AXIS, None, None))(dummy_inputs, weights)     print(outputs.shape) if (jax.process_index() == 0):     libcudart.cudaProfilerStop()",2024-03-20T01:54:33Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/20328,Seems to be fixed by https://github.com/google/jax/commit/3f9540761e092772860cf7ca33a3cdca9ad40eb5.  Please reopen this issue if it's not.,"Yes, this should be covered by https://github.com/google/jax/commit/69878c4924f62a7b73d25c7721e5f07a75a96c18 and https://github.com/google/jax/commit/3f9540761e092772860cf7ca33a3cdca9ad40eb5 together. Thanks for noticing and closing."
yi,jax.pure_callback and jax.experimental.io_callback now use jax.Arrays,"jax.pure_callback and jax.experimental.io_callback now use jax.Arrays The motivation for this change is twofold * JAX APIs should use jax.Arrays. * Using jax.Arrays potentially allows keeping the data on device, instead   of always copying it to the host. Note that the version here still always   copies to the host. If this change breaks you, you can recover the old behavior by changing     jax.pure_callback(         f,         result_shape_dtypes,         *args,         **kwargs,     ) to     jax.pure_callback(         lambda *args: f(*jax.tree.map(np.asarray, args)),         result_shape_dtypes,         *args,         **kwargs,     ) so that the callback function is called with NumPy arrays as before. I will update the ""External callbacks"" tutorial in a follow up.",2024-03-19T20:24:00Z,,closed,0,0,https://github.com/jax-ml/jax/issues/20325
yi,jaxlib: Add `ifrt_proxy.pyi` to `build_wheel.py`.,jaxlib: Add `ifrt_proxy.pyi` to `build_wheel.py`.,2024-03-19T19:04:12Z,,closed,0,1,https://github.com/jax-ml/jax/issues/20324,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
yi,doctest: avoid modifying global flag state,This was causing flaky tests due to changing order of execution.,2024-03-19T18:28:05Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/20323
yi,[key reuse] add internal function_type_signature utility,"I've found I often recreate this utility for local testing; it won't be public, but it's useful to have it around.",2024-03-19T17:47:18Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/20320
yi,"jax-metal: Memory leak on jit boundary, MPS"," Description Simply calling a  function with any kind of array input leaks memory in an noninsignificant way. CPU is okay, but device (MPS) exhibits the leak. Sample program: ```python import os import psutil import jax import jax.numpy as jnp  uncomment to test on CPU:  jax.config.update('jax_platform_name', 'cpu') key = jax.random.PRNGKey(42) array = jax.random.uniform(key, shape=(39325, 173), dtype=jnp.float32) .jit def jax_func(arr):     pass ps = psutil.Process(os.getpid()) for i in range(1000000000):     if i % 1000000 == 0:         print(f'{ps.memory_info().rss:,}')     jax_func(array) ``` Output on CPU: ``` 241,319,936 243,138,560 243,138,560 243,138,560 243,138,560 243,138,560 243,122,176 243,122,176 243,122,176 243,122,176 243,122,176 243,122,176 243,122,176 243,122,176 243,122,176 243,122,176 243,122,176 ``` Output on MPS: ``` 795,508,736 857,686,016 890,961,920 924,090,368 956,465,152 988,774,400 1,021,132,800 1,053,589,504 1,085,865,984 1,118,224,384 1,150,517,248 1,182,744,576 1,214,988,288 1,247,281,152 1,279,557,632 1,311,801,344 1,344,045,056 1,376,305,152 ```  System info (python version, jaxlib version, accelerator, etc.) Platform 'METAL' is experimental and not all JAX functionality may be correctly supported! 20240318 18:17:36.186791: W pjrt_plugin/src/mps_client.cc:563] WARNING: JAX Apple GPU support is experimental and not all JAX functionality is correctly supported! Metal device set to: Apple M3 Max systemMemory: 128.00 GB maxCacheSize: 48.00 GB jax:    0.4.25 jaxlib: 0.4.23 numpy:  1.26.4 python: 3.12.2  (main, Feb 27 2024, 12:57:28) [Clang 14.0.6 ] jax.devices (1 total, 1 local): [METAL(id=0)] process_count: 1 platform: uname_result(system='Darwin', node='JurajsMacBookPro.local', release='23.4.0', version='Darwin Kernel Version 23.4.0: Wed Feb 21 21:44:54 PST 2024; root:xnu10063.101.15~2/RELEASE_ARM64_T6031', machine='arm64') jaxmetal                 0.0.6",2024-03-18T16:18:58Z,bug Apple GPU (Metal) plugin,open,0,7,https://github.com/jax-ml/jax/issues/20296,"Can't even try with jaxmetal version 0.0.4 as recommented here: https://developer.apple.com/metal/jax/ Because pip says: > jaxmetal 0.0.4 depends on jaxlib==0.4.11 and > ERROR: Could not find a version that satisfies the requirement jaxlib==0.4.11 (from versions: 0.4.17, 0.4.18, 0.4.19, 0.4.20, 0.4.21, 0.4.22, 0.4.23, 0.4.24, 0.4.25) also not possible to go for the oldest available jaxlib 0.4.17: > The conflict is caused by: >   The user requested jaxlib==0.4.17 >    jaxmetal 0.0.4 depends on jaxlib==0.4.11 So we're in a bit of a pickle here as any longer/heavier training on the MPS will slow down dramatically as it swaps gigabytes of allocated (leaked) memory.","jaxmetal==0.0.5 with (its pip dependency) jaxlib==0.4.20 (which pulls jax0.4.20), on MLIR 1.0.0 (Sonoma 14.2.1)  also does exhibit the leak. ``` Metal device set to: Apple M1 Max systemMemory: 64.00 GB maxCacheSize: 24.00 GB 796,737,536 841,973,760 879,001,600 913,145,856 946,585,600 978,878,464 ```","Hi guys, would there be any outlook on this? It's a show stopper for me, and if nobody is free to have a look, maybe I could. What is the chance this is a bug in jaxmetal? (I don't think Apple's jaxmetal sources are public.) Last question  should this be raised in https://developer.apple.com/forums/tags/tensorflowmetal? (This page points to it.)",This is reproducible and we will take a look. ,"For anyone else having the same issue and being blocked on MPS due to it, have a look at the MLX framework which is similar to JAX and tailored specifically to Apple Silicon.", it is similar but also misses so much. have you found a good solution to MLX profiling?,"I am also experiencing the same issue. My program starts with 12 GB of memory usage, but after a few epochs, it increases to around 90 GB. At this point, GPU usage drops to 0%, and my code stops training altogether. Any updates or potential workarounds for this problem would be greatly appreciated. Platform 'METAL' is experimental and not all JAX functionality may be correctly supported! Metal device set to: Apple M3 Pro systemMemory: 18.00 GB maxCacheSize: 6.00 GB jax:    0.4.26 jaxlib: 0.4.26 numpy:  1.26.0 python: 3.11.7 (v3.11.7:fa7a6f2303, Dec  4 2023, 15:22:56) [Clang 13.0.0 (clang1300.0.29.30)] jax.devices (1 total, 1 local): [METAL(id=0)] process_count: 1 platform: uname_result(system='Darwin', node='AtasMacBookPro.fritz.box', release='24.0.0', version='Darwin Kernel Version 24.0.0: Tue Sep 24 23:37:25 PDT 2024; root:xnu11215.1.12~1/RELEASE_ARM64_T6030', machine='arm64') WARNING: All log messages before absl::InitializeLog() is called are written to STDERR W0000 00:00:1728661436.923585 7587819 mps_client.cc:510] WARNING: JAX Apple GPU support is experimental and not all JAX functionality is correctly supported! I0000 00:00:1728661436.937859 7587819 service.cc:145] XLA service 0x1143d2270 initialized for platform METAL (this does not guarantee that XLA will be used). Devices: I0000 00:00:1728661436.937877 7587819 service.cc:153]   StreamExecutor device (0): Metal,  I0000 00:00:1728661436.939592 7587819 mps_client.cc:406] Using Simple allocator. I0000 00:00:1728661436.939615 7587819 mps_client.cc:384] XLA backend will use up to 12884443136 bytes on device 0 for SimpleAllocator."
yi,Building from source fails with GCC 8.5 clang 16," Description From version 0.4.24 (previous versions didn't have this problem) I am facing the following issue running `python build/build.py` ``` ERROR: /root/.cache/bazel/_bazel_root/bd0cd99199adea4c34404ff9d621925c/external/xla/xla/hlo/evaluator/BUILD:20:11: Compiling xla/hlo/evaluator/hlo_evaluator.: (Exit 1): gcc failed: error executing command (from target //xla/hlo/evaluator:hlo_evaluator)    (cd /root/.cache/bazel/_bazel_root/bd0cd99199adea4c34404ff9d621925c/execroot/__main__ && \   exec env  \     LD_LIBRARY_PATH=/opt/bitnami/python/lib \     PATH=/opt/bitnami/python/bin:/opt/bitnami/python/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \     PWD=/proc/self/cwd \   /usr/bin/gcc U_FORTIFY_SOURCE fstackprotector Wall Wunusedbutsetparameter Wnofreenonheapobject fnoomitframepointer g0 O2 'D_FORTIFY_SOURCE=1' DNDEBUG ffunctionsections fdatasections 'std=c++0x' MD MF bazelout/k8opt/bin/external/xla/xla/hlo/evaluator/_objs/hlo_evaluator/hlo_evaluator.pic.d 'frandomseed=bazelout/k8opt/bin/external/xla/xla/hlo/evaluator/_objs/hlo_evaluator/hlo_evaluator.pic.o' fPIC 'DEIGEN_MAX_ALIGN_BYTES=64' DEIGEN_ALLOW_UNALIGNED_SCALARS 'DEIGEN_USE_AVX512_GEMM_KERNELS=0' DHAVE_SYS_UIO_H DTF_USE_SNAPPY 'DBAZEL_CURRENT_REPOSITORY=""xla""' iquote external/xla iquote bazelout/k8opt/bin/external/xla iquote external/com_google_absl iquote bazelout/k8opt/bin/external/com_google_absl iquote external/eigen_archive iquote bazelout/k8opt/bin/external/eigen_archive iquote external/tsl iquote bazelout/k8opt/bin/external/tsl iquote external/ml_dtypes iquote bazelout/k8opt/bin/external/ml_dtypes iquote external/nsync iquote bazelout/k8opt/bin/external/nsync iquote external/double_conversion iquote bazelout/k8opt/bin/external/double_conversion iquote external/com_google_protobuf iquote bazelout/k8opt/bin/external/com_google_protobuf iquote external/zlib iquote bazelout/k8opt/bin/external/zlib iquote external/snappy iquote bazelout/k8opt/bin/external/snappy iquote external/com_googlesource_code_re2 iquote bazelout/k8opt/bin/external/com_googlesource_code_re2 iquote external/farmhash_archive iquote bazelout/k8opt/bin/external/farmhash_archive Ibazelout/k8opt/bin/external/ml_dtypes/_virtual_includes/float8 Ibazelout/k8opt/bin/external/ml_dtypes/_virtual_includes/int4 isystem external/eigen_archive isystem bazelout/k8opt/bin/external/eigen_archive isystem external/eigen_archive/mkl_include isystem bazelout/k8opt/bin/external/eigen_archive/mkl_include isystem external/ml_dtypes isystem bazelout/k8opt/bin/external/ml_dtypes isystem external/ml_dtypes/ml_dtypes isystem bazelout/k8opt/bin/external/ml_dtypes/ml_dtypes isystem external/nsync/public isystem bazelout/k8opt/bin/external/nsync/public isystem external/com_google_protobuf/src isystem bazelout/k8opt/bin/external/com_google_protobuf/src isystem external/zlib isystem bazelout/k8opt/bin/external/zlib isystem external/farmhash_archive/src isystem bazelout/k8opt/bin/external/farmhash_archive/src 'fvisibility=hidden' Wnosigncompare Wnounknownwarningoption Wnostringoptruncation Wnoarrayparameter 'DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir.' mavx 'std=c++17' fnocanonicalsystemheaders Wnobuiltinmacroredefined 'D__DATE__=""redacted""' 'D__TIMESTAMP__=""redacted""' 'D__TIME__=""redacted""' c external/xla/xla/hlo/evaluator/hlo_evaluator./k8opt/bin/external/xla/xla/hlo/evaluator/_objs/hlo_evaluator/hlo_evaluator.pic.o)  Configuration: ec04adbc7ee0ec8c50ebf14ea7c18a102c1dd9b250453cd1566ab89af658d638  Execution platform: //:platform ... ... external/xla/xla/hlo/evaluator/hlo_evaluator.cc:3724:70: error: no matching function for call to 'StochasticConvertOp::operator std::integral_constant::value_type())))>(const xla::Literal&, const xla::Literal&, const xla::Shape&)'            return StochasticConvertOp(                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^                operand_literal, random_literal, result_shape);                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~           external/xla/xla/hlo/evaluator/hlo_evaluator.cc:3617:25: note: candidate: 'template absl::lts_20230802::StatusOr xla::{anonymous}::StochasticConvertOp(const xla::Literal&, const xla::Literal&, const xla::Shape&)'  absl::StatusOr StochasticConvertOp(const Literal& operand_literal,                          ^~~~~~~~~~~~~~~~~~~ external/xla/xla/hlo/evaluator/hlo_evaluator.cc:3617:25: note:   template argument deduction/substitution failed: external/xla/xla/hlo/evaluator/hlo_evaluator.cc:3681:25: note: candidate: 'template absl::lts_20230802::StatusOr xla::{anonymous}::StochasticConvertOp(const xla::Literal&, const xla::Literal&, const xla::Shape&)'  absl::StatusOr StochasticConvertOp(const Literal& operand_literal,                          ^~~~~~~~~~~~~~~~~~~ external/xla/xla/hlo/evaluator/hlo_evaluator.cc:3681:25: note:   template argument deduction/substitution failed: In file included from external/xla/xla/shape.h:29,                  from external/xla/xla/index_util.h:24,                  from external/xla/xla/literal.h:41,                  from external/xla/xla/hlo/ir/dfs_hlo_visitor.h:26,                  from external/xla/xla/hlo/ir/dfs_hlo_visitor_with_default.h:24,                  from external/xla/xla/hlo/evaluator/hlo_evaluator.h:29,                  from external/xla/xla/hlo/evaluator/hlo_evaluator.cc:15: external/xla/xla/hlo/evaluator/hlo_evaluator.cc:3724:43:   in 'constexpr' expansion of 'xla::primitive_util::BitWidth(primitive_type_constant.std::integral_constant::operator std::integral_constant::value_type())' external/xla/xla/primitive_util.h:539:54: error: 'constexpr int xla::primitive_util::internal::WidthForType(xla::PrimitiveType) [with const std::array& kWidths = xla::primitive_util::internal::kBitWidths]' called in a constant expression    return internal::WidthForType(type);           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~ external/xla/xla/primitive_util.h:529:22: note: 'constexpr int xla::primitive_util::internal::WidthForType(xla::PrimitiveType) [with const std::array& kWidths = xla::primitive_util::internal::kBitWidths]' is not usable as a 'constexpr' function because:  inline constexpr int WidthForType(PrimitiveType type) {                       ^~~~~~~~~~~~ external/xla/xla/primitive_util.h:533:45: error: call to non'constexpr' function 'std::basic_ostream& std::basic_ostream::operator]'    LOG(FATAL) ( ``` I know they are related to XLA but If I download the code related to the commit 12eee889e1f2ad41e27d7b0e970cb92d282d3ec5 (the one fixed for version 0.4.24) I can compile it with bazel 6.1.2 (`bazel build //xla/...`) without any problem and I am not sure where is the issue. Is there any kind of requirements about the build tools?  System info (python version, jaxlib version, accelerator, etc.) ```console $ python version Python 3.11.8 $ clang version clang version 16.0.6 (Red Hat 16.0.62.module+el8.9.0+19521+190d7aba) Target: x86_64redhatlinuxgnu Thread model: posix InstalledDir: /usr/bi $ gcc version gcc (GCC) 8.5.0 20210514 (Red Hat 8.5.020) Copyright (C) 2018 Free Software Foundation, Inc. This is free software; see the source for copying conditions.  There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. ```",2024-03-18T11:22:06Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/20290,My failure. I tried again to compile the XLA project from commit 12eee889e1f2ad41e27d7b0e970cb92d282d3ec5 and it failed. I've just opened the issue https://github.com/openxla/xla/issues/10643,"I'm going to close, because the XLA bug should suffice. No need to have both! (In general, XLA only does presubmit testing with clang, not gcc, so it's probably something that broke under gcc but not clang. Further, your gcc is quite old: v8.5 is 6 years old at this point.)"
yi,add a jax.Array analog that can contain extended dtypes," What problem are you trying to solve? We want to be able to return arrays with arbitrary extended dtypes from jitted computations. That is, we want a `jax.Array`like type that works with extended dtypes. That will be useful for all of our alreadyknown applications of extended dtypes: 1. scale dtypes for AQT (though we hope to replace the need for these with other mechanisms) 2. dtypes with nonstandard tangent dtypes, like floats with different precisions or this AQT application 3. a replacement for float0 so we don't have to rely on a numpy 'void' dtype 4. a replacement for our `core.DArray` for handling bints (Creating a fully general extended dtype is an internal API, not a userfacing one. But we likely want to give users float0 arrays, or bint arrays, or give them ways to create nonstandard primaltangentdtype associations.) More concretely, we want the tests in this PR to pass. Currently, we can use arrays with arbitrary extended dtypes _within_ jitted computations (see e.g. these tests), but we can't return them because `jax.Array` doesn't allow extended dtypes. Moreover extending it would require editing its C++ implementation. And it's not even clear we should: it's nice that `jax.Array` models PjRt arrays, with 'physical' PjRt / HLO dtypes only.  So how should we solve it? One way is to introduce an `EArray` type, like in the current code of this PR. It'd just be a thin wrapper around a `jax.Array` representing the 'physical' data together with an `AbstractValue` with an extended dtype encoding the logical meaning of the data. We delegate most arraylike methods to those two things. It would hit the fast C++ dispatch path by use of the ""dispatch pytree registry"" just like `PRNGKeyArray` does.  Wait, how is this related to `PRNGKeyArray`? Aren't they the same? They're very similar! Maybe we can share much of the implementation. But `PRNGKeyArray` is specialized on PRNG key details that we don't want a generic extended dtype array to be: for example, it has a `_consumed` attribute we wouldn't want in general, and its current `_impl` attribute doesn't really have an analogue either. Plus we may not want to support some operations on `PRNGKeyArrays` which we would want to support on generic arrays, or viceversa. So we hope to share implementation, and perhaps `PRNGKeyArray` can become a subclass or something. Then we could add it to the list of applications above! But we can save the unification for later. **To start with we've basically copied the nonPRNGKeyspecific aspects of `PRNGKeyArray` into a new class.**  How to read this PR The implementation is incomplete. So instead of jumping to the code, instead think of this PR as a small design proposal. Does the motivation make sense? If it does, then look at the tests; do those encode what we want? Finally, if they do, please help add commits to finish off the implementation :) collaboration with    (so far)",2024-03-15T05:24:54Z,pull ready,closed,2,2,https://github.com/jax-ml/jax/issues/20266,"Looking ahead a bit: several unit tests that currently act on `PRNGKeyArray` are effectively earray tests more broadly. At some point after merging this, we should try to find and repurpose such tests for our earray fork/implementation (then work to get them passing).","> expose this via jax.experimental.earray (or similar) for now? Turns out we don't even need to expose it in this PR! That is, the only thing that ever needs to construct an EArray is the `global_sharded_result_handler` of an extended dtype rule set. So no new userfacing APIs here."
yi,`jnp.digitize` does not work in corner case with no bins," Description `np.digitize(x, [])` is equivalent to `zeros_like(x, int)`. `jnp.digitize` blows up instead. The problem looks like a simple typo in the code. ```python from jax import numpy as jnp jnp.digitize(jnp.arange(4), jnp.empty(0)) ``` ```  TypeError                                 Traceback (most recent call last) Cell In[123], line 2       1 from jax import numpy as jnp > 2 jnp.digitize(jnp.arange(4), jnp.empty(0))     [... skipping hidden 12 frame] File ~/Documents/Programmi/micromamba/envs/bartz/lib/python3.12/sitepackages/jax/_src/numpy/lax_numpy.py:5383, in digitize(x, bins, right)    5381   raise ValueError(f""digitize: bins must be a 1dimensional array; got {bins=}"")    5382 if bins_arr.shape[0] == 0: > 5383   return zeros(x, dtype=dtypes.canonicalize_dtype(int_))    5384 side = 'right' if not right else 'left'    5385 return where(    5386   bins_arr[1] >= bins_arr[0],    5387   searchsorted(bins_arr, x, side=side),    5388   len(bins_arr)  searchsorted(bins_arr[::1], x, side=side)    5389 ) File ~/Documents/Programmi/micromamba/envs/bartz/lib/python3.12/sitepackages/jax/_src/numpy/lax_numpy.py:2332, in zeros(shape, dtype, device)    2330 if (m := _check_forgot_shape_tuple(""zeros"", shape, dtype)): raise TypeError(m)    2331 dtypes.check_user_dtype_supported(dtype, ""zeros"") > 2332 shape = canonicalize_shape(shape)    2333 return lax.full(shape, 0, _jnp_dtype(dtype), sharding=_normalize_to_sharding(device)) File ~/Documents/Programmi/micromamba/envs/bartz/lib/python3.12/sitepackages/jax/_src/numpy/lax_numpy.py:86, in canonicalize_shape(shape, context)      84   return core.canonicalize_shape((shape,), context)   type: ignore      85 else: > 86   return core.canonicalize_shape(shape, context) File ~/Documents/Programmi/micromamba/envs/bartz/lib/python3.12/sitepackages/jax/_src/core.py:2117, in canonicalize_shape(shape, context)    2115 except TypeError:    2116   pass > 2117 raise _invalid_shape_error(shape, context) TypeError: Shapes must be 1D sequences of concrete values of integer type, got Tracedwith. If using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions. The error occurred while tracing the function digitize at /Users/giacomo/Documents/Programmi/micromamba/envs/bartz/lib/python3.12/sitepackages/jax/_src/numpy/lax_numpy.py:5374 for jit. This concrete value was not available in Python because it depends on the value of the argument x. The error occurred while tracing the function digitize at /Users/giacomo/Documents/Programmi/micromamba/envs/bartz/lib/python3.12/sitepackages/jax/_src/numpy/lax_numpy.py:5374 for jit. This concrete value was not available in Python because it depends on the value of the argument x. The error occurred while tracing the function digitize at /Users/giacomo/Documents/Programmi/micromamba/envs/bartz/lib/python3.12/sitepackages/jax/_src/numpy/lax_numpy.py:5374 for jit. This concrete value was not available in Python because it depends on the value of the argument x. The error occurred while tracing the function digitize at /Users/giacomo/Documents/Programmi/micromamba/envs/bartz/lib/python3.12/sitepackages/jax/_src/numpy/lax_numpy.py:5374 for jit. This concrete value was not available in Python because it depends on the value of the argument x. ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.25 jaxlib: 0.4.25 numpy:  1.26.4 python: 3.12.2  (main, Feb 16 2024, 20:54:21) [Clang 16.0.6 ] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Darwin', release='23.4.0', version='Darwin Kernel Version 23.4.0: Wed Feb 21 21:44:43 PST 2024; root:xnu10063.101.15~2/RELEASE_ARM64_T6000', machine='arm64') ```",2024-03-14T21:11:20Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/20256,Good catch! Are you interested in putting together a PR with the fix? If not one of the team members can take care of it. Thanks!,"ok, i'll do the pr"
yi,[mutable-arrays] support closed-over mutable arrays in jit,"This PR adds support for jitted functions closing over mutable arrays, like this: ```python x_mut = mutable_array(jnp.zeros(3)) .jit def f():   x_mut[0] += 1 f()   don't crash! print(x_mut)   [1, 0, 0] ``` Previously we only handled mutable arrays that were explicit arguments, like `def f(x_mut): ...`. Here are the main ingredients: 1. When we lower to an XLA computation, we don't want closedover MutableArrays to be treated as constants in the XLA computation; instead we want them to correspond to inputs (and outputs, with aliasing) so that the computation can be fed their current value (and return the new value) on each execution. To produce such an XLA computation, starting from a ClosedJaxpr with MutableArrays in its consts, we hoist out those MutableArray consts from the ClosedJaxpr and convert the corresponding jaxpr binders from constbinders to lambdabinders. (We also do some bookkeeping to handle the newlyintroduced inputs, e.g. triviallyextending the list of input shardings.) 2. Correspondingly, we pass along those MutableArray objects to the dispatch path, so that their buffers can be fed in on every dispatch. That is, where before this PR we had the dispatch logic take a `list[int  None]]` which passes along the extra arguments. 3. To handle the nested jit case, we ensure any MutableArrays closed over by inner jits get hoisted into constants in the outer jit by changing the pjit staging rule. Ultimately we'll want to change the C++ dispatch path to handle the `MutableData` information and perform the same logic that the Python dispatch path is now performing. That's it! Not so bad.",2024-03-13T04:14:22Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/20218
yi,Slow host to GPU transfer with `device_put`," Description I've observed very slow hosttogpu transfer speeds. I'm using the following benchmark script, which you may enjoy at your leisure. ```python import os import argparse import time os.environ[""TF_CPP_MIN_LOG_LEVEL""] = ""0"" os.environ[""TF_STDERR_MIN_LOG_LEVEL""] = ""0"" os.environ[""TF_XLA_FLAGS""] = ""vmodule=gpu_transfer_manager=3"" import jax import numpy as np  A100 and tpu v4 are on 64GB/s PCI busses (16GB/s/chip), H100 and TPUv5  are on 128GB/s (I think?) (32GB/s/chip for the TPU), PCI_BUS_SPEED = 64 * 10**9 NUM_GIGABYTES = 10 DTYPE = np.float32 jax.device_put(jax.numpy.array(0)).delete()   warmup pjit def benchmark_transfer(pci_bandwidth=PCI_BUS_SPEED, num_gigabytes=NUM_GIGABYTES, dtype=DTYPE):     denom = np.finfo(dtype).bits // 8     candidate_array = np.arange(num_gigabytes * 1000 * 10**6 // denom, dtype=dtype)     array_size = candidate_array.itemsize * candidate_array.size     print(f""array created with size {array_size / 10**9:.2f}GB"")     for _ in range(5):         t = time.time()         on_device = jax.device_put(candidate_array)         on_device.block_until_ready()         duration = time.time()  t         transfer_speed = array_size / duration         print(             f""transfer time {duration:.2f}s, transfer rate {transfer_speed / 10**9:.2f}GB/s, pci bus utilization {transfer_speed/pci_bandwidth:.1%}""         )         on_device.delete() if __name__ == ""__main__"":     parser = argparse.ArgumentParser()     parser.add_argument(""pci_bandwidth"", type=float, default=PCI_BUS_SPEED, help=""PCI bus speed in bytes per second"")     parser.add_argument(""num_gigabytes"", type=int, default=NUM_GIGABYTES, help=""Number of gigabytes to transfer"")     parser.add_argument(""dtype"", type=str, default=DTYPE, help=""Data type of the array"")     args = parser.parse_args()     benchmark_transfer(args.pci_bandwidth, args.num_gigabytes, args.dtype) ``` On my A100 system it generates the following output. ``` 20240313 05:39:41.827859: I external/xla/xla/pjrt/pjrt_c_api_client.cc:137] PjRtCApiClient created. 20240313 05:39:41.939174: I external/xla/xla/stream_executor/cuda/cuda_dnn.cc:517] Loaded cuDNN version 8907 array created with size 10.00GB transfer time 18.10s, transfer rate 0.55GB/s, pci bus utilization 0.9% ``` Please let me know if I'm doing something wrong (likely) or if this to be expected! Many thanks for your assistance and time. Zac Update: modified the script to rerun the transfer several times.  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.25 jaxlib: 0.4.25 numpy:  1.26.4 python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] jax.devices (1 total, 1 local): [cuda(id=0)] process_count: 1 platform: uname_result(system='Linux', node='zacgpu', release='6.5.01014gcp', version=' CC(Add copyright notice to quickstart notebook.)~22.04.1Ubuntu SMP Sat Feb 10 04:57:00 UTC 2024', machine='x86_64') $ nvidiasmi Tue Mar 12 22:22:19 2024        ++  ++ ```",2024-03-12T22:27:35Z,bug NVIDIA GPU,closed,0,6,https://github.com/jax-ml/jax/issues/20209,"``` import os import argparse import time os.environ[""TF_CPP_MIN_LOG_LEVEL""] = ""0"" os.environ[""TF_STDERR_MIN_LOG_LEVEL""] = ""0"" os.environ[""TF_XLA_FLAGS""] = ""vmodule=gpu_transfer_manager=3"" import jax import numpy as np  A100 and tpu v4 are on 64GB/s PCI busses (16GB/s/chip), H100 and TPUv5  are on 128GB/s (I think?) (32GB/s/chip for the TPU), PCI_BUS_SPEED = 32 * 10**9 NUM_GIGABYTES = 10 DTYPE = np.float32 jax.device_put(jax.numpy.array(0)).delete()   warmup pjit def benchmark_transfer(pci_bandwidth=PCI_BUS_SPEED, num_gigabytes=NUM_GIGABYTES, dtype=DTYPE):     denom = np.finfo(dtype).bits // 8     candidate_array = np.arange(num_gigabytes * 1000 * 10**6 // denom, dtype=dtype)     array_size = candidate_array.itemsize * candidate_array.size     print(f""array created with size {array_size / 10**9:.2f}GB"")     def t():         t = time.time()         on_device = jax.device_put(candidate_array)         on_device.block_until_ready()         duration = time.time()  t         transfer_speed = array_size / duration         print(         f""transfer time {duration:.2f}s, transfer rate {transfer_speed / 10**9:.2f}GB/s, pci bus utilization {transfer_speed/pci_bandwidth:.1%}""         )         on_device.delete()     t()     t()     t() if __name__ == ""__main__"":     parser = argparse.ArgumentParser()     parser.add_argument(""pci_bandwidth"", type=float, default=PCI_BUS_SPEED, help=""PCI bus speed in bytes per second"")     parser.add_argument(""num_gigabytes"", type=int, default=NUM_GIGABYTES, help=""Number of gigabytes to transfer"")     parser.add_argument(""dtype"", type=str, default=DTYPE, help=""Data type of the array"")     args = parser.parse_args()     benchmark_transfer(args.pci_bandwidth, args.num_gigabytes, args.dtype) ``` Can you try this script? The first call is slower then others as JAX is compiling. So you must discard it. Also, on A100, you have half the perf you expected as your number is the sum on both direction. Here is the output I'm having on V100, so the PCI ratio isn't the right one as it should be lower. ``` 20240313 16:23:50.370963: I external/xla/xla/service/dump.cc:507] HloModule dump enabled with path prefix: , suffix: before_optimizations mp 20240313 16:23:54.283633: I external/xla/xla/stream_executor/cuda/cuda_dnn.cc:517] Loaded cuDNN version 8907 array created with size 10.00GB transfer time 11.06s, transfer rate 0.90GB/s, pci bus utilization 2.8% transfer time 2.75s, transfer rate 3.64GB/s, pci bus utilization 11.4% transfer time 2.76s, transfer rate 3.63GB/s, pci bus utilization 11.3% ``` That would still be only 22% efficient for V100. When I run under nsys, I see the comm is split into many part. I'm not sure if this is CUDA or XLA that trigger this behavoir. Are you only looking to understand or you need to speed this up?",I think this may have regressed in 0.4.25. Try 0.4.24? (Both jax and jaxlib.),"  When I run this repeatedly there is a slight speedup  ``` 20240313 18:31:36.445516: I external/xla/xla/pjrt/pjrt_c_api_client.cc:137] PjRtCApiClient created. 20240313 18:31:36.555472: I external/xla/xla/stream_executor/cuda/cuda_dnn.cc:517] Loaded cuDNN version 8907 array created with size 10.00GB transfer time 18.06s, transfer rate 0.55GB/s, pci bus utilization 0.9% transfer time 4.96s, transfer rate 2.02GB/s, pci bus utilization 3.2% transfer time 4.97s, transfer rate 2.01GB/s, pci bus utilization 3.1% transfer time 4.94s, transfer rate 2.02GB/s, pci bus utilization 3.2% transfer time 4.96s, transfer rate 2.02GB/s, pci bus utilization 3.2% ``` 3% is still unacceptable. However, for the development cycle I will often need to load a checkpoint at the start, and if that is slow then it slows everything down. Even if it is due to compilation, Jax should not require 15 seconds to compile a simple host to device transfer.   Under jax 0.4.24 it's a hair better, but nothing to write home about ``` 20240313 18:37:21.249077: I external/xla/xla/pjrt/pjrt_c_api_client.cc:137] PjRtCApiClient created. 20240313 18:37:21.433903: I external/xla/xla/stream_executor/cuda/cuda_dnn.cc:469] Loaded cuDNN version 8907 array created with size 10.00GB transfer time 16.09s, transfer rate 0.62GB/s, pci bus utilization 1.0% transfer time 2.90s, transfer rate 3.44GB/s, pci bus utilization 5.4% transfer time 2.89s, transfer rate 3.46GB/s, pci bus utilization 5.4% transfer time 2.92s, transfer rate 3.43GB/s, pci bus utilization 5.4% transfer time 2.89s, transfer rate 3.46GB/s, pci bus utilization 5.4% ```","https://github.com/openxla/xla/pull/10528 will revert to the 0.4.24 speeds, which is something.", Did you fix the expected speed as discussed at https://github.com/google/jax/issues/20209issuecomment1995011832 ? I can take another look when the PR is merged.,"https://github.com/openxla/xla/pull/10629 should fix this issue, although I think we can do better still (next week!)"
yi,JAX-based gradient descent plateaus," Description I'm writing my own implementation of some numerical solution papers that solves differential equations using machine learning. However, my implementation fails to converge for examples with slightly complex terms: for example, it can solve the one with exp(y), but it can't solve 4*(exp(y)+exp(y/2)). And when my implementation converges, it does so slower than reported results; what they achieve with 100 epochs I achieve with 1000. Now, perhaps there is a lot to explain in terms of this research to get us to the same page, but here I just want to know how JAX might have been causing the plateau here. I'm suspecting the stacking operation which happens in the first line of the function `N(w, x)`. There I'm just trying to represent a scalar by plugging it to first 5 Chebyshev polynomials, placing each result to a vector respective to the order of the polynomial and getting a 5dimensional representation.  GPU returns the same result but slower. ``` import jax from jax import random, grad, vmap, jit import jax.numpy as jnp jax.config.update(""jax_enable_x64"", True) def chebyshev(n, x):   if n == 0:     return jnp.ones_like(x)   elif n == 1:     return x   else:     return 2*x*chebyshev(n1, x)  chebyshev(n2, x) def N(w, x):    numerical transformation   x = jnp.stack([chebyshev(i, x) for i in range(5)]).T   x = x.T if x.shape[0]==1 else x    learning   z = jnp.dot(w, x).squeeze()   return jnp.tanh(z) def trial(w, x):   return 1 + (x**2)*N(w, x) trial_vect = vmap(trial, (None, 0)) grad_trial = grad(trial, 1) grad_trial_vect = vmap(grad_trial, (None, 0)) grad2_trial = grad(grad_trial, 1) grad2_trial_vect = vmap(grad2_trial, (None, 0)) inputs = jnp.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.])  def error(params, inputs): 	term = grad2_trial_vect(params, inputs) + (2/inputs)*grad_trial_vect(params, inputs) + 4*(2*jnp.exp(trial_vect(params, inputs))+jnp.exp(trial_vect(params, inputs)/2))   	return jnp.sum(0.5*term**2) grad_error = jit(grad(error, 0)) key = random.PRNGKey(0) params = random.normal(key, shape=(1, 5)) epochs = 1000001 lr = 0.0000005 for epoch in range(epochs):     if epoch % 100000  == 0:       print('epoch: %3d error: %.6f' % (epoch, error(params, inputs)))     grads = grad_error(params, inputs)     params = params  lr*grads ``` Result: ``` epoch:   0 error: 5474.779454 epoch: 100000 error: 1281.560668 epoch: 200000 error: 1279.912948 epoch: 300000 error: 1279.364945 epoch: 400000 error: 1279.091227 epoch: 500000 error: 1278.927101 epoch: 600000 error: 1278.817733 epoch: 700000 error: 1278.739640 epoch: 800000 error: 1278.681087 epoch: 900000 error: 1278.635557 epoch: 1000000 error: 1278.599140 ```  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.23 jaxlib: 0.4.23 numpy:  1.25.2 python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1",2024-03-12T13:32:14Z,question,closed,0,11,https://github.com/jax-ml/jax/issues/20192,"If the problem is slow convergence, I would check the learning rate. `0.0000005` seems very small.","> If the problem is slow convergence, I would check the learning rate. `0.0000005` seems very small. Oh, I tried many learning rates, indeed I thought lr decay would solve it, but it didn't. Then I forgot to run with the largest appropriate one before sharing. Anyway, with .5 it converges to `error: 1278.272054` then it's stuck there.","Without digging further, my guess would be that you've found a local optimum.","> Without digging further, my guess would be that you've found a local optimum. Paper omits training details, so this is possible. I'd like to discuss here if this implementation of hardcoded representation is correct; does it hurt the backpropagation or is it fine? ```    numerical transformation   x = jnp.stack([chebyshev(i, x) for i in range(5)]).T   x = x.T if x.shape[0]==1 else x ```","That formulation should be fine when it comes to autodiff. The only issue I see is that the recursive `chebyshev` implementation you're using is not as efficient as it could be: I suspect that the compiler optimizes it well enough, but it will generate a lot of duplicate statements and lead to slow compile times. It shouldn't affect the numerical results though.","(that said, I've not looked at the paper you're trying to implement, so I can't comment on whether what you're doing is similar to what it is doing)","Is the gradient norm approximately zero, i.e. do the iterates stop moving (in addition to the loss not progressing)? To rule out numerical approximation issues, you could try comparing to Autograd, since it is close to a dropin replacement (no vmap though) and it'd use NumPy for numerical operations. Can you link the paper in question?","Also, another thing I noticed: if you change the initial `params` state by using a different seed for `random.key`, you end up with a different answer (also with zero gradient). This strongly suggests that you have a multimodal likelihood space, and you're landing in a local rather than a global optimimum. One way around this would be to use more sophisticated optimization methods; see for example the jaxopt package.","> Is the gradient norm approximately zero, i.e. do the iterates stop moving (in addition to the loss not progressing)? >  > To rule out numerical approximation issues, you could try comparing to Autograd, since it is close to a dropin replacement (no vmap though) and it'd use NumPy for numerical operations. >  > Can you link the paper in question? Tuning the learning rate along training, for the above equation (loss function) I got `error: 70.535873 grad norm: 0.010273`, so yes, they vanish (Additional question here: I knew about learning rate decay, but I had never increased learning rates during training before, yet increasing it got me out of local optimums here. But if I had started with that learning rate, loss would return `nan`.  Any source on this?) This particular equation is not solved in the paper proposing this architecture, but I expect it to solve it, at least reach an error  Also, another thing I noticed: if you change the initial `params` state by using a different seed for `random.key`, you end up with a different answer (also with zero gradient). This strongly suggests that you have a multimodal likelihood space, and you're landing in a local rather than a global optimimum. One way around this would be to use more sophisticated optimization methods; see for example the jaxopt package. Few experiments with Adam resulted in same kind of behavior. And that one paper I mentioned, where it reported convergence in 100 epochs but my JAX implementation did after 1000, it reported using vanilla GD and so that's how I did it. I'll look into it though, thanks! ","> Additional question here: I knew about learning rate decay, but I had never increased learning rates during training before, yet increasing it got me out of local optimums here. But if I had started with that learning rate, loss would return nan. Any source on this? I think it depends on the problem at hand, but in some problems choosing good initializers is difficult and so if we ""warm up"" (ie increase) the step size at first we can get into better optimization regimes. I think that is common in deep learning: see e.g. this paper and references, though I don't know what the canonical references are. On the theory side, the work of Altschuler and Parrilo like here and here (and Altschuler's 2015 masters thesis under Parrilo) offer some explanation of nonmonotonic, notjustconstantordecaying step sizes. Should we keep this issue open, or close it until new questions arise? (Sorry, I'm just not sure if there are outstanding JAX questions at the moment!)","> > Additional question here: I knew about learning rate decay, but I had never increased learning rates during training before, yet increasing it got me out of local optimums here. But if I had started with that learning rate, loss would return nan. Any source on this? >  > I think it depends on the problem at hand, but in some problems choosing good initializers is difficult and so if we ""warm up"" (ie increase) the step size at first we can get into better optimization regimes. I think that is common in deep learning: see e.g. this paper and references, though I don't know what the canonical references are. On the theory side, the work of Altschuler and Parrilo like here and here (and Altschuler's 2015 masters thesis under Parrilo) offer some explanation of nonmonotonic, notjustconstantordecaying step sizes. >  > Should we keep this issue open, or close it until new questions arise? (Sorry, I'm just not sure if there are outstanding JAX questions at the moment!) I think we can close since there is no clear evidence that the problem is JAXrelated. Not sure if we must consider it ""completed"" though. Thanks for quick responses."
rag,Added a lowering rule for lax.sign_p and improved test coverage for binary ops,Added a lowering rule for lax.sign_p and improved test coverage for binary ops Closes CC(bug(pallas): Unimplemented primitive in Pallas GPU lowering: sign),2024-03-12T13:10:29Z,,closed,0,0,https://github.com/jax-ml/jax/issues/20191
yi,Fixed lowering of binary ops for signed dtypes,"Fixed lowering of binary ops for signed dtypes All integers in Trition are signless, so we need to manually forward the signedness of the abstract values. I wonder if we should avoid relying on MLIR types altogether and change _cast and similar APIs to accept JAX dtypes instead?",2024-03-11T21:03:25Z,,closed,0,0,https://github.com/jax-ml/jax/issues/20180
yi,Add int4 test to ArrayImpl.,Add int4 test to ArrayImpl.,2024-03-11T20:11:28Z,,closed,0,0,https://github.com/jax-ml/jax/issues/20178
yi,"Add `where` argument to `argmax`, `argmin`, `ptp`, `cumsum`, `cumprod`","The following functions receive a `where` argument, which limits the reduction to a given boolean mask:  max  min  sum  mean  all  any  prod  var  std **Feature request:** Add a `where` argument to the following functions:  [ ] argmax  [ ] argmin  [ ] ptp  [ ] cumsum  [ ] cumprod Related:  https://github.com/numpy/numpy/issues/14371  https://github.com/numpy/numpy/pull/21625",2024-03-11T20:03:12Z,enhancement,open,1,17,https://github.com/jax-ml/jax/issues/20177,"Hi  thanks for the request! We generally follow the NumPy API in `jax.numpy`, and as far as I can tell, numpy does not support a `where` argument for any of these functions."," Recalling what we did for jax.numpy.fill_diagonal with `inplace` (vs. numpy.fill_diagonal):  https://github.com/google/jax/issues/2680issuecomment1763273560  https://github.com/google/jax/pull/18180 I think that, since the API will be identical/compatible when this functionality is added to numpy (see the linked issue in the OP), JAX implementing it ahead of time should not cause any issues. Personally, I'd find it very helpful to have this functionality.","`fill_diagonal` is somewhat different: it's *impossible* to implement that in JAX without changing the API to avoid inplace modification. Adding additional functionality to existing APIs is a qualitatively different discussion. There is some upside in flexibility, but it has some downsides too, namely: 1. It increases the userfacing API surface that we need to support for the rest of time. 2. It makes `jax.numpy` harder to test, because we must also implement the ground truth version to test against. 3. If NumPy or the Array API eventually adds the same keyword with different semantics, it makes for an awkward deprecation (see e.g. the `arr.device` method deprecation that we're doing currently). 4.  It increases, if only marginally, the cognitive burden of switching between `jax.numpy` and original `numpy`. For those reason, I lean toward not adding these sorts of keywords until they are part of either the NumPy API or the Python Array API."," Thanks for your response! Is there a namespace for useful array functionality that has not yet been incorporated into NumPy? Perhaps `jax.lax` or some theoretical `jax.numpy_experimental`?  Somewhat offtopic, opinionated tangent: It seems detrimental to JAX's evolution to tie itself so strictly to NumPy. 😔 This tying has also caused issues in the past, when NumPy's way of doing things is awkward. Is there any existing theoretical discussion on the benefits, drawbacks, and future of JAX tying itself strictly to NumPy's API? This is perhaps a provocative opinion, and purely speculative at this point, but I'd love to see JAX liberate itself from NumPy's straightjacket. 🙂 Especially if it eclipses NumPy in terms of userbase (which I'm hoping and rooting for). For example, I find certain aspects of PyTorch's tensor API to be superior to NumPy's array API, from a usability perspective. It's also worth noting that NumPy itself currently breaks with the Python Array API.","We have a related discussion here: https://jax.readthedocs.io/en/latest/jep/18137numpyscipyscope.html, although it kind of takes as given that we won't implement things in `jax.numpy` or `jax.scipy` that are not in `numpy` or `scipy` respectively. JAX has plenty of APIs that exist outside of the numpy API, they are just not generally found in the `jax.numpy` namespace.","> For example, I find certain aspects of PyTorch's tensor API to be superior to NumPy's array API, from a usability perspective. Very interesting! Do you have examples in mind?"," One example is how PyTorch makes it easier to chain array operations. Consider the following: ```python3 jnp.sqrt(jnp.square(abs(x.sum(2)).max(1)).sum(0)) ``` Notice the ""backandforth"" or ""spiral"" pattern of the function calls. Compare it to ```python3 x.sum(2).abs().max(1).square().sum(0).sqrt() ``` which is much easier to write and read: 1. It follows the logical order of the operations. 2. It avoids using (and importing) `jnp.`, which reduces code clutter. In my experience, this kind of pattern (with varying lengths) is common. Therefore, from a usability perspective, it would be convenient to attach more methods to JAX arrays themselves.","Thanks for elaboration. I've seen this kind of idea come up before in other contexts (e.g. https://github.com/numpy/numpy/issues/24081) I personally don't feel strongly about this (it would be convenient in some cases, but there are costs to that large an expansion of the API surface, both in terms of maintenance burden and cognitive overhead) but I'm happy to hear what others might think.","Thanks for the link. It's indeed very relevant. It also brings up another advantage I forgot to include: > With fluent API in common, users may more easily write libraryagnostic code. I saw this in practice here: https://github.com/cvxpy/cvxpy/issues/2237. Anecdotally, since it was mentioned in that thread, I've personally also found pandas's methodchaining API a pleasure to work with. Unfortunately, I think NumPy's devs are making the wrong call here. If I *had* to choose between standalone functions and methods, I'd choose the latter, due to the aforementioned advantages from the perspective of user experience. It's also easy to get a standalone function from the method (e.g. `sin = lambda x: x.sin()`), whereas the converse is not true.","I understand the choice the numpy devs have made. For what it’s worth, I wouldn’t suggest trying to reopen that discussion. They’ve made their opinions pretty clear, and it’s a very small team supporting a very large user community.","> I understand the choice the numpy devs have made. [...] They’ve made their opinions pretty clear But do you agree with it? Does the user community agree with it? I won't try to steer NumPy's course on this design choice; it looks like that ship has sailed. I'm more interested in JAX's future. (Again, speaking theoretically at this point.) It might be wise to carefully survey the community for these kinds of impactful, longlasting design decisions. And to ensure surveyees have a good understanding of the issue, each option could be presented with a list of its pros and cons. Perhaps even an argument map. I'd also create a space for the community to discuss the options, and keep it open for a reasonably extended period of time before committing. Together, these could help the project establish a confident, communitywide consensus on the best direction for JAX's evolution. Any thoughts welcome. 🙂","> But do you agree with it? On the whole, yes I do. I understand the reason that you are advocating for methodbased access to all ufuncs and reductions: there are certain situations in which it could make for cleaner, more concise code. But I think the biggest danger to programming languages and DSLs in the long term is bloat in the API surface, and so it is prudent to be conservative when considering addition of new APIs or duplicate spellings of existing APIs. NumPy is quite conservative in this way, and I think that's inextricably tied to why it's remained a successful project for a quarter century. On the JAX side, we have benefitted from using numpy (and more recently the Python Array API) as a standard, because it lets us take the prudent stance without having to debate the individual merits of every single proposed API extension. I believe that is good for JAX as a whole. Is the JAX API perfect? No. Are we willing to change it? Yes, and we add features every day! But it would not be wise to approach changes like the ones proposed here without careful consideration of the costs as well as the benefits.","Just making a note of this here so I don't forget when https://github.com/numpy/numpy/issues/26336 is completed:  Once we add `where` to `argmax`, we can add `where` to `random.categorical` (which uses `argmax` internally), removing the need to manually mask logits with `jnp.inf`."," While this is being worked on from the NumPy side, in the meantime, would it be acceptable to add an optional mask argument to `lax.argmax`, which `numpy.argmax` uses internally? That way, as soon as NumPy adds the `where` argument, JAX can quickly do the same.",`lax` functions by design are moreorless a direct wrapper for the underlying XLA operation; I don't think it's appropriate to add masking support there when the XLA op doesn't implement it.,I didn't find an XLA op for argmax. Is the argmax reduction defined in JAX itself? I found the following chain of definitions: argmax > argmax_p > _compute_argminmax > _ArgMinMaxReducer.,`lax.argmax` is essentially a convenience wrapper that generates a single call to `lax.reduce`
yi,Pallas: transpose matmul throws segmentation fault," Description Transposing a matrix and doing a matrix vector product produces `Segmentation fault (core dumped)` but multiplying by the non transposed matrix works fine. Here is a small reproducer: ```python from functools import partial import jax import jax.experimental.pallas as pl import jax.numpy as jnp def bug_kernel(O_ref, B, d):     A  = jnp.ones([B, B])     x = A.T @ jnp.ones([B, d])   fails  with ""Segmentation fault (core dumped)""      x = A @ jnp.ones([B, d])   works                    pl.store(O_ref, jnp.arange(0, B), x)   def call_buggy_kernel():     B, d = 32 , 16     grid = (1,)     out = pl.pallas_call(         partial(bug_kernel, B=B, d=d),         grid=grid,         out_shape = jax.ShapeDtypeStruct((B, d), jnp.float32),     )() call_buggy_kernel() ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.25 jaxlib: 0.4.25 numpy:  1.26.4 python: 3.11.4 (main, Dec  7 2023, 15:43:41) [GCC 12.3.0] jax.devices (2 total, 2 local): [cuda(id=0) cuda(id=1)] process_count: 1 platform: uname_result(system='Linux', node='jacobmanifestai', release='6.2.039generic', version=' CC(Invalid proto descriptor for file ""tensorflow/compiler/xla/xla_data.proto"")Ubuntu SMP PREEMPT_DYNAMIC Tue Nov 14 14:18:00 UTC 2023', machine='x86_64') $ nvidiasmi Sat Mar  9 09:44:04 2024 ++  ++ ```",2024-03-09T14:53:42Z,bug pallas,closed,0,8,https://github.com/jax-ml/jax/issues/20152,"Thanks, this looks like a Triton bug.  I am able to reproduce this on A100 internally. The error message coming from one of the Triton passes is `getElemsPerThread is not supported for shared layout`. I will try to find a reproducer using Triton Python APIs.","For what is worth, I moved to pallas to see if I could avoid this other bug I had found on triton which also gave the error ``` getElemsPerThread is not supported for shared layout ```",hmm. I thought the other user mentioned that it's solved using the latest triton.," can you check if the issue is still present in the Triton nightly? If yes, the next jaxlib release should have the upstream fixes as well.","That other issue is not solved by the Triton nightly (which, as per their instructions, I got from here: `pip install U indexurl https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/TritonNightly/pypi/simple/ tritonnightly`), because the Triton nightly build is an older version of Triton (2.1.0) that does not include 3D matmul functionality at all. It _is_ fixed by building Triton from source, at the most recent commit to main. Will that make it into the next jaxlib release?"," likely yes, assuming that version of Triton is integrated into the openxla/triton repository by the next release.",The nightly build has been broken for quite a while...sorry about that,I confirmed this does not crash with the latest Triton nightly. Closing.
yi,JAX reports inaccurate error when trying to acquire already-owned TPU," Description JAX doesn't nicely share TPUs with other frameworks (e.g. PyTorch/XLA, TF, etc). This is fine, but the error reported by JAX is misleading. It'd be preferable to actively check if JAX is unable to use the TPU due to multiple libraries using the TPU and report the issue to the user. Here's an example: ``` import torch import torch_xla.core.xla_model as xm t = torch.tensor([1.0, 2.0, 3.0], device=xm.xla_device())  PyTorch/XLA acquires the TPU here. import jax print(jax.devices()) ``` `jax.devices()` results in the following error: ``` RuntimeError: Unable to initialize backend 'tpu': INVALID_ARGUMENT: Mismatched PJRT plugin PJRT API version (0.23) and framework PJRT API version 0.40). (set JAX_PLATFORMS='' to automatically choose an available backend) ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.23 jaxlib: 0.4.23 numpy:  1.25.2 python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] jax.devices (8 total, 8 local): [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0) TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1) ... TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0) TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)] process_count: 1 ```",2024-03-08T23:12:56Z,bug,open,3,1,https://github.com/jax-ml/jax/issues/20148,"Hi  ,  I tested the provided example in a colab notebook connected to a v28 TPU runtime with JAX 0.5.0 version, it executed without any error. The gist is attached for your reference. Could you please verify in your setup with latest JAX version, let us know if the issue still persists? Thank you."
rag,Latest JAX `0.4.24` does not detect GPUs," Description The latest JAX `0.4.24` does not detect GPUs, both using local cuda and pipinstalled cuda. The latest working version for me is `0.4.23`.  Reproduce `pip install ""jax[cuda12_pip]""==0.4.24 f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` With `python c ""import jax; jax.devices('gpu')""`, I get: ``` CUDA backend failed to initialize: Found cuBLAS version 120205, but JAX was built against version 120304, which is newer. The copy of cuBLAS that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) Traceback (most recent call last):   File """", line 1, in    File ~/miniforge3/envs/navix/lib/python3.10/sitepackages/jax/_src/xla_bridge.py"", line 872, in devices     return get_backend(backend).devices()   File ""~/miniforge3/envs/navix/lib/python3.10/sitepackages/jax/_src/xla_bridge.py"", line 806, in get_backend     return _get_backend_uncached(platform)   File ""~/miniforge3/envs/navix/lib/python3.10/sitepackages/jax/_src/xla_bridge.py"", line 788, in _get_backend_uncached     platform = canonicalize_platform(platform)   File ""~/miniforge3/envs/navix/lib/python3.10/sitepackages/jax/_src/xla_bridge.py"", line 614, in canonicalize_platform     raise RuntimeError(f""Unknown backend: '{platform}' requested, but no "" RuntimeError: Unknown backend: 'gpu' requested, but no platforms that are instances of gpu are present. Platforms present are: cpu ```  System info (python version, jaxlib version, accelerator, etc.) `nvcc version` ``` nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 20052023 NVIDIA Corporation Built on Tue_Aug_15_22:02:13_PDT_2023 Cuda compilation tools, release 12.2, V12.2.140 Build cuda_12.2.r12.2/compiler.33191640_0 ``` `nvidiasmi` ``` ++ +++ ```",2024-03-08T10:51:28Z,bug,open,0,12,https://github.com/jax-ml/jax/issues/20133,"The issue is in your error message: ``` Found cuBLAS version 120205, but JAX was built against version 120304, which is newer. ``` You'll need to install cuBLAS version 12.3 or later when using the prebuilt wheels for jax v0.4.24, as those wheels are built against cuda 12.3. Since you used `jax[cuda12_pip]`, you should get compatible cuda automatically. My guess is that you have another CUDA installation on your system that is taking precedence. I would suggest figuring out where those other CUDA sources are installed, and either removing them, or ensuring that the pipinstalled CUDA sources take precedence in your path.","> Since you used jax[cuda12_pip], you should get compatible cuda automatically.  Yeah, that's why I thought the problem was not on my side somehow, thanks very much for the info. I would expect that `jax[cuda12_pip]` prioritises the CUDA installed from pip, though, no? Also, I didn't know which CUDA version was v0.4.24 targeting. Perhaps it would be useful to have the minimum CUDA version in https://github.com/google/jax/?tab=readmeovfileinstallation? It used to be there, IIRC.","`jax[cuda12_pip]` installs the correct CUDA sources in your Python `site_packages`. However, if you have other CUDA installations on your system, and your system is set up to load those other sources, they may be loaded before the ones installed with `pip`. There is nothing that JAX or pip can do about this: it is a property of your system. jax 0.4.24 targets CUDA 12.2, and jax 0.4.25 targets CUDA 12.3, if that helps.","Okay, great! Thanks, Jake!","> `jax[cuda12_pip]` installs the correct CUDA sources in your Python `site_packages`. > However, if you have other CUDA installations on your system, and your system is set up to load those other sources, they may be loaded before the ones installed with `pip`. There is nothing that JAX or pip can do about this: it is a property of your system. > jax 0.4.24 targets CUDA 12.2, and jax 0.4.25 targets CUDA 12.3, if that helps. That is it. Different from most other things in python venvs, cuda seem to add itself at the end of the path. So if you have a supercomputer with modules, strangely the system's cuda will take precedence.  Other stuff installed with pip in a venv will take over the ones from the modules, because venv adds them at the beginning of the python path, for example. Except for cuda. Unloading the cuda module forces it to find the one from site_packages and things work. Go figure.",Is there a proper way to check the location of the set of CUDA libraries that jax is loading? `TF_CPP_MIN_LOG_LEVEL=0` is not showing anything more than usual.,"Sorry, I have got to reopen this. Here's the issue. Different packages both install CUDA binaries (e.g., JAX and pytorch). These CUDA binaries have different versions. Using `jax[cuda12_pip]` or similar makes the two requirements conflict. Using `jax[cuda12_local]` and having a local version of CUDA (e.g., in a cluster) does not help because JAX prefers loading the binaries installed via `pip` by some other package. This happens despite setting `XLA_ARGS=""xla_gpu_cuda_data_dir=""`. Is there a way to better diagnose the issue? For example, printing the folder that JAX uses to load the CUDA binaries? (`TF_CPP_MIN_LOG_LEVEL=0` does not help). Is there any other way to force JAX loading from a different folder?"," It's currently not possible to override the search path, because it's set by an `RPATH` on the .so files in jaxlib. We could perhaps fix that with some cunning. However, note that JAX 0.4.26 relaxed its CUDA version constraints. You should be able to just use the same version of CUDA that Pytorch is using (12.1, last I checked).","Thank very much, . I resorted to that in the meanwhile but JAX v0.4.23 introduces breaking changes to the API. I could access those libraries and align them, but that's not a very general case.","Hey  still issues here. I migrated to the latest JAX version with `pip install ""jax[cuda12]""`. I removed all cuda path references in `LD_LIBRARY_PATH` and `PATH` itself, but for some reason JAX still tries to load an older version of the drivers. I get ``` The NVIDIA driver's CUDA version is 12.4 which is older than the PTX compiler version (12.6.20). Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIAprovided CUDA forward compatibility packages. ``` with ``` echo $LD_LIBRARY_PATH /lib64:/lib:/usr/local/lib:/usr/local/lib64:/usr/lib64:/usr/lib: ``` and (empty) ``` echo $PATH | grep i cuda ``` My `nvidiasmi` shows: ``` NVIDIASMI 550.78                 Driver Version: 550.78         CUDA Version: 12.4 ``` But I guess JAX does not use this CUDA, but installs its own, correct? Where should I look at to address this?","> But I guess JAX does not use this CUDA, but installs its own, correct? With `pip install jax[cuda12]`, it will download its own CUDA; with `pip install jax[cuda12_local]`, it will use the local CUDA. See https://jax.readthedocs.io/en/latest/installation.htmlnvidiagpu for details. If you are seeing this error, and have both a locallyinstalled CUDA and a pipinstalled CUDA on your machine and have adjusted `LD_LIBRARY_PATH`, then I suspect the issue is crosstalk between your two CUDA installations.","Thanks, ! That's what I don't understand. Where could the locally installed CUDA be set as higher priority than the pipinstalled one, if `LD_LIBRARY_PATH` and `PATH` are clean? Do you have any idea?"
yi,"Metal: failed assertion `New volume: 6 should match old volume: 24` for einsum ""ijk,kji->kij"""," Description distinct failure mode from CC(Metal: failed to legalize operation 'mhlo.dot_general' for einsum ""ijk,kji>k"")  ```python >>> import jax.numpy as jnp >>> a = jnp.ones((2,3,4)) Platform 'METAL' is experimental and not all JAX functionality may be correctly supported! 20240306 22:50:31.964310: W pjrt_plugin/src/mps_client.cc:563] WARNING: JAX Apple GPU support is experimental and not all JAX functionality is correctly supported! Metal device set to: Apple M1 Pro systemMemory: 32.00 GB maxCacheSize: 10.67 GB >>> b = jnp.ones((4,3,2)) >>>  jnp.einsum(""ijk,kji>kij"", a, b)   File """", line 1     jnp.einsum(""ijk,kji>kij"", a, b) IndentationError: unexpected indent >>> jnp.einsum(""ijk,kji>kij"", a, b) /AppleInternal/Library/BuildRoots/4e1473ee9f6611ee8dafcedaeb4cabe2/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShaders/MPSNDArray/Kernels/MPSNDArrayIdentity.mm:813: failed assertion `New volume: 6 should match old volume: 24 [reshapeWithCommandBuffer] MPSNDArrayIdentity.' Abort trap: 6 ```  System info (python version, jaxlib version, accelerator, etc.) ``` python c ""import jax; jax.print_environment_info()"" Platform 'METAL' is experimental and not all JAX functionality may be correctly supported! 20240306 22:52:59.325589: W pjrt_plugin/src/mps_client.cc:563] WARNING: JAX Apple GPU support is experimental and not all JAX functionality is correctly supported! Metal device set to: Apple M1 Pro systemMemory: 32.00 GB maxCacheSize: 10.67 GB jax:    0.4.20 jaxlib: 0.4.20 numpy:  1.26.4 python: 3.10.13  (main, Dec 23 2023, 15:35:25) [Clang 16.0.6 ] jax.devices (1 total, 1 local): [METAL(id=0)] process_count: 1 ```",2024-03-07T06:53:13Z,bug Apple GPU (Metal) plugin,closed,0,2,https://github.com/jax-ml/jax/issues/20115,The issue will be fixed in the upcoming release.,Fixed in jaxmetal 0.0.6.
gemma,Auto-labels 'gemma' on 'gemma' issues/PRs.,"This workflow automatically identifies issues and pull requests (PRs) related to `Gemma`. It searches for the keyword `Gemma` (caseinsensitive) in both the title and description of the issue/PR. If a match is found, the workflow adds the label `Gemma` to the issue/PR. On first run, the workflow will try to create the label `Gemma`. If unsuccessful, the repository owner will need to create it manually.  label: `Gemma`",2024-03-07T06:35:38Z,,open,0,0,https://github.com/jax-ml/jax/issues/20113
rag,Improve documentation for multi-node/host training,"As laid out in CC(未找到相关数据), I feel that there is a need to have a minimal example for how to use the new sharding API with `device_put` for training in a multiprocess fashion, like a TPU Pod slice. So far, I've seen multiple different methods to accomplish the same which leverage ideas (like `pjit`) that are out of date and some that straight up don't work. For example, I was following this discussion where apparently `make_array_from_single_device_arrays` wasn't returning a sharded array for the host  but rather the global array which was `n` times as big (`n` is the number of hosts). This is a major pain point, as the API around parallelization should be topnotch considering scalability is a strong focus for the jax team. Rather, in practice the end user ends up wading through a sea of a variety of `xmap`/`pmap`/`sharding` and intermixing of both, some of which are deprecated and not recommended to be used. This seriously needs to be improved. I feel the sharding API is *really* well written for parallelizing across multipledevices on a single host, and it only took me 15 mins to integrate that. However, multinode/process **definitely** needs a minimal example.",2024-03-06T16:24:17Z,enhancement,open,4,3,https://github.com/jax-ml/jax/issues/20099,"I can't agree more! I'm also trying to write some multinode codes, but it's confusing when I want to stick to the style of automatic parallelization with multiple hosts. A minimum example will be extremely helpful. At least it can tell us what is the recommended way to implement parallelization. For now I can also spend a month implementing `make_array_from_single_device_arrays` everywhere in my codes, but I'm afraid it may be deprecated in the future, in which case my effort will be fully wasted.",You can also use `jax.make_array_from_process_local_data` to create global jax.Array from data on your hosts. ,"FWIW, I use scalax for multinode currently. however it does have a bit of a learning curve, but its pretty easy once you do figure it out. Rather disappointing that 3rd party libs have to put up support for something so fundamental as multinode parallelization. I feel it goes against the jax spirit. "
llm,"CPU: incorrect result from combo of arg donation, sharding constraint, and --xla_force_host_platform_device_count=2"," Description This test passes on GPU (1 or 2 GPUs), on CPU on MacOS M1 (with or without the XLA FLAG), and Linux CPU (without the flag) But with the `xla_force_host_platform_device_count=2` (or 8), on linux, on x64 CPU, the assertion fails with seemingly random garbage being returned instead. I've seen it on three different machines. Removing the donation or the sharding constraints, or the flag makes this test past. I've instantiated other versions of this bug, but I think (hope) they're all the same. ``` (levanter) dlwh:~/src/levanter$ XLA_FLAGS=xla_force_host_platform_device_count=8 python tests/test_xx.py Traceback (most recent call last):   File ""/juice2/u/dlwh/src/levanter/tests/test_xx.py"", line 45, in      test_minimize()   File ""/juice2/u/dlwh/src/levanter/tests/test_xx.py"", line 41, in test_minimize     assert jnp.all(model0[1] == state[1]), f""{model0[1]} != {state[1]}... {model0[0]}"" AssertionError: [[ 0.00386154]  [0.01053566]] != [[0.00554077]  [ 0.02703212]]... [[0.00099001]  [0.01423136]] ``` ```python  from functools import partial import jax import jax.numpy as jnp from jax.sharding import Mesh, NamedSharding, PartitionSpec def test_minimize():     In = 2     Out = 1     mesh = Mesh(jax.devices(), (""devices"",))     def init_fn(key):         k1, k2 = jax.random.split(key)         w1 = jax.random.normal(k1, (In, Out)) * 0.02         w2 = jax.random.normal(k2, (In, Out)) * 0.02          without these sharding constraints, the test passes         w1 = jax.lax.with_sharding_constraint(w1, NamedSharding(mesh, PartitionSpec()))         w2 = jax.lax.with_sharding_constraint(w2, NamedSharding(mesh, PartitionSpec()))         return (w1, w2)      without argument donation, the test passes      .jit     (jax.jit, donate_argnums=0)     def init_and_merge(l2, k):         l1, _ = init_fn(k)         return l1, l2     k0 = jax.random.PRNGKey(0)     k1 = jax.random.PRNGKey(1)     model0 = jax.jit(init_fn)(k0)     state = init_and_merge(model0[1], k1)      reinit b/c donated     model0 = jax.jit(init_fn)(k0)     assert jnp.all(model0[1] == state[1]), f""{model0[1]} != {state[1]}... {model0[0]}"" if __name__ == ""__main__"":     test_minimize() ```  System info (python version, jaxlib version, accelerator, etc.) ``` python c ""import jax; jax.print_environment_info()"" jax:    0.4.23 jaxlib: 0.4.23 numpy:  1.26.2 python: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 ```",2024-03-05T23:30:07Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/20088,"I can't seem to repro this in my environment: ``` In [10]: jax.print_environment_info() jax:    0.4.25.dev20240224+f9e20d587 jaxlib: 0.4.24.dev20240120 numpy:  1.24.3 python: 3.11.7 (main, Dec  8 2023, 14:22:46) [GCC 13.2.0] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 ``` Can you try with jax nightly on CPU?",Commands: https://jax.readthedocs.io/en/latest/installation.htmlnightlyinstallation,"yep, works fine. my bad."
yi,"`jax.experimental.attrs`: Spurious ""AssertionError: a jaxpr variable must be created only once per tracer"""," Description The following code results in an erroneous jax internal assertion > `AssertionError: a jaxpr variable must be created only once per tracer` The issue is a useafterfree due to `experimental.attrs` creating a Tracer but not adequately ensuring its lifetime. ```python import jax from jax.experimental.attrs import jax_getattr, jax_setattr class StatefulRNG:     key: jax.Array     def __init__(self, key: jax.Array):         self.key = key     def split(self) > jax.Array:         key = jax_getattr(self, ""key"")         new_key, returned_key = jax.random.split(key)         jax_setattr(self, ""key"", new_key)         return returned_key rng = StatefulRNG(jax.random.key(0)) def jitted():     rng.split()     rng.split() jax.jit(jitted)() ``` (n.b. this is not code or a style I intend to use; I was just trying to experiment with `experimental.attrs` to wrap my head around it) Output on my laptop (`JAX_TRACEBACK_FILTERING=off` version attached) ``` ❯ JAX_PLATFORMS=cpu python attrs_bug.py Traceback (most recent call last):   File ""/Users/nelhage/attrs_bug.py"", line 27, in      jax.jit(jitted)()   File ""/Users/nelhage/attrs_bug.py"", line 24, in jitted     rng.split()   File ""/Users/nelhage/attrs_bug.py"", line 14, in split     new_key, returned_key = jax.random.split(key)                             ^^^^^^^^^^^^^^^^^^^^^   File ""/opt/homebrew/Caskroom/miniforge/base/envs/py311/lib/python3.11/sitepackages/jax/_src/random.py"", line 295, in split     return _return_prng_keys(wrapped, _split(typed_key, num))                                       ^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/homebrew/Caskroom/miniforge/base/envs/py311/lib/python3.11/sitepackages/jax/_src/random.py"", line 281, in _split     return prng.random_split(key, shape=shape)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/homebrew/Caskroom/miniforge/base/envs/py311/lib/python3.11/sitepackages/jax/_src/prng.py"", line 631, in random_split     return random_split_p.bind(keys, shape=shape)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ AssertionError: a jaxpr variable must be created only once per tracer  For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. ``` The problem arises from these lines in `attrs.py`: https://github.com/google/jax/blob/f9e20d58754283de87b2ed35cc9df58bcdff2073/jax/experimental/attrs.pyL70L71 The `tracer` we create is referenced only by way of the `setattr` on the following line. If a future mutation to the tracked object drops that reference, the tracer will be freed, and its address (`id`) may be reused by a future object. If a new `Tracer` then happens to get allocated at the same address, the `tracer_to_var` map will still contain the old mapping and result in the observed problem. We can hackily test that theory by making sure to manually retain all the tracers inside of our reproducer; this version works reliably for me: ```python import jax from jax.experimental.attrs import jax_getattr, jax_setattr class StatefulRNG:     key: jax.Array     def __init__(self, key: jax.Array):         self.key = key     def split(self, l) > jax.Array:         key = jax_getattr(self, ""key"")         l.append(key)         new_key, returned_key = jax.random.split(key)         l.append(new_key)         l.append(returned_key)         jax_setattr(self, ""key"", new_key)         return returned_key rng = StatefulRNG(jax.random.key(0)) def jitted():     l = []     rng.split(l)     rng.split(l)     l.clear() jax.jit(jitted)() ```  System info (python version, jaxlib version, accelerator, etc.) ``` ❯ JAX_PLATFORMS='cpu' python c 'import jax; jax.print_environment_info()' jax:    0.4.25 jaxlib: 0.4.25 numpy:  1.24.4 python: 3.11.6  (main, Oct  3 2023, 10:37:07) [Clang 15.0.7 ] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Darwin', node='NelsonElhageMacBook', release='23.3.0', version='Darwin Kernel Version 23.3.0: Wed Dec 20 21:30:27 PST 2023; root:xnu10002.81.5~7/RELEASE_ARM64_T8103', machine='arm64') ```",2024-03-05T19:57:55Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/20082,"Brilliant, thank you for the clear diagnosis! I think we should probably persist all the Tracers we create during jaxpr tracing. That's usually what we do with this attribute on the builder, and we append to it when we make new tracers in the usual path, but as you point out we neglected to do it on the attrs path."
yi,cusparse_build_version outputs version greater than latest cusparse version," Description Hi There, I'm trying to run JAX on a fresh VM on cloud infrastructure. The error is on the first line when printing the environment info ``` CUDA backend failed to initialize: Found cuSPARSE version 12103, but JAX was built against version 12200, which is newer. The copy of cuSPARSE that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) ``` This seems like a bug in the code that checks the cuSPARSE version because the latest version of cuSPARSE is `12.2.0.103`.  As seen on my system, I have this version: ``` $ python3 m pip list  ++ ```",2024-03-05T18:59:12Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/20079,"JAX's `cusparse_build_version` just returns `CUSPARSE_VERSION` as defined in the cusparse C headers *at the time that `jaxlib` is built*. The definition looks something like this depending on which CUDA version you're using: ``` define CUSPARSE_VER_MAJOR 12 define CUSPARSE_VER_MINOR 1 define CUSPARSE_VER_PATCH 3 define CUSPARSE_VER_BUILD 153 define CUSPARSE_VERSION (CUSPARSE_VER_MAJOR * 1000 + \                           CUSPARSE_VER_MINOR *  100 + \                           CUSPARSE_VER_PATCH) ``` This would become `cusparse_version = 12103`. `cusparse_version = 12200` would come from version `12.2.0`. It looks like you're using a jaxlib that was built against version `12.2.0`, but you have version `12.1.0` on your system. Does that help answer your question?","Ah, I see what you mean. I was confused because the `103` of `12103` happened to match the `103` of `12.2.0.103`. It turns out these are not correlated. So even though the latest version of cusparse I have through pip is `12.2.0.1031`, jax must be pulling the cusparse version from elsewhere. Likely I have cusparse `12.1.3.153` installed somewhere, though where I'm not sure yet. I'll go digging for that and see if I can remove it from the path. Confirmed. Cusparse was installed elsewhere. I removed it from LD_LIBRARY_PATH and all is good now. ","Great, glad you figured it out!"
yi,Add copy argument to Array.__array__,"This is used by NumPy 2.0, and will be required in a future numpy version (see https://github.com/numpy/numpy/issues/25941). Part of CC(Tracking issue: NumPy 2.0 Compatibility) ",2024-03-05T17:32:47Z,kokoro:force-run pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/20077
yi,Stochastic jaxlib.xla_extension.XlaRuntimeError with sine activation function on NVIDIA A100 (all algorithms tried for... failed.)," Description I sometimes, but not consistently, get the following jaxlib.xla_extension.XlaRuntimeError when training a neural network with sine activations in JAX on an NVIDIA A100 GPU:  ``` jaxlib.xla_extension.XlaRuntimeError: INTERNAL: All algorithms tried for %dot.128 = f32[384,384]{1,0} dot(f32[216000,384]{1,0} %multiply.51, f32[216000,384]{1,0} %sine.3), lhs_contracting_dims={0}, rhs_contracting_dims={0}, metadata={op_name=""jit(train_step)/jit(main)/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=float32]"" source_file=""./layers.py"" source_line=160} failed. Falling back to default algorithm.  Peralgorithm errors:   Results do not match the reference. This is likely a bug/unexpected loss of precision.   ...   Results do not match the reference. This is likely a bug/unexpected loss of precision. ``` The final part of the traceback is ``` File ""/home/skoop/.conda/envs/crystals/lib/python3.10/sitepackages/equinox/_jit.py"", line 206, in __call__     return self._call(False, args, kwargs)   File ""/home/skoop/.conda/envs/crystals/lib/python3.10/sitepackages/equinox/_module.py"", line 935, in __call__     return self.__func__(self.__self__, *args, **kwargs)   File ""/home/skoop/.conda/envs/crystals/lib/python3.10/sitepackages/equinox/_jit.py"", line 200, in _call     out = self._cached(dynamic_donate, dynamic_nodonate, static) ``` **This error does however not occur when I use an NVIDIA GeForce RTX 3080 or NVIDIA GeForce RTX 3060.** Some accompanying logs are ``` 20240216 16:05:48.264495: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 3557: 12.5601, expected 8.76762 20240216 16:05:48.264583: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 7281: 17.2671, expected 26.1401 20240216 16:05:48.264685: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 31490: 46.9089, expected 35.7881 20240216 16:05:48.264734: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 42098: 92.8985, expected 82.6593 20240216 16:05:48.264871: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 77426: 14.7896, expected 10.7041 20240216 16:05:48.264960: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 99275: 61.807, expected 51.012 20240216 16:05:48.264979: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 101673: 54.8168, expected 64.0151 20240216 16:05:48.265027: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 111729: 23.3462, expected 28.2862 20240216 16:05:48.265046: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 114033: 54.0808, expected 44.5037 20240216 16:05:48.265094: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 124619: 35.2514, expected 46.6975 20240216 16:05:48.265104: E external/xla/xla/service/gpu/triton_autotuner.cc:816] Results do not match the reference. This is likely a bug/unexpected loss of precision. 20240216 16:05:48.297565: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 3557: 12.5601, expected 8.76762 20240216 16:05:48.297609: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 7281: 17.2671, expected 26.1401 20240216 16:05:48.297709: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 31490: 46.9089, expected 35.7881 20240216 16:05:48.297757: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 42098: 92.8985, expected 82.6593 20240216 16:05:48.297901: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 77426: 14.7896, expected 10.7041 20240216 16:05:48.297993: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 99275: 61.807, expected 51.012 20240216 16:05:48.298012: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 101673: 54.8168, expected 64.0151 20240216 16:05:48.298058: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 111729: 23.3462, expected 28.2862 20240216 16:05:48.298076: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 114033: 54.0808, expected 44.5037 20240216 16:05:48.298124: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 124619: 35.2514, expected 46.6975 20240216 16:05:48.298133: E external/xla/xla/service/gpu/triton_autotuner.cc:816] Results do not match the reference. This is likely a bug/unexpected loss of precision. 20240216 16:05:48.933430: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 3557: 12.5601, expected 8.76762 20240216 16:05:48.933503: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 7281: 17.2671, expected 26.1401 20240216 16:05:48.933612: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 31490: 46.9089, expected 35.7881 20240216 16:05:48.933663: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 42098: 92.8985, expected 82.6593 20240216 16:05:48.933808: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 77426: 14.7896, expected 10.7041 20240216 16:05:48.933897: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 99275: 61.807, expected 51.012 20240216 16:05:48.933916: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 101673: 54.8168, expected 64.0151 20240216 16:05:48.933962: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 111729: 23.3462, expected 28.2862 20240216 16:05:48.933981: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 114033: 54.0808, expected 44.5037 20240216 16:05:48.934031: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 124619: 35.2514, expected 46.6975 20240216 16:05:48.934042: E external/xla/xla/service/gpu/triton_autotuner.cc:816] Results do not match the reference. This is likely a bug/unexpected loss of precision. 20240216 16:05:49.506808: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 3557: 12.5601, expected 8.76762 20240216 16:05:49.506871: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 7281: 17.2671, expected 26.1401 20240216 16:05:49.506970: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 31490: 46.9089, expected 35.7881 20240216 16:05:49.507018: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 42098: 92.8985, expected 82.6593 20240216 16:05:49.507155: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 77426: 14.7896, expected 10.7041 20240216 16:05:49.507243: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 99275: 61.807, expected 51.012 20240216 16:05:49.507262: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 101673: 54.8168, expected 64.0151 20240216 16:05:49.507308: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 111729: 23.3462, expected 28.2862 20240216 16:05:49.507326: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 114033: 54.0808, expected 44.5037 20240216 16:05:49.507374: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 124619: 35.2514, expected 46.6975 20240216 16:05:49.507384: E external/xla/xla/service/gpu/triton_autotuner.cc:816] Results do not match the reference. This is likely a bug/unexpected loss of precision. 20240216 16:05:51.473397: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 3557: 12.4619, expected 8.76762 20240216 16:05:51.473461: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 7281: 17.3246, expected 26.1401 20240216 16:05:51.473567: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 31490: 46.9092, expected 35.7881 20240216 16:05:51.473616: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 42098: 92.9825, expected 82.6593 20240216 16:05:51.473762: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 77426: 14.8192, expected 10.7041 20240216 16:05:51.473796: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 84145: 0.918945, expected 1.13448 20240216 16:05:51.473861: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 99275: 61.8836, expected 51.012 20240216 16:05:51.473880: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 101673: 54.8101, expected 64.0151 20240216 16:05:51.473926: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 111729: 23.3323, expected 28.2862 20240216 16:05:51.473945: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 114033: 54.0575, expected 44.5037 20240216 16:05:51.473955: E external/xla/xla/service/gpu/triton_autotuner.cc:816] Results do not match the reference. This is likely a bug/unexpected loss of precision. 20240216 16:05:51.502365: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 3557: 12.449, expected 8.76762 20240216 16:05:51.502411: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 7281: 17.3652, expected 26.1401 20240216 16:05:51.502515: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 31490: 46.9354, expected 35.7881 20240216 16:05:51.502574: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 42098: 92.9954, expected 82.6593 20240216 16:05:51.502723: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 77426: 14.8499, expected 10.7041 20240216 16:05:51.502812: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 99275: 61.8535, expected 51.012 20240216 16:05:51.502831: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 101673: 54.8031, expected 64.0151 20240216 16:05:51.502877: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 111729: 23.2869, expected 28.2862 20240216 16:05:51.502896: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 114033: 53.988, expected 44.5037 20240216 16:05:51.502944: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 124619: 35.3679, expected 46.6975 20240216 16:05:51.502954: E external/xla/xla/service/gpu/triton_autotuner.cc:816] Results do not match the reference. This is likely a bug/unexpected loss of precision. 20240216 16:05:51.531594: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 3557: 12.4182, expected 8.76762 20240216 16:05:51.531638: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 7281: 17.3456, expected 26.1401 20240216 16:05:51.531741: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 31490: 46.8993, expected 35.7881 20240216 16:05:51.531761: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 34034: 64.7217, expected 58.1465 20240216 16:05:51.531802: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 42098: 93.0271, expected 82.6593 20240216 16:05:51.531940: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 77426: 14.8545, expected 10.7041 20240216 16:05:51.532029: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 99275: 61.8445, expected 51.012 20240216 16:05:51.532048: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 101673: 54.8469, expected 64.0151 20240216 16:05:51.532094: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 111729: 23.2837, expected 28.2862 20240216 16:05:51.532113: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 114033: 53.994, expected 44.5037 20240216 16:05:51.532122: E external/xla/xla/service/gpu/triton_autotuner.cc:816] Results do not match the reference. This is likely a bug/unexpected loss of precision. 20240216 16:05:52.266140: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 3557: 12.5601, expected 8.76762 20240216 16:05:52.266194: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 7281: 17.2671, expected 26.1401 20240216 16:05:52.266300: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 31490: 46.9089, expected 35.7881 20240216 16:05:52.266349: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 42098: 92.8985, expected 82.6593 20240216 16:05:52.266492: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 77426: 14.7896, expected 10.7041 20240216 16:05:52.266589: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 99275: 61.807, expected 51.012 20240216 16:05:52.266609: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 101673: 54.8168, expected 64.0151 20240216 16:05:52.266656: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 111729: 23.3462, expected 28.2862 20240216 16:05:52.266676: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 114033: 54.0808, expected 44.5037 20240216 16:05:52.266724: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 124619: 35.2514, expected 46.6975 20240216 16:05:52.266734: E external/xla/xla/service/gpu/triton_autotuner.cc:816] Results do not match the reference. This is likely a bug/unexpected loss of precision. 20240216 16:05:52.889293: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 3557: 12.5601, expected 8.76762 20240216 16:05:52.889346: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 7281: 17.2671, expected 26.1401 20240216 16:05:52.889456: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 31490: 46.9089, expected 35.7881 20240216 16:05:52.889505: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 42098: 92.8985, expected 82.6593 20240216 16:05:52.889651: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 77426: 14.7896, expected 10.7041 20240216 16:05:52.889743: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 99275: 61.807, expected 51.012 20240216 16:05:52.889762: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 101673: 54.8168, expected 64.0151 20240216 16:05:52.889808: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 111729: 23.3462, expected 28.2862 20240216 16:05:52.889826: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 114033: 54.0808, expected 44.5037 20240216 16:05:52.889874: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 124619: 35.2514, expected 46.6975 20240216 16:05:52.889884: E external/xla/xla/service/gpu/triton_autotuner.cc:816] Results do not match the reference. This is likely a bug/unexpected loss of precision. 20240216 16:05:52.909936: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 3557: 12.4314, expected 8.76762 20240216 16:05:52.909981: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 7281: 17.3356, expected 26.1401 20240216 16:05:52.910086: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 31490: 46.9041, expected 35.7881 20240216 16:05:52.910134: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 42098: 92.9747, expected 82.6593 20240216 16:05:52.910272: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 77426: 14.8142, expected 10.7041 20240216 16:05:52.910361: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 99275: 61.8406, expected 51.012 20240216 16:05:52.910380: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 101673: 54.7939, expected 64.0151 20240216 16:05:52.910427: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 111729: 23.2614, expected 28.2862 20240216 16:05:52.910445: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 114033: 54.0424, expected 44.5037 20240216 16:05:52.910495: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 124619: 35.2993, expected 46.6975 20240216 16:05:52.910505: E external/xla/xla/service/gpu/triton_autotuner.cc:816] Results do not match the reference. This is likely a bug/unexpected loss of precision. 20240216 16:05:52.983981: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 3557: 12.5601, expected 8.76762 20240216 16:05:52.984024: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 7281: 17.2671, expected 26.1401 20240216 16:05:52.984129: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 31490: 46.9089, expected 35.7881 20240216 16:05:52.984179: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 42098: 92.8985, expected 82.6593 20240216 16:05:52.984323: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 77426: 14.7896, expected 10.7041 20240216 16:05:52.984412: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 99275: 61.807, expected 51.012 20240216 16:05:52.984431: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 101673: 54.8168, expected 64.0151 20240216 16:05:52.984479: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 111729: 23.3462, expected 28.2862 20240216 16:05:52.984497: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 114033: 54.0808, expected 44.5037 20240216 16:05:52.984546: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 124619: 35.2514, expected 46.6975 20240216 16:05:52.984561: E external/xla/xla/service/gpu/triton_autotuner.cc:816] Results do not match the reference. This is likely a bug/unexpected loss of precision. 20240216 16:05:52.990249: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 3557: 12.4182, expected 8.76762 20240216 16:05:52.990290: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 7281: 17.3456, expected 26.1401 20240216 16:05:52.990396: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 31490: 46.8993, expected 35.7881 20240216 16:05:52.990415: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 34034: 64.7217, expected 58.1465 20240216 16:05:52.990457: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 42098: 93.0271, expected 82.6593 20240216 16:05:52.990601: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 77426: 14.8545, expected 10.7041 20240216 16:05:52.990696: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 99275: 61.8445, expected 51.012 20240216 16:05:52.990715: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 101673: 54.8469, expected 64.0151 20240216 16:05:52.990761: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 111729: 23.2837, expected 28.2862 20240216 16:05:52.990780: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 114033: 53.994, expected 44.5037 20240216 16:05:52.990789: E external/xla/xla/service/gpu/triton_autotuner.cc:816] Results do not match the reference. This is likely a bug/unexpected loss of precision. 20240216 16:05:53.003926: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 3557: 12.5601, expected 8.76762 20240216 16:05:53.003967: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 7281: 17.2671, expected 26.1401 20240216 16:05:53.004071: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 31490: 46.9089, expected 35.7881 20240216 16:05:53.004122: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 42098: 92.8985, expected 82.6593 20240216 16:05:53.004260: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 77426: 14.7896, expected 10.7041 20240216 16:05:53.004349: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 99275: 61.807, expected 51.012 20240216 16:05:53.004368: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 101673: 54.8168, expected 64.0151 20240216 16:05:53.004414: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 111729: 23.3462, expected 28.2862 20240216 16:05:53.004433: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 114033: 54.0808, expected 44.5037 20240216 16:05:53.004481: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 124619: 35.2514, expected 46.6975 20240216 16:05:53.004491: E external/xla/xla/service/gpu/triton_autotuner.cc:816] Results do not match the reference. This is likely a bug/unexpected loss of precision. 20240216 16:05:53.021304: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 3557: 12.5601, expected 8.76762 20240216 16:05:53.021347: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 7281: 17.2671, expected 26.1401 20240216 16:05:53.021452: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 31490: 46.9089, expected 35.7881 20240216 16:05:53.021503: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 42098: 92.8985, expected 82.6593 20240216 16:05:53.021653: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 77426: 14.7896, expected 10.7041 20240216 16:05:53.021744: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 99275: 61.807, expected 51.012 20240216 16:05:53.021763: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 101673: 54.8168, expected 64.0151 20240216 16:05:53.021811: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 111729: 23.3462, expected 28.2862 20240216 16:05:53.021830: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 114033: 54.0808, expected 44.5037 20240216 16:05:53.021880: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 124619: 35.2514, expected 46.6975 20240216 16:05:53.021890: E external/xla/xla/service/gpu/triton_autotuner.cc:816] Results do not match the reference. This is likely a bug/unexpected loss of precision. 20240216 16:05:53.060989: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 3557: 12.5601, expected 8.76762 20240216 16:05:53.061030: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 7281: 17.2671, expected 26.1401 20240216 16:05:53.061134: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 31490: 46.9089, expected 35.7881 20240216 16:05:53.061185: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 42098: 92.8985, expected 82.6593 20240216 16:05:53.061322: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 77426: 14.7896, expected 10.7041 20240216 16:05:53.061415: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 99275: 61.807, expected 51.012 20240216 16:05:53.061434: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 101673: 54.8168, expected 64.0151 20240216 16:05:53.061480: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 111729: 23.3462, expected 28.2862 20240216 16:05:53.061499: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 114033: 54.0808, expected 44.5037 20240216 16:05:53.061547: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 124619: 35.2514, expected 46.6975 20240216 16:05:53.061561: E external/xla/xla/service/gpu/triton_autotuner.cc:816] Results do not match the reference. This is likely a bug/unexpected loss of precision. 20240216 16:05:53.139898: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 3557: 12.5601, expected 8.76762 20240216 16:05:53.139939: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 7281: 17.2671, expected 26.1401 20240216 16:05:53.140042: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 31490: 46.9089, expected 35.7881 20240216 16:05:53.140092: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 42098: 92.8985, expected 82.6593 20240216 16:05:53.140230: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 77426: 14.7896, expected 10.7041 20240216 16:05:53.140319: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 99275: 61.807, expected 51.012 20240216 16:05:53.140338: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 101673: 54.8168, expected 64.0151 20240216 16:05:53.140384: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 111729: 23.3462, expected 28.2862 20240216 16:05:53.140403: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 114033: 54.0808, expected 44.5037 20240216 16:05:53.140451: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 124619: 35.2514, expected 46.6975 20240216 16:05:53.140461: E external/xla/xla/service/gpu/triton_autotuner.cc:816] Results do not match the reference. This is likely a bug/unexpected loss of precision. 20240216 16:05:53.859318: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 3557: 12.5601, expected 8.76762 20240216 16:05:53.859371: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 7281: 17.2671, expected 26.1401 20240216 16:05:53.859473: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 31490: 46.9089, expected 35.7881 20240216 16:05:53.859524: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 42098: 92.8985, expected 82.6593 20240216 16:05:53.859673: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 77426: 14.7896, expected 10.7041 20240216 16:05:53.859764: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 99275: 61.807, expected 51.012 20240216 16:05:53.859783: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 101673: 54.8168, expected 64.0151 20240216 16:05:53.859829: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 111729: 23.3462, expected 28.2862 20240216 16:05:53.859848: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 114033: 54.0808, expected 44.5037 20240216 16:05:53.859896: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 124619: 35.2514, expected 46.6975 20240216 16:05:53.859906: E external/xla/xla/service/gpu/triton_autotuner.cc:816] Results do not match the reference. This is likely a bug/unexpected loss of precision. 20240216 16:05:54.787867: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 3557: 12.5601, expected 8.76762 20240216 16:05:54.787922: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 7281: 17.2671, expected 26.1401 20240216 16:05:54.788026: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 31490: 46.9089, expected 35.7881 20240216 16:05:54.788074: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 42098: 92.8985, expected 82.6593 20240216 16:05:54.788212: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 77426: 14.7896, expected 10.7041 20240216 16:05:54.788305: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 99275: 61.807, expected 51.012 20240216 16:05:54.788323: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 101673: 54.8168, expected 64.0151 20240216 16:05:54.788369: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 111729: 23.3462, expected 28.2862 20240216 16:05:54.788388: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 114033: 54.0808, expected 44.5037 20240216 16:05:54.788438: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 124619: 35.2514, expected 46.6975 20240216 16:05:54.788448: E external/xla/xla/service/gpu/triton_autotuner.cc:816] Results do not match the reference. This is likely a bug/unexpected loss of precision. 20240216 16:05:54.808215: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 3557: 12.5601, expected 8.76762 20240216 16:05:54.808259: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 7281: 17.2671, expected 26.1401 20240216 16:05:54.808362: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 31490: 46.9089, expected 35.7881 20240216 16:05:54.808413: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 42098: 92.8985, expected 82.6593 20240216 16:05:54.808551: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 77426: 14.7896, expected 10.7041 20240216 16:05:54.808652: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 99275: 61.807, expected 51.012 20240216 16:05:54.808672: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 101673: 54.8168, expected 64.0151 20240216 16:05:54.808720: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 111729: 23.3462, expected 28.2862 20240216 16:05:54.808739: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 114033: 54.0808, expected 44.5037 20240216 16:05:54.808787: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 124619: 35.2514, expected 46.6975 20240216 16:05:54.808796: E external/xla/xla/service/gpu/triton_autotuner.cc:816] Results do not match the reference. This is likely a bug/unexpected loss of precision. 20240216 16:05:55.033790: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 3557: 12.5601, expected 8.76762 20240216 16:05:55.033834: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 7281: 17.2671, expected 26.1401 20240216 16:05:55.033938: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 31490: 46.9089, expected 35.7881 20240216 16:05:55.033990: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 42098: 92.8985, expected 82.6593 20240216 16:05:55.034134: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 77426: 14.7896, expected 10.7041 20240216 16:05:55.034223: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 99275: 61.807, expected 51.012 20240216 16:05:55.034242: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 101673: 54.8168, expected 64.0151 20240216 16:05:55.034290: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 111729: 23.3462, expected 28.2862 20240216 16:05:55.034309: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 114033: 54.0808, expected 44.5037 20240216 16:05:55.034357: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 124619: 35.2514, expected 46.6975 20240216 16:05:55.034367: E external/xla/xla/service/gpu/triton_autotuner.cc:816] Results do not match the reference. This is likely a bug/unexpected loss of precision. 20240216 16:05:55.078042: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 3557: 12.5601, expected 8.76762 20240216 16:05:55.078084: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 7281: 17.2671, expected 26.1401 20240216 16:05:55.078190: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 31490: 46.9089, expected 35.7881 20240216 16:05:55.078240: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 42098: 92.8985, expected 82.6593 20240216 16:05:55.078378: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 77426: 14.7896, expected 10.7041 20240216 16:05:55.078468: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 99275: 61.807, expected 51.012 20240216 16:05:55.078487: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 101673: 54.8168, expected 64.0151 20240216 16:05:55.078533: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 111729: 23.3462, expected 28.2862 20240216 16:05:55.078551: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 114033: 54.0808, expected 44.5037 20240216 16:05:55.078607: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 124619: 35.2514, expected 46.6975 20240216 16:05:55.078616: E external/xla/xla/service/gpu/triton_autotuner.cc:816] Results do not match the reference. This is likely a bug/unexpected loss of precision. 20240216 16:05:55.551993: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 3557: 12.5601, expected 8.76762 20240216 16:05:55.552042: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 7281: 17.2671, expected 26.1401 20240216 16:05:55.552145: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 31490: 46.9089, expected 35.7881 20240216 16:05:55.552198: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 42098: 92.8985, expected 82.6593 20240216 16:05:55.552341: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 77426: 14.7896, expected 10.7041 20240216 16:05:55.552430: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 99275: 61.807, expected 51.012 20240216 16:05:55.552449: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 101673: 54.8168, expected 64.0151 20240216 16:05:55.552497: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 111729: 23.3462, expected 28.2862 20240216 16:05:55.552515: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 114033: 54.0808, expected 44.5037 20240216 16:05:55.552570: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 124619: 35.2514, expected 46.6975 20240216 16:05:55.552579: E external/xla/xla/service/gpu/triton_autotuner.cc:816] Results do not match the reference. This is likely a bug/unexpected loss of precision. 20240216 16:05:55.930834: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 3557: 12.4384, expected 8.76762 20240216 16:05:55.930882: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 7281: 17.2967, expected 26.1401 20240216 16:05:55.930988: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 31490: 46.8611, expected 35.7881 20240216 16:05:55.931036: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 42098: 92.9786, expected 82.6593 20240216 16:05:55.931174: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 77426: 14.8248, expected 10.7041 20240216 16:05:55.931266: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 99275: 61.8658, expected 51.012 20240216 16:05:55.931285: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 101673: 54.8564, expected 64.0151 20240216 16:05:55.931333: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 111729: 23.2874, expected 28.2862 20240216 16:05:55.931352: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 114033: 54.0519, expected 44.5037 20240216 16:05:55.931401: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 124619: 35.3099, expected 46.6975 20240216 16:05:55.931411: E external/xla/xla/service/gpu/triton_autotuner.cc:816] Results do not match the reference. This is likely a bug/unexpected loss of precision. 20240216 16:05:56.905503: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 3557: 12.4314, expected 8.76762 20240216 16:05:56.905563: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 7281: 17.3356, expected 26.1401 20240216 16:05:56.905669: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 31490: 46.9041, expected 35.7881 20240216 16:05:56.905718: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 42098: 92.9747, expected 82.6593 20240216 16:05:56.905856: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 77426: 14.8142, expected 10.7041 20240216 16:05:56.905945: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 99275: 61.8406, expected 51.012 20240216 16:05:56.905964: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 101673: 54.7939, expected 64.0151 20240216 16:05:56.906010: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 111729: 23.2614, expected 28.2862 20240216 16:05:56.906029: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 114033: 54.0424, expected 44.5037 20240216 16:05:56.906077: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 124619: 35.2993, expected 46.6975 20240216 16:05:56.906086: E external/xla/xla/service/gpu/triton_autotuner.cc:816] Results do not match the reference. This is likely a bug/unexpected loss of precision. 20240216 16:05:57.417363: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 3557: 12.5601, expected 8.76762 20240216 16:05:57.417414: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 7281: 17.2671, expected 26.1401 20240216 16:05:57.417517: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 31490: 46.9089, expected 35.7881 20240216 16:05:57.417572: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 42098: 92.8985, expected 82.6593 20240216 16:05:57.417726: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 77426: 14.7896, expected 10.7041 20240216 16:05:57.417815: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 99275: 61.807, expected 51.012 20240216 16:05:57.417834: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 101673: 54.8168, expected 64.0151 20240216 16:05:57.417882: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 111729: 23.3462, expected 28.2862 20240216 16:05:57.417901: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 114033: 54.0808, expected 44.5037 20240216 16:05:57.417951: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 124619: 35.2514, expected 46.6975 20240216 16:05:57.417961: E external/xla/xla/service/gpu/triton_autotuner.cc:816] Results do not match the reference. This is likely a bug/unexpected loss of precision. 20240216 16:05:57.460488: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 3557: 12.5601, expected 8.76762 20240216 16:05:57.460531: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 7281: 17.2671, expected 26.1401 20240216 16:05:57.460638: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 31490: 46.9089, expected 35.7881 20240216 16:05:57.460692: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 42098: 92.8985, expected 82.6593 20240216 16:05:57.460830: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 77426: 14.7896, expected 10.7041 20240216 16:05:57.460919: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 99275: 61.807, expected 51.012 20240216 16:05:57.460938: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 101673: 54.8168, expected 64.0151 20240216 16:05:57.460985: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 111729: 23.3462, expected 28.2862 20240216 16:05:57.461003: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 114033: 54.0808, expected 44.5037 20240216 16:05:57.461052: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 124619: 35.2514, expected 46.6975 20240216 16:05:57.461062: E external/xla/xla/service/gpu/triton_autotuner.cc:816] Results do not match the reference. This is likely a bug/unexpected loss of precision. 20240216 16:05:57.520325: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 3557: 12.5601, expected 8.76762 20240216 16:05:57.520366: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 7281: 17.2671, expected 26.1401 20240216 16:05:57.520472: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 31490: 46.9089, expected 35.7881 20240216 16:05:57.520523: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 42098: 92.8985, expected 82.6593 20240216 16:05:57.520675: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 77426: 14.7896, expected 10.7041 20240216 16:05:57.520765: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 99275: 61.807, expected 51.012 20240216 16:05:57.520784: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 101673: 54.8168, expected 64.0151 20240216 16:05:57.520830: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 111729: 23.3462, expected 28.2862 20240216 16:05:57.520849: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 114033: 54.0808, expected 44.5037 20240216 16:05:57.520897: E external/xla/xla/service/gpu/buffer_comparator.cc:149] Difference at 124619: 35.2514, expected 46.6975 20240216 16:05:57.520907: E external/xla/xla/service/gpu/triton_autotuner.cc:816] Results do not match the reference. This is likely a bug/unexpected loss of precision. ``` The code for the Neural Network layer that seems to be at fault more or less (I'm giving a simplified version without inheritance etc.) comes down to ``` import jax jax.config.update(""jax_default_device"", jax.devices('cpu')[0]) from jax import numpy as jnp import equinox as eqx class SirenLayer(eqx.Module):     weights: jax.Array     biases: jax.Array     activation_kwargs: dict = eqx.field(static=True)     def __call__(self, x):         wxb = (self.weights + self.biases, )         return self.activation_function(*wxb)     def activation_function(self, *args):         return self._activation_function(*args, **self.activation_kwargs)          def _activation_function(x, w0):         return jnp.sin(w0*x)          def from_config(cls, in_size:int, out_size:int, *, key:jax.Array, is_first_layer:bool, w0:float):         w_key, b_key = jax.random.split(key)         if is_first_layer:             lim = 1./in_size from https://github.com/vsitzmann/siren/blob/4df34baee3f0f9c8f351630992c1fe1f69114b5f/modules.pyL630         else:             lim = jnp.sqrt(6./in_size)/w0   from https://arxiv.org/pdf/2006.09661.pdf subsection.3.2 and appendix 1.5 and https://github.com/vsitzmann/siren/blob/4df34baee3f0f9c8f351630992c1fe1f69114b5f/modules.pyL627         weight = jax.random.uniform(             key=w_key,             shape=(out_size, in_size),             minval=lim,              maxval=lim             )         bias = jax.random.uniform(             key=b_key,             shape=(out_size,),             minval=1,             maxval=1         )         bias_factor = jnp.pi/jnp.sqrt(jnp.sum(jnp.square(weight), axis=1))  from https://arxiv.org/pdf/2102.02611.pdf page 6 third paragaph         bias = bias_factor * bias         return cls(weight, bias, {'w0': w0}) ``` One of the networks that would sometimes give this error had an input size of 3, a linear output layer with output size 1, a hidden size of 384, and 6 layers total (including final linear layer and input layer). The value of w0 used was 16.  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.24 jaxlib: 0.4.24 numpy:  1.26.4 python: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] jax.devices (1 total, 1 local): [cuda(id=0)] process_count: 1 $ nvidiasmi Tue Mar  5 16:06:24 2024 ++  ++  ``` The version of Equinox I use (in case it's important) is 0.11.3.",2024-03-05T15:43:57Z,bug,open,1,5,https://github.com/jax-ml/jax/issues/20075,"I'm also seeing this error when using `sine` activations. However, it appears to only occur when `jitting` the function.",I'm running into the same issue when using sine activation function. Just wanted to check if you guys were able to find any solution to this? Thanks!,I used this repo instead of my network and no longer had the issue. Perhaps something to do with how the network was defined. ,"The problem disappeared for me after updating JAX at some point. No idea what changed, but I'm happy my code works 😅 Op do 5 sep 2024 om 18:42 schreef Mukesh Ghimire ***@***.***>: > I'm running into the same issue when using sine activation function. Just > wanted to check if you guys were able to find any solution to this? Thanks! > > — > Reply to this email directly, view it on GitHub > , or > unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >",Just curious if the models performed differently when you had the issue vs now? Appreciate you responding.  I'm still getting the error when the weight $w_0 = 30$ in the activation. 
yi,Pallas slicing makes Jupyter Kernel Crash," Description Hi there, I am trying to experiment with slicing in Pallas and I noticed a strange behavior that I can't explain. The following code works fine. ``` from functools import partial import jax from jax.experimental import pallas as pl import jax.numpy as jnp import numpy as np def add_vectors_kernel(x_ref, y_ref, o_ref):     x, y = x_ref[slice(1,5)], y_ref[slice(1,5)]     o_ref[slice(1,5)] = x * y .jit def add_vectors(x: jax.Array, y: jax.Array) > jax.Array:     return pl.pallas_call(add_vectors_kernel,         out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype)     )(x, y) add_vectors(jnp.arange(8), jnp.arange(8)) ``` But if I just change the boundaries of my slicing, my Jupyter kernel crashes: ``` from functools import partial import jax from jax.experimental import pallas as pl import jax.numpy as jnp import numpy as np def add_vectors_kernel(x_ref, y_ref, o_ref):     x, y = x_ref[slice(2,5)], y_ref[slice(2,5)]     o_ref[slice(2,5)] = x * y .jit def add_vectors(x: jax.Array, y: jax.Array) > jax.Array:     return pl.pallas_call(add_vectors_kernel,         out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype)     )(x, y) add_vectors(jnp.arange(8), jnp.arange(8)) ``` !Screenshot from 20240304 102128 Any ideas and especially some guidelines when dealing with slicing?  System info (python version, jaxlib version, accelerator, etc.) jax=0.4.25 jaxlib=0.4.25+cuda11.cudnn86 jaxtyping=0.2.25 Python=3.11.8",2024-03-04T09:23:10Z,bug,closed,0,8,https://github.com/jax-ml/jax/issues/20056,"Didn't have a chance to take a really close look but my guess is that you are slicing a (3,)sized array which Triton doesn't like (I think they have to be powersof2 shaped).","Interesting, isn't that like a massive downside when doing GPU operations, to only deal with powersof2 shaped arrays?","You can use masking to handle non powers of 2. Check out the `mask` and `other` argument to `pl.load`. if you want to operate on a, say, (5,)shaped array, you can load an (8,) sized block but use a mask that pads the last three elements.","Thank you very much, this makes more sense now!","``` key = jax.random.PRNGKey(65) other = jax.random.normal(key, (10, 10)) diags = jax.random.normal(key, (1, 10)) offsets = 2 def sub_process(offset, diag, other, other_shape, N):     print(diag.shape)     out = jnp.zeros_like(other)     start = max(0, offset)     end = min(N, N + offset)     top = max(0, offset)     bottom = top  + end  start     sliced_diag = jax.lax.dynamic_slice_in_dim(diag, start, endstart).reshape(1, 1)     sliced_other = jax.lax.dynamic_slice(other, (start, 0), (endstart, other_shape))     out = out.at[top:bottom, :].add(         sliced_diag * sliced_other     )     return out def dia_matmul_kernel_vmap(diags_ref, offsets, other_ref, o_ref):     diags, other = diags_ref[...], other_ref[...]     print(diags_ref)     N = other.shape[0]     other_shape = other_ref.shape[1]     batched_output = jax.vmap(sub_process, in_axes=(None, 0, None, None, None), out_axes=0)(offsets, diags, other, other_shape, N) .partial(jax.jit, static_argnames=[""offsets""]) def dia_matmul(diags: Array, offsets: tuple[int],other:Array) > Array:     return pl.pallas_call(         lambda diags, other, o_ref: dia_matmul_kernel_vmap(diags, offsets, other, o_ref),         out_shape=jax.ShapeDtypeStruct(other.shape, other.dtype),     )(diags, other) dia_matmul(diags, offsets, other) ``` For some reason this also crashes the kernel, if you have any ideas why, it would be greatly appreciated.","`dynamic_slice` isn't supported as well, unfortunately.",I see so  `pl.load` is the only way to handle any sort of slicing with Pallas? ,"On GPU, yes."
llm,Matrix-vector multiply: ValueError: all dimensions of x and y must be >= 16 ," Description A simple matrixvector multiply Pallas implementation fails: ``` def matrix_multiply(x_ref, y_ref, o_ref):     o_ref[:, :] = x_ref[:, :] @ y_ref[:, :] .jit def matmul(x: jax.Array, y: jax.Array) > jax.Array:     return pl.pallas_call(matrix_multiply,                           out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), x.dtype),                            interpret=True                           )(x, y) k1, k2 = jax.random.split(jax.random.PRNGKey(0)) x = jax.random.normal(k1, (32, 32)) y = jax.random.normal(k2, (32, 1))  print(matmul(x, y)) ``` Error: ValueError: all dimensions of x and y must be >= 16  ...       dimension_numbers=(([1], [0]), ([], []))       preferred_element_type=float32     ] d e     c[:,:] < f   in () } Interestingly, changing the dimensions of y to (32, 16) or (32, 32) works fine, while (32, 8) still fails. There is no issue in the interpret=True mode. What exactly does the interpret mode do?  Thank you.   System info (python version, jaxlib version, accelerator, etc.) jaxlib=0.4.25 python=3.9 GPU=RTX4090",2024-03-02T17:18:11Z,question pallas,closed,0,2,https://github.com/jax-ml/jax/issues/20051,"This is a limitation of triton, not a pallas thing I think, see here: https://github.com/openai/triton/issues/1815","Thanks for the question! At first I thought we needed to improve the error, but actually ""all dimensions of x and y must be >=16"" seems pretty good! So I think we can close this issue, unless you have a specific recommendation for how we should improve the error message. > What exactly does the interpret mode do? Instead of lowering to Triton, it runs the computation in XLA:HLO. That means it doesn't have the same physical semantics as running in Triton, but it can be easier to debug."
rag,"[mutable-arrays] move MutableArray, add eager, improve tests, fix bug","1. move MutableArray to core.py, and some handlers to their respective files 2. fix a bug in aliasing setup (it was just broken before, now better test coverage) 3. add eager support by enabling get_p, swap_p, and addupdate_p impls 4. improve tests slightly",2024-03-01T19:13:44Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/20044
yi,Marking non-trainable / frozen parameters,"Is there a recommended way for tagging an array as not trainable? Specifically in the case where it may not be known beforehand that we do not wish to train the parameter (i.e. so ``stop_gradient`` is not coded into the model). I am also aware that e.g. optax allows specifying which parameters are trainable, but in many cases it would be much simpler to tag the arrays in someway, rather than specifying the trainable parameters using optax. Option 1: make use of duck typing with ``__jax_array__``,  (from https://github.com/google/jax/issues/10065), this is experimental and seems to be at risk of being removed. ``` import typing import jax.numpy as jnp from jax.lax import stop_gradient class Buffer(typing.NamedTuple):     array: jnp.ndarray     def __jax_array__(self):         return stop_gradient(self.array) ``` Option 2:  add an attribute to the arrays, and use this to partition the model e.g. using equinox ``` import equinox as eqx import jax import jax.numpy as jnp import equinox as eqx def make_my_module():     array = jnp.array([3.])     frozen_array = jnp.array([3.])     frozen_array.is_frozen = True     return (array, frozen_array) def f(diff, static):     module = eqx.combine(diff, static)     return jnp.sum(module[0]) + jnp.sum(module[1]) def partition_fn(leaf):     if eqx.is_inexact_array(leaf):         if hasattr(leaf, ""is_frozen""):             return not leaf.is_frozen         return True     return False my_module = make_my_module() val, grad = jax.value_and_grad(f)(*eqx.partition(my_module, filter_spec=partition_fn)) expected_val = 6 expected_grad = (1, None) assert val==expected_val assert grad==expected_grad  But this attribute might not be maintained e.g. under vmap my_module_vmapped = jax.vmap(make_my_module, axis_size=1)() val, grad = jax.value_and_grad(f)(     *eqx.partition(my_module_vmapped, filter_spec=partition_fn)     ) assert grad!=expected_grad ``` Option 2 seems like it will have some issues, e.g. losing the attributed when constructed with vmap. Is there a recommended way to achieve this?",2024-02-28T17:36:40Z,enhancement,closed,0,3,https://github.com/jax-ml/jax/issues/20012,"JAX itself doesn't have any notion of model training, so the answer to your question would depend on what framework you're using. It looks like you're using `equinox`, so you may find more useful answers by asking at https://github.com/patrickkidger/equinox.","Ok thanks. I guess more broadly, is there a way to associate metadata with an array without duck typing? ","No, not really. The only supported way to associate metadata with an array would be using a pytree (see https://jax.readthedocs.io/en/latest/pytrees.html), but there's no way to make a pytree ducktype as an array (note that `__jax_array__`, despite existing in some places, is not meant as a public API and is not fully supported throughout the package)."
yi,Pallas Tutorial outputs RESOURCE_EXHAUSTED," Description Hi there, I am trying to learn Pallas by running the code bits from the official documentation from Jax but I am having trouble understanding why I'm short on GPU memory. ``` from functools import partial import jax from jax.experimental import pallas as pl import jax.numpy as jnp import numpy as np def matmul_kernel(x_ref, y_ref, z_ref):   z_ref[...] = x_ref[...] @ y_ref[...] def matmul(x: jax.Array, y: jax.Array):   return pl.pallas_call(     matmul_kernel,     out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), x.dtype),     grid=(2, 2),     in_specs=[       pl.BlockSpec(lambda i, j: (i, 0), (x.shape[0] // 2, x.shape[1])),       pl.BlockSpec(lambda i, j: (0, j), (y.shape[0], y.shape[1] // 2))     ],     out_specs=pl.BlockSpec(       lambda i, j: (i, j), (x.shape[0] // 2, y.shape[1] // 2)     )   )(x, y) k1, k2 = jax.random.split(jax.random.PRNGKey(43)) x = jax.random.normal(k1, (1024, 1024)) y = jax.random.normal(k2, (1024, 1024)) z = matmul(x, y) np.testing.assert_allclose(z, x @ y)  ``` This code which is copy pasted from the documentation (except for the PRNGKey) is giving me the following error: `XlaRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 4194304, available: 101376` I saw that to manage memory allocation, we can set 3 environment variables to skip the part where Jax preallocates 75% of the memory, but it is still giving me the error. Any ideas on this? First time really diving into GPUs so I might not have all the theoretical concepts.  System info (python version, jaxlib version, accelerator, etc.) jax=0.4.25 jaxlib=0.4.25+cuda11.cudnn86 jaxtyping=0.2.25 python=3.11.8",2024-02-28T09:54:54Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/20008,"Why did I close the issue? Just realised someone else was on the shared GPU, I ran the code with smaller matrices and it works fine."
llm,Vectorised operation on string arrays?,"I know this has been discussed already (pandax), but maybe it comes back as LLMs usage grows. I have the following problem.   I have a big array of `jnp.int32` elements, for example, of shape (2000, 10, 100, 100).  Each id in the array can be mapped to its character `char(id)`, ideally.  However, JAX does not support string arrays.  Is there any way to vectorise/parallelise the computation?",2024-02-28T09:41:11Z,enhancement,closed,0,1,https://github.com/jax-ml/jax/issues/20007,"Hi  unfortunately, since XLA does not have any support for string types, JAX cannot provide any vectorized string operations. There are a few options in this case: First, you could create a hostside mapping of integers to strings, and use JAX to manipulate the array of integers. This is effectively what the `pandax` experiment does under the hood. Once the integer manipulations are finished, you could map those integers back to strings on the host. This may or may not be possible for the particular string manipulations you have in mind. The only other option would be to do all your string manipulation on the host, for example using `pandas`. This is unlikely to change in the future, because string support is not on the roadmap for XLA. Hope that helps!"
rag,Fix unnecessary memory copies between GPU and CPU when `jax2tf.call_tf()` is used.,"Fix unnecessary memory copies between GPU and CPU when `jax2tf.call_tf()` is used.  The root cause of the bug is that dtype lookups are incorrect because hashes behave differently between dtype instances and their types. Added comments to `jax.dlpack.SUPPORTED_DTYPES` about this.  Added unit test coverage.  Fixing this bug revealed a limitation of causing ""hosttodevice"" copy in the following two situations. See the details in the unit test comments.:    When the dtype is 'int32'.    When using PJRT C API runtime.",2024-02-27T17:02:41Z,,closed,0,0,https://github.com/jax-ml/jax/issues/19996
yi,"XLA ""cannot remove instruction"" when compiling big MoE model "," Description Hi, i've implemented MoE transformer which is configurable by some hyperparams (number of experts, layers, hidden dim, etc.) and i'm training such model on GPU cluster (H100 GPUs) using model, expert and data parallelism. It works perfectly with smalltomedium sizes (125M, 1B, 7B backbones x 16 experts > 1B, 9B, 70B total params), but when i'm trying to train big model (30B backbone, 16 experts, 300B params) using exactly same code and i'm getting very strange error when compiling its train step: ``` jaxlib.xla_extension.XlaRuntimeError: INTERNAL: RET_CHECK failure (external/xla/xla/hlo/ir/hlo_computation.cc:425) ignore_safety_check  ++ ```",2024-02-27T15:16:27Z,bug,closed,2,3,https://github.com/jax-ml/jax/issues/19994,https://github.com/openxla/xla/issues/10013,The problem is still present in JAX 0.4.25,Hasn't tested yet but it must have been fixed in  https://github.com/openxla/xla/issues/10013issuecomment2004969257
agent,Errors when building on AMD GPU," Description Hi！I am very interested in using STARRED (a python package designed for astronomical data processing with GPU support). However, it depends on jax and jaxlib. I followed the instructions from your documentation website and build with the following scripts: `python build/build.py enable_rocm rocm_path=/opt/rocm6.0.2   bazel_options=override_repository=xla=/home/phylmf/lib/xla ` I am sure the rocm is prepared well and I can use pytorch and stable diffusion with no troubles.  Anyway, the building process stopped and reports: ``` [637 / 2,481] Compiling xla/hlo/evaluator/hlo_evaluator.cc; 16s local ... (12 actions running) [640 / 2,481] Compiling xla/hlo/evaluator/hlo_evaluator.cc; 18s local ... (12 actions, 11 running) [640 / 2,481] Compiling xla/hlo/evaluator/hlo_evaluator.cc; 19s local ... (12 actions running) ERROR: /home/phylmf/.cache/bazel/_bazel_phylmf/23e2a325a95637686413138cca4e49b3/external/xla/xla/service/gpu/BUILD:1434:23: Compiling xla/service/gpu/cub_sort_kernel.cu.: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command (from target //xla/service/gpu:cub_sort_kernel_u64_b64)    (cd /home/phylmf/.cache/bazel/_bazel_phylmf/23e2a325a95637686413138cca4e49b3/execroot/__main__ && \   exec env  \     LD_LIBRARY_PATH=/home/phylmf/lib/MultiNest/lib/:/home/phylmf/anaconda3/pkgs/mpi1.0mpich/lib/ \     PATH=/home/phylmf/lib/idl_lib/idlutils/bin:/home/phylmf/anaconda3/bin:/home/phylmf/anaconda3/condabin:/home/phylmf/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/snap/bin \     PWD=/proc/self/cwd \     ROCM_PATH=/opt/rocm6.0.2 \     TF_ROCM_AMDGPU_TARGETS=gfx900,gfx906,gfx908,gfx90a,gfx1030 \   external/local_config_rocm/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc U_FORTIFY_SOURCE fstackprotector Wall Wunusedbutsetparameter Wnofreenonheapobject fnoomitframepointer g0 O2 'D_FORTIFY_SOURCE=1' DNDEBUG ffunctionsections fdatasections 'std=c++14' MD MF bazelout/k8opt/bin/external/xla/xla/service/gpu/_objs/cub_sort_kernel_u64_b64/cub_sort_kernel.cu.pic.d 'frandomseed=bazelout/k8opt/bin/external/xla/xla/service/gpu/_objs/cub_sort_kernel_u64_b64/cub_sort_kernel.cu.pic.o' fPIC 'DEIGEN_MAX_ALIGN_BYTES=64' DEIGEN_ALLOW_UNALIGNED_SCALARS 'DEIGEN_USE_AVX512_GEMM_KERNELS=0' 'DTENSORFLOW_USE_ROCM=1' DCUB_TYPE_U64_B64 'DBAZEL_CURRENT_REPOSITORY=""xla""' iquote external/xla iquote bazelout/k8opt/bin/external/xla iquote external/eigen_archive iquote bazelout/k8opt/bin/external/eigen_archive iquote external/tsl iquote bazelout/k8opt/bin/external/tsl iquote external/local_config_rocm iquote bazelout/k8opt/bin/external/local_config_rocm isystem external/eigen_archive isystem bazelout/k8opt/bin/external/eigen_archive isystem external/eigen_archive/mkl_include isystem bazelout/k8opt/bin/external/eigen_archive/mkl_include isystem external/local_config_rocm/rocm isystem bazelout/k8opt/bin/external/local_config_rocm/rocm isystem external/local_config_rocm/rocm/rocm/include/hipcub isystem bazelout/k8opt/bin/external/local_config_rocm/rocm/rocm/include/hipcub isystem external/local_config_rocm/rocm/rocm/include/rocprim isystem bazelout/k8opt/bin/external/local_config_rocm/rocm/rocm/include/rocprim isystem external/local_config_rocm/rocm/rocm/include isystem bazelout/k8opt/bin/external/local_config_rocm/rocm/rocm/include isystem external/local_config_rocm/rocm/rocm/include/rocrand isystem bazelout/k8opt/bin/external/local_config_rocm/rocm/rocm/include/rocrand isystem external/local_config_rocm/rocm/rocm/include/roctracer isystem bazelout/k8opt/bin/external/local_config_rocm/rocm/rocm/include/roctracer 'fvisibility=hidden' Wnosigncompare Wnounknownwarningoption Wnostringoptruncation Wnoarrayparameter 'DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir.' mavx 'std=c++17' x rocm 'amdgputarget=gfx900' 'amdgputarget=gfx906' 'amdgputarget=gfx908' 'amdgputarget=gfx90a' 'amdgputarget=gfx1030' fnocanonicalsystemheaders Wnobuiltinmacroredefined 'D__DATE__=""redacted""' 'D__TIMESTAMP__=""redacted""' 'D__TIME__=""redacted""' 'DTENSORFLOW_USE_ROCM=1' D__HIP_PLATFORM_AMD__ DEIGEN_USE_HIP nocanonicalprefixes fnocanonicalsystemheaders c external/xla/xla/service/gpu/cub_sort_kernel.cu./k8opt/bin/external/xla/xla/service/gpu/_objs/cub_sort_kernel_u64_b64/cub_sort_kernel.cu.pic.o)  Configuration: 2fefc4c2633450a982f0e7bdbf0123ec0a48cb805d729dfac1feb2058e0090a6  Execution platform: //:platform clang: warning: argument unused during compilation: 'fgpuflushdenormalstozero' [Wunusedcommandlineargument] error: Illegal instruction detected: Invalid dpp_ctrl value: broadcasts are not supported on GFX10+ renamable $vgpr52 = V_MOV_B32_dpp undef $vgpr52(tieddef 0), $vgpr12, 322, 15, 15, 0, implicit $exec error: Illegal instruction detected: Invalid dpp_ctrl value: broadcasts are not supported on GFX10+ renamable $vgpr4 = V_MOV_B32_dpp undef $vgpr4(tieddef 0), killed $vgpr3, 322, 15, 15, 0, implicit $exec error: Illegal instruction detected: Invalid dpp_ctrl value: broadcasts are not supported on GFX10+ renamable $vgpr3 = V_MOV_B32_dpp undef $vgpr3(tieddef 0), $vgpr2, 322, 15, 15, 0, implicit $exec error: Illegal instruction detected: Invalid dpp_ctrl value: broadcasts are not supported on GFX10+ renamable $vgpr7 = V_MOV_B32_dpp undef $vgpr7(tieddef 0), killed $vgpr0, 322, 15, 15, 0, implicit $exec error: Illegal instruction detected: Invalid dpp_ctrl value: broadcasts are not supported on GFX10+ renamable $vgpr0 = V_MOV_B32_dpp undef $vgpr0(tieddef 0), $vgpr4, 322, 15, 15, 0, implicit $exec error: Illegal instruction detected: Invalid dpp_ctrl value: broadcasts are not supported on GFX10+ renamable $vgpr44 = V_MOV_B32_dpp undef $vgpr44(tieddef 0), $vgpr42, 322, 15, 15, 0, implicit $exec error: Illegal instruction detected: Invalid dpp_ctrl value: broadcasts are not supported on GFX10+ renamable $vgpr47 = V_MOV_B32_dpp undef $vgpr47(tieddef 0), $vgpr43, 322, 15, 15, 0, implicit $exec error: Illegal instruction detected: Invalid dpp_ctrl value: broadcasts are not supported on GFX10+ renamable $vgpr40 = V_MOV_B32_dpp undef $vgpr40(tieddef 0), $vgpr38, 322, 15, 15, 0, implicit $exec error: Illegal instruction detected: Invalid dpp_ctrl value: broadcasts are not supported on GFX10+ renamable $vgpr43 = V_MOV_B32_dpp undef $vgpr43(tieddef 0), $vgpr39, 322, 15, 15, 0, implicit $exec error: Illegal instruction detected: Invalid dpp_ctrl value: broadcasts are not supported on GFX10+ renamable $vgpr44 = V_MOV_B32_dpp undef $vgpr44(tieddef 0), $vgpr42, 322, 15, 15, 0, implicit $exec error: Illegal instruction detected: Invalid dpp_ctrl value: broadcasts are not supported on GFX10+ renamable $vgpr47 = V_MOV_B32_dpp undef $vgpr47(tieddef 0), $vgpr43, 322, 15, 15, 0, implicit $exec error: Illegal instruction detected: Invalid dpp_ctrl value: broadcasts are not supported on GFX10+ renamable $vgpr40 = V_MOV_B32_dpp undef $vgpr40(tieddef 0), $vgpr38, 322, 15, 15, 0, implicit $exec error: Illegal instruction detected: Invalid dpp_ctrl value: broadcasts are not supported on GFX10+ renamable $vgpr43 = V_MOV_B32_dpp undef $vgpr43(tieddef 0), $vgpr39, 322, 15, 15, 0, implicit $exec error: Illegal instruction detected: Invalid dpp_ctrl value: broadcasts are not supported on GFX10+ renamable $vgpr52 = V_MOV_B32_dpp undef $vgpr52(tieddef 0), $vgpr20, 322, 15, 15, 0, implicit $exec error: Illegal instruction detected: Invalid dpp_ctrl value: broadcasts are not supported on GFX10+ renamable $vgpr44 = V_MOV_B32_dpp undef $vgpr44(tieddef 0), $vgpr42, 322, 15, 15, 0, implicit $exec error: Illegal instruction detected: Invalid dpp_ctrl value: broadcasts are not supported on GFX10+ renamable $vgpr47 = V_MOV_B32_dpp undef $vgpr47(tieddef 0), $vgpr43, 322, 15, 15, 0, implicit $exec error: Illegal instruction detected: Invalid dpp_ctrl value: broadcasts are not supported on GFX10+ renamable $vgpr40 = V_MOV_B32_dpp undef $vgpr40(tieddef 0), $vgpr38, 322, 15, 15, 0, implicit $exec error: Illegal instruction detected: Invalid dpp_ctrl value: broadcasts are not supported on GFX10+ renamable $vgpr43 = V_MOV_B32_dpp undef $vgpr43(tieddef 0), $vgpr39, 322, 15, 15, 0, implicit $exec error: Illegal instruction detected: Invalid dpp_ctrl value: broadcasts are not supported on GFX10+ renamable $vgpr44 = V_MOV_B32_dpp undef $vgpr44(tieddef 0), $vgpr42, 322, 15, 15, 0, implicit $exec fatal error: too many errors emitted, stopping now [ferrorlimit=] renamable $vgpr47 = V_MOV_B32_dpp undef $vgpr47(tieddef 0), $vgpr43, 322, 15, 15, 0, implicit $exec renamable $vgpr40 = V_MOV_B32_dpp undef $vgpr40(tieddef 0), $vgpr38, 322, 15, 15, 0, implicit $exec renamable $vgpr43 = V_MOV_B32_dpp undef $vgpr43(tieddef 0), $vgpr39, 322, 15, 15, 0, implicit $exec 20 errors generated when compiling for gfx1034. Target //jaxlib/tools:build_wheel failed to build INFO: Elapsed time: 307.781s, Critical Path: 46.77s INFO: 650 processes: 13 internal, 637 local. FAILED: Build did NOT complete successfully ERROR: Build failed. Not running target Traceback (most recent call last):   File ""/home/phylmf/lib/jax/build/build.py"", line 706, in      main()   File ""/home/phylmf/lib/jax/build/build.py"", line 674, in main     shell(command)   File ""/home/phylmf/lib/jax/build/build.py"", line 45, in shell     output = subprocess.check_output(cmd)   File ""/home/phylmf/anaconda3/lib/python3.10/subprocess.py"", line 421, in check_output     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,   File ""/home/phylmf/anaconda3/lib/python3.10/subprocess.py"", line 526, in run     raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command '['/usr/bin/bazel', 'run', 'verbose_failures=true', '//jaxlib/tools:build_wheel', '', 'output_path=/home/phylmf/lib/jax/dist', 'jaxlib_git_hash=fab8f6cfdd1065d02bc48c3f7d65e31d0979d3c6', 'cpu=x86_64']' returned nonzero exit status 1. ``` Anything from you will be appreciated! Thanks a lot in making jax/jaxlib available! Mingfeng Liu Nanjing Normal University  System info (python version, jaxlib version, accelerator, etc.) (base) phylmfastroPrecision3260:~$ rocminfo ROCk module is loaded =====================     HSA System Attributes     =====================     Runtime Version:         1.1 System Timestamp Freq.:  1000.000000MHz Sig. Max Wait Duration:  18446744073709551615 (0xFFFFFFFFFFFFFFFF) (timestamp count) Machine Model:           LARGE                               System Endianness:       LITTLE                              Mwaitx:                  DISABLED DMAbuf Support:          YES ==========                HSA Agents                ==========                *******                   Agent 1                   *******                     Name:                    12th Gen Intel(R) Core(TM) i512500   Uuid:                    CPUXX                                Marketing Name:          12th Gen Intel(R) Core(TM) i512500   Vendor Name:             CPU                                   Feature:                 None specified                        Profile:                 FULL_PROFILE                          Float Round Mode:        NEAR                                  Max Queue Number:        0(0x0)                                Queue Min Size:          0(0x0)                                Queue Max Size:          0(0x0)                                Queue Type:              MULTI                                 Node:                    0                                     Device Type:             CPU                                   Cache Info:                   L1:                      49152(0xc000) KB                      Chip ID:                 0(0x0)                                ASIC Revision:           0(0x0)                                Cacheline Size:          64(0x40)                              Max Clock Freq. (MHz):   4600                                  BDFID:                   0                                     Internal Node ID:        0                                     Compute Unit:            12                                    SIMDs per CU:            0                                     Shader Engines:          0                                     Shader Arrs. per Eng.:   0                                     WatchPts on Addr. Ranges:1                                     Features:                None   Pool Info:                    Pool 1                          Segment:                 GLOBAL; FLAGS: FINE GRAINED               Size:                    32540432(0x1f08710) KB                    Allocatable:             TRUE                                      Alloc Granule:           4KB                                       Alloc Alignment:         4KB                                       Accessible by all:       TRUE                                    Pool 2                          Segment:                 GLOBAL; FLAGS: KERNARG, FINE GRAINED       Size:                    32540432(0x1f08710) KB                    Allocatable:             TRUE                                      Alloc Granule:           4KB                                       Alloc Alignment:         4KB                                       Accessible by all:       TRUE                                    Pool 3                          Segment:                 GLOBAL; FLAGS: COARSE GRAINED             Size:                    32540432(0x1f08710) KB                    Allocatable:             TRUE                                      Alloc Granule:           4KB                                       Alloc Alignment:         4KB                                       Accessible by all:       TRUE                                  ISA Info:                 *******                   Agent 2                   *******                     Name:                    gfx1030                               Uuid:                    GPUXX                                Marketing Name:          AMD Radeon RX 6400                    Vendor Name:             AMD                                   Feature:                 KERNEL_DISPATCH                       Profile:                 BASE_PROFILE                          Float Round Mode:        NEAR                                  Max Queue Number:        128(0x80)                             Queue Min Size:          64(0x40)                              Queue Max Size:          131072(0x20000)                       Queue Type:              MULTI                                 Node:                    1                                     Device Type:             GPU                                   Cache Info:                   L1:                      16(0x10) KB                             L2:                      1024(0x400) KB                          L3:                      16384(0x4000) KB                      Chip ID:                 29759(0x743f)                         ASIC Revision:           0(0x0)                                Cacheline Size:          64(0x40)                              Max Clock Freq. (MHz):   2320                                  BDFID:                   768                                   Internal Node ID:        1                                     Compute Unit:            12                                    SIMDs per CU:            2                                     Shader Engines:          1                                     Shader Arrs. per Eng.:   2                                     WatchPts on Addr. Ranges:4                                     Coherent Host Access:    FALSE                                 Features:                KERNEL_DISPATCH    Fast F16 Operation:      TRUE                                  Wavefront Size:          32(0x20)                              Workgroup Max Size:      1024(0x400)                           Workgroup Max Size per Dimension:     x                        1024(0x400)                             y                        1024(0x400)                             z                        1024(0x400)                           Max Waves Per CU:        32(0x20)                              Max Workitem Per CU:    1024(0x400)                           Grid Max Size:           4294967295(0xffffffff)                Grid Max Size per Dimension:     x                        4294967295(0xffffffff)                  y                        4294967295(0xffffffff)                  z                        4294967295(0xffffffff)                Max fbarriers/Workgrp:   32                                    Packet Processor uCode:: 116                                   SDMA engine uCode::      34                                    IOMMU Support::          None                                  Pool Info:                    Pool 1                          Segment:                 GLOBAL; FLAGS: COARSE GRAINED             Size:                    4177920(0x3fc000) KB                      Allocatable:             TRUE                                      Alloc Granule:           4KB                                       Alloc Alignment:         4KB                                       Accessible by all:       FALSE                                   Pool 2                          Segment:                 GLOBAL; FLAGS: EXTENDED FINE GRAINED       Size:                    4177920(0x3fc000) KB                      Allocatable:             TRUE                                      Alloc Granule:           4KB                                       Alloc Alignment:         4KB                                       Accessible by all:       FALSE                                   Pool 3                          Segment:                 GROUP                                     Size:                    64(0x40) KB                               Allocatable:             FALSE                                     Alloc Granule:           0KB                                       Alloc Alignment:         0KB                                       Accessible by all:       FALSE                                 ISA Info:                     ISA 1                           Name:                    amdgcnamdamdhsagfx1030                Machine Models:          HSA_MACHINE_MODEL_LARGE                   Profiles:                HSA_PROFILE_BASE                          Default Rounding Mode:   NEAR                                      Default Rounding Mode:   NEAR                                      Fast f16:                TRUE                                      Workgroup Max Size:      1024(0x400)                               Workgroup Max Size per Dimension:         x                        1024(0x400)                                 y                        1024(0x400)                                 z                        1024(0x400)                               Grid Max Size:           4294967295(0xffffffff)                    Grid Max Size per Dimension:         x                        4294967295(0xffffffff)                      y                        4294967295(0xffffffff)                      z                        4294967295(0xffffffff)                    FBarrier Max Size:       32                                  *** Done ***  ",2024-02-27T02:24:06Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/19989,"Hi, currently JAX on ROCM is supported only for MI Instinct GPUs. We are working to get support for Navi/Radeon in the near future."
yi,Conditional array update on GPU using jnp.where vs fori_loop,"**Description:**  Hi!  I am fairly new to using JAX. I have been trying to update 1 or more entries of a 1D array based on some condition inside a `jax.jit` and `jnp.vectorize` function. I managed to find a very fast way of doing this on CPU, however when I tested on GPU, it suffered from extreme slow down (around x1000). For more context, the way which seems to be fastest on CPU is similar to this code, ```python import jax from jax.lax import cond, fori_loop import jax.numpy as jnp .partial(jnp.vectorize, excluded=(1, 2, 3), signature=""()>(k)"") def fun_vectorized(r, m, n, dr):     """"""      m.size == n.size      m and n may have repeating values      N_max and M_max values are found using some functions     """"""     def update(i, args):         """"""Updates the output if required.""""""         alpha, N, result, out = args         idx = jnp.where(jnp.logical_and(m[i] == alpha, n[i] == N), i, 1)         def falseFun(args):            """""" Do nothing.""""""             _, _, out = args             return out         def trueFun(args):            """""" Update the value at idx.""""""             idx, result, out = args             out = out.at[idx].set(result)             return out         out = cond(idx >= 0, trueFun, falseFun, (idx, result, out))         return (alpha, N, result, out)         def body_inner(N, args):         alpha, out = args         result = find_some_array()  for this issue irrelevant          Update array         _, _, _, out = fori_loop(0, m.size, update, (alpha, N, result, out))         return (alpha, out)     def body(alpha, out):          Find max value of n corresponding to alpha          This requires another function which is not relevant         N_max = find_Nmax()          Loop over required n values         _, out = fori_loop(             0, (N_max + 1).astype(int), body_inner, (alpha, out)         )         return out     out = jnp.zeros(m.size)     M_max = jnp.max(m)      Loop over different m values     out = fori_loop(0, (M_max + 1).astype(int), body, (out))     return out ``` To be fair, only the `update` function is relevant to this issue, but I would like to include some other part of the code to show the architecture of my overall function. Since GPUs are not good at conditional branching, I changed my code to look like this, ```python .partial(jnp.vectorize, excluded=(1, 2, 3), signature=""()>(k)"") def fun_vectorized(r, m, n, dr):     def body_inner(N, args):         alpha, out = args         result = find_some_array()          Find which indices to update         mask = jnp.logical_and(m == alpha, n == N)         out = jnp.where(mask, result, out)         return (alpha, out)     def body(alpha, out):         N_max = find_Nmax()          Loop over required n values         _, out = fori_loop(             0, (N_max + 1).astype(int), body_inner, (alpha, out)         )         return out     out = jnp.zeros(m.size)     M_max = jnp.max(m)      Loop over different m values     out = fori_loop(0, (M_max + 1).astype(int), body, (out))     return out ``` I noticed that using `update` function with a fori_loop is really fast compared to `jnp.where` counterpart on CPU by 45 times. I would think they perform the same, but that is not the case. I can understand the slow down due to `jax.lax.cond` on GPU, but I was wondering **if there is a better way of implementing what I am trying to do?** And also, why is `jnp.where` with 3 arguments is slower than setting new values to an array? My guess is creating the copy of array instead of inplace update, but I am not sure. Some other requirements for my application:  I need to use `jnp.where` or update functions because there can be 0 or multiple indices which satisfy the condition (for strictly 1 index case is way easier to handle using `index = jnp.sum(jnp.where(jnp.logical_and(m == alpha, n == N), jnp.arange(m.size), 0))` ). I couldn't find any issue on this, I would be happy if anyone can help me with it.",2024-02-26T03:57:02Z,enhancement,closed,0,6,https://github.com/jax-ml/jax/issues/19972,"Thanks for the question! I think the issue you're running into is related to the execution model of loops (`fori_loop`, `while_loop`, and `scan`) on GPU. For GPU backends, each iteration effectively requires a kernel launch, so if you have very cheap iterations it can lead to a lot of overhead. On CPU, there is no such overhead. On GPU I'd suggest doing this kind of conditional update using `lax.select` or `jnp.where`, which is basically a more flexible wrapper to `lax.select`.","Thank you very much for the reply! Yes, on GPU jnp.where works way better than the fori_loop that basically does nothing for most of the iterations (if I was able to use ""continue"", maybe that would work but trueFun and falseFun have to return same type of things). A related question to your reply. Does every fori_loop step use a different core of the same GPU? So, basically, if each loop iteration is cheap, it is more efficient to use jnp.vectorize? Until now, I thought the problem was due to jax.lax.cond(). Thanks for the headsup. For people having similar slowdown on GPU, here is a very simple code for comparison, ```python import jax import jax.numpy as jnp from jax.lax import fori_loop .jit .vectorize def add_vec(x):     return x+5 .jit def loop_for(x):     def add_for(i,x):         return x.at[i].add(5)     x = fori_loop(0, x.size, add_for, x)     return x a = jnp.ones(100000) b = jnp.ones(100000) c = jnp.ones(100000) %timeit _ = add_vec(a).block_until_ready() %timeit _ = loop_for(b).block_until_ready() %timeit _ = (c+5).block_until_ready() ``` ``` 45.5 µs ± 257 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each) 541 ms ± 3.14 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) 83.8 µs ± 335 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each) ``` For reference, same code on CPU resultsin, ``` 51.9 µs ± 77.5 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each) 172 µs ± 151 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each) 54.5 µs ± 97 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each) ```"," > For GPU backends, each iteration effectively requires a kernel launch Is this a limitation of XLA or a fundamental limitation of the hardware itself? (I also asked this here.)",I don't know,"Hi , I have a question regarding the efficiency of `jax.lax.select` vs `jnp.where` on GPU. Could you please clarify which one is more efficient for GPU computations? Thanks.",`jnp.where` lowers to `jax.lax.select`; the only difference is that `jnp.where` will do NumPystyle implicit rank and dtype promotion. So they should be essentially identical in terms of computational efficiency.
yi,TPU backend gets stuck ," Description ``` python jax_test.py  /home/alberta/.local/lib/python3.10/sitepackages/jax/_src/xla_bridge.py:146: UserWarning: TPU backend initialization is taking more than 60.0 seconds. Did you run your code on all TPU hosts? See https://jax.readthedocs.io/en/latest/multi_process.html for more information.   warnings.warn( ``` I have been trying multiple attempts to run a simple JAX script, but it keeps stuck. I am using TPU v432, on ubuntu 22.04 and Python 3.10. **jax_test.py** ```  The following code snippet will be run on all TPU hosts import jax  The total number of TPU cores in the Pod device_count = jax.device_count()  The number of TPU cores attached to this host local_device_count = jax.local_device_count()  The psum is performed over all mapped devices across the Pod xs = jax.numpy.ones(jax.local_device_count()) r = jax.pmap(lambda x: jax.lax.psum(x, 'i'), axis_name='i')(xs)  Print from a single host to avoid duplicated output if jax.process_index() == 0:     print('global device count:', jax.device_count())     print('local device count:', jax.local_device_count())     print('pmap result:', r)` ```  System info (python version, jaxlib version, accelerator, etc.) Same behaviour on **jax.print_environment_info()**.",2024-02-25T20:48:14Z,bug,open,0,7,https://github.com/jax-ml/jax/issues/19971,"After a decent amount of time, the following exception is thrown: ``` Traceback (most recent call last):   File ""/home/alberta/.local/lib/python3.10/sitepackages/jax/_src/xla_bridge.py"", line 679, in backends     backend = _init_backend(platform)   File ""/home/alberta/.local/lib/python3.10/sitepackages/jax/_src/xla_bridge.py"", line 761, in _init_backend     backend = registration.factory()   File ""/home/alberta/.local/lib/python3.10/sitepackages/jax/_src/xla_bridge.py"", line 157, in tpu_client_timer_callback     client = xla_client.make_tpu_client(_get_tpu_library_path())   File ""/home/alberta/.local/lib/python3.10/sitepackages/jaxlib/xla_client.py"", line 198, in make_tpu_client     return make_tfrt_tpu_c_api_client()   File ""/home/alberta/.local/lib/python3.10/sitepackages/jaxlib/xla_client.py"", line 129, in make_tfrt_tpu_c_api_client     return _xla.get_c_api_client('tpu', options) jaxlib.xla_extension.XlaRuntimeError: UNKNOWN: TPU initialization failed: Failed to establish SliceBuilder grpc channel to 10.130.0.14:8471. During handling of the above exception, another exception occurred: Traceback (most recent call last):   File """", line 1, in    File ""/home/alberta/.local/lib/python3.10/sitepackages/jax/_src/environment_info.py"", line 44, in print_environment_info     devices_short = str(np.array(xla_bridge.devices())).replace('\n', '')   File ""/home/alberta/.local/lib/python3.10/sitepackages/jax/_src/xla_bridge.py"", line 872, in devices     return get_backend(backend).devices()   File ""/home/alberta/.local/lib/python3.10/sitepackages/jax/_src/xla_bridge.py"", line 806, in get_backend     return _get_backend_uncached(platform)   File ""/home/alberta/.local/lib/python3.10/sitepackages/jax/_src/xla_bridge.py"", line 786, in _get_backend_uncached     bs = backends()   File ""/home/alberta/.local/lib/python3.10/sitepackages/jax/_src/xla_bridge.py"", line 695, in backends     raise RuntimeError(err_msg) RuntimeError: Unable to initialize backend 'tpu': UNKNOWN: TPU initialization failed: Failed to establish SliceBuilder grpc channel to 10.130.0.14:8471. (set JAX_PLATFORMS='' to automatically choose an available backend) ```",I've encountered the same issue when using TPU v432.,"According to the warning, you should run your code on all TPU hosts. As TPU v432 has 4 hosts, you should run the code simultaneously on all 4 hosts. You can refer to usingtpupodusingtpupod)  for help.","I had the same issue. However, when I switched to this code ``` gcloud compute tpus tpuvm ssh tpuvmname zone=europewest4a   worker=all   command=""python3 c 'import jax; jax.distributed.initialize(); jax.process_index() == 0 and print(jax.devices())'"" ``` it worked. I think the key is to use jax.distributed.initialize()","I've encountered this warning on v332 as well, but after some waiting, script was executed without any errors. Seems like it just takes some time sometimes.","To configure TPU devices and run commands across hosts for TPU Pods, you can now use tpux!",Same problem here.
rag,Colab tpu initialization in XLA and JAX fails," Description Getting `cloud_tpu_init failed` using the google colab tpu runtime and torch xla package The torch_xla is installed with the command: ``` !pip install torch_xla[tpu] f https://storage.googleapis.com/libtpureleases/index.html ``` The imports used in notebook are: ``` import torch import torch_xla ``` Which shows the following error: ``` /usr/local/lib/python3.10/distpackages/jax/__init__.py:27: UserWarning: cloud_tpu_init failed: KeyError('')  This a JAX bug; please report an issue at https://github.com/google/jax/issues   _warn(f""cloud_tpu_init failed: {repr(exc)}\n This a JAX bug; please report ""  ImportError                               Traceback (most recent call last) [](https://localhost:8080/) in ()       1 import torch > 2 import torch_xla /usr/local/lib/python3.10/distpackages/torch_xla/__init__.py in      140 from .version import __version__     141  > 142 import _XLAC     143      144 _found_libtpu = _setup_tpu_vm_library_path() ImportError: /usr/local/lib/python3.10/distpackages/_XLAC.cpython310x86_64linuxgnu.so: undefined symbol: _ZN3c109TupleTypeC1ESt6vectorINS_4Type24SingletonOrSharedTypePtrIS2_EESaIS4_EESt8optionalINS_13QualifiedNameEESt10shared_ptrINS_14FunctionSchemaEE ```  System info (python version, jaxlib version, accelerator, etc.) Tensorflow shows the TPU: ``` import tensorflow as tf print(""Tensorflow version "" + tf.__version__) try:   tpu = tf.distribute.cluster_resolver.TPUClusterResolver()   TPU detection   print('Running on TPU ', tpu.cluster_spec().as_dict()['worker']) except ValueError:   raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!') ``` ``` Tensorflow version 2.12.0 Running on TPU  ['10.65.22.138:8470'] ``` JAX is printing this: ``` WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) jax:    0.3.25 jaxlib: 0.3.25 numpy:  1.23.5 python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 ```",2024-02-24T14:43:41Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/19963,"I note that: a) that error comes from `torch_xla`, not JAX, and b) JAX, as of v0.4, does not support colab TPUs. (We *do* support Kaggle TPUs, if you want notebook with TPUs in it). v0.3.25 is very old and we won't be able to help debug issues with it. c) even notwithstanding the above, TPUs currently cannot be shared between multiple frameworks at the same time. If you've opened a TPU with torch or TensorFlow, you won't be able to use it from JAX and vice versa. Hope that helps! (Closing, since there's nothing we can do here.)",Thanks for looking into it and for pointers to `torch_xla` and Kaggle TPUs.
rag,[attrs] add linearize and vjp support,"There are two commits here: * 67572d30949df295b5b09c20247517c5658a401b is a small tweak to the implementation of `attrs.jvp` to be simpler (IMO, though I'm not sure yet...) so that we handle input perturbations at the traceable level and the inner transformations never need to worry about them (e.g. we don't create separate attr input tracers) * 2ce8c57b4124c26e96301c036ca323001439b3d5 adds `attrs.linearize` and `attrs.vjp` The plan is for all of these features to be incorporated into the normal `jax.jvp`, `jax.linearize`, and `jax.vjp`, but we're keeping them separate for now while we play with them. The signatures generalize `jax.linearize` and `jax.vjp`, like: ```python InPrimal = OutPrimal = PyTree[Array] def vjp(f: Callable, *primals: InPrimal, attrs: list[tuple[Object, str]]         ) > tuple[OutPrimal, VJPFun]:   ... OutCT = PyTree[Array] ArgCTs = tuple[PyTree[Array], ...] vjpfun : VJPFun def vjpfun(out_ct: OutCT, *, attr_cotangents: dict[tuple[Object, str], Array] = {}            ) > tuple[ArgCTs, dict[tuple[Object, str], Array]]:   ... ``` We're currently pretty inconsistent between using lists like `list[tuple[Object, str]]` and `list[tuple[Object, str, Array]]` vs sets and dicts like `set[tuple[Object, str]]` and `dict[tuple[Object, str], Array]`. For the latter we require the mutable objects of interest to use `object.__hash__` / `object.__eq__`. We're also being inconsistent about whether tangent/cotangent result dicts represent zeros by missing entries, or symbolic zeros, or dense zeros. We'll make these things consistent at some point. These APIs are general but pretty lowlevel. A neural net library, for example, might help handle some of these details for the user, e.g. a model might be able to report `trainable_params : set[(Object, str)]` to be fed into this kind of API. The implementation for `attrs.linearize` and `attrs.vjp` in this PR ended up being very straightforward, leveraging one assumption we may want to relax later: so long as the linearized computation staged out into a jaxpr never involves any jax_getattr/jax_setattr (notice custom_jvp/custom_vjp rules which include jax_getattr/jax_setattr satisfy this definition), we don't need to change the partial eval or transpose machinery. The partialevaledout computation remains pure. That is, the JVP support we already landed is enough. So `attrs.linearize` and `attrs.vjp` are actually just like `jax.linearize` and `jax.vjp`, with two bookkeeping differences: 1. call `attrs.jvp` under the hood rather than `jax.jvp`, and 2. in the returned `f_lin` / `f_vjp` function, route attrs dict entries to the appropriate jaxpr inputs/outputs.",2024-02-24T00:12:11Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/19960
rag,Cleanup: access tree utilities via jax.tree.*,"Going forward, we plan to encourage downstream users to use `jax.tree.xxx` rather than `jax.tree_util.tree_xxx`, so we should probably lead by example in our own code. Done in `tests/` only for now, because we don't necessarily want to depend on toplevel `jax` submodules in `jax/_src`.",2024-02-23T19:34:59Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/19958
yi,Strange behavior of jax.experimental.jet," Description I was running the following code: ``` import jax import jax.numpy as jnp import jax.experimental.jet as jet import haiku as hk rng_key = jax.random.PRNGKey(0) def fn(x):     model = hk.Sequential(             [hk.Linear(64),               jax.nn.tanh] +              [hk.Linear(64),               jax.nn.tanh] * 2 +             [hk.Linear(64),               hk.Linear(1)]         )     return model(x) fn_t = hk.transform(fn) fn_t = hk.without_apply_rng(fn_t) params = fn_t.init(rng_key, jnp.ones((1,))) fn = jax.jit(lambda x: fn_t.apply(params, x)) x = jax.random.uniform(rng_key, shape=(2,1)) fn_jet = lambda x: jet.jet(fn, (x,), ((1.0, 0.0, 0.0, 0.0, 0.0),)) fn_jet = jax.jit(fn_jet) fn_jet(x) ``` And get the following error: ``` JaxStackTraceBeforeTransformation         Traceback (most recent call last) File ~/anaconda3/envs/qmace/lib/python3.10/runpy.py:196, in _run_module_as_main()     195     sys.argv[0] = mod_spec.origin > 196 return _run_code(code, main_globals, None,     197                  ""__main__"", mod_spec) File ~/anaconda3/envs/qmace/lib/python3.10/runpy.py:86, in _run_code()      79 run_globals.update(__name__ = mod_name,      80                    __file__ = fname,      81                    __cached__ = cached,    (...)      84                    __package__ = pkg_name,      85                    __spec__ = mod_spec) > 86 exec(code, run_globals)      87 return run_globals File ~/anaconda3/envs/qmace/lib/python3.10/sitepackages/ipykernel_launcher.py:17      15 from ipykernel import kernelapp as app > 17 app.launch_new_instance() File ~/anaconda3/envs/qmace/lib/python3.10/sitepackages/traitlets/config/application.py:1077, in launch_instance()    1076 app.initialize(argv) > 1077 app.start() ...    2527          ""less than the number of axes of the rhs value, got ""    2528          f""rhs_batch of {rhs_batch} and rhs_contracting of {rhs_contracting} ""    2529          f""for rhs of rank {rhs.ndim}"") TypeError: dot_general requires lhs dimension numbers to be nonnegative and less than the number of axes of the lhs value, got lhs_batch of () and lhs_contracting of (1,) for lhs of rank 0 ``` However, when I change the function definition to  ``` def fn(x):     model = hk.Sequential(             [hk.Linear(64),               jax.nn.tanh] +              [hk.Linear(64),               jax.nn.tanh] * 2 +             [hk.Linear(64),               hk.Linear(1)]         )     return model(x * 1) < only change is multiplication with scalar 1 ``` everything works. I'm not sure what could cause something like this; I guess multiplying by 1 creates a new node in the computational graph?   System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.20 jaxlib: 0.4.20 numpy:  1.26.2 python: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] jax.devices (1 total, 1 local): [cuda(id=0)] process_count: 1 $ nvidiasmi Fri Feb 23 10:42:37 2024        ++  ++ ```",2024-02-23T15:54:43Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/19949
yi,Include build information in compilation cache key.,Include build information in compilation cache key. Testing: test workloads.,2024-02-23T00:55:21Z,,closed,0,1,https://github.com/jax-ml/jax/issues/19939,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
yi,Promote `isclose` arguments to inexact dtype (fixes #19935).,"It turns out that CC(`jax.numpy.isclose` differs from `numpy.isclose`) is easily fixed by promoting to inexact dtypes. This ensures `np.isclose(6, 10, rtol=0.5)` and `jnp.isclose(6, 10, rtol=0.5)` give the same result.",2024-02-22T22:29:16Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/19936,"This is causing some downstream test failures for `isclose` called on key types: ```python import jax key = jax.random.key(0) jax.numpy.isclose(key, key) ``` We should probably fall back to equality checks when the following is true `dtypes.issubdtype(dtype, dtypes.extended)`."
yi,`jax.numpy.isclose` differs from `numpy.isclose`," Description The result of `jax.numpy.isclose` and `numpy.isclose` differ because the former treats integers differently than floats. Specifically for ""exact"" dtypes, jax demands equality rather than being ""close"". The relevant line is https://github.com/google/jax/blob/8d6bb0197b65f7aa0439087d4fc4e34ccb88c509/jax/_src/numpy/lax_numpy.pyL974 ```python >>> import numpy as np >>> from jax import numpy as jnp >>> np.isclose(6, 10, rtol=0.5) True >>> jnp.isclose(6, 10, rtol=0.5) Array(False, dtype=bool, weak_type=True) ``` Casting to a float first gives the expected result. ```python >>> jnp.isclose(6, 10., rtol=0.5) Array(True, dtype=bool) ```  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.24 jaxlib: 0.4.24 numpy:  1.26.3 python: 3.10.10 (main, Mar  3 2023, 16:31:35) [GCC 9.4.0] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 ```",2024-02-22T22:20:15Z,bug,closed,0,0,https://github.com/jax-ml/jax/issues/19935
rag,`jax.make_array_from_async_callback`,"`jax.make_array_from_callback` does a sequential loop over devices attached to this host: https://github.com/google/jax/blob/ef40b85c8b2686f64bc9ca67de267a6b1a7935bb/jax/_src/array.pyL693. When fetching from remote storage with high latency, this sequential loop can become latencylimited rather than throughputlimited on the network connection. In that case, it's typically a latency improvement to be able to issue the network requests for all devices in parallel.  As a user, I do that manually from `jax.make_array_from_single_device_arrays`, but that's a lower level API. Instead, the ideal would be a new function `jax.make_array_from_async_callback` that takes an async callback. Besides changing the type signature, the only change in implementation would be to add `await asyncio.gather(*...)` to https://github.com/google/jax/blob/ef40b85c8b2686f64bc9ca67de267a6b1a7935bb/jax/_src/array.pyL693, making it: ```     per_device_values = await asyncio.gather(*[data_callback(device_to_index_map[device])                          for device in devices]) ``` Then `jax.make_array_from_callback` could wrap the async version.",2024-02-22T01:59:12Z,enhancement,open,0,3,https://github.com/jax-ml/jax/issues/19919,Is this what you are looking for? https://github.com/google/jax/blob/main/jax/experimental/array_serialization/serialization.pyL67L78,"You can write a wrapper like this for your code base too! Or if the above utility is helpful and what you were looking for, I can expose that.",This seems quite close to CC(Wrapping a slow Python function in an asynchronous DeviceArray)
yi,"Memory issue when randomly initializing large parameters, sharding cannot help"," Description Consider the following code snippet: ```python import jax import flax.linen as nn from jax.sharding import Mesh import functools class Model(nn.Module):     output_dim = 32768 * 8     .compact     def __call__(self, inputs):         block = nn.Dense(features=self.output_dim, use_bias=False)         return block(inputs) class ShardedModel(nn.Module):     output_dim = 32768 * 8     .compact     def __call__(self, inputs):         init_fn = nn.initializers.lecun_normal()          init_fn = nn.initializers.zeros         block = nn.Dense(             features=self.output_dim,             use_bias=False,             kernel_init=nn.with_logical_partitioning(                 init_fn, (""logical_axis"", ""unmodelled"")             ),         )         return block(inputs) def test_model(model: nn.Module):     key = jax.random.PRNGKey(0)     input_shape = (1, 32768)     inputs = jax.random.normal(key, input_shape)     devices = jax.devices()     mesh = Mesh(devices, {""mesh_axis"": len(devices)})     print(f""Device mesh: {mesh}"")     sharding_rules = [         (""logical_axis"", ""mesh_axis""),     ]     abstract_params = jax.eval_shape(model.init, key, inputs)     params_partition_spec = nn.get_partition_spec(abstract_params)     params_sharding = nn.logical_to_mesh_sharding(         params_partition_spec,         mesh,         rules=sharding_rules,     )     print(f""Intended sharding: {params_sharding}"")     init_fn = functools.partial(model.init, key, inputs)     init_fn = jax.jit(         init_fn,         out_shardings=params_sharding,     )     params = init_fn()     actual_sharding = jax.tree_util.tree_map(lambda leaf: leaf.sharding, params)     print(f""Actual sharding: {actual_sharding}"") def main():     test_model(Model())      test_model(ShardedModel()) if __name__ == ""__main__"":     main() ``` Here I'm trying to initialize a very large dense layer. Despite the layer weights requiring only 32Gb of RAM (and I'm running on 80Gb H100), this code will fail because jax will try to simultaneously allocate quite a few buffers for RNG keys so that the total memory consumption is 112 Gb! ``` jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 85899347204 bytes. BufferAssignment OOM Debugging. BufferAssignment stats:              parameter allocation:         0B               constant allocation:         8B         maybe_live_out allocation:   32.00GiB      preallocated temp allocation:   80.00GiB   preallocated temp fragmentation:       124B (0.00%)                  total allocation:  112.00GiB               total fragmentation:   16.00GiB (14.29%) Peak buffers:         Buffer 1:                 Size: 32.00GiB                 Operator: op_name=""jit()/jit(main)/Model/Dense_0/mul"" source_file=""/usr/local/lib/python3.10/distpackages/flax/core/scope.py"" source_line=968                 XLA Label: fusion                 Shape: f32[32768,262144]                 ==========================         Buffer 2:                 Size: 16.00GiB                 Operator: op_name=""jit()/jit(main)/Model/Dense_0/jit(_truncated_normal)/jit(_uniform)/threefry2x32"" source_file=""/usr/local/lib/python3.10/distpackages/flax/core/scope.py"" source_line=968                 XLA Label: customcall                 Shape: u32[2,2147483648]                 ==========================         Buffer 3:                 Size: 16.00GiB                 Operator: op_name=""jit()/jit(main)/Model/Dense_0/jit(_truncated_normal)/jit(_uniform)/threefry2x32"" source_file=""/usr/local/lib/python3.10/distpackages/flax/core/scope.py"" source_line=968                 XLA Label: customcall                 Shape: u32[2,2147483648]                 ==========================         Buffer 4:                 Size: 16.00GiB                 Operator: op_name=""jit()/jit(main)/Model/Dense_0/jit(_truncated_normal)/jit(_uniform)/threefry2x32"" source_file=""/usr/local/lib/python3.10/distpackages/flax/core/scope.py"" source_line=968                 XLA Label: fusion                 Shape: u32[2,2147483648]                 ==========================         Buffer 5:                 Size: 16.00GiB                 Operator: op_name=""jit()/jit(main)/Model/Dense_0/jit(_truncated_normal)/jit(_uniform)/threefry2x32"" source_file=""/usr/local/lib/python3.10/distpackages/flax/core/scope.py"" source_line=968                 XLA Label: fusion                 Shape: u32[2,2147483648]                 ==========================         Buffer 6:                 Size: 16.00GiB                 Operator: op_name=""jit()/jit(main)/Model/Dense_0/jit(_truncated_normal)/jit(_uniform)/threefry2x32"" source_file=""/usr/local/lib/python3.10/distpackages/flax/core/scope.py"" source_line=968                 XLA Label: fusion                 Shape: u32[2,2147483648]                 ========================== ``` Is this intended? Do we really need to store these buffers in memory simultaneously to initialize the layer? In any case, we can try to fix the problem by sharding the layer over the available devices (8x80Gb H100, comment line 66 and uncomment line 67 in the code above). Interestingly, while this change reduces the size of the parameter tensor as intended, rng buffers are still being allocated in full! ``` jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 103079216656 bytes. BufferAssignment OOM Debugging. BufferAssignment stats:              parameter allocation:         0B               constant allocation:        40B         maybe_live_out allocation:    4.00GiB      preallocated temp allocation:   96.00GiB   preallocated temp fragmentation:       124B (0.00%)                  total allocation:  100.00GiB               total fragmentation:    4.00GiB (4.00%) Peak buffers:         Buffer 1:                 Size: 16.00GiB                 Operator: op_name=""jit()/jit(main)/ShardedModel/Dense_0/jit(_truncated_normal)/jit(_uniform)/threefry2x32"" source_file=""/usr/local/lib/python3.10/distpackages/flax/linen/spmd.py"" source_line=350                 XLA Label: customcall                 Shape: u32[2,2147483648]                 ==========================         Buffer 2:                 Size: 16.00GiB                 Operator: op_name=""jit()/jit(main)/ShardedModel/Dense_0/jit(_truncated_normal)/jit(_uniform)/threefry2x32"" source_file=""/usr/local/lib/python3.10/distpackages/flax/linen/spmd.py"" source_line=350                 XLA Label: customcall                 Shape: u32[2,2147483648]                 ==========================         Buffer 3:                 Size: 16.00GiB                 Operator: op_name=""jit()/jit(main)/ShardedModel/Dense_0/jit(_truncated_normal)/jit(_uniform)/threefry2x32"" source_file=""/usr/local/lib/python3.10/distpackages/flax/linen/spmd.py"" source_line=350                 XLA Label: fusion                 Shape: u32[2,2147483648]                 ==========================         Buffer 4:                 Size: 16.00GiB                 Operator: op_name=""jit()/jit(main)/ShardedModel/Dense_0/jit(_truncated_normal)/jit(_uniform)/threefry2x32"" source_file=""/usr/local/lib/python3.10/distpackages/flax/linen/spmd.py"" source_line=350                 XLA Label: fusion                 Shape: u32[2,2147483648]                 ==========================         Buffer 5:                 Size: 16.00GiB                 Operator: op_name=""jit()/jit(main)/ShardedModel/Dense_0/jit(_truncated_normal)/jit(_uniform)/threefry2x32"" source_file=""/usr/local/lib/python3.10/distpackages/flax/linen/spmd.py"" source_line=350                 XLA Label: fusion                 Shape: u32[2,2147483648]                 ==========================         Buffer 6:                 Size: 16.00GiB                 Operator: op_name=""jit()/jit(main)/ShardedModel/Dense_0/jit(_truncated_normal)/jit(_uniform)/threefry2x32"" source_file=""/usr/local/lib/python3.10/distpackages/flax/linen/spmd.py"" source_line=350                 XLA Label: fusion                 Shape: u32[2,2147483648]                 ==========================         Buffer 7:                 Size: 4.00GiB                 Operator: op_name=""jit()/jit(main)/ShardedModel/Dense_0/mul"" source_file=""/usr/local/lib/python3.10/distpackages/flax/linen/spmd.py"" source_line=350                 XLA Label: fusion                 Shape: f32[4096,262144]                 ========================== ``` This seems to be a bug: why is jax trying to materliaze the full rng tensor on each shard if it's not needed in full there? Finally, if I use all zeros initialization (uncomment line 22 in the code above), the issue goes away. So, to summarize, I have the following questions: * Is it expected that jax will try to simultaneously allocate so many rng buffers for weight initialization? * Why does the rng buffer allocation not respect sharding? * Are there any workarounds I can use to achieve what I need without resorting to allzeros initialization? The example above, while artificial, is inspired by a real problem that we've encountered while trying to initialize a large model.  System info (python version, jaxlib version, accelerator, etc.) ``` jax:    0.4.20 jaxlib: 0.4.20 numpy:  1.24.3 python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] jax.devices (8 total, 8 local): [cuda(id=0) cuda(id=1) ... cuda(id=6) cuda(id=7)] process_count: 1 $ nvidiasmi Tue Feb 20 16:53:25 2024 ++  ++ ```",2024-02-20T17:07:03Z,bug,open,5,1,https://github.com/jax-ml/jax/issues/19893,Can confirm that shardingbased solution works if using `jax_default_prng_impl=rbg`
yi,Traced<ShapedArray(float32[])>with<JVPTrace(level=2/1)>," Description ``` Tracedwith with   primal = Tracedwith   tangent = Tracedwith with     pval = (ShapedArray(float32[]), None)     recipe = JaxprEqnRecipe(eqn_id=, in_tracers=(Traced, Traced, Traced), out_tracer_refs=[], out_avals=[ShapedArray(float32[])], primitive=pjit, params={'jaxpr': { lambda ; a:f32[64,192,20] b:f32[64,192,20] c:f32[]. let     d:f32[64,192,20] = mul a b     e:f32[] = reduce_sum[axes=(0, 1, 2)] d     f:f32[] = div e c   in (f,) }, 'in_shardings': (UnspecifiedValue, UnspecifiedValue, UnspecifiedValue), 'out_shardings': (UnspecifiedValue,), 'resource_env': None, 'donated_invars': (False, False, False), 'name': '_reduce_max', 'keep_unused': False, 'inline': True}, effects=set(), source_info=SourceInfo(traceback=, name_stack=NameStack(stack=(Transform(name='jvp'), Scope(name='NerfModel'))))) max_reflection_encoding ``` when I am printing the shape of a jax array, it is printing for the first batch, and then showing this message, what is this message exactly saying.  System info (python version, jaxlib version, accelerator, etc.) python = 3.11 jaxlibversion = 0.4.23 accelarator = gpu jax:    0.4.23 jaxlib: 0.4.23 numpy:  1.26.3 python: 3.11.7  ++",2024-02-20T10:32:25Z,bug needs info,closed,0,2,https://github.com/jax-ml/jax/issues/19887,"Hi  thanks for the question, and sorry for the unclear error message, but I think we'll need more information in order to help you. What you printed above looks like the normal `repr` of a traced object within an autodiff transformation; for example: ```python In [1]: import jax In [2]: def f(x):    ...:     print(x)   this will print the tracer value within a grad transformation    ...:     return jax.numpy.sin(x)    ...:  In [3]: jax.grad(f)(1.0) Tracedwith with   primal = 1.0   tangent = Tracedwith with     pval = (ShapedArray(float32[], weak_type=True), None)     recipe = LambdaBinding() Out[3]: Array(0.5403023, dtype=float32, weak_type=True) ``` Can you paste the code you were running (a [minimal reproducible example]() if possible) and the full error traceback if applicable? Also, it helps to put code and tracebacks between triple tick marks (```` ``` ````) to format them as code. Thanks!",Closing due to lack of activity here – feel free to comment here or open another issue if you're still running into problems!
yi,Jax metal failed to install," Description Using the instructions on the pip website the jax_metal failed to install  ```bash (base) jakub.dokulilimp134 jd_python_learning % conda create n jax_metal python=3.10           Channels:   defaults Platform: osx64 Collecting package metadata (repodata.json): done Solving environment: done  Package Plan    environment location: /Users/jakub.dokulil/opt/anaconda3/envs/jax_metal   added / updated specs:      python=3.10 The following NEW packages will be INSTALLED:   bzip2              pkgs/main/osx64::bzip21.0.8h1de35cc_0    cacertificates    pkgs/main/osx64::cacertificates2023.12.12hecd8cb5_0    libffi             pkgs/main/osx64::libffi3.4.4hecd8cb5_0    ncurses            pkgs/main/osx64::ncurses6.4hcec6c5f_0    openssl            pkgs/main/osx64::openssl3.0.13hca72f7f_0    pip                pkgs/main/osx64::pip23.3.1py310hecd8cb5_0    python             pkgs/main/osx64::python3.10.13h5ee71fb_0    readline           pkgs/main/osx64::readline8.2hca72f7f_0    setuptools         pkgs/main/osx64::setuptools68.2.2py310hecd8cb5_0    sqlite             pkgs/main/osx64::sqlite3.41.2h6c40b1e_0    tk                 pkgs/main/osx64::tk8.6.12h5d9f67b_0    tzdata             pkgs/main/noarch::tzdata2023dh04d1e81_0    wheel              pkgs/main/osx64::wheel0.41.2py310hecd8cb5_0    xz                 pkgs/main/osx64::xz5.4.5h6c40b1e_0    zlib               pkgs/main/osx64::zlib1.2.13h4dc903c_0  Proceed ([y]/n)?  Downloading and Extracting Packages: Preparing transaction: done Verifying transaction: done Executing transaction: done   To activate this environment, use       $ conda activate jax_metal   To deactivate an active environment, use       $ conda deactivate (base) jakub.dokulilimp134 jd_python_learning % conda activate jax_metal (jax_metal) jakub.dokulilimp134 jd_python_learning % python m pip install U pip                        Requirement already satisfied: pip in /Users/jakub.dokulil/opt/anaconda3/envs/jax_metal/lib/python3.10/sitepackages (23.3.1) Collecting pip   Using cached pip24.0py3noneany.whl.metadata (3.6 kB) Using cached pip24.0py3noneany.whl (2.1 MB) Installing collected packages: pip   Attempting uninstall: pip     Found existing installation: pip 23.3.1     Uninstalling pip23.3.1:       Successfully uninstalled pip23.3.1 Successfully installed pip24.0 (jax_metal) jakub.dokulilimp134 jd_python_learning % python m pip install numpy                         Collecting numpy   Using cached numpy1.26.4cp310cp310macosx_10_9_x86_64.whl.metadata (61 kB) Using cached numpy1.26.4cp310cp310macosx_10_9_x86_64.whl (20.6 MB) Installing collected packages: numpy Successfully installed numpy1.26.4 (jax_metal) jakub.dokulilimp134 jd_python_learning % python m pip install jaxmetal                     Collecting jaxmetal   Using cached jax_metal0.0.5py3nonemacosx_10_14_x86_64.whl.metadata (1.4 kB) Requirement already satisfied: wheel~=0.35 in /Users/jakub.dokulil/opt/anaconda3/envs/jax_metal/lib/python3.10/sitepackages (from jaxmetal) (0.41.2) Collecting six>=1.15.0 (from jaxmetal)   Using cached six1.16.0py2.py3noneany.whl (11 kB) Collecting jax==0.4.20 (from jaxmetal)   Using cached jax0.4.20py3noneany.whl.metadata (23 kB) Collecting jaxlib==0.4.20 (from jaxmetal)   Downloading jaxlib0.4.20cp310cp310macosx_10_14_x86_64.whl.metadata (2.1 kB) Collecting mldtypes>=0.2.0 (from jax==0.4.20>jaxmetal)   Using cached ml_dtypes0.3.2cp310cp310macosx_10_9_universal2.whl.metadata (20 kB) Requirement already satisfied: numpy>=1.22 in /Users/jakub.dokulil/opt/anaconda3/envs/jax_metal/lib/python3.10/sitepackages (from jax==0.4.20>jaxmetal) (1.26.4) Collecting opteinsum (from jax==0.4.20>jaxmetal)   Using cached opt_einsum3.3.0py3noneany.whl (65 kB) Collecting scipy>=1.9 (from jax==0.4.20>jaxmetal)   Using cached scipy1.12.0cp310cp310macosx_10_9_x86_64.whl.metadata (60 kB) Using cached jax_metal0.0.5py3nonemacosx_10_14_x86_64.whl (54.6 MB) Using cached jax0.4.20py3noneany.whl (1.7 MB) Downloading jaxlib0.4.20cp310cp310macosx_10_14_x86_64.whl (82.6 MB)    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 82.6/82.6 MB 3.5 MB/s eta 0:00:00 Using cached ml_dtypes0.3.2cp310cp310macosx_10_9_universal2.whl (389 kB) Using cached scipy1.12.0cp310cp310macosx_10_9_x86_64.whl (38.9 MB) Installing collected packages: six, scipy, opteinsum, mldtypes, jaxlib, jax, jaxmetal Successfully installed jax0.4.20 jaxmetal0.0.5 jaxlib0.4.20 mldtypes0.3.2 opteinsum3.3.0 scipy1.12.0 six1.16.0 (jax_metal) jakub.dokulilimp134 jd_python_learning % python c 'import jax; print(jax.numpy.arange(10))' Traceback (most recent call last):   File """", line 1, in    File ""/Users/jakub.dokulil/opt/anaconda3/envs/jax_metal/lib/python3.10/sitepackages/jax/__init__.py"", line 39, in      from jax import config as _config_module   File ""/Users/jakub.dokulil/opt/anaconda3/envs/jax_metal/lib/python3.10/sitepackages/jax/config.py"", line 15, in      from jax._src.config import config as _deprecated_config   noqa: F401   File ""/Users/jakub.dokulil/opt/anaconda3/envs/jax_metal/lib/python3.10/sitepackages/jax/_src/config.py"", line 28, in      from jax._src import lib   File ""/Users/jakub.dokulil/opt/anaconda3/envs/jax_metal/lib/python3.10/sitepackages/jax/_src/lib/__init__.py"", line 83, in      cpu_feature_guard.check_cpu_features() RuntimeError: This version of jaxlib was built using AVX instructions, which your CPU and/or operating system do not support. You may be able work around this issue by building jaxlib from source. ```  System info (python version, jaxlib version, accelerator, etc.) Macbook Air M2 Macos Sonoma 14.3.1 (23D60) Python 3.10",2024-02-20T09:15:46Z,bug Apple GPU (Metal) plugin,open,0,8,https://github.com/jax-ml/jax/issues/19886,"Based on the packages, it is  AMD GPU? Could you try a venv with  python=3.9?",Reproduces on my m2 mac. with both py 3.10.6 and 3.9.13,Tried jax==0.4.11 jaxlib==0.4.11 jaxmetal==0.0.4  same thing,Haven't been able to reproduce the issue. The below config shows an installation and verification result:  ProductName:		macOS ProductVersion:		14.4 ``` The following NEW packages will be INSTALLED:   cacertificates    pkgs/main/osx64::cacertificates2023.12.12hecd8cb5_0    libcxx             pkgs/main/osx64::libcxx14.0.6h9765a3e_0    libffi             pkgs/main/osx64::libffi3.4.4hecd8cb5_0    ncurses            pkgs/main/osx64::ncurses6.4hcec6c5f_0    openssl            pkgs/main/osx64::openssl3.0.13hca72f7f_0    pip                pkgs/main/osx64::pip23.3.1py39hecd8cb5_0    python             pkgs/main/osx64::python3.9.18h5ee71fb_0    readline           pkgs/main/osx64::readline8.2hca72f7f_0    setuptools         pkgs/main/osx64::setuptools68.2.2py39hecd8cb5_0    sqlite             pkgs/main/osx64::sqlite3.41.2h6c40b1e_0    tk                 pkgs/main/osx64::tk8.6.12h5d9f67b_0    tzdata             pkgs/main/noarch::tzdata2024ah04d1e81_0    wheel              pkgs/main/osx64::wheel0.41.2py39hecd8cb5_0    xz                 pkgs/main/osx64::xz5.4.6h6c40b1e_0    zlib               pkgs/main/osx64::zlib1.2.13h4dc903c_0  ``` ``` Package            Version   importlib_metadata 7.0.2 jax                0.4.20 jaxmetal          0.0.5 jaxlib             0.4.20 mldtypes          0.3.2 numpy              1.26.4 opteinsum         3.3.0 pip                24.0 scipy              1.12.0 setuptools         68.2.2 six                1.16.0 wheel              0.41.2 zipp               3.17.0 ``` ``` python c 'import jax; print(jax.numpy.arange(10))' Platform 'METAL' is experimental and not all JAX functionality may be correctly supported! 20240308 17:33:36.946600: W pjrt_plugin/src/mps_client.cc:563] WARNING: JAX Apple GPU support is experimental and not all JAX functionality is correctly supported! Metal device set to: AMD Radeon Pro Vega 20 systemMemory: 32.00 GB maxCacheSize: 1.99 GB [0 1 2 3 4 5 6 7 8 9] ```,"Right, i think i was able to figure it out  in my case it was due python being `i386` arch and not `arm64`. After switching arch and installing native python, it worked. ","> Right, i think i was able to figure it out  in my case it was due python being `i386` arch and not `arm64`. After switching arch and installing native python, it worked. I have just tried to install following the instructions in the apple website (https://developer.apple.com/metal/jax/) and it failed. Same error than everyone here in a M2. How did you switched your native python3?  I have just ran the following code: ```python import platform  Check the machine architecture machine = platform.machine() if machine == 'arm64':     print(""Your Python version is ARM64"") elif machine == 'i386':     print(""Your Python version is i386 (32bit)"") elif machine == 'x86_64':     print(""Your Python version is x86_64 (64bit)"") else:     print(f""Unknown machine architecture: {machine}"") ``` and the print out is: ``` Your Python version is x86_64 (64bit) ```"," you switch in you CLI with `arch` command, then you install python afresh (it will be a different python) and go with jax m install instruct from apple.", thanks for the tip. It worked for me!
yi,jax-metal: Failed assertion...expected element type f32 but received si32," Description I set up a venv for my project using `jaxmetal`, but hit the following assertion error when I ran my otherwise functioning code: ``` /AppleInternal/Library/BuildRoots/0032d1ee80fd11ee82276aecfccc70fe/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphExecutable.mm:1650: failed assertion `Incompatible element type for parameter at index 18, mlir module expected element type f32 but received si32' ``` The error does not come with a stack trace. Disabling jit for the entire script avoided the issue, but likewise didn't help with stack tracing. On a hunch, I found that the issue was somehow related to a noop Flax module I have that looks like this: ```python class Foo(nn.Module):     """"""Noop module with stub parameters""""""     .compact     def __call__(self, x):          need some sort of unused param for pytree reasons         self.param('null', zeros, 0)  changing 0 > (0,) does not fix anything                                       changing 0 > (1,) does fix the assertion         return x ``` As noted in the comment, changing the shape for the unused param does fix the error. However, the assertion is **not** raised when the module's apply function is called. After stepping through linebyline with a debugger, I've found that it's raised much later, during the teardown of my jit'd training step function. Here is a schematic example of my code organization. This code **does not** reproduce the error, but is intended to show where the assertion gets raised in relation to the module's apply function: ```python import flax.linen as nn import jax import jax.numpy as jnp import optax from flax.linen.initializers import zeros from flax.training.train_state import TrainState from jax import random class Foo(nn.Module):     """"""Noop module with stub parameters""""""     .compact     def __call__(self, x):         self.param('null', zeros, 0)   changing 0 > (1,) does fix the assertion         return x .jit def fwd(p, s, x):     """"""Model forward function""""""     y = s.apply_fn(p, x)   apply the problematic module     y = ...   apply multiple other modules and functions     l = jnp.mean((y  x) ** 2)     return l, {'val': y, 'loss': l} .jit def step(ss, xx):     """"""Training step helper""""""     (l, m), grads = jax.value_and_grad(fwd, has_aux=True)(ss.params, ss, xx)     new_state = ss.apply_gradients(grads=grads)     return new_state, l, m   assertion is raised here in original code  initialize the model rng = random.PRNGKey(0) foo = Foo() params = jax.jit(foo.init)(rng, jnp.ones((1,))) state = TrainState.create(     apply_fn=jax.jit(foo.apply),     params=params,     tx=optax.sgd(0.01) )  run for some steps for _ in range(10):     state, loss, metrics = step(state, jnp.ones((10, 10))) ``` Anyway, I'm not sure if this is a bug or intended behavior for Metal. I have a fix, but I would like to understand why that parameter can't be a zerosized array in my project code when it does work in this example code. Or maybe it's not related to that module? But then, why does changing that module fix the error? It seems similar to CC(Metal: Failed Assertion  / Crash when trying to create arrays in doubleprecision), but that also wasn't a zerosized array issue, so I don't know.  System info (python version, jaxlib version, accelerator, etc.) ``` Metal device set to: Apple M1 Max systemMemory: 64.00 GB maxCacheSize: 24.00 GB jax:    0.4.20 jaxlib: 0.4.20 jaxmetal : 0.0.5 numpy:  1.26.4 python: 3.10.13 (main, Aug 24 2023, 12:59:26) [Clang 15.0.0 (clang1500.0.40.1)] jax.devices (1 total, 1 local): [METAL(id=0)] process_count: 1 ```",2024-02-16T02:43:42Z,bug Apple GPU (Metal) plugin,open,0,2,https://github.com/jax-ml/jax/issues/19841,The issue is not  reproducible.   Do you still see the same problem with the latest OS 14.4 and jaxmetal 0.0.6? ,"I just updated to macOS 14.4 and jaxmetal 0.0.6, and the issue does still occur if I pass `0` or `(0,)` to my module initializer, but not if I pass `(1,)`. As I say, the above code is a schematic that does not reproduce the error, and I'm unfortunately not in a position at the moment to start trimming my full code base down to a minimal reproducible example. It'll probably be at least a month or so before I'll have that sort of time.  I understand if you want to close the issue since it's not reproducible. I just wanted to make sure this occurrence was at least documented in case someone else has a similar issue."
yi,Problem with Pallas in JAX 0.4.24," Description I trying to setup an environment for jax 0.4.24 When I try to running a pallas kernel (like an example), I get an error: `LLVM ERROR: Trying to register different dialects for the same namespace: builtin` Triton kernels work well  System info (python version, jaxlib version, accelerator, etc.) Tax 0.4.24, jaxlib 0.4.24, triton on commit https://github.com/openai/triton/commits/9f816a7b984ce20ec684866f9c8bb5ffd49e2500, jaxtriton on commit 3666738dd9fef74f702a82a3fa068d7e2bc80e2a",2024-02-15T14:18:04Z,bug pallas,closed,3,4,https://github.com/jax-ml/jax/issues/19825,"That error should have been fixed in https://github.com/google/jax/commit/5e2e609a9b8da90989a593575ad83eb2e43109e8, so you would need a version including that. If it suits your workflow, you could try the containers distributed as `ghcr.io/nvidia/jax:pallasYYYYMMDD` (or `ghcr.io/nvidia/jax:pallas` for the latest one).",Got it! Thank you for the answer,"Tried the latest one (20240214) and got en error: ``` python3: /opt/openxlatriton/include/triton/Tools/Sys/GetEnv.hpp:49: bool triton::tools::getBoolEnv(const string&): Assertion `::triton::ENV_VARS.find(env.c_str()) != ::triton::ENV_VARS.end() && msg.c_str()' failed. ``` As I understand, it rises because of the commit: https://github.com/openxla/triton/commit/f2d49c8fd08cef4adbfddd6e428d93f98576653f The flag `DISABLE_MMA_V3` was renamed to `ENABLE_MMA_V3` without fixing it in https://github.com/openxla/triton/blob/f2d49c8fd08cef4adbfddd6e428d93f98576653f/include/triton/Tools/Sys/GetEnv.hppL32L35",jaxlib 0.4.25 no longer has this issue. Closing as fixed.
yi,Incompatible CUDA and ptxas CUDA version," Description Hey,  I installed the cuda version of jax for my cuda 12.2 using: ```bash pip install U ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` but I get this warning when trying to run the code: ``` >>> import jax >>> jax.numpy.array(1) 20240215 14:18:20.201828: W external/xla/xla/service/gpu/nvptx_compiler.cc:744] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.3.107). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIAprovided CUDA forward compatibility packages. Array(1, dtype=int32, weak_type=True) ``` It disables parallel execution and hence my code takes more time. What can I do to fix it?  System info (python version, jaxlib version, accelerator, etc.) ``` >>> jax.print_environment_info() jax:    0.4.24 jaxlib: 0.4.24 numpy:  1.26.4 python: 3.12.1  ++ ```",2024-02-15T08:52:11Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/19821,"I think the error message is selfexplanatory: ""You should update your NVIDIA driver or use the NVIDIAprovided CUDA forward compatibility packages."" However, if that's hard to do, don't worry about it, it just makes `jit` compilation slightly slower. Hope that helps!"
yi,AttributeError message on jax.random.KeyArray," Description Hello, I am working with a student in order to install a package that with jax dependencies. Our current version is jax 0.4.24 There is an AttributeError thrown for a call to jax.random.KeyArray. The full error message is below. ``` import harmonic as hm Traceback (most recent call last):  File """", line 1, in   File ""/usr/local/anaconda/envs/harmonic_env/lib/python3.9/sitepackages/harmonic/_init_.py"", line 5, in    from . import model  File ""/usr/local/anaconda/envs/harmonic_env/lib/python3.9/sitepackages/harmonic/model.py"", line 3, in    from harmonic import flows  File ""/usr/local/anaconda/envs/harmonic_env/lib/python3.9/sitepackages/harmonic/flows.py"", line 6, in    import distrax  File ""/usr/local/anaconda/envs/harmonic_env/lib/python3.9/sitepackages/distrax/_init_.py"", line 20, in    from distrax._src.bijectors.block import Block  File ""/usr/local/anaconda/envs/harmonic_env/lib/python3.9/sitepackages/distrax/_src/bijectors/block.py"", line 20, in    from distrax._src.utils import conversion  File ""/usr/local/anaconda/envs/harmonic_env/lib/python3.9/sitepackages/distrax/_src/utils/conversion.py"", line 19, in    import chex  File ""/usr/local/anaconda/envs/harmonic_env/lib/python3.9/sitepackages/chex/_init_.py"", line 17, in    from chex._src.asserts import assert_axis_dimension  File ""/usr/local/anaconda/envs/harmonic_env/lib/python3.9/sitepackages/chex/_src/asserts.py"", line 26, in    from chex._src import asserts_internal as _ai  File ""/usr/local/anaconda/envs/harmonic_env/lib/python3.9/sitepackages/chex/_src/asserts_internal.py"", line 34, in    from chex._src import pytypes  File ""/usr/local/anaconda/envs/harmonic_env/lib/python3.9/sitepackages/chex/_src/pytypes.py"", line 54, in    PRNGKey = jax.random.KeyArray  File ""/usr/local/anaconda/envs/harmonic_env/lib/python3.9/sitepackages/jax/_src/deprecations.py"", line 53, in getattr   raise AttributeError(f""module {module!r} has no attribute {name!r}"") AttributeError: module 'jax.random' has no attribute 'KeyArray' ```  System info (python version, jaxlib version, accelerator, etc.) Python 3.9.18  Jax environment info ``` $ python3 Python 3.9.18 (main, Sep 11 2023, 08:38:23) [Clang 14.0.6 ] :: Anaconda, Inc. on darwin Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import jax >>> jax.print_environment_info() jax:  0.4.24 jaxlib: 0.4.24 numpy: 1.22.4 python: 3.9.18 (main, Sep 11 2023, 08:38:23) [Clang 14.0.6 ] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 ```  Conda environment ``` > conda list  packages in environment at /usr/local/anaconda/envs/harmonic_env:   Name          Version          Build Channel abslpy          2.1.0          pypi_0  pypi anyio           4.2.0          pypi_0  pypi appnope          0.1.4          pypi_0  pypi argon2cffi        23.1.0          pypi_0  pypi argon2cffibindings   21.2.0          pypi_0  pypi arrow           1.3.0          pypi_0  pypi asttokens         2.4.1          pypi_0  pypi asynclru         2.0.4          pypi_0  pypi attrs           23.2.0          pypi_0  pypi babel           2.14.0          pypi_0  pypi beautifulsoup4      4.12.3          pypi_0  pypi bleach          6.1.0          pypi_0  pypi cacertificates      2023.12.12      hecd8cb5_0 certifi          2024.2.2         pypi_0  pypi cffi           1.16.0          pypi_0  pypi charsetnormalizer    3.3.2          pypi_0  pypi chex           0.1.7          pypi_0  pypi cloudpickle        3.0.0          pypi_0  pypi colorlog         6.8.2          pypi_0  pypi comm           0.2.1          pypi_0  pypi contourpy         1.2.0          pypi_0  pypi cycler          0.12.1          pypi_0  pypi cython          3.0.8          pypi_0  pypi debugpy          1.8.1          pypi_0  pypi decorator         5.1.1          pypi_0  pypi defusedxml        0.7.1          pypi_0  pypi distrax          0.1.3          pypi_0  pypi dmtree          0.1.8          pypi_0  pypi emcee           3.1.4          pypi_0  pypi etils           1.5.2          pypi_0  pypi exceptiongroup      1.2.0          pypi_0  pypi executing         2.0.1          pypi_0  pypi fastjsonschema      2.19.1          pypi_0  pypi flax           0.8.1          pypi_0  pypi fonttools         4.48.1          pypi_0  pypi fqdn           1.5.1          pypi_0  pypi fsspec          2024.2.0         pypi_0  pypi gast           0.5.4          pypi_0  pypi getdist          1.4.7          pypi_0  pypi h11            0.14.0          pypi_0  pypi harmonic         1.2.0          pypi_0  pypi httpcore         1.0.2          pypi_0  pypi httpx           0.26.0          pypi_0  pypi idna           3.6           pypi_0  pypi importlibmetadata    7.0.1          pypi_0  pypi importlibresources    6.1.1          pypi_0  pypi ipykernel         6.29.2          pypi_0  pypi ipython          8.18.1          pypi_0  pypi ipywidgets        8.1.2          pypi_0  pypi isoduration        20.11.0         pypi_0  pypi jax            0.4.24          pypi_0  pypi jaxlib          0.4.24          pypi_0  pypi jedi           0.19.1          pypi_0  pypi jinja2          3.1.3          pypi_0  pypi joblib          1.3.2          pypi_0  pypi json5           0.9.14          pypi_0  pypi jsonpointer        2.4           pypi_0  pypi jsonschema        4.21.1          pypi_0  pypi jsonschemaspecifications 2023.12.1        pypi_0  pypi jupyter          1.0.0          pypi_0  pypi jupyterclient      8.6.0          pypi_0  pypi jupyterconsole      6.6.3          pypi_0  pypi jupytercore       5.7.1          pypi_0  pypi jupyterevents      0.9.0          pypi_0  pypi jupyterlsp        2.2.2          pypi_0  pypi jupyterserver      2.12.5          pypi_0  pypi jupyterserverterminals 0.5.2          pypi_0  pypi jupyterlab        4.1.1          pypi_0  pypi jupyterlabpygments    0.3.0          pypi_0  pypi jupyterlabserver     2.25.2          pypi_0  pypi jupyterlabwidgets    3.0.10          pypi_0  pypi kiwisolver        1.4.5          pypi_0  pypi libcxx          14.0.6        h9765a3e_0 libffi          3.4.4        hecd8cb5_0 markdownitpy      3.0.0          pypi_0  pypi markupsafe        2.1.5          pypi_0  pypi matplotlib        3.8.2          pypi_0  pypi matplotlibinline     0.1.6          pypi_0  pypi mdurl           0.1.2          pypi_0  pypi mistune          3.0.2          pypi_0  pypi mldtypes         0.3.2          pypi_0  pypi msgpack          1.0.7          pypi_0  pypi nbclient         0.9.0          pypi_0  pypi nbconvert         7.16.0          pypi_0  pypi nbformat         5.9.2          pypi_0  pypi ncurses          6.4         hcec6c5f_0 nestasyncio       1.6.0          pypi_0  pypi notebook         7.1.0          pypi_0  pypi notebookshim       0.2.3          pypi_0  pypi numpy           1.22.4          pypi_0  pypi openssl          3.0.13        hca72f7f_0 opteinsum        3.3.0          pypi_0  pypi optax           0.1.9          pypi_0  pypi orbaxcheckpoint     0.5.3          pypi_0  pypi overrides         7.7.0          pypi_0  pypi packaging         23.2           pypi_0  pypi pandocfilters       1.5.1          pypi_0  pypi parso           0.8.3          pypi_0  pypi pexpect          4.9.0          pypi_0  pypi pillow          10.2.0          pypi_0  pypi pip            23.3.1      py39hecd8cb5_0 platformdirs       4.2.0          pypi_0  pypi prometheusclient     0.20.0          pypi_0  pypi prompttoolkit      3.0.43          pypi_0  pypi protobuf         4.25.2          pypi_0  pypi psutil          5.9.8          pypi_0  pypi ptyprocess        0.7.0          pypi_0  pypi pureeval         0.2.2          pypi_0  pypi pycparser         2.21           pypi_0  pypi pygments         2.17.2          pypi_0  pypi pyparsing         3.1.1          pypi_0  pypi python          3.9.18        h5ee71fb_0 pythondateutil      2.8.2          pypi_0  pypi pythonjsonlogger    2.0.7          pypi_0  pypi pyyaml          6.0.1          pypi_0  pypi pyzmq           25.1.2          pypi_0  pypi qtconsole         5.5.1          pypi_0  pypi qtpy           2.4.1          pypi_0  pypi readline         8.2         hca72f7f_0 referencing        0.33.0          pypi_0  pypi requests         2.31.0          pypi_0  pypi rfc3339validator     0.1.4          pypi_0  pypi rfc3986validator     0.1.1          pypi_0  pypi rich           13.7.0          pypi_0  pypi rpdspy          0.18.0          pypi_0  pypi scikitlearn       1.4.0          pypi_0  pypi scipy           1.12.0          pypi_0  pypi send2trash        1.8.2          pypi_0  pypi setuptools        68.0.0          pypi_0  pypi six            1.16.0          pypi_0  pypi sniffio          1.3.0          pypi_0  pypi soupsieve         2.5           pypi_0  pypi sqlite          3.41.2        h6c40b1e_0 stackdata        0.6.3          pypi_0  pypi tensorflowprobability  0.23.0          pypi_0  pypi tensorstore        0.1.53          pypi_0  pypi terminado         0.18.0          pypi_0  pypi threadpoolctl       3.3.0          pypi_0  pypi tinycss2         1.2.1          pypi_0  pypi tk            8.6.12        h5d9f67b_0 tomli           2.0.1          pypi_0  pypi toolz           0.12.1          pypi_0  pypi tornado          6.4           pypi_0  pypi tqdm           4.66.2          pypi_0  pypi traitlets         5.14.1          pypi_0  pypi typespythondateutil   2.8.19.20240106     pypi_0  pypi typingextensions     4.9.0          pypi_0  pypi tzdata          2023d        h04d1e81_0 uritemplate       1.3.0          pypi_0  pypi urllib3          2.2.0          pypi_0  pypi wcwidth          0.2.13          pypi_0  pypi webcolors         1.13           pypi_0  pypi webencodings       0.5.1          pypi_0  pypi websocketclient     1.7.0          pypi_0  pypi wheel           0.41.0          pypi_0  pypi widgetsnbextension    4.0.10          pypi_0  pypi xz            5.4.5        h6c40b1e_0 zipp           3.17.0          pypi_0  pypi zlib           1.2.13        h4dc903c_0 ```",2024-02-14T19:48:00Z,question,closed,0,2,https://github.com/jax-ml/jax/issues/19812,"Hi  thanks for the question. `jax.random.KeyArray` was deprecated in jax v0.4.16, and removed in jax v0.4.24. It looks like the import is coming from `chex`, in which case your best course of action is probably to update `chex`. If that's not possible, you could install JAX version 0.4.23 or older. You can find information at those CHANGELOG links regarding how to replace other uses of the symbol. Please let me know if you have any further questions!",It seems like this is resolved  please let us know if you still have any questions about this!
yi,sin/cos results are incorrect on pure imaginary inputs with large absolute value," Description As reported in MPMath vs Jax sin, CPU, there exists regions in complex plane where `jax.numpy.sin` returns incorrect results. For example: ```python >>> jnp.sin(jnp.array(100j, dtype=jnp.complex64)) Array(nan+infj, dtype=complex64) ``` The expected result is `infj`. Also, MPMath vs Jax cos, CPU reports a similar issue in `jax.numpy.cos`: ```python >>> jnp.cos(jnp.array(100j, dtype=jnp.complex64)) Array(inf+nanj, dtype=complex64) ``` The expected result is `inf+0j`  What jax/jaxlib version are you using? 0.4.23  Which accelerator(s) are you using? CPU/GPU",2024-02-12T14:28:16Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/19754,Fixed via CC(Fix complex sin and cos on inputs with small absolute value or large pure imaginary part) 
yi,Square on large complex(64) inputs return incorrect results," Description As reported in MPMath vs Jax square, CPU and MPMath vs JAX square, CUDA, there exists regions in complex plane where `jax.numpy.square` returns incorrect results. For example: ```python >>> with jax.default_device(jax.devices('cpu')[0]): ...   jnp.square(jnp.array(4.468666049354532e+325.8683295066770075e+26j, dtype=jnp.complex64)) ...  Array(infinfj, dtype=complex64) >>> with jax.default_device(jax.devices('cuda')[0]): ...   jnp.square(jnp.array(4.468666049354532e+325.8683295066770075e+26j, dtype=jnp.complex64)) ...  Array(naninfj, dtype=complex64) ``` The expected result is `inf  infj` as suggested by: ```python >>> (4.468666049354532e+325.8683295066770075e+26j)**2 (1.9968976260619403e+655.244720966582595e+59j) ``` or ```python >>> with jax.default_device(jax.devices('cpu')[0]): ...   jnp.square(jnp.array(4.893543125372159e+304.893543125372159e+30j, dtype=jnp.complex64)) ...  Array(infinfj, dtype=complex64) ``` where the correct result is `infj`. Other samples problematic to jax square include: ```                                                       x >      jax x**2   mpmath x**2         (4.468666049354532e+325.8683295066770075e+26j) >   (infinfj)   (infinfj)         (4.468666049354532e+32+5.8683295066770075e+26j) >   (inf+infj)   (inf+infj)        (4.468666049354532e+32+5.8683295066770075e+26j) >   (infinfj)   (infinfj)                            (inf5.358817286521974e+28j) >    (naninfj)   (infinfj)                            (inf+4.893543125372159e+30j) >    (nan+infj)   (inf+infj)                           (inf5.358817286521974e+28j) >    (nan+infj)   (inf+infj)                           (inf+4.893543125372159e+30j) >    (naninfj)   (infinfj)          (4.893543125372159e+304.893543125372159e+30j) >   (naninfj)         infj          (4.893543125372159e+30+4.893543125372159e+30j) >   (nan+infj)          infj         (4.893543125372159e+304.893543125372159e+30j) >   (nan+infj)          infj         (4.893543125372159e+30+4.893543125372159e+30j) >   (naninfj)         infj ```  What jax/jaxlib version are you using? 0.4.23  Which accelerator(s) are you using? CPU/GPU",2024-02-12T12:24:00Z,bug,closed,1,1,https://github.com/jax-ml/jax/issues/19751,This issue is fixed via https://github.com/jaxml/jax/pull/24874 . The current state of JAX `square` accuracy is summarized in https://github.com/pearu/functional_algorithms/issues/52.
yi,Core-dump after upgrade," Description Hi, I recently upgraded jax in my environment to v0.4.24, using CUDA 12.3 from outside the venv. After that, some operations started to yield strange core dumps like the following: ```python >>> import jax.numpy as jnp >>>  this seems to be fine: >>> jnp.array([1,2,3]) Array([1, 2, 3], dtype=int32) >>> jnp.array([1,2,3]).devices() {cuda(id=0)} >>>  this results in a segmentation fault >>> jnp.arange(3) 20240209 13:05:06.278310: F external/xla/xla/service/gpu/nvptx_compiler.cc:619] ptxas returned an error during compilation of ptx to sass: 'INTERNAL: ptxas 12.3.103 has a bug that we think can affect XLA. Please use a different version.'  If the error message indicates that a file could not be written, please verify that sufficient filesystem space is provided. Aborted (core dumped) ``` After the core dump, the Python REPL exited with SIGABRT. My system has ~800GB available disk space and around ~25GB available RAM, but since the error message is not about failing to write a file, I assume insufficient space not to be the cause of the issue. To investigate the issue, I cleared my environment to include nothing but the jax installation: ```bash $ pip freeze jax==0.4.24 jaxlib==0.4.24+cuda12.cudnn89 mldtypes==0.3.2 numpy==1.26.4 opteinsum==3.3.0 scipy==1.12.0 setuptools==68.2.2 wheel==0.41.2 ``` I would be grateful for any advice on what could have gone wrong. Thank you! Johannes  What jax/jaxlib version are you using? jax v0.4.24, jaxlib v0.4.24  Which accelerator(s) are you using? GPU  Additional system info? numpy v1.26.4, Python 3.12.1 installed with conda v23.10.0 on Arch Linux x86_64  NVIDIA GPU info ```   ++  ++ ```",2024-02-09T12:13:07Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/19730,"This only happens when I use CUDA/cudnn from outside the venv. When using a pip install of jax, the error magically disappears.","This is a slightly less than graceful failure from XLA when it finds a bad ptxas version. The action item for us here would be to fail more gracefully (with a Python error, rather than a fatal crash). This fix on your side is to do as you say: you should use jax inside the virtualenv in which you installed it, because that way JAX will find a working copy of ptxas from the virtualenv (it's inside the `nvidianvcccu12` package, most likely).","Since I am using a Linux version with a rolling release update model, I believe the issue to be related to a version mismatch due to the recency of the packages, which fits your explanation. However, the core dumps vanished after updating my system (and rebooting). Thank you for your help!"
yi,Issue with static fields in `in_axes` pytree structure in `jax.vmap`," Description Hi, I am encountering an issue with the `in_axes` parameter when using `jax.vmap`. Specifically, I am passing `in_axes` as a pytree that mirrors the structure of the arguments for the function I am applying `vmap` to. However, due to certain applicationspecific requirements, the static fields (i.e., data elements that are bypassed by `tree_map`) within both pytrees do not match. This discrepancy leads to an error indicating a mismatch in the structures of the two pytrees, despite their relevant structures being identical. The error is resolved when I ensure that all static fields are identical across both pytrees. It appears that the root cause of this issue might be related to the `treedefs.node_data()` for `in_axes` and the corresponding values not matching. Below a MWE: ```python import jax import jax.numpy as jnp from flax import struct .dataclass class Bar:     b: jax.typing.ArrayLike     static_field: object = struct.field(pytree_node=False, default=None) .dataclass class Foo:     a: jax.typing.ArrayLike     bar: Bar     static_field: int = struct.field(pytree_node=False, default=None) val = Foo(jnp.array([1]), Bar(jnp.array([1]), static_field=1), static_field=2) in_axes = jax.tree_util.tree_map(lambda x: 0, val) in_axes = in_axes.replace(static_field=99)    Uncommment and error disappears. jax.vmap(lambda x: x, in_axes=(in_axes,))(val) ``` Results in: ```python ValueError: vmap in_axes specification must be a tree prefix of the corresponding value, got specification (Foo(a=0, bar=Bar(b=0, static_field=1), static_field=99),) for value tree PyTreeDef((CustomNode(Foo[(2,)], [*, CustomNode(Bar[(1,)], [*])]),)). ```  What jax/jaxlib version are you using? jax v0.4.23, jaxlib v0.4.23+cuda12.cudnn89  Which accelerator(s) are you using? CPU/GPU  Additional system info? Ubuntu 20.04  NVIDIA GPU info ``` ```",2024-02-09T10:47:04Z,bug,open,0,4,https://github.com/jax-ml/jax/issues/19729,"Hi  this is moreorless working as expected, because pytree equivalence includes equivalence of static elements. Can you say more about why it's not possible to specify `in_axes` with an equivalent pytree to the input?","In our application, the construction of the `in_axes` and `val` pytrees occurs at distinct stages due to the nature of the codebase. Initially, when the `in_axes` pytree is assembled, some static data necessary for its completion is not yet available, leading us to fill these fields with `None` as placeholders. Only at a later stage, when we construct the `val` pytree, do we have access to all the requisite static data, allowing us to populate it accordingly. This sequential process results in a mismatch between the static elements of the `in_axes` and `val` pytrees. Adding to the complexity, the structure of the `in_axes` pytree does not mirror the `val` pytree exactly. Specifically, the `in_axes` pytree includes `None` leaves in places where the `val` pytree contains pytree nodes. This setup is designed so  that the `None` values in `in_axes` correspond to all leaves under those nodes in the `val` pytree. I did not anticipate the inclusion of static data as a factor for pytree equivalence, as my expectation was that this only concerned the ""nonstatic part"". Moreover, this equivalence is not strictly enforced, since `None` values for leafs where the `val` pytree has a deeper pytree structure is generally allowed. This expectation was partly due to my interpretation of the documentation on the `in_axes` parameter, which did not explicitly state the necessity for identical static data.","I don't entirely follow – if `None` can be used in place of full subtrees, how could you possibly expect a generic `in_axis` to match a runtime pytree that may have more leaves than the specification? Overall, my recommendation would be to specify `in_axes` at a point where you actually have the data, and therefore know what the `in_axes` specification should be.","Using `None` or any `int` axis specification as leaves in `in_axes` where `val` contains full subtrees appears to function correctly (refer to the provided minimal working example). Looking at the internals, it seems this functionality is supported through this line, where the (partially incomplete) `in_axes` pytree is extended with the missing subtrees from `val`. This is also where the static data error comes from. This working is also discussed here.  Initially, it was not apparent that `in_axes` and `val` pytrees should mirror each other structurally, especially since ""prefix"" pytrees can define the `in_axes` parameter for whole subtrees. The error message, lacking details about the necessity for static data alignment, were confusing, leading to some time spent troubleshooting. While I still don't really understand why the static data of `in_axes` should exactly match that of `val`, a more informative error message that states this would already be helpful! In the example below I replace `in_axes.bar=None` such that `in_axes` has a None value in a place with `val` has a subtree. Internally, this line extends the None value to all leafs of the subtree. ```python import jax import jax.numpy as jnp from flax import struct .dataclass class Bar:     a: jax.typing.ArrayLike     b: jax.typing.ArrayLike     static_field: object = struct.field(pytree_node=False, default=None) .dataclass class Foo:     a: jax.typing.ArrayLike     bar: Bar     static_field: int = struct.field(pytree_node=False, default=None) val = Foo(jnp.array([1]), Bar(jnp.array([1]), jnp.array([1]), static_field=1), static_field=2) in_axes = jax.tree_util.tree_map(lambda x: 0, val) in_axes = in_axes.replace(bar=None)    prefix pytree jax.vmap(lambda x: x, in_axes=(in_axes,))(val) ```"
yi,"Sharp edges around various collective ops, e.g. `axis_name=()` and negative indices"," Description Various semantics are a bit confusing. What should happen with degenerate axis names, e.g. `axis_name=()`? Right now behavior is inconsistent. ```python import jax from jax import numpy as jnp print('psum', jax.lax.psum(1, ())) print('axis_index', jax.lax.axis_index(())) x = jnp.arange(4).reshape((2, 2)) try:   jax.lax.all_gather(x, ()) except Exception as e:   print('all_gather failed:', e) try:   jax.lax.all_to_all(x, (), 0, 0, tiled=True) except Exception as e:   print('all_to_all failed:', e) try:   jax.lax.ppermute(x, (), ((0, 0),)) except Exception as e:   print('ppermute failed:', e) try:   jax.lax.psum_scatter(x, (), tiled=True) except Exception as e:   print('psum_scatter failed:', e) ``` produces ``` psum 1 axis_index 0 all_gather failed: Unexpected call to _all_gather_impl all_to_all failed: Evaluation rule for 'all_to_all' not implemented ppermute failed: Evaluation rule for 'ppermute' not implemented psum_scatter failed: Evaluation rule for 'reduce_scatter' not implemented ``` How should we deal with negative indices? It works for `all_gather` but not `psum_scatter` or `all_to_all`. ```python import functools import jax from jax import numpy as jnp from jax import sharding from jax.experimental import shard_map x = jnp.arange(64).reshape((8, 8)) mesh = sharding.Mesh(jax.devices(), ('x',)) .partial(     shard_map.shard_map,     mesh=mesh,     in_specs=(sharding.PartitionSpec(None, 'x'),),     out_specs=sharding.PartitionSpec(), ) def f(x):   print('all_gather', jax.lax.all_gather(x, 'x', axis=1))   try:     print('all_to_all', jax.lax.all_to_all(x, 'x', 2, 1, tiled=True))   except Exception as e:     print('all_to_all(..., 2, 1, tiled=True) failed:', e)   print(       'all_to_all',       jax.lax.all_to_all(           x, 'x', 2 % len(x.shape), 1 % len(x.shape), tiled=True       ),   )   try:     print(         'psum_scatter',         jax.lax.psum_scatter(x, 'x', scatter_dimension=2, tiled=True),     )   except Exception as e:     print('psum_scatter(..., scatter_dimension=2, tiled=True) failed:', e)   print(       'psum_scatter',       jax.lax.psum_scatter(           x, 'x', scatter_dimension=2 % len(x.shape), tiled=True       ),   )   return () f(x) ``` produces ``` all_gather On TPU_0(process=0,(0,0,0,0)) at mesh coordinates (x,) = (0,): [[[ 0  1  2  3  4  5  6  7]]  [[ 8  9 10 11 12 13 14 15]]  [[16 17 18 19 20 21 22 23]]  [[24 25 26 27 28 29 30 31]]  [[32 33 34 35 36 37 38 39]]  [[40 41 42 43 44 45 46 47]]  [[48 49 50 51 52 53 54 55]]  [[56 57 58 59 60 61 62 63]]]  all_to_all(..., 2, 1, tiled=True) failed: Operation creation failed all_to_all On TPU_0(process=0,(0,0,0,0)) at mesh coordinates (x,) = (0,): [[0 1 2 3 4 5 6 7]]  psum_scatter(..., scatter_dimension=2, tiled=True) failed: Cannot lower jaxpr with verifier errors: 	expects scatter_dimension >= 0 		at loc(""jit()/jit(main)/jit(shmap_body)/reduce_scatter[axis_name=('x',) scatter_dimension=2 axis_index_groups=None axis_size=8 tiled=True]""(callsite(""f""("""":35:8) at callsite(""""("""":49:0) at callsite(""InteractiveShell.run_code""(""third_party/py/IPython/v3_2_3/core/interactiveshell.py"":3066:16) at callsite(""InteractiveShell.run_ast_nodes""(""third_party/py/IPython/v3_2_3/core/interactiveshell.py"":3012:19) at callsite(""InteractiveShell.run_cell""(""third_party/py/IPython/v3_2_3/core/interactiveshell.py"":2901:16) at callsite(""IPythonKernel.do_execute""(""third_party/py/IPython/v3_2_3/kernel/zmq/ipkernel.py"":181:12) at callsite(""Kernel.execute_request""(""third_party/py/IPython/v3_2_3/kernel/zmq/kernelbase.py"":361:24) at callsite(""ColabKernel.execute_request""(""research/colab/notebook/colab_kernel.py"":223:4) at callsite(""Kernel.dispatch_shell""(""third_party/py/IPython/v3_2_3/kernel/zmq/kernelbase.py"":213:16) at ""Kernel.start..make_dispatcher..dispatcher""(""third_party/py/IPython/v3_2_3/kernel/zmq/kernelbase.py"":252:23))))))))))))Define JAX_DUMP_IR_TO to dump the module. psum_scatter On TPU_0(process=0,(0,0,0,0)) at mesh coordinates (x,) = (0,): [[28]]  () ``` Obviously, I can add `if axis_name:` around my code or do `negative_index % len(x.shape)` but it's not ideal.  What jax/jaxlib version are you using? 0.4.24  Which accelerator(s) are you using? _No response_  Additional system info? _No response_  NVIDIA GPU info _No response_",2024-02-08T20:58:46Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/19720
yi,"""E1130: bad operand type for unary -: ArrayImpl (invalid-unary-operand-type)"" pylint errors in latest release (0.4.24)"," Description Running pylint (`pylint test.py`) on this simple file ``` """"""Test file for 'E1130: bad operand type for unary :'."""""" from jax import numpy as jnp a = jnp.ones([]) ``` fails with the latest release of jax (0.4.24) : ``` ❯ pylint test.py ************* Module test test.py:4:4: E1130: bad operand type for unary : ArrayImpl (invalidunaryoperandtype)  ``` The same code doesn't raise any error with jax 0.4.23. This has led to some tests failures in other packages such as optax  What jax/jaxlib version are you using? 0.4.24 0.4.24  Which accelerator(s) are you using? CPU  Additional system info? Tried on both linux and mac  NVIDIA GPU info _No response_",2024-02-08T11:42:40Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/19713,"Hi  thanks for the report. Do you understand where pytype is inferring that `ones` returns `ArrayImpl`? The pyi file has not changed, and declares that `ones` returns `Array` unconditionally: https://github.com/google/jax/blob/4c505f8bac45517970149b13343988090fdaa920/jax/numpy/__init__.pyiL610L611 I just doublechecked that the pyi file is included in the v0.4.24 distribution, so I'm not sure why pytype would be ignoring it. Do you have any ideas?","Thanks Jake for the prompt reply. No, I don't understand why pylint is doing that inference. A couple of more observations: * changing `jnp.ones([])` to `jnp.negative(jnp.ones([]))` works fine (does not raise the error) * I don't think the error is specific to `jnp.ones`. In particular, `jnp.full([], 2.)` has the same issue","Chatting with , it seems this is coming from `pylint`, not `pytype`. `pylint` has some level of type checking builtin, but it seems that it ignores `pyi` files and thus will not work correctly with dynamicallydefined attributes. Here's a repro: ```bash $ ls mymodule.py  mymodule.pyi  test.py $ cat mymodule.py class Foo:   pass setattr(Foo, ""__neg__"", lambda self: self) $ cat mymodule.pyi class Foo:   def __neg__(self) > Foo: ... $ cat test.py """"""Test file for incorrect E1130"""""" from mymodule import Foo result = Foo() $ mypy test.py Success: no issues found in 1 source file $ pylint test.py ************* Module test test.py:3:9: E1130: bad operand type for unary : Foo (invalidunaryoperandtype)  Your code has been rated at 0.00/10 (previous run: 0.00/10, +0.00) ``` So in summary, this looks like a bug in `pylint` – it's trying to do type checking without taking into account the interface files that we've defined for the sake of type checking. What do you think?",Thanks  for reporting this upstream! I'll close the issue here since it does seem more of an issue in pylint
yi,[JAX] Add an option subset_by_index that allows computing a contiguous subset of singular components from svd.,[JAX] Add an option subset_by_index that allows computing a contiguous subset of singular components from svd.,2024-02-08T00:28:51Z,,closed,0,1,https://github.com/jax-ml/jax/issues/19707,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
yi,shard_map *much* faster than pjit for simple data parallelism ," Description I'm trying to scale up some transformer training (currently at ~400m params), and as such I've been playing around with various ways to save memory and improve performance. On a whim, I tried replacing my `jax.jit(in_shardings=..., out_shardings=...)` setup for data parallelism with `jax.experimental.shard_map`, as so: ```python     from jax.experimental import mesh_utils, shard_map     mesh = jax.sharding.Mesh(mesh_utils.create_mesh([jax.device_count()]), [""dp""])     dp_spec = jax.sharding.PartitionSpec(""dp"")     rep_spec = jax.sharding.PartitionSpec()     dp_sharding = jax.sharding.NamedSharding(mesh, dp_spec)     rep_sharding = jax.sharding.NamedSharding(mesh, rep_spec)     (         jax.jit,         in_shardings=(rep_sharding, dp_sharding),         out_shardings=(rep_sharding, rep_sharding),         donate_argnums=0,     )      ADDED THE FOLLOWING 7 LINES      (         shard_map.shard_map,         mesh=mesh,         in_specs=(rep_spec, dp_spec),         out_specs=(rep_spec, rep_spec),         check_rep=False,     )          def train_step(state: TrainState, batch: Data):         rng, dropout_rng = jax.random.split(state.rng)         (loss, info), grads = jax.value_and_grad(loss_fn, has_aux=True)(             state.model.params, batch, dropout_rng, train=True         )          AND THE FOLLOWING LINE          loss, info, grads = jax.lax.pmean((loss, info, grads), axis_name=""dp"")                  new_state = state.apply_gradients(grads=grads, rng=rng)         return new_state, info ``` and I immediately saw a 2.8x (!) speedup. The reason why this is a problem is because I would like to move on to more advanced parallelism techniques (tensor parallel, fullysharded data parallel, etc) but it seems like it would be prohibitively difficult to write these manually using `shard_map`. However, if I continue using pjit's automatic partitioning, I worry that I'm leaving a bunch of performance on the table. I would think the automatic partitioner would be able to produce code with more or less equal performance in this very simple case. Here are the debugging steps I've tried so far:  I used `jax.debug.inspect_array_sharding` to look at the sharding of intermediate activations, and they all looked correct (fully sharded along the DP axis)   I added a bunch of sharding annotations anyway just to to make sure  I used `custom_vjp` to look at shardings during the backward pass, and they also looked correct  I dumped the HLO for both versions, but it's hard to make heads or tails of. If anything, it looks like the shard_map version has more AllReduce operations than the pjit version (10 vs 2). I've attached the HLO below. I would really appreciate any guidance on this, thanks! noshardmap.txt shardmap.txt  What jax/jaxlib version are you using? 0.4.23  Which accelerator(s) are you using? TPUv4  Additional system info? Python 3.10.12, tpuvmv4base  NVIDIA GPU info _No response_",2024-02-04T22:51:42Z,bug,closed,0,12,https://github.com/jax-ml/jax/issues/19657,"Hi  I'm having trouble understanding your question. It sounds like you're comparing two implementations, but you've only shown us one implementation. Could you edit your question to show the code for both approaches?"," sorry, I've edited my question to hopefully make things more clear. The only difference between the two implementations is the addition of the 8 lines indicated (the shard_map itself and the corresponding pmean). ","Thanks! Assigning to , who might have some insights here.",Don't you need to `jnp.mean` for the jit version (without shard_map)?, The `jnp.mean` happens inside the loss function (a scalar is returned).,I don't see that loss function :) Can you create a minimal reproducer that we can run?,"Sure thing, here's my repro. With the shard_map version, I get 1.09 s/it, and with no shard_map, I get 2.95 s/it. This is on a v48 TPU VM.","Hey  sorry for the late reply, can you try with the latest jax and jaxlib version? (or better try with nightly) Also can you tell me what TPU you were using? TPUv4 but how many devices? ","This was a v48 VM (smallest you can get, I think). I no longer have easy access to TPUs, but I replicated the issue with `jax[cuda12]==0.4.33` on an 8xH100 DGX machine. With no shard map, I get `1.10s/it`, and with shard map, I get `1.70it/s`.","The culprit ended up being dropout, and the unpartitionable threefry algorithm (ref). Either removing dropout or setting `jax.config.update(""jax_threefry_partitionable"", True)` speeds up the pjit version to match the shard_map version.","Oof, sorry that caused so much pain. We've been meaning to switch it on by default, but haven't landed it since it requires updating a bunch of downstream google monorepo tests. ","No worries, looking forward for that to land! (I totally understand, because we've also had tests fail several times thanks to inconsistent RNG algorithms 😅 ). Luckily this did not actually affect our internal training runs, since we *were* using `jax_threefry_partitionable`  I was mainly concerned about this issue because I thought it was revealing some much larger misunderstanding of pjit/shard_map on my part, so it's actually a relief that it was something unrelated."
rag,CUDA driver not found after installing with cuda12_local wheel," Description ``` Python 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> from jax.lib import xla_bridge 20240204 13:19:26.381840: I external/tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used. 20240204 13:19:26.399021: I external/tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used. 20240204 13:19:26.399574: I external/tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used. >>> exit() ``` The above output was encountered after installing JAX on WSL2 (Ubuntu 22.04) with the cuda12_local wheel.   Specifically, the command to install JAX was:   `pip3 install upgrade ""jax[cuda12_local]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` Prior to running the pip install command, I have installed the CUDA 12.3 and CuDNN 8.9.   CUDA was installed the standard WSL way (with driver on host Windows machine and rest of CUDA in the Linux system).   From the JAX installation page, I gather that the wheel installed is for CUDA 12.2 and CuDNN 8.9, but CUDA 12.3 should also work due to having the same major version and a newer minor version. I have checked that `/usr/local/cuda12.3/bin` is in PATH, and that `/usr/local/cuda12.3/lib64` is in LD_LIBRARY_PATH. Is this combination of CUDA, CuDNN, and JAX supported? Please advise on the recommended way to run JAX with GPU on WSL2.   Thanks!  What jax/jaxlib version are you using? 0.4.23, 0.4.23  Which accelerator(s) are you using? GPU  Additional system info? _No response_  NVIDIA GPU info ``` Sun Feb  4 13:23:16 2024 ++  ++ ```",2024-02-04T21:37:38Z,bug,open,1,2,https://github.com/jax-ml/jax/issues/19655,similiar issue with 12.5.  Error I got says  jax 0.4.28 does not provide the extra 'cuda12local',"Hi Zhao,  I have tested the issue with latest JAX cuda local version 0.4.33 on WSL with NVIDIA RTX A5000 GPU. I could not reproduce the issue. The `xla_bridge` was imported successfully. Please find the attached screeshot for reference: !image !image Could you please verify with latest JAX cuda local version, if the issue still exists? Thank you."
yi,.at[].set() is dangerous," Description The inplace modification of arrays in jax is dangerous, and behavior of \.at[] under  breaks the outofplace (functional) paradigm. The documentation page carries a warning: ``` Unlike NumPy inplace operations such as x[idx] += y, if multiple indices refer to the same location, all updates will be applied (NumPy would only apply the last update, rather than applying all updates.) The order in which conflicting updates are applied is implementationdefined and may be nondeterministic (e.g., due to concurrency on some hardware platforms). ``` However, the different behavior under jit and nonjit may cause difficult to debug data issues.... Case in point ```  def inverse_permutation(X, idx):     return X.at[idx, :].set(X) ``` given a nontrivial permutation `idx`, this simple function will return a corrupted version of X (in my case there were nan's, luckily)... I wonder what other nondeterministic behavior usage of .at[].set() might be enabling in jax. I understand that there is a desire for performance optimization, however correctness should take precedence. Perhaps a warning could be printed if assigning an array to itself with permutation indexing.  What jax/jaxlib version are you using? 0.4.24.dev20240122+b512b576a (jax), 0.4.24.dev20240122 (jaxlib)  Which accelerator(s) are you using? GPU, ROCm  Additional system info? Linux login2 5.14.21150400.24.46_12.0.83cray_shasta_c CC(Python 3 compatibility issues) SMP Tue May 23 03:16:47 UTC 2023 (c6cda89) x86_64 x86_64 x86_64 GNU/Linux  AMD GPU info ``` ========================= ROCm System Management Interface ========================= =================================== Concise Info =================================== GPU[1]		: get_power_avg, Not supported on the given system GPU  Temp (DieEdge)  AvgPwr  SCLK    MCLK     Fan  Perf  PwrCap  VRAM%  GPU%   0    32.0c           91.0W   800Mhz  1600Mhz  0%   auto  560.0W    0%   0%     1    39.0c           N/A     800Mhz  1600Mhz  0%   auto  0.0W      0%   0%     ==================================================================================== =============================== End of ROCm SMI Log ================================ ```",2024-02-03T00:53:21Z,bug,open,1,6,https://github.com/jax-ml/jax/issues/19643,Update: I observed this in automatic SPMD (multi rank) calculations on sharded jax.Array's... but still trying to pin down the source of the data corruption (can't seem to be able to reproduce on single GPU),"Just to try and make what's going on here a bit clearer to me. First of all, doing `X.at[idx].set(...)`, where `idx` has repeated indices, is indeed undefined behaviour in JAX (regardless of what is being set). This is what the existing warning is about / is known / is considered fine. Are you saying that you've found a case in which `idx` does *not* have repeated indices, but for which specifically the pattern `X.at[idx].set(X)` produces the wrong result? If so, that's a bug.","> Just to try and make what's going on here a bit clearer to me. >  > First of all, doing `X.at[idx].set(...)`, where `idx` has repeated indices, is indeed undefined behaviour in JAX (regardless of what is being set). This is what the existing warning is about / is known / is considered fine. >  > Are you saying that you've found a case in which `idx` does _not_ have repeated indices, but for which specifically the pattern `X.at[idx].set(X)` produces the wrong result? If so, that's a bug. No, no ... I was just under the assumption that jax would do out of place transforms all the time and was surprised that it does inplace, but only under certain conditions ... (namely, jit)... so the sometimes inplace, sometimes out of place behavior seems confusing","Right. So logically, `.at[].set()` always operates outofplace. But physically, the XLA compiler (the backend to JAX) will attempt to do `.at[].set()` inplace where possible. Doing this is important for performance, to avoid lots of copies of the underlying data. You'll see similar behaviour in the compilers for almost all functional languages. Whether something happens inplace or outofplace shouldn't be observable from a user perspective, though. (Other than indirectly, through things like computation time or memory usage.) How is it that you think you're observing this distinction?"," Here's a code sample to elaborate on what Patrick is saying: ```python import jax import jax.numpy as jnp def f(x):     return x.at[3].set(0) x = jnp.arange(5) print('x:', x) print('\nf(x):', f(x)) print('x after f:', x) print('\njit(f)(x):', jax.jit(f)(x)) print('x after jit(f):', x) print('\njit(f)(x) with donate:', jax.jit(f, donate_argnums=(0,))(x)) print('x after jit(f) with donate:', x) Output:  x: [0 1 2 3 4]  f(x): [0 1 2 0 4]  x after f: [0 1 2 3 4]  jit(f)(x): [0 1 2 0 4]  x after jit(f): [0 1 2 3 4]  jit(f)(x) with donate: [0 1 2 0 4]  RuntimeError: Array has been deleted with shape=int32[5]. ``` Basically it's just that JAX will avoid doing inplace operations on transformed function _arguments_ unless explicitly given permission to. (This includes constants which your function closes over). The inplace behavior will ideally happen for intermediate tensors which you don't have direct visibility into anyways, and you can optin to your input arguments getting clobbered with `donate_argnums`.","Hi  thanks for the report – I recall in the past XLA fixed a similar bug, where something like `x.at[idx].set(x)` would return the wrong result under JIT, but I can't seem to find the previous issue. I tried briefly to reproduce this, but wasn't able to on a CPU or GPU runtime. Would you be able to put together a minimal example of inputs for which you're seeing this problematic behavior?"
yi,"Import submodules from jax._src explicitly, instead of relying on import side-effects. It will lead to the missing x-refs in code search according to go/pywald-sawmill-analysis.","Import submodules from jax._src explicitly, instead of relying on import sideeffects. It will lead to the missing xrefs in code search according to go/pywaldsawmillanalysis.",2024-02-03T00:28:01Z,,closed,0,1,https://github.com/jax-ml/jax/issues/19642,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
yi,More type annotations of Pallas primitives,I also noticed annoying errors on multiple_of so figured I'd annotate the rest of the primitives.,2024-02-01T00:50:22Z,pull ready,open,0,4,https://github.com/jax-ml/jax/issues/19608,Hi  can you squash your changes into a single commit? See https://jax.readthedocs.io/en/latest/contributing.htmlsinglechangecommitsandpullrequests. Thanks!,"Done, was being lazy and doing things from the Github UI. I squashed. I was just rebasing to get around the CI flake.",Thanks!,"This is leading to some pytype errors because `k_offset` below is annotated as an int, but is being passed the output of `program_id`: https://github.com/google/jax/blob/6e17bb3aa5aa9c612fd3942c0ae48c70cecba801/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_kernel.pyL585 I suspect the best fix there is to change this annotation to `int | Array`."
rag,[shmap] Support multiple axes for standard collectives in shard_map,Added test coverage and have tested several nontrivial use cases like collective matrix multiplication. Note that I don't actually see the `all_gather_invariant` anywhere in the code that is mentioned here: https://jax.readthedocs.io/en/latest/jep/17111shmaptranspose.htmltrackingdeviceinvarianceinavalsakaavalswithnamesrevived. Fixes CC([shmap] shard_map doesn't support multiple axes).,2024-01-31T17:14:46Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/19601
yi,gRPC error during multiprocess on Google Compute Engine instance," Description I am trying to get a simple multiprocess script working on a Google Cloud instance with four T4s. I've gotten multiprocess working elsewhere, but something seems to be broken or incompatible with the GCE infra. Here is my test script: ```python  Identify rank import os local_rank = os.environ.get('OMPI_COMM_WORLD_LOCAL_RANK')  Initialize JAX for distributed computing import jax jax.distributed.initialize()  Confirm which device we are using print(f""Process {local_rank} sees devices: "", jax.local_devices()) device = jax.local_devices()[0]  Ten ones per local device xs = jax.numpy.ones([jax.local_device_count(), 10]) print(local_rank, xs, xs.devices()) out = jax.pmap(lambda x: jax.lax.psum(x.sum(), axis_name='i'), axis_name='i')(xs) print(local_rank, out) ``` It crashes during the final step due to the `psum`. (Removing the `psum` there is no crash.) This is the error message: ```bash $ mpirun np 4 H localhost:4 python3 jaxtest.py Process 3 sees devices:  [cuda(id=3)] Process 2 sees devices:  [cuda(id=2)] Process 0 sees devices:  [cuda(id=0)] Process 1 sees devices:  [cuda(id=1)] 3 [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]] {cuda(id=3)} 2 [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]] {cuda(id=2)} 1 [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]] {cuda(id=1)} 0 [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]] {cuda(id=0)} 20240131 10:21:35.025511: W external/tsl/tsl/distributed_runtime/preemption/preemption_sync_manager.cc:170] Failed to retrieve preemption notice from coordination service: UNAVAILABLE: Socket closed Additional GRPC error information from remote target unknown_target_for_coordination_leader while calling /tensorflow.CoordinationService/GetKeyValue: :{""created"":"".008693800"",""description"":""Error received from peer ipv4:10.150.0.4:62464"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}. This is only expected if one of the tasks is unhealthy. Check the logs for the actual root cause. 20240131 10:21:35.025469: W external/tsl/tsl/distributed_runtime/preemption/preemption_sync_manager.cc:170] Failed to retrieve preemption notice from coordination service: UNAVAILABLE: Socket closed Additional GRPC error information from remote target unknown_target_for_coordination_leader while calling /tensorflow.CoordinationService/GetKeyValue: :{""created"":"".011876844"",""description"":""Error received from peer ipv4:10.150.0.4:62464"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}. This is only expected if one of the tasks is unhealthy. Check the logs for the actual root cause. 20240131 10:21:35.025531: W external/tsl/tsl/distributed_runtime/preemption/preemption_sync_manager.cc:170] Failed to retrieve preemption notice from coordination service: UNAVAILABLE: Socket closed Additional GRPC error information from remote target unknown_target_for_coordination_leader while calling /tensorflow.CoordinationService/GetKeyValue: :{""created"":"".011629625"",""description"":""Error received from peer ipv4:10.150.0.4:62464"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}. This is only expected if one of the tasks is unhealthy. Check the logs for the actual root cause. 20240131 10:21:35.036603: E external/tsl/tsl/distributed_runtime/preemption/preemption_sync_manager.cc:182] Failed to cancel preemption barrier: UNAVAILABLE: failed to connect to all addresses Additional GRPC error information from remote target unknown_target_for_coordination_leader while calling /tensorflow.CoordinationService/CancelBarrier: :{""created"":"".036460874"",""description"":""Failed to pick subchannel"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3941,""referenced_errors"":[{""created"":"".036458810"",""description"":""failed to connect to all addresses"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":393,""grpc_status"":14}]} 20240131 10:21:35.036777: E external/tsl/tsl/distributed_runtime/preemption/preemption_sync_manager.cc:182] Failed to cancel preemption barrier: UNAVAILABLE: failed to connect to all addresses Additional GRPC error information from remote target unknown_target_for_coordination_leader while calling /tensorflow.CoordinationService/CancelBarrier: :{""created"":"".036747775"",""description"":""Failed to pick subchannel"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3941,""referenced_errors"":[{""created"":"".036746296"",""description"":""failed to connect to all addresses"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":393,""grpc_status"":14}]} 20240131 10:21:35.036951: E external/tsl/tsl/distributed_runtime/preemption/preemption_sync_manager.cc:182] Failed to cancel preemption barrier: UNAVAILABLE: failed to connect to all addresses Additional GRPC error information from remote target unknown_target_for_coordination_leader while calling /tensorflow.CoordinationService/CancelBarrier: :{""created"":"".036925138"",""description"":""Failed to pick subchannel"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3941,""referenced_errors"":[{""created"":"".036923973"",""description"":""failed to connect to all addresses"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":393,""grpc_status"":14}]}  Primary job  terminated normally, but 1 process returned a nonzero exit code. Per userdirection, the job has been aborted.  20240131 10:21:37.353041: W external/tsl/tsl/distributed_runtime/preemption/preemption_notifier.cc:89] SIGTERM caught at 20240131T10:21:37.350786369+00:00 20240131 10:21:37.353179: W external/tsl/tsl/distributed_runtime/preemption/preemption_notifier.cc:89] SIGTERM caught at 20240131T10:21:37.350804131+00:00 20240131 10:21:37.352902: W external/tsl/tsl/distributed_runtime/preemption/preemption_notifier.cc:89] SIGTERM caught at 20240131T10:21:37.350697216+00:00  mpirun noticed that process rank 0 with PID 0 on node gpudev3 exited on signal 9 (Killed).  ``` I've done what I can to debug this but no luck. I've tried multiple installs of mpirun, NCCL, both localcuda & pipcuda JAX installers, and adjusting firewall settings. I'm confused why the coordination step (during `.initialize()`) would work, only to fail later during the `.psum`.  I've also run the `nccltest` suite and that all seems to work fine.  What jax/jaxlib version are you using? jax v0.4.23, jaxlib v0.4.23  Which accelerator(s) are you using? GPU  Additional system info? I've tried both Python3.10 & 3.11. 1.26.3 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] uname_result(system='Linux', node='gpudev3', release='6.2.01019gcp', version=' CC(Typo)~22.04.1Ubuntu SMP Thu Nov 16 18:18:34 UTC 2023', machine='x86_64')  NVIDIA GPU info ``` ++  ++ ```",2024-01-31T10:28:11Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/19596
llama,"Getting loss as `nan` with jit, but no `nan`s without jit"," Description Using keras 3 with JAX backend.  jit_compile=True * After few epochs of training properly, I started getting the loss as `nan`. (Trains properly without jit) * So I enabled `jax.config.update(""jax_debug_nans"", True)` and got the error after training a few epochs properly ```python { 	""name"": ""FloatingPointError"", 	""message"": ""invalid value (nan) encountered in jit(compiled_train_step). Because jax_config.debug_nans.value and/or config.jax_debug_infs is set, the deoptimized function (i.e., the function as if the `jit` decorator were removed) was called in an attempt to get a more precise error message. However, the deoptimized function did not produce invalid values during its execution. This behavior can result from `jit` optimizations causing the invalid value to be produced. It may also arise from having nan/inf constants as outputs, like `jax.jit(lambda ...: jax.numpy.nan)(...)`.  It may be possible to avoid the invalid value by removing the `jit` decorator, at the cost of losing optimizations.  If you see this error, consider opening a bug report at https://github.com/google/jax."", 	""stack"": "" FloatingPointError                        Traceback (most recent call last) Cell In[21], line 1 > 1 history = llama.fit(       2     x=datasets[\""train\""],       3     epochs=TArgs.epochs,       4     steps_per_epoch=TArgs.steps_per_epoch,       5     validation_data=datasets[\""val\""],       6      callbacks=[EvaluateModel()],, saveW]       7 ) File ~/.local/lib/python3.11/sitepackages/keras/src/utils/traceback_utils.py:123, in filter_traceback..error_handler(*args, **kwargs)     120     filtered_tb = _process_traceback_frames(e.__traceback__)     121      To get the full stack trace, call:     122      `keras.config.disable_traceback_filtering()` > 123     raise e.with_traceback(filtered_tb) from None     124 finally:     125     del filtered_tb File ~/.local/lib/python3.11/sitepackages/jax/_src/api.py:115, in _nan_check_posthook(fun, args, kwargs, output)     112 assert config.debug_nans.value or config.debug_infs.value     113 print(\""Invalid nan value encountered in the output of a C++jit/pmap \""     114       \""function. Calling the deoptimized version.\"") > 115 fun._cache_miss(*args, **kwargs)[0]     [... skipping hidden 9 frame] File ~/.local/lib/python3.11/sitepackages/jax/_src/pjit.py:1209, in _pjit_call_impl_python(jaxpr, in_shardings, out_shardings, resource_env, donated_invars, name, keep_unused, inline, *args)    1192  If control reaches this line, we got a NaN on the output of `compiled`    1193  but not `fun.call_wrapped` on the same arguments. Let's tell the user.    1194 msg = (f\""{str(e)}. Because \""    1195        \""jax_config.debug_nans.value and/or config.jax_debug_infs is set, the \""    1196        \""deoptimized function (i.e., the function as if the `jit` \""    (...)    1207        \""If you see this error, consider opening a bug report at \""    1208        \""https://github.com/google/jax.\"") > 1209 raise FloatingPointError(msg) FloatingPointError: invalid value (nan) encountered in jit(compiled_train_step). Because jax_config.debug_nans.value and/or config.jax_debug_infs is set, the deoptimized function (i.e., the function as if the `jit` decorator were removed) was called in an attempt to get a more precise error message. However, the deoptimized function did not produce invalid values during its execution. This behavior can result from `jit` optimizations causing the invalid value to be produced. It may also arise from having nan/inf constants as outputs, like `jax.jit(lambda ...: jax.numpy.nan)(...)`.  It may be possible to avoid the invalid value by removing the `jit` decorator, at the cost of losing optimizations.  If you see this error, consider opening a bug report at https://github.com/google/jax."" } ```  jit_compile=False * The model runs perfectly fine and converges with no `nan`s (according to the error I think this is an Error from JAX and not Keras, so I posted the error here...)  What jax/jaxlib version are you using? 0.4.23 0.4.23  Which accelerator(s) are you using? GPU  Additional system info? > 1.26.2 3.11.7 (main, Dec  8 2023, 18:56:58) [GCC 11.4.0] uname_result(system='Linux', node='Enigma', release='6.5.015generic', version='15~22.04.1Ubuntu SMP PREEMPT_DYNAMIC Fri Jan 12 18:54:30 UTC 2', machine='x86_64')  NVIDIA GPU info _No response_",2024-01-31T10:19:54Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/19595,"Had a bug in my implementation of the model, now I corrected it, no `nan`s. Wonder why `nan` were coming only when `jit` was enabled though."
yi,log1p results are incorrect on complex inputs with large absolute value," Description As in the title. The results of evaluating log1p on a grid over complex plane are reported in NumPyvsJAX. As an example, consider a sample from the report for complex32 inputs using CPU: ```python >>> jnp.log1p(5e19 + 0j) Array(inf+0.j, dtype=complex64, weak_type=True) >>> jnp.log1p(5e19)        expected real part of the result Array(45.358555, dtype=float32, weak_type=True) ```  What jax/jaxlib version are you using? 0.4.24.dev20240130+66308c30a  Which accelerator(s) are you using? CPU/GPU  Additional system info? 1.26.2 3.11.0  ++ ```",2024-01-30T14:52:07Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/19573,It turns out that NumPy provided log1p has inaccuracy issues for inputs with small absolute values. A better illustration of log1p inaccuracies is obtained when using MPMath library as a reference. Here are some samples where JaX `log1py` returns incorrect results: ```                                      x >                   jax.log1p(x)                       reference: mpmath log1p(x) (7.70639220474525e+2011082385063936j) >   (inf1.4380769464139576e08j)    (48.0937538146972661.4380769464139576e08j) (11082385063936+7.70639220474525e+20j) >       (inf+1.5707963705062866j)        (48.093753814697266+1.5707963705062866j)  (1213608919047.70639220474525e+20j) >        (inf1.570796251296997j)        (48.0937538146972661.5707963705062866j)  (121360891904+7.70639220474525e+20j) >        (inf+1.570796251296997j)        (48.093753814697266+1.5707963705062866j)            (5.358817286521974e+28+0j) >       (inf+3.1415927410125732j)           (66.151123046875+3.1415927410125732j)             (5.358817286521974e+28+0j) >                        (inf+0j)                            (66.151123046875+0j)                 4.893543125372159e+30j >       (inf+1.5707963705062866j)         (70.66546630859375+1.5707963705062866j)                5.358817286521974e+28j >       (inf1.5707963705062866j)           (66.1511230468751.5707963705062866j)              (inf19.11201286315918j) >       (nan3.1415927410125732j)                       (inf3.1415927410125732j)              (inf+19.11201286315918j) >       (nan+3.1415927410125732j)                       (inf+3.1415927410125732j)                              (inf+0j) >       (nan+3.1415927410125732j)                                      (inf+nanj)                  7.70639220474525e+20 >                        (nan+0j)        (48.093753814697266+3.1415927410125732j) ```,"It seems that this issue has been resolved in JAX version 0.4.27 on CPU/GPU with https://github.com/openxla/xla/pull/10503. I tested the mentioned code with JAX versions 0.4.27 on colab CPU and GPU. JAX now produces the results similar to MPMath. ```python import jax.numpy as jnp import mpmath as mp print(jnp.log1p(5e19 + 0j), mp.log1p(5e19 + 0j)) jnp.log1p(5e19) ``` Output: ``` (45.358555+0j) (45.358554679321 + 0.0j) Array(45.358555, dtype=float32, weak_type=True) ``` Also tested for the values mentioned in the comment: ```python values = [7.70639220474525e+2011082385063936j,            11082385063936+7.70639220474525e+20j,            1213608919047.70639220474525e+20j,            121360891904+7.70639220474525e+20j,           5.358817286521974e+28+0j,            5.358817286521974e+28+0j,            4.893543125372159e+30j,            5.358817286521974e+28j,            jnp.inf19.11201286315918j,           jnp.inf+19.11201286315918j,            jnp.inf+0j,           7.70639220474525e+20+0j] for value in values:   print(""JAX:"", jnp.log1p(value), ""\t\t"",  ""MPMath:"", mp.log1p(value)) ``` Output: ``` JAX: (48.0937541.438077e08j) 		 MPMath: (48.0937520008117  1.43807695864635e8j) JAX: (48.093754+1.5707964j) 		 MPMath: (48.0937520008117 + 1.57079631241413j) JAX: (48.0937541.5707964j) 		 MPMath: (48.0937520008117  1.57079632695238j) JAX: (48.093754+1.5707964j) 		 MPMath: (48.0937520008117 + 1.57079632695238j) JAX: (66.15112+3.1415927j) 		 MPMath: (66.1511258990683 + 3.14159265358979j) JAX: (66.15112+0j) 		 MPMath: (66.1511258990683 + 0.0j) JAX: (70.66547+1.5707964j) 		 MPMath: (70.6654693964531 + 1.5707963267949j) JAX: (66.151121.5707964j) 		 MPMath: (66.1511258990683  1.5707963267949j) JAX: (inf3.1415927j) 		 MPMath: (+inf  3.14159265358979j) JAX: (inf+3.1415927j) 		 MPMath: (+inf + 3.14159265358979j) JAX: (inf+3.1415927j) 		 MPMath: (+inf + 3.14159265358979j) JAX: (48.093754+3.1415927j) 		 MPMath: (48.0937520008117 + 3.14159265358979j) ``` However the issue still exists on TPU. Attaching the gist for reference. Thank you",Hi   This issue appears resolved on TPU with JAX 0.4.38 and later.  log1p values are now similar between JAX and MpMath for these versions. Attaching the colab gist on TPU for reference. Thank you.,"Thanks,  for checking on the TPU platform. FTR, the origin of complex log1p fixes as well as log1p accuracy reports are available in https://github.com/pearu/functional_algorithms/issues/47issuecomment2563895024 ."
yi,JAX jit fails with identifical pytree registered class instances," Description I have encountered an issue with JAX's JIT compilation when passing two instances of a custom class registered as a Pytree node to a JITcompiled function. The function works as expected when called with the first instance but raises an error with the second instance, despite both instances being identical in data. This happens when x is set as auxiliary data. The smallest example that reproduces that issue: ``` import jax from jax.tree_util import register_pytree_node_class from jax import numpy as jnp  class SimpleClass:   def __init__(self, x, y):             self.x = x     self.y = y   def tree_flatten(self):       children = (self.y, )       aux_data = (self.x,)  x has to be auxiliary to raise this error       return (children, aux_data,)      def tree_unflatten(cls, aux_data, children):         x, = children         y, = aux_data         return cls(x, y)  create similar objects a = SimpleClass(x = jnp.array([1, 2]), y = jnp.array([2.5, 3.5, 4.5, 5.5])) b = SimpleClass(x = jnp.array([1, 2]), y = jnp.array([2.5, 3.5, 4.5, 5.5]))  just define the simplest function ever to return the object passed to it def func(param):    return param resa = func(a)  this works resb = func(b)  this works  jitting the function func = jax.jit(func)  doing the same as above, but after jitting resa = func(a)  this works resb = func(b)  this doesn't, raises error ``` ```  ValueError                                Traceback (most recent call last) example.py in line 37      35  doing the same as above      36 resa = func(a)  this works > 37 resb = func(b)  this doesn't      39  File ~/.local/lib/python3.9/sitepackages/jax/_src/core.py:625, in check_bool_conversion(arr, warn_on_empty)      40      622     raise ValueError(""The truth value of an empty array is ambiguous. Use ""      41      623                      ""`array.size > 0` to check that an array is not empty."")      42      624 if arr.size > 1:      43  > 625   raise ValueError(""The truth value of an array with more than one element is ""      44      626                     ""ambiguous. Use a.any() or a.all()"") File ~/.local/lib/python3.9/sitepackages/jax/_src/array.py:261, in ArrayImpl.__bool__(self)     258 def __bool__(self):     259    deprecated 2023 September 18.     260    TODO(jakevdp) change to warn_on_empty=False > 261   core.check_bool_conversion(self, warn_on_empty=True)     262   return bool(self._value) File ~/.local/lib/python3.9/sitepackages/jax/_src/core.py:625, in check_bool_conversion(arr, warn_on_empty)     622     raise ValueError(""The truth value of an empty array is ambiguous. Use ""     623                      ""`array.size > 0` to check that an array is not empty."")     624 if arr.size > 1: > 625   raise ValueError(""The truth value of an array with more than one element is ""     626                     ""ambiguous. Use a.any() or a.all()"") ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all() ```  What jax/jaxlib version are you using? 0.4.23  Which accelerator(s) are you using? GPU  Additional system info? _No response_  NVIDIA GPU info _No response_",2024-01-28T17:46:05Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/19547,"Hi  the issue here is that `aux_data` must contain hashable static entries, that can be evaluated for equality using normal `bool(a1 == a2)`. In your case, you've passed a nonstatic array to `aux_data`, and this cannot be hashed or evaluated for such equality: ```python x = jnp.arange(4) if x == x: pass  ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all() ``` To fix this, make sure during pytree flattening that you always pass array attributes to `children`, and never to `aux_data`."
yi,Unable to use JAX pmap with CPU cores," Description I am trying to use JAX pmap but I am getting the error that XLA devices aren't visible  Here's my code  ``` import os os.environ[""XLA_FLAGS""] = 'xla_force_host_platform_device_count=8' import jax from jax import pmap import jax.numpy as jnp out = pmap(lambda x: x ** 2)(jnp.arange(8)) print(out) ``` ``` Traceback (most recent call last):   File ""new.py"", line 10, in      out = pmap(lambda x: x ** 2)(jnp.arange(8))   File ""/home/thoma/anaconda3/envs/tbd/lib/python3.8/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/home/thoma/anaconda3/envs/tbd/lib/python3.8/sitepackages/jax/_src/api.py"", line 1779, in cache_miss     execute = pxla.xla_pmap_impl_lazy(fun_, *tracers, **params)   File ""/home/thoma/anaconda3/envs/tbd/lib/python3.8/sitepackages/jax/_src/interpreters/pxla.py"", line 411, in xla_pmap_impl_lazy     compiled_fun, fingerprint = parallel_callable(   File ""/home/thoma/anaconda3/envs/tbd/lib/python3.8/sitepackages/jax/_src/linear_util.py"", line 345, in memoized_fun     ans = call(fun, *args)   File ""/home/thoma/anaconda3/envs/tbd/lib/python3.8/sitepackages/jax/_src/interpreters/pxla.py"", line 682, in parallel_callable     pmap_executable = pmap_computation.compile()   File ""/home/thoma/anaconda3/envs/tbd/lib/python3.8/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/home/thoma/anaconda3/envs/tbd/lib/python3.8/sitepackages/jax/_src/interpreters/pxla.py"", line 923, in compile     executable = UnloadedPmapExecutable.from_hlo(   File ""/home/thoma/anaconda3/envs/tbd/lib/python3.8/sitepackages/jax/_src/interpreters/pxla.py"", line 993, in from_hlo     raise ValueError(msg.format(shards.num_global_shards, jax._src.traceback_util.UnfilteredStackTrace: ValueError: compiling computation that requires 8 logical devices, but only 1 XLA devices are available (num_replicas=8) The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""new.py"", line 10, in      out = pmap(lambda x: x ** 2)(jnp.arange(8)) ValueError: compiling computation that requires 8 logical devices, but only 1 XLA devices are available (num_replicas=8) (tbd) thomaLenovoLegion515IMH05H:~/PycharmProjects/tbd$ python new.py Traceback (most recent call last):   File ""new.py"", line 10, in      out = pmap(lambda x: x ** 2)(jnp.arange(8))   File ""/home/thoma/anaconda3/envs/tbd/lib/python3.8/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/home/thoma/anaconda3/envs/tbd/lib/python3.8/sitepackages/jax/_src/api.py"", line 1779, in cache_miss     execute = pxla.xla_pmap_impl_lazy(fun_, *tracers, **params)   File ""/home/thoma/anaconda3/envs/tbd/lib/python3.8/sitepackages/jax/_src/interpreters/pxla.py"", line 411, in xla_pmap_impl_lazy     compiled_fun, fingerprint = parallel_callable(   File ""/home/thoma/anaconda3/envs/tbd/lib/python3.8/sitepackages/jax/_src/linear_util.py"", line 345, in memoized_fun     ans = call(fun, *args)   File ""/home/thoma/anaconda3/envs/tbd/lib/python3.8/sitepackages/jax/_src/interpreters/pxla.py"", line 682, in parallel_callable     pmap_executable = pmap_computation.compile()   File ""/home/thoma/anaconda3/envs/tbd/lib/python3.8/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/home/thoma/anaconda3/envs/tbd/lib/python3.8/sitepackages/jax/_src/interpreters/pxla.py"", line 923, in compile     executable = UnloadedPmapExecutable.from_hlo(   File ""/home/thoma/anaconda3/envs/tbd/lib/python3.8/sitepackages/jax/_src/interpreters/pxla.py"", line 993, in from_hlo     raise ValueError(msg.format(shards.num_global_shards, jax._src.traceback_util.UnfilteredStackTrace: ValueError: compiling computation that requires 8 logical devices, but only 1 XLA devices are available (num_replicas=8) The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""new.py"", line 10, in      out = pmap(lambda x: x ** 2)(jnp.arange(8)) ValueError: compiling computation that requires 8 logical devices, but only 1 XLA devices are available (num_replicas=8) ```  What jax/jaxlib version are you using? 0.4.13 0.4.13  Which accelerator(s) are you using? CPU/GPU  Additional system info? uname_result(system='Linux', node='thomaLenovoLegion515IMH05H', release='6.5.015generic', version=' CC(rename in_bdims, out_bdims > in_axes, out_axes)~22.04.1Ubuntu SMP PREEMPT_DYNAMIC Fri Jan 12 18:54:30 UTC 2', machine='x86_64', processor='x86_64')  NVIDIA GPU info ``` ++  ++ ```",2024-01-27T14:38:13Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/19543,It seems like this is a duplicate of CC(未找到相关数据) – no need to ask this question multiple times. Thanks!,"My apologies. I thought that there was a bug. Thanks a lot for the help. On Sat, Jan 27, 2024 at 10:57 AM Jake Vanderplas ***@***.***> wrote: > It seems like this is a duplicate of CC(未找到相关数据) >  – no need to ask this > question multiple times. Thanks! > > — > Reply to this email directly, view it on GitHub > , or > unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >"
gemma,run time error: custom call 'xla.gpu.cublas.lt.matmul' failed," Description XLA option: ```bash ""xla_gpu_enable_latency_hiding_scheduler=true xla_gpu_enable_triton_gemm=false  xla_gpu_simplify_all_fp_conversions xla_gpu_enable_async_all_gather=true  xla_gpu_enable_async_reduce_scatter=true  xla_gpu_enable_highest_priority_async_stream=true  xla_gpu_enable_triton_softmax_fusion=false  xla_gpu_all_reduce_combine_threshold_bytes=51200  xla_gpu_graph_level=0 xla_gpu_enable_async_all_reduce=true  xla_gpu_enable_async_collectives=true xla_gpu_enable_async_collective_permute=true  xla_gpu_enable_async_all_gather=true xla_gpu_enable_async_reduce_scatter=true  xla_gpu_enable_async_all_to_all=true xla_gpu_all_reduce_contiguous=true  xla_gpu_all_reduce_blueconnect_num_devices_per_host=true  xla_gpu_enable_cudnn_frontend=true xla_gpu_enable_cudnn_fmha=true xla_gpu_fused_attention_use_cudnn_rng=true  xla_gpu_enable_cudnn_layer_norm=true xla_gpu_enable_cublaslt=true  xla_gpu_enable_triton_gemm=false xla_gpu_enable_triton_softmax_fusion=false xla_gpu_triton_gemm_any=false"" ``` Raise Error: ```txt E0127 03:41:58.168505   64850 pjrt_stream_executor_client.cc:2766] Execution of replica 0 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.cublas.lt.matmul' failed: cublasLtMatmul( blas_lt_ref_.blas_lt_.get(), op_desc_.get(), alpha, a.opaque(), a_desc_.get(), b.opaque(), b_desc_.get(), beta, c.opaque(), c_desc_.get(), d.opaque(), d_desc_.get(), palgo, workspace, algorithm.workspace_size, gpu::AsGpuStreamValue(stream)): the function failed to launch on the GPU; current tracing scope: customcall.35; current profiling annotation: XlaModule:hlo_module=pjit__wrapped_step_fn,program_id=24. F0127 03:42:08.165752   64427 pjrt_stream_executor_client.cc:2912] Replicated computation launch failed, but not all replicas terminated. Aborting process to work around deadlock. Failure message (there may have been multiple failures, see the error log for all failures): Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.cublas.lt.matmul' failed: cublasLtMatmul( blas_lt_ref_.blas_lt_.get(), op_desc_.get(), alpha, a.opaque(), a_desc_.get(), b.opaque(), b_desc_.get(), beta, c.opaque(), c_desc_.get(), d.opaque(), d_desc_.get(), palgo, workspace, algorithm.workspace_size, gpu::AsGpuStreamValue(stream)): the function failed to launch on the GPU; current tracing scope: customcall.35; current profiling annotation: XlaModule:hlo_module=pjit__wrapped_step_fn,program_id=24. *** Check failure stack trace: ***     @     0x7f5fa09ca32d  absl::lts_20230802::log_internal::LogMessageFatal::~LogMessageFatal()     @     0x7f5f9ba364ee  xla::PjRtStreamExecutorLoadedExecutable::Execute()     @     0x7f5f9b8d876d  xla::ifrt::PjRtLoadedExecutable::Execute()     @     0x7f5f9929cbbe  xla::(anonymous namespace)::ExecuteShardedOnLocalDevicesInternal()     @     0x7f5f9929e0c1  xla::PyLoadedExecutable::ExecuteSharded()     @     0x7f5f9907ab5d  pybind11::cpp_function::initialize()::{lambda() CC(Undefined name: from ..core import JaxTuple)}::_FUN()     @     0x7f5f9902ad09  pybind11::cpp_function::dispatcher()     @     0x55e6a5b7a10e  (unknown) Fatal Python error: Aborted Current thread 0x00007f6043737480 (most recent call first):   File ""/opt/jax/jax/_src/interpreters/pxla.py"", line 1209 in __call__   File ""/opt/jax/jax/_src/profiler.py"", line 336 in wrapper   File ""/opt/jax/jax/_src/pjit.py"", line 1310 in _pjit_call_impl_python   File ""/opt/jax/jax/_src/pjit.py"", line 1354 in call_impl_cache_miss   File ""/opt/jax/jax/_src/pjit.py"", line 1371 in _pjit_call_impl   File ""/opt/jax/jax/_src/core.py"", line 935 in process_primitive   File ""/opt/jax/jax/_src/core.py"", line 447 in bind_with_trace   File ""/opt/jax/jax/_src/core.py"", line 2743 in bind   File ""/opt/jax/jax/_src/pjit.py"", line 137 in _python_pjit_helper   File ""/opt/jax/jax/_src/pjit.py"", line 227 in cache_miss   File ""/opt/jax/jax/_src/traceback_util.py"", line 179 in reraise_with_filtered_traceback   File ""/opt/paxml/paxml/trainer_lib.py"", line 1747 in call   File ""/opt/paxml/paxml/programs.py"", line 642 in train_step   File ""/opt/paxml/paxml/programs.py"", line 351 in run   File ""/opt/praxis/praxis/py_utils.py"", line 1028 in wrapper   File ""/opt/paxml/paxml/executors.py"", line 426 in _train_and_evaluate_common   File ""/opt/paxml/paxml/executors.py"", line 278 in start   File ""/opt/paxml/paxml/train.py"", line 282 in train_and_evaluate   File ""/opt/praxis/praxis/py_utils.py"", line 1028 in wrapper   File ""/opt/paxml/paxml/main.py"", line 310 in run_experiment   File ""/opt/praxis/praxis/py_utils.py"", line 1028 in wrapper   File ""/opt/paxml/paxml/main.py"", line 450 in run   File ""/opt/praxis/praxis/py_utils.py"", line 1028 in wrapper   File ""/opt/paxml/paxml/main.py"", line 536 in _main   File ""/opt/praxis/praxis/py_utils.py"", line 1028 in wrapper   File ""/opt/paxml/paxml/main.py"", line 475 in main   File ""/usr/local/lib/python3.10/distpackages/absl/app.py"", line 254 in _run_main   File ""/usr/local/lib/python3.10/distpackages/absl/app.py"", line 308 in run   File ""/opt/paxml/paxml/main.py"", line 559 in    File ""/usr/lib/python3.10/runpy.py"", line 86 in _run_code   File ""/usr/lib/python3.10/runpy.py"", line 196 in _run_module_as_main ```  What jax/jaxlib version are you using? jax0.4.24.dev20240125+a6f26306b jaxlib0.4.24.dev20240125  Which accelerator(s) are you using? GPU  Additional system info? train with PaxML   NVIDIA GPU info ++  ++",2024-01-26T19:46:41Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/19534,xla_gpu_enable_cublaslt cause this bug.
yi,Shorter errors for lax primitives,"When I run snippets like `jax.lax.select` on incorrect shapes or dtypes, which should be an easy and fast check, I get absurdly long error messages. Could this be improved?  EDIT: jax version: 0.20.0 on Ubuntu WSL Python 3.10 Example, ```python import jax import jax.numpy as jnp jax.lax.select(     True,     jnp.ones(3, jnp.int32),     jnp.zeros(5, jnp.float32) ) >>TypeError                                 Traceback (most recent call last) Cell In[8], line 1 > 1 jax.lax.select(       2     True,       3     jnp.ones(3, jnp.int32),       4     jnp.zeros(5, jnp.float32)       5 ) File ~/anaconda3/envs/jax3.10/lib/python3.10/sitepackages/jax/_src/lax/lax.py:920, in select(pred, on_true, on_false)     898 """"""Selects between two branches based on a boolean predicate.     899      900 Wraps XLA's `Select    (...)     916   result: array with same shape and dtype as ``on_true`` and ``on_false``.     917 """"""     918  Caution! The select_n_p primitive has the *opposite* order of arguments to     919  select(). This is because it implements `select_n`. > 920 return select_n_p.bind(pred, on_false, on_true) File ~/anaconda3/envs/jax3.10/lib/python3.10/sitepackages/jax/_src/core.py:385, in Primitive.bind(self, *args, **params)     382 def bind(self, *args, **params):     383   assert (not config.enable_checks.value or     384           all(isinstance(arg, Tracer) or valid_jaxtype(arg) for arg in args)), args > 385   return self.bind_with_trace(find_top_trace(args), args, params) File ~/anaconda3/envs/jax3.10/lib/python3.10/sitepackages/jax/_src/core.py:388, in Primitive.bind_with_trace(self, trace, args, params)     387 def bind_with_trace(self, trace, args, params): > 388   out = trace.process_primitive(self, map(trace.full_raise, args), params)     389   return map(full_lower, out) if self.multiple_results else full_lower(out) File ~/anaconda3/envs/jax3.10/lib/python3.10/sitepackages/jax/_src/core.py:868, in EvalTrace.process_primitive(self, primitive, tracers, params)     867 def process_primitive(self, primitive, tracers, params): > 868   return primitive.impl(*tracers, **params) File ~/anaconda3/envs/jax3.10/lib/python3.10/sitepackages/jax/_src/dispatch.py:128, in apply_primitive(prim, *args, **params)     126   in_avals, in_shardings = util.unzip2([_arg_spec(a) for a in args])     127   in_tree = tree_util.tree_structure(args) > 128   compiled_fun = xla_primitive_callable(     129       prim, in_avals, in_tree, OrigShardings(in_shardings), **params)     130 except pxla.DeviceAssignmentMismatchError as e:     131   fails, = e.args File ~/anaconda3/envs/jax3.10/lib/python3.10/sitepackages/jax/_src/util.py:284, in cache..wrap..wrapper(*args, **kwargs)     282   return f(*args, **kwargs)     283 else: > 284   return cached(config.config._trace_context(), *args, **kwargs) File ~/anaconda3/envs/jax3.10/lib/python3.10/sitepackages/jax/_src/util.py:277, in cache..wrap..cached(_, *args, **kwargs)     275 .lru_cache(max_size)     276 def cached(_, *args, **kwargs): > 277   return f(*args, **kwargs) File ~/anaconda3/envs/jax3.10/lib/python3.10/sitepackages/jax/_src/dispatch.py:157, in xla_primitive_callable(prim, in_avals, in_tree, orig_in_shardings, **params)     155 wrapped_fun = lu.wrap_init(prim_fun)     156 flat_fun, out_tree = api_util.flatten_fun_nokwargs(wrapped_fun, in_tree) > 157 computation = sharded_lowering(     158     flat_fun, prim.name, donated_invars, keep_unused=False,     159     inline=True, in_avals=in_avals, in_shardings=orig_in_shardings.shardings,     160     lowering_parameters=mlir.LoweringParameters())     161 compiled = computation.compile()     162 if xla_extension_version >= 192: File ~/anaconda3/envs/jax3.10/lib/python3.10/sitepackages/jax/_src/dispatch.py:188, in sharded_lowering(fun, name, donated_invars, keep_unused, inline, in_avals, in_shardings, lowering_parameters)     183 in_shardings_unspec = [UNSPECIFIED if i is None else i for i in in_shardings]     185  Pass in a singleton `UNSPECIFIED` for out_shardings because we don't know     186  the number of output avals at this stage. lower_sharding_computation will     187  apply it to all out_avals. > 188 return pxla.lower_sharding_computation(     189     fun, 'jit', name, in_shardings_unspec, UNSPECIFIED, donated_invars,     190     in_avals, keep_unused=keep_unused, inline=inline,     191     devices_from_context=None,     192     lowering_parameters=lowering_parameters) File ~/anaconda3/envs/jax3.10/lib/python3.10/sitepackages/jax/_src/profiler.py:340, in annotate_function..wrapper(*args, **kwargs)     337 (func)     338 def wrapper(*args, **kwargs):     339   with TraceAnnotation(name, **decorator_kwargs): > 340     return func(*args, **kwargs)     341   return wrapper File ~/anaconda3/envs/jax3.10/lib/python3.10/sitepackages/jax/_src/interpreters/pxla.py:1971, in lower_sharding_computation(fun_or_jaxpr, api_name, fun_name, in_shardings, out_shardings, donated_invars, global_in_avals, keep_unused, inline, devices_from_context, lowering_parameters)    1965  1. Trace to jaxpr and preprocess/verify it    1966 auto_spmd_lowering = (    1967     check_if_any_auto(in_shardings) if is_unspecified(out_shardings) else    1968     check_if_any_auto(it.chain.from_iterable([in_shardings, out_shardings])))   type: ignore    1970 (closed_jaxpr, global_in_avals, global_out_avals, donated_invars, > 1971  kept_var_idx, name_stack) = _trace_to_jaxpr_and_dce(    1972     fun_or_jaxpr, global_in_avals, api_name, fun_name, keep_unused,    1973     donated_invars, auto_spmd_lowering)    1974 jaxpr = closed_jaxpr.jaxpr    1975 in_shardings = tuple(s for i, s in enumerate(in_shardings) if i in kept_var_idx) File ~/anaconda3/envs/jax3.10/lib/python3.10/sitepackages/jax/_src/interpreters/pxla.py:1672, in cache_wrap..wrapped(f, *args, **kwargs)    1670 def wrapped(f, *args, **kwargs):    1671   if isinstance(f, lu.WrappedFun): > 1672     return _wrapped_with_lu_cache(f, *args, **kwargs)    1673   else:    1674     return _wrapped_with_weakref_lru_cache(f, *args, **kwargs) File ~/anaconda3/envs/jax3.10/lib/python3.10/sitepackages/jax/_src/linear_util.py:349, in cache..memoized_fun(fun, *args)     347   fun.populate_stores(stores)     348 else: > 349   ans = call(fun, *args)     350   cache[key] = (ans, fun.stores)     352 return ans File ~/anaconda3/envs/jax3.10/lib/python3.10/sitepackages/jax/_src/interpreters/pxla.py:1697, in _trace_to_jaxpr_and_dce(fun_or_jaxpr, global_in_avals, api_name, fun_name, keep_unused, donated_invars, auto_spmd_lowering)    1693 if isinstance(fun_or_jaxpr, lu.WrappedFun):    1694   with dispatch.log_elapsed_time(    1695       ""Finished tracing + transforming {fun_name} in {elapsed_time} sec"",    1696       fun_name=str(name_stack), event=dispatch.JAXPR_TRACE_EVENT): > 1697     jaxpr, global_out_avals, consts = pe.trace_to_jaxpr_final(    1698         fun_or_jaxpr, global_in_avals)    1699 else:    1700   assert isinstance(fun_or_jaxpr, core.ClosedJaxpr) File ~/anaconda3/envs/jax3.10/lib/python3.10/sitepackages/jax/_src/profiler.py:340, in annotate_function..wrapper(*args, **kwargs)     337 (func)     338 def wrapper(*args, **kwargs):     339   with TraceAnnotation(name, **decorator_kwargs): > 340     return func(*args, **kwargs)     341   return wrapper File ~/anaconda3/envs/jax3.10/lib/python3.10/sitepackages/jax/_src/interpreters/partial_eval.py:2356, in trace_to_jaxpr_final(fun, in_avals, debug_info, keep_inputs)    2354   main.jaxpr_stack = ()   type: ignore    2355   with core.new_sublevel(): > 2356     jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(    2357       fun, main, in_avals, keep_inputs=keep_inputs, debug_info=debug_info)    2358   del fun, main    2359 return jaxpr, out_avals, consts File ~/anaconda3/envs/jax3.10/lib/python3.10/sitepackages/jax/_src/interpreters/partial_eval.py:2300, in trace_to_subjaxpr_dynamic(fun, main, in_avals, keep_inputs, debug_info)    2298 in_tracers = _input_type_to_tracers(trace.new_arg, in_avals)    2299 in_tracers_ = [t for t, keep in zip(in_tracers, keep_inputs) if keep] > 2300 ans = fun.call_wrapped(*in_tracers_)    2301 out_tracers = map(trace.full_raise, ans)    2302 jaxpr, consts = frame.to_jaxpr(out_tracers) File ~/anaconda3/envs/jax3.10/lib/python3.10/sitepackages/jax/_src/linear_util.py:191, in WrappedFun.call_wrapped(self, *args, **kwargs)     188 gen = gen_static_args = out_store = None     190 try: > 191   ans = self.f(*args, **dict(self.params, **kwargs))     192 except:     193    Some transformations yield from inside context managers, so we have to     194    interrupt them before reraising the exception. Otherwise they will only     195    get garbagecollected at some later time, running their cleanup tasks     196    only after this exception is handled, which can corrupt the global     197    state.     198   while stack: File ~/anaconda3/envs/jax3.10/lib/python3.10/sitepackages/jax/_src/dispatch.py:149, in xla_primitive_callable..prim_fun(*args)     148 def prim_fun(*args): > 149   out = prim.bind(*args, **params)     150   if prim.multiple_results:     151     return out File ~/anaconda3/envs/jax3.10/lib/python3.10/sitepackages/jax/_src/core.py:385, in Primitive.bind(self, *args, **params)     382 def bind(self, *args, **params):     383   assert (not config.enable_checks.value or     384           all(isinstance(arg, Tracer) or valid_jaxtype(arg) for arg in args)), args > 385   return self.bind_with_trace(find_top_trace(args), args, params) File ~/anaconda3/envs/jax3.10/lib/python3.10/sitepackages/jax/_src/core.py:388, in Primitive.bind_with_trace(self, trace, args, params)     387 def bind_with_trace(self, trace, args, params): > 388   out = trace.process_primitive(self, map(trace.full_raise, args), params)     389   return map(full_lower, out) if self.multiple_results else full_lower(out) File ~/anaconda3/envs/jax3.10/lib/python3.10/sitepackages/jax/_src/interpreters/partial_eval.py:1972, in DynamicJaxprTrace.process_primitive(self, primitive, tracers, params)    1970 if primitive in custom_staging_rules:    1971   return custom_staging_rulesprimitive > 1972 return self.default_process_primitive(primitive, tracers, params) File ~/anaconda3/envs/jax3.10/lib/python3.10/sitepackages/jax/_src/interpreters/partial_eval.py:1976, in DynamicJaxprTrace.default_process_primitive(self, primitive, tracers, params)    1974 def default_process_primitive(self, primitive, tracers, params):    1975   avals = [t.aval for t in tracers] > 1976   out_avals, effects = primitive.abstract_eval(*avals, **params)    1977    == serve as a ""not xor"" here.    1978   if not (isinstance(out_avals, (tuple,list)) == primitive.multiple_results): File ~/anaconda3/envs/jax3.10/lib/python3.10/sitepackages/jax/_src/core.py:421, in _effect_free_abstract_eval..abstract_eval_(*args, **kwargs)     420 def abstract_eval_(*args, **kwargs): > 421   return abstract_eval(*args, **kwargs), no_effects File ~/anaconda3/envs/jax3.10/lib/python3.10/sitepackages/jax/_src/lax/utils.py:60, in standard_abstract_eval(prim, shape_rule, dtype_rule, weak_type_rule, named_shape_rule, *avals, **kwargs)      58   return core.ConcreteArray(out.dtype, out, weak_type=weak_type)      59 elif least_specialized is core.ShapedArray: > 60   return core.ShapedArray(shape_rule(*avals, **kwargs),      61                           dtype_rule(*avals, **kwargs), weak_type=weak_type,      62                           named_shape=named_shape_rule(*avals, **kwargs))      63 elif least_specialized is core.DShapedArray:      64   shape = shape_rule(*avals, **kwargs) File ~/anaconda3/envs/jax3.10/lib/python3.10/sitepackages/jax/_src/lax/lax.py:3515, in _select_shape_rule(which, *cases)    3513 if any(case.shape != cases[0].shape for case in cases[1:]):    3514   msg = ""select cases must have the same shapes, got [{}]."" > 3515   raise TypeError(msg.format("", "".join([str(c.shape) for c in cases])))    3516 if which.shape and which.shape != cases[0].shape:    3517   msg = (""select `which` must be scalar or have the same shape as cases, ""    3518          ""got `which` shape {} but case shape {}."") TypeError: select cases must have the same shapes, got [(5,), (3,)]. ```",2024-01-26T12:03:19Z,enhancement better_errors,closed,0,6,https://github.com/jax-ml/jax/issues/19527,Thanks for the suggestion! Maybe we could add some of these abstract evaluation frames to the pieces hidden by `jax_traceback_filtering`.,"Actually, it looks like this is already covered. With `JAX_TRACEBACK_FILTERING='auto'` (the default value) I get this when running this script: ```pytb $ python tmp.py  jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/Users/vanderplas/github/google/jax/tmp.py"", line 5, in      jax.lax.select(   File ""/Users/vanderplas/github/google/jax/jax/_src/lax/lax.py"", line 931, in select     return select_n_p.bind(pred, on_false, on_true)   File ""/Users/vanderplas/github/google/jax/jax/_src/core.py"", line 444, in bind     return self.bind_with_trace(find_top_trace(args), args, params)   File ""/Users/vanderplas/github/google/jax/jax/_src/core.py"", line 447, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/Users/vanderplas/github/google/jax/jax/_src/core.py"", line 935, in process_primitive     return primitive.impl(*tracers, **params)   File ""/Users/vanderplas/github/google/jax/jax/_src/dispatch.py"", line 87, in apply_primitive     outs = fun(*args) TypeError: select cases must have the same shapes, got [(5,), (3,)]. ``` Can you check whether you're setting `JAX_TRACEBACK_FILTERING` to something different? It looks like you're running this in a notebook, so it's possible that you're running under a version of IPython that does not support filtering the traceback; see the logic here: https://github.com/google/jax/blob/f34bcc326ba882a500c9348d7992ae4f580bc4c3/jax/_src/traceback_util.pyL138L150","I experienced a similar traceback when using Colab (see below). , would it be possible for us to change that `JAX_TRACEBACK_FILTERING ` default setting for Colab? ```python  TypeError                                 Traceback (most recent call last) [](https://localhost:8080/) in ()       3        4  > 5 jax.lax.select(       6     True,       7     jnp.ones(3, jnp.int32), 5 frames /usr/local/lib/python3.10/distpackages/jax/_src/lax/lax.py in select(pred, on_true, on_false)     926    Caution! The select_n_p primitive has the *opposite* order of arguments to     927    select(). This is because it implements `select_n`. > 928   return select_n_p.bind(pred, on_false, on_true)     929      930 def select_n(which: ArrayLike, *cases: ArrayLike) > Array: /usr/local/lib/python3.10/distpackages/jax/_src/core.py in bind(self, *args, **params)     442     assert (not config.enable_checks.value or     443             all(isinstance(arg, Tracer) or valid_jaxtype(arg) for arg in args)), args > 444     return self.bind_with_trace(find_top_trace(args), args, params)     445      446   def bind_with_trace(self, trace, args, params): /usr/local/lib/python3.10/distpackages/jax/_src/core.py in bind_with_trace(self, trace, args, params)     445      446   def bind_with_trace(self, trace, args, params): > 447     out = trace.process_primitive(self, map(trace.full_raise, args), params)     448     return map(full_lower, out) if self.multiple_results else full_lower(out)     449  /usr/local/lib/python3.10/distpackages/jax/_src/core.py in process_primitive(self, primitive, tracers, params)     933      934   def process_primitive(self, primitive, tracers, params): > 935     return primitive.impl(*tracers, **params)     936      937   def process_call(self, primitive, f, tracers, params): /usr/local/lib/python3.10/distpackages/jax/_src/dispatch.py in apply_primitive(prim, *args, **params)      85     prev = lib.jax_jit.swap_thread_local_state_disable_jit(False)      86     try: > 87       outs = fun(*args)      88     finally:      89       lib.jax_jit.swap_thread_local_state_disable_jit(prev)     [... skipping hidden 19 frame] /usr/local/lib/python3.10/distpackages/jax/_src/lax/lax.py in _select_shape_rule(which, *cases)    3520   if any(case.shape != cases[0].shape for case in cases[1:]):    3521     msg = ""select cases must have the same shapes, got [{}]."" > 3522     raise TypeError(msg.format("", "".join([str(c.shape) for c in cases])))    3523   if which.shape and which.shape != cases[0].shape:    3524     msg = (""select `which` must be scalar or have the same shape as cases, "" TypeError: select cases must have the same shapes, got [(5,), (3,)]. ```",Thanks Paige – it looks like things are working as expected there (note the`skipping hidden 19 frame` in your output). The traceback hiding code uses different mechanisms depending on whether it's being run in IPython or run in a script.," it looks like you may be using an older JAX version as well. These tracebacks have been improved in recent releases, you might try updating JAX to see if the error is more useful. Thanks!","Hey thanks for the quick reply! Yes forgot to state that my jax version is `0.20.0` I edited it in the post; I tested this in a IPython shell. I tried setting `os.environ['JAX_TRACEBACK_FILTERING']` to all shown options: `[""off"", ""tracebackhide"", ""remove_frames"", ""quiet_remove_frames"", ""auto""]`, but none of the options did anything.  At the moment v:0.20.0 is stable for my PC, on newer versions my GPU doesn't get detected by an unrelated error: cuBLAS version mismatch. But you're correct, when I tested this on another system where I do have jax v 0.23.0, the error message is automatically shortened. So I guess this is already fixed :). Thanks!"
yi,"Fix `jax.lax.fori_loop(..., unroll=True)` with non-positive length","According to the docs: https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.fori_loop.html > As the Python version suggests, setting `upper <= lower` will produce no iterations. Negative or custom increments are not supported. This isn't supported by the underlying `scan` implementation: https://github.com/google/jax/blob/a6f26306b387ce082358e8e94fdcfd777773475f/jax/_src/lax/control_flow/loops.pyL1025",2024-01-25T16:48:41Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/19517,Removed usage of union types in the test that were causing Python 3.9 tests to fail.
yi,Remove unnecessary Array.register,This is unnecessary because `PRNGKeyArrayImpl` already has `jax.Array` in its hierarchy: https://github.com/google/jax/blob/cfb62501583b14face8e23d6a4f8a33950a8d64d/jax/_src/prng.pyL245 https://github.com/google/jax/blob/cfb62501583b14face8e23d6a4f8a33950a8d64d/jax/_src/prng.pyL150,2024-01-24T22:55:37Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/19509
yi,Can vmap vstack the outputs,"I am trying to run ``vmap`` on different sized inputs (that have the same number of columns). For example ``` x1 = np.random.rand(10, 3) x2 = np.random.rand(20, 3) ``` I have two functions, ``f1, f2``, that I want evaluated on x1 and x2 respectively through vmap. The following is **NOT** allowed by jax ``` functions = [f1, f2] xs = [x1, x2] vfunc = vmap(lambda i, xs: lax.switch(i, functions, xs[i]) vfunc(jnp.arange(2), xs) ``` This throws the error because vmap by default does not vstack the outputs. Is there a way to vstack the outputs? Thank you.",2024-01-24T21:14:14Z,enhancement,closed,0,2,https://github.com/jax-ml/jax/issues/19506,"Hi, thanks for the question! `vmap` can stack outputs, but not for the data you provide in this example. The problem is that indices in `lax.switch` cannot be used to index into Python lists, because the indices are dynamic rather than static. You can address this by turning `xs` into an array rather than a list, but since JAX doesn't support ragged arrays, this requires making each of the entries the same shape. Once you do that, it should work (so long as you adjust the `in_axes` to not batch `xs` in your mapped function): ```python import numpy as np import jax.numpy as jnp from jax import lax, vmap x1 = np.random.rand(10, 3) x2 = np.random.rand(10, 3) functions = [jnp.sin, jnp.cos] xs = jnp.array([x1, x2]) vfunc = vmap(lambda i, xs: lax.switch(i, functions, xs[i]), in_axes=(0, None)) result = vfunc(jnp.arange(2), xs) print(result.shape)  (2, 10, 3) ``` Is that along the lines of what you have in mind?","Hi Jake! First, thank you so much for your prompt reply and answer. I really appreciate the help. I now have a better understanding of ``vmap`` and ``lax.switch``. Unfortunately for my use case, the input shapes are different for each function along the first dimension. I will rethink my approach."
yi,Computing Jacobian w.r.t. parameters used in lax.associative_scan is unexpectedly slow," Description Thanks for considering this issue. My research requires me to compute the jacobian of a recurrent neural network with respect to a set of quantities. The minimal example below considers a linear recurrence $x_t = \lambda \odot x_{t1} + B u_t$ as found in recent deep statespace models such as S5. Here, $x_t, \lambda \in\mathbb{C}^n, u_t\in\mathbb{R}^m, B\in\mathbb{C}^{n\times m}$.   Problem Description Since recursion relations can be formulated as associative operators, they can be parallelized using `lax.associative_scan`.  ```python def binary_operator(element_i, element_j):      Binary operator for parallel scan of linear recurrence.     a_i, bu_i = element_i     a_j, bu_j = element_j     return a_j * a_i, a_j * bu_i + bu_j ``` My issue arises when using AD to compute the following quantities   $\frac{\partial x_t}{\partial x_0}$  $\frac{\partial x_t}{\partial u_s}$ for $s=1,\dots,t$  $\frac{\partial x_t}{\partial B}$  $\frac{\partial x_t}{\partial \lambda}$ I would expect these operations to   consume similar amounts of GPU time  parallelize with `associative_scan` as the forward pass is also parallelized When measuring the compute time on A100 (40GB), I find that particularly the derivative $\frac{\partial x_t}{\partial \lambda}$ is much slower than the other ones. The compute time of all derivatives increases linearly with sequence length (measured many different specifications, but the example below does it from $t=32,\dots,512$). Yet, the derivative $\frac{\partial x_t}{\partial \lambda}$ has much steeper slope as shown in the image below. The parameter $\lambda$ is fed to the `binary_operator` as `a_i, a_j`. If I replace the return statement with `return jax.numpy.ones_like(a_j) * jax.numpy.ones_like(a_i), a_j * bu_i + bu_j` such that AD doesn't need to trace the value of $\lambda$ through the first argument of the `binary_operator`, the severe differences in compute times vanish.  For completeness, the example below contains forward and backward mode AD results for networks with 1 and 2 layers. I am now wondering 1. Why is this particular jacobian so slow in contrast to the others? 2. Why it appears not to parallelize the computation along sequence length despite applying associative_scan (guessing from linearly increasing compute times with large slope) Figure: Note the slope of the linear fits in brackets is an order of magnitude larger for the jacobian (scan), i.e. w.r.t. the parameter iteratively used in the scan: $\lambda$ !jacobian_measurements  Code to reproduce the measurement The code below also allows to measure the same program implemented with `lax.scan` and a simple python for loop. These are commented out below. Just uncomment them to also measure these quantities. Compile times for the python for loop will be quite significant for larger sequence length. ```python import jax import jax.numpy as jnp from timeit import timeit from time import time from functools import partial import pandas as pd def scan(weight, input_sequence, init_state, unroll=1):      input sequence has shape (T, state_size)     def scan_fn(h_t, x_t):         h_t = weight * h_t + x_t         return h_t, h_t     carry, ys = jax.lax.scan(scan_fn, init=init_state, xs=input_sequence, unroll=unroll)     return carry, ys def for_loop(weight, input_sequence, init_state):      input sequence has shape (T, state_size)     h_t = init_state     ys = []     for x_t in input_sequence:         h_t = weight * h_t + x_t         ys.append(h_t)     return h_t, jnp.stack(ys) def binary_operator(element_i, element_j):      Binary operator for parallel scan of linear recurrence.     a_i, bu_i = element_i     a_j, bu_j = element_j     return a_j * a_i, a_j * bu_i + bu_j     return jnp.ones_like(a_i) * jnp.ones_like(a_j), a_j * bu_i + bu_j def associative_scan(weight, input_sequence, init_state):     sequence_length, state_size = input_sequence.shape     W_elements = jnp.repeat(weight[None, ...], sequence_length + 1, axis=0)     elements = (W_elements, jnp.concatenate([init_state[None, ...], input_sequence], axis=0))     _, ys = jax.lax.associative_scan(binary_operator, elements, axis=0)     return ys[1], ys[1:] def single_layer_model(fun, params, params_scan, input_sequence, init_state):     x = jnp.einsum('ij,lj>li', params['weight1'], input_sequence) + params_scan['bias1']     return fun(params_scan['lambda1'], x, init_state) def two_layer_model(fun, params, params_scan, input_sequence, init_state):     state1, state2 = init_state     lambda1, lambda2 = params_scan['lambda1'], params_scan['lambda2']     x = jnp.einsum('ij,lj>li', params['weight1'], input_sequence) + params_scan['bias1']     state1, x = fun(lambda1, x, state1)     x = jnp.einsum('ij,lj>li', params['weight2'], x) + params_scan['bias2']     state2, x = fun(lambda2, x, state2)     return state2, x def jacobian(fun, mode='backward'):     if mode == 'backward':         return jax.jacrev(fun)     elif mode == 'forward':         return jax.jacfwd(fun)     else:         raise NotImplementedError def state_jacobian_fun(fun, params, params_scan, input_sequence, init_state, mode):     return jacobian(lambda h: fun(params, params_scan, input_sequence, h)[0], mode=mode)(init_state) def param_jacobian_fun(fun, params, params_scan, input_sequence, init_state, mode):     return jacobian(lambda p: fun(p, params_scan, input_sequence, init_state)[0], mode=mode)(params) def param_scan_jacobian_fun(fun, params, params_scan, input_sequence, init_state, mode):     return jacobian(lambda p: fun(params, p, input_sequence, init_state)[0], mode=mode)(params_scan) def input_jacobian_fun(fun, params, params_scan, input_sequence, init_state, mode):     return jacobian(lambda x: fun(params, params_scan, x, init_state)[0], mode=mode)(input_sequence) def measure_base(fun, params, params_scan, input_sequence, init_state, num_iterations=100):      jit the function over a batch jit>vmap>fun     jit_fn = jax.jit(jax.vmap(fun, (None, None, 0, 0)))      measure compilation time     start = time()     out = jax.block_until_ready(jit_fn(params, params_scan, input_sequence, init_state))     t_compile = time()  start      measure run time     t_run = timeit(lambda: jax.block_until_ready(jit_fn(params, params_scan, input_sequence, init_state)), number=num_iterations) / num_iterations * 1000     return t_run, t_compile def main(T, state_size, batch_size):     key = jax.random.PRNGKey(0)     x_key, init_key, w1_key, w2_key, l1_key, l2_key, l3_key = jax.random.split(key, 7)     xs = jax.random.normal(x_key, (batch_size, T, state_size))     params = {         'weight1': jax.random.normal(w1_key, (state_size, state_size)),         'weight2': jax.random.normal(w2_key, (state_size, state_size)),     }     params_scan = {         'lambda1': jax.random.uniform(l1_key, (state_size,)),         'lambda2': jax.random.uniform(l2_key, (state_size,)),         'bias1': jax.random.normal(w1_key, (state_size,)),         'bias2': jax.random.normal(w2_key, (state_size,))     }     experiments = {         'for loop': for_loop,         'scan  (1)': partial(scan, unroll=1),         'scan  (8)': partial(scan, unroll=8),         'scan (64)': partial(scan, unroll=64),         'associative scan': associative_scan     }     results = []     for mode in ['backward', 'forward']:         for tag1, base_fun in experiments.items():             for tag2, forward_fun, init in zip(                 [1, 2],                 [single_layer_model, two_layer_model],                 [jax.random.normal(init_key, (batch_size, state_size)),                  (jax.random.normal(init_key, (batch_size, state_size)),                   jax.random.normal(init_key, (batch_size, state_size)))]             ):                 measure = partial(                     measure_base,                     params=params,                     params_scan=params_scan,                     input_sequence=xs,                     init_state=init                 )                 model_fun = partial(forward_fun, base_fun)                 forward, _ = measure(model_fun)                 state_jac, _ = measure(partial(state_jacobian_fun, model_fun, mode=mode))                 param_jac, _ = measure(partial(param_jacobian_fun, model_fun, mode='backward'))                 param_scan_jac, _ = measure(partial(param_scan_jacobian_fun, model_fun, mode=mode))                 inout_jac, _ = measure(partial(input_jacobian_fun, model_fun, mode='backward'))                 results.append([tag1, mode, tag2, T, forward, state_jac, param_jac, param_scan_jac, inout_jac])     df = pd.DataFrame(data=results, columns=[         'func', 'mode', 'layers', 'steps', 'forward [ms]',         'jacobian (state) [ms]', 'jacobian (params) [ms]', 'jacobian (scan) [ms]',         'inputoutput [ms]'])     df = df.sort_values(by=['layers', 'func', 'mode']).reset_index(drop=True)     return df if __name__ == '__main__':     pd.options.display.float_format = '{:,.2f}'.format     data = []     for T in range(1, 17):         T = T * 32         print(40 * ""*"")         print(""Evaluating T ="", T)         df = main(T, state_size=64, batch_size=16)         data.append(df)     df = pd.concat(data, ignore_index=True)     df = df.sort_values(by=['layers', 'func', 'mode']).reset_index(drop=True)     df.to_csv('measurements/jacobian_minimal_example.csv') ```  What jax/jaxlib version are you using? jax v0.4.20 jaxlib v0.4.20+cuda12.cudnn89  Which accelerator(s) are you using? GPU  Additional system info? Python 3.10.8, Linux  NVIDIA GPU info ``` ++  ++++ ```",2024-01-24T16:59:13Z,bug,open,2,2,https://github.com/jax-ml/jax/issues/19498,"Just to mention, I am already happy about helpful comments how I can try to debug this myself. In the process of debugging, I'd like to inspect the XLA HLO results to see if it is compiled into a serial computation or if XLA recognizes the scan as a parallel operation. Therefore, I specify the flags  ``` XLA_FLAGS=""xla_dump_to=my/file/path xla_dump_hlo_as_dot=true xla_dump_fusion_visualization=true"" ``` Is there a way to make XLA print human readable function names? Currently, the output reads for example `module_0012.jit__unnamed_function`. From the operations, I can guess which kernel this refers to, but it would make debugging much easier if the operations could be named.","Looking into the HLO output, I am quite sure about the correspondence between graphs and functions in the code. HLO for T=4 recurrence steps  Forward pass: module_0009.jit__unnamed_function_.sm_8.0_gpu_after_optimizations.pdf  $\frac{\partial h_4}{\partial \lambda}$ backward (VJP) module_0010.jit__unnamed_function_.sm_8.0_gpu_after_optimizations.pdf HLO for T=128 recurrence steps  Forward pass module_0009.jit__unnamed_function_.sm_8.0_gpu_after_optimizations.pdf  $\frac{\partial h_{128}}{\partial W}$ module_0010.jit__unnamed_function_.sm_8.0_gpu_after_optimizations.pdf  $\frac{\partial h_{128}}{\partial \lambda}$ module_0011.jit__unnamed_function_.sm_8.0_gpu_after_optimizations.pdf For T=128, the striking difference between $\frac{\partial h_{128}}{\partial W}$ and $\frac{\partial h_{128}}{\partial \lambda}$  with VJP is the much larger number of operations that seem to be required to compute the vjp."
yi,Minor bug in jax.vjp? (difference w.r.t. jax.grad on scalar function)," Description Consider the following function: `f = lambda a : jax.numpy.multiply(a,a)`. I compute its gradient using not `grad`, but `vjp` (it is something I want, to be able to handle functions with multiple outputs). Given that `grad` is characterized in the doc as a restriction of `vjp`, the result should be the same. And yet, after applying the following code: ```  def myfun(x):     _,y = jax.vjp(f,x)     return y exchange_dialect = 'stablehlo' str_hlo = str(myfun.lower(1.0).compiler_ir(dialect=exchange_dialect)) print(str_hlo) ``` I obtain: ``` func.func public (%arg0: tensor {mhlo.layout_mode = ""default"", mhlo.sharding = ""{replicated}""}) > (tensor {jax.result_info = ""[][0][][0][0]"", mhlo.layout_mode = ""default""}) {     return %arg0 : tensor   } ``` The result is correct if I use `grad` instead of `vjp`, or if the function `f` is defined as `jax.numpy.square`. This makes me think that the bug is related to the handling of broadcast (the fact that variable `a` is used twice inside the lambda).   What jax/jaxlib version are you using? 0.4.23 0.4.23  Which accelerator(s) are you using? CPU  Additional system info? Python 3.11.4  NVIDIA GPU info _No response_",2024-01-23T09:49:23Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/19479,"The second value returned by `vjp` is a pullback function. If you return it, from a `jit`decorated function JAX *will* return it and treat it as opaque, ~but it would be an error to call it~. (*edit*: curiously in this case it does work, perhaps it does in general). See the documentation for an example of how to use it: https://jax.readthedocs.io/en/latest/_autosummary/jax.vjp.html Does that help?"
yi,Struggling with multiprocess in JAX," Description I'm trying to compile a piece of code on multiple CPUs and evaluate it in parallel for nested sampling.  I'm not trying to parallelize individual instances of the code; I just want to control these separate processes from the same script so I know when I can stop sampling. I found I had to do a warmup call on each CPU in order to run the code in my nested sampling routine, otherwise it recompiled each time.  But I'm finding that the warmup call often stalls. Here's a snippet that reproduces the error: ``` import jax import jax.numpy as jnp from jax import vmap import multiprocessing rho_g_vec = jnp.logspace(3,14,263) .jit def get_t(rho_g_vec):     def k2(x):         return jnp.where(x     p.join()   File ""/ext3/miniconda3/lib/python3.11/multiprocessing/process.py"", line 149, in join     res = self._popen.wait(timeout)           ^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/ext3/miniconda3/lib/python3.11/multiprocessing/popen_fork.py"", line 43, in wait     return self.poll(os.WNOHANG if timeout == 0.0 else 0)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/ext3/miniconda3/lib/python3.11/multiprocessing/popen_fork.py"", line 27, in poll     pid, sts = os.waitpid(self.pid, flag)                ^^^^^^^^^^^^^^^^^^^^^^^^^^ KeyboardInterrupt ``` This also has to be interrupted, giving ``` Exception ignored in atexit callback:  Traceback (most recent call last):   File ""/ext3/miniconda3/lib/python3.11/multiprocessing/util.py"", line 357, in _exit_function     p.join()   File ""/ext3/miniconda3/lib/python3.11/multiprocessing/process.py"", line 149, in join     res = self._popen.wait(timeout)           ^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/ext3/miniconda3/lib/python3.11/multiprocessing/popen_fork.py"", line 43, in wait     return self.poll(os.WNOHANG if timeout == 0.0 else 0)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/ext3/miniconda3/lib/python3.11/multiprocessing/popen_fork.py"", line 27, in poll     pid, sts = os.waitpid(self.pid, flag)                ^^^^^^^^^^^^^^^^^^^^^^^^^^ KeyboardInterrupt:  ```  What jax/jaxlib version are you using? 0.4.23 0.4.23  Which accelerator(s) are you using? CPU  Additional system info? _No response_  NVIDIA GPU info _No response_",2024-01-22T20:05:55Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/19467,"See, e.g., https://github.com/google/jax/issues/1805issuecomment561244991 JAX is incompatible with the `fork` strategy for `multiprocessing`. Does that fix the problem?","Yes, thanks!  Sorry, looks like I missed that when I was searching"
transformer,Incompatible CUDA versions installed from Pip," Description When using the default install command: ``` pip install U ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` it tries to install CUDA 12.3 libraries. However, the most recent stable version of PyTorch (2.1.2) is pinned to 12.1, and that's what I have on my machine. (I'm compiling flashattn, TransformerEngine and MSAMP on this machine after installing the main package dependencies, and the compilation appears to be sensitive to CUDA version.)  What jax/jaxlib version are you using? 0.4.23  Which accelerator(s) are you using? GPU, Nvidia A100  Additional system info? 1.24.4 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] uname_result(system='Linux', node='7e72bd4e01', release='5.15.091generic', version=' CC(Numpystyle indexed update support.)Ubuntu SMP Tue Nov 14 13:30:08 UTC 2023', machine='x86_64')  NVIDIA GPU info Mon Jan 22 11:32:43 2024        ++  ++",2024-01-22T19:33:06Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/19465,Hi  thanks for the question. You'll find some relevant info here: https://github.com/google/jax/issues/18032issuecomment1755389043 Our general approach is to provide two JAX CUDA builds: one at the most recent release (currently 12.3) and one older version that seeks to maintain compatibility with pytorch's requirements. Currently that is our cuda11 build.,"I'm going to close this, since it's essentially a duplicate of CC(JAX and TORCH). Thanks!",I'll note that the safest thing to do in general is use two separate venvs. Different frameworks have different version requirements. JAX tracks CUDA versions faster than PyTorch does.
yi,Bump actions/cache from 3.3.3 to 4.0.0,"Bumps actions/cache from 3.3.3 to 4.0.0.  Release notes Sourced from actions/cache's releases.  v4.0.0 What's Changed  Update action to node20 by @​takost in actions/cache CC(未找到相关数据) feat: savealways flag by @​tos in actions/cache CC(restore the behavior that Nones are pytrees)  New Contributors  @​takost made their first contribution in actions/cache CC(未找到相关数据) @​tos made their first contribution in actions/cache CC(restore the behavior that Nones are pytrees)  Full Changelog: https://github.com/actions/cache/compare/v3...v4.0.0    Changelog Sourced from actions/cache's changelog.  Releases 3.0.0  Updated minimum runner version support from node 12 &gt; node 16  3.0.1  Added support for caching from GHES 3.5. Fixed download issue for files &gt; 2GB during restore.  3.0.2  Added support for dynamic cache size cap on GHES.  3.0.3  Fixed avoiding empty cache save when no files are available for caching. (issue)  3.0.4  Fixed tar creation error while trying to create tar with path as ~/ home folder on ubuntulatest. (issue)  3.0.5  Removed error handling by consuming actions/cache 3.0 toolkit, Now cache server error handling will be done by toolkit. (PR)  3.0.6  Fixed  CC(Add a block_until_ready method to DeviceArray.)  zstd d: no such file or directory error Fixed  CC(improve while_loop/fori_loop dtype error message)  cache doesn't work with github workspace directory  3.0.7  Fixed  CC(`lax.scan` is ~6x slower to run than handwritten loops)  download stuck issue. A new timeout is introduced in the download process to abort the download if it gets stuck and doesn't finish within an hour.  3.0.8  Fix zstd not working for windows on gnu tar in issues  CC(jax.numpy.sum behaving funny for nonexistent axis) and  CC(Large memory needed in gathering operations). Allowing users to provide a custom timeout as input for aborting download of a cache segment using an environment variable SEGMENT_DOWNLOAD_TIMEOUT_MINS. Default is 60 minutes.  3.0.9  Enhanced the warning message for cache unavailablity in case of GHES.  3.0.10  Fix a bug with sorting inputs. Update definition for restorekeys in README.md    ... (truncated)   Commits  13aacd8 Merge pull request  CC(restore the behavior that Nones are pytrees) from tos/main 53b35c5 Merge branch 'main' into main 65b8989 Merge pull request  CC(未找到相关数据) from takost/updatetonode20 d0be34d Fix dist 66cf064 Merge branch 'main' into updatetonode20 1326563 Merge branch 'main' into main e718767 Fix format 0122982 Apply workaround for earlyExit 3185ecf Update &quot;only&quot; actions to node20 25618a0 Bump version Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-01-22T17:51:43Z,dependencies github_actions,closed,0,2,https://github.com/jax-ml/jax/issues/19461,We need to adjust the ratchet comments – I'll do this manually.,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
yi,Support for AMD/ROCm with Pallas,"I have JAX and Triton installed. On trying the code below I get the following error: I assume this is due to XLA treating my GPU to be a NVIDIA GPU? ``` import jax import jax.numpy as jnp from jax.experimental.pallas.ops import attention bs = 2 seqlen = 1000 n_heads = 32 dim = 128 rng = jax.random.PRNGKey(0) xq = jax.random.normal(rng, (bs, seqlen, n_heads, dim)) xk = jax.random.normal(rng, (bs, seqlen, n_heads, dim)) xv = jax.random.normal(rng, (bs, seqlen, n_heads, dim)) print('reference') res = attention.mha_reference(xq, xk, xv, None) print(res) print(res.shape) print('real kernel') print(attention.mha(xq, xk, xv, None)) ``` ``` >>> print(res.shape) (2, 1000, 32, 128) >>> print('real kernel') real kernel >>> print(attention.mha(xq, xk, xv, None)) Traceback (most recent call last):   File """", line 1, in    File ""/users/tavangani/.local/lib/python3.10/sitepackages/jax/experimental/pallas/ops/attention.py"", line 216, in mha     return pl.pallas_call(   File ""/users/tavangani/.local/lib/python3.10/sitepackages/jax/_src/pallas/pallas_call.py"", line 410, in wrapped     out_flat = pallas_call_p.bind( jax._src.source_info_util.JaxStackTraceBeforeTransformation: AttributeError: module 'triton.compiler.compiler' has no attribute 'CudaTargetDescriptor' The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. The above exception was the direct cause of the following exception: Traceback (most recent call last):   File """", line 1, in    File ""/users/tavangani/.local/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py"", line 1551, in pallas_call_lowering     compilation_result = compile_jaxpr(   File ""/users/tavangani/.local/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py"", line 1493, in compile_jaxpr     lowering_result = lower_jaxpr_to_triton_module(   File ""/users/tavangani/.local/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py"", line 241, in lower_jaxpr_to_triton_module     builder.target = tc.CudaTargetDescriptor( AttributeError: module 'triton.compiler.compiler' has no attribute 'CudaTargetDescriptor' ```",2024-01-22T13:18:06Z,enhancement contributions welcome AMD GPU,open,0,18,https://github.com/jax-ml/jax/issues/19453,I don't think we've ever tried this on AMD GPUs. Contributions welcome!  ?,   Yes we are working on upstreaming support for Pallas along with Triton and JaxTriton on AMD, thanks I did see that you had a fork for jaxtriton. Anything semi broken I can try right now? Also let me know if I can take up something particular and help. Thanks :),  Please try this docker image for now.  docker pull rocm/jaxbuild:rocm6.0.0jax0.4.20py3.10.0jax_triton,Thanks will do. Are there any binaries available. I am on LUMI and usually the step of converting from docker to singularity messes Jax containers for me. , I've tried above code with Jax image you provided and it failed with following error: ``` NotImplementedError: MLIR translation rule for primitive 'pallas_call' not found for platform cpu ``` Any tips on how to run it? Thanks!,"  The above code runs totally fine for me in the container I provided. In your case, it's running on CPU.  By the way, AFAIK, pallas call will lower to cpu only if interpret mode is true https://github.com/google/jax/blob/main/jax/experimental/pallas/ops/attention.pyL179C5L179C14", it seems like in my case jax doesn't recognize gpu: ``` > sudo docker run it capadd=SYS_PTRACE securityopt seccomp=unconfined device=/dev/kfd device=/dev/dri groupadd video ipc=host shmsize 8G docker.io/rocm/jaxbuild:rocm6.0.0jax0.4.20py3.10.0jax_triton > python > import jax > jax.devices() [CpuDevice(id=0)] ``` What's a correct way to run the docker container so jax can recognize AMD GPU available? I used docker command line from pytorch rocm tutorial. Thank you very much for helping!,> docker run it capadd=SYS_PTRACE securityopt seccomp=unconfined device=/dev/kfd device=/dev/dri groupadd video ipc=host shmsize 8G docker.io/rocm/jaxbuild:rocm6.0.0jax0.4.20py3.10.0jax_triton No issue with how you are running the docker. What kind of AMD GPU do you have? Do these commands work ok for you?  `rocmsmi` `rocminfo`," 2x 7900 XTX `rocmsmi`: ``` ====================================== ROCm System Management Interface ====================================== ================================================ Concise Info ================================================ Device  [Model : Revision]    Temp    Power  Partitions      SCLK     MCLK    Fan  Perf  PwrCap  VRAM%  GPU%           Name (20 chars)       (Edge)  (Avg)  (Mem, Compute)                                                    ============================================================================================================== 0       [0x471e : 0xc8]       28.0°C  79.0W  N/A, N/A        1484Mhz  456Mhz  0%   auto  303.0W    0%   25%            0x744c                                                                                                 1       [0x471e : 0xc8]       25.0°C  42.0W  N/A, N/A        1930Mhz  96Mhz   0%   auto  303.0W    0%   33%            0x744c                                                                                                 ============================================================================================================== ============================================ End of ROCm SMI Log ============================================= ``` `rocminfo`: ``` ROCk module is loaded =====================     HSA System Attributes     =====================     Runtime Version:         1.1 System Timestamp Freq.:  1000.000000MHz Sig. Max Wait Duration:  18446744073709551615 (0xFFFFFFFFFFFFFFFF) (timestamp count) Machine Model:           LARGE                               System Endianness:       LITTLE                              Mwaitx:                  DISABLED DMAbuf Support:          YES ==========                HSA Agents                ==========                *******                   Agent 1                   *******                     Name:                    AMD EPYC 7252 8Core Processor        Uuid:                    CPUXX                                Marketing Name:          AMD EPYC 7252 8Core Processor        Vendor Name:             CPU                                   Feature:                 None specified                        Profile:                 FULL_PROFILE                          Float Round Mode:        NEAR                                  Max Queue Number:        0(0x0)                                Queue Min Size:          0(0x0)                                Queue Max Size:          0(0x0)                                Queue Type:              MULTI                                 Node:                    0                                     Device Type:             CPU                                   Cache Info:                   L1:                      32768(0x8000) KB                      Chip ID:                 0(0x0)                                ASIC Revision:           0(0x0)                                Cacheline Size:          64(0x40)                              Max Clock Freq. (MHz):   3100                                  BDFID:                   0                                     Internal Node ID:        0                                     Compute Unit:            16                                    SIMDs per CU:            0                                     Shader Engines:          0                                     Shader Arrs. per Eng.:   0                                     WatchPts on Addr. Ranges:1                                     Features:                None   Pool Info:                    Pool 1                          Segment:                 GLOBAL; FLAGS: FINE GRAINED               Size:                    131768212(0x7da9f94) KB                   Allocatable:             TRUE                                      Alloc Granule:           4KB                                       Alloc Alignment:         4KB                                       Accessible by all:       TRUE                                    Pool 2                          Segment:                 GLOBAL; FLAGS: KERNARG, FINE GRAINED       Size:                    131768212(0x7da9f94) KB                   Allocatable:             TRUE                                      Alloc Granule:           4KB                                       Alloc Alignment:         4KB                                       Accessible by all:       TRUE                                    Pool 3                          Segment:                 GLOBAL; FLAGS: COARSE GRAINED             Size:                    131768212(0x7da9f94) KB                   Allocatable:             TRUE                                      Alloc Granule:           4KB                                       Alloc Alignment:         4KB                                       Accessible by all:       TRUE                                  ISA Info:                 *******                   Agent 2                   *******                     Name:                    gfx1100                               Uuid:                    GPUa18fbfbd757cdfd0                  Marketing Name:          Radeon RX 7900 XTX                    Vendor Name:             AMD                                   Feature:                 KERNEL_DISPATCH                       Profile:                 BASE_PROFILE                          Float Round Mode:        NEAR                                  Max Queue Number:        128(0x80)                             Queue Min Size:          64(0x40)                              Queue Max Size:          131072(0x20000)                       Queue Type:              MULTI                                 Node:                    1                                     Device Type:             GPU                                   Cache Info:                   L1:                      32(0x20) KB                             L2:                      6144(0x1800) KB                         L3:                      98304(0x18000) KB                     Chip ID:                 29772(0x744c)                         ASIC Revision:           0(0x0)                                Cacheline Size:          64(0x40)                              Max Clock Freq. (MHz):   2371                                  BDFID:                   49920                                 Internal Node ID:        1                                     Compute Unit:            96                                    SIMDs per CU:            2                                     Shader Engines:          6                                     Shader Arrs. per Eng.:   2                                     WatchPts on Addr. Ranges:4                                     Coherent Host Access:    FALSE                                 Features:                KERNEL_DISPATCH    Fast F16 Operation:      TRUE                                  Wavefront Size:          32(0x20)                              Workgroup Max Size:      1024(0x400)                           Workgroup Max Size per Dimension:     x                        1024(0x400)                             y                        1024(0x400)                             z                        1024(0x400)                           Max Waves Per CU:        32(0x20)                              Max Workitem Per CU:    1024(0x400)                           Grid Max Size:           4294967295(0xffffffff)                Grid Max Size per Dimension:     x                        4294967295(0xffffffff)                  y                        4294967295(0xffffffff)                  z                        4294967295(0xffffffff)                Max fbarriers/Workgrp:   32                                    Packet Processor uCode:: 550                                   SDMA engine uCode::      19                                    IOMMU Support::          None                                  Pool Info:                    Pool 1                          Segment:                 GLOBAL; FLAGS: COARSE GRAINED             Size:                    25149440(0x17fc000) KB                    Allocatable:             TRUE                                      Alloc Granule:           4KB                                       Alloc Alignment:         4KB                                       Accessible by all:       FALSE                                   Pool 2                          Segment:                 GLOBAL; FLAGS: EXTENDED FINE GRAINED       Size:                    25149440(0x17fc000) KB                    Allocatable:             TRUE                                      Alloc Granule:           4KB                                       Alloc Alignment:         4KB                                       Accessible by all:       FALSE                                   Pool 3                          Segment:                 GROUP                                     Size:                    64(0x40) KB                               Allocatable:             FALSE                                     Alloc Granule:           0KB                                       Alloc Alignment:         0KB                                       Accessible by all:       FALSE                                 ISA Info:                     ISA 1                           Name:                    amdgcnamdamdhsagfx1100                Machine Models:          HSA_MACHINE_MODEL_LARGE                   Profiles:                HSA_PROFILE_BASE                          Default Rounding Mode:   NEAR                                      Default Rounding Mode:   NEAR                                      Fast f16:                TRUE                                      Workgroup Max Size:      1024(0x400)                               Workgroup Max Size per Dimension:         x                        1024(0x400)                                 y                        1024(0x400)                                 z                        1024(0x400)                               Grid Max Size:           4294967295(0xffffffff)                    Grid Max Size per Dimension:         x                        4294967295(0xffffffff)                      y                        4294967295(0xffffffff)                      z                        4294967295(0xffffffff)                    FBarrier Max Size:       32                                  *******                   Agent 3                   *******                     Name:                    gfx1100                               Uuid:                    GPUd59bfe6e2d839dab                  Marketing Name:          Radeon RX 7900 XTX                    Vendor Name:             AMD                                   Feature:                 KERNEL_DISPATCH                       Profile:                 BASE_PROFILE                          Float Round Mode:        NEAR                                  Max Queue Number:        128(0x80)                             Queue Min Size:          64(0x40)                              Queue Max Size:          131072(0x20000)                       Queue Type:              MULTI                                 Node:                    2                                     Device Type:             GPU                                   Cache Info:                   L1:                      32(0x20) KB                             L2:                      6144(0x1800) KB                         L3:                      98304(0x18000) KB                     Chip ID:                 29772(0x744c)                         ASIC Revision:           0(0x0)                                Cacheline Size:          64(0x40)                              Max Clock Freq. (MHz):   2371                                  BDFID:                   33536                                 Internal Node ID:        2                                     Compute Unit:            96                                    SIMDs per CU:            2                                     Shader Engines:          6                                     Shader Arrs. per Eng.:   2                                     WatchPts on Addr. Ranges:4                                     Coherent Host Access:    FALSE                                 Features:                KERNEL_DISPATCH    Fast F16 Operation:      TRUE                                  Wavefront Size:          32(0x20)                              Workgroup Max Size:      1024(0x400)                           Workgroup Max Size per Dimension:     x                        1024(0x400)                             y                        1024(0x400)                             z                        1024(0x400)                           Max Waves Per CU:        32(0x20)                              Max Workitem Per CU:    1024(0x400)                           Grid Max Size:           4294967295(0xffffffff)                Grid Max Size per Dimension:     x                        4294967295(0xffffffff)                  y                        4294967295(0xffffffff)                  z                        4294967295(0xffffffff)                Max fbarriers/Workgrp:   32                                    Packet Processor uCode:: 550                                   SDMA engine uCode::      19                                    IOMMU Support::          None                                  Pool Info:                    Pool 1                          Segment:                 GLOBAL; FLAGS: COARSE GRAINED             Size:                    25149440(0x17fc000) KB                    Allocatable:             TRUE                                      Alloc Granule:           4KB                                       Alloc Alignment:         4KB                                       Accessible by all:       FALSE                                   Pool 2                          Segment:                 GLOBAL; FLAGS: EXTENDED FINE GRAINED       Size:                    25149440(0x17fc000) KB                    Allocatable:             TRUE                                      Alloc Granule:           4KB                                       Alloc Alignment:         4KB                                       Accessible by all:       FALSE                                   Pool 3                          Segment:                 GROUP                                     Size:                    64(0x40) KB                               Allocatable:             FALSE                                     Alloc Granule:           0KB                                       Alloc Alignment:         0KB                                       Accessible by all:       FALSE                                 ISA Info:                     ISA 1                           Name:                    amdgcnamdamdhsagfx1100                Machine Models:          HSA_MACHINE_MODEL_LARGE                   Profiles:                HSA_PROFILE_BASE                          Default Rounding Mode:   NEAR                                      Default Rounding Mode:   NEAR                                      Fast f16:                TRUE                                      Workgroup Max Size:      1024(0x400)                               Workgroup Max Size per Dimension:         x                        1024(0x400)                                 y                        1024(0x400)                                 z                        1024(0x400)                               Grid Max Size:           4294967295(0xffffffff)                    Grid Max Size per Dimension:         x                        4294967295(0xffffffff)                      y                        4294967295(0xffffffff)                      z                        4294967295(0xffffffff)                    FBarrier Max Size:       32                                  *** Done ***         ```","  Unfortunately, JAX is currently not supported on Navi i.e. 7900 XTX. We are working on adding support for this platform and hopefully will have it supported soon. Thanks!", I couldn't get it work because our HPC has issues detecting GPUs inside singularity containers with JAX. Can you please share the exact branches/versions of the libs below so I can try building myself? Can you also point out if there is a minimum rocm version requirement for this to work? jaxlib jax jaxtriton triton ,>  I couldn't get it work because our HPC has issues detecting GPUs inside singularity containers with JAX. Can you please share the exact branches/versions of the libs below so I can try building myself? Can you also point out if there is a minimum rocm version requirement for this to work? >  > jaxlib jax jaxtriton triton   https://github.com/ROCm/jax/tree/rocmjaxlibv0.4.20rocm6.0jaxtriton https://github.com/ROCm/xla/tree/rocmjaxlibv0.4.20rocm6.0 https://github.com/ROCm/triton/tree/tritonjaxtriton https://github.com/rahulbatra85/jaxtriton/tree/jaxtritonrocm ROCm 5.7 or ROCm 6.0," thank you! I tried the setup proposed above and I run into this error on the line when I run MHA as shown at the start of the issue Any ideas what might be causing this? I am on a MI250x ``` Traceback (most recent call last):   File ""/recommended_jax/jaxrocmjaxlibv0.4.20rocm6.0jaxtriton/jax/_src/pallas/triton/lowering.py"", line 332, in lower_jaxpr_to_triton_ir     outvals = rule(rule_ctx, *invals, **eqn.params)   File ""/recommended_jax/jaxrocmjaxlibv0.4.20rocm6.0jaxtriton/jax/_src/pallas/triton/lowering.py"", line 900, in _dot_general_lowering     return tl.dot(   File ""/recommended_jax/jax_build_venv2/lib/python3.10/sitepackages/triton/language/core.py"", line 31, in wrapper     return fn(*args, **kwargs)   File ""/recommended_jax/jax_build_venv2/lib/python3.10/sitepackages/triton/language/core.py"", line 971, in dot     return semantic.dot(input, other, allow_tf32, out_dtype, _builder)   File ""/recommended_jax/jax_build_venv2/lib/python3.10/sitepackages/triton/language/semantic.py"", line 1264, in dot     assert lhs.dtype == rhs.dtype, f""First input ({lhs.dtype}) and second input ({rhs.dtype}) must have the same dtype!"" AssertionError: First input (bf16) and second input (fp32) must have the same dtype! The above exception was the direct cause of the following exception: Traceback (most recent call last):   File """", line 1, in    File ""/recommended_jax/jaxrocmjaxlibv0.4.20rocm6.0jaxtriton/jax/experimental/pallas/ops/attention.py"", line 211, in mha     return pl.pallas_call(   File ""/recommended_jax/jaxrocmjaxlibv0.4.20rocm6.0jaxtriton/jax/_src/pallas/pallas_call.py"", line 383, in wrapped     out_flat = pallas_call_p.bind( jax._src.source_info_util.JaxStackTraceBeforeTransformation: jax._src.pallas.triton.lowering.TritonLoweringException: Exception while lowering eqn:   a:f32[128,128] = dot_general[   dimension_numbers=(([1], [0]), ([], []))   preferred_element_type=float32 ] b c With context:   TritonLoweringRuleContext(context=TritonModuleContext(name='mha_forward', ir_context=, builder=, module=, grid_mapping=GridMapping(grid=(8, 2, 32), block_mappings=(BlockMapping(block_shape=(, 1000, , 128), index_map_jaxpr={ lambda ; a:i32[] b:i32[] c:i32[]. let  in (b, 0, c, 0) }, memory_space=None), BlockMapping(block_shape=(, 1000, , 128), index_map_jaxpr={ lambda ; a:i32[] b:i32[] c:i32[]. let  in (b, 0, c, 0) }, memory_space=None), BlockMapping(block_shape=(, 1000, , 128), index_map_jaxpr={ lambda ; a:i32[] b:i32[] c:i32[]. let  in (b, 0, c, 0) }, memory_space=None), BlockMapping(block_shape=(, 1000, , 128), index_map_jaxpr={ lambda ; a:i32[] b:i32[] c:i32[]. let  in (b, 0, c, 0) }, memory_space=None)), mapped_dims=(), num_index_operands=0, num_scratch_operands=0), program_ids=[, , ]), avals_in=[ShapedArray(bfloat16[128,64]), ShapedArray(float32[64,128])], avals_out=[ShapedArray(float32[128,128])], block_infos=[None, None]) With inval shapes=[[constexpr[128], constexpr[64]], [constexpr[64], constexpr[128]]] With inval types=[, ] In jaxpr: { lambda ; a:Ref{float32[1000,128]} b:f32[128,128] c:Ref{float32[1000,128]} d:i32[]     e:f32[128,128] f:f32[128] g:f32[128]. let     h:i32[] = mul d 64     i:f32[64,128] "", line 1, in    File ""/recommended_jax/jaxrocmjaxlibv0.4.20rocm6.0jaxtriton/jax/_src/pallas/triton/lowering.py"", line 1541, in pallas_call_lowering     compilation_result = compile_jaxpr(   File ""/recommended_jax/jaxrocmjaxlibv0.4.20rocm6.0jaxtriton/jax/_src/pallas/triton/lowering.py"", line 1483, in compile_jaxpr     lowering_result = lower_jaxpr_to_triton_module(   File ""/recommended_jax/jaxrocmjaxlibv0.4.20rocm6.0jaxtriton/jax/_src/pallas/triton/lowering.py"", line 277, in lower_jaxpr_to_triton_module     () = lower_jaxpr_to_triton_ir(ctx, jaxpr, block_infos, *args)   File ""/recommended_jax/jaxrocmjaxlibv0.4.20rocm6.0jaxtriton/jax/_src/pallas/triton/lowering.py"", line 332, in lower_jaxpr_to_triton_ir     outvals = rule(rule_ctx, *invals, **eqn.params)   File ""/recommended_jax/jaxrocmjaxlibv0.4.20rocm6.0jaxtriton/jax/_src/pallas/triton/lowering.py"", line 1240, in _scan_lowering_rule     for_out = _lower_jaxpr_to_for_loop(   File ""/recommended_jax/jaxrocmjaxlibv0.4.20rocm6.0jaxtriton/jax/_src/pallas/triton/lowering.py"", line 1190, in _lower_jaxpr_to_for_loop     all_out = lower_jaxpr_to_triton_ir(   File ""/recommended_jax/jaxrocmjaxlibv0.4.20rocm6.0jaxtriton/jax/_src/pallas/triton/lowering.py"", line 336, in lower_jaxpr_to_triton_ir     raise TritonLoweringException( jax._src.pallas.triton.lowering.TritonLoweringException: Exception while lowering eqn:   a:f32[128,128] = dot_general[   dimension_numbers=(([1], [0]), ([], []))   preferred_element_type=float32 ] b c With context:   TritonLoweringRuleContext(context=TritonModuleContext(name='mha_forward', ir_context=, builder=, module=, grid_mapping=GridMapping(grid=(8, 2, 32), block_mappings=(BlockMapping(block_shape=(, 1000, , 128), index_map_jaxpr={ lambda ; a:i32[] b:i32[] c:i32[]. let  in (b, 0, c, 0) }, memory_space=None), BlockMapping(block_shape=(, 1000, , 128), index_map_jaxpr={ lambda ; a:i32[] b:i32[] c:i32[]. let  in (b, 0, c, 0) }, memory_space=None), BlockMapping(block_shape=(, 1000, , 128), index_map_jaxpr={ lambda ; a:i32[] b:i32[] c:i32[]. let  in (b, 0, c, 0) }, memory_space=None), BlockMapping(block_shape=(, 1000, , 128), index_map_jaxpr={ lambda ; a:i32[] b:i32[] c:i32[]. let  in (b, 0, c, 0) }, memory_space=None)), mapped_dims=(), num_index_operands=0, num_scratch_operands=0), program_ids=[, , ]), avals_in=[ShapedArray(bfloat16[128,64]), ShapedArray(float32[64,128])], avals_out=[ShapedArray(float32[128,128])], block_infos=[None, None]) With inval shapes=[[constexpr[128], constexpr[64]], [constexpr[64], constexpr[128]]] With inval types=[, ] In jaxpr: { lambda ; a:Ref{float32[1000,128]} b:f32[128,128] c:Ref{float32[1000,128]} d:i32[]     e:f32[128,128] f:f32[128] g:f32[128]. let     h:i32[] = mul d 64     i:f32[64,128] < a[h:h+64,:]     j:f32[128,64] = broadcast_in_dim[broadcast_dimensions=() shape=(128, 64)] 0.0     k:f32[128,64] = transpose[permutation=(1, 0)] i     l:f32[128,64] = dot_general[       dimension_numbers=(([1], [0]), ([], []))       preferred_element_type=float32     ] b k     m:f32[128,64] = add j l     n:f32[128] = reduce_max[axes=(1,)] m     o:f32[128] = max f n     p:f32[128] = sub f o     q:f32[128] = exp2 p     r:f32[128,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(128, 1)] o     s:f32[128,64] = sub m r     t:f32[128,64] = exp2 s     u:f32[128] = mul g q     v:f32[128] = reduce_sum[axes=(1,)] t     w:f32[128] = add u v     x:f32[128] = mul g 0.0     y:f32[128] = add x q     z:f32[128,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(128, 1)] y     ba:f32[128,128] = mul e z     bb:i32[] = mul d 64     bc:f32[64,128] < c[bb:bb+64,:]     bd:bf16[128,64] = convert_element_type[new_dtype=bfloat16 weak_type=False] t     be:f32[128,128] = dot_general[       dimension_numbers=(([1], [0]), ([], []))       preferred_element_type=float32     ] bd bc     bf:f32[128,128] = add ba be   in (bf, o, w) } ```","Further on making all the inputs bfloat16 I run into a different error: ``` >>> xq = jax.random.normal(rng, (bs, seqlen, n_heads, dim), dtype=""bfloat16"") >>> print(attention.mha(xq, xq, xq, None)) Traceback (most recent call last):   File """", line 1, in    File ""/recommended_jax/jaxrocmjaxlibv0.4.20rocm6.0jaxtriton/jax/experimental/pallas/ops/attention.py"", line 211, in mha     return pl.pallas_call(   File ""/recommended_jax/jaxrocmjaxlibv0.4.20rocm6.0jaxtriton/jax/_src/pallas/pallas_call.py"", line 383, in wrapped     out_flat = pallas_call_p.bind( jax._src.source_info_util.JaxStackTraceBeforeTransformation: TypeError: ttir_to_ttgir() got an unexpected keyword argument 'num_ctas' The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. The above exception was the direct cause of the following exception: Traceback (most recent call last):   File """", line 1, in    File ""/recommended_jax/jaxrocmjaxlibv0.4.20rocm6.0jaxtriton/jax/_src/pallas/triton/lowering.py"", line 1541, in pallas_call_lowering     compilation_result = compile_jaxpr(   File ""/recommended_jax/jaxrocmjaxlibv0.4.20rocm6.0jaxtriton/jax/_src/pallas/triton/lowering.py"", line 1488, in compile_jaxpr     ptx, name, shared_mem_bytes, compute_capability = compile_ttir_to_ptx_inplace(   File ""/recommended_jax/jaxtritonjaxtritonrocm/jax_triton/triton_lib.py"", line 256, in compile_ttir_to_ptx_inplace     ttgir = tc.ttir_to_ttgir(ttir, num_warps, warpsize=64, num_ctas=num_ctas, target=arch_full_details) TypeError: ttir_to_ttgir() got an unexpected keyword argument 'num_ctas' ```",Bump when is this expected?,  Can you try these branches instead? https://github.com/ROCm/jax/tree/rocmjaxlibv0.4.24jaxtriton https://github.com/ROCm/xla/commits/rocmjaxlibv0.4.24jaxtriton/ https://github.com/rahulbatra85/jaxtriton/tree/jaxtritonrocm0.4.24 https://github.com/ROCm/triton/tree/jaxtritonrocm0.4.24, thanks for the update. I will try and get back to you. On our main server (lumi) we are unfortunately stuck with rocm 5.6.1. Is there any way to build these for for <rocm5.7  Thank you!
yi,sm_scale in flash_attention_tpu does not work like what it should, Description hi. I'm trying to create a port for flash_attention and when I read the code I found out that (I guess I'm wrong) that the sm_scale is replace for attn_w / sqrt(head_dim)  but it's being multiplied instead of that so I thought the  calculated sm_scale should be  `2 ** (head_dim / 2)` and I set sm_scale to that but that simply wont work and even made predictions worst is it just  a bug or I'm not doing that right ? where sm_scale being used  https://github.com/google/jax/blob/main/jax/experimental/pallas/ops/tpu/flash_attention.pyL888 and why I thought this will work? ```python sqrt_ = sqrt(head_dim) (x * ( 2 ** (head_dim / 2) )== x / sqrt_) ``` thanks for any help :)  What jax/jaxlib version are you using? 0.4.23  Which accelerator(s) are you using? TPU  Additional system info? Linux  NVIDIA GPU info _No response_,2024-01-20T16:54:22Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/19450,https://github.com/google/jax/blob/main/jax/experimental/pallas/ops/tpu/flash_attention.pyL832,You can set sm_scale to 1/sqrt(head_dim). I think that should work.
rag,"Linear solvers (jax.numpy.linalg.solve, jax.scipy.linalg.solve) bugged when allocating large memory"," Description Hi JAX team, I have noticed that there seems to be a bug in the linear solvers which seems to happen when taking up a significant part of GPU memory. Below is an example. I solve well conditioned linear systems ( A = identity, b =  all ones) with A are 10x10 matrices. In one test the number of matrices n is 10^6 (so the matrices A take a total of 0.4 GB), and in the other one the number of matrices is 10^7 (for a total memory of 4GB). Note that 63 GB are reserved by JAX on the GPU so this is well below the capacity, and the reported ""peak"" usage is 13 GB. This happens both with jax.numpy.linalg.solve and jax.scipy.linalg.solve  ``` import jax import jax.numpy as jnp from jax import random device = jax.local_devices()[0] print('on device:', device) solve_fns = [ jax.scipy.linalg.solve, jnp.linalg.solve] m = 10 for solve_fn in solve_fns:     for n in [ int(1e6), int(1e7)]:         A = jnp.repeat(jnp.identity(m)[None], n, axis = 0)         x = jnp.ones([n,m])         b = jax.lax.batch_matmul(A,x[...,None])[...,0]         x_solved = solve_fn(A,b)         print(f""Average error with n ={n}, {jnp.mean(jnp.linalg.norm(x  x_solved, axis=1))} "")         print(""Memory info "", device.memory_stats()) ``` The output is as follows: ``` on device: cuda:0 Average error with n =1000000, 0.0  Memory info  {'bytes_in_use': 520001024, 'bytes_limit': 63880937472, 'bytes_reserved': 0, 'largest_alloc_size': 804000512, 'largest_free_block_bytes': 0, 'num_allocs': 42, 'peak_bytes_in_use': 1324001536, 'peak_bytes_reserved': 0, 'peak_pool_bytes': 63880937472, 'pool_bytes': 63880937472} Average error with n =10000000, nan  Memory info  {'bytes_in_use': 5200000000, 'bytes_limit': 63880937472, 'bytes_reserved': 0, 'largest_alloc_size': 8040000512, 'largest_free_block_bytes': 0, 'num_allocs': 71, 'peak_bytes_in_use': 13280000512, 'peak_bytes_reserved': 0, 'peak_pool_bytes': 63880937472, 'pool_bytes': 63880937472} Average error with n =1000000, 0.0  Memory info  {'bytes_in_use': 520000000, 'bytes_limit': 63880937472, 'bytes_reserved': 0, 'largest_alloc_size': 8040000512, 'largest_free_block_bytes': 0, 'num_allocs': 100, 'peak_bytes_in_use': 13280000512, 'peak_bytes_reserved': 0, 'peak_pool_bytes': 63880937472, 'pool_bytes': 63880937472} Average error with n =10000000, nan  Memory info  {'bytes_in_use': 5200000256, 'bytes_limit': 63880937472, 'bytes_reserved': 0, 'largest_alloc_size': 8040000512, 'largest_free_block_bytes': 0, 'num_allocs': 129, 'peak_bytes_in_use': 13280000512, 'peak_bytes_reserved': 0, 'peak_pool_bytes': 63880937472, 'pool_bytes': 63880937472} ``` This same code works fine on CPU, and I have also tried on a different GPU with the same results. Thank you very much for all your great work!  What jax/jaxlib version are you using? jax v0.4.18; jaxlib v0.4.18  Which accelerator(s) are you using? GPU/CPU  Additional system info? Python 3.9.18, OS Linux  NVIDIA GPU info ++  ++",2024-01-19T16:39:02Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/19431,"I have been making some tests about different batched solvers and decompositions, see below: The Cholesky (and LU probably), seems to be the main issue here. I tried to swap to a QR solver, but it is somehow 300 times slower than Cholesky for large batch size, and ~100 slower than SVD. SVD seems to be the most reliable, though I have seen it fail too.     import jax     import jax.numpy as jnp     device = jax.local_devices()[0]     print('on device:', device)     m = 10     import time     for n in [1e5, 1e6, 1e7]:         n = int(n)         A = jnp.repeat(jnp.identity(m)[None], n, axis = 0).block_until_ready()         print(""n="", n)         st_time = time.time()         U,S,Vh = jax.scipy.linalg.svd(A)         A2 = jax.lax.batch_matmul(U * S[...,None,:], Vh)         print(f""SVD error {jnp.mean(jnp.linalg.norm(A2  A, axis=(1, 2))/jnp.linalg.norm(A, axis=(1,2)))}, time = {time.time()  st_time} "")         st_time = time.time()         L = jax.scipy.linalg.cholesky(A)         A2 = jax.lax.batch_matmul(L, L.swapaxes(1,2)).block_until_ready()         print(f""Cholesky error {jnp.mean(jnp.linalg.norm(A2  A, axis=(1, 2))/jnp.linalg.norm(A, axis=(1,2)))}, time = {time.time()  st_time} "")         if n <= 1e6:             st_time = time.time()             Q,R = jnp.linalg.qr(A)             A2 = jax.lax.batch_matmul(Q,R).block_until_ready()             print(f""QR error {jnp.mean(jnp.linalg.norm(A2  A, axis=(1, 2))/jnp.linalg.norm(A, axis=(1,2)))}, time = {time.time()  st_time} "") Output:     on device: cuda:0     n= 100000     SVD error 0.0, time = 0.6100842952728271      Cholesky error 0.0, time = 0.15522980690002441      QR error 0.0, time = 3.7535462379455566      n= 1000000     SVD error 0.0, time = 0.5560173988342285      Cholesky error 0.0, time = 0.1310713291168213      QR error 0.0, time = 35.838584184646606      n= 10000000     SVD error 0.0, time = 2.056659460067749      Cholesky error nan, time = 0.27480244636535645  Note that JAX doesn't throw an error or warning  Here is a SVD based solver to use in the mean time, in case someone else needs a stopgap:     def solve_by_SVD(A,b):         U,S,Vh = jax.scipy.linalg.svd(A)         if b.ndim == A.ndim 1:             expand = True             b = b[...,None]         else:             expand = False         Uhb = jax.lax.batch_matmul(jnp.conj(U.swapaxes(1,2)),b)/ S[...,None]         x = jax.lax.batch_matmul(jnp.conj(Vh.swapaxes(1,2)),Uhb)         if expand:             x = x[...,0]         return x","As far as I can tell, the LU/Cholesky bugs are fixed by updating jax to 0.4.23 . See https://github.com/patrickkidger/lineax/issues/79issuecomment1929203311 for more info ","Well, fixed is fixed, I guess. We could dig into why, but it would mostly be of historical interest. Please reopen if it happens again!"
rag,Jax with cuda  installation command not correct," Description I'm installing jax following the command in README: `pip install U ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` However, this raises warning: ``` WARNING: jax 0.4.23 does not provide the extra 'cuda12pip' ``` and the installed cuda Jax is not available with the failure below: ``` CUDA backend failed to initialize: Found cuBLAS version 120103, but JAX was built against version 120205, which is newer. The copy of cuBLAS that is  installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)      ``` After checking the release website I tried to change the installation command to: `pip install U ""jax[cuda12]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` and it gave the correctly installed Jax  What jax/jaxlib version are you using? jax 0.4.23; jaxlib 0.4.23  Which accelerator(s) are you using? GPU  Additional system info? 1.26.3 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] uname_result(system='Linux', node='kellislab3.csail.mit.edu', release='5.15.086generic', version=' CC(add misc numpy ops (c.f. 70))Ubuntu SMP Wed Sep 20 08:23:49 UTC 2023', machine='x86_64')  NVIDIA GPU info ``` ++  ++++ ```",2024-01-19T14:22:17Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/19430,"Hi, thanks for the report. JAX does specify the `cuda12_pip` extra, but not `cuda12pip`. Can you check whether you had a typo when you ran the installation command?","> Hi, thanks for the report. JAX does specify the `cuda12_pip` extra, but not `cuda12pip`. Can you check whether you had a typo when you ran the installation command? Thanks for reply. I checked the command and it is 'cuda12_pip'. However, it seems like then pip identify it as `cuda12pip`. This happens when I just created a new conda env with python=3.10",What version of `pip` do you have installed?,It looks like you're running into the issue discussed here: https://discuss.python.org/t/whatextrasnamesaretreatedasequalandwhy/7614,"many thanks for pointing this out. according to their discussion, this is fixed in PEP685, so i tried python 3.12 and it works!",Is there a resolution for python<3.12?,"I've never been able to reproduce this issue personally; I'm sure it arises with some special combination of Python version, pip version, and jax version. I'd try installing newer or older pip versions and see if that fixes things for you."
yi,Memory leak when allocating arrays in a for loop," Description Hi there,  The issue was surfaced in Numpyro initially: https://github.com/pyroppl/numpyro/issues/1699, it seems all memory is not properly released when allocating a new array in a for loop. One reproducible example: ```python import jax, jaxlib; import jax.numpy as jnp import os import psutil import gc process = psutil.Process(os.getpid()) print(jax.__version__, jaxlib.__version__) .jit def fn(x):   return jnp.sum(jnp.sin(x)) for i in range(0, 1_001):     res = fn(jnp.zeros(i))     jax.clear_caches()     del res     for x in jax.live_arrays():         x.delete()     gc.collect()     if i % 100 == 0:       print(f""{i=}: {process.memory_info().rss / 1024 / 1024}"") ``` Gives: ``` 0.4.23 0.4.23 i=0: 153.28125 i=100: 179.421875 i=200: 182.40625 i=300: 183.84375 i=400: 186.25 i=500: 187.71875 i=600: 189.703125 i=700: 192.046875 i=800: 194.734375 i=900: 196.890625 i=1000: 198.265625 ``` More examples are available here: https://github.com/pyroppl/numpyro/issues/1699issuecomment1877573525 Thanks!  What jax/jaxlib version are you using? 0.4.23, 0.4.23  Which accelerator(s) are you using? CPU  Additional system info? 1.26.0 3.10.12  (main, Jun 23 2023, 22:41:52) [Clang 15.0.7 ] uname_result(system='Darwin', node='ClementsMacBookPro2.local', release='23.2.0', version='Darwin Kernel Version 23.2.0: Wed Nov 15 21:53:18 PST 2023; root:xnu10002.61.3~2/RELEASE_ARM64_T6000', machine='arm64')  NVIDIA GPU info _No response_",2024-01-19T13:27:31Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/19429,"The `Array` implementation uses a toplevel lrucaching function here: https://github.com/google/jax/blob/0b542ff585b30b24aceadd12a2335b0fa26b8209/jax/_src/array.pyL124 There may be other places with similar caches that I didn't go deep enough to dig up... but these would cause some amount of memory growth independent of the jaxspecific caching (e.g. anything that would be cleared by `clear_caches`) and a garbage collection would not free these LRU caches either. I also tried your test code without the JIT annotation and observed the same linear growth which makes me think that at least that is not the problem... So I experimented a bit. If you replace: ``` res = fn(jnp.zeros(i)) ``` ... with: ``` res = fn(jnp.zeros(5)) ``` You will still observe some memory growth, but the growth is more stable. For example, I tested by increasing the iteration count to 5000 (larger than the cache limit) and ran it with a fixed array size and got: ``` 0.4.24 0.4.24 i=0: 127.640625 i=500: 138.46875 i=1000: 140.53125 i=1500: 140.59375 i=2000: 145.265625 i=2500: 145.28125 i=3000: 147.28125 i=3500: 147.953125 i=4000: 147.953125 i=4500: 147.96875 i=5000: 148.0 ``` This is sort of the behavior you would expect from an LRU cache, so I would only be concerned if the memory growth is unbounded. You mentioned in the other ticket: > my model has varying input sizes and needs to reestimate all parameters regularly This would mean you are likely to have a lot of cache misses and would end up fully utilizing the cache space without benefiting much from the cached values.",It seems like this has been answered. Feel free to open another issue if you still have questions!
yi,[XLA:Python] Fail with an AttributeError if __cuda_array_interface__ is called on a sharded array.,[XLA:Python] Fail with an AttributeError if __cuda_array_interface__ is called on a sharded array. Fixes https://github.com/google/jax/issues/19134,2024-01-19T12:16:10Z,,closed,0,0,https://github.com/jax-ml/jax/issues/19428
mixtral,using jnp.nozero is not possible in jax.eval_shape," Description im implementing mixtral model in jax and when i want to use jnp.nozero it causes error and i cant init the params and get shape here's how im using that  ```python class FlaxMixtralBlocKSparesTop2MLPCollection(nn.Module):     config: MixtralConfig     dtype: jnp.dtype = jnp.bfloat16     param_dtype: jnp.dtype = jnp.bfloat16     precision: Optional[jax.lax.Precision] = jax.lax.Precision(""fastest"")     def setup(self) > None:         self.layers = [             FlaxMixtralBLockSparseTop2MLP(                 config=self.config,                 dtype=self.dtype,                 param_dtype=self.param_dtype,                 precision=self.precision,                 name=str(i)             )             for i in range(self.config.num_local_experts)         ]     def __call__(             self,             expert_mask: chex.Array,             hidden_states: chex.Array,             routing_weights: chex.Array,             batch_size: int,             sequence_length: int,             hidden_dim: int     ) > chex.Array:         assert hidden_states.ndim == 2         final_hidden_states = jnp.zeros(             (batch_size * sequence_length, hidden_dim), dtype=hidden_states.dtype         )         for expert_idx, expert_layer in enumerate(self.layers):             selected_mask = expert_mask[expert_idx]             idx, top_x = jnp.nonzero(selected_mask)             if top_x.shape[0] == 0:                 continue             current_state = hidden_states[None, top_x].reshape(1, hidden_dim)             current_hidden_states = expert_layer(                 current_state             ) * routing_weights[top_x, idx, None]             final_hidden_states = final_hidden_states.at[top_x].set(                 current_hidden_states + final_hidden_states[top_x]             )         return final_hidden_states.reshape(batch_size, sequence_length, hidden_dim) ``` is there any recommendation or help that i can get ?  What jax/jaxlib version are you using? 0.4.20 JAX and JAXlib  Which accelerator(s) are you using? CPU/TPU  Additional system info? Linux  NVIDIA GPU info _No response_",2024-01-19T09:53:19Z,bug,closed,0,8,https://github.com/jax-ml/jax/issues/19424,">  Description > im implementing mixtral model in jax and when i want to use jnp.nozero it causes error and i cant init the params and get shape >  > here's how im using that >  > ```python > class FlaxMixtralBlocKSparesTop2MLPCollection(nn.Module): >     config: MixtralConfig >     dtype: jnp.dtype = jnp.bfloat16 >     param_dtype: jnp.dtype = jnp.bfloat16 >     precision: Optional[jax.lax.Precision] = jax.lax.Precision(""fastest"") >  >     def setup(self) > None: >         self.layers = [ >             FlaxMixtralBLockSparseTop2MLP( >                 config=self.config, >                 dtype=self.dtype, >                 param_dtype=self.param_dtype, >                 precision=self.precision, >                 name=str(i) >             ) >             for i in range(self.config.num_local_experts) >         ] >  >     def __call__( >             self, >             expert_mask: chex.Array, >             hidden_states: chex.Array, >             routing_weights: chex.Array, >             batch_size: int, >             sequence_length: int, >             hidden_dim: int >     ) > chex.Array: >         assert hidden_states.ndim == 2 >         final_hidden_states = jnp.zeros( >             (batch_size * sequence_length, hidden_dim), dtype=hidden_states.dtype >         ) >  >         for expert_idx, expert_layer in enumerate(self.layers): >             selected_mask = expert_mask[expert_idx] >  >             idx, top_x = jnp.nonzero(selected_mask) >             if top_x.shape[0] == 0: >                 continue >  >             current_state = hidden_states[None, top_x].reshape(1, hidden_dim) >  >             current_hidden_states = expert_layer( >                 current_state >             ) * routing_weights[top_x, idx, None] >             final_hidden_states = final_hidden_states.at[top_x].set( >                 current_hidden_states + final_hidden_states[top_x] >             ) >  >         return final_hidden_states.reshape(batch_size, sequence_length, hidden_dim) > ``` >  > is there any recommendation or help that i can get ? >  >  What jax/jaxlib version are you using? > 0.4.20 JAX and JAXlib >  >  Which accelerator(s) are you using? > CPU/TPU >  >  Additional system info? > Linux >  >  NVIDIA GPU info > _No response_ https://github.com/erfanzar/EasyDeL/blob/main/lib/python/EasyDel/modules/mixtral/modelling_mixtral_flax.pyL373","Hi  the issue is that `jnp.nonzero` creates a dynamicallyshaped array (i.e. an array shape that depends on the values in the array passed to it), and thus is incompatible with `eval_shape`. You can address this by passing a static `size` argument to `nonzero`, in order to statiscally specify the size of the output array; for example: ```python import jax import jax.numpy as jnp def f1(x):   return jnp.nonzero(x) def f2(x):   return jnp.nonzero(x, size=5) x = jax.ShapeDtypeStruct(shape=(10,), dtype='float32') jax.eval_shape(f1, x)   error jax.eval_shape(f2, x)   ok ``` If the array passed to `nonzero` has fewer nonzero entries than the specified `size`, the results will be padded with zeros. If it has more nonzero entries, they will be truncated. This static shape requirement is fundamental to the design of JAX transformations; for more discussion, see https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.htmldynamicshapes.",Yes i have tried that and noticed that with using size argument it will work correctly but it's no longer dynamic for the purpose that in using that," If you're trying to do megablocksstyle MoE layers, I don't know if you actually need/want to be computing the nonzeros for each expert. Instead, you know that each token gets assigned exactly k experts, so the number of token/expert pairs actually is statically knowable. The thing I haven't thought carefully about is whether you can actually implement the necessary blocksparse matmul efficiently in pure JAX. I suspect you can't do it memoryefficiently without a custom kernel but I'm not sure.",  guess an scan function does the job but anyway have you seen somebody implement this in jax or i should figure it out myself ," I've actually been experimenting with implementing the megablocks stuff. So far I've made the forward pass in pure JAX (so backwards will work as well), and I also have pallas kernels for the DSD and SDD matmuls done. There's still quite a lot to do, but I can prioritize getting something shareable uploaded if you're interested.",I would be more than happy to connect with you in that case but have you tried flax.linen.scan and conditioner to make it?,"I'm going to close this because the original question is answered; if you want to chat about ideas for how to implement megablocks MoE, perhaps a dedicated discussion would be a better place. Thanks!"
llm,Use jit's jaxpr creation function for eval_shape to maximize tracing cache hits.,"Use jit's jaxpr creation function for eval_shape to maximize tracing cache hits. This comes up in LLM models, where we trace twice (one for eval_shape (usually the init function) and another during jit) when the output jaxpr is the same. This shouldn't happen and we should cache as much as possible.  The only caveat here is that in eval_shape the `traced_for` on `DebugInfo` is set to `jit`. But maybe it's ok to do that if we want to deprecate eval_shape for a AOT style method on `jax.jit` or have it be a thin wrapper around something like `jax.jit(f).eval_shape`",2024-01-18T17:31:22Z,,closed,0,0,https://github.com/jax-ml/jax/issues/19407
yi,`jax.jvp` is tricky to use with non-differentiable inputs.," Description This ticket is a followup of issue CC(jnp.zeros does not support float0 as a dtype) following a comment by  [[link]](https://github.com/google/jax/issues/4433issuecomment1894147442_). The following reproducer tries to compute the forward derivative of a simple function that has a differentiable and a nondifferentiable (scalar, in this case) input. ```python import numpy as np import jax import jax.numpy as jnp bug = 0 .jit def f(x, y):     return x + y x  = jnp.array([1,2], dtype=jnp.float32) dx = jnp.array([10,20], dtype=jnp.float32) y = 1 if bug == 0:     dy = 0 elif bug == 1:     dy = jnp.empty((), dtype=jax.float0) elif bug == 2:     dy = np.empty((), dtype=jax.float0) out, tangents = jax.jvp(f, (x, y), (dx, dy)) print(f'{out=}, {tangents=}') ``` When the `bug` variable is set to zero at the top, the IMO most intuitive version of the code is used. This yields the error ``` Traceback (most recent call last):   File ""/Users/wjakob/test.py"", line 22, in      out, tangents = jax.jvp(f, (x, y), (dx, dy))                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/homebrew/lib/python3.12/sitepackages/jax/_src/api.py"", line 1945, in jvp     return _jvp(lu.wrap_init(fun), primals, tangents, has_aux=has_aux)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/homebrew/lib/python3.12/sitepackages/jax/_src/api.py"", line 1962, in _jvp     raise TypeError(""primal and tangent arguments to jax.jvp do not match; "" TypeError: primal and tangent arguments to jax.jvp do not match; dtypes must be equal, or in case of int/bool primal dtype the tangent dtype must be float0.Got primal dtype int32 and so expected tangent dtype [('float0', 'V')], but got tangent dtype int32 instead. ``` It seems impossible to actually create a JAX array with that dtype, I tried many different things. Setting `bug=1` gives an example of those difficulties: ```python Traceback (most recent call last):   File ""/Users/wjakob/test.py"", line 18, in      dy = jnp.empty((), dtype=jax.float0)          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/homebrew/lib/python3.12/sitepackages/jax/_src/numpy/lax_numpy.py"", line 2305, in empty     dtypes.check_user_dtype_supported(dtype, ""empty"")   File ""/opt/homebrew/lib/python3.12/sitepackages/jax/_src/dtypes.py"", line 685, in check_user_dtype_supported     raise TypeError(msg) TypeError: JAX only supports number and bool dtypes, got dtype [('float0', 'V')] in empty ``` Setting `bug=2` finally works by creating such a `float0` array using NumPy (of all things!) ``` out=Array([2., 3.], dtype=float32), tangents=Array([10., 20.], dtype=float32) ``` Anyways, this seems like a somewhat weird interface and is probably a bug. And it seems contradictory with 's comment: >  float0 arrays are purely a tracetime entity, and can only exist on the host (XLA does not have any equivalent of a float0 type).  I would also say that it's quite common for functions to accept nondifferentiable arguments and still expect `jax.jvp` to work (such variables are bound to occur somewhere in a pytree in more complex applications)  What jax/jaxlib version are you using? 0.4.23 0.4.23  Which accelerator(s) are you using? CPU  Additional system info? uname_result(system='Darwin', node='Wenzels1Macbook', release='22.6.0', version='Darwin Kernel Version 22.6.0: Thu Nov  2 07:43:57 PDT 2023; root:xnu8796.141.3.701.17~6/RELEASE_ARM64_T6000', machine='arm64')  NVIDIA GPU info _No response_",2024-01-16T20:05:32Z,bug,open,0,6,https://github.com/jax-ml/jax/issues/19386,"Thanks for the detail. I'm going to assign , because he has the most context on `float0`. Matt, maybe this is a good case for replacing `float0` with an implementation based on extended dtypes?","+1 on some kind of easy tracetime symbolic zero existing, but for unrelated reasons. I've had a few cases where constant folding of huge zero arrays leads to really long compile times, and I'm not aware of a nice pure JAX workaround atm.","Thanks for raising this. Yes we should expose a nice symboliczero dtype. However, for this `jvp` issue in the OP, I'd suggest writing it like this so you never have to make a `dy`: `out, tangents = jax.jvp(lambda x: f(x, y), (x,), (dx,))`.","Dear , thanks for looking into this, and for your great work on JAX. The partial evaluation workaround you suggested will work fine when the signature of the function is static and the nondiff. arguments nicely map to positional arguments.  My use case is too weird and generic: I am writing a differentiable bridge between JAX and another differentiable programming framework called Dr.Jit (drjit.wrap). The JVPed function has the signature `*args, **kwargs`, where any element could contain arbitrarily nested Pytrees including custom data structures where attempts to remove a field may cause problems. Having an officially supported symbolic zero would solve the issue. My current workaround is to create it with NumPy (the `bug=2` variant above), but I am unsure as to the longterm stability of that hack. Best, Wenzel","To handle the generic case you might be able to use equinox.filter_jvp which uses `None` as the appropriate tangent type. I wouldn't conflate this with symbolic zeros, though: I think choice of tangent dtype really is a separate thing. (What dtype would such a symbolic zero have, after all?) I'd also advocate against removing `float0`: this won't be a fun compatibility change, e.g. I have code that specialcases this is in a few spots and I would really prefer that not break. What I think *would* make sense though is for fixing various ergonomic issues, e.g. allowing for `float0` JAX arrays or adding a public `jax.unit = np.empty((), dtype=jax.float0)` that can be used.","> I'd also advocate against removing float0 Yes, sorry for not being clear. I'd want to fix the implementation and completeness of float0 (along the lines you described) while preserving the existing API."
yi,Bump actions/cache from 3.3.2 to 3.3.3,"Bumps actions/cache from 3.3.2 to 3.3.3.  Release notes Sourced from actions/cache's releases.  v3.3.3 What's Changed  Cache v3.3.3 by @​robherley in actions/cache CC(Temporarily disable test_jit_device_assignment.)  New Contributors  @​robherley made their first contribution in actions/cache CC(Temporarily disable test_jit_device_assignment.)  Full Changelog: https://github.com/actions/cache/compare/v3...v3.3.3    Changelog Sourced from actions/cache's changelog.  Releases 3.0.0  Updated minimum runner version support from node 12 &gt; node 16  3.0.1  Added support for caching from GHES 3.5. Fixed download issue for files &gt; 2GB during restore.  3.0.2  Added support for dynamic cache size cap on GHES.  3.0.3  Fixed avoiding empty cache save when no files are available for caching. (issue)  3.0.4  Fixed tar creation error while trying to create tar with path as ~/ home folder on ubuntulatest. (issue)  3.0.5  Removed error handling by consuming actions/cache 3.0 toolkit, Now cache server error handling will be done by toolkit. (PR)  3.0.6  Fixed  CC(Add a block_until_ready method to DeviceArray.)  zstd d: no such file or directory error Fixed  CC(improve while_loop/fori_loop dtype error message)  cache doesn't work with github workspace directory  3.0.7  Fixed  CC(`lax.scan` is ~6x slower to run than handwritten loops)  download stuck issue. A new timeout is introduced in the download process to abort the download if it gets stuck and doesn't finish within an hour.  3.0.8  Fix zstd not working for windows on gnu tar in issues  CC(jax.numpy.sum behaving funny for nonexistent axis) and  CC(Large memory needed in gathering operations). Allowing users to provide a custom timeout as input for aborting download of a cache segment using an environment variable SEGMENT_DOWNLOAD_TIMEOUT_MINS. Default is 60 minutes.  3.0.9  Enhanced the warning message for cache unavailablity in case of GHES.  3.0.10  Fix a bug with sorting inputs. Update definition for restorekeys in README.md    ... (truncated)   Commits  e12d46a Merge pull request  CC(Temporarily disable test_jit_device_assignment.) from actions/robherley/v3.3.3 1baebfc licensed eb94f1a cache v3.3.3 See full diff in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2024-01-15T18:02:32Z,pull ready dependencies github_actions,closed,0,0,https://github.com/jax-ml/jax/issues/19367
yi,Pallas is very slow," Description Hi, I am trying to write a simple cumulative sum program in Pallas and standard jax to compare their performance. However, it seems like Pallas is much slower.  When Pallas kernel is launching, there's a message saying `Removed command buffer support for CUBLAS as it's not supported with gpu toolkit version 12020 and driver version 12020. This might negatively impact performance.` I am wondering if this is the cause and how I could solve it. Thank you so much for your time and help!   Below is my program code: ``` from functools import partial from timeit import timeit import jax from jax.experimental import pallas as pl import jax.numpy as jnp import numpy as np def timing(f, *args):     f_time = lambda: f(*args)     timeit(f_time, number=20)   warmup     return timeit(f_time, number=100) .jit def cumsum_for(A):     output_list = []     prefix_sum = jnp.zeros(A.shape[1:])     for i in range(A.shape[0]):         prefix_sum = prefix_sum + A[i]         output_list.append(prefix_sum)     output_list = jnp.stack(output_list)     return output_list .jit def cumsum_lib(A):     return jnp.cumsum(A, axis=0) def cumsum_kernel(A_ref, O_ref):     prefix_sum = jnp.zeros((32, 32))     for i in range(64):         prefix_sum += A_ref[i]         O_ref[i] = prefix_sum def launch_cumsum_kernel(A: jax.Array):   return pl.pallas_call(     cumsum_kernel,     out_shape=jax.ShapeDtypeStruct(A.shape, A.dtype),     )(A) rng = jax.random.PRNGKey(0) A = jax.random.normal(rng, (64, 32, 32), dtype=jnp.float32) print(f""for {timing(cumsum_for, A) * 1000:.2f}ms\n""       f""lib {timing(cumsum_lib, A) * 1000:.2f}ms\n""       f""kernel {timing(launch_cumsum_kernel, A) * 1000:.2f}ms"") ``` And the output is   What jax/jaxlib version are you using? jax==0.4.24.dev20240104; jaxtriton==0.1.4; jaxlib==0.4.24.dev20240103+cuda12.cudnn89  Which accelerator(s) are you using? GPU  Additional system info? Python 3.10.13; Ubuntu 20.04;   NVIDIA GPU info  Cuda Toolkit: 12.3 cuDNN: 8.9.4 for CUDA 12.x",2024-01-13T04:03:09Z,bug pallas,open,0,15,https://github.com/jax-ml/jax/issues/19350,"Oh, it's my bad, I didn't jax.jit ```launch_cumsum_kernel```. **Please ignore my above message.** Once it's jitted, I test the below program ``` from functools import partial from timeit import timeit import jax from jax.experimental import pallas as pl import jax.numpy as jnp import numpy as np def timing(f, *args):     f_time = lambda: f(*args)     timeit(f_time, number=20)   warmup     return timeit(f_time, number=100) N = 16 D = 1024 grid_size = 32 block_size = D // grid_size .jit def cumsum_for(A):     output_list = []     prefix_sum = jnp.zeros((D, D))     for i in range(N):         prefix_sum = prefix_sum + A[i]         output_list.append(prefix_sum)     output_list = jnp.stack(output_list)     return output_list .jit def cumsum_lib(A):     return jnp.cumsum(A, axis=0) def cumsum_kernel(A_ref, O_ref):      N, d1, d2 = A_ref[...].shape      prefix_sum = jnp.zeros((d1, d2))      prefix_sum = jnp.zeros((768, 768))     prefix_sum = jnp.zeros((block_size, block_size))     for i in range(N):         prefix_sum += A_ref[i]         O_ref[i] = prefix_sum .jit def launch_cumsum_kernel(A: jax.Array):   return pl.pallas_call(     cumsum_kernel,     out_shape=jax.ShapeDtypeStruct((N, D, D), A.dtype),     grid=(1, grid_size, grid_size),     in_specs=[       pl.BlockSpec( lambda k, i, j: (0, i, j), (A.shape[0], A.shape[1] // grid_size, A.shape[2] // grid_size) ),     ],     out_specs=pl.BlockSpec(       lambda k, i, j: (0, i, j), (A.shape[0], A.shape[1] // grid_size, A.shape[2] // grid_size)     )   )(A) rng = jax.random.PRNGKey(0) A = jax.random.normal(rng, (N, D, D), dtype=jnp.float32) print(f""for {timing(cumsum_for, A) * 1000:.2f}ms\n""       f""lib {timing(cumsum_lib, A) * 1000:.2f}ms\n""       f""kernel {timing(launch_cumsum_kernel, A) * 1000:.2f}ms"") ``` The output of the above `D = 1024` and `grid_size = 32` is reasonable:  **However, when `D = 768` and `grid_size = 32`, I got this error despite that 768 is divisible by 32:** ``` File ""/miniconda3/envs/pallas/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py"", line 1621, in pallas_call_lowering     compilation_result = compile_jaxpr(   File ""/miniconda3/envs/pallas/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py"", line 1569, in compile_jaxpr     ptx, name, shared_mem_bytes, compute_capability = compile_ttir_to_ptx_inplace(   File ""/miniconda3/envs/pallas/lib/python3.10/sitepackages/jax_triton/triton_lib.py"", line 206, in compile_ttir_to_ptx_inplace     raise ValueError(""TTIR>TTGIR pass failed!"") from e ValueError: TTIR>TTGIR pass failed! ``` Could you please help me understand that?  Thank you very much!",I think blocks need to be powers of 2 sized.,"Thank you very much for your kind reply. Another question is that it seems like Pallas has some issues handling **batched matrix multiplication**. A simple example below: ``` def bmul_kernel(x_ref, y_ref, o_ref):   o_ref[...] = x_ref[...] @ y_ref[...] .jit def launch_bmul_kernel(x: jax.Array, y: jax.Array) > jax.Array:   return pl.pallas_call(bmul_kernel,                         out_shape=jax.ShapeDtypeStruct((x.shape[0], x.shape[1], y.shape[2]), x.dtype)                         )(x, y) x = jax.random.normal(rng, (2,3,4), dtype=jnp.float32) y = jax.random.normal(rng, (2,4,5), dtype=jnp.float32) print(launch_bmul_kernel(x, y).shape) ``` The error message is ``` Traceback (most recent call last):   File ""/miniconda3/envs/pallas/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py "", line 335, in lower_jaxpr_to_triton_ir     outvals = rule(rule_ctx, *invals, **eqn.params)   File ""/miniconda3/envs/pallas/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py "", line 949, in _dot_general_lowering     assert batch_dims == ((), ())                                                                            AssertionError                                                                                                                                                                                                            The above exception was the direct cause of the following exception:                                                                                                                                                      Traceback (most recent call last):   File ""/Kernel_test/pallas_cumsum_forward.py"", line 123, in      print(launch_bmul_kernel(x, y).shape)   File ""/Kernel_test/pallas_cumsum_forward.py"", line 117, in launch_bmul_kernel     return pl.pallas_call(bmul_kernel,   File ""/miniconda3/envs/pallas/lib/python3.10/sitepackages/jax/_src/pallas/pallas_call.py"", l ine 456, in wrapped     out_flat = pallas_call_p.bind( jax._src.source_info_util.JaxStackTraceBeforeTransformation: jax._src.pallas.triton.lowering.TritonLoweringEx ception: Exception while lowering eqn:   a:f32[2,3,5] = dot_general[   dimension_numbers=(([2], [1]), ([0], [0]))   preferred_element_type=float32 ] b c With context:   TritonLoweringRuleContext(context=TritonModuleContext(name='bmul_kernel', ir_context=, builder=, module=, grid_mapping=GridMapping(gri d=(), block_mappings=(None, None, None), mapped_dims=(), num_index_operands=0, num_scratch_operands=0), progr am_ids=[]), avals_in=[ShapedArray(float32[2,3,4]), ShapedArray(float32[2,4,5])], avals_out=[ShapedArray(float 32[2,3,5])], block_infos=[None, None]) With inval shapes=[[constexpr[2], constexpr[3], constexpr[4]], [constexpr[2], constexpr[4], constexpr[5]]] With inval types=[, ] In jaxpr: { lambda ; a:Ref{float32[2,3,4]} b:Ref{float32[2,4,5]} c:Ref{float32[2,3,5]}. let     d:f32[2,3,4] < a[:,:,:]     e:f32[2,4,5] < b[:,:,:]     f:f32[2,3,5] = dot_general[       dimension_numbers=(([2], [1]), ([0], [0]))       preferred_element_type=float32     ] d e     c[:,:,:] < f   in () } The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the fol lowing exception. ```","To do a batched matmul, it often makes sense to make the batch dimension a parallel dimension in the grid. The local matmuls on each SM will be unbatched (which is that the TensorCores are capable of anyways). You can do this easily by using `jax.vmap` on an unbatched matmul kernel.","Thanks for your kind reply. It seems like the error occurs even for simple matmul (not batched): ``` def bmul_kernel(x_ref, y_ref, o_ref):   o_ref[...] = x_ref[...] @ y_ref[...] .jit def launch_bmul_kernel(x: jax.Array, y: jax.Array) > jax.Array:   return pl.pallas_call(bmul_kernel,                         out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), x.dtype)                         )(x, y) x = jax.random.normal(rng, (2,4), dtype=jnp.float32) y = jax.random.normal(rng, (4,8), dtype=jnp.float32) print(launch_bmul_kernel(x, y).shape) ``` The error message is ``` The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/Kernel_test/pallas_cumsum_forward.py"", line 126, in      print(launch_bmul_kernel(x, y).shape)   File ""/miniconda3/envs/pallas/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py"", line 1621, in pallas_call_lowering     compilation_result = compile_jaxpr(   File ""/miniconda3/envs/pallas/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py"", line 1564, in compile_jaxpr     lowering_result = lower_jaxpr_to_triton_module(   File ""/miniconda3/envs/pallas/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py"", line 280, in lower_jaxpr_to_triton_module     () = lower_jaxpr_to_triton_ir(ctx, jaxpr, block_infos, *args)   File ""/miniconda3/envs/pallas/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py"", line 339, in lower_jaxpr_to_triton_ir     raise TritonLoweringException( jax._src.pallas.triton.lowering.TritonLoweringException: Exception while lowering eqn:   a:f32[2,8] = dot_general[   dimension_numbers=(([1], [0]), ([], []))   preferred_element_type=float32 ] b c With context:   TritonLoweringRuleContext(context=TritonModuleContext(name='bmul_kernel', ir_context=, builder=, module=, grid_mapping=GridMapping(grid=(), block_mappings=(None, None, None), mapped_dims=(), num_index_operands=0, num_scratch_operands=0), program_ids=[]), avals_in=[ShapedArray(float32[2,4]), ShapedArray(float32[4,8])], avals_out=[ShapedArray(float32[2,8])], block_infos=[None, None]) With inval shapes=[[constexpr[2], constexpr[4]], [constexpr[4], constexpr[8]]] With inval types=[, ] In jaxpr: { lambda ; a:Ref{float32[2,4]} b:Ref{float32[4,8]} c:Ref{float32[2,8]}. let     d:f32[2,4] < a[:,:]     e:f32[4,8] < b[:,:]     f:f32[2,8] = dot_general[       dimension_numbers=(([1], [0]), ([], []))       preferred_element_type=float32     ] d e     c[:,:] < f   in () } ```",Can you print the above exception? There should be a Triton exception as well,"``` Traceback (most recent call last):   File ""/miniconda3/envs/pallas/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py"", line 335, in lower_jaxpr_to_triton_ir     outvals = rule(rule_ctx, *invals, **eqn.params)   File ""/miniconda3/envs/pallas/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py"", line 966, in _dot_general_lowering     return tl.dot(   File ""/miniconda3/envs/pallas/lib/python3.10/sitepackages/triton/language/core.py"", line 27, in wrapper     return fn(*args, **kwargs)   File ""/miniconda3/envs/pallas/lib/python3.10/sitepackages/triton/language/core.py"", line 1060, in dot     return semantic.dot(input, other, acc, allow_tf32, max_num_imprecise_acc, out_dtype, _builder)   File ""/miniconda3/envs/pallas/lib/python3.10/sitepackages/triton/language/semantic.py"", line 1254, in dot     and rhs.shape[1].value >= 16, \ AssertionError: All values in both first input shape ([constexpr[2], constexpr[4]]) and second input shape ([constexpr[4], constexpr[8]]) must be >= 16! The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/Kernel_test/pallas_cumsum_forward.py"", line 126, in      print(launch_bmul_kernel(x, y).shape)   File ""/Kernel_test/pallas_cumsum_forward.py"", line 117, in launch_bmul_kernel     return pl.pallas_call(bmul_kernel,   File ""/miniconda3/envs/pallas/lib/python3.10/sitepackages/jax/_src/pallas/pallas_call.py"", line 456, in wrapped     out_flat = pallas_call_p.bind( jax._src.source_info_util.JaxStackTraceBeforeTransformation: jax._src.pallas.triton.lowering.TritonLoweringException: Exception while lowering eqn:   a:f32[2,8] = dot_general[   dimension_numbers=(([1], [0]), ([], []))   preferred_element_type=float32 ] b c With context:   TritonLoweringRuleContext(context=TritonModuleContext(name='bmul_kernel', ir_context=, builder=, module=, grid_mapping=GridMapping(grid=(), block_mappings=(None, None, None), mapped_dims=(), num_index_operands=0, num_scratch_operands=0), program_ids=[]), avals_in=[ShapedArray(float32[2,4]), ShapedArray(float32[4,8])], avals_out=[ShapedArray(float32[2,8])], block_infos=[None, None]) With inval shapes=[[constexpr[2], constexpr[4]], [constexpr[4], constexpr[8]]] With inval types=[, ] In jaxpr: { lambda ; a:Ref{float32[2,4]} b:Ref{float32[4,8]} c:Ref{float32[2,8]}. let     d:f32[2,4]      print(launch_bmul_kernel(x, y).shape)   File ""/miniconda3/envs/pallas/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py"", line 1621, in pallas_call_lowering     compilation_result = compile_jaxpr(   File ""/miniconda3/envs/pallas/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py"", line 1564, in compile_jaxpr     lowering_result = lower_jaxpr_to_triton_module(   File ""/miniconda3/envs/pallas/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py"", line 280, in lower_jaxpr_to_triton_module     () = lower_jaxpr_to_triton_ir(ctx, jaxpr, block_infos, *args)   File ""/miniconda3/envs/pallas/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py"", line 339, in lower_jaxpr_to_triton_ir     raise TritonLoweringException( jax._src.pallas.triton.lowering.TritonLoweringException: Exception while lowering eqn:   a:f32[2,8] = dot_general[   dimension_numbers=(([1], [0]), ([], []))   preferred_element_type=float32 ] b c With context:   TritonLoweringRuleContext(context=TritonModuleContext(name='bmul_kernel', ir_context=, builder=, module=, grid_mapping=GridMapping(grid=(), block_mappings=(None, None, None), mapped_dims=(), num_index_operands=0, num_scratch_operands=0), program_ids=[]), avals_in=[ShapedArray(float32[2,4]), ShapedArray(float32[4,8])], avals_out=[ShapedArray(float32[2,8])], block_infos=[None, None]) With inval shapes=[[constexpr[2], constexpr[4]], [constexpr[4], constexpr[8]]] With inval types=[, ] In jaxpr: { lambda ; a:Ref{float32[2,4]} b:Ref{float32[4,8]} c:Ref{float32[2,8]}. let     d:f32[2,4] < a[:,:]     e:f32[4,8] < b[:,:]     f:f32[2,8] = dot_general[       dimension_numbers=(([1], [0]), ([], []))       preferred_element_type=float32     ] d e     c[:,:] < f   in () } ```",The shapes you are using are too small. Try with bigger ones (like 32 at the very least).,"Thank you for your reply. I tried making `x` and `y` both have shape `(32,32)`, still got the below error. Making them `(64, 64)` or larger still has the similar error. ``` Traceback (most recent call last):   File ""/miniconda3/envs/kernel/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py"", line 335, in lower_jaxpr_to_triton_ir     outvals = rule(rule_ctx, *invals, **eqn.params)   File ""/miniconda3/envs/kernel/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py"", line 966, in _dot_general_lowering     return tl.dot(   File ""/miniconda3/envs/kernel/lib/python3.10/sitepackages/triton/language/core.py"", line 27, in wrapper     return fn(*args, **kwargs)   File ""/miniconda3/envs/kernel/lib/python3.10/sitepackages/triton/language/core.py"", line 1060, in dot     return semantic.dot(input, other, acc, allow_tf32, max_num_imprecise_acc, out_dtype, _builder)   File ""/miniconda3/envs/kernel/lib/python3.10/sitepackages/triton/language/semantic.py"", line 1276, in dot     if is_hip() and not mfma_supported(M, N, lhs.type.shape[1], allow_tf32, ret_scalar_ty):   File ""/miniconda3/envs/kernel/lib/python3.10/sitepackages/triton/common/build.py"", line 15, in is_hip     import torch ModuleNotFoundError: No module named 'torch' The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/Kernel_test/pallas_cumsum_forward.py"", line 126, in      print(launch_bmul_kernel(x, y).shape)   File ""/Kernel_test/pallas_cumsum_forward.py"", line 117, in launch_bmul_kernel     return pl.pallas_call(bmul_kernel,   File ""/miniconda3/envs/kernel/lib/python3.10/sitepackages/jax/_src/pallas/pallas_call.py"", line 456, in wrapped     out_flat = pallas_call_p.bind( jax._src.source_info_util.JaxStackTraceBeforeTransformation: jax._src.pallas.triton.lowering.TritonLoweringException: Exception while lowering eqn:   a:f32[32,32] = dot_general[   dimension_numbers=(([1], [0]), ([], []))   preferred_element_type=float32 ] b c With context:   TritonLoweringRuleContext(context=TritonModuleContext(name='bmul_kernel', ir_context=, builder=, module=, grid_mapping=GridMapping(grid=(), block_mappings=(None, None, None), mapped_dims=(), num_index_operands=0, num_scratch_operands=0), program_ids=[]), avals_in=[ShapedArray(float32[32,32]), ShapedArray(float32[32,32])], avals_out=[ShapedArray(float32[32,32])], block_infos=[None, None]) With inval shapes=[[constexpr[32], constexpr[32]], [constexpr[32], constexpr[32]]] With inval types=[, ] In jaxpr: { lambda ; a:Ref{float32[32,32]} b:Ref{float32[32,32]} c:Ref{float32[32,32]}. let     d:f32[32,32]      print(launch_bmul_kernel(x, y).shape)   File ""/nlp/scr/yusun/miniconda3/envs/kernel/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py"", line 1621, in pallas_call_lowering     compilation_result = compile_jaxpr(   File ""/nlp/scr/yusun/miniconda3/envs/kernel/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py"", line 1564, in compile_jaxpr     lowering_result = lower_jaxpr_to_triton_module(   File ""/nlp/scr/yusun/miniconda3/envs/kernel/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py"", line 280, in lower_jaxpr_to_triton_module     () = lower_jaxpr_to_triton_ir(ctx, jaxpr, block_infos, *args)   File ""/nlp/scr/yusun/miniconda3/envs/kernel/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py"", line 339, in lower_jaxpr_to_triton_ir     raise TritonLoweringException( jax._src.pallas.triton.lowering.TritonLoweringException: Exception while lowering eqn:   a:f32[32,32] = dot_general[   dimension_numbers=(([1], [0]), ([], []))   preferred_element_type=float32 ] b c With context:   TritonLoweringRuleContext(context=TritonModuleContext(name='bmul_kernel', ir_context=, builder=, module=, grid_mapping=GridMapping(grid=(), block_mappings=(None, None, None), mapped_dims=(), num_index_operands=0, num_scratch_operands=0), program_ids=[]), avals_in=[ShapedArray(float32[32,32]), ShapedArray(float32[32,32])], avals_out=[ShapedArray(float32[32,32])], block_infos=[None, None]) With inval shapes=[[constexpr[32], constexpr[32]], [constexpr[32], constexpr[32]]] With inval types=[, ] In jaxpr: { lambda ; a:Ref{float32[32,32]} b:Ref{float32[32,32]} c:Ref{float32[32,32]}. let     d:f32[32,32] < a[:,:]     e:f32[32,32] < b[:,:]     f:f32[32,32] = dot_general[       dimension_numbers=(([1], [0]), ([], []))       preferred_element_type=float32     ] d e     c[:,:] < f   in () } ```",You'll now need to have torch installed.,"Thank you very much! We are essentially trying to fuse cumsum and matmul, but have been constantly getting errors when assigning results to output buffer inside kernel. Our simple code is: ``` N = 16 D = 1024 gs_N = 2 gs_D = 32 def cumsum_kernel(X_ref, W_ref, G_ref, O_ref):     G_cumsum = jnp.zeros_like(G_ref[0])     W_cumulative = W_ref[0]     for i in range(N // gs_N):         G_cumsum += G_ref[i]         W_cumulative = G_cumsum         z = X_ref[i] @ W_cumulative         O_ref[i] = z    get error!         break .jit def launch_cumsum_kernel(X, W, G):     """"""     :param X: [N,D] > [N,1,D] > [K,1,1] blocks, each: [N//K, 1, D]     :param W: [D,D] > [1,D,D] > [1,1,M] blocks, each: [1, D, D//M]     :param G: [N,D,D] > [K,1,M] blocks, each: [N//K, D, D//M]     :return: Z: [N,1,D] > [K,1,M] blocks, each [N//K, 1, D//M]     """"""     X = jnp.expand_dims(X, axis=1)     W = jnp.expand_dims(W, axis=0)     Z = pl.pallas_call(     cumsum_kernel,     out_shape=jax.ShapeDtypeStruct((N, 1, D), X.dtype),     grid=(gs_N, 1, gs_D),     in_specs=[         pl.BlockSpec(lambda k, i, j: (k, 0, 0), (X.shape[0] // gs_N, X.shape[1], X.shape[2])),         pl.BlockSpec(lambda k, i, j: (0, 0, j), (W.shape[0], W.shape[1], W.shape[2] // gs_D)),         pl.BlockSpec(lambda k, i, j: (k, 0, j), (G.shape[0] // gs_N, G.shape[1], G.shape[2] // gs_D)),     ],     out_specs=pl.BlockSpec(         lambda k, i, j: (k, 0, j), (X.shape[0] // gs_N, X.shape[1], X.shape[2] // gs_D)     )     )(X, W, G)     return Z[:,0,:] rng = jax.random.PRNGKey(0) G = jax.random.normal(rng, (N, D, D), dtype=jnp.float32) W = jax.random.normal(rng, (D, D), dtype=jnp.float32) X = jax.random.normal(rng, (N, D), dtype=jnp.float32) O_4 = launch_cumsum_kernel(X, W, G) ``` The error message is: ``` Traceback (most recent call last):   File ""/miniconda3/envs/kernel/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py"", line 335, in lower_jaxpr_to_triton_ir     outvals = rule(rule_ctx, *invals, **eqn.params)   File ""/nlp/scr/yusun/miniconda3/envs/kernel/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py"", line 966, in _dot_general_lowering     return tl.dot(   File ""/miniconda3/envs/kernel/lib/python3.10/sitepackages/triton/language/core.py"", line 27, in wrapper     return fn(*args, **kwargs)   File ""/miniconda3/envs/kernel/lib/python3.10/sitepackages/triton/language/core.py"", line 1060, in dot     return semantic.dot(input, other, acc, allow_tf32, max_num_imprecise_acc, out_dtype, _builder)   File ""/miniconda3/envs/kernel/lib/python3.10/sitepackages/triton/language/semantic.py"", line 1254, in dot     and rhs.shape[1].value >= 16, \ AssertionError: All values in both first input shape ([constexpr[1], constexpr[1024]]) and second input shape ([constexpr[1024], constexpr[32]]) must be >= 16! The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/Kernel_test/pallas_cumsum_forward.py"", line 118, in      O_4 = launch_cumsum_kernel(X, W, G)   File ""/Kernel_test/pallas_cumsum_forward.py"", line 85, in launch_cumsum_kernel     Z = pl.pallas_call(   File ""/kernel/lib/python3.10/sitepackages/jax/_src/pallas/pallas_call.py"", line 456, in wrapped     out_flat = pallas_call_p.bind( jax._src.source_info_util.JaxStackTraceBeforeTransformation: jax._src.pallas.triton.lowering.TritonLoweringException: Exception while lowering eqn:   a:f32[1,32] = dot_general[   dimension_numbers=(([1], [0]), ([], []))   preferred_element_type=float32 ] b c With context:   TritonLoweringRuleContext(context=TritonModuleContext(name='cumsum_kernel', ir_context=, builder=, module=, grid_mapping=GridMapping(grid=(2, 1, 32), block_mappings=(BlockMapping(block_shape=(8, 1, 1024), index_map_jaxpr={ lambda ; a:i32[] b:i32[] c:i32[]. let  in (a, 0, 0) }, memory_space=None), BlockMapping(block_shape=(1, 1024, 32), index_map_jaxpr={ lambda ; a:i32[] b:i32[] c:i32[]. let  in (0, 0, c) }, memory_space=None), BlockMapping(block_shape=(8, 1024, 32), index_map_jaxpr={ lambda ; a:i32[] b:i32[] c:i32[]. let  in (a, 0, c) }, memory_space=None), BlockMapping(block_shape=(8, 1, 32), index_map_jaxpr={ lambda ; a:i32[] b:i32[] c:i32[]. let  in (a, 0, c) }, memory_space=None)), mapped_dims=(), num_index_operands=0, num_scratch_operands=0), program_ids=[, , ]), avals_in=[ShapedArray(float32[1,1024]), ShapedArray(float32[1024,32])], avals_out=[ShapedArray(float32[1,32])], block_infos=[None, None]) With inval shapes=[[constexpr[1], constexpr[1024]], [constexpr[1024], constexpr[32]]] With inval types=[, ] In jaxpr: { lambda ; a:Ref{float32[8,1,1024]} b:Ref{float32[1,1024,32]} c:Ref{float32[8,1024,32]}     d:Ref{float32[8,1,32]}. let     _:f32[1024,32]      O_4 = launch_cumsum_kernel(X, W, G)   File ""/miniconda3/envs/kernel/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py"", line 1621, in pallas_call_lowering     compilation_result = compile_jaxpr(   File ""/kernel/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py"", line 1564, in compile_jaxpr     lowering_result = lower_jaxpr_to_triton_module(   File ""/miniconda3/envs/kernel/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py"", line 280, in lower_jaxpr_to_triton_module     () = lower_jaxpr_to_triton_ir(ctx, jaxpr, block_infos, *args)   File ""/miniconda3/envs/kernel/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py"", line 339, in lower_jaxpr_to_triton_ir     raise TritonLoweringException( jax._src.pallas.triton.lowering.TritonLoweringException: Exception while lowering eqn:   a:f32[1,32] = dot_general[   dimension_numbers=(([1], [0]), ([], []))   preferred_element_type=float32 ] b c With context:   TritonLoweringRuleContext(context=TritonModuleContext(name='cumsum_kernel', ir_context=, builder=, module=, grid_mapping=GridMapping(grid=(2, 1, 32), block_mappings=(BlockMapping(block_shape=(8, 1, 1024), index_map_jaxpr={ lambda ; a:i32[] b:i32[] c:i32[]. let  in (a, 0, 0) }, memory_space=None), BlockMapping(block_shape=(1, 1024, 32), index_map_jaxpr={ lambda ; a:i32[] b:i32[] c:i32[]. let  in (0, 0, c) }, memory_space=None), BlockMapping(block_shape=(8, 1024, 32), index_map_jaxpr={ lambda ; a:i32[] b:i32[] c:i32[]. let  in (a, 0, c) }, memory_space=None), BlockMapping(block_shape=(8, 1, 32), index_map_jaxpr={ lambda ; a:i32[] b:i32[] c:i32[]. let  in (a, 0, c) }, memory_space=None)), mapped_dims=(), num_index_operands=0, num_scratch_operands=0), program_ids=[, , ]), avals_in=[ShapedArray(float32[1,1024]), ShapedArray(float32[1024,32])], avals_out=[ShapedArray(float32[1,32])], block_infos=[None, None]) With inval shapes=[[constexpr[1], constexpr[1024]], [constexpr[1024], constexpr[32]]] With inval types=[, ] In jaxpr: { lambda ; a:Ref{float32[8,1,1024]} b:Ref{float32[1,1024,32]} c:Ref{float32[8,1024,32]}     d:Ref{float32[8,1,32]}. let     _:f32[1024,32] < c[0,:,:]     e:f32[1024,32] = broadcast_in_dim[broadcast_dimensions=() shape=(1024, 32)] 0.0     f:f32[1024,32] < b[0,:,:]     _:f32[8,1,32] < d[:,:,:]     g:f32[1024,32] < c[0,:,:]     h:f32[1024,32] = add e g     i:f32[1024,32] = sub f h     j:f32[1,1024] < a[0,:,:]     k:f32[1,32] = dot_general[       dimension_numbers=(([1], [0]), ([], []))       preferred_element_type=float32     ] j i     d[0,:,:] < k   in () } ```",Here it seems like you're trying to do a matmuls with a 1sized dimension. Either it needs to be padded or you should be doing a matrix vector product (multiply then reduce sum).,"I printed out the shape of variables inside the kernel function: ``` def cumsum_kernel(X_ref, W_ref, G_ref, O_ref):     G_cumsum = jnp.zeros_like(G_ref[0])     W_cumulative = W_ref[0]     for i in range(N // gs_N):         G_cumsum += G_ref[i]         W_cumulative = G_cumsum   shape: (1024, 32)         z = X_ref[i] @ W_cumulative   shape: (1, 1024)         O_ref[i] = z    O_ref[i].shape: (1, 1024)         break ``` And the shape of `O_ref[i]` is the same as `z` (e.g., (1,1024)) in the for loop, but the assignment fails.","`z = X_ref[i] @ W_cumulative   shape: (1, 1024)` seems to be a matrix vector product, not a matrix multiplication right?","Hi, here the shape of X_ref[...] is [N//gs, 1, D], so X_ref[i] is of shape [1, D]. Thus the above is matrix multiplication. The simplified code we hope to test is as below, could you please help us understand why `O_ref[i] = z` fails? Thank you! ``` from functools import partial import jax from jax.experimental import pallas as pl import jax.numpy as jnp import numpy as np def cumsum_kernel(X_ref, W_ref, G_ref, O_ref):     '''     Fuse cumsum and matmul without materializing W_cumulative in HBM     '''     G_cumsum = jnp.zeros_like(G_ref[0])     W_cumulative = W_ref[0]     for i in range(16):         G_cumsum += G_ref[i]         W_cumulative = G_cumsum         z = X_ref[i] @ W_cumulative         print(z.shape)   shape: [1,32]         print(O_ref[i].shape)   shape: [1,32]         O_ref[i] = z   TODO: Why fails? .jit def launch_cumsum_kernel(X, W, G):     X = jnp.expand_dims(X, axis=1)     W = jnp.expand_dims(W, axis=0)     Z = pl.pallas_call(     cumsum_kernel,     out_shape=jax.ShapeDtypeStruct((16, 1, 32), X.dtype),     grid=(1, 1, 1),     in_specs=[         pl.BlockSpec(lambda k, i, j: (0, 0, 0), (X.shape[0], X.shape[1], X.shape[2])),         pl.BlockSpec(lambda k, i, j: (0, 0, 0), (W.shape[0], W.shape[1], W.shape[2])),         pl.BlockSpec(lambda k, i, j: (0, 0, 0), (G.shape[0], G.shape[1], G.shape[2])),     ],     out_specs=pl.BlockSpec(         lambda k, i, j: (0, 0, 0), (X.shape[0], X.shape[1], X.shape[2])     )     )(X, W, G)     return Z[:,0,:] rng = jax.random.PRNGKey(0) G = jax.random.normal(rng, (16, 32, 32), dtype=jnp.float32) W = jax.random.normal(rng, (32, 32), dtype=jnp.float32) X = jax.random.normal(rng, (16, 32), dtype=jnp.float32) O_4 = launch_cumsum_kernel(X, W, G) print(O_4.shape)   should be [16,32] ```"
yi,Improve jax.Array documentation,"https://jax.readthedocs.io/en/latest/_autosummary/jax.Array.htmljax.Array Many of the methods and properties of `jax.Array` are not sufficiently documented. * I don't think `ArrayImpl` appears in the documentation at all. It at least needs an explanation. * For pretty much all methods, especially those that don't correspond to NumPy APIs, we should link to documentation that shows an example of usage, including a typical input and output. * Some of the documentation is very cryptic and needs more details. For example: > addressable_data(index)  List of addressable shards. What type of object is a shard? Does they come in any particular order? * Some attributes aren't documented as best I can tell, e.g., `device` and `devices`.",2024-01-12T18:47:23Z,enhancement documentation,open,2,2,https://github.com/jax-ml/jax/issues/19342,"> I don't think `ArrayImpl` appears in the documentation at all This came up when we were creating the linked page. I recall  was a strong 1 on documenting `ArrayImpl`. Some implonly functions are not documented, becuase they don't exist on the base `Array` class. For the others, getting more detailed docs is mainly about changing how we declare `jax.Array` in the sphinx sources.","I think we should document the existence of `ArrayImpl` but not say a whole lot about it, pointing the user to `Array."
rag,[shape_poly] Protect shape_poly: rename to _shape_poly.py.,"[shape_poly] Protect shape_poly: rename to _shape_poly.py. The public APIs can be accessed through `jax.experimental.export`. The shape_poly and serialization modules are still changing and I saw external references to various symbols in them, even protected ones. I have removed such references from the Google code base, and I want to take another step to discourage direct access to its symbols.",2024-01-12T10:11:34Z,,closed,0,0,https://github.com/jax-ml/jax/issues/19332
yi,vmap conditional debug.print ignores conditioning," Description I'm trying to debug a vmapped code and wanted to print a message when certain conditions are met, and notices some odd results. Here's a minimal reproducible example: ``` import jax def f(x):     key = jax.random.PRNGKey(123)     u = jax.random.uniform(key)     jax.lax.cond(u > x, lambda w,z: jax.debug.print(""selecting u: {w}>{z}"",w=w,z=z), lambda x,y: None, u, x)     v = jax.lax.select(u > x, u, x)     return v vmapped = jax.vmap(f, in_axes=(0,)) key = jax.random.PRNGKey(1) x_grid = jax.random.uniform(key, (4,)) vmapped(x_grid) ``` This prints out the debug print for all values of x_grid regardless if they meet the condition or not.  What jax/jaxlib version are you using? 0.4.23 0.4.23  Which accelerator(s) are you using? GPU  Additional system info? 1.26.2 3.10.13  (main, Oct 26 2023, 18:07:37) [GCC 12.3.0] uname_result(system='Linux', node='node2415', release='4.18.0348.el8.0.2.x86_64', version=' CC(Python 3 compatibility issues) SMP Sun Nov 14 00:51:12 UTC 2021', machine='x86_64')  NVIDIA GPU info NVIDIASMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0",2024-01-11T15:39:48Z,question,closed,0,2,https://github.com/jax-ml/jax/issues/19314,"Hi, thanks for the question! This is behaving as expected: a vmapped `cond` becomes a `select` (as noted in the docstring) and a `select` executes both branches.",Thank you for the quick reply! I misunderstood what that part of the docstring meant! Sorry about the unnecessary issue report.
yi,Pallas: `jnp.indices` fails to lower to mosaic," Description The following kernel fails with the error listed below: ```python   def generate_kernel(n):     def sign_matrix_kernel(o_ref):       row = pl.program_id(axis=0)       col = pl.program_id(axis=1)       ones = jnp.ones(o_ref.shape, dtype=jnp.int32)       indices = jnp.indices(o_ref.shape)       x_indices = row * block_row_size + indices[0]       y_indices = col * block_col_size + indices[1]       o_ref[:] = jnp.where(x_indices >= y_indices, ones, ones)     return sign_matrix_kernel ``` Error: ``` XlaRuntimeError: INTERNAL: Mosaic failed to compile TPU kernel: iota rank below 2D unsupported The MLIR operation involved:   %6 = ""tpu.iota""() {dimension = 0 : i32} : () > vector; ``` Full reproducer ```python from functools import partial import jax from jax.experimental import pallas as pl import jax.numpy as jnp import numpy as np def generate_sign_matrix_kernelized(     n: int, block_row_size=8, block_col_size=128 ) > jnp.ndarray:   """"""Generates a matrix with 1s on/below the diagonal and 1 above.""""""   def generate_kernel(n):     def sign_matrix_kernel(o_ref):       row = pl.program_id(axis=0)       col = pl.program_id(axis=1)       ones = jnp.ones(o_ref.shape, dtype=jnp.int32)       indices = jnp.indices(o_ref.shape)       x_indices = row * block_row_size + indices[0]       y_indices = col * block_col_size + indices[1]       o_ref[:] = jnp.where(x_indices >= y_indices, ones, ones)     return sign_matrix_kernel   return pl.pallas_call(       generate_kernel(n),       out_shape=jax.ShapeDtypeStruct((n, n), jnp.int32),       grid=(n // block_row_size, n // block_col_size),       out_specs=pl.BlockSpec(           lambda i, j: (i, j), (block_row_size, block_col_size)       ),   )() n = 256 generate_sign_matrix_kernelized(256) ```  What jax/jaxlib version are you using? jax v0.4.24  Which accelerator(s) are you using? TPU  Additional system info? numpy version: 1.26.3, python version: 3.11.6, OS: gLinux (Debian 6.5.131rodete1)  NVIDIA GPU info _No response_",2024-01-10T19:07:34Z,bug pallas,open,0,0,https://github.com/jax-ml/jax/issues/19291
yi,pallas: improve indexing trace time,"The problem here is that for values with large trace contexts, constructing a `ConcretizationTypeError` can be expensive, which leads to slow kernel compilation. We could maybe improve this by lazily constructing this string in `ConcretizationTypeError`, but it derives from `TypeError` which expects the message to be available at instantiation. Rather than trying to hack a lazy version of `TypeError`, I'm opting to just avoid the problem at the source. I've searched the rest of the code for `except ConcretizationTypeError` and found a few other instances of this, none of which are good candidates for a similar fix.",2024-01-09T19:32:23Z,pull ready,closed,1,0,https://github.com/jax-ml/jax/issues/19272
yi,Some PRNG seeds causing zero gradients," Description Hey, I have a minimal example here of an MLP (using flax) training script that, for some seeds, doesn't see any convergence (loss is constant and gradients are all zero). I've tried digging deeper but can't seem to figure out why this is happening I've included a couple of bad seeds, in which the issue occurs, and some random seeds where I don't see it. Also, changing the second dimension of the input from 16 to 17 mitigates the issue (line 82, I left a comment there). ```python from typing import Any, Callable, Dict, List, Sequence, Tuple, Union import flax.linen as nn import jax import jax.numpy as jnp import numpy as np import optax from flax.training.train_state import TrainState from torch.utils.data import DataLoader, Dataset batch_size = 64 learning_rate = 0.0001 weight_decay = 0.0001  Bad seeds (results in gradients becoming 0) seed = 620  seed = 987  seed = 218  Good keys  seed = 252  seed = 846  seed = 189 class MultiLayerPerceptron(nn.Module):     mlp_hidden_layer_sizes: Sequence[int]     mlp_output_layer: int     def setup(self) > None:         self.hidden_layers = [nn.Dense(mlp_layer) for mlp_layer in self.mlp_hidden_layer_sizes]         self.output_layer = nn.Dense(self.mlp_output_layer)     def __call__(self, x: jnp.ndarray, *args: Any, **kwargs: Any) > jnp.ndarray:         for layer in self.hidden_layers:             x = nn.relu(layer(x))         x = nn.relu(self.output_layer(x))         return x class JaxDataset(Dataset):     """"""Pytorch Dataset wrapper for jax arrays""""""     def __init__(self, x: np.ndarray, y: np.ndarray) > None:         self.x = x         self.y = y     def __len__(self) > int:         return len(self.x)     def __getitem__(self, index: int) > Tuple[np.ndarray, np.ndarray]:         return self.x[index], self.y[index] def numpy_collate_fn(     batch: Union[Tuple[np.ndarray], List[np.ndarray], Tuple[Tuple], List[Tuple], Tuple[List], List[List]], ) > np.ndarray:     """"""Collate function used to make pytorch Dataloader return jaxcompatible np arrays""""""     if isinstance(batch[0], np.ndarray):         return np.stack(batch)     if isinstance(batch[0], (tuple, list)):         transposed = zip(*batch)         return [numpy_collate_fn(samples) for samples in transposed]     return np.array(batch) def sum_of_gradients(gradients):     sum_grads = 0     for k, v in gradients[""params""].items():         sum_grads += np.sum(v[""kernel""])         sum_grads += np.sum(v[""bias""])     return sum_grads def train_model():     rng = jax.random.PRNGKey(seed)     rng, key = jax.random.split(rng)     inputs = jnp.ones((22999, 16))  Changing 16>17 removes the issue     labels = jnp.ones((22999, 1))     train_dataset = JaxDataset(np.asarray(inputs), np.asarray(labels))     train_dataloader = DataLoader(         train_dataset,         batch_size=batch_size,         drop_last=False,         collate_fn=numpy_collate_fn,         num_workers=0,     )     network = MultiLayerPerceptron(mlp_hidden_layer_sizes=(64, 32), mlp_output_layer=1)     params = network.init(key, inputs)     state = TrainState.create(         apply_fn=network.apply,         params=params,         tx=optax.adamw(learning_rate=learning_rate, weight_decay=weight_decay),     )           def mse(params: Dict, X: jnp.ndarray, y: jnp.ndarray) > jnp.ndarray:         err = state.apply_fn(params, X)  y         return jnp.mean(jnp.square(err)) / 2     loss_grad_fn = jax.value_and_grad(mse)           def train_step(state: TrainState, batch_x: jnp.ndarray, batch_y: jnp.ndarray) > Tuple[TrainState, float]:         mse_loss, gradients = loss_grad_fn(state.params, batch_x, batch_y)         print(f""batch loss: {mse_loss}"")         print(f""Sum of gradients: {sum_of_gradients(gradients)}"")         state = state.apply_gradients(grads=gradients)         return state, mse_loss     for batch_x, batch_y in train_dataloader:         state, mse_loss = train_step(state, batch_x, batch_y) if __name__ == ""__main__"":     train_model() ```  What jax/jaxlib version are you using? jax==0.4.23, jaxlib==0.4.23+cuda11.cudnn86, jaxopt==0.8.1, flax==0.7.5  Which accelerator(s) are you using? GPU  Additional system info? Python 3.11.6, Ubuntu 20.04.6 LTS  NVIDIA GPU info ``` ++  ++++ ```",2024-01-09T14:42:03Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/19268,I think the problem is that you apply relu to the output of the last layer. For the given seeds and input sizes the output is just < 0 and thus there is no gradient.,"I think  is right – if you add `jax.debug.print(""Value of relu(x) is {x}"", x=x)` at the end of `MultiLayerPerceptron.__call__`, you can see that the result at each iteration is exactly zero.","Yep that's correct, thanks! It seems this was causing gradients to always be zero and no learning to occur. Removing the last relu or using leaky relu mitigated the issue"
yi,"Missing ""Memory profile"" in Tensorboard when using CPU"," Description Hello,  I am trying to profile a script on both the CPU and GPU using Tensorboard (following the instructions in the docs).  When profiling on GPU, I can access the 'Memory profile' tab of Tensorboard, but when I change the device to CPU with the following command:  ``` jax.config.update(""jax_platform_name"", ""cpu"") ``` I can no longer see the profile output. Instead, I see the following error message:  !image I am particularly interested in measuring the peak memory use of the program so I would also be interested in knowing about any workarounds for obtaining this information.   What jax/jaxlib version are you using? jax v.0.4.23, jaxlib v.0.4.23  Which accelerator(s) are you using? CPU/GPU  Additional system info? WSL 2   NVIDIA GPU info ``` ++  ++ ```",2024-01-09T14:00:15Z,bug,open,1,1,https://github.com/jax-ml/jax/issues/19266,"Since your environment includes a mix of WSL 2 and GPU, also ensure that there are no compatibility issues related to running JAX and Tensorboard within the WSL 2 environment."
transformer,Error writing Perfetto trace to GCS," Description I have a usecase where I'd like to collect a profiling trace without blocking, and save it somewhere nonemphemeral, like GCS, so that I can browse the trace later, even if the host VM is preempted. To avoid blocking, I am not creating a link to```ui.perfetto.dev```. Instead, I am saving the trace directly as described in the docs.  However, when I do ```jax.profiler.start_trace(log_dir=LOGDIR, create_perfetto_trace=True)```, there is an error when calling ```jax.profiler.stop_trace()```, if LOGDIR is a Google Cloud Storage URI. For example, if ```LOGDIR = ""gs://tpu_persist_bucket/mu_transformer/openwebtext_large/logging""```, the stacktrace is:  ``` I0109 10:21:57.647162 140667646041152 launch.py:423] Starting profiler trace...                                                                                                                                            I0109 10:22:35.699894 140667646041152 launch.py:400] {'step': 220, 'sec_per_step': 8.417642556499999, 'loss_avg': 4.786579132080078, 'val_loss_avg': 4.875473620148095}                                                    I0109 10:23:13.737155 140667646041152 launch.py:400] {'step': 240, 'sec_per_step': 1.901862946450001, 'loss_avg': 4.730668544769287, 'val_loss_avg': 4.875473620148095}                                                    I0109 10:23:51.766414 140667646041152 launch.py:400] {'step': 260, 'sec_per_step': 1.9014628015999961, 'loss_avg': 4.629239082336426, 'val_loss_avg': 4.875473620148095}                                                   I0109 10:24:29.795805 140667646041152 launch.py:400] {'step': 280, 'sec_per_step': 1.901469791149998, 'loss_avg': 4.605014801025391, 'val_loss_avg': 4.875473620148095}                                                    I0109 10:25:07.798005 140667646041152 launch.py:400] {'step': 300, 'sec_per_step': 1.9001099338000018, 'loss_avg': 4.600289344787598, 'val_loss_avg': 4.875473620148095}                                                   I0109 10:25:45.815146 140667646041152 launch.py:400] {'step': 320, 'sec_per_step': 1.9008569251500034, 'loss_avg': 4.5285468101501465, 'val_loss_avg': 4.875473620148095}                                                  I0109 10:26:23.859546 140667646041152 launch.py:400] {'step': 340, 'sec_per_step': 1.9022201665000011, 'loss_avg': 4.475864887237549, 'val_loss_avg': 4.875473620148095}                                                   I0109 10:27:01.878135 140667646041152 launch.py:400] {'step': 360, 'sec_per_step': 1.9009284916999944, 'loss_avg': 4.491670608520508, 'val_loss_avg': 4.875473620148095}                                                   I0109 10:27:39.880990 140667646041152 launch.py:400] {'step': 380, 'sec_per_step': 1.9001437830999977, 'loss_avg': 4.3752546310424805, 'val_loss_avg': 4.875473620148095}                                                                                                                                                                                                                                                 I0109 10:28:17.889149 140667646041152 launch.py:400] {'step': 400, 'sec_per_step': 1.900407919700001, 'loss_avg': 4.3445658683776855, 'val_loss_avg': 4.875473620148095}                                                   I0109 10:28:17.891910 140667646041152 launch.py:412] Stopping profiler trace...                                                                                                                                            Traceback (most recent call last):                                                                                                                                                                                           File ""/home/lucaslingle/mu_transformer/mu_transformer/launch.py"", line 586, in                                                                                                                                       app.run(main)                                                                                                                                                                                                            File ""/home/lucaslingle/.cache/pypoetry/virtualenvs/mutransformerRv8OjmyLpy3.9/lib/python3.9/sitepackages/absl/app.py"", line 308, in run                                                                                 _run_main(main, args)                                                                                                                                                                                                    File ""/home/lucaslingle/.cache/pypoetry/virtualenvs/mutransformerRv8OjmyLpy3.9/lib/python3.9/sitepackages/absl/app.py"", line 254, in _run_main                                                                           sys.exit(main(argv))                                                                                                                                                                                                     File ""/home/lucaslingle/mu_transformer/mu_transformer/launch.py"", line 574, in main                                                                                                                                          train_loop()                                                                                                                                                                                                             File ""/home/lucaslingle/mu_transformer/mu_transformer/launch.py"", line 413, in train_loop                                                                                                                                    jax.profiler.stop_trace()                                                                                                                                                                                                File ""/home/lucaslingle/.cache/pypoetry/virtualenvs/mutransformerRv8OjmyLpy3.9/lib/python3.9/sitepackages/jax/_src/profiler.py"", line 208, in stop_trace                                                                 abs_filename = _write_perfetto_trace_file(_profile_state.log_dir)                                                                                                                                                        File ""/home/lucaslingle/.cache/pypoetry/virtualenvs/mutransformerRv8OjmyLpy3.9/lib/python3.9/sitepackages/jax/_src/profiler.py"", line 140, in _write_perfetto_trace_file                                                 trace_folder in os.listdir(root_trace_folder)]                                                                                                                                                                         FileNotFoundError: [Errno 2] No such file or directory: '/home/lucaslingle/mu_transformer/gs:/tpu_persist_bucket/mu_transformer/openwebtext_large/logging/plugins/profile'   ``` Notably, the leading slash of the GCS URI is now missing, and a local path has been prepended, as if the provided logdir was a relative path. On the other hand, the nonperfetto traces are seemingly written without error to GCS when ```create_perfetto_trace=False```.  What jax/jaxlib version are you using? jax 0.4.23; jaxlib 0.4.23  Which accelerator(s) are you using? TPU v38  Additional system info? Numpy version: 1.23.5. Python version: 3.9.5 (default, Nov 23 2021, 15:27:38)  [GCC 9.3.0]. Platform uname: uname_result(system='Linux', node='t1vne84c95a6w0', release='5.13.01027gcp', version=' CC(Fix the bug in classifier example, batching_test and README)~20.04.1Ubuntu SMP Thu May 26 10:53:08 UTC 2022', machine='x86_64')  NVIDIA GPU info _No response_",2024-01-09T10:50:26Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/19263,Closing this. I seem to be able to get a perfettoreadable trace written to GCS even when ```create_perfetto_trace=False```. 
yi,Tracking issue: NumPy 2.0 Compatibility,"NumPy 2.0 is coming, and there are a number of things we need to do to ensure a smooth transition for users. This issue tracks these TODOs. Relevant NumPy issue is here: https://github.com/numpy/numpy/issues/24300. This has some overlap with CC(Tracking issue: support Array API), as NumPy is aiming for array API compatibility in v2.0.  [x] add an upstream nightly build (kokoro only, because of jaxlib build requirement)  [x] fix `np.ComplexWarning` references ( CC(Remove reference to np.ComplexWarning))  [x] new `sign` convention for complex entries ( CC(jnp.sign: use x/abs(x) for complex arguments))  [x] fix `jax.scipy.special.logsumexp` for new sign convention ( CC(logsumexp: use NumPy 2.0 convention for complex sign))  [x] New shape for `inverse_indices` in `jnp.unique` ( CC(jnp.unique: make return_inverse shape match NumPy 2.0))  [x] add new `jax.numpy` functions:    [x] `concat` CC([array api] add jax.numpy.concat)    [x] `isdtype` CC(Add jnp.isdtype function, following np.isdtype in NumPy 2.0)    [x]  `permute_dims` CC([array api] add jax.numpy.permute_dims function)    [x] `bitwise_invert` CC(array api: add jnp.bitwise_* aliases)    [x] `bitwise_left_shift` CC(array api: add jnp.bitwise_* aliases)    [x] `bitwise_right_shift` CC(array api: add jnp.bitwise_* aliases)    [x] `pow` CC([array API] implement jnp.pow; alias for jnp.power)    [x] `vecdot` CC(Add jnp.vecdot) Fix some relevant bugs in NumPy & SciPy:  [x] https://github.com/numpy/numpy/issues/25553  [x] https://github.com/numpy/numpy/pull/25554  [x] https://github.com/numpy/numpy/pull/25560  [x] https://github.com/scipy/scipy/pull/19870 Once there is a NumPy 2.0 release candidate, we should do an `ml_dtypes` and `jaxlib` release built against that candidate; these should be compatible with numpy versions back to our minimum supported version.",2024-01-08T18:36:24Z,enhancement,closed,0,2,https://github.com/jax-ml/jax/issues/19246,"We're in good shape here; the only failures as of today are fft bugs upstream (https://github.com/numpy/numpy/issues/25661 and https://github.com/numpy/numpy/issues/25679). These are being fixed in https://github.com/numpy/numpy/pull/25668 Beyond that, our main TODO is to update our builds to use the numpy 2.0 ABI once the release candidates are out.","I think we can declare this fixed. We've made a release with NumPy 2.0, which was the last main TODO."
yi,Pallas flash attention `causal` flag has no effect ," Description I am trying to use the pallas tpu flash_attention. It seems like setting the causal flag is not changing the output. I tested with the following code. ``` import jax from jax.experimental.pallas.ops.tpu import flash_attention bs = 2 seqlen = 1024 n_heads = 128 dim = 512 rng = jax.random.PRNGKey(0) xq = jax.random.normal(rng, (bs, n_heads, seqlen, dim)) xk = jax.random.normal(rng, (bs, n_heads, seqlen, dim)) xv = jax.random.normal(rng, (bs, n_heads, seqlen, dim)) print('causal = False') print(flash_attention.flash_attention(xq, xk, xv, None, causal=False)[0,0,:4,:4]) print('causal = True') print(flash_attention.flash_attention(xq, xk, xv, None, causal=True)[0,0,:4,:4]) ``` It gives the following output: ```output causal = False [[ 0.515625    0.99609375  0.34179688  0.52734375]  [0.12060547  1.25       0.49023438  0.828125  ]  [ 0.59765625  0.83203125  0.01226807 1.015625  ]  [0.14257812 0.51171875  1.171875    0.8671875 ]] causal = True [[ 0.515625    0.99609375  0.34179688  0.52734375]  [0.12060547  1.25       0.49023438  0.828125  ]  [ 0.59765625  0.83203125  0.01226807 1.015625  ]  [0.14257812 0.51171875  1.171875    0.8671875 ]] ```  What jax/jaxlib version are you using? 0.4.21 0.4.21  Which accelerator(s) are you using? tpuv38  Additional system info? 1.26.2 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] uname_result(system='Linux', node='t1vn5b478b6dw0', release='5.19.01022gcp', version=' CC(attempt to centerjustify the jax logo in readme)~22.04.1Ubuntu SMP Sun Apr 23 09:51:08 UTC 2023', machine='x86_64')  NVIDIA GPU info _No response_",2024-01-08T06:57:47Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/19237,Closing as it seems to be a precision caused artifact.
yi,Add CUDA Array Interface consumer support,"This PR adds CUDA Array Interface (versions 2 and 3) consumer support to JAX. In addition, the PR enables constructing JAX arrays from objects that implement dlpack provider support. Fixes CC(Support __cuda_array_interface__ on GPU )  Requires https://github.com/openxla/xla/pull/8237",2024-01-07T17:33:06Z,cla: yes pull ready NVIDIA GPU,closed,0,15,https://github.com/jax-ml/jax/issues/19233,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Thanks for the contribution – it's not clear to me that `asarray` and `array` should transparently handle `dlpack` and other data interchange formats. For example, the Python Array API standard explicitly decided not to do this (see https://github.com/dataapis/arrayapi/pull/301 and linked issues). This suggests that we should stick with explicit data interchange functions like `jnp.from_dlpack`. What do you think?","> This suggests that we should stick with explicit data interchange functions like `jnp.from_dlpack`. What do you think?  Yes, I agree. I'll update the PR accordingly.","Similar to `from_dlpack`, I wonder if an explicit `from_cuda_interface` function would better achieve the goal here?","> Similar to `from_dlpack`, I wonder if an explicit `from_cuda_interface` function would better achieve the goal here? While arrayapi does not mention CUDA Array Interface nor NumPy Array Interface, these are kind of legacy buffer protocols and objects implementing the Buffer Protocol are allowed inputs to `asarray` according to the arrayapi. Here's a summary how the objects implementing CUDA Array Interface protocol are resolved elsewhere:  cupy resolves `__cuda_array_interface__` in `array` (`asarray` typically calls `array`). Btw, it has internal method `_array_from_cuda_array_interface` for that.  PyTorch resolves `__cuda_array_interface__` in `tensor` (analogue of `array`) and in `as_tensor` (analogue of `asarray`) and in `asarray`.  mpi4py resolves `__cuda_array_interface__` in `frombuffer` and `getbuffer`. Considering the above, my first choice would be to resolve CAI objects in the `array` function (as in this PR). However, the explicit `from_cai` or similar also makes sense as this is about importing a view of an existing buffer similar to `from_dlpack`. What do you think?","That sounds good, we can do it in `asarray`. Though it introduces some complexities, e.g. should lists of cudacompatible objects be treated as lists of arrays? That's possible currently with our handling of buffer protocol objects.","> Though it introduces some complexities, e.g. should lists of cudacompatible objects be treated as lists of arrays? That's possible currently with our handling of buffer protocol objects. Currently, jax, numpy, and cupy implement this using the following principle: if `input` to `asarray` is a native array object then `asarray([input])` is an array with `ndim` larger than `input.ndim`. (PyTorch does not support list of tensors as an input to `asarray`). Your concern corresponds to the case where `input` is not a native array object, say, it is anything that implements one of the data exchange protocols. Currently, all above mentioned libraries will raise an exception on `asarray([input])`:  numpy raises `TypeError` with a FutureWarning:   ```   The input object of type 'W' is an arraylike implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0D).\   In the future, this object will be coerced as if it was first converted using `np.array(obj) ...   ```  cupy raises `ValueError: Unsupported dtype object`  jax raises `TypeError: Cannot interpret '' as a data type` Considering the numpy FutureWarning, the answer might be affirmative, but notice that https://dataapis.org/arrayapi/latest/API_specification/generated/array_api.asarray.htmlarray_api.asarray does not support this nor does it support the current behavior where input list items are native array objects.","Thanks  before we proceed with the review, can you sign the CLA?","> Similar to `from_dlpack`, I wonder if an explicit `from_cuda_interface` function would better achieve the goal here? I'm not 100% sure about this one. DLPack should provide a superset of functionality of `__cuda_array_interface__` (and for CUDA specifically, the stream support in DLPack was improved based on the lessons of `__cuda_array_interface__`, so I'd consider the latter legacy and nudge any remaining libraries that don't have DLPack support that's on par with their `__cuda_array_interface__` implementation to fix that.  There's something to be said for this either way of course, given that not all libraries today have support that's fully equivalent between the two protocols. But there isn't too much traffic on gh1100 in 3+ years. And we have too many different protocols in Python that are overlapping  with DLPack being the most viable one longterm, as the only one that has multidevice support. `__cuda_array_interface__` doesn't even seem to support ROCm, so unless I'm missing something, it may not be needed longterm? Keeping its support inside another function (or multiple) makes it easier to evolve / phase out.","Circling back here: after chatting with  a bit, I think this is probably the right approach. We'll have to wait on https://github.com/openxla/xla/pull/8237 before we can do anything here, but once that's in let's plan to proceed with this review.","OK, looks good! Last thing: could you please squash the changes into a single commit? Thanks!","> OK, looks good! Last thing: could you please squash the changes into a single commit? Thanks! Done.","I guess before merging this PR, https://github.com/openxla/xla/pull/8237 needs to land soon.", I think we can move forward with this PR as https://github.com/openxla/xla/pull/8237 has landed,Sorry for the delay in mergind: we're running into some test failures when testing this against PJRT runtimes; I'm trying to debug internally.
gpt,[Pallas] Error Lowering to Triton," Description I'm attempting to test out taking gradients in a Pallas Kernel. Related to discussion 19184. My Pallas installation follows CC(Make Pallas/GPU easier to install). Here's my playground code: ``` from functools import partial import jax from jax.experimental import pallas as pl import jax.numpy as jnp import numpy as np def mlp(x, w1, w2, y):   hidden1 = jnp.dot(x, w1)   hidden2 = jax.nn.relu(hidden1)   output = jnp.dot(hidden2, w2)   loss = jnp.mean(jnp.square(output  y))   return loss def gradient_kernel(x_ref, w1_ref, w2_ref, y_ref, output_ref):   x, w1, w2, y = x_ref[...], w1_ref[...], w2_ref[...], y_ref[...]   grad_mlp = jax.grad(mlp, argnums=(1))   grad_w1 = grad_mlp(x, w1, w2, y)   output_ref[...] = grad_w1 .jit def gradient_kernel_test(x: jax.Array, w1: jax.Array, w2: jax.Array, y: jax.Array) > jax.Array:   return pl.pallas_call(gradient_kernel,                         out_shape=jax.ShapeDtypeStruct(w1.shape, w1.dtype)                         )(x, w1, w2, y) key = jax.random.PRNGKey(0) x = jnp.arange(5).astype(float) w1 = jax.random.normal(key, shape=(5,5)) w2 = jax.random.normal(key, shape=(5,5)) y = jax.random.normal(key, shape=(5,))  Use Pallas kernel or standard JAX. use_kernel = True if use_kernel:   grad_w1 = gradient_kernel_test(x, w1, w2, y) else:   grad_mlp = jax.grad(mlp, argnums=(1))   grad_w1 = grad_mlp(x, w1, w2, y) print(grad_w1) ``` When I use standard JAX, I can successfully get the gradient with respect to _w1_. But when I use the Pallas Kernel, I get the following error: ``` The above exception was the direct cause of the following exception: jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/juice5/scr5/yusun/data/karan/tttgpt/kernels/pallas_playground.py"", line 38, in      grad_w1 = gradient_kernel_test(x, w1, w2, y)   File ""/nlp/scr/yusun/miniconda3/envs/pallas/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py"", line 1621, in pallas_call_lowering     compilation_result = compile_jaxpr(   File ""/nlp/scr/yusun/miniconda3/envs/pallas/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py"", line 1564, in compile_jaxpr     lowering_result = lower_jaxpr_to_triton_module(   File ""/nlp/scr/yusun/miniconda3/envs/pallas/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py"", line 280, in lower_jaxpr_to_triton_module     () = lower_jaxpr_to_triton_ir(ctx, jaxpr, block_infos, *args)   File ""/nlp/scr/yusun/miniconda3/envs/pallas/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py"", line 339, in lower_jaxpr_to_triton_ir     raise TritonLoweringException( jax._src.pallas.triton.lowering.TritonLoweringException: Exception while lowering eqn:   a:f32[5] = dot_general[   dimension_numbers=(([0], [0]), ([], []))   preferred_element_type=float32 ] b c With context:   TritonLoweringRuleContext(context=TritonModuleContext(name='gradient_kernel', ir_context=, builder=, module=, grid_mapping=GridMapping(grid=(), block_mappings=(None, None, None, None, None), mapped_dims=(), num_index_operands=0, num_scratch_operands=0), program_ids=[]), avals_in=[ShapedArray(float32[5]), ShapedArray(float32[5,5])], avals_out=[ShapedArray(float32[5])], block_infos=[None, None]) With inval shapes=[[constexpr[5]], [constexpr[5], constexpr[5]]] With inval types=[, ] In jaxpr: { lambda ; a:Ref{float32[5]} b:Ref{float32[5,5]} c:Ref{float32[5,5]} d:Ref{float32[5]}     e:Ref{float32[5,5]}. let     f:f32[5] .memoized at 0x7f6ee0094ca0>       num_consts=0       symbolic_zeros=False     ] j     p:bool[5] = gt j 0.0     q:f32[5] = dot_general[       dimension_numbers=(([0], [0]), ([], []))       preferred_element_type=float32     ] k h     r:f32[5] = sub q i     s:f32[5] = integer_pow[y=1] r     t:f32[5] = mul 2.0 s     u:f32[] = div 1.0 5.0     v:f32[5] = broadcast_in_dim[broadcast_dimensions=() shape=(5,)] u     w:f32[5] = mul v t     x:f32[5] = dot_general[       dimension_numbers=(([0], [1]), ([], []))       preferred_element_type=float32     ] w h     y:f32[5] = broadcast_in_dim[broadcast_dimensions=() shape=(5,)] 0.0     z:bool[5] = eq p True     ba:f32[5] = select_n z y x     bb:f32[5,5] = dot_general[       dimension_numbers=(([], []), ([], []))       preferred_element_type=float32     ] ba f     bc:f32[5,5] = transpose[permutation=(1, 0)] bb     e[:,:] ",2024-01-05T19:16:53Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/19222,"Could you try rerunning this with the latest jaxlib version (0.4.25)? Pallas should now do a slightly better job at reporting lowering errors on GPU. My guess is that the problem is that the issue is that block dimensions are not powers of 2, but you should get an error message saying that.",I will close the issue for now. Please reopen if you are able to reproduce with jaxlib 0.4.25+.
rag,XLA_PYTHON_CLIENT_PREALLOCATE=false is not honored anymore," Description Reproducer: ```python import os os.environ[""XLA_PYTHON_CLIENT_PREALLOCATE""] = ""false"" import jax arr = jax.numpy.array([1]) + 1 arr.block_until_ready() print(arr.devices()) os.system(""nvidiasmi"") ``` Output with recent JAX: ```bash  oops, preallocates anyway $ python jaxbug.py {cuda(id=0)} Fri Jan  5 12:37:36 2024        ++  ++ ``` Working JAX version: ```bash $ pip install f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html \     ""jax[cuda11_local]==0.4.20"" ``` Not working for JAX>=0.4.21  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? GPU  Additional system info? Python 3.9, Ubuntu  NVIDIA GPU info _No response_",2024-01-05T12:37:03Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/19213,Sorry for the breakage. This is already fixed (see https://github.com/google/jax/issues/19035) but needs a new release.,"That release has happened by now, so we should be good to close this."
yi,ImportError: cannot import name 'index_update' from 'jax.ops' (/home/home/.local/lib/python3.8/site-packages/jax/ops/__init__.py),"Please:  [ ] Check for duplicate requests.  [ ] Describe your goal, and if possible provide a code snippet with a motivating example. Hi. I'm trying to run PRIEST, and the following link is about priest package https://github.com/fatemehrastgar/PRIEST The problem I'm having is I tried to run ""rosrun priest planner_holonomic.py"", but this error is happening homeB760MDS3HDDR4:~/priest_ws/src$ rosrun priest planner_holonomic.py Traceback (most recent call last):   File ""/home/home/priest_ws/src/PRIEST/priest/src/planner_holonomic.py"", line 21, in      import mpc_expert   File ""/home/home/priest_ws/src/PRIEST/priest/src/mpc_expert.py"", line 9, in      from jax.ops import index_update, index ImportError: cannot import name 'index_update' from 'jax.ops' (/home/home/.local/lib/python3.8/sitepackages/jax/ops/__init__.py) I guess it's a problem with version of jax and jaxlib, but to be honest due to my lacking skills, I have no idea which version should I have to use to run this package. What should I do to run this package?? Best regards :)",2024-01-05T08:25:29Z,question,closed,0,2,https://github.com/jax-ml/jax/issues/19211,"`jax.ops.index_update` was deprecated in JAX v0.2.22, and removed in JAX v0.3.2. Instead of `jax.ops.index_update(x, i, y)`, you can use `x.at[i].set(y)`. If you are using another package that calls `index_update` so that you're unable to change the callsite, then you'll have to use `jax` version 0.3.1 or older: ``` pip install jax[cpu]==0.3.1 f https://storage.googleapis.com/jaxreleases/jax_releases.html ``` You can find more information about installing older jaxlib versions at https://jax.readthedocs.io/en/latest/installation.htmlinstallingolderjaxlibwheels.",It looks like this has been resolved  thanks!
yi,"[XLA:Python] Raise an AttributeError if __cuda_array_interface__ is called on various invalid buffers, rather than a RuntimeError.","[XLA:Python] Raise an AttributeError if __cuda_array_interface__ is called on various invalid buffers, rather than a RuntimeError. This makes hasattr(x, ""__cuda_array_interface__"") fail gracefully. In passing, also move the implementation into py_array.cc, and use an allowlist of supported types rather than a denylist. Fixes https://github.com/google/jax/issues/19134",2024-01-04T18:32:34Z,,closed,0,0,https://github.com/jax-ml/jax/issues/19199
llm,Need sequence parallel to improve compute strength and save HBM space when LLM training.,"Feature sequence parallel has already support by Megatron and DeepSpeed for a long time. Input samples are not only separated at batch dim, but also at sequence length dim when using tensor parallelism. Paper:https://arxiv.org/pdf/2105.13120.pdf  Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example.",2024-01-04T16:21:32Z,enhancement,open,0,0,https://github.com/jax-ml/jax/issues/19195
yi,jnp.ndarray.item(): add args support,"Addresses https://github.com/google/jax/pull/19181issuecomment1875833407 This implements the optional index arguments to `jnp.ndarray.item`, which previously were not implemented in JAX. It also makes `arr.item()` continue to work correctly independent of the deprecation in CC(Error for deprecated scalar conversions of nonscalar arrays).",2024-01-03T20:14:34Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/19182
yi,Add the githash that the jaxlib was built at to __init__.py. This is to allow identifying the githash of nightlies.,Add the githash that the jaxlib was built at to __init__.py. This is to allow identifying the githash of nightlies.,2024-01-02T20:56:09Z,,closed,0,0,https://github.com/jax-ml/jax/issues/19173
