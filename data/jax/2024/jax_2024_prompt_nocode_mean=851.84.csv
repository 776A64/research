189,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Title)ï¼Œ å†…å®¹æ˜¯ (Body)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",key,Title,Body,Created At,Tags,State,Reactions,Comments_count,Link,Comments
516,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Return arrays from `ArrayImpl._check_and_rearrange`.)ï¼Œ å†…å®¹æ˜¯ (Return arrays from `ArrayImpl._check_and_rearrange`. This is in preparation for a larger change, so that input buffers can be checked before Array creation in XLA and the user gets more helpful JAX error messages instead of XLA errors. Reverts 3b2410f77cdb0acc6951e1770c1229e6689b7409)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Return arrays from `ArrayImpl._check_and_rearrange`.,"Return arrays from `ArrayImpl._check_and_rearrange`. This is in preparation for a larger change, so that input buffers can be checked before Array creation in XLA and the user gets more helpful JAX error messages instead of XLA errors. Reverts 3b2410f77cdb0acc6951e1770c1229e6689b7409",2025-02-04T21:43:45Z,,open,0,0,https://github.com/jax-ml/jax/issues/26316
1357,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jax.distributed with slurm/mpi not working on rocm)ï¼Œ å†…å®¹æ˜¯ (on the rocmplatfrom, `jax.distributed` with the clusterautodetection mechanisms is not working as it is with cuda. Whereas in `jax._src.distributed.initialize()` the visible_devices are set properly for both 'cuda' and 'rocm', in the xla_bridge, only the cuda_visible_devices are queried upon client creation: https://github.com/jaxml/jax/blob/124e123946d09661b6b28dcd82a618be93cb132c/jax/_src/xla_bridge.pyL643 This results in nodelocal processes seeing all local devices which can lead to OOM errors in case of oversubscription or hangs or errors in rccl communication, depending on the specific setup. Thus atm only settings with a single process per node are possible with rocm, whereas with cuda also one process per device works. To my understanding and tests this could be easily fixed by querying also the 'jax_rocm_visible_devices' in xla_bridge and I would be happy to provide a PR in case the current behavior is not the intended one? According to a small set of simple tests, also the whole gpumock setup that is enclosed in the relevant context linked above works with rocm, so the changes might really be minimal.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jax.distributed with slurm/mpi not working on rocm,"on the rocmplatfrom, `jax.distributed` with the clusterautodetection mechanisms is not working as it is with cuda. Whereas in `jax._src.distributed.initialize()` the visible_devices are set properly for both 'cuda' and 'rocm', in the xla_bridge, only the cuda_visible_devices are queried upon client creation: https://github.com/jaxml/jax/blob/124e123946d09661b6b28dcd82a618be93cb132c/jax/_src/xla_bridge.pyL643 This results in nodelocal processes seeing all local devices which can lead to OOM errors in case of oversubscription or hangs or errors in rccl communication, depending on the specific setup. Thus atm only settings with a single process per node are possible with rocm, whereas with cuda also one process per device works. To my understanding and tests this could be easily fixed by querying also the 'jax_rocm_visible_devices' in xla_bridge and I would be happy to provide a PR in case the current behavior is not the intended one? According to a small set of simple tests, also the whole gpumock setup that is enclosed in the relevant context linked above works with rocm, so the changes might really be minimal.",2025-02-04T08:48:00Z,AMD GPU,open,0,2,https://github.com/jax-ml/jax/issues/26298,"Looking over the block of code there that is setting up the mock devices settings, yeah, I don't think this would be too big of change to make. I think it comes down to changing the block to also run if the plugin is rocm, and checking _ROCM_VISIBLE_DEVICES instead of CUDA_VISIBLE_DEVICES and then running through the same mock settings block. I am not sure if anyone has tried running one process per device yet with jax+rocm, but I don't really see any reason it wouldn't work. I don't really have a slurm/mpi cluster on hand to easily test this, so if you wanted to build the change and verify it on your environment and push it up as a PR that would be helpful. Thanks!",Thanks for looking into this issue! Please find the changes in the PR mentioned above. I tested with A100 and MI300 with slurm and rocm is now consistent with cuda wrt devices being visible to processes.
941,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([JAX] Add a test verifying the behavior of module-level state accessed by colocated Python)ï¼Œ å†…å®¹æ˜¯ ([JAX] Add a test verifying the behavior of modulelevel state accessed by colocated Python A new test verifies that * Python modulelevel variables can be created/set and read from a colocated Python function * Python modulelevel variables are not pickled on the controller (JAX) or sent to executors via pickling An API for defining userdefined state and accessing it from multiple colocated Python functions (i.e., object support) will be added later. That will be a recommended way to express userdefined state. The capability of accessing Python module variables is still crucial because a lot of Python code (including JAX) requires this behavior to implement caching.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[JAX] Add a test verifying the behavior of module-level state accessed by colocated Python,"[JAX] Add a test verifying the behavior of modulelevel state accessed by colocated Python A new test verifies that * Python modulelevel variables can be created/set and read from a colocated Python function * Python modulelevel variables are not pickled on the controller (JAX) or sent to executors via pickling An API for defining userdefined state and accessing it from multiple colocated Python functions (i.e., object support) will be added later. That will be a recommended way to express userdefined state. The capability of accessing Python module variables is still crucial because a lot of Python code (including JAX) requires this behavior to implement caching.",2025-02-04T03:56:29Z,,open,0,0,https://github.com/jax-ml/jax/issues/26293
429,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([JAX][DOC] memory_kind, with_memory_kind and out_shardings)ï¼Œ å†…å®¹æ˜¯ (Add documentation for the `memory_kind` parameter in sharding, the `with_memory_kind` method, and the `out_shardings` parameter for specifying output sharding in the {func}`jax.jit` function.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"[JAX][DOC] memory_kind, with_memory_kind and out_shardings","Add documentation for the `memory_kind` parameter in sharding, the `with_memory_kind` method, and the `out_shardings` parameter for specifying output sharding in the {func}`jax.jit` function.",2025-02-03T23:22:35Z,,open,0,0,https://github.com/jax-ml/jax/issues/26285
490,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TSAN race in //tests:export_back_compat_test_cpu under free-threading)ï¼Œ å†…å®¹æ˜¯ ( Description This 313t freethreading CI run: https://github.com/jaxml/jax/actions/runs/13111654401/job/36576699494?pr=26261 contains this race in MLIR code:  PTAL?  System info (python version, jaxlib version, accelerator, etc.) Python 3.13t)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,TSAN race in //tests:export_back_compat_test_cpu under free-threading," Description This 313t freethreading CI run: https://github.com/jaxml/jax/actions/runs/13111654401/job/36576699494?pr=26261 contains this race in MLIR code:  PTAL?  System info (python version, jaxlib version, accelerator, etc.) Python 3.13t",2025-02-03T15:27:53Z,bug free threading,open,0,2,https://github.com/jax-ml/jax/issues/26272,Another similar race report   Click    To reproduce: ,"A perhaps silly question: when we compile the MLIR C++ code, do we compile it with threading enabled? I seem to recall some locking there is only enabled if you build with threading enabled. Perhaps also we need to enable threading on our MLIR contexts? IIRC right now we disable it because we don't want MLIR to create thread pools, but that's orthogonal to ""is thread safe""."
968,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Build fails due to missing visibility declaration for hipfft target)ï¼Œ å†…å®¹æ˜¯ ( Description  Description When building JAX with ROCm support, the build fails due to a visibility issue with the hipfft target.  Error Message   Environment  ROCm Version: 6.2.411340  GPU: AMD Radeon Pro W7900 (gfx1100)  OS: Arch Linux  Build Command:     Solution The hipfft target in `/external/local_config_rocm/rocm/BUILD` needs a public visibility declaration. Other similar targets in the same file (hipblas, hiprand, etc.) already have this visibility set, but it seems to have been overlooked for hipfft. The fix is to add `visibility = [""//visibility:public""]` to the hipfft target:  After applying this change, the build proceeded successfully.  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Build fails due to missing visibility declaration for hipfft target," Description  Description When building JAX with ROCm support, the build fails due to a visibility issue with the hipfft target.  Error Message   Environment  ROCm Version: 6.2.411340  GPU: AMD Radeon Pro W7900 (gfx1100)  OS: Arch Linux  Build Command:     Solution The hipfft target in `/external/local_config_rocm/rocm/BUILD` needs a public visibility declaration. Other similar targets in the same file (hipblas, hiprand, etc.) already have this visibility set, but it seems to have been overlooked for hipfft. The fix is to add `visibility = [""//visibility:public""]` to the hipfft target:  After applying this change, the build proceeded successfully.  System info (python version, jaxlib version, accelerator, etc.) ",2025-02-03T13:27:51Z,bug build AMD GPU,open,0,3,https://github.com/jax-ml/jax/issues/26269,"If you know the fix, send a PR? In this case, it probably needs to be sent to the openxla/xla project, I think.","Let me take a look. On Mon, Feb 3, 2025 at 9:13â€¯AM Peter Hawkins ***@***.***> wrote: > If you know the fix, send a PR? In this case, it probably needs to be sent > to the openxla/xla project, I think. > > â€” > Reply to this email directly, view it on GitHub > , or > unsubscribe >  > . > You are receiving this because you were assigned.Message ID: > ***@***.***> >",I think this might have already been fixed in xla and jax just hasn't picked up the latest. see: https://github.com/openxla/xla/commit/991a2289dc4a8750f0009e5f2a87507407523964
1506,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Grouped convolutions are much slower than standard convolutions when increasing kernel dilation)ï¼Œ å†…å®¹æ˜¯ ( Description Iâ€™ve been experimenting with depthwise grouped convolutions in JAX, and noticed that as I increase the kernel dilation rate, they become much slower than standard convolutions, even though they are performing fewer mathematical operations. I observed this on several different GPUs, including T4, P100 and H100. The following example demonstrates this issue using 1D convolutions across different kernel sizes and channel sizes. I tried 3 different methods to implement depthwise convolutions: 1. Use `lax.conv_general_dilated(â€¦, feature_group_count=num_channels)`. 2. Use `vmap` to map `lax.conv_general_dilated(â€¦, feature_group_count=1)` across the `out_channels` dim of both the input and the kernel. 3. Loop across slices of the `out_channels` dim of both the input and the kernel, then call `lax.conv_general_dilated(â€¦, feature_group_count=1)` on these slices and concatenate the results.  !Image Some observations: * With method 1 (`feature_group_count=num_channels`), depthwise convolutions are often faster than standard convolutions for small dilation rates, but their performance scales poorly as we increase the dilation rate. * Method 2 (`vmap`) has the same performance as method 1 â€” inspecting the `jaxpr)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Grouped convolutions are much slower than standard convolutions when increasing kernel dilation," Description Iâ€™ve been experimenting with depthwise grouped convolutions in JAX, and noticed that as I increase the kernel dilation rate, they become much slower than standard convolutions, even though they are performing fewer mathematical operations. I observed this on several different GPUs, including T4, P100 and H100. The following example demonstrates this issue using 1D convolutions across different kernel sizes and channel sizes. I tried 3 different methods to implement depthwise convolutions: 1. Use `lax.conv_general_dilated(â€¦, feature_group_count=num_channels)`. 2. Use `vmap` to map `lax.conv_general_dilated(â€¦, feature_group_count=1)` across the `out_channels` dim of both the input and the kernel. 3. Loop across slices of the `out_channels` dim of both the input and the kernel, then call `lax.conv_general_dilated(â€¦, feature_group_count=1)` on these slices and concatenate the results.  !Image Some observations: * With method 1 (`feature_group_count=num_channels`), depthwise convolutions are often faster than standard convolutions for small dilation rates, but their performance scales poorly as we increase the dilation rate. * Method 2 (`vmap`) has the same performance as method 1 â€” inspecting the `jaxpr",2025-02-02T20:19:23Z,bug performance NVIDIA GPU,open,0,1,https://github.com/jax-ml/jax/issues/26266,"I came up with yet another implementation for depthwise convolutions, which just expands the depthwise kernel into a standard kernel and then runs a standard convolution:  I verified that this produces the same result as the other depthwise methods described above, and it has the same performance as standard convolutions. This still feels less than satisfactory since we don't reap the performance benefits of depthwise convolutions."
1438,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Support Bounded Shape Input)ï¼Œ å†…å®¹æ˜¯ (JAX currently supports dynamic shape input through padding or bucket padding. However, this approach isn't always ideal.  For example, some models require complex logic within their structure to manage padding.  Consider the Pixtral encoder as an example. Padding input image shapes to minimize recompilation requires additional logic for position embedding and masks, as these are dependent on the input shapes. This padding logic isn't immediately obvious to users. What's more, different models need different padding logic, that is really a large challenge to make jax/xla support such models. This feature would allow the compiler to handle padding internally, simplifying user code and improving the overall experience.  This enhancement requires work on both the JAX framework and the XLA compiler.  Discussions with the XLA team have confirmed that XLA already supports this capability. In the example below, xla compiler needs additional scalar input(s) to know the actual shape of the `param`.  Enabling XLA's bounded shape capabilities within JAX would be highly beneficial. One possible solution could involve the following steps: 1. Augment the jax.Array class with a new attribute to store the actual shape of the arr)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Support Bounded Shape Input,"JAX currently supports dynamic shape input through padding or bucket padding. However, this approach isn't always ideal.  For example, some models require complex logic within their structure to manage padding.  Consider the Pixtral encoder as an example. Padding input image shapes to minimize recompilation requires additional logic for position embedding and masks, as these are dependent on the input shapes. This padding logic isn't immediately obvious to users. What's more, different models need different padding logic, that is really a large challenge to make jax/xla support such models. This feature would allow the compiler to handle padding internally, simplifying user code and improving the overall experience.  This enhancement requires work on both the JAX framework and the XLA compiler.  Discussions with the XLA team have confirmed that XLA already supports this capability. In the example below, xla compiler needs additional scalar input(s) to know the actual shape of the `param`.  Enabling XLA's bounded shape capabilities within JAX would be highly beneficial. One possible solution could involve the following steps: 1. Augment the jax.Array class with a new attribute to store the actual shape of the arr",2025-02-02T18:51:24Z,enhancement,open,0,0,https://github.com/jax-ml/jax/issues/26265
1498,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Out-of-memory error when processing large images with overlapping patch-based gradients)ï¼Œ å†…å®¹æ˜¯ ( Description Hello! I am encountering an OutofMemory (OOM) issue when performing patchbased image processing using JAX. The core idea is to process an image by patches, applying a Fourier transform and a learned kernel in the frequency domain. Due to overlapping patches, I need to compute gradients over the entire image rather than individual patches. To manage memory, I initially attempted a `fori_loop` to iterate over patches instead of processing them using vmap. However, even with this approach, I experience an OOM error, despite the gradient computation being applied to only a subset of the patches.  Code Overview The patch processing is implemented in the `PatchTransformModule`, which: 1. Extracts patches from the image. 2. Applies a transformation function (in this case, Fourierbased filtering, simplified version of what I use in my codebase). 3. Accumulates the transformed patches back into an output image. 4. Supports computing gradients for a subset of patches, controlled by `grad_fraction` or `grad_num_patches`. Fully working code (if you have a lot of memory on your machine you can lower the stride to be sure to have OOM): PS: if the code do the execution with any size without the backward pass, I do)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Out-of-memory error when processing large images with overlapping patch-based gradients," Description Hello! I am encountering an OutofMemory (OOM) issue when performing patchbased image processing using JAX. The core idea is to process an image by patches, applying a Fourier transform and a learned kernel in the frequency domain. Due to overlapping patches, I need to compute gradients over the entire image rather than individual patches. To manage memory, I initially attempted a `fori_loop` to iterate over patches instead of processing them using vmap. However, even with this approach, I experience an OOM error, despite the gradient computation being applied to only a subset of the patches.  Code Overview The patch processing is implemented in the `PatchTransformModule`, which: 1. Extracts patches from the image. 2. Applies a transformation function (in this case, Fourierbased filtering, simplified version of what I use in my codebase). 3. Accumulates the transformed patches back into an output image. 4. Supports computing gradients for a subset of patches, controlled by `grad_fraction` or `grad_num_patches`. Fully working code (if you have a lot of memory on your machine you can lower the stride to be sure to have OOM): PS: if the code do the execution with any size without the backward pass, I do",2025-02-01T22:07:50Z,bug type:support,closed,0,0,https://github.com/jax-ml/jax/issues/26263
453,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add `num_updates` dimension to `input/output_offset` and `send/recv_sizes` of `ragged_all_to_all`.)ï¼Œ å†…å®¹æ˜¯ (Add `num_updates` dimension to `input/output_offset` and `send/recv_sizes` of `ragged_all_to_all`. For now, only verify args since it isn't supported by the backends just yet.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Add `num_updates` dimension to `input/output_offset` and `send/recv_sizes` of `ragged_all_to_all`.,"Add `num_updates` dimension to `input/output_offset` and `send/recv_sizes` of `ragged_all_to_all`. For now, only verify args since it isn't supported by the backends just yet.",2025-02-01T00:11:32Z,,open,0,0,https://github.com/jax-ml/jax/issues/26256
294,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Move `ragged_all_to_all` test under appropriate test file)ï¼Œ å†…å®¹æ˜¯ (Move `ragged_all_to_all` test under appropriate test file)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Move `ragged_all_to_all` test under appropriate test file,Move `ragged_all_to_all` test under appropriate test file,2025-02-01T00:10:58Z,,closed,0,0,https://github.com/jax-ml/jax/issues/26255
573,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Return arrays from `ArrayImpl._check_and_rearrange`. Build IFRT shardings with both addressable and non-addressable devices, instead of only addressable devices.)ï¼Œ å†…å®¹æ˜¯ (Return arrays from `ArrayImpl._check_and_rearrange`. Build IFRT shardings with both addressable and nonaddressable devices, instead of only addressable devices. This is a rollforward of two previous rollbacks after fixing breakages.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"Return arrays from `ArrayImpl._check_and_rearrange`. Build IFRT shardings with both addressable and non-addressable devices, instead of only addressable devices.","Return arrays from `ArrayImpl._check_and_rearrange`. Build IFRT shardings with both addressable and nonaddressable devices, instead of only addressable devices. This is a rollforward of two previous rollbacks after fixing breakages.",2025-01-31T23:45:30Z,,open,0,0,https://github.com/jax-ml/jax/issues/26253
406,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Replace Python 3.12 with Python 3.13 and add Python 3.10 to the matrix)ï¼Œ å†…å®¹æ˜¯ (Replace Python 3.12 with Python 3.13 and add Python 3.10 to the matrix Expands test coverage to cover the oldest and newest Python versions that we support.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Replace Python 3.12 with Python 3.13 and add Python 3.10 to the matrix,Replace Python 3.12 with Python 3.13 and add Python 3.10 to the matrix Expands test coverage to cover the oldest and newest Python versions that we support.,2025-01-31T22:18:02Z,,open,0,0,https://github.com/jax-ml/jax/issues/26250
1424,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Gradient of `jnp.linalg.norm` does not respect absolute homogeneity.)ï¼Œ å†…å®¹æ˜¯ ( Description The norm) should satisfy $d= \left\Vert a \mathbf{b}\right\Vert = \left\vert a\right\vert \left\Vert\mathbf{b}\right\Vert$ for scalar $a$ and vector $\mathbf{b}$. Consequently $\frac{\partial d}{\partial a} = \mathrm{sign}\left(a\right) \left\Vert\mathbf{b}\right\Vert$. However, the gradient of the standard implementation of the $p$norm $\left(\sum_{i} b_i ^ p\right)^{1/p}$ is not defined at $\mathbf{b}=\mathbf{0}$ because applying the chain rule includes terms involving negative powers of zero.  This particular situation arises, for example, in the evaluation of the diagonal of the Matern covariance kernel for Gaussian processes in more than one dimension. Specifically, for heterogeneous `length_scales`, the rescaled distance between two points `x` and `y` is `jnp.linalg.norm((x  y) / length_scales)`, and the derivative fails for `x == y`[^1]. This is not an issue for homogeneous `length_scales` because we can rewrite as `jnp.linalg.norm(x  y) / length_scales`.  System info (python version, jaxlib version, accelerator, etc.)  [^1]: If we use a Mahalanobisstyle distance rather than just considering the product of kernels in different dimensions.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Gradient of `jnp.linalg.norm` does not respect absolute homogeneity.," Description The norm) should satisfy $d= \left\Vert a \mathbf{b}\right\Vert = \left\vert a\right\vert \left\Vert\mathbf{b}\right\Vert$ for scalar $a$ and vector $\mathbf{b}$. Consequently $\frac{\partial d}{\partial a} = \mathrm{sign}\left(a\right) \left\Vert\mathbf{b}\right\Vert$. However, the gradient of the standard implementation of the $p$norm $\left(\sum_{i} b_i ^ p\right)^{1/p}$ is not defined at $\mathbf{b}=\mathbf{0}$ because applying the chain rule includes terms involving negative powers of zero.  This particular situation arises, for example, in the evaluation of the diagonal of the Matern covariance kernel for Gaussian processes in more than one dimension. Specifically, for heterogeneous `length_scales`, the rescaled distance between two points `x` and `y` is `jnp.linalg.norm((x  y) / length_scales)`, and the derivative fails for `x == y`[^1]. This is not an issue for homogeneous `length_scales` because we can rewrite as `jnp.linalg.norm(x  y) / length_scales`.  System info (python version, jaxlib version, accelerator, etc.)  [^1]: If we use a Mahalanobisstyle distance rather than just considering the product of kernels in different dimensions.",2025-01-31T21:46:59Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/26248,Ping  because he daydreams about autodiff corner cases
651,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Adjust docs/conf.py to remove prompt when copying docs examples by default)ï¼Œ å†…å®¹æ˜¯ (This PR adjusts `conf.py` to remove the prompt and output of docs examples when copied using the copy button extension. I have also tried the `copybutton_prompt_text = "">>> ""` option, however this removes multiline inputs e.g. present when defining functions. It tested locally and it works as intended. Resolves CC(Adjust config of sphinxcopybutton to remove prompts and indentation by default?).)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Adjust docs/conf.py to remove prompt when copying docs examples by default,"This PR adjusts `conf.py` to remove the prompt and output of docs examples when copied using the copy button extension. I have also tried the `copybutton_prompt_text = "">>> ""` option, however this removes multiline inputs e.g. present when defining functions. It tested locally and it works as intended. Resolves CC(Adjust config of sphinxcopybutton to remove prompts and indentation by default?).",2025-01-31T17:04:27Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/26236
730,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Adjust config of sphinx-copybutton to remove prompts and indentation by default?)ï¼Œ å†…å®¹æ˜¯ (Jax already uses the `sphinxcopybutton` extension to make code examples in the documentation copyable. However the copy currently still contains the prompt markers `>>>`. This is slightly annoying, when copy and pasting examples into notebooks or editors. `sphinxcopybutton` can be configured to exclude the prompt and indentation identifiers on copy, e.g. like this:  Which leads the copied content changing from:  To:  I think this is the much more reasonable default.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Adjust config of sphinx-copybutton to remove prompts and indentation by default?,"Jax already uses the `sphinxcopybutton` extension to make code examples in the documentation copyable. However the copy currently still contains the prompt markers `>>>`. This is slightly annoying, when copy and pasting examples into notebooks or editors. `sphinxcopybutton` can be configured to exclude the prompt and indentation identifiers on copy, e.g. like this:  Which leads the copied content changing from:  To:  I think this is the much more reasonable default.",2025-01-30T15:50:09Z,enhancement,closed,1,2,https://github.com/jax-ml/jax/issues/26210,Sounds reasonable â€“ would you like to put together a pull request?," Yes, sure!"
418,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Skip ragged collective tests on CPU)ï¼Œ å†…å®¹æ˜¯ (The raggedalltoall op isn't supported on CPU. The CPU test isn't executed by bazel, but when running with pytest, it looks like we need to explicitly skip. Fixes https://github.com/jaxml/jax/issues/26203)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Skip ragged collective tests on CPU,"The raggedalltoall op isn't supported on CPU. The CPU test isn't executed by bazel, but when running with pytest, it looks like we need to explicitly skip. Fixes https://github.com/jaxml/jax/issues/26203",2025-01-30T15:05:13Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/26208
585,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix `lax.ragged_all_to_all` degenerate case)ï¼Œ å†…å®¹æ˜¯ (Fix `lax.ragged_all_to_all` degenerate case In a singleton group case, unlike regular all_to_all, the ragged op becomes a generic equivalent of DynamicUpdateSlice, except update size is not statically known. This operation can't be expressed with standard HLO instructions  the backend will handle this case separately. Added small improvement to error messages.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Fix `lax.ragged_all_to_all` degenerate case,"Fix `lax.ragged_all_to_all` degenerate case In a singleton group case, unlike regular all_to_all, the ragged op becomes a generic equivalent of DynamicUpdateSlice, except update size is not statically known. This operation can't be expressed with standard HLO instructions  the backend will handle this case separately. Added small improvement to error messages.",2025-01-29T22:53:20Z,,closed,0,0,https://github.com/jax-ml/jax/issues/26198
467,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Return arrays from `ArrayImpl._check_and_rearrange`.)ï¼Œ å†…å®¹æ˜¯ (Return arrays from `ArrayImpl._check_and_rearrange`. This is in preparation for a larger change, so that input buffers can be checked before Array creation in XLA and the user gets more helpful JAX error messages instead of XLA errors.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Return arrays from `ArrayImpl._check_and_rearrange`.,"Return arrays from `ArrayImpl._check_and_rearrange`. This is in preparation for a larger change, so that input buffers can be checked before Array creation in XLA and the user gets more helpful JAX error messages instead of XLA errors.",2025-01-28T22:14:10Z,,closed,0,0,https://github.com/jax-ml/jax/issues/26166
972,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Operation very slow to compile since jax 0.4.36)ï¼Œ å†…å®¹æ˜¯ ( Description Hi folks, We've been having slow compilation issues since jax 0.4.36 with some of our JAX code.  The slow compilation (i.e. takes O(hours) to run instead of  Install packages  pip install upgrade jax[cuda] jaxlib  pip install upgrade mujoco pip install upgrade mujoco_mjx pip install upgrade brax    Run this Python code:  The corresponding XLA dump is attached. I also reran the same script with `num_evals=0` within `train_fn`, and the code runs fine (the slow compilation occurs somewhere here). I'm attaching both the working and nonworking XLA dumps. We would really appreciate any help on this issue. xla_dump_hanging_compilation.tar.gz xla_dump_working.tar.gz  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",agent,Operation very slow to compile since jax 0.4.36," Description Hi folks, We've been having slow compilation issues since jax 0.4.36 with some of our JAX code.  The slow compilation (i.e. takes O(hours) to run instead of  Install packages  pip install upgrade jax[cuda] jaxlib  pip install upgrade mujoco pip install upgrade mujoco_mjx pip install upgrade brax    Run this Python code:  The corresponding XLA dump is attached. I also reran the same script with `num_evals=0` within `train_fn`, and the code runs fine (the slow compilation occurs somewhere here). I'm attaching both the working and nonworking XLA dumps. We would really appreciate any help on this issue. xla_dump_hanging_compilation.tar.gz xla_dump_working.tar.gz  System info (python version, jaxlib version, accelerator, etc.) ",2025-01-28T21:31:36Z,bug,open,0,7,https://github.com/jax-ml/jax/issues/26162,Related to: https://github.com/google/brax/issues/569 https://github.com/googledeepmind/mujoco_playground/issues/11  , Would appreciate your help here ğŸ™,HLO reproducer: https://gist.github.com/jreiffers/b6b8427ef64c00e688e79fd5af25b571  I'll take a look which pass is blowing up. Hopefully it's not in LLVM :).,"I initially suspected the unnecessary concatenate at the end, but it happens even when that's fixed manually. There are at least two issues here: 1. codegen/emitters/computation_partitioner., because of the bitcasts above the concatenate. The partitioner does not track the real indexing, so after the bitcast it considers the inputs to the bitcasts (e.g. `add.56926.1.clone.1`) to have users with inconsistent indexing (the root tuple and the bitcast), which cascades all the way to the parameters. We end up with lots of functions even though ~everything is elementwise. 2. Afterwards, inlining breaks, generating a huge function. I stopped it after a few hundred thousand ops. Not entirely sure yet what's happening here, maybe failed/insufficient CSE or canonicalization. The correct fix for 1. is to use proper indexing maps in the partitioner. A quick hack is to change `all_users_elementwise` in `computation_partitioner.cc` to this:  However, I'm not sure that's safe.  could you give that a try and see what breaks?", Thank you for the reproducer. Trying.,The quick hack did not work. I'll try to migrate us to the indexing maps next week.,"Thanks  and  for taking a look, really appreciate it!"
1438,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(CPU slowdown with new runtime (v0.4.32 and newer))ï¼Œ å†…å®¹æ˜¯ ( Description Thanks a lot for your efforts in building JAX, I love working with it! On my MacBook Pro CPU (M3), my differentiable simulator runs 5x to 10x slower on new versions of JAX (`v0.4.32` or newer) as compared to older versions (`v0.4.31` or older). Setting the following xlaflag fixes the issue in newer versions of JAX for me (i.e. speed is as before):  Unfortunately, 5x slower runtime would probably kill any relevant usecase of my simulator. As such, I have two questions:  Will the old CPU runtime continue to be maintained via the XLA flag?  Do you have any obvious candidates for operations that could cause this behavior? Any ideas on where I should start looking? Related to CC(CPU Slowdown introduced in 0.4.32  and in following versions) and CC(Performance degradation of `fftn` with the new CPU runtime)  Thanks a lot!  To reproduce As my codebase is fairly large, I cannot easily provide a selfcontained example without relying on my toolbox. To reproduce:  and  On my MacBook Pro (and JAX `v0.5.0`, jaxlib `v0.5.0`, Python `3.12`), I get:  Removing `os.environ['XLA_FLAGS'] = 'xla_cpu_use_thunk_runtime=false'`, I get:    System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,CPU slowdown with new runtime (v0.4.32 and newer)," Description Thanks a lot for your efforts in building JAX, I love working with it! On my MacBook Pro CPU (M3), my differentiable simulator runs 5x to 10x slower on new versions of JAX (`v0.4.32` or newer) as compared to older versions (`v0.4.31` or older). Setting the following xlaflag fixes the issue in newer versions of JAX for me (i.e. speed is as before):  Unfortunately, 5x slower runtime would probably kill any relevant usecase of my simulator. As such, I have two questions:  Will the old CPU runtime continue to be maintained via the XLA flag?  Do you have any obvious candidates for operations that could cause this behavior? Any ideas on where I should start looking? Related to CC(CPU Slowdown introduced in 0.4.32  and in following versions) and CC(Performance degradation of `fftn` with the new CPU runtime)  Thanks a lot!  To reproduce As my codebase is fairly large, I cannot easily provide a selfcontained example without relying on my toolbox. To reproduce:  and  On my MacBook Pro (and JAX `v0.5.0`, jaxlib `v0.5.0`, Python `3.12`), I get:  Removing `os.environ['XLA_FLAGS'] = 'xla_cpu_use_thunk_runtime=false'`, I get:    System info (python version, jaxlib version, accelerator, etc.) ",2025-01-28T10:18:16Z,bug,open,1,4,https://github.com/jax-ml/jax/issues/26145,"EDIT: When using `float`, the gap between the old versions and new versions of JAX is even more prominent (almost 10x then...) ","Ping  and  for CPU thunks runtime performance As noted, it's hard to say too much without a more minimal reproducer, but to answer the specific questions: > Will the old CPU runtime continue to be maintained via the XLA flag? I think there continue to be enough performance regressions that we would like to keep this working for the time being. I think the right approach is to suggest the use of that flag for now with the assumption that the reported performance regressions will be reasonably addressed before it is removed.  and  might be able to comment more about timelines on the XLA side! > Do you have any obvious candidates for operations that could cause this behavior? Any ideas on where I should start looking? The most common culprit in my experience is loops (like scan or while_loops), although sometimes these can be implicit. If you have explicit `scan`s in your library, you can experiment with trading off compile time with runtime performance using the `unroll` parameter (increase that for longer compile times, but typically better runtime performance.)","We plan to fix performance regression in then next couple of weeks (), and only after that we'll start removing the old runtime.","That's great to know, thanks a lot for the quick response! My simulator is an ODE which indeed uses a `scan` (across 40k timepoints), so this might well be the culprit. Unrolling is not an option for me because compile time becomes excessive. For now, I will simply use the old CPU runtime via the XLA flag. Thank you for the suggestions!"
451,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jax.experimental.mosaic.gpu.profiler.measure is missing a warmup?)ï¼Œ å†…å®¹æ˜¯ ( Description I'm getting a runtime too long the first time I profile. I expect `profiler.measure` to take care of compiling and warming up.   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jax.experimental.mosaic.gpu.profiler.measure is missing a warmup?," Description I'm getting a runtime too long the first time I profile. I expect `profiler.measure` to take care of compiling and warming up.   System info (python version, jaxlib version, accelerator, etc.) ",2025-01-28T10:11:54Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/26144,"Originally I skipped warmup in the CUPTIbased implementation since it isn't affected by host runtime (unlike the eventsbased implementation). But as you are showing here, there are cases when the measurements in the first run can be affected by the autotuner on the device:  Maybe we should change the CUPTIbased profiler to warm up just like the eventsbased profiler does."
725,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Run test job irrespective of if the build jobs succeeds or fails )ï¼Œ å†…å®¹æ˜¯ (Run test job irrespective of if the build jobs succeeds or fails  This lets us avoid losing test coverage if a single unrelated build job fails. E.g Windows build job fails but everything else succeeds. In this case, we still want to run the tests for other platforms. Also, if a build job fails, its corresponding test job will also report a failure as a result of not being able to download the wheel artifact so we should still be able to tell the source of job failure easily.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Run test job irrespective of if the build jobs succeeds or fails ,"Run test job irrespective of if the build jobs succeeds or fails  This lets us avoid losing test coverage if a single unrelated build job fails. E.g Windows build job fails but everything else succeeds. In this case, we still want to run the tests for other platforms. Also, if a build job fails, its corresponding test job will also report a failure as a result of not being able to download the wheel artifact so we should still be able to tell the source of job failure easily.",2025-01-27T22:26:12Z,CI Connection Halt - On Retry,open,0,0,https://github.com/jax-ml/jax/issues/26135
1341,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Exceptions from `jax.pure_callback` have no `__cause__`)ï¼Œ å†…å®¹æ˜¯ ( Description `jax.pure_callback` can raise any arbitrary exception:  The problem is that `XlaRuntimeError` is not just reraised from the original exception  it has completely replaced it. It has `__cause__` set to `None`, which makes it very hard to write tests that iterate on multiple backends, like https://github.com/scipy/scipy/blob/main/scipy/_lib/_lazy_testing.py / https://github.com/dataapis/arrayapiextra/blob/main/src/array_api_extra/testing.py allows for:  Same as above, but without scipy's and array_api_extra's magic:  The above test fails on jax, because it raises a XlaRuntimeError instead of an AssertionError.  Expected behaviour `XlaRuntimeError.__cause__` should be the original `AssertionError`. At that point, `lazy_xp_function` and similar test tools will have an easy time unwrapping the exception under the hood, so that the tests don't have to think about it.  Workaround Write an adhoc workaround to `pytest.raises`:  IMHO it would be very ugly to have such a wrapper littered all over the scipy tests.  System info (python version, jaxlib version, accelerator, etc.) jax 0.4.35)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Exceptions from `jax.pure_callback` have no `__cause__`," Description `jax.pure_callback` can raise any arbitrary exception:  The problem is that `XlaRuntimeError` is not just reraised from the original exception  it has completely replaced it. It has `__cause__` set to `None`, which makes it very hard to write tests that iterate on multiple backends, like https://github.com/scipy/scipy/blob/main/scipy/_lib/_lazy_testing.py / https://github.com/dataapis/arrayapiextra/blob/main/src/array_api_extra/testing.py allows for:  Same as above, but without scipy's and array_api_extra's magic:  The above test fails on jax, because it raises a XlaRuntimeError instead of an AssertionError.  Expected behaviour `XlaRuntimeError.__cause__` should be the original `AssertionError`. At that point, `lazy_xp_function` and similar test tools will have an easy time unwrapping the exception under the hood, so that the tests don't have to think about it.  Workaround Write an adhoc workaround to `pytest.raises`:  IMHO it would be very ugly to have such a wrapper littered all over the scipy tests.  System info (python version, jaxlib version, accelerator, etc.) jax 0.4.35",2025-01-25T23:40:22Z,bug,open,1,16,https://github.com/jax-ml/jax/issues/26102,I suspect that this would also be useful to 'improve' `equinox.error_if` which is quite useful in some circumstances. Right now the XLARuntimeError makes the stack trace/error message quite hard to read.  ,"Thanks for the tag! So  you can probably work around JAX's lack of support for runtime errors in the exact same way that Equinox does: wrap both the callback and the JIT, and smuggle the exception between the two using global state. It's ugly, but it works! (We do the exact same thing between `eqx.filter_jit` and `eqx.error_if` to support pretty readable runtime error messages.) Here's a quick demo:  Try it out:   FWIW I'd suggest only using the above in tests, and not as part of any public API. If you're going to wrap `jax.jit` for public use then there are other changes I'd still recommend making here  for example preserving the pytreeness of `f`.","Note that raising an exception is a sideeffect, so technically it's a misuse of `pure_callback` and is unsupported. With the current implementation, doing so will lead to a particular behavior, but that behavior may change in future JAX versions because it's essentially an accidental implementation detail. The fix would be to not rely on raising errors inside `pure_callback`. It's true that packages like `equinox` wrap these implementation details in their public APIs, but this has never been an approach that the JAX team has recommended, and we would not suggest relying on such implementations in important code.","> Note that raising an exception is a sideeffect, so technically it's a misuse of `pure_callback` I can make a strong case for exceptions being a return value. The case becomes evident if you have familiarity with how error propagation works in languages such as Rust and Go. Python's `try... except` is just syntax sugar; you could rewrite all python code to return a tuple of [return value, error] and it would be functionally the same as before. > and is unsupported. With the current implementation, doing so will lead to a particular behavior, but that behavior may change in future JAX versions because it's essentially an accidental implementation detail. I'm not sure I understand  are you saying that, in the future, if a `pure_callback` raises JAX may decide to swallow the exception, e.g. fabricate a fake output array full of NaNs, and continue? I believe this is completely unreasonable. If you look at codebases such as scipy, there is _extensive_ use of eager validation, typically by calling `__bool__`, followed by the actual calculation. Validation can prevent things such as segmentation faults and infinite loops, like the equinox documentation illustrates. It would be a major burden to be forced to put validation and the calculation that _needs_ the validation to happen within the same `pure_callback` block, and it would make it impossible to write code that works on multiple array API backends including JAX without having a person with expert knowledge of JAX read through every single line of the code and ensure that every single edge case of malformed input is thoroughly unit tested. So I urge you to formalize that `pure_callback` functions can raise, and if they do whatever follows won't be executed and that when the user finally materializes the graph the exception will be reraised (either the original exception or JAX's own exception with the original one as its `__cause__`).  Aside: `array_api_extra.lazy_raise`, which I'm the middle of writing now, is almost exactly the same as `equinox.error_if`. I didn't even know that equinox existed and just naturally converged towards the same design. I think that speaks volumes on real world needs.","> I'm not sure I understand  are you saying that, in the future, if a `pure_callback` raises JAX may decide to swallow the exception, e.g. fabricate a fake output array full of NaNs, and continue? I'm saying the current behavior is undefined, so relying on the details of the current implementation is not a good idea. I understand that eager validation is desirable in some programming contexts. What I'm telling you is that it's not currently supported in JAX, and working around that by misusing existing APIs is not a viable longterm solution.","FWIW  whilst I obviously am also in favour of exceptions continuing to behave as they do now  Equinox actually includes a guard against that behaviour ever changing: if the computation proceeds past the raising `pure_callback`, then we hit a *second* `pure_callback` that then sits in an infinite loop: https://github.com/patrickkidger/equinox/blob/f8df0e648bf3af8c9b041fc76f58fcc5b4ab37c6/equinox/_errors.pyL113L116 Not as easy to debug for an end user, but it's a JAXspeccompatible backup to ensure that even then, no downstream computation ever occurs.","Regarding the root question here â€” the request that the `XLARuntimeError` raised when a computation fails should set its `__cause__` to the error that is raised in the pure callback â€” I'm not sure whether that's feasible. The problem is that it's not just Python code calling a Python callback â€” it's Python code on the host launching an XLA process, which calls back to Python, then returns a nonpython error code with the string representation of the error encountered in the callback, and this error code results in an `XLARuntimeError` in the host. ~In general I don't believe there's any requirement that the hostside code and the callback are even running in the same process, and I don't think you can have a `__cause__` attribute that links to exceptions from another process.~ Edit: I think I was wrong: callbacks do need to be on the same process as the host/tracing code, so we could do something like propagate the `id` of the callback error to XLA, pass that along from XLA back to the host, and then somehow access the object in memory at the specified id to get a reference to it. We could maybe do that through some kind of customized error handler for callbacks that ensures errors are added to a global registry before they are raised. It's kind of tricky given the computation model, and it wouldn't be a very clean solution I think. Given that the behavior of exceptions in callbacks is not welldefined, I don't think it's warranted to attempt those kinds of gymnastics at the moment. What do you think?","A broader comment: there's a reason that the array API standard does not require any valuedependent error semantics: there are packages for which such semantics are a nonstarter. It's fine to pursue adding such semantics in `arrayapiextra`, but doing so may disqualify packages like JAX which don't support such semantics. If scipy et al. cannot implement their API without going beyond what's available in the array API standard, then I suspect their choice is either to relax their requirements, or to recognize that they will not be able to support all standardcompliant libraries.","> it's Python code on the host launching an XLA process, which calls back to Python, then returns a nonpython error code You can just pickle whatever exception is raised by the userdefined function and unpickle it when the user finally queries a material answer with `__str__`, `__array__`, exc. This would be pretty much the same as what Dask does. Some userdefined exception struggle to pickle due to wellknown design mistakes in `Exception`, and you may encounter problems such as locallydefined classes or lambdas attached to them, but that's the users' problem. Just fall back to the current string if they fail to pickle.","Thanks  the pickle approach is an interesting idea. It might be a good option if we eventually choose to support exception handling within `pure_callback`. Still, it would not be worthwhile pursuing particular mechanisms of error propagation until the behavior of impure (exception raising) callbacks can be betterdefined. Note that although things work fairly straightforwardly now on CPU, this is not the case in general on accelerators â€“ for example, TPU has no mechanism for runtime error handling, so an exception in a `pure_callback` does not currently end the process in the same way that it does on CPU. It's possible we could make headway here toward general support for runtime errors via `pure_callback`, but it would require figuring out the right computational model within the constraints of the TPU runtime.","> If scipy et al. cannot implement their API without going beyond what's available in the array API standard, then I suspect their choice is either to relax their requirements, or to recognize that they will not be able to support all standardcompliant libraries. Speaking to this point  `eqx.error_if` exists solely because of the needs of Diffrax, Optimistix etc. I've not been involved in the Array API standard, but some kind of ""if this then panic"" is indeed necessary for scicomp use cases. (Just a comment, I recognize your statements so far about this being undefined behavior.)","I was chatting about this with  and he mentioned an important point: the assumption that `pure_callback` callbacks have no sideeffects is baked deeply into how it interacts with JAX transformations, and if you use the mechanism discussed here for runtime value checks, you can easily end up with false positives. Here's a short example where `equinox.error_if` raises an unexpected error:  The problem is that `pure_callback` (which is used by `error_if`) transforms under `vmap` under the assumption that the callback is pure: violating that contract means that you violate the assumptions made by the batching rule, which can lead to incorrect program outputs.",You can recover the same issue in pure JAX: ,"Ha, fair enough. I guess we assume pure  {can diverge} for these transformations but I think the point still stands (users tend to write code with errors in them but tend not to write code that may not halt).","> The problem is that `pure_callback` (which is used by `error_if`) transforms under `vmap` under the assumption that the callback is pure: violating that contract means that you violate the assumptions made by the batching rule, which can lead to incorrect program outputs. Again, a callback that raises _is pure_. Nobody asks you to change the execution graph depending on its output. You just need to propagate the exception along with the return value, treating the tuple of `(retval, exc)` as the output, and test for `exc is None` before you materialize the nodes down the graph. As if you were writing Rust or Go code.","> Again, a callback that raises _is pure_. You're arguing semantics, and this is very unhelpful. Yes, you can define ""pure"" in such a way that exceptions are allowed; but **that definition of pure is not relevant to JAX**. In JAX what matters is that all function behavior can be determined via its abstract trace: this does not admit runtime valuedependent Python exceptions, and so functions that rely on Python exceptions are not allowed. > You just need to propagate the exception along with the return value, treating the tuple of (retval, exc) as the output, and test for exc is None before you materialize the nodes down the graph. As if you were writing Rust or Go code. Sure, that's a very reasonable approach, and is essentially what `jax.experimental.checkify` does. It is not what JAX does by default, and changing JAX's computational model to do so by default would be a quite large project. We have some ongoing work to try to make the checkify approach more transparent, but unfortunately right now that's not a solution we can depend on. None of that changes what I've said above: in JAX's current implementation, you cannot depend on raising errors in `pure_callback`, and any solution based on that approach is a nogo. Please do not spend any more time on this."
473,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(printing new style prng keys on the cpu results in XlaRuntimeError)ï¼Œ å†…å®¹æ˜¯ ( Description Trying to print (or generally calling `__repr__` on) a new style PRNG key that has been put on the cpu results in an XlaRuntimeError.   results in    System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,printing new style prng keys on the cpu results in XlaRuntimeError," Description Trying to print (or generally calling `__repr__` on) a new style PRNG key that has been put on the cpu results in an XlaRuntimeError.   results in    System info (python version, jaxlib version, accelerator, etc.) ",2025-01-25T17:53:32Z,bug,open,0,10,https://github.com/jax-ml/jax/issues/26101,I can't repro this with 0.5.0:  Please reopen if you can still repro with jax and jaxlib 0.5.0,"When updating jax on colab, the issue persists  The behaviour does not happen when in a cpu only runtime. It does happen in a T4 GPU runtime.",Maybe colab on GPU is not on the right cuda version. Can you try it out locally and do a clean install?,"Yes, I'll try to reproduce the bug on a local machine. I did stumble onto some strange result in the mean while though: I added the following code in a new cell in the notebook  and the result is ","On my local machine, it happens as well. ","I wonder what happens when you turn off this flag: jax.config.update('jax_threefry_partitionable', False)  do this immediately after you import jax I am assuming it shouldn't affect since you could repro with 0.4.33 too but doesn't hurt to try :)",The result is the same,"Actually, the same thing seems to happen when going from cpu to gpu  results in  ","Quick question, does this happen with any array, or just with keys?  If it's happening with any array, then I suspect the issue relates to a mismatch in CUDA or CUDNN versions.","It only happens with new style keys.  In the above, only the last line results in an error."
524,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Suppress XLA slow operation alarm)ï¼Œ å†…å®¹æ˜¯ (Quoting https://github.com/jaxml/jax/discussions/21914 : I'm trying to suppress the XLA warning about slow compilation that looks like:  Any idea how to suppress this? I know why my code takes a while to compile and I'm okay with it  I'm just struggling to find any relevant XLA flags or relevant documentation.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Suppress XLA slow operation alarm,Quoting https://github.com/jaxml/jax/discussions/21914 : I'm trying to suppress the XLA warning about slow compilation that looks like:  Any idea how to suppress this? I know why my code takes a while to compile and I'm okay with it  I'm just struggling to find any relevant XLA flags or relevant documentation.,2025-01-25T15:28:51Z,enhancement,open,0,0,https://github.com/jax-ml/jax/issues/26098
336,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([better_errors] Add more debug info test coverage)ï¼Œ å†…å®¹æ˜¯ (Some of the tests added show missing or incorrect debug info, marked with TODO. Fixes will come separately.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[better_errors] Add more debug info test coverage,"Some of the tests added show missing or incorrect debug info, marked with TODO. Fixes will come separately.",2025-01-25T07:24:00Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/26093
890,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Custom primitives + ordered effects + linear_solve = bug in 0.5 ?)ï¼Œ å†…å®¹æ˜¯ ( Description In mpi4jax we use some ordered effect to ensure that our mpi communication is ordered. Version 0.5 of jax breaks our ordered effect logic when we use our custom primitives inside of `jax.lax.custom_linear_solve`. The breakage only appears if the call is jitted, while it does not do nothing if it's not jitted. While this might be a bug on our end, the code was working previously, and I'm at a loss at what could be wrong. Do you have any suggestion on what could be going wrong? MWE:  and the stack trace     System info (python version, jaxlib version, accelerator, etc.)  ```python In [4]: mpi4jax.__version__ Out[4]: '0.7.0' ``)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,Custom primitives + ordered effects + linear_solve = bug in 0.5 ?," Description In mpi4jax we use some ordered effect to ensure that our mpi communication is ordered. Version 0.5 of jax breaks our ordered effect logic when we use our custom primitives inside of `jax.lax.custom_linear_solve`. The breakage only appears if the call is jitted, while it does not do nothing if it's not jitted. While this might be a bug on our end, the code was working previously, and I'm at a loss at what could be wrong. Do you have any suggestion on what could be going wrong? MWE:  and the stack trace     System info (python version, jaxlib version, accelerator, etc.)  ```python In [4]: mpi4jax.__version__ Out[4]: '0.7.0' ``",2025-01-24T18:26:30Z,bug,closed,0,9,https://github.com/jax-ml/jax/issues/26087,Here's a reproducer without `mpi4jax`:  Same error:  Looks like any ordered effect will break `jax.lax.custom_linear_solve`.,"Sorry for the bump,  , is the reproducer above enough to nail down the problem?","At a guess it's because the abstract evaluation rule doesn't handle effects (and I think just silently swallows them): https://github.com/jaxml/jax/blob/af84143e618a5b13c1a26f3331058236cb5cc1a7/jax/_src/lax/control_flow/solves.pyL485 In contrast effectful primitives (or higherorder primitives which may wrap effectful operations) tend to use `def_effectful_abstract_eval` instead, e.g. here for `jax.lax.cond`: https://github.com/jaxml/jax/blob/af84143e618a5b13c1a26f3331058236cb5cc1a7/jax/_src/lax/control_flow/conditionals.pyL795 or here for `jax.debug.print`: https://github.com/jaxml/jax/blob/af84143e618a5b13c1a26f3331058236cb5cc1a7/jax/_src/debugging.pyL98  FWIW this is an issue I know we have over in Lineax as well: https://github.com/patrickkidger/lineax/blob/fa6c777ef6e5edf2c798a58994e07946a3ce50d7/lineax/_solve.pyL111L127 because AFAIK, JAX doesn't provide any public API for getting the effects. Ideally `jax.eval_shape` would optionally provide these as well!","Oh, thanks! This does look like it. Is there a specific reason why this was not addressed in Jax itself, or itâ€™s just lack of time and therefore this could be easily fixed?","Probably just a bug that noone noticed/fixed before now! FWIW at least for the implementation JAX has here it should be pretty easy to fix, jaxprs store their effects so something like `jaxprs solve_jaxpr.effects` would probably get what you need.","Good suggestion, kidger! Yeah, I agree that this seems like the most likely culprit and I expect it would be a simple fix. I'm happy to do it unless someone on this thread would be interested in opening a PR. Let me know!","I'd have to get familiar with this part of the jax codebase, so please do go ahead! This would be very helpful for us","This should be fixed by https://github.com/jaxml/jax/pull/26275. I'll close this as completed, but please let me know if you continue to run into issues!",Thanks! As soon as jax 0.5.1 makes it out I'll test it!
568,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Jax 0.5.0 error: no type named 'string' in namespace 'std')ï¼Œ å†…å®¹æ˜¯ ( Description I'm not able to build jaxlib version 0.5.0 on aarch64, I see the following build error:  Previous clang versions like 16.x or 17.x from llvmproject/releases generate a different error. For instance, Clang 16.0.0 works for x86_64, but not for aarch64:   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Jax 0.5.0 error: no type named 'string' in namespace 'std'," Description I'm not able to build jaxlib version 0.5.0 on aarch64, I see the following build error:  Previous clang versions like 16.x or 17.x from llvmproject/releases generate a different error. For instance, Clang 16.0.0 works for x86_64, but not for aarch64:   System info (python version, jaxlib version, accelerator, etc.) ",2025-01-23T16:27:06Z,bug,open,0,7,https://github.com/jax-ml/jax/issues/26062,"Thank you for reporting! The error didn't happen for me when I tried with LLVM 18 on Ubuntu. But I could reproduce the error in a RHEL 9 docker image (+ Bazel 7.4.1, Python 3.12.5, clang 18.1.8). I'll submit a fix soon. In the meanwhile, you should be able to skip the error by passing `disable_mkl_dnn` to build.py.",Thanks for confirming the issue! Do you plan to cut a new release with the fix in the short term? I'd rather not disable any feature.,"We're trying for a monthly release schedule, absent extraordinary reasons. The next scheduled release is on or about Feb 15. (Any reason you can't use a prebuilt jax/jaxlib?)"," In the meanwhile, you can build JAX with a local ACL repository that contains the fix from PR https://github.com/openxla/xla/pull/21810:  (`exclude_omp_scheduler.patch` fixes an OpenMP dependency issue and may not be needed for your case.)",Thank you for your help! We build all of our assets from source code to ensure traceability and reproducible builds. We would wait for the next official release including the updated dependencies.,"Got it. Could you please help verify if you can build JAX at HEAD without this issue? If so, we can close this issue.", sorry for the delay. I'm facing a new build issue now: 
638,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯('arith.index_cast' op operand #0 must be signless-integer-like or memref of signless-integer, but got 'f32')ï¼Œ å†…å®¹æ˜¯ ( Description Hi, I was following the Pallas quickstart guide and tried modifying the `iota` kernel to produce fp32 results. But I got the following error:  I ran the following code.  What's strange is that if I move the `.astype(o_ref.dtype)` down to the next line, the error goes away  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"'arith.index_cast' op operand #0 must be signless-integer-like or memref of signless-integer, but got 'f32'"," Description Hi, I was following the Pallas quickstart guide and tried modifying the `iota` kernel to produce fp32 results. But I got the following error:  I ran the following code.  What's strange is that if I move the `.astype(o_ref.dtype)` down to the next line, the error goes away  System info (python version, jaxlib version, accelerator, etc.) ",2025-01-22T15:17:41Z,bug,open,0,2,https://github.com/jax-ml/jax/issues/26034,"The error here is on the index_cast (since you are storing to `o_ref[i]`), not on the cast produced by `as_type`. The error isn't very clear, but it's essentially saying you can't index into an array with a float. That being said, this is an error that we should catch in Pallas itself and raise a better error instead of outputting an invalid MLIR program that fails during the verification check.",Oh yes right I didn't see that
963,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(CPU Slowdown introduced in 0.4.32  and in following versions)ï¼Œ å†…å®¹æ˜¯ ( Description I've been running a speed test in my repository (https://github.com/danielpmorton/cbfpy/blob/main/test/test_speed.py) and I've noticed a significant reduction in speed with newer versions of Jax/Jaxlib. Running this with newer versions of Jax after 0.4.32 results in this script being approximately 1/3 the speed as with 0.4.31 and previous versions I know 0.4.32 is a yanked release, but this slowdown still exists in 0.4.33 and later I'll try to come up with a minimal working example, as I know this is hard to reproduce without installing my repo. In general, I'm solving a lot of QPs on CPU using qpax (https://github.com/kevintracy/qpax)   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,CPU Slowdown introduced in 0.4.32  and in following versions," Description I've been running a speed test in my repository (https://github.com/danielpmorton/cbfpy/blob/main/test/test_speed.py) and I've noticed a significant reduction in speed with newer versions of Jax/Jaxlib. Running this with newer versions of Jax after 0.4.32 results in this script being approximately 1/3 the speed as with 0.4.31 and previous versions I know 0.4.32 is a yanked release, but this slowdown still exists in 0.4.33 and later I'll try to come up with a minimal working example, as I know this is hard to reproduce without installing my repo. In general, I'm solving a lot of QPs on CPU using qpax (https://github.com/kevintracy/qpax)   System info (python version, jaxlib version, accelerator, etc.) ",2025-01-21T23:35:18Z,bug,open,1,3,https://github.com/jax-ml/jax/issues/26021,"Another note: the following code does not fix the problem, and in fact makes the issue much worse. I noticed this on a few other threads here: https://github.com/clemisch/jaxtomo/issues/3 https://github.com/jaxml/jax/issues/23590 ","Thanks for the report! There's quite a lot going on in your example, and I'm not very familiar with the dependencies, so I don't immediately have intuition about what to try. Although, I'm also surprised that setting that environment variable didn't improve the situation! So it would be useful if you could try to dig a little deeper and isolate the issue with a smaller self contained reproducer so that we have something to work with.", may have insights given the discussion here.
320,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Mosaic GPU] Implement basic WGMMAFragLayout inference and propagation)ï¼Œ å†…å®¹æ˜¯ ([Mosaic GPU] Implement basic WGMMAFragLayout inference and propagation)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[Mosaic GPU] Implement basic WGMMAFragLayout inference and propagation,[Mosaic GPU] Implement basic WGMMAFragLayout inference and propagation,2025-01-21T12:43:47Z,,closed,0,0,https://github.com/jax-ml/jax/issues/26007
415,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Mosaic GPU] Add support for converting all fragmented layouts to ir and back.)ï¼Œ å†…å®¹æ˜¯ ([Mosaic GPU] Add support for converting all fragmented layouts to ir and back. This will be used in the layout inference and lowering of the dialect WGMMA op)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[Mosaic GPU] Add support for converting all fragmented layouts to ir and back.,[Mosaic GPU] Add support for converting all fragmented layouts to ir and back. This will be used in the layout inference and lowering of the dialect WGMMA op,2025-01-21T10:40:57Z,,closed,0,0,https://github.com/jax-ml/jax/issues/26003
1017,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jax.scipy.linalg.expm yields wrong values (for symmetric matrices))ï¼Œ å†…å®¹æ˜¯ ( Description When calculating the matrix exponential using jax.scipy.linalg.expm the results mismatch those of scipy.linalg.expm for some (in this case symmetric) sampled matrices. The calculation via jax.scipy.linalg.eigh works fine. The same holds for scaling down the matrix elements and using jax.numpy.linalg.matrix_power.  As mentioned, this happens only for some of the sampled matrices, I cannot identify a clear pattern. This might be related to CC(Better expm). In my case this deviation caused completely false results in a simulation i was running. So it has potential to cause a lot of trouble for those who are not aware of the issue. Here is a code to reproduce the issue:  Example output:   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jax.scipy.linalg.expm yields wrong values (for symmetric matrices)," Description When calculating the matrix exponential using jax.scipy.linalg.expm the results mismatch those of scipy.linalg.expm for some (in this case symmetric) sampled matrices. The calculation via jax.scipy.linalg.eigh works fine. The same holds for scaling down the matrix elements and using jax.numpy.linalg.matrix_power.  As mentioned, this happens only for some of the sampled matrices, I cannot identify a clear pattern. This might be related to CC(Better expm). In my case this deviation caused completely false results in a simulation i was running. So it has potential to cause a lot of trouble for those who are not aware of the issue. Here is a code to reproduce the issue:  Example output:   System info (python version, jaxlib version, accelerator, etc.) ",2025-01-20T13:12:48Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/25987,Probably related to this https://github.com/jaxml/jax/issues/15195issuecomment1488964513
1421,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jax.nn.dot_product_attention CuDNN implementation raises tensor stride error during jit compile)ï¼Œ å†…å®¹æ˜¯ ( Description I am currently experiencing an issue where I am getting a CuDNN error relating to the stride of my K matrix when using `jax.nn.dot_product_attention` within a flax model. This occurs when jitting and the error stems from the CuDNN dimension checks here. I am not sure what exactly is causing the striding issue with the `k` tensor, and I have checked the shapes and sharding for the inputs; however, I am struggling to find a way to debug this issue further. When using the `implementation` argument set to `'xla'`, the model jits, and I am able to train with it. The shapes for `q`, `k` and `v` are all `(8, 2048, 40, 128)` and all are sharded along the first (batch) dimension, having the following sharding: `NamedSharding(mesh=Mesh('dp': 1, 'fsdp': 8), spec=PartitionSpec('fsdp',), memory_kind=device)`. The function is called as below:  This gives the following error:  If there are any ways to further debug the striding of my underlying tensor, and, if possible, how to force a contiguous layout that matches that of the shape of my tensor, please let me know.  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jax.nn.dot_product_attention CuDNN implementation raises tensor stride error during jit compile," Description I am currently experiencing an issue where I am getting a CuDNN error relating to the stride of my K matrix when using `jax.nn.dot_product_attention` within a flax model. This occurs when jitting and the error stems from the CuDNN dimension checks here. I am not sure what exactly is causing the striding issue with the `k` tensor, and I have checked the shapes and sharding for the inputs; however, I am struggling to find a way to debug this issue further. When using the `implementation` argument set to `'xla'`, the model jits, and I am able to train with it. The shapes for `q`, `k` and `v` are all `(8, 2048, 40, 128)` and all are sharded along the first (batch) dimension, having the following sharding: `NamedSharding(mesh=Mesh('dp': 1, 'fsdp': 8), spec=PartitionSpec('fsdp',), memory_kind=device)`. The function is called as below:  This gives the following error:  If there are any ways to further debug the striding of my underlying tensor, and, if possible, how to force a contiguous layout that matches that of the shape of my tensor, please let me know.  System info (python version, jaxlib version, accelerator, etc.) ",2025-01-20T11:38:25Z,bug NVIDIA GPU,open,0,4,https://github.com/jax-ml/jax/issues/25986,"Do you have a complete reproducer? I believe the problem here might be that XLA uses an incompatible layout for some intermediate value. If that's the case, the behavior will probably depend on what's around the cudnn call.",", unfortunately, I can't seem to reproduce this error with a minimal example of our model, which is one of the reasons it is quite hard to debug. It seems to only occur when used with the rest of our training code (which is part of a fairly large codebase).  That said, regarding the layout, I have an XLA dump containing some of the HLO from the ""beforeoptimisation"" pass. I assume there are no other passes because the compilation fails at this point. I think the layout for the q, k and v tensors seems okay here. The inner two dimensions are swapped in the layout; however, the embedding dimension that the error message complains about seems like it should have a stride of 1 here (unless I am reading something wrong). ","Did you run this just with `xla_dump_to`, or also with `xla_dump_hlo_pass_re`? The latter should be set to `.*` to get dumps after every pass.","No, I hadn't initially run with the regex flag  sorry about that! I have done so now, and I am seeing many more passes in the XLA dump. I have attached the final pass below. module_0027.jit__unnamed_wrapped_function_.0176.fusiondispatchpipeline.after_pipelinestart.before_fusionblocklevelrewriter.txt Does this imply that the error occurs within the fusionblocklevelrewriter? I have extracted the relevant lines around the CuDNN calls from the attached file here: "
931,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`lambda x: jnp.log(x == 1)` is not jit-able on metal.)ï¼Œ å†…å®¹æ˜¯ ( Description The function `lambda x: jnp.log(x == 1)` is not jitable on metal and raises the following error message.  The expression above is of course equivalent to `lambda x: jnp.where(x == 1, 0, jnp.inf)`, but, for example, appears in the evaluation of the log density of the delta distribution in `numpyro` here. I couldn't find a simpler expression that raised the same error. Here's a script to reproduce the issue.  Running on metal, I get the following.  Restricting to CPU gives the following.  This seems to be specific to the combination of equality testing and `jnp.log`, e.g., the following all work on both backends:   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,`lambda x: jnp.log(x == 1)` is not jit-able on metal.," Description The function `lambda x: jnp.log(x == 1)` is not jitable on metal and raises the following error message.  The expression above is of course equivalent to `lambda x: jnp.where(x == 1, 0, jnp.inf)`, but, for example, appears in the evaluation of the log density of the delta distribution in `numpyro` here. I couldn't find a simpler expression that raised the same error. Here's a script to reproduce the issue.  Running on metal, I get the following.  Restricting to CPU gives the following.  This seems to be specific to the combination of equality testing and `jnp.log`, e.g., the following all work on both backends:   System info (python version, jaxlib version, accelerator, etc.) ",2025-01-16T22:31:15Z,bug Apple GPU (Metal) plugin,open,3,1,https://github.com/jax-ml/jax/issues/25935,"Thx, we are looking into it. "
611,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([sharding_in_types] If an indexing operation hits into `gather_p`, error out saying to use `.at[...].get(out_spec=...)` instead.)ï¼Œ å†…å®¹æ˜¯ ([sharding_in_types] If an indexing operation hits into `gather_p`, error out saying to use `.at[...].get(out_spec=...)` instead. This will basically drop the gather operation into full auto mode and add a sharding constraint on the output given by the user via `out_spec`. Coauthoredby: Matthew Johnson )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"[sharding_in_types] If an indexing operation hits into `gather_p`, error out saying to use `.at[...].get(out_spec=...)` instead.","[sharding_in_types] If an indexing operation hits into `gather_p`, error out saying to use `.at[...].get(out_spec=...)` instead. This will basically drop the gather operation into full auto mode and add a sharding constraint on the output given by the user via `out_spec`. Coauthoredby: Matthew Johnson ",2025-01-16T18:19:47Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25929
1430,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([better_errors] Improvements in propagation of debugging info)ï¼Œ å†…å®¹æ˜¯ (Added some documentation for `TracingDebugInfo` (docstring, comments about `arg_names`, since it was not obvious to me that this would flatten the nonstatic arguments). Laying the ground for the unification of the old `api_util.debug_info` and `partial_eval.tracing_debug_info`: we rename the former to `api_util.tracing_debug_info`, we push inside the calls to `fun_sourceinfo` and `fun_signature` (which were done by the callers until now), and we rewrite the latter in terms of the former. We leave for a future PR the actual replacing of the latter with the former throughout. In the process of above, cleaned up the one case when `partial_eval.tracing_debug_info` received None for the `in_tree` and `out_tracer_thunk`. The function contained catchall exception clauses to handle those, but doing so it masked other places where we fail to collect debug info due to programming mistakes. E.g., in one place we passed a `WrappedFun` instead of a `Callable`, resulting in missing debugging info. Added more type declarations. Added a `state_test` with a failure to track debugging information, manifested with a leaked tracer without function provenance. Fixing this in a subsequent PR.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[better_errors] Improvements in propagation of debugging info,"Added some documentation for `TracingDebugInfo` (docstring, comments about `arg_names`, since it was not obvious to me that this would flatten the nonstatic arguments). Laying the ground for the unification of the old `api_util.debug_info` and `partial_eval.tracing_debug_info`: we rename the former to `api_util.tracing_debug_info`, we push inside the calls to `fun_sourceinfo` and `fun_signature` (which were done by the callers until now), and we rewrite the latter in terms of the former. We leave for a future PR the actual replacing of the latter with the former throughout. In the process of above, cleaned up the one case when `partial_eval.tracing_debug_info` received None for the `in_tree` and `out_tracer_thunk`. The function contained catchall exception clauses to handle those, but doing so it masked other places where we fail to collect debug info due to programming mistakes. E.g., in one place we passed a `WrappedFun` instead of a `Callable`, resulting in missing debugging info. Added more type declarations. Added a `state_test` with a failure to track debugging information, manifested with a leaked tracer without function provenance. Fixing this in a subsequent PR.",2025-01-15T23:09:26Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/25916
467,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Examine `partial.func` when trying to find function names)ï¼Œ å†…å®¹æ˜¯ (Make utils for reporting function name work with `functools.partial` by using the inner `.func` attribute if the object doesn't have a `__name__` attribute. `functools.partial` objects do not have `__name__` attributes by default.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Examine `partial.func` when trying to find function names,Make utils for reporting function name work with `functools.partial` by using the inner `.func` attribute if the object doesn't have a `__name__` attribute. `functools.partial` objects do not have `__name__` attributes by default.,2025-01-15T17:25:00Z,,closed,0,2,https://github.com/jax-ml/jax/issues/25908,Can you add a test for this please? (Or maybe modify an existing test if there are failing tests),(also please squash your commits)
1117,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Mosaic GPU] Allow querying layouts from a `FuncOp`'s block arguments if set.)ï¼Œ å†…å®¹æ˜¯ ([Mosaic GPU] Allow querying layouts from a `FuncOp`'s block arguments if set. The motivation behind this change is twofold: 1. it simplifies test writing (no need to produce arbitrary, manual, nonsplat    constants to produce arguments with a strided layout); 2. it'll allow running layout inference on different `FuncOp`s in isolation,    before inlining. While the primary motivation is to simplify test writing for upcoming changes, `2.` is useful if we ever intend to call functions whose body's layout we have inferred from other functions. It's not clear to me that we have a use case for that, but the theoretical benefit is worth pointing out. Crucially, layout inference does not set default layouts for `FuncOp`s, since the caller may choose a different layout for its arguments. As a result, there is also no layout inference rule for `func.FuncOp`.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[Mosaic GPU] Allow querying layouts from a `FuncOp`'s block arguments if set.,"[Mosaic GPU] Allow querying layouts from a `FuncOp`'s block arguments if set. The motivation behind this change is twofold: 1. it simplifies test writing (no need to produce arbitrary, manual, nonsplat    constants to produce arguments with a strided layout); 2. it'll allow running layout inference on different `FuncOp`s in isolation,    before inlining. While the primary motivation is to simplify test writing for upcoming changes, `2.` is useful if we ever intend to call functions whose body's layout we have inferred from other functions. It's not clear to me that we have a use case for that, but the theoretical benefit is worth pointing out. Crucially, layout inference does not set default layouts for `FuncOp`s, since the caller may choose a different layout for its arguments. As a result, there is also no layout inference rule for `func.FuncOp`.",2025-01-15T13:49:42Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25899
604,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(JAX cant find shardings (""AttributeError: 'UnspecifiedValue' object has no attribute 'is_fully_replicated'""))ï¼Œ å†…å®¹æ˜¯ ( Description Below is the code I ran and the stack trace.   pip freeze output:   System info (python version, jaxlib version, accelerator, etc.) **Versions** Python 3.10.14 jax==0.4.23 jaxlib==0.4.23 **Accelerator** TPU v5e8 Note, this code runs without error on TPU v3. **import jax; jax.print_environment_info()** )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"JAX cant find shardings (""AttributeError: 'UnspecifiedValue' object has no attribute 'is_fully_replicated'"")"," Description Below is the code I ran and the stack trace.   pip freeze output:   System info (python version, jaxlib version, accelerator, etc.) **Versions** Python 3.10.14 jax==0.4.23 jaxlib==0.4.23 **Accelerator** TPU v5e8 Note, this code runs without error on TPU v3. **import jax; jax.print_environment_info()** ",2025-01-14T20:58:32Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/25882,"Can you try with the latest jax version? 0.4.23 is from 2023. Also what TPU image version are you using? According to https://cloud.google.com/tpu/docs/runtimes, you should use `v2alphatpuv5lite`. Using the wrong image can sometimes cause weird problems.","Thank you! I updated to 0.4.38 with `pip install U ""jax[tpu]"" f https://storage.googleapis.com/jaxreleases/libtpu_releases.html` and this was fixed!"
1344,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Different results in bf16/f32 mixed precision in `conv_general_dilated` in jax2tf)ï¼Œ å†…å®¹æ˜¯ ( Description Hi Sirs.   I noticed Flax's Conv module was giving me different results between Python and C++ via TensorFlow Serving, which I could track and isolate to the following example. The problem is the most acute when I use `bfloat16` for calculations (which happens to be the type I am using most). The code below calculates a simple convolution in 5 different ways, and only `jax2tf` with `native_serialization=True` stands out. It seems almost that using `native_serialization=True` actually does calculations in f32, because the results match to all other methods if I set `dtype` to `f32`.   I am not sure it is a bug in earnest, just trying to understand the discrepancy between research (Jax) and production (TF Serving). In reallife fullsized model the discrepancy is quite significant because it accumulates between layers. It seems to be somehow related to CC(jax2tf: support mixedprecision inputs to `dot_general`) and CC([jax2tf] Fix for dot_general with different dtypes for graph serialization).   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Different results in bf16/f32 mixed precision in `conv_general_dilated` in jax2tf," Description Hi Sirs.   I noticed Flax's Conv module was giving me different results between Python and C++ via TensorFlow Serving, which I could track and isolate to the following example. The problem is the most acute when I use `bfloat16` for calculations (which happens to be the type I am using most). The code below calculates a simple convolution in 5 different ways, and only `jax2tf` with `native_serialization=True` stands out. It seems almost that using `native_serialization=True` actually does calculations in f32, because the results match to all other methods if I set `dtype` to `f32`.   I am not sure it is a bug in earnest, just trying to understand the discrepancy between research (Jax) and production (TF Serving). In reallife fullsized model the discrepancy is quite significant because it accumulates between layers. It seems to be somehow related to CC(jax2tf: support mixedprecision inputs to `dot_general`) and CC([jax2tf] Fix for dot_general with different dtypes for graph serialization).   System info (python version, jaxlib version, accelerator, etc.) ",2025-01-14T12:10:19Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/25873
1032,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(WARNING: jax does not provide the extra 'rocm')ï¼Œ å†…å®¹æ˜¯ ( Description I'm trying to install jax with ROCm support on bare metal. The instructions in the README (https://github.com/jaxml/jax?tab=readmeovfileinstructions) point to AMD's instructions which say to do:  However, when I do that, I get:  And indeed, when I list the devices after installation, the GPUs do not show up:  So either that ""rocm"" target is missing or the instructions on the AMD website are wrong. If it's the second case, that I need to open this bug report there. I tried forcing the installation of the jaxlib I found on https://repo.radeon.com/rocm/manylinux/rocmrel6.3.1/, but that didn't have any effect.  System info (python version, jaxlib version, accelerator, etc.)  Since this of course doesn't show and ROCM related info. Here's the ROCM part:  2x AMD MI210 (gfx90a)  ROCM 6.3.1)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,WARNING: jax does not provide the extra 'rocm'," Description I'm trying to install jax with ROCm support on bare metal. The instructions in the README (https://github.com/jaxml/jax?tab=readmeovfileinstructions) point to AMD's instructions which say to do:  However, when I do that, I get:  And indeed, when I list the devices after installation, the GPUs do not show up:  So either that ""rocm"" target is missing or the instructions on the AMD website are wrong. If it's the second case, that I need to open this bug report there. I tried forcing the installation of the jaxlib I found on https://repo.radeon.com/rocm/manylinux/rocmrel6.3.1/, but that didn't have any effect.  System info (python version, jaxlib version, accelerator, etc.)  Since this of course doesn't show and ROCM related info. Here's the ROCM part:  2x AMD MI210 (gfx90a)  ROCM 6.3.1",2025-01-13T15:50:13Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/25856,"Hi   the `rocm` extra was added after the most recent release, and will be part of JAX v0.5.0 which will likely be released later this week. Until then you could try running this instead:  (pinned to 0.4.35 because that's currently the only plugin version available on PyPI)","Oooh, sneaky! Yes, that worked. Thanks!"
325,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(ptxas unsupported version error)ï¼Œ å†…å®¹æ˜¯ ( Description Minimal example:  yields the error   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ptxas unsupported version error," Description Minimal example:  yields the error   System info (python version, jaxlib version, accelerator, etc.) ",2025-01-13T05:46:14Z,bug,closed,0,10,https://github.com/jax-ml/jax/issues/25853,xref https://github.com/jaxml/jax/issues/25344,From https://github.com/jaxml/jax/issues/25718: adding the python packaged ptxas to the PATH was an effective workaround.. ,"Thanks for the report and the update. This workaround _shouldn't_ be needed anymore, so let's look into what happened. Can you share all the steps that you used to install JAX? It might also be useful to know what the following outputs (before you change `PATH`): ","The output I get for this is:  I think that the problem is that I have `/usr/local/cuda12.2/bin` in my path, which points to outdated binaries? Removing this from my PATH fixes the problem.","Thanks! Yeah, but XLA _should_ search that `cuda_path/bin` first. Can you please let me know how you installed JAX (e.g. did you use conda or pip?)?","I used pip to install jax via `pip install ""jax[cuda12]""`!","Thanks. I am able to reproduce this issue, and I think I tracked down the place where XLA is ignoring JAX's CUDA path, but I'm not totally sure how to fix it. For now, I'm glad that you found a workaround, and I'll keep pushing on getting this fixed.",awesome happy to have helped!,This should be fixed in the next JAX release thanks to this change: https://github.com/openxla/xla/pull/21547,I'm going to close this because it seems to be fixed in my environment with v0.5.0. Please let us know if you run into it again with the newer versions. Thanks again for the report!
669,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tracer escaping in `linalg.solve` with `ensure_compile_time_eval` as of jax 0.4.36)ï¼Œ å†…å®¹æ˜¯ ( Description I am seeing unexpected jax tracer escape when using `jax.linalg.solve` in the `jax.ensure_compile_time_eval` context manager. This seems to occur for jax >= 0.4.36. Below is a simple reproduction.  This gives the following error:  I tried using the `jax.checking_leaks` context manager but it does not yield any additional info.  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Tracer escaping in `linalg.solve` with `ensure_compile_time_eval` as of jax 0.4.36," Description I am seeing unexpected jax tracer escape when using `jax.linalg.solve` in the `jax.ensure_compile_time_eval` context manager. This seems to occur for jax >= 0.4.36. Below is a simple reproduction.  This gives the following error:  I tried using the `jax.checking_leaks` context manager but it does not yield any additional info.  System info (python version, jaxlib version, accelerator, etc.) ",2025-01-11T17:34:21Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/25847,"Given the error and the version number, I'm sure this has something to do with the ""stackless"" change described as the first item in the 0.4.36 changelog: https://jax.readthedocs.io/en/latest/changelog.htmljax0436dec52024 I wonder if  has any suggestions here?"
359,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Got an error when offloading SplashAttention pallas call)ï¼Œ å†…å®¹æ˜¯ ( Description Reproducer:  Error traceback:   System info (python version, jaxlib version, accelerator, etc.) Tested on v5p )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,Got an error when offloading SplashAttention pallas call," Description Reproducer:  Error traceback:   System info (python version, jaxlib version, accelerator, etc.) Tested on v5p ",2025-01-10T19:36:33Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/25841
347,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Adding GPU paged attention kernel)ï¼Œ å†…å®¹æ˜¯ (Pallas kernel for paged ragged flash attention on GPU. It partitions the computation across the KV sequence similar to flash decoding.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Adding GPU paged attention kernel,Pallas kernel for paged ragged flash attention on GPU. It partitions the computation across the KV sequence similar to flash decoding.,2025-01-10T18:20:38Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/25839
326,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(build error  error: unknown type name 'svuint8_t')ï¼Œ å†…å®¹æ˜¯ ( Description   System info (python version, jaxlib version, accelerator, etc.) Python3.10.x ARM64)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,build error  error: unknown type name 'svuint8_t'," Description   System info (python version, jaxlib version, accelerator, etc.) Python3.10.x ARM64",2025-01-09T12:16:43Z,bug,open,0,3,https://github.com/jax-ml/jax/issues/25799,"Hi  thanks for including the traceback! It looks like this error is not directly releated to JAX, but rather is coming from the ARM compute library: https://github.com/ARMsoftware/ComputeLibrary/blob/32bcced2af7feea6969dd1d22e58d0718dc488e3/src/core/NEON/wrapper/traits.hL133 You might try filing an issue there, or looking for other help forums associated with those tools.",It may be that your compiler does not support SVE instructions. What compiler version are you using?,"Clang 12 is almost certainly too old to build JAX, I note. You probably want clang 17 or 18 at this point."
1249,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add a JVP rule for lax.linalg.tridiagonal_solve, fixing some bugs along the way)ï¼Œ å†…å®¹æ˜¯ (As noted in CC(Differentiation rule for tridiagonal_solve), `lax.linalg.tridiagonal_solve` couldn't be used with AD because it didn't have a JVP rule, so this PR adds that. While implementing this, I noticed and fixed some other `tridiagonal_solve` bugs: 1. The transpose rule for `tridiagonal_solve` was wrong (it should be `A^T @ ct`, not `A^1 @ ct`). 2. The Thomas algorithm implementation of the solve that is used on CPU on TPU gave the wrong results if the endpoints `dl[0]` and `du[1]` weren't explicitly zeroed. I ended up just reimplementing this function from scratch because the existing version was obfuscated by the handling of batch dimension and (imo) some confusing notation. The new version is more compact and only uses 2 scans instead of 3 which should be slightly more efficient, and it no longer requires the user to explicitly zero out the endpoints. 3. Not quite a bug, but I also added a bit more test coverage.  Fixes CC(Differentiation rule for tridiagonal_solve) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,"Add a JVP rule for lax.linalg.tridiagonal_solve, fixing some bugs along the way","As noted in CC(Differentiation rule for tridiagonal_solve), `lax.linalg.tridiagonal_solve` couldn't be used with AD because it didn't have a JVP rule, so this PR adds that. While implementing this, I noticed and fixed some other `tridiagonal_solve` bugs: 1. The transpose rule for `tridiagonal_solve` was wrong (it should be `A^T @ ct`, not `A^1 @ ct`). 2. The Thomas algorithm implementation of the solve that is used on CPU on TPU gave the wrong results if the endpoints `dl[0]` and `du[1]` weren't explicitly zeroed. I ended up just reimplementing this function from scratch because the existing version was obfuscated by the handling of batch dimension and (imo) some confusing notation. The new version is more compact and only uses 2 scans instead of 3 which should be slightly more efficient, and it no longer requires the user to explicitly zero out the endpoints. 3. Not quite a bug, but I also added a bit more test coverage.  Fixes CC(Differentiation rule for tridiagonal_solve) ",2025-01-08T20:57:04Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/25787,"> Just because we're looking at this, I thought I'd mention: there's probably an associative_scanlike way to implement these solves, like [Sec 1.4 here (https://www.cs.cmu.edu/~guyb/papers/Ble93.pdf), though there are probably more recent and more relevant papers about how to do it on GPUs in CUDA. Thanks! Yeah that's a good suggestion. I will note that on CPU and GPU we actually now lower to LAPACK and cuSPARSE respectively. These should have better numerics, but I must admit that I'm not 100% sure what algorithms are used there."
1069,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Cache initialization fails when a JAX Array is created before enabling local cache)ï¼Œ å†…å®¹æ˜¯ ( Description The persistent compilation cache in JAX fails to initialize if a JAX array is created prior to activating the local cache using jax.config.update. Removing the array creation line allows the cache to initialize correctly.  MRE with array allocation:  Full Log      This issue is present also with `ClassVar` default values if they are JAX NumPy arrays and with default arguments of functions. (See also https://github.com/amiiit/jaxsim/issues/322 and https://github.com/amiiit/jaxsim/pull/329)  MRE with `ClassVar`:   MRE with default arguments:  This was quite hard to spot for me, so I would expect a more clear error message if for some reason the cache cannot be initialized. Thank you for your help! FYI  95    System info (python version, jaxlib version, accelerator, etc.)  pip list     )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Cache initialization fails when a JAX Array is created before enabling local cache," Description The persistent compilation cache in JAX fails to initialize if a JAX array is created prior to activating the local cache using jax.config.update. Removing the array creation line allows the cache to initialize correctly.  MRE with array allocation:  Full Log      This issue is present also with `ClassVar` default values if they are JAX NumPy arrays and with default arguments of functions. (See also https://github.com/amiiit/jaxsim/issues/322 and https://github.com/amiiit/jaxsim/pull/329)  MRE with `ClassVar`:   MRE with default arguments:  This was quite hard to spot for me, so I would expect a more clear error message if for some reason the cache cannot be initialized. Thank you for your help! FYI  95    System info (python version, jaxlib version, accelerator, etc.)  pip list     ",2025-01-08T11:09:23Z,bug,closed,3,3,https://github.com/jax-ml/jax/issues/25768,I think this is working as expected: the compilation cache state must be enabled before backend initialization. Many of the JAX configuration options have similar requirements. I'm assigning  who knows more about this code path and may be able to confirm.,Thanks for reporting this issue. We have an idea of how to address it and will work on a fix. ,This issue has been resolved with PR CC(Fix cache init when JAX Array is created early (25768)). Closing now. 
1032,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`jax.lax.composite` and `jax.nn.softmax` composes strangely)ï¼Œ å†…å®¹æ˜¯ ( Description I want to export a jax function that uses softmax, and have that softmax be in a composite region. Like this one:   However, `f` cannot be invoked, let alone exported  or   raises:  Attempting to mark the `dim` arg as static with   yields a different error:   System info (python version, jaxlib version, accelerator, etc.) In [13]: import jax; jax.print_environment_info() jax:    0.4.39.dev20250103 jaxlib: 0.4.39.dev20250103 numpy:  2.0.1 python: 3.10.14 (main, Mar 21 2024, 11:21:31) [Clang 14.0.6 ] device info: cpu1, 1 local devices"" process_count: 1 platform: uname_result(system='Darwin', node='hanqmacbookpro.roam.internal', release='24.2.0', version='Darwin Kernel Version 24.2.0: Fri Dec  6 19:01:59 PST 2024; root:xnu11215.61.5~2/RELEASE_ARM64_T6000', machine='arm64'))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,`jax.lax.composite` and `jax.nn.softmax` composes strangely," Description I want to export a jax function that uses softmax, and have that softmax be in a composite region. Like this one:   However, `f` cannot be invoked, let alone exported  or   raises:  Attempting to mark the `dim` arg as static with   yields a different error:   System info (python version, jaxlib version, accelerator, etc.) In [13]: import jax; jax.print_environment_info() jax:    0.4.39.dev20250103 jaxlib: 0.4.39.dev20250103 numpy:  2.0.1 python: 3.10.14 (main, Mar 21 2024, 11:21:31) [Clang 14.0.6 ] device info: cpu1, 1 local devices"" process_count: 1 platform: uname_result(system='Darwin', node='hanqmacbookpro.roam.internal', release='24.2.0', version='Darwin Kernel Version 24.2.0: Fri Dec  6 19:01:59 PST 2024; root:xnu11215.61.5~2/RELEASE_ARM64_T6000', machine='arm64')",2025-01-08T05:36:08Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/25767,"Thanks for this report! There are two ways to get your example to work, depending on what behavior you want. 1. If you want your composite to take `axis` as an input, you need `axis` to be an ""attribute"" on your composite. This is indicated by using a keyword argument for `axis` as follows:  2. Alternatively, if you don't need to interpret the axis number on the backend, you could instead close over axis:  Hope this helps!",Also pinging  and  for visibility.,Solution 1. works great. Thanks ! To summarize: keyword arguments are interpreted as attributes and positional are inputs?,"> To summarize: keyword arguments are interpreted as attributes and positional are inputs? That's right, but this is a notable UX issue.. I'm not sure if there's a way to give a better error. An ideal solution would enforce the things that should be attributes must be specified as kwargs in python, i.e. ","You'd probably want `def softmax(x, /, *, axis):` to make `x` positiononly and `axis` keywordonly.","Thanks  and ! I'm also not sure about how we could raise a better error on the JAX side, but it's worth considering. I think it would be great to (at least!) add more details about this to the `lax.composite` docstring here: https://github.com/jaxml/jax/blob/5c097c8f62cc1233cb6ffa85ed4aefe310075706/jax/_src/lax/lax.pyL671L701",Thank you  for the feedback! I agree we should add documentation on this. I'll use jax.nn.softmax as an example in the doc. See https://github.com/jaxml/jax/pull/25792 for fix.
1144,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(XLA runtime error when taking grad + vmap + scan on GPU)ï¼Œ å†…å®¹æ˜¯ ( Description I'm encountering a bug that only appears when using JAX on the GPU and when taking the gradient of a function that composes vmap and scan.  The code that I've pasted below seems to work if when the device, vmap or scan are changed to CPU mode or for loops.  I've been able to recreate the bug on an a100 gpu (see the system info below) and also a 2080ti.  Any help would be greatly appreciated!  System info (python version, jaxlib version, accelerator, etc.) ```jax:    0.4.38 jaxlib: 0.4.38 numpy:  1.24.4 python: 3.10.15 (main, Oct  3 2024, 07:27:34) [GCC 11.2.0] device info: NVIDIA A100SXM480GB1, 1 local devices"" process_count: 1 platform: uname_result(system='Linux', node='gpu021', release='6.8.048generic', version=' CC(Unimplemented: binary integer op 'power')Ubuntu SMP PREEMPT_DYNAMIC Fri Sep 27 14:04:52 UTC 2024', machine='x86_64') $ nvidiasmi Tue Jan  7 21:00:10 2025        ++  ++)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,XLA runtime error when taking grad + vmap + scan on GPU," Description I'm encountering a bug that only appears when using JAX on the GPU and when taking the gradient of a function that composes vmap and scan.  The code that I've pasted below seems to work if when the device, vmap or scan are changed to CPU mode or for loops.  I've been able to recreate the bug on an a100 gpu (see the system info below) and also a 2080ti.  Any help would be greatly appreciated!  System info (python version, jaxlib version, accelerator, etc.) ```jax:    0.4.38 jaxlib: 0.4.38 numpy:  1.24.4 python: 3.10.15 (main, Oct  3 2024, 07:27:34) [GCC 11.2.0] device info: NVIDIA A100SXM480GB1, 1 local devices"" process_count: 1 platform: uname_result(system='Linux', node='gpu021', release='6.8.048generic', version=' CC(Unimplemented: binary integer op 'power')Ubuntu SMP PREEMPT_DYNAMIC Fri Sep 27 14:04:52 UTC 2024', machine='x86_64') $ nvidiasmi Tue Jan  7 21:00:10 2025        ++  ++",2025-01-07T21:08:56Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/25759,I forgot to add the actual error that I get: ,Minimized reproducer:  Output of //xla/tools:run_hlo_module: ,Input to LayoutNormalization: ,Reproducer unit test (probably not minimal yet): ,Minimized: ,https://github.com/openxla/xla/pull/21511 should fix this. Hopefully I didn't miss anything :).,"Great, thank you!"
465,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([shape_poly] Remove the deprecated PolyShape object for specifying symbolic dimensions)ï¼Œ å†…å®¹æ˜¯ ([shape_poly] Remove the deprecated PolyShape object for specifying symbolic dimensions PolyShape has been deprecated in January 2024. The constructor has been raising a DeprecationWarning since then.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[shape_poly] Remove the deprecated PolyShape object for specifying symbolic dimensions,[shape_poly] Remove the deprecated PolyShape object for specifying symbolic dimensions PolyShape has been deprecated in January 2024. The constructor has been raising a DeprecationWarning since then.,2025-01-07T11:40:00Z,,open,0,0,https://github.com/jax-ml/jax/issues/25751
1090,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(debugging/make_color_iter() returns the same index when the number of colors is 2 and do not use all the possible colors)ï¼Œ å†…å®¹æ˜¯ ( Description Hi JAX team,  I have noticed the following behaviors:  make_color_iter() returns the same color index when the number of colors is 2.  make_color_iter() does not use all possible colors (for example with num_rows=2 and num_cols=3, the function returns  [0, 4, 2, 0, 4, 2] as color indexes, so 3 colors among 6 possible. I am not sure if there is a logic behind the selection of the color index besides not selecting the color twice in a row and ensuring that the adjacent colors are not too similar, but this is a proposed quick fix that I think solves the issue:  I have tested several cases, and cannot see anything wrong in this possible fix. Please, let me know :) Best regards Jerome  System info (python version, jaxlib version, accelerator, etc.) jaxlib version = 0.4.33)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,debugging/make_color_iter() returns the same index when the number of colors is 2 and do not use all the possible colors," Description Hi JAX team,  I have noticed the following behaviors:  make_color_iter() returns the same color index when the number of colors is 2.  make_color_iter() does not use all possible colors (for example with num_rows=2 and num_cols=3, the function returns  [0, 4, 2, 0, 4, 2] as color indexes, so 3 colors among 6 possible. I am not sure if there is a logic behind the selection of the color index besides not selecting the color twice in a row and ensuring that the adjacent colors are not too similar, but this is a proposed quick fix that I think solves the issue:  I have tested several cases, and cannot see anything wrong in this possible fix. Please, let me know :) Best regards Jerome  System info (python version, jaxlib version, accelerator, etc.) jaxlib version = 0.4.33",2024-12-30T00:02:09Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/25695
933,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Crash with jit of ordered io_callback under shard_map and then no shard_map on CPU)ï¼Œ å†…å®¹æ˜¯ ( Description On my M1 Mac, with jax==0.4.39.dev20241223 and jaxlib==0.4.39.dev20241223 and Python 3.12.6, the following code:  crashes when running the last line, with:  When I update `test_empty_io_callback_under_shard_map` in `tests/pjit_test.py` at HEAD to match this code  see https://github.com/jaxml/jax/pull/25670  the test fails in GitHub's CI and on my Mac.  For example, see https://github.com/jaxml/jax/actions/runs/12470134880/job/34804668225?pr=25670 :  The code fails with the same error if the last two lines are switched  i.e., if the notshardmapped `_f` is run before the shardmapped `f`.  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Crash with jit of ordered io_callback under shard_map and then no shard_map on CPU," Description On my M1 Mac, with jax==0.4.39.dev20241223 and jaxlib==0.4.39.dev20241223 and Python 3.12.6, the following code:  crashes when running the last line, with:  When I update `test_empty_io_callback_under_shard_map` in `tests/pjit_test.py` at HEAD to match this code  see https://github.com/jaxml/jax/pull/25670  the test fails in GitHub's CI and on my Mac.  For example, see https://github.com/jaxml/jax/actions/runs/12470134880/job/34804668225?pr=25670 :  The code fails with the same error if the last two lines are switched  i.e., if the notshardmapped `_f` is run before the shardmapped `f`.  System info (python version, jaxlib version, accelerator, etc.) ",2024-12-23T17:09:42Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/25671,"Assigning , who may have ideas! Note however that due to the holiday it may take a while to look into this."
336,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add support for `axis_name` and `axis_index_groups` to `lax.ragged_all_to_all`)ï¼Œ å†…å®¹æ˜¯ (Add support for `axis_name` and `axis_index_groups` to `lax.ragged_all_to_all`)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Add support for `axis_name` and `axis_index_groups` to `lax.ragged_all_to_all`,Add support for `axis_name` and `axis_index_groups` to `lax.ragged_all_to_all`,2024-12-23T05:03:06Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25660
1469,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add mode='fan_geo_avg' to nn.initializers.variance_scaling)ï¼Œ å†…å®¹æ˜¯ (**Feature request:** Add `'fan_geo_avg'` as an option for the `mode` argument of `nn.initializers.variance_scaling`, in order to use the geometric mean rather than arithmetic mean of `fan_in` and `fan_out` for the denominator. Beyond Folklore: A Scaling Calculus for the Design and Initialization of ReLU Networks: > This scaling calculus results in a number of consequences, among them the fact that the **geometric mean** of the fanin and fanout, rather than the fanin, fanout, or arithmetic mean, should be used for the initialization of the variance of weights in a neural network. > > Initialization using the **geometricmean** of the fanin and fanout ensures a constant layer scaling factor throughout the network, aiding optimization. > > The use of **geometric initialization** results in an equally weighted diagonal, in contrast to the other initializations considered. SplitNets: Designing Neural Architectures for Efficient Distributed Computing on HeadMounted Systems: > Using **geometric average** allows us to find a better compromise between forward and backward passes and significantly improve training stability and final accuracy > > Our splitaware initialization adopts **geometric average** instead of arithme)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Add mode='fan_geo_avg' to nn.initializers.variance_scaling,"**Feature request:** Add `'fan_geo_avg'` as an option for the `mode` argument of `nn.initializers.variance_scaling`, in order to use the geometric mean rather than arithmetic mean of `fan_in` and `fan_out` for the denominator. Beyond Folklore: A Scaling Calculus for the Design and Initialization of ReLU Networks: > This scaling calculus results in a number of consequences, among them the fact that the **geometric mean** of the fanin and fanout, rather than the fanin, fanout, or arithmetic mean, should be used for the initialization of the variance of weights in a neural network. > > Initialization using the **geometricmean** of the fanin and fanout ensures a constant layer scaling factor throughout the network, aiding optimization. > > The use of **geometric initialization** results in an equally weighted diagonal, in contrast to the other initializations considered. SplitNets: Designing Neural Architectures for Efficient Distributed Computing on HeadMounted Systems: > Using **geometric average** allows us to find a better compromise between forward and backward passes and significantly improve training stability and final accuracy > > Our splitaware initialization adopts **geometric average** instead of arithme",2024-12-20T20:52:37Z,enhancement,closed,0,0,https://github.com/jax-ml/jax/issues/25649
1518,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add `replace: bool` argument to `random.categorical` to sample without replacement using Gumbel-top-k trick)ï¼Œ å†…å®¹æ˜¯ (`random.categorical` uses the Gumbelmax trick to sample with replacement. An extension of the Gumbelmax trick allows sampling _without_ replacement. As noted in this paper: > The wellknown GumbelMax trick for sampling from a categorical distribution can be extended to sample k elements without replacement. > The GumbelMax trick [...] allows sampling from the categorical distribution, simply by _perturbing_ the logprobability for each category by adding _independent_ Gumbel distributed noise and returning the category with maximum _perturbed logprobability_. [...] However, there is more: as was noted (in a blog post) by Vieira (2014), taking the **top k largest perturbed logprobabilities** (instead of the maximum, or _top 1_) yields a sample of size k from the categorical distribution _without replacement_. We refer to this extension of the GumbelMax trick as the _GumbelTopk_ trick. **Proposal:** Add a `replace: bool` argument to `random.categorical`. `replace=False` should use the GumbelTopk trick (replacing `argmax` with `top_k`) to sample without replacement. **Alternatives:** `random.choice` can sample without replacement, but uses a less efficient `cumsum`based method, and operates on probabilities rather th)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add `replace: bool` argument to `random.categorical` to sample without replacement using Gumbel-top-k trick,"`random.categorical` uses the Gumbelmax trick to sample with replacement. An extension of the Gumbelmax trick allows sampling _without_ replacement. As noted in this paper: > The wellknown GumbelMax trick for sampling from a categorical distribution can be extended to sample k elements without replacement. > The GumbelMax trick [...] allows sampling from the categorical distribution, simply by _perturbing_ the logprobability for each category by adding _independent_ Gumbel distributed noise and returning the category with maximum _perturbed logprobability_. [...] However, there is more: as was noted (in a blog post) by Vieira (2014), taking the **top k largest perturbed logprobabilities** (instead of the maximum, or _top 1_) yields a sample of size k from the categorical distribution _without replacement_. We refer to this extension of the GumbelMax trick as the _GumbelTopk_ trick. **Proposal:** Add a `replace: bool` argument to `random.categorical`. `replace=False` should use the GumbelTopk trick (replacing `argmax` with `top_k`) to sample without replacement. **Alternatives:** `random.choice` can sample without replacement, but uses a less efficient `cumsum`based method, and operates on probabilities rather th",2024-12-19T20:24:01Z,enhancement,open,0,5,https://github.com/jax-ml/jax/issues/25617,Sounds reasonable â€“Â is this something you'd like to contribute?,"Yes, I can create a PR for this."," After thinking about it more, since the semantics of shapes and axes would differ from those of sampling with replacement, I think it might be better to create a separate function. What would be a good name for it? Here are a few candidates:  `categorical_wor`  `categorical_without_replacement`  `categorical_no_replace`  `categorical_no_replacement`  `sample_without_replacement` Here's an example implementation:   (As expected, the sample frequencies become equal by the time we get to `k == logits.shape[axis]`.)",Can you say more about why this needs its own function?,"The output shape of sampling with replacement is `delete(logits.shape, axis)`. The output shape of sampling without replacement would be `insert(delete(logits.shape, axis), axis2, k)`, where `k` is the length of the sequential samplingwithoutreplacement process and `axis2` is some axis (possibly `axis`). Strictly speaking, it need not be its own function, but the argument signature as well as the documentation describing the output shape of `random.categorical` would need to be modified. Perhaps there's a risk of making the function too convoluted by overloading it with the task of doing two different things."
606,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add support for N-D FFTs with N>3)ï¼Œ å†…å®¹æ˜¯ (The current implementation of `jax.numpy.fft.fftn` (and related) don't support ND transforms with `N > 3` because that's all that XLA supports. However, we can (like in the numpy implementation) support higher dimensional transforms by repeatedly applying the supported low order transforms. In this PR, I add that logic for `N > 3`, but the behavior remains unchanged for `N <= 3` transforms.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add support for N-D FFTs with N>3,"The current implementation of `jax.numpy.fft.fftn` (and related) don't support ND transforms with `N > 3` because that's all that XLA supports. However, we can (like in the numpy implementation) support higher dimensional transforms by repeatedly applying the supported low order transforms. In this PR, I add that logic for `N > 3`, but the behavior remains unchanged for `N <= 3` transforms.",2024-12-19T12:13:18Z,pull ready,closed,2,2,https://github.com/jax-ml/jax/issues/25606,Might be worth a changelog entry.,> Might be worth a changelog entry. Good point! Added.
1182,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Surprising difference of output between NumPy's `float32` and JAX's `float32`)ï¼Œ å†…å®¹æ˜¯ ( Description Hello! I am not sure how to correctly title this issue, but I recently had an issue where the output of my some function would differ when `.jit` was applied to the function. Here is the simplified version:  Which yields:  I know Python float are 64bit precision, while JAX's are 32bit, and JIT can also optimize some operations, which could (I guess) change the order of operations and lead to different floating point results (?). I tried to force using float32, in both cases:  But the results are still *surprising*:  The NumPy version outputs the expected value, while the JAX variant doesn't. Am I doing something wrong? I could not find documentation about this, and I wonder if those kinds of things can be prevented, or should be expected? For this problem, I found a very simple fix:  that is, using multiplication instead of division.   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Surprising difference of output between NumPy's `float32` and JAX's `float32`," Description Hello! I am not sure how to correctly title this issue, but I recently had an issue where the output of my some function would differ when `.jit` was applied to the function. Here is the simplified version:  Which yields:  I know Python float are 64bit precision, while JAX's are 32bit, and JIT can also optimize some operations, which could (I guess) change the order of operations and lead to different floating point results (?). I tried to force using float32, in both cases:  But the results are still *surprising*:  The NumPy version outputs the expected value, while the JAX variant doesn't. Am I doing something wrong? I could not find documentation about this, and I wonder if those kinds of things can be prevented, or should be expected? For this problem, I found a very simple fix:  that is, using multiplication instead of division.   System info (python version, jaxlib version, accelerator, etc.) ",2024-12-19T09:00:28Z,bug,open,0,8,https://github.com/jax-ml/jax/issues/25601,"A simpler reproducer to this issue is:  It looks like numpy is using intermediate float64 arithmetic in float32 division and JAX float32 division uses strictly float32 arithmetic. To illustrate this, consider:  So, the issue not about a bug in JAX but rather belongs to https://jax.readthedocs.io/en/latest/faq.html ","Thanks for the reply ! That is quite surprising that NumPy doens't document that behavior (or I couldn't find it) on their Data type promotion page, but it isn't JAX's fault ^^'. But I think that documenting this in JAX's FAQ could be great :)",What exactly do you have in mind for documenting in the JAX FAQ?,"> What exactly do you have in mind for documenting in the JAX FAQ? That NumPy, as opposed to JAX, can promote data types for some computation, even if that is not documented, which might lead to difference between a JAX program and a NumPy program, even though you explicitly use the same precision. And probably link to this issue or provide example division. I am not sure how it is better to phrase this.",OK â€“ maybe something like that would fit as a bullet point here? https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.htmlmiscellaneousdivergencesfromnumpy,"> OK â€“ maybe something like that would fit as a bullet point here? https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.htmlmiscellaneousdivergencesfromnumpy Yes, I think this is a good place for that :)","FWIW, JAX and Numba are in the same boat regarding float32 semantics with respect to NumPy: https://numba.readthedocs.io/en/stable/reference/fpsemantics.html",Should I start a PR editing this page > OK â€“ maybe something like that would fit as a bullet point here? https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.htmlmiscellaneousdivergencesfromnumpy ?
568,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(WIP: no special treatment for ShapeDtypeStruct)ï¼Œ å†…å®¹æ˜¯ (I'm confused as to why we need to register this, because `ShapeDtypeStruct` should behave exactly the same as any ducktyped value with a `shape` and `dtype`. This registration recently led to the rollback of CC(Remove core.concrete_aval and replace with abstractify) because a downstream library was relying on this behavior in a dubious way.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,WIP: no special treatment for ShapeDtypeStruct,"I'm confused as to why we need to register this, because `ShapeDtypeStruct` should behave exactly the same as any ducktyped value with a `shape` and `dtype`. This registration recently led to the rollback of CC(Remove core.concrete_aval and replace with abstractify) because a downstream library was relying on this behavior in a dubious way.",2024-12-18T17:44:14Z,,open,0,1,https://github.com/jax-ml/jax/issues/25583,"You will need to run TGP to find the real failures. But users pass `SDS` in as an input to normal `jit` and expect it to work. And it does work because those inputs are unused and we DCE them. Last time I looked, the failures were a lot so I didn't really take any action but maybe you can!"
320,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Clarify documentation for output_offsets operand of ragged_all_to_all.)ï¼Œ å†…å®¹æ˜¯ (Clarify documentation for output_offsets operand of ragged_all_to_all.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Clarify documentation for output_offsets operand of ragged_all_to_all.,Clarify documentation for output_offsets operand of ragged_all_to_all.,2024-12-18T13:02:37Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25567
1340,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Is `jax.scipy.stats.norm.logcdf` twice differentiable?)ï¼Œ å†…å®¹æ˜¯ ( Description It seems that `jax.scipy.stats.norm.logcdf` is not twice differentiatiable.  I implemented the following function in `jax`.      where `jstat.norm.logcdf(., scale = 1/jnp.sqrt(2))` is used in placed of `jax.scipy.special.erfc`, due to numerical instability problems with the latter. **s_i is very small, approximately in the scale of ~1e5**.   I am trying to minimize this function (i.e. maximize log likelihood) using Newton's method. However, when computing the second derivative I found some wierd behavior. Drawing a simple graph with `matplotlib` for a single `lambda`, the graph appears to be convex.  !image However, drawing the second derivative, instead of being positive, as I expected (since it is a convex function), it appears that it oscillates wildly and blows up.  !image I think that I should add a custom derivative to `jstat.norm.logcdf`, but I am unsure of how to get started. What causes this numerical instability in the twicederivative calculation of `jstat.norm.logcdf`?  Any help would be appreciated.   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Is `jax.scipy.stats.norm.logcdf` twice differentiable?," Description It seems that `jax.scipy.stats.norm.logcdf` is not twice differentiatiable.  I implemented the following function in `jax`.      where `jstat.norm.logcdf(., scale = 1/jnp.sqrt(2))` is used in placed of `jax.scipy.special.erfc`, due to numerical instability problems with the latter. **s_i is very small, approximately in the scale of ~1e5**.   I am trying to minimize this function (i.e. maximize log likelihood) using Newton's method. However, when computing the second derivative I found some wierd behavior. Drawing a simple graph with `matplotlib` for a single `lambda`, the graph appears to be convex.  !image However, drawing the second derivative, instead of being positive, as I expected (since it is a convex function), it appears that it oscillates wildly and blows up.  !image I think that I should add a custom derivative to `jstat.norm.logcdf`, but I am unsure of how to get started. What causes this numerical instability in the twicederivative calculation of `jstat.norm.logcdf`?  Any help would be appreciated.   System info (python version, jaxlib version, accelerator, etc.) ",2024-12-18T08:20:44Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/25564,"`norm.logcdf` is implemented in terms of `special.log_ndtr`, which in turn is implemented using an asymptotic series. The second derivative plots you're showing look to me like the result of floating point rounding within that asymptotic series computation. One question to help diagnose this: how do the results look if you enable float64 precision?","Enabling `float64` precision by doing `jax.config.update(""jax_enable_x64"", True)` solved my issue perfectly, thank you! Here is the resulting Hessian and Gradient Graph for reference after enabling 64bit precision.  !image And here I was thinking I would have to make my own custom gradients ğŸ˜“. Thank you for your help!"
252,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Move ragged tests under a new class.)ï¼Œ å†…å®¹æ˜¯ (Move ragged tests under a new class.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Move ragged tests under a new class.,Move ragged tests under a new class.,2024-12-18T07:32:04Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25563
550,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Streamline some core.concrete_aval compute paths)ï¼Œ å†…å®¹æ˜¯ (Builds on CC(Cleanup: toward merging core.concrete_aval & xla.abstractify); brings `core.abstractify` and `core.concrete_aval` much closer. Though I've not benchmarked it, each of these new code paths should be more efficient than what it replaces. The two paths only differ now in handling of `ArrayImpl` and of `DArray`.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Streamline some core.concrete_aval compute paths,"Builds on CC(Cleanup: toward merging core.concrete_aval & xla.abstractify); brings `core.abstractify` and `core.concrete_aval` much closer. Though I've not benchmarked it, each of these new code paths should be more efficient than what it replaces. The two paths only differ now in handling of `ArrayImpl` and of `DArray`.",2024-12-17T18:15:31Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/25534
363,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Make gmm TPU kernel tests significantly cheaper)ï¼Œ å†…å®¹æ˜¯ (Make gmm TPU kernel tests significantly cheaper We were testing lots of very similar cases that did not really help a lot with coverage.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Make gmm TPU kernel tests significantly cheaper,Make gmm TPU kernel tests significantly cheaper We were testing lots of very similar cases that did not really help a lot with coverage.,2024-12-17T15:07:18Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25529
567,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Allow `lax.ragged_all_to_all` input and output operands to have different ragged dimension sizes.)ï¼Œ å†…å®¹æ˜¯ (Allow `lax.ragged_all_to_all` input and output operands to have different ragged dimension sizes. We need to guaranty that the outermost dimension of the output is big enough to fit all received elements, but it's not necessary for input and output outermost dimensions to be exactly equal.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Allow `lax.ragged_all_to_all` input and output operands to have different ragged dimension sizes.,"Allow `lax.ragged_all_to_all` input and output operands to have different ragged dimension sizes. We need to guaranty that the outermost dimension of the output is big enough to fit all received elements, but it's not necessary for input and output outermost dimensions to be exactly equal.",2024-12-16T21:00:31Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25518
982,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Clarification re: supported data types in `jax.linearize` and `jax.linear_transpose`)ï¼Œ å†…å®¹æ˜¯ (Hi All,  IIUC `jax.linearize` and `jax.linear_transpose` do not support taking derivatives with respect to integer values, i.e. there is no forwardmode analogy of `jax.grad(..., allow_int=True)`. In JAX forwardmode autodiff, integers are treated as constants.  When transposing the linearised function taking an integer input, the following error is raised >ValueError: linearized function called on tangent values inconsistent with the original primal values: got ShapedArray(int32[], weak_type=True) for primal aval ShapedArray(int32[], weak_type=True) Could we change that error message to something more informative by specialcasing integer inputs? I'm happy to do it and open a PR.  This issue came up downstream.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Clarification re: supported data types in `jax.linearize` and `jax.linear_transpose`,"Hi All,  IIUC `jax.linearize` and `jax.linear_transpose` do not support taking derivatives with respect to integer values, i.e. there is no forwardmode analogy of `jax.grad(..., allow_int=True)`. In JAX forwardmode autodiff, integers are treated as constants.  When transposing the linearised function taking an integer input, the following error is raised >ValueError: linearized function called on tangent values inconsistent with the original primal values: got ShapedArray(int32[], weak_type=True) for primal aval ShapedArray(int32[], weak_type=True) Could we change that error message to something more informative by specialcasing integer inputs? I'm happy to do it and open a PR.  This issue came up downstream.",2024-12-16T19:24:44Z,enhancement,closed,0,4,https://github.com/jax-ml/jax/issues/25517,"Assigning , who has been working on linearization improvements (cf. CC(More linearization fixes))","Yes it's a good idea to mention the expected tangent types. Here's a PR that does that: CC(Mention expected tangent aval in error message, see 25517.). With that, the error message now reads:  How does that look?","Yup, this is more informative! Thanks. ",Closed by CC(Clarification re: supported data types in `jax.linearize` and `jax.linear_transpose`)
564,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jax.random.choice(replace=True) samples 0 probability index)ï¼Œ å†…å®¹æ˜¯ ( Description `jax.random.choice(replace=True)` will sample 0 probability entry when the input array is large, and the average probability is low (`~1e07`):  The `numpy` counter part `np.random.choice` behaves correctly:  Seems like an unexpected behavior/bug?  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,jax.random.choice(replace=True) samples 0 probability index," Description `jax.random.choice(replace=True)` will sample 0 probability entry when the input array is large, and the average probability is low (`~1e07`):  The `numpy` counter part `np.random.choice` behaves correctly:  Seems like an unexpected behavior/bug?  System info (python version, jaxlib version, accelerator, etc.) ",2024-12-16T00:41:02Z,bug duplicate,open,0,1,https://github.com/jax-ml/jax/issues/25498,Thanks for the report â€“ this looks to be a duplicate of CC(jax.random.choice can return entries with zero probability).
522,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Mosaic GPU] Add layout inference and initial lowering for `vector.{load,store}`.)ï¼Œ å†…å®¹æ˜¯ ([Mosaic GPU] Add layout inference and initial lowering for `vector.{load,store}`. For now, the lowering only works for the strided fragmented layout. This is mostly an exercise in plugging in lowering rules using `FragmentedArray`, and will be expanded shortly.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,"[Mosaic GPU] Add layout inference and initial lowering for `vector.{load,store}`.","[Mosaic GPU] Add layout inference and initial lowering for `vector.{load,store}`. For now, the lowering only works for the strided fragmented layout. This is mostly an exercise in plugging in lowering rules using `FragmentedArray`, and will be expanded shortly.",2024-12-14T07:42:35Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25495
668,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(fix `gamma_p` in vmap-based impl rule mode)ï¼Œ å†…å®¹æ˜¯ (This was a regression introduced in CC(represent `random.key_impl` of builtin RNGs by canonical string name), which broke this check: https://github.com/jaxml/jax/blob/c73f3060997ac3b1c6de4f075111b684ea20b6ac/jax/_src/random.pyL1233L1234 It would be nice to remove `gamma_p` altogether if we can, and always back it by the vmapbased implementation that is guarded above. fixes CC(`jax.random.beta` 3 orders of magnitude slower from 0.4.36 on GPU))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,fix `gamma_p` in vmap-based impl rule mode,"This was a regression introduced in CC(represent `random.key_impl` of builtin RNGs by canonical string name), which broke this check: https://github.com/jaxml/jax/blob/c73f3060997ac3b1c6de4f075111b684ea20b6ac/jax/_src/random.pyL1233L1234 It would be nice to remove `gamma_p` altogether if we can, and always back it by the vmapbased implementation that is guarded above. fixes CC(`jax.random.beta` 3 orders of magnitude slower from 0.4.36 on GPU)",2024-12-13T23:36:42Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/25487
484,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add an experimental Cloud TPU presubmit job)ï¼Œ å†…å®¹æ˜¯ (Add an experimental Cloud TPU presubmit job This adds an experimental nonblocking presubmit job that will run a subset of TPU tests, focusing on frequently failing tests. The goal is to achieve comprehensive coverage while keeping the runtime around 10 minutes.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Add an experimental Cloud TPU presubmit job,"Add an experimental Cloud TPU presubmit job This adds an experimental nonblocking presubmit job that will run a subset of TPU tests, focusing on frequently failing tests. The goal is to achieve comprehensive coverage while keeping the runtime around 10 minutes.",2024-12-13T21:31:39Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25482
352,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(std::bad_cast)ï¼Œ å†…å®¹æ˜¯ ( Description  The above causes   Experimented on both Google Colab GPU & CPU  System info (python version, jaxlib version, accelerator, etc.) Google Colab CPU )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,std::bad_cast," Description  The above causes   Experimented on both Google Colab GPU & CPU  System info (python version, jaxlib version, accelerator, etc.) Google Colab CPU ",2024-12-13T13:14:54Z,bug,closed,0,5,https://github.com/jax-ml/jax/issues/25468,"I'm not able to reproduce this: on a Colab CPU runtime I ran the following two cells and saw no error:   It's possible your runtime got into some kind of strange state (perhaps you imported some code, then installed fresh packages, and then continued execution in the same environment?) Could you try restarting your runtime and see if the problem persists? Have you installed any other packages that may be relevant?",Turns out that having numpy >= 2.0 causes this issue. Downgrading numpy solves the issue.,"It's actually more complicated than that. NumPy is a module that is already loaded even in a completely empty colab session. If you upgrade it with `pip` without restarting the runtime, you end up in a weird state where you have pieces of NumPy 1 and NumPy 2 at the same time. I can reproduce by: * delete and create a completely fresh runtime * `!pip install jax==0.4.37 numpy==2.2.0` * run your code. You are in a somewhat bad state at this point, but JAX probably should fail more gracefully. But if you simply restart the runtime and rerun your code, you then have a consistent version of NumPy, and the problem goes away.",gdb backtrace: ,"Hmm. Any fix is going to be invasive, because basically ""NumPy isn't working correctly"" at this point. I'll fix the proximate cause of the `std::bad_cast` since that's a minor cleanup anyway, but then you immediately end up with a different weird failure. Closing because it's a losing battle to make JAX report good errors in the presence of a broken NumPy."
1321,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Slow convolution, many memory warnings)ï¼Œ å†…å®¹æ˜¯ ( Description Hi, I wrote some code to run a spatial filter on some frames of data. I'm basically convolving a batch of images with a ~30 x 30 image kernel. I'd like to take advantage of jit + vmap here (see code below) so that this code can run as fast as possible on GPU. When I do this, the execution is extremely slow (it takes 20 seconds to run the code below). Several warnings show up about memory allocation (included below). Based on the warnings it looks like under the hood a lot of time is being spent trying to figure out the best strategy for running the convolution  not sure though. Any and all help on this greatly appreciated!  Code:   Various Warnings/Messages:    System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.33 jaxlib: 0.4.33 numpy:  1.26.4 python: 3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 9.4.0] jax.devices (1 total, 1 local): [CudaDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='panda', release='5.4.0200generic', version=' CC(Support for numpy.take_along_axis)Ubuntu SMP Fri Sep 27 13:19:16 UTC 2024', machine='x86_64'))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"Slow convolution, many memory warnings"," Description Hi, I wrote some code to run a spatial filter on some frames of data. I'm basically convolving a batch of images with a ~30 x 30 image kernel. I'd like to take advantage of jit + vmap here (see code below) so that this code can run as fast as possible on GPU. When I do this, the execution is extremely slow (it takes 20 seconds to run the code below). Several warnings show up about memory allocation (included below). Based on the warnings it looks like under the hood a lot of time is being spent trying to figure out the best strategy for running the convolution  not sure though. Any and all help on this greatly appreciated!  Code:   Various Warnings/Messages:    System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.33 jaxlib: 0.4.33 numpy:  1.26.4 python: 3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 9.4.0] jax.devices (1 total, 1 local): [CudaDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='panda', release='5.4.0200generic', version=' CC(Support for numpy.take_along_axis)Ubuntu SMP Fri Sep 27 13:19:16 UTC 2024', machine='x86_64')",2024-12-13T04:33:00Z,bug,open,0,3,https://github.com/jax-ml/jax/issues/25461,"Thanks for the report! Some comments about this case: Can you comment on where your 20 second number is coming from? I can't exactly reproduce it locally, although compilation of this function is quite slow. If you're not already familiar with it, you might want to check out the JAX docs about microbenchmarking to work out where you're seeing a bottleneck. Running your test code on Colab, I find that compilation takes about 5 seconds, then the runtime after it is compiled is about 300400ms. It would be interesting to know if you consider that extremely slow for your use cases. If these compile times are a blocker for you, you might be ways to control XLA's autotuning behavior to trade off compile time vs. runtime performance, but I must admit I don't know how to do that off the top of my head!"," Thank you for the quick response! I'm inferring the 20 second number by running the following code snippet, following the jax benchmarking guide you linked:   The difference in wall times between the above executions is >20 seconds (24.1s vs. 92.9ms).  One thing I'll add is that if you increase the number of frames (i.e. instead of data = np.random(100, 500, 1400), you do data = np.random(1000, 500, 1400), so you are processing 1,000 frames instead of 100), the compilation time goes up drastically, to ~3.5 minutes! Perhaps this suggests that JAX and/or XLA is struggling to find the right convolution algorithm under the hood?  I guess a separate question is whether the algorithm that it ends up picking is the fastest one for execution on the GPU.  Happy to do some digging into any/all of the above, just let me know what you think!","We've found that CUDA aggressively attempts to determine the largest kernel that can maximally fit inside the GPU, and there are many such possible arrangements.  The arrangement possibilities only grow as the input (batch) size grows.  Another barrier to this are non multiple of 16 dimension sizes (that I've seen in practice, although large powers of 2 are usually preferred), and I notice your sizes are f32[100,1,501,1401].  (https://docs.nvidia.com/deeplearning/performance/dlperformanceconvolutional/index.htmlparamsperf)"
918,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Update the advanced autodiff tutorial and replace some vmap with grad)ï¼Œ å†…å®¹æ˜¯ (Hi everyone, I was reading the advanced autodiff tutorial, and starting from the section **""Hessianvector products with jax.gradofjax.grad""** I was getting progressively more confused; some explanations didn't make much sense to me. I spent some time trying to figure out what was wrong, and I believe in several places the authors mistakenly refer to `jax.vmap()` instead of `jax.grad()` . I made the change in the few places I found relevant, please see them in the diff editor and let me know what you think! I hope I found them correctly and sorry for not opening the issue first, I thought that in this case it is okay to have a ""hot fix"" pull request. Thank you! )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Update the advanced autodiff tutorial and replace some vmap with grad,"Hi everyone, I was reading the advanced autodiff tutorial, and starting from the section **""Hessianvector products with jax.gradofjax.grad""** I was getting progressively more confused; some explanations didn't make much sense to me. I spent some time trying to figure out what was wrong, and I believe in several places the authors mistakenly refer to `jax.vmap()` instead of `jax.grad()` . I made the change in the few places I found relevant, please see them in the diff editor and let me know what you think! I hope I found them correctly and sorry for not opening the issue first, I thought that in this case it is okay to have a ""hot fix"" pull request. Thank you! ",2024-12-12T17:09:42Z,pull ready,closed,0,6,https://github.com/jax-ml/jax/issues/25441,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Hey guys, it's been almost a month since I opened this PR, I hope you don't mind if I tag e.g.   (sorry if I come across as pushy, I definitely don't mean to: I do recognise that you might have a lot on your plates; so I am doing this just in case the PR somehow went under the radar)",Thanks for the ping! Can you start by completing the CLA as requested by the bot above?,"> Thanks for the ping! Can you start by completing the CLA as requested by the bot above? Gosh, I am sorry, I did sign the CLA on the day I submitted the PR, I just didn't see I have to reinvoke the check myself; should be all good now!",Can you rebase this PR onto the `main` branch? It looks like we're hitting some unrelated build issues.,"> Can you rebase this PR onto the `main` branch? It looks like we're hitting some unrelated build issues. Done, hope it works!"
1038,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Unclear documentation errors)ï¼Œ å†…å®¹æ˜¯ ( Description Just ran in to this trying to get code working. Said code's documentation referenced `xla_gpu_enable_async_collective*`, however it wasn't working so I came here to check documentation, to discover that it is confirmed at: https://github.com/jaxml/jax/blob/dda6b88864905987480c493db27ce9ede1699f10/docs/gpu_performance_tips.md?plain=1L49C1L49C47  Poked around for a while trying to figure out why the flag was being rejected until I found: https://github.com/openxla/xla/commit/4298a065f7b8381d690a95fe6d437cff037b3ff4 noting that the collection of `gpu_enable_async` flags are now deprecated for `disable` flags instead. Just figured fixing it here will save others the debugging trouble  the instruction lurking in other's codebases notwithstanding.  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Unclear documentation errors," Description Just ran in to this trying to get code working. Said code's documentation referenced `xla_gpu_enable_async_collective*`, however it wasn't working so I came here to check documentation, to discover that it is confirmed at: https://github.com/jaxml/jax/blob/dda6b88864905987480c493db27ce9ede1699f10/docs/gpu_performance_tips.md?plain=1L49C1L49C47  Poked around for a while trying to figure out why the flag was being rejected until I found: https://github.com/openxla/xla/commit/4298a065f7b8381d690a95fe6d437cff037b3ff4 noting that the collection of `gpu_enable_async` flags are now deprecated for `disable` flags instead. Just figured fixing it here will save others the debugging trouble  the instruction lurking in other's codebases notwithstanding.  System info (python version, jaxlib version, accelerator, etc.) ",2024-12-12T06:33:00Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/25430,Thanks for the catch! I made a PR to remove the deprecated flags from the docs.,PR has been merged; closing.
276,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add a simple ragged all-to-all Pallas TPU kernel.)ï¼Œ å†…å®¹æ˜¯ (Add a simple ragged alltoall Pallas TPU kernel.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Add a simple ragged all-to-all Pallas TPU kernel.,Add a simple ragged alltoall Pallas TPU kernel.,2024-12-11T20:42:39Z,,open,0,0,https://github.com/jax-ml/jax/issues/25421
394,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jax.lax: raise TypeError for mismatched dtypes)ï¼Œ å†…å®¹æ˜¯ (This currently results in a verbose jaxpr verifyer error. It looks like this regressed in CC(fix pow_p jvp rule at x=0. y=0). Current behavior:  Behavior after this PR )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,jax.lax: raise TypeError for mismatched dtypes,This currently results in a verbose jaxpr verifyer error. It looks like this regressed in CC(fix pow_p jvp rule at x=0. y=0). Current behavior:  Behavior after this PR ,2024-12-11T20:02:46Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/25419
606,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Replace np.array() with np.fromfile() to improve performance)ï¼Œ å†…å®¹æ˜¯ (This line of code first copies the file stream fh to a Python object and then immediately copies it to a NumPy object, which makes the first copy useless. I propose to remove the first copy by directly copying fh to a NumPy object. The patch is as follows.  +return np.array(array.array(""B"", fh.read()), dtype=np.uint8)  return np.fromfile(fh.read(), dtype=np.uint8))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Replace np.array() with np.fromfile() to improve performance,"This line of code first copies the file stream fh to a Python object and then immediately copies it to a NumPy object, which makes the first copy useless. I propose to remove the first copy by directly copying fh to a NumPy object. The patch is as follows.  +return np.array(array.array(""B"", fh.read()), dtype=np.uint8)  return np.fromfile(fh.read(), dtype=np.uint8)",2024-12-11T19:11:13Z,enhancement,open,0,0,https://github.com/jax-ml/jax/issues/25418
1026,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Latency Hiding Scheduler not working with jax 0.4.35)ï¼Œ å†…å®¹æ˜¯ ( Description More of an XLA problem (rather than Jax). It looks like the latency hiding scheduler is not working correctly in jax 0.4.35. I am running my code on a single DGX node with 8 H100. I am only passing the following flags to XLA: `XLA_FLAGS=xla_gpu_enable_latency_hiding_scheduler=true xla_gpu_enable_triton_gemm=false` During compilation of my code, I get the following error:  I also tried to modify the value for `config.parallel_collective_overlap_limit` to check if it would make any difference by modifying the `XLA_FLAGS` var to:  but in that case I get:  If I downgrade to jax 0.4.34, the error does not occur.   System info (python version, jaxlib version, accelerator, etc.) jax: 0.4.35 jaxlib: 0.4.34 numpy: 2.0.2 python: 3.11.11 (main, Dec 6 2024, 20:02:44) [Clang 18.1.8 ])è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Latency Hiding Scheduler not working with jax 0.4.35," Description More of an XLA problem (rather than Jax). It looks like the latency hiding scheduler is not working correctly in jax 0.4.35. I am running my code on a single DGX node with 8 H100. I am only passing the following flags to XLA: `XLA_FLAGS=xla_gpu_enable_latency_hiding_scheduler=true xla_gpu_enable_triton_gemm=false` During compilation of my code, I get the following error:  I also tried to modify the value for `config.parallel_collective_overlap_limit` to check if it would make any difference by modifying the `XLA_FLAGS` var to:  but in that case I get:  If I downgrade to jax 0.4.34, the error does not occur.   System info (python version, jaxlib version, accelerator, etc.) jax: 0.4.35 jaxlib: 0.4.34 numpy: 2.0.2 python: 3.11.11 (main, Dec 6 2024, 20:02:44) [Clang 18.1.8 ]",2024-12-11T13:59:43Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/25404,"Thanks for the report! It's possible that someone on this repo will be able to provide suggestions, but (as you mention!) you might get more milage asking this question on the XLA repository if you haven't already: https://github.com/openxla/xla/issues"
319,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Pallas] Remove `grid=1` in tests)ï¼Œ å†…å®¹æ˜¯ ([Pallas] Remove `grid=1` in tests Remove `grid=1` in tests because it's the same as not specifying `grid`.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[Pallas] Remove `grid=1` in tests,[Pallas] Remove `grid=1` in tests Remove `grid=1` in tests because it's the same as not specifying `grid`.,2024-12-11T13:22:20Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25402
1469,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(XLA-introduced copies supersede `lax.optimization_barrier`)ï¼Œ å†…å®¹æ˜¯ ( Description Almost certainly an XLA bug and happy to report there if so. Consider the function  Since XLA has control over scheduling, for efficiency it should schedule the slice first and then the inplace update, to avoid an unnecessary copy. However, on specifically the CPU backend it chooses to copy twice instead, generating  (I'm not sure why it needs to make _two_ copies here instead of just one, but the important part is that it copies at all.) By the semantics of `lax.optimization_barrier`, I would expect that introducing an explicit dependency of `x` on `y` would force the slice to happen first, and then the liveliness analysis will kick in and remove the copies.  However, what ends up happening is XLA still introduces copies and reorders the calls, so the generated code is the same as the one shown above. This seems to violate the scheduling control one expects from `optimization_barrier`. Note that for this particular example, setting the XLA flag `xla_cpu_copy_insertion_use_region_analysis=true` removes the copy and generates  as expected, with or without `optimization_barrier`. Also, using a GPU device generates the copyless  also with or without `optimization_barrier`.  Some miscellaneous related q)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,XLA-introduced copies supersede `lax.optimization_barrier`," Description Almost certainly an XLA bug and happy to report there if so. Consider the function  Since XLA has control over scheduling, for efficiency it should schedule the slice first and then the inplace update, to avoid an unnecessary copy. However, on specifically the CPU backend it chooses to copy twice instead, generating  (I'm not sure why it needs to make _two_ copies here instead of just one, but the important part is that it copies at all.) By the semantics of `lax.optimization_barrier`, I would expect that introducing an explicit dependency of `x` on `y` would force the slice to happen first, and then the liveliness analysis will kick in and remove the copies.  However, what ends up happening is XLA still introduces copies and reorders the calls, so the generated code is the same as the one shown above. This seems to violate the scheduling control one expects from `optimization_barrier`. Note that for this particular example, setting the XLA flag `xla_cpu_copy_insertion_use_region_analysis=true` removes the copy and generates  as expected, with or without `optimization_barrier`. Also, using a GPU device generates the copyless  also with or without `optimization_barrier`.  Some miscellaneous related q",2024-12-11T10:50:46Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/25399,"Yes, I think this would be better reported on the XLA github issue tracker. There's current no JAX way to control the HLO schedule, but that's something we're actively looking into adding as a way to control communication/compute overlap.",Opened https://github.com/openxla/xla/issues/20440 on the XLA side. Should this issue be closed?,"Yeah, let's track this there."
1286,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jax.full allocates memory on the wrong device)ï¼Œ å†…å®¹æ˜¯ ( Description When I use jax.numpy.full, it allocates memory on TPU, even when instructed to allocate it on the CPU if the memory requested is smaller then the free memory on the TPU, the memory is deallocated and reallocated on the CPU instead however if the memory requested is more than the free memory, the device crashes with OOM in my snippet, it's an extremely edge case where I am trying to allocate 40GB, which normally never happens, however, during training and execution, allocating jax buffers on CPU may be the difference for some training pipelines fitting withing specific memory constraints or not. snippet:   System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.37 jaxlib: 0.4.36 numpy:  1.26.4 python: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] device info: TPU v6 lite4, 4 local devices"" process_count: 1 platform: uname_result(system='Linux', node='t1vn7691bba4w0', release='6.8.01015gcp', version=' CC(Implement np.repeat for scalar repeats.)~22.04.1Ubuntu SMP Tue Sep  3 16:11:52 UTC 2024', machine='x86_64'))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jax.full allocates memory on the wrong device," Description When I use jax.numpy.full, it allocates memory on TPU, even when instructed to allocate it on the CPU if the memory requested is smaller then the free memory on the TPU, the memory is deallocated and reallocated on the CPU instead however if the memory requested is more than the free memory, the device crashes with OOM in my snippet, it's an extremely edge case where I am trying to allocate 40GB, which normally never happens, however, during training and execution, allocating jax buffers on CPU may be the difference for some training pipelines fitting withing specific memory constraints or not. snippet:   System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.37 jaxlib: 0.4.36 numpy:  1.26.4 python: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] device info: TPU v6 lite4, 4 local devices"" process_count: 1 platform: uname_result(system='Linux', node='t1vn7691bba4w0', release='6.8.01015gcp', version=' CC(Implement np.repeat for scalar repeats.)~22.04.1Ubuntu SMP Tue Sep  3 16:11:52 UTC 2024', machine='x86_64')",2024-12-11T08:34:09Z,bug,open,0,2,https://github.com/jax-ml/jax/issues/25396,"I think, the best solution here is to do the following to make sure all ops inside `jnp.full` run on CPU: ","> I think, the best solution here is to do the following to make sure all ops inside `jnp.full` run on CPU: >  >  That's a very good workaround! However I think the behaviour of jnp.full and similar API needs to be more faithful to their arguments then their global environments, especially if it can lead to OOM crashes"
569,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯( Unable to parse backend config for custom call: Could not convert JSON string to proto: Expected : between key:value pair.)ï¼Œ å†…å®¹æ˜¯ ( Description The following test code produces the aforementioned error when using JAX v0.4.37 (its works error free before this). This is running on Ubuntu 24.04 with Python 3.12 and using CUDA 12.4    System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm, Unable to parse backend config for custom call: Could not convert JSON string to proto: Expected : between key:value pair.," Description The following test code produces the aforementioned error when using JAX v0.4.37 (its works error free before this). This is running on Ubuntu 24.04 with Python 3.12 and using CUDA 12.4    System info (python version, jaxlib version, accelerator, etc.) ",2024-12-10T23:54:02Z,bug,open,1,4,https://github.com/jax-ml/jax/issues/25389,"Thanks for the report. This is known and I believe that's a warning, not an error. Are you seeing a runtime exception somewhere, or just extra logging?  ",Just the warning / extra logging.,"Hi,  Is there any way to suppress this warning?","This should be fixed in the nightly jaxlib. I don't know if there a way to suppress the warning otherwise, unfortunately."
1471,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Pallas Kernel Expected Output Shape Error Using Grids On TPU)ï¼Œ å†…å®¹æ˜¯ ( Description  ğŸ› Bug I am trying to write a custom Pallas kernel to use it in TPU. I am using blocking method to keep my kernel from going OOM. However, when I am using grids, it seems that I get kernel problems with the expected output and input shapes. It seems that the chunking / splitting of the input does not perform as expected. I checked that my code indeed has the right shapes, grid and indexing method. However, the kernel itself is getting wrong input.  I think it may be bug in how the TPU is handling the chunking in pallas kernels, but I am not sure. Any help would be appreciated!  To Reproduce I am attaching here tests for replication. You can see that only the tests with original input tensors larger than block size fails.  My Kernel Code   Debug Output And Stack Trace RuntimeError: Bad StatusOr access: INVALID_ARGUMENT: Mosaic failed to compile TPU kernel: Failed to verify layout for Mosaic kernel operand 1: XLA layout does not match MLIR layout for an operand of shape f32[192]: expected {0:T(128)}, got {0:T(256)} But printing the values I provide to the pallas_call Out shape: ShapeDtypeStruct(shape=(192,), dtype=float32), grid: 2, block_shape: (128,)  Expected behavior The tests should not fail. Whe)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",text chunking,Pallas Kernel Expected Output Shape Error Using Grids On TPU," Description  ğŸ› Bug I am trying to write a custom Pallas kernel to use it in TPU. I am using blocking method to keep my kernel from going OOM. However, when I am using grids, it seems that I get kernel problems with the expected output and input shapes. It seems that the chunking / splitting of the input does not perform as expected. I checked that my code indeed has the right shapes, grid and indexing method. However, the kernel itself is getting wrong input.  I think it may be bug in how the TPU is handling the chunking in pallas kernels, but I am not sure. Any help would be appreciated!  To Reproduce I am attaching here tests for replication. You can see that only the tests with original input tensors larger than block size fails.  My Kernel Code   Debug Output And Stack Trace RuntimeError: Bad StatusOr access: INVALID_ARGUMENT: Mosaic failed to compile TPU kernel: Failed to verify layout for Mosaic kernel operand 1: XLA layout does not match MLIR layout for an operand of shape f32[192]: expected {0:T(128)}, got {0:T(256)} But printing the values I provide to the pallas_call Out shape: ShapeDtypeStruct(shape=(192,), dtype=float32), grid: 2, block_shape: (128,)  Expected behavior The tests should not fail. Whe",2024-12-10T15:18:25Z,bug type:Bug,open,0,3,https://github.com/jax-ml/jax/issues/25379,"I think this is actually related to handling of 1D arrays in Pallas. Usually, arrays in our kernel are 2D. I'm going to need some time to track down what exactly the problem is, but your code should run if you add a leading (1,) dimension to the block shape and input array shape.","> I think this is actually related to handling of 1D arrays in Pallas. Usually, arrays in our kernel are 2D. >  > I'm going to need some time to track down what exactly the problem is, but your code should run if you add a leading (1,) dimension to the block shape and input array shape. Ok it resolved the problem! However, I notice that training is much slower when using the kernels. The kernels perform bitwise operations. Could it be that these bitwise operations are being executed on the CPU, even though I wrapped them in a kernel and further wrapped it in torch_xla? Or do I need to change something in my function for it to be more optimized for tpus?  ","Pallas lowers to Mosaic which doesn't have a CPU backend, so it's impossible that it runs on CPU unintentionally. The only way to run on CPU is to pass in interpret=True, but in that case you wouldn't see Mosaic error messages. Without seeing the rest of your code, my guess is that it's related to fusion. When you run your code without custom kernels, XLA will automatically try to fuse neighboring ops together into it's own generated kernels so it can avoid memory copies between HBM  VMEM. However, when you use a pallas_call, XLA doesn't know how to fuse into a custom kernel, so you get stuck with redundant copies. Since your kernel is elementwise, it's probably memorybound, meaning that most of the time spent in the kernel is waiting for memory copies to finish and not actually doing computation. So fusion would actually help a lot here if this is the case. The solution to this problem is to fold in the neighboring operations into the kernel. For example, if you're rounding followed by a bf16 matmul, then you should do the rounding inside the matmul kernel and not as a standalone kernel."
749,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Introduce `lax.ragged_all_to_all` primitive)ï¼Œ å†…å®¹æ˜¯ (Introduce `lax.ragged_all_to_all` primitive This version emits a StableHLO custom call. The test outputs the following MLIR module:  For now, the API assumes `split_axis` and `concat_axis` of `all_to_all` to be the outermost (ragged) dim, and `axis_index_groups` is default to all replicas (e.g. there is only one group and covers all axis indices aka iota like the example above). The current API is inspired from https://www.mpich.org/static/docs/v3.1/www3/MPI_Alltoallv.html which essentially also does a ragged all to all.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Introduce `lax.ragged_all_to_all` primitive,"Introduce `lax.ragged_all_to_all` primitive This version emits a StableHLO custom call. The test outputs the following MLIR module:  For now, the API assumes `split_axis` and `concat_axis` of `all_to_all` to be the outermost (ragged) dim, and `axis_index_groups` is default to all replicas (e.g. there is only one group and covers all axis indices aka iota like the example above). The current API is inspired from https://www.mpich.org/static/docs/v3.1/www3/MPI_Alltoallv.html which essentially also does a ragged all to all.",2024-12-10T01:57:57Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25370
417,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(vmap support for optimization barrier)ï¼Œ å†…å®¹æ˜¯ (Calling jax.lax.optimization_barrier inside of a vmap'd function yields: `NotImplementedError: Batching rule for 'optimization_barrier' not implemented` This would be superuseful if supported, thanks!)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,vmap support for optimization barrier,"Calling jax.lax.optimization_barrier inside of a vmap'd function yields: `NotImplementedError: Batching rule for 'optimization_barrier' not implemented` This would be superuseful if supported, thanks!",2024-12-10T00:13:55Z,enhancement,closed,0,1,https://github.com/jax-ml/jax/issues/25365,Fixed by https://github.com/jaxml/jax/commit/944d822ce64450f698bd9b4e8236421ade401e84
386,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Adding more tests for multi-head attention)ï¼Œ å†…å®¹æ˜¯ (Adding more test coverage to the multihead attention pallas kernel. jax==0.4.33 caused the kernel to segfault with an untested configuration and it went undetected.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Adding more tests for multi-head attention,Adding more test coverage to the multihead attention pallas kernel. jax==0.4.33 caused the kernel to segfault with an untested configuration and it went undetected.,2024-12-09T20:55:40Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/25361
689,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add utility script and env for running the CI scripts under Docker)ï¼Œ å†…å®¹æ˜¯ (Add utility script and env for running the CI scripts under Docker This commit adds the scripts and envs that set up a Docker container named ""jax"". It is used in internal JAX CI jobs that handle building and publishing JAX artifacts to PyPI and/or GCS. While GitHub action workflows use the same Docker images, they do not run `run_docker_container.sh` script as they leverage builtin containerization features to run jobs within a container.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Add utility script and env for running the CI scripts under Docker,"Add utility script and env for running the CI scripts under Docker This commit adds the scripts and envs that set up a Docker container named ""jax"". It is used in internal JAX CI jobs that handle building and publishing JAX artifacts to PyPI and/or GCS. While GitHub action workflows use the same Docker images, they do not run `run_docker_container.sh` script as they leverage builtin containerization features to run jobs within a container.",2024-12-09T17:52:32Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25356
334,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([mosaic_gpu] Allow calling `reduce_sum` on a fragmented array in splat layout)ï¼Œ å†…å®¹æ˜¯ ([mosaic_gpu] Allow calling `reduce_sum` on a fragmented array in splat layout)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[mosaic_gpu] Allow calling `reduce_sum` on a fragmented array in splat layout,[mosaic_gpu] Allow calling `reduce_sum` on a fragmented array in splat layout,2024-12-09T14:26:40Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25343
1008,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Jax][Pallas][Mosaic] Implement platform dependent diag, with branch selection driven by constant prop in mosaic lowering.)ï¼Œ å†…å®¹æ˜¯ ([Jax][Pallas][Mosaic] Implement platform dependent diag, with branch selection driven by constant prop in mosaic lowering. This CL builds out a simple sketch of constant prop by construction in mosaic  we walk the graph up from cond, collecting the values and either const propping or failing out of const prop. Failure out of const prop is not a bug, but hitting an unimplemented const prop func is for now, in order to drive better coverage.  This then allows us to pick a single branch, and ignore branches which do not have a viable mosaic implementation. And, finally, for diag, this means we can replace the initial gatherdependent implementation in lax with a mosaic specific one that avoids gather.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,"[Jax][Pallas][Mosaic] Implement platform dependent diag, with branch selection driven by constant prop in mosaic lowering.","[Jax][Pallas][Mosaic] Implement platform dependent diag, with branch selection driven by constant prop in mosaic lowering. This CL builds out a simple sketch of constant prop by construction in mosaic  we walk the graph up from cond, collecting the values and either const propping or failing out of const prop. Failure out of const prop is not a bug, but hitting an unimplemented const prop func is for now, in order to drive better coverage.  This then allows us to pick a single branch, and ignore branches which do not have a viable mosaic implementation. And, finally, for diag, this means we can replace the initial gatherdependent implementation in lax with a mosaic specific one that avoids gather.",2024-12-06T23:38:15Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25324
362,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([mosaic_gpu] Emit a slightly more informative error message in `FragmentedArray._pointwise`)ï¼Œ å†…å®¹æ˜¯ ([mosaic_gpu] Emit a slightly more informative error message in `FragmentedArray._pointwise`)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[mosaic_gpu] Emit a slightly more informative error message in `FragmentedArray._pointwise`,[mosaic_gpu] Emit a slightly more informative error message in `FragmentedArray._pointwise`,2024-12-06T13:51:43Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25312
449,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([pallas:mosaic_gpu] `FragmentedArray.reduce_sum` now returns a `FragmentedArray`)ï¼Œ å†…å®¹æ˜¯ ([pallas:mosaic_gpu] `FragmentedArray.reduce_sum` now returns a `FragmentedArray` This aligns it with the `reduce` method and also makes it clear that the reduction always produces a scalar.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[pallas:mosaic_gpu] `FragmentedArray.reduce_sum` now returns a `FragmentedArray`,[pallas:mosaic_gpu] `FragmentedArray.reduce_sum` now returns a `FragmentedArray` This aligns it with the `reduce` method and also makes it clear that the reduction always produces a scalar.,2024-12-06T11:43:35Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25305
687,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([pallas] fix jumble test flakiness)ï¼Œ å†…å®¹æ˜¯ ([pallas] fix jumble test flakiness * Enable interpret mode in tests * Ensure that the kernel is run multiple times where weve seen data corruption * Use masked comparison  prior comparison was reading garbage data as we were basically relying on past behavior of how uninitialized memory was behaving. * This was being hidden by a cache, where the interpret test, which always has 0.0 for uninitialized memory was being hit first, where TPU does not have the same behavior.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[pallas] fix jumble test flakiness,"[pallas] fix jumble test flakiness * Enable interpret mode in tests * Ensure that the kernel is run multiple times where weve seen data corruption * Use masked comparison  prior comparison was reading garbage data as we were basically relying on past behavior of how uninitialized memory was behaving. * This was being hidden by a cache, where the interpret test, which always has 0.0 for uninitialized memory was being hit first, where TPU does not have the same behavior.",2024-12-05T22:14:07Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25295
477,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(array API: improve test coverage)ï¼Œ å†…å®¹æ˜¯ (`max_examples` defaults to 200; previously there were some performance issues in the test suite that led to timeouts with the full run, so we limited it to 5 examples. That seems to have been fixed, and we can now do the full test run in about 3 minutes of CI time.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,array API: improve test coverage,"`max_examples` defaults to 200; previously there were some performance issues in the test suite that led to timeouts with the full run, so we limited it to 5 examples. That seems to have been fixed, and we can now do the full test run in about 3 minutes of CI time.",2024-12-05T20:35:34Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/25294
786,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(unexpected `vmap` error due to commit `c36e1f7`)ï¼Œ å†…å®¹æ˜¯ ( Description Hi,   and I noticed a bug in our tests here that occurs in the latest JAX version. After doing gitbisect, we found the bad commit to be: c36e1f7c1ad4782060cbc8e8c596d85dfb83986f. Here's the traceback with `JAX_TRACEBACK_FILTERING=off`:    It might be that the bug is coming from transformations created by lineax, as the test doesn't fail when using the CG solver from JAX, (the test still fails, but only because of the precision, not the above `ValueError`). Code to reproduce:   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,unexpected `vmap` error due to commit `c36e1f7`," Description Hi,   and I noticed a bug in our tests here that occurs in the latest JAX version. After doing gitbisect, we found the bad commit to be: c36e1f7c1ad4782060cbc8e8c596d85dfb83986f. Here's the traceback with `JAX_TRACEBACK_FILTERING=off`:    It might be that the bug is coming from transformations created by lineax, as the test doesn't fail when using the CG solver from JAX, (the test still fails, but only because of the precision, not the above `ValueError`). Code to reproduce:   System info (python version, jaxlib version, accelerator, etc.) ",2024-12-05T17:22:41Z,bug,open,0,10,https://github.com/jax-ml/jax/issues/25289,Thanks for the clear report!," could you minimize this? There's a lot going on in this code that we aren't familiar with, and it's much harder for us to minimize unfamiliar code than for you to minimize your own code. Think of the time for us to debug this as exponential in the length of the repro you give us. > It might be that the bug is coming from transformations created by lineax, Are any of lineax's tests failing?","> Are any of lineax's tests failing? No, all of them are passing with the latest version of JAX/lineax.","thanks a lot   for taking a look! Here's a simpler example crafted by  and Rythme demonstrating the problem, which arises indeed from `lineax` (tagging kidger ), leading to the same `vmap` error: ","Yeah, I've also been seeing widespread failures in Diffrax's test suite, due to what looks like a totally different vmap failure. I've spent most of today digging through this and haven't identified a root cause yet. It might take a while to update the JAX ecosystem to be compatible with this version of JAX.","Okay, I think I've identified the root cause: with the latest changes, the batch interpreter has become a dynamic trace, i.e. it calls batch rules when it previously wouldn't. This meant that a lot of arrays were having their nonbatch dimensions now being turned into batch dimensions! With that problem identified it's been a relatively simple matter to update a couple of batching rules in Equinox to handle this new calling case appropriately.   can you try https://github.com/patrickkidger/equinox/pull/907 on your full example / on your tests? If it passes then I'll do a new release of Equinox that is compatible with latest JAX.","Nice find kidger ! Thatâ€™s right, actually everything is a dynamic tracer now. No more automatic rulesonlycalledbasedondatadependence, though rules themselves can choose to behave based on dependence. I believe it gives rules strictly more power/expressiveness. ","Hey , can we consolidate this kind of knowledge into an updated version of Autodidax?",">   can you try https://github.com/patrickkidger/equinox/pull/907 on your full example / on your tests? If it passes then I'll do a new release of Equinox that is compatible with latest JAX. Works great, thanks!"," yes, good idea, thatâ€™s our plan! Cc  "
556,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Python crashes when trying to compute the gradient of jnp.take)ï¼Œ å†…å®¹æ˜¯ ( Description When running the following script:  Python crashes with the following error:  I ran into this error while trying to use `optax`'s `softmax_cross_entropy_with_integer_labels` and found that `jnp.take` on this line was causing the error.  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Python crashes when trying to compute the gradient of jnp.take," Description When running the following script:  Python crashes with the following error:  I ran into this error while trying to use `optax`'s `softmax_cross_entropy_with_integer_labels` and found that `jnp.take` on this line was causing the error.  System info (python version, jaxlib version, accelerator, etc.) ",2024-12-05T12:20:49Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/25284,"I'm unable to reproduce this locally or on Colab, in either a CPU or GPU (T4) runtime. It's possibly related to the particular GPU hardware you're using, but I'm not sure how to diagnose that. Does this reproduce for other jax/jaxlib versions?","The bug disappeared when I downgraded `jax` to `0.4.34`.  By the way, I forgot to mention this warning message before the core dumped line. ",Can you try on the newly released jax 0.4.36?,It works now. Thank you.
477,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([export] Removed __gpu$xla.gpu.triton (Pallas GPU) from the list of custom calls with guaranteed compatibility.)ï¼Œ å†…å®¹æ˜¯ ([export] Removed __gpu$xla.gpu.triton (Pallas GPU) from the list of custom calls with guaranteed compatibility. This is because the underlying Triton IR does not guarantee compatibility.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[export] Removed __gpu$xla.gpu.triton (Pallas GPU) from the list of custom calls with guaranteed compatibility.,[export] Removed __gpu$xla.gpu.triton (Pallas GPU) from the list of custom calls with guaranteed compatibility. This is because the underlying Triton IR does not guarantee compatibility.,2024-12-05T10:24:13Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25279
1317,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Mosaic GPU] Add an initial skeleton for a layout inference pass.)ï¼Œ å†…å®¹æ˜¯ ([Mosaic GPU] Add an initial skeleton for a layout inference pass. Layouts are added as annotations on MLIR ops, using the `in_layouts` and `out_layouts` attributes. At this point, layout inference is done in two passes: one ""backwards"" pass (roottoparameters), and one ""forward"" pass (parameterstoroot). Each pass goes through all the ops in the specified order, and infers a possible layout from the layout information that is available. We expect to need two passes because partial layout annotations may be provided on intermediate nodes (e.g. `wgmma`), and a single pass from the root to the parameters is therefore insufficient to properly annotate all the operations. We do not perform any check as to whether the inferred layouts can be further lowered correctlymeaning that the produced IR can possibly fail to lower later. Layouts are only inferred for ops involving at least one operand or result of type `VectorType`/`RankedTensorType`. When layouts can't be inferred for an op that should have them, we default to annotating it with strided fragmented layouts.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[Mosaic GPU] Add an initial skeleton for a layout inference pass.,"[Mosaic GPU] Add an initial skeleton for a layout inference pass. Layouts are added as annotations on MLIR ops, using the `in_layouts` and `out_layouts` attributes. At this point, layout inference is done in two passes: one ""backwards"" pass (roottoparameters), and one ""forward"" pass (parameterstoroot). Each pass goes through all the ops in the specified order, and infers a possible layout from the layout information that is available. We expect to need two passes because partial layout annotations may be provided on intermediate nodes (e.g. `wgmma`), and a single pass from the root to the parameters is therefore insufficient to properly annotate all the operations. We do not perform any check as to whether the inferred layouts can be further lowered correctlymeaning that the produced IR can possibly fail to lower later. Layouts are only inferred for ops involving at least one operand or result of type `VectorType`/`RankedTensorType`. When layouts can't be inferred for an op that should have them, we default to annotating it with strided fragmented layouts.",2024-12-04T18:07:57Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25258
465,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([shape_poly] Remove the deprecated PolyShape object for specifying symbolic dimensions)ï¼Œ å†…å®¹æ˜¯ ([shape_poly] Remove the deprecated PolyShape object for specifying symbolic dimensions PolyShape has been deprecated in January 2024. The constructor has been raising a DeprecationWarning since then.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[shape_poly] Remove the deprecated PolyShape object for specifying symbolic dimensions,[shape_poly] Remove the deprecated PolyShape object for specifying symbolic dimensions PolyShape has been deprecated in January 2024. The constructor has been raising a DeprecationWarning since then.,2024-12-04T16:55:38Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25256
512,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Improve trace-time performance of jnp.isscalar)ï¼Œ å†…å®¹æ˜¯ (This avoids calling `device_put` on nonjax values within `isscalar`. This should improve trace time for `isscalar` and functions that use it, such as array indexing. Before:  After:  (This is part of addressing CC(Difference between numpy and jax.numpy in advanced indexing axes order)))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Improve trace-time performance of jnp.isscalar,"This avoids calling `device_put` on nonjax values within `isscalar`. This should improve trace time for `isscalar` and functions that use it, such as array indexing. Before:  After:  (This is part of addressing CC(Difference between numpy and jax.numpy in advanced indexing axes order))",2024-12-03T23:44:51Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/25237
424,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Installation fails on a cluster)ï¼Œ å†…å®¹æ˜¯ ( Description Hello, try to install this package with either pip or uv results in the following error: Do you have some ideas of why it happens?   System info (python version, jaxlib version, accelerator, etc.) ...)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Installation fails on a cluster," Description Hello, try to install this package with either pip or uv results in the following error: Do you have some ideas of why it happens?   System info (python version, jaxlib version, accelerator, etc.) ...",2024-12-02T12:21:44Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/25195,"Sorry, messed up the repository. Was supposed to open this elsewhere"
912,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Array.view(jnp.complex64) doesn't work on TPU with jax_enable_x64)ï¼Œ å†…å®¹æ˜¯ ( Description Trying to call `view(jnp.complex64)` when `jax_enable_x64` is enabled fails with: `XlaRuntimeError: INVALID_ARGUMENT: Element type C128 is not supported on TPU.`  It works without jax_enable_x64, when running on CPU/GPU (though presumably by going via complex128), or when called inside JITed code on TPU. Notebook reproduction (using latest version jax0.4.35 at the time of filing this issue): https://colab.research.google.com/drive/1IuyyeJQf60Febek2G4SWtqTAdeNAlzQt  System info (python version, jaxlib version, accelerator, etc.) Running on a v48 TPU VM, in a notebook (though it also fails if run outside a notebook, or on a TPUv2, see link above): )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Array.view(jnp.complex64) doesn't work on TPU with jax_enable_x64," Description Trying to call `view(jnp.complex64)` when `jax_enable_x64` is enabled fails with: `XlaRuntimeError: INVALID_ARGUMENT: Element type C128 is not supported on TPU.`  It works without jax_enable_x64, when running on CPU/GPU (though presumably by going via complex128), or when called inside JITed code on TPU. Notebook reproduction (using latest version jax0.4.35 at the time of filing this issue): https://colab.research.google.com/drive/1IuyyeJQf60Febek2G4SWtqTAdeNAlzQt  System info (python version, jaxlib version, accelerator, etc.) Running on a v48 TPU VM, in a notebook (though it also fails if run outside a notebook, or on a TPUv2, see link above): ",2024-11-30T18:41:43Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/25189,"Hi , Thanks for the report. As per the comment, JAX TPU does not have support for many 64bit operations and it is suggested using the default `jax_enable_x64=False` setting, to limit all operations to 32bit. When it comes to this issue, the error is coming from the following line: https://github.com/jaxml/jax/blob/e124c051f2c6f70b52ca87e10fefe8a5ce9e0d15/jax/_src/numpy/array_methods.pyL542 The error arises because `1j` is treated as a complex number with 64bit precision (C128 type) when `jax_enable_x64` is enabled.",Agreed with  â€“ the fix here is to not use `jax_enable_x64=True` with TPUs.
1004,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Value becoming a tracer error)ï¼Œ å†…å®¹æ˜¯ ( Description Hi, I get the following error when I run the code below: TracerIntegerConversionError: The __index__() method was called on traced array with shape int64[] The error occurred while tracing the function forwardEuler at /home/alex/PathOpt/integrationMethods/fwdEulerModel_3_GHquad_fwd_model_jax.py:110 for jit. This value became a tracer due to JAX operations on these lines: operation a:i64[] = sub b c from line /home/alex/PathOpt/integrationMethods/fwdEulerModel_3_GHquad_fwd_model_jax.py:113 (forwardEuler) Does the issue have to do with setting some of the function inputs as static? I tried doing that and it didn't work. How do you seet a jnp array as static? Do you declare it as anumpy array? Thank you, Alex   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Value becoming a tracer error," Description Hi, I get the following error when I run the code below: TracerIntegerConversionError: The __index__() method was called on traced array with shape int64[] The error occurred while tracing the function forwardEuler at /home/alex/PathOpt/integrationMethods/fwdEulerModel_3_GHquad_fwd_model_jax.py:110 for jit. This value became a tracer due to JAX operations on these lines: operation a:i64[] = sub b c from line /home/alex/PathOpt/integrationMethods/fwdEulerModel_3_GHquad_fwd_model_jax.py:113 (forwardEuler) Does the issue have to do with setting some of the function inputs as static? I tried doing that and it didn't work. How do you seet a jnp array as static? Do you declare it as anumpy array? Thank you, Alex   System info (python version, jaxlib version, accelerator, etc.) ",2024-11-29T19:02:37Z,bug,open,0,9,https://github.com/jax-ml/jax/issues/25186,"Please put together a *minimal*, reproducible example that isolates the problem. The included code includes a lot of extra pieces that aren't necessary for demonstrating this specific problem. Thanks!","Hi dfm, I updated the code thank you for the comment","You need to change this:  to this:  `forwardEuler` takes static arguments, and so when you wrap the gradient in `jit`, you must ensure those arguments are marked as static.","Hi Jake, thank you that works! I tried doing the same for the commented part at the end of the code for applyQuadratureRule() that applies a numerical quadrature for integration (of the final state) of the forward Euler model forwardEuler(). If I use jit without static_argnums=(6,) for the fixed shape array quadProd I get the same error, but if I add this I get: ValueError: Nonhashable static arguments are not supported. An error occurred while trying to hash an object of type , [[ 0.125  0.0005  0.0005 0.02   0.02  ]  [ 0.125  0.0005  0.0005 0.02    0.02  ]]. The error was: TypeError: unhashable type: 'numpy.ndarray' Is there a way of telling jax this is static both for jit and when getting the forward gradient?","This error means you're passing an array to a parameter marked as static, which is not allowed. Arrays aren't hashable, so they can't be static arguments. The solution is either to pass a hashable value to that argument (if the array was passed erroneously), or to not mark the argument as static (if it was marked static erroneously).","But if I don't pass it as static I get the trace error of the original question. Also I just checked, the original commented applyQuadratureRule() function takes 20 seconds, while the function using vmap takes 30 seconds. Both without using jit. Shoudn't the function using vmap without the for loop be faster? ",The trace error in the original question is because a parameter was marked static. Did you remove that annotation in both places?,"I did for applyQuadratureRule(), I'm running the code as posted","The error you mentioned means that you're passing a traced value to a parameter marked static. The solution is either to ensure it's static in the whole call stack (my initial recommendation) or to ensure it's not static in the whole call stack. Since the value you're passing is apparently an array, it cannot be static, so you have to ensure it's not marked as static anywhere. Does that make sense?"
1314,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Unable to run FFI example with stateful function)ï¼Œ å†…å®¹æ˜¯ ( Description I am trying to evaluate the new JAX FFI and use it to expose some of my code to be used with JAX/XLA. Since I have stateful functions, I tried to run the test linked in https://jax.readthedocs.io/en/latest/ffi.htmladvancedtopics as an example.  The following code (slightly adapted to replace `absl::Status` with `ffi::Error`)  Fails to compile, pointing me to a missing `TypeId id` member in `SomeState`. The following  Does compile, but we now have a missing symbol for `_XLA_FFI_GetApi`. Looking around in jaxlib, I found `xla_extension.so`, which defines this symbol as private/hidden.  I thus have a couple of questions:  Do I need to use `XLA_FFI_REGISTER_TYPE` to register custom context for stateful functions? It looks like a part of the public API.  If `XLA_FFI_REGISTER_TYPE` is part of the public API, should I be able to call `XLA_FFI_GetApi`?   And if I am able to call `XLA_FFI_GetApi`, should it be available for linking in `xla_extension.so`? Or is this one a private implementation detail?  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Unable to run FFI example with stateful function," Description I am trying to evaluate the new JAX FFI and use it to expose some of my code to be used with JAX/XLA. Since I have stateful functions, I tried to run the test linked in https://jax.readthedocs.io/en/latest/ffi.htmladvancedtopics as an example.  The following code (slightly adapted to replace `absl::Status` with `ffi::Error`)  Fails to compile, pointing me to a missing `TypeId id` member in `SomeState`. The following  Does compile, but we now have a missing symbol for `_XLA_FFI_GetApi`. Looking around in jaxlib, I found `xla_extension.so`, which defines this symbol as private/hidden.  I thus have a couple of questions:  Do I need to use `XLA_FFI_REGISTER_TYPE` to register custom context for stateful functions? It looks like a part of the public API.  If `XLA_FFI_REGISTER_TYPE` is part of the public API, should I be able to call `XLA_FFI_GetApi`?   And if I am able to call `XLA_FFI_GetApi`, should it be available for linking in `xla_extension.so`? Or is this one a private implementation detail?  System info (python version, jaxlib version, accelerator, etc.) ",2024-11-29T16:31:27Z,enhancement,closed,0,4,https://github.com/jax-ml/jax/issues/25185,"Thanks for the questions! We don't actually support userdefined state for FFIs registered via JAX yet. There are some subtleties to getting dyanamically registered types to work, but it will get added eventually (something just always seems to be higher priority :D). In the meantime, the usual approach that we use throughout `jaxlib` is to use global state that gets initialized on the first call to the handler. For example, check out this example: https://github.com/jaxml/jax/blob/aff7714dc0f49cc0097e4db08e028b68182c8ab9/examples/ffi/src/jax_ffi_example/cpu_examples.ccL71L104 Hope this helps!","Ok, thanks for the clarification! Using global/thread local state could work in some cases (when the state is some kind of cache allocation/work array) that can be hidden from the user, but I also have some cases where the end user needs to manipulate the types. I'll see if there is a way to handle this with the current API, or if I need to wait for better integration.","Reporting back here that JAX now supports stateful FFI handlers, and CC(Add FFI example demonstrating the use of XLA's FFI state.) adds an example. I'm going to close this as complete, but please report back if you run into further issues.","Thanks a lot, I'll have a look ASAP!"
611,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Cannot generate from `jax.experimental.sparse.random_bcoo` for arrays larger than the int32 max )ï¼Œ å†…å®¹æ˜¯ ( Description `jax.experimental.sparse.random_bcoo` yields an error for arrays with size larger than the int32 max. It looks like one cannot generate random indices that overflow int32 even if the specified index dtype if int64. Here is code to reproduce:  yields error   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Cannot generate from `jax.experimental.sparse.random_bcoo` for arrays larger than the int32 max ," Description `jax.experimental.sparse.random_bcoo` yields an error for arrays with size larger than the int32 max. It looks like one cannot generate random indices that overflow int32 even if the specified index dtype if int64. Here is code to reproduce:  yields error   System info (python version, jaxlib version, accelerator, etc.) ",2024-11-28T20:03:30Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/25182,"never mind, enabling 64 bit dtypes fixes this (although it ooms)"
386,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([mgpu_pallas] Optionally pass default value instead of raising an error when trying to ensure ir Value.)ï¼Œ å†…å®¹æ˜¯ ([mgpu_pallas] Optionally pass default value instead of raising an error when trying to ensure ir Value.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[mgpu_pallas] Optionally pass default value instead of raising an error when trying to ensure ir Value.,[mgpu_pallas] Optionally pass default value instead of raising an error when trying to ensure ir Value.,2024-11-28T16:28:51Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25177
313,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Replace dependency on pre-built wheels with `py_import` dependency.)ï¼Œ å†…å®¹æ˜¯ (Replace dependency on prebuilt wheels with `py_import` dependency.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Replace dependency on pre-built wheels with `py_import` dependency.,Replace dependency on prebuilt wheels with `py_import` dependency.,2024-11-27T20:25:53Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25152
968,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(NaN's produced by jax.numpy.linalg.pinv not produced by numpy)ï¼Œ å†…å®¹æ˜¯ ( Description I've been hunting down this a bug for a while! I've got as far as I can with a MWE. Symptoms  I've been getting NaNs produced on a CPU after a bunch of iterations of my algorithms (say between around 2080 batches in) but I have found that on a GPU I am *never* seeing this same issue (even after 1000 iterations). As I'm using `vmap` + `jax.lax.cond` I've been unable to use the normal 'debug nans' flags (well I was using them but they were *all* red herrings!) I think I've managed to isolate a calculation that produces these NaNs that Numpy does not... (Note that I've copied the pinv implementation out of Jax in order to add extra debugging)   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,NaN's produced by jax.numpy.linalg.pinv not produced by numpy," Description I've been hunting down this a bug for a while! I've got as far as I can with a MWE. Symptoms  I've been getting NaNs produced on a CPU after a bunch of iterations of my algorithms (say between around 2080 batches in) but I have found that on a GPU I am *never* seeing this same issue (even after 1000 iterations). As I'm using `vmap` + `jax.lax.cond` I've been unable to use the normal 'debug nans' flags (well I was using them but they were *all* red herrings!) I think I've managed to isolate a calculation that produces these NaNs that Numpy does not... (Note that I've copied the pinv implementation out of Jax in order to add extra debugging)   System info (python version, jaxlib version, accelerator, etc.) ",2024-11-26T12:52:08Z,bug,open,0,2,https://github.com/jax-ml/jax/issues/25111,"I imagine the first response will be something like 'well due to numeric differences, this specific matrix will produce a nan with Jax, but not with Numpy' BUT I've made tweaks and changes to my algorithm (even feeding it different data) and NEVER see this issue on a GPU (even after 1000 iterations) but it ALWAYS seems to crop up on CPU. And (as best I can tell) this is where that NaN is being generated.","What NumPy version are you using? With NumPy v2.1.3, I'm getting `numpy.linalg.LinAlgError: SVD did not converge`, which suggests that JAX's NaN outputs are correct here. The NumPy results may also depend on the underlying LAPACK library that is being linked, which will depend on your operating system and how you installed NumPy (e.g. pip vs conda, etc.). "
1343,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add lax.composite primitive)ï¼Œ å†…å®¹æ˜¯ (Add lax.composite primitive A composite function can encapsulate an operation made up of other JAX functions. The semantics of the op is implemented by the `decomposition` function. For example, a `tangent` operation can be implemented as `sin(x) / cos(x)`. This is what the HLO looks like for a tangent composite:  Similarly, this can scale to something like Attention. By preserving such an abstraction, it greatly simplifies pattern matching. Instead of matching the set of ops that represent Attention, the matcher can simply look for a uniquely identifying composite op like ""MyAttention"". This is useful for preserving high level abstraction that would otherwise be lost during lowering. The hardwareaware compiler can recognize the single composite op and emit efficient code rather than patternmatching a generic lowering which is then replaced with your own efficient lowering. And then the decomposition function can be DCE'd away. If the hardware does not have an efficient lowering, it can inline the `decomposition` which implements the semantics of the abstraction. For more details on the API, refer to the documentation.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add lax.composite primitive,"Add lax.composite primitive A composite function can encapsulate an operation made up of other JAX functions. The semantics of the op is implemented by the `decomposition` function. For example, a `tangent` operation can be implemented as `sin(x) / cos(x)`. This is what the HLO looks like for a tangent composite:  Similarly, this can scale to something like Attention. By preserving such an abstraction, it greatly simplifies pattern matching. Instead of matching the set of ops that represent Attention, the matcher can simply look for a uniquely identifying composite op like ""MyAttention"". This is useful for preserving high level abstraction that would otherwise be lost during lowering. The hardwareaware compiler can recognize the single composite op and emit efficient code rather than patternmatching a generic lowering which is then replaced with your own efficient lowering. And then the decomposition function can be DCE'd away. If the hardware does not have an efficient lowering, it can inline the `decomposition` which implements the semantics of the abstraction. For more details on the API, refer to the documentation.",2024-11-25T23:59:56Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25104
1489,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Weird defjvp behavior when finding grad of a scalar that depends on the primal)ï¼Œ å†…å®¹æ˜¯ ( Description Hello! I was testing the defjvp feature to deal with some stability issues of the SVD, but I noticed that defjvp doesn't work as expected if the output of the function to be differentiated depends on the primal. Let me share some code. I started by defining ``new_svd`` , which should be exactly the same as ``jax.numpy.svd`` but with its derivatives defined with defjvp (code from ``jax._src.lax.linalg.py`` with minor modifications):     import jax.lax as lax     from jax._src.lax import lax as lax_internal     from jax import custom_jvp     import jax.numpy as jnp     import jax.random as jrandom     import jax     def _extract_diagonal(s):         i = lax.iota(""int32"", min(s.shape[2], s.shape[1]))         return s[..., i, i]     def _construct_diagonal(s):         i = lax.iota(""int32"", s.shape[1])         return lax.full((*s.shape, s.shape[1]), 0, s.dtype).at[..., i, i].set(s)     def _H(x):         return _T(x).conj()     def _T(x):         return lax.transpose(x, (*range(x.ndim  2), x.ndim  1, x.ndim  2))          def new_SVD(x):         return jnp.linalg.svd(x, full_matrices=False)     .defjvp     def _svd_jvp_rule(primals, tangents):         (A,) = primals         (dA,) = tangents         U, s, )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Weird defjvp behavior when finding grad of a scalar that depends on the primal," Description Hello! I was testing the defjvp feature to deal with some stability issues of the SVD, but I noticed that defjvp doesn't work as expected if the output of the function to be differentiated depends on the primal. Let me share some code. I started by defining ``new_svd`` , which should be exactly the same as ``jax.numpy.svd`` but with its derivatives defined with defjvp (code from ``jax._src.lax.linalg.py`` with minor modifications):     import jax.lax as lax     from jax._src.lax import lax as lax_internal     from jax import custom_jvp     import jax.numpy as jnp     import jax.random as jrandom     import jax     def _extract_diagonal(s):         i = lax.iota(""int32"", min(s.shape[2], s.shape[1]))         return s[..., i, i]     def _construct_diagonal(s):         i = lax.iota(""int32"", s.shape[1])         return lax.full((*s.shape, s.shape[1]), 0, s.dtype).at[..., i, i].set(s)     def _H(x):         return _T(x).conj()     def _T(x):         return lax.transpose(x, (*range(x.ndim  2), x.ndim  1, x.ndim  2))          def new_SVD(x):         return jnp.linalg.svd(x, full_matrices=False)     .defjvp     def _svd_jvp_rule(primals, tangents):         (A,) = primals         (dA,) = tangents         U, s, ",2024-11-25T23:18:14Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/25101,"It looks to me like this is working ok. When I run your demo code with ""case 1"" the differences seem to be within a few 10^8, which seems fine for `float32` precision. `jnp.allclose` uses an absolute tolerance of `1e8` which probably isn't suitable for your case. What do you think of that?",I agree! Thank you.
354,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Create a `null_mesh_context` internal context manager to handle null contexts properly.)ï¼Œ å†…å®¹æ˜¯ (Create a `null_mesh_context` internal context manager to handle null contexts properly.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,Create a `null_mesh_context` internal context manager to handle null contexts properly.,Create a `null_mesh_context` internal context manager to handle null contexts properly.,2024-11-25T22:55:21Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25100
320,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([mosaic_gpu] Fixed unbounded recursion in `FragmentedArray._pointwise`)ï¼Œ å†…å®¹æ˜¯ ([mosaic_gpu] Fixed unbounded recursion in `FragmentedArray._pointwise`)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[mosaic_gpu] Fixed unbounded recursion in `FragmentedArray._pointwise`,[mosaic_gpu] Fixed unbounded recursion in `FragmentedArray._pointwise`,2024-11-25T22:08:16Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25098
1136,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([cuda] Bump nvidia-cuda-nvcc-cu12 dependency to 12.6.85)ï¼Œ å†…å®¹æ˜¯ (Bumps the minimum version for `nvidiacudanvcccu12` to `12.6.85`, which is the earliest published version that of that wheel incorporating CUDA 12.6.3. This resolves the issue underlying https://github.com/jaxml/jax/pull/24438. Here's a simplified reproducer:  As a followup we might also want to add a warning asking users with an affected version of `nvidiacudanvcccu12` to upgrade (e.g., via `pip install U ""nvidiacudanvcccu12>=12.6.85""`).  Any opinions on whether to have that warning / where it should go? As to how to check for the current ptxas version (https://github.com/jaxml/jax/pull/24438/filesr1818345222), I suppose we might expose that alongside other CUDA versions here (https://github.com/jaxml/jax/blob/c35f8b22c1b081135e0644a936c262a974c75f07/jaxlib/cuda/versions.ccL26)? A cruder alternative approach might be to check for the `nvidiacudanvcccu12` wheel's `__version__`, if present.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[cuda] Bump nvidia-cuda-nvcc-cu12 dependency to 12.6.85,"Bumps the minimum version for `nvidiacudanvcccu12` to `12.6.85`, which is the earliest published version that of that wheel incorporating CUDA 12.6.3. This resolves the issue underlying https://github.com/jaxml/jax/pull/24438. Here's a simplified reproducer:  As a followup we might also want to add a warning asking users with an affected version of `nvidiacudanvcccu12` to upgrade (e.g., via `pip install U ""nvidiacudanvcccu12>=12.6.85""`).  Any opinions on whether to have that warning / where it should go? As to how to check for the current ptxas version (https://github.com/jaxml/jax/pull/24438/filesr1818345222), I suppose we might expose that alongside other CUDA versions here (https://github.com/jaxml/jax/blob/c35f8b22c1b081135e0644a936c262a974c75f07/jaxlib/cuda/versions.ccL26)? A cruder alternative approach might be to check for the `nvidiacudanvcccu12` wheel's `__version__`, if present.",2024-11-25T18:27:52Z,pull ready,closed,0,2,https://github.com/jax-ml/jax/issues/25091,I think the best place for the warning would be in XLA: it should warn if it detects a ptxas that is known to be buggy.,Corresponding XLA PR that emits a warning: https://github.com/openxla/xla/pull/19927
1450,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump hypothesis from 6.102.4 to 6.119.4)ï¼Œ å†…å®¹æ˜¯ (Bumps hypothesis from 6.102.4 to 6.119.4.  Release notes Sourced from hypothesis's releases.  Hypothesis for Python  version 6.119.4 This patch fixes a bug since 6.99.13  20240324 where only interactivelygenerated values (via &quot;data.draw&quot;) would be reported in the &quot;arguments&quot; field of our observability output. Now, all values are reported. The canonical version of these notes (with links) is on readthedocs. Hypothesis for Python  version 6.119.3 Hypothesis collects coverage information during the &quot;shrink&quot; and &quot;explain&quot; phases in order to show a more informative error message. On 3.12+, this uses &quot;sys.monitoring&quot;. This patch improves the performance of coverage collection on 3.12+ by disabling events we don't need. The canonical version of these notes (with links) is on readthedocs. Hypothesis for Python  version 6.119.2 This patch refactors some internals to prepare for future work using our IR (issue  CC(update version and changelog for pypi)). The canonical version of these notes (with links) is on readthedocs. Hypothesis for Python  version 6.119.1 This patch migrates some more internals (around generating novel inputs) to the IR layer (issue  CC(update version)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Bump hypothesis from 6.102.4 to 6.119.4,"Bumps hypothesis from 6.102.4 to 6.119.4.  Release notes Sourced from hypothesis's releases.  Hypothesis for Python  version 6.119.4 This patch fixes a bug since 6.99.13  20240324 where only interactivelygenerated values (via &quot;data.draw&quot;) would be reported in the &quot;arguments&quot; field of our observability output. Now, all values are reported. The canonical version of these notes (with links) is on readthedocs. Hypothesis for Python  version 6.119.3 Hypothesis collects coverage information during the &quot;shrink&quot; and &quot;explain&quot; phases in order to show a more informative error message. On 3.12+, this uses &quot;sys.monitoring&quot;. This patch improves the performance of coverage collection on 3.12+ by disabling events we don't need. The canonical version of these notes (with links) is on readthedocs. Hypothesis for Python  version 6.119.2 This patch refactors some internals to prepare for future work using our IR (issue  CC(update version and changelog for pypi)). The canonical version of these notes (with links) is on readthedocs. Hypothesis for Python  version 6.119.1 This patch migrates some more internals (around generating novel inputs) to the IR layer (issue  CC(update version",2024-11-25T18:14:04Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/25089,Superseded by CC(Bump hypothesis from 6.102.4 to 6.122.1).
318,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([mgpu] FragentedArray.foreach() can now optionally return a new array)ï¼Œ å†…å®¹æ˜¯ ([mgpu] FragentedArray.foreach() can now optionally return a new array)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",agent,[mgpu] FragentedArray.foreach() can now optionally return a new array,[mgpu] FragentedArray.foreach() can now optionally return a new array,2024-11-25T17:13:07Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25086
686,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Possible leak in random number generation)ï¼Œ å†…å®¹æ˜¯ ( Description When using `jax` based package `pychastic` (an SDE solver) jax backend keeps eating memory indefinitely.  Output (abbreviated)  I apologize for the contrived code to reproduce the issue. I'd be happy to chase the leak further, but I'm unfamiliar with any tools that could help diagnose the issue. Is there some way to see what's taking up all this space?  System info (python version, jaxlib version, accelerator, etc.)  also tested (same result) with )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Possible leak in random number generation," Description When using `jax` based package `pychastic` (an SDE solver) jax backend keeps eating memory indefinitely.  Output (abbreviated)  I apologize for the contrived code to reproduce the issue. I'd be happy to chase the leak further, but I'm unfamiliar with any tools that could help diagnose the issue. Is there some way to see what's taking up all this space?  System info (python version, jaxlib version, accelerator, etc.)  also tested (same result) with ",2024-11-22T21:33:22Z,bug,open,0,8,https://github.com/jax-ml/jax/issues/25069,A couple potentially helpful tips: (1) You can use `jax.live_arrays()` https://jax.readthedocs.io/en/latest/_autosummary/jax.live_arrays.html to return all of the live arrays and check for potential leaks. (2) Does manually running `gc.collect()` help with the problem?,`gc.collect()` changes nothing `jax.live_arrays()` gives `[]` at the same time running  `jax.clear_backends()` once per loop fixes the problem  I've attached code for your convenience     ,"The fact that `clear_backends` solves this probably means that it's a JIT cache issue. Looking at the `solve_many` source, it looks like `problem` is closed over in `scan`: https://github.com/RadostW/stochastic/blob/bea391e57ad6e6bca734dd0ee243ab0b231d6048/pychastic/sde_solver.pyL158 Because of this, in scan's internal JIT, `problem` is treated as a static variable, and because you redefine it every iteration, it leads to a cache miss on every iteration. This means the JIT cache grows each iteration until hitting the LRU cache limit (which is 4096, I believe). This is all working as intended, and the fix to your issue would be for `pychastic` to be more careful about static arguments within its `scan` implementation. For example, the package could register `SDEProblem` as a pytree so that it would not be treated as a static variable. This would also potentially lead to faster runtime, because the scan function would not have to be recompiled at each iteration.","First, thanks for going through that code and finding where the issue really is. (Sorry it looks like this  it mirrors a textbook written by a mathematician, not a computer scientist). Second, what you say about cache misses is confusing. `problem` variable is closed over like you say, but this variable does not change. Are you saying a mere presence of a messy class as a implicit argument of a JITed function makes the cache explode? Is there a section of `jax` readme with good and bad examples of this? Third, suppose we exhaust the cache running `solve_many` one time. I would expect that this cache is eventually garbage collected or at least it does not grow in size indefinitely. Current behavior is that each time `solve_many` is run the program takes another bit of memory which eventually makes it run out of space and it crashes (without even raising a python error which can be handled).","> `problem` variable is closed over like you say, but this variable does not change. The variable does change. If you do `print(id(problem))`, you'll see that it's a different variable in each loop. The *contents* of the problem may not change, and if you want the JAX machinery to know this, then you can either define a meaningful hash function if `problem` doesn't contain any references to arrays, or register it as a pytree if it does contain references to arrays. > I would expect that this cache is eventually garbage collected or at least it does not grow in size indefinitely. Exactly â€“ the cache is an LRU cache with a max size of (I believe) 4096, so once you call the function enough times it will start evicting older entries.",I don't think this is the problem. If I have just one `problem` object the issue is the same.   Code below:     ,"Hmm, in that case I'm not sure what's causing the cache size to grow. What happens if you do `gc.collect()` followed by `time.sleep(5)` between each iteration?","This does not help. I have also moved functional transforms outside of `step` function in the `pychastic` package, but this changed nothing as well (and to my surprise didn't change performance as far as I can tell)."
727,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Mosaic TPU] Support packed type matmul with arbitrary shapes.)ï¼Œ å†…å®¹æ˜¯ ([Mosaic TPU] Support packed type matmul with arbitrary shapes. This cl removes all the shape constrains in matmul for all types. We only need to mask out subelement on contracting dim. Instead of unpacking data and applying masks, we create a VREGsized i32 ""mask"" which contains subelement mask info to logical and with target vreg. Through this way, in order to mask subelements, each target vreg only needs to apply 1 op (logical_and) instead of 3 ops (unpacking + select + packing).)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[Mosaic TPU] Support packed type matmul with arbitrary shapes.,"[Mosaic TPU] Support packed type matmul with arbitrary shapes. This cl removes all the shape constrains in matmul for all types. We only need to mask out subelement on contracting dim. Instead of unpacking data and applying masks, we create a VREGsized i32 ""mask"" which contains subelement mask info to logical and with target vreg. Through this way, in order to mask subelements, each target vreg only needs to apply 1 op (logical_and) instead of 3 ops (unpacking + select + packing).",2024-11-22T20:13:00Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25068
936,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`AssertionError: Unexpected XLA layout override` when adding two `from_dlpack` arrays)ï¼Œ å†…å®¹æ˜¯ ( Description I have a test case that broke somewhere between jax versions 0.4.19 and 0.4.28. In particular, I am using `jax.dlpack.from_dlpack` on some PyTorch Tensors and then after hitting them with some jax operations I'm getting  To reproduce run the `test_vit_b16` test in https://github.com/samuela/torch2jax/commit/93ed7065c582393384e958ea96f3ad6221b09626. It was last working on https://github.com/samuela/torch2jax/commit/bd7bd9c95253c89ffb7a25cc0ff2ccb296f6cfbf. Happy to provide any other info that might be helpful in reproducing. Potentially related: https://github.com/jaxml/jax/issues/24680  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",dspy,`AssertionError: Unexpected XLA layout override` when adding two `from_dlpack` arrays," Description I have a test case that broke somewhere between jax versions 0.4.19 and 0.4.28. In particular, I am using `jax.dlpack.from_dlpack` on some PyTorch Tensors and then after hitting them with some jax operations I'm getting  To reproduce run the `test_vit_b16` test in https://github.com/samuela/torch2jax/commit/93ed7065c582393384e958ea96f3ad6221b09626. It was last working on https://github.com/samuela/torch2jax/commit/bd7bd9c95253c89ffb7a25cc0ff2ccb296f6cfbf. Happy to provide any other info that might be helpful in reproducing. Potentially related: https://github.com/jaxml/jax/issues/24680  System info (python version, jaxlib version, accelerator, etc.) ",2024-11-22T18:52:52Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/25066,"Thanks for the report! Yes, this is the same issue as https://github.com/jaxml/jax/issues/24680, and the real issue here is that XLA doesn't generally support layout assignments on CPU (it requires that all arrays be rowmajor).  is planning on adding support, but we're not sure what the timeline for that will be. The reason why this shows up with PyTorch and DLPack specifically has to do with the strides that PyTorch reports for tensors where one of the dimensions has size `1`. The tl;dr is that it's ambiguous where in the layout these dimensions should show up because you can put them anywhere without changing the striding behavior of the array, and PyTorch always gives them a stride of `1` via DLPack. Our stridestolayout logic will then always put these dimensions at the end of the layout. https://github.com/openxla/xla/pull/19327 includes a hack which would hide this assertion error (i.e. it would produce a rowmajor array in this case), but the real problem won't be solved until XLA CPU adds layout support!","I've finally added layouts support to the CPU client (here's the relevant commit: https://github.com/openxla/xla/pull/22048) and confirmed that this example now works. I'm going to close this because everything should work properly with the next JAX release, but please let me know if there are any further issues. Thanks again for the report!","Woohoo, thank you so much !"
294,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([pallas:mosaic_gpu] Add test for FragmentedArray.bitcast.)ï¼Œ å†…å®¹æ˜¯ ([pallas:mosaic_gpu] Add test for FragmentedArray.bitcast.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[pallas:mosaic_gpu] Add test for FragmentedArray.bitcast.,[pallas:mosaic_gpu] Add test for FragmentedArray.bitcast.,2024-11-22T17:34:26Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25058
1456,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Rank-one updates to eigenvalue decompositions)ï¼Œ å†…å®¹æ˜¯ (Hi team, First off, I absolutely love JAX. It's the core engine behind our startup. It would be fantastic to have a rankone update to an eigenvalue decomposition of a symmetric PSD matix $A$. I.e. when $A=LDL^\top$, we can compute $\tilde{L}\tilde{D}\tilde{L}^\top=\tilde{A}=A+\rho xx^\top$ in $\mathcal{O}(n^2)$, (rather than $\mathcal{O}(n^3)$ by running the eigenvalue decomposition again on $\tilde{A}$). In principle this is very similar to a Cholesky rankone update, which is implemented in `jax._src.lax.linalg.cholesky_update`, albeit without batching rule. **Motivating case** The rankone update to eigenvalue decompositions comes up in several rolling matrix algorithms. My own personal usecase is covariance matrix shrinkage in an exponentially weighted setting:  where I need to regularize the eigenvalues of each $\Sigma_t$. I do this for batched timeseries where the matrices themselves are of moderate size (of dim 30x30 to 300x300). With a rank one update I could run a scan on the eigenvalue decomposition rather than the covariance matrix. **Required algorithms** The underlying algorithms this requires are available in `scipy.linalg.cython_lapack` as `dlasd4` and `slasd4` for CPU (which I think jaxlib referenc)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Rank-one updates to eigenvalue decompositions,"Hi team, First off, I absolutely love JAX. It's the core engine behind our startup. It would be fantastic to have a rankone update to an eigenvalue decomposition of a symmetric PSD matix $A$. I.e. when $A=LDL^\top$, we can compute $\tilde{L}\tilde{D}\tilde{L}^\top=\tilde{A}=A+\rho xx^\top$ in $\mathcal{O}(n^2)$, (rather than $\mathcal{O}(n^3)$ by running the eigenvalue decomposition again on $\tilde{A}$). In principle this is very similar to a Cholesky rankone update, which is implemented in `jax._src.lax.linalg.cholesky_update`, albeit without batching rule. **Motivating case** The rankone update to eigenvalue decompositions comes up in several rolling matrix algorithms. My own personal usecase is covariance matrix shrinkage in an exponentially weighted setting:  where I need to regularize the eigenvalues of each $\Sigma_t$. I do this for batched timeseries where the matrices themselves are of moderate size (of dim 30x30 to 300x300). With a rank one update I could run a scan on the eigenvalue decomposition rather than the covariance matrix. **Required algorithms** The underlying algorithms this requires are available in `scipy.linalg.cython_lapack` as `dlasd4` and `slasd4` for CPU (which I think jaxlib referenc",2024-11-22T15:39:18Z,enhancement,open,0,4,https://github.com/jax-ml/jax/issues/25057,"The approach you're using here looks good to me! One suggestion that might improve performance would be to push the batching logic into Cython. In other words, perhaps you could update `slasd4_all` (etc.) to support 2D inputs and then have a nested `for`loop in the body. Then you could use `vmap_method='broadcast_all'` instead of `vmap_method='sequential'`. This might help because the sequential method results in a separate Python call (with the associated overhead) for each element in the batch, instead of just a single ""vectorized"" Python call. On the longer term, it's a reasonable feature request for JAX to support this operation outofthebox, but it would need a strong argument for the team to implement it at high priority. So, it would be useful to make sure that your current implementation works well enough for now!","Thanks Dan! I appreciate the tip, and I understand that this is a pretty niche matrix operation.","Hey all, I just happened to be looking for this operation and found this discussion. Are you sure the complexity is quadratic? The linked discussion on math.stackexchange suggests that quadratic complexity is only possible for the eigenvalues, and the formula that I think you use for eigenvectors, given in ""RankOne Modification of the Symmetric Eigenproblem."" by Bunch et al (https://eudml.org/doc/132565content) is cubic in the dimension (or in the number of eigenvalues after deflation; per attached screenshot): !image For my particular use case this result is suggesting that updates are not worth it (my matrix is low rank), so I am curious if I'm missing something. ","You are absolutely correct. The O(n^2) statement I made was only about the eigenvalues. Regardless, the matrix multiplication to compute the eigenvalues afterwards is a lot faster (even if it runs in O(n^3)), and I believe it is quadratic in its memory usage (I am not sure about eigenvalue decomposition but I believe it is cubic)."
1480,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Added stream annotation support via @compute_on('stream:#') decorator)ï¼Œ å†…å®¹æ˜¯ (This is a tiny change that will add the stream annotation `frontend_attribute` when usin a `compute_on(""stream:"")` decorator. This feature is intended to be used when trying to implement a collective matrix multiplication ""by hand"" within a `shard_map` to be able to better utilize SMs.  During most cublass / cutlass gemm subroutines, not all SMs are utilized by the kernels due to a process called ""wave quantization"". Essentially what happens is that each gemm kernel will only use SMs until it reaches an amount that is a divisor of the contracting dimension, leaving the other SMs essentially sitting idle. These idle SMs prevent us from getting full performance of our GPUs.  If we want to be able to use these idle SMs in our CM subroutine, we would need to be able to run at least 2 gemms in parallel. That isn't currently possible within a `shard_map`, as all of the compute kernels get launched on a single cuda stream. By adding this feature, we would be able to explicitly launch these kernels on several streams. This feature is not yet fully enabled in XLA, and will sit behind a `xla_gpu_experimental_stream_annotation` flag for the foreseeable future. When this flag is not enabled, this attribute is just ignored. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Added stream annotation support via @compute_on('stream:#') decorator,"This is a tiny change that will add the stream annotation `frontend_attribute` when usin a `compute_on(""stream:"")` decorator. This feature is intended to be used when trying to implement a collective matrix multiplication ""by hand"" within a `shard_map` to be able to better utilize SMs.  During most cublass / cutlass gemm subroutines, not all SMs are utilized by the kernels due to a process called ""wave quantization"". Essentially what happens is that each gemm kernel will only use SMs until it reaches an amount that is a divisor of the contracting dimension, leaving the other SMs essentially sitting idle. These idle SMs prevent us from getting full performance of our GPUs.  If we want to be able to use these idle SMs in our CM subroutine, we would need to be able to run at least 2 gemms in parallel. That isn't currently possible within a `shard_map`, as all of the compute kernels get launched on a single cuda stream. By adding this feature, we would be able to explicitly launch these kernels on several streams. This feature is not yet fully enabled in XLA, and will sit behind a `xla_gpu_experimental_stream_annotation` flag for the foreseeable future. When this flag is not enabled, this attribute is just ignored. ",2024-11-22T15:33:57Z,pull ready,open,0,2,https://github.com/jax-ml/jax/issues/25056,Would be nice to include more in the description of why support this! i.e. make the description more fleshed out.,"I added the test, updated the description, and changed the `startswith` checks to look for `stream:`"
646,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Rework custom hermetic python instructions. )ï¼Œ å†…å®¹æ˜¯ (Rework custom hermetic python instructions.  The focus was shifted from how one should build custom python, as it seems like people don't really have issues with that and the process is fairly standard.  Instead the focus was made on demystifying of what hermetic (custom or not) Python actually is and explaining how a user can customize the build while still keeping it as close to a regular Python workflow as possible.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Rework custom hermetic python instructions. ,"Rework custom hermetic python instructions.  The focus was shifted from how one should build custom python, as it seems like people don't really have issues with that and the process is fairly standard.  Instead the focus was made on demystifying of what hermetic (custom or not) Python actually is and explaining how a user can customize the build while still keeping it as close to a regular Python workflow as possible.",2024-11-22T09:52:55Z,,closed,0,0,https://github.com/jax-ml/jax/issues/25052
1077,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Running the executable compiled directly from jax.jit is more than three times slower than jax.jit itself.)ï¼Œ å†…å®¹æ˜¯ ( Description I want to save the executable file generated by jax.jit in the main process and execute this executable file in another process. However, I have found that the performance of the executable file is much slower than that of the JIT itself. I would like to know why this is the case.  1.The execution code and results of jax.jit are as follows.  1.1 The execution code    1.2 The execution results   Train for 5 epochs, with 10,000 steps per epoch, and each epoch takes 15 seconds. !cnn  2.The code and results from running the executable file generated by jax.jit are as follows.  2.1 The code    2.2 The results  Train for 5 epochs, with 10,000 steps per epoch, and each epoch takes 50 seconds. !executable  System info (python version, jaxlib version, accelerator, etc.) !image)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Running the executable compiled directly from jax.jit is more than three times slower than jax.jit itself.," Description I want to save the executable file generated by jax.jit in the main process and execute this executable file in another process. However, I have found that the performance of the executable file is much slower than that of the JIT itself. I would like to know why this is the case.  1.The execution code and results of jax.jit are as follows.  1.1 The execution code    1.2 The execution results   Train for 5 epochs, with 10,000 steps per epoch, and each epoch takes 15 seconds. !cnn  2.The code and results from running the executable file generated by jax.jit are as follows.  2.1 The code    2.2 The results  Train for 5 epochs, with 10,000 steps per epoch, and each epoch takes 50 seconds. !executable  System info (python version, jaxlib version, accelerator, etc.) !image",2024-11-21T01:51:41Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/25023,Can you try using these APIs instead? https://github.com/jaxml/jax/blob/main/jax/experimental/serialize_executable.pyL25L60 or jax.export: https://jax.readthedocs.io/en/latest/export/export.html
711,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(libtpu.so Not Found for JAX TPU 0.4.14)ï¼Œ å†…å®¹æ˜¯ ( Description Hi, I'm trying to **install jax[tpu]==0.4.14 on a V5e256 TPU pod** by `pip install U ""jax[tpu]==0.4.14"" f https://storage.googleapis.com/jaxreleases/libtpu_releases.html`.  However, after installation, when I launch a jax job, I got the below error:  I then check ed`/home/XXXXX/.local/lib/python3.10/sitepackages/libtpu` and found no `libtpu.so` is there. Could you please help me with this? Thank you very much!   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,libtpu.so Not Found for JAX TPU 0.4.14," Description Hi, I'm trying to **install jax[tpu]==0.4.14 on a V5e256 TPU pod** by `pip install U ""jax[tpu]==0.4.14"" f https://storage.googleapis.com/jaxreleases/libtpu_releases.html`.  However, after installation, when I launch a jax job, I got the below error:  I then check ed`/home/XXXXX/.local/lib/python3.10/sitepackages/libtpu` and found no `libtpu.so` is there. Could you please help me with this? Thank you very much!   System info (python version, jaxlib version, accelerator, etc.) ",2024-11-19T22:45:21Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/24987,"I can reproduce this with `jax[tpu]==0.4.14`, which pins `libtpunightly==0.1.dev20230727`. If I update to `jax[tpu]==0.4.16`, which pins `libtpunightly==0.1.dev20230918`, the issue goes away. I suspect this means that that version of libtpunightly has a bug that was undetected at the time; your best bet is probably to upgrade to a more recent JAX release: the issue has already been fixed, and in general we don't do patch releases for old JAX versions.",I see. Will try to upgrade my JAX. Thank you very much!
300,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Disable some complex function accuracy tests that fail on Mac ARM.)ï¼Œ å†…å®¹æ˜¯ (Issue https://github.com/jaxml/jax/issues/24787 ( FYI))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Disable some complex function accuracy tests that fail on Mac ARM.,Issue https://github.com/jaxml/jax/issues/24787 ( FYI),2024-11-18T22:05:46Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/24957
210,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(fix typo in numpy/__init__.pyi)ï¼Œ å†…å®¹æ˜¯ ()è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,fix typo in numpy/__init__.pyi,,2024-11-18T19:04:53Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/24952
1450,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump hypothesis from 6.102.4 to 6.119.3)ï¼Œ å†…å®¹æ˜¯ (Bumps hypothesis from 6.102.4 to 6.119.3.  Release notes Sourced from hypothesis's releases.  Hypothesis for Python  version 6.119.3 Hypothesis collects coverage information during the &quot;shrink&quot; and &quot;explain&quot; phases in order to show a more informative error message. On 3.12+, this uses &quot;sys.monitoring&quot;. This patch improves the performance of coverage collection on 3.12+ by disabling events we don't need. The canonical version of these notes (with links) is on readthedocs. Hypothesis for Python  version 6.119.2 This patch refactors some internals to prepare for future work using our IR (issue  CC(update version and changelog for pypi)). The canonical version of these notes (with links) is on readthedocs. Hypothesis for Python  version 6.119.1 This patch migrates some more internals (around generating novel inputs) to the IR layer (issue  CC(update version and changelog for pypi)). The canonical version of these notes (with links) is on readthedocs. Hypothesis for Python  version 6.119.0 This release improves Hypothesis' handling of ExceptionGroup  it's now able to detect marker detections if they're inside a  group and attempts to resolve them. Note that this handling is still a work )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Bump hypothesis from 6.102.4 to 6.119.3,"Bumps hypothesis from 6.102.4 to 6.119.3.  Release notes Sourced from hypothesis's releases.  Hypothesis for Python  version 6.119.3 Hypothesis collects coverage information during the &quot;shrink&quot; and &quot;explain&quot; phases in order to show a more informative error message. On 3.12+, this uses &quot;sys.monitoring&quot;. This patch improves the performance of coverage collection on 3.12+ by disabling events we don't need. The canonical version of these notes (with links) is on readthedocs. Hypothesis for Python  version 6.119.2 This patch refactors some internals to prepare for future work using our IR (issue  CC(update version and changelog for pypi)). The canonical version of these notes (with links) is on readthedocs. Hypothesis for Python  version 6.119.1 This patch migrates some more internals (around generating novel inputs) to the IR layer (issue  CC(update version and changelog for pypi)). The canonical version of these notes (with links) is on readthedocs. Hypothesis for Python  version 6.119.0 This release improves Hypothesis' handling of ExceptionGroup  it's now able to detect marker detections if they're inside a  group and attempts to resolve them. Note that this handling is still a work ",2024-11-18T18:00:27Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/24950,Superseded by CC(Bump hypothesis from 6.102.4 to 6.119.4).
407,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([mosaic_gpu] Fix signedness handling in FragmentedArray._pointwise.)ï¼Œ å†…å®¹æ˜¯ ([mosaic_gpu] Fix signedness handling in FragmentedArray._pointwise. Only propagate signedness from operands when the output type of `op` is an `ir.IntegerType`.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[mosaic_gpu] Fix signedness handling in FragmentedArray._pointwise.,[mosaic_gpu] Fix signedness handling in FragmentedArray._pointwise. Only propagate signedness from operands when the output type of `op` is an `ir.IntegerType`.,2024-11-18T16:12:36Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24946
304,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([mosaic_gpu] Fixed `FragmentedArray` comparisons with literals)ï¼Œ å†…å®¹æ˜¯ ([mosaic_gpu] Fixed `FragmentedArray` comparisons with literals)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[mosaic_gpu] Fixed `FragmentedArray` comparisons with literals,[mosaic_gpu] Fixed `FragmentedArray` comparisons with literals,2024-11-18T11:29:47Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24943
1098,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([GPU] FlashAttention performance lags behind PyTorch)ï¼Œ å†…å®¹æ˜¯ ( Description I'm benchmarking naive FlashAttention in `Jax` vs. the Pallas's version of `FA3` vs. the new `dot_product_attention` interface with `cudnn` backend.  JAX/XLA's performance:   !image  Torch's performance:   !image Why the discrepancy? I'd have expected performance to touch 550600 TFLOPS/s. I'm using a few XLA flags, as specified in the script below  but is there anything I'm missing? Or is this about the maximum `XLA` can deliver on H100s?  Steps to reproduce 1. Recreate the environment using `uv`. I'm assuming the drivers are installed. If not, you can use the `pytorch/pytorch:2.4.0cuda12.4.1cudnn8runtime` image on the GPU, run the preliminary `aptget update` and `aptget upgrade` to set everything up.  3. Run either script **JAX script**      **PyTorch Benchmark script**        System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[GPU] FlashAttention performance lags behind PyTorch," Description I'm benchmarking naive FlashAttention in `Jax` vs. the Pallas's version of `FA3` vs. the new `dot_product_attention` interface with `cudnn` backend.  JAX/XLA's performance:   !image  Torch's performance:   !image Why the discrepancy? I'd have expected performance to touch 550600 TFLOPS/s. I'm using a few XLA flags, as specified in the script below  but is there anything I'm missing? Or is this about the maximum `XLA` can deliver on H100s?  Steps to reproduce 1. Recreate the environment using `uv`. I'm assuming the drivers are installed. If not, you can use the `pytorch/pytorch:2.4.0cuda12.4.1cudnn8runtime` image on the GPU, run the preliminary `aptget update` and `aptget upgrade` to set everything up.  3. Run either script **JAX script**      **PyTorch Benchmark script**        System info (python version, jaxlib version, accelerator, etc.) ",2024-11-17T02:14:54Z,bug,open,0,4,https://github.com/jax-ml/jax/issues/24934,Update: I changed the `torch` script to use `FlopCounterMode`. Now the results are more realistic/accurate but JAX still lags behind despite explicitly being forced to use `CuDNN`.  ,"i think your torch script might not work as expected, since the inputs format of `torch.nn.functional.scaled_dot_product_attention` is [B, H, T, C] [https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html] ","This is a complicated benchmarking setup, with many things potentially going wrong. Can you simplify this to just measuring milliseconds, and also have a correctness test (that PyTorch and JAX give the same output for the same input)."," Thank you for pointing that out.  I have updated both scripts to now report times as well. However, I opted to skip correctness tests because reproducibility requires us to sacrifice performance which I'm afraid of touching The variance however is very low between runs plus we can average over multiple steps (`sx`) so this should be a nonissue.  On `A100`:  **JAX**: !image  **Torch**: !image"
775,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Allow einsum to support naive contraction strategy)ï¼Œ å†…å®¹æ˜¯ (I would like to compute an einsum according to the following formula:  I want to express the computation as 4 nested for loops over indices i, j, k, l without creating any intermediate arrays.  As far as einsum_path is concerned, I can do this by passing the einsum path directly as [(0, 1, 2, 3, 4, 5)] via the optimize kwarg).  However, when I try to do the einsum, I get this NotImplementedError with a comment that says "" if this is actually reachable, open an issue!"" https://github.com/jaxml/jax/blob/main/jax/_src/numpy/lax_numpy.pyL9775 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Allow einsum to support naive contraction strategy,"I would like to compute an einsum according to the following formula:  I want to express the computation as 4 nested for loops over indices i, j, k, l without creating any intermediate arrays.  As far as einsum_path is concerned, I can do this by passing the einsum path directly as [(0, 1, 2, 3, 4, 5)] via the optimize kwarg).  However, when I try to do the einsum, I get this NotImplementedError with a comment that says "" if this is actually reachable, open an issue!"" https://github.com/jaxml/jax/blob/main/jax/_src/numpy/lax_numpy.pyL9775 ",2024-11-15T17:39:48Z,enhancement,open,0,12,https://github.com/jax-ml/jax/issues/24915,"I think your path specification is invalid. For example, if you pass it to NumPy, you get this error:  ","Thank you for taking a look!  My understanding is that this path is the default behavior for numpy.  I.e., it corresponds to the basic implementation that you have in  https://github.com/jaxml/jax/blob/main/tests/lax_numpy_einsum_test.pyL295 It is much more memory efficient than doing the einsum as a sequence of dot_general's in this case, which from my investigation is hardcoded into the JAX implementation.  It makes sense because dot_general is very highly optimized, but being able to get the more memoryefficient behavior seems desirable in some settings.  ","I prototyped a version of this using a sequence of nested jax.lax.scan calls, but it was ugly and I don't think the most performant.  I also played around with using Jax.vmap over the indices (i, j) and using jnp.einsum using the perelement path Complete contraction:  ij,ik,il,jk,jl,kl>ij [vmap] PerRow contraction:  j,k,l,jk,jl,kl>j [double vmap] Perelement contraction:  ,k,l,k,l,kl> It was pretty cool to use JAX's abstractions to achieve this, and the vmap implementation did have better performance characteristics than jnp.einsum  in this case, but I still think it uses more memory than the naive approach.   If Jax.lax.map supported the in_axes argument, I think that would help, since I could just replace my usage of vmap with map.","Here is a basic implementation of the naive strategy in terms of jax.vmap and jax.lax.scan, specialized to the formula 'ij,ik,il,jk,jl,kl>ij'.  when I benchmark it using n x n arrays for n = [128, 256, 512, 1024] here is what I get for timing information (measured in seconds, not counting JIT compilation).  The story is that jnp.einsum is faster up to n=512, but fails at n=1024, while the naive approach implemented above still runs, albeit it takes more time than I'd like. ","Here's another impl one can throw into the mix: `scan_einsum` where we strip out a nonoutput axis and sequentially compute + add up the resulting smaller einsums, as follows:  Benchmarks show that this is significantly better than the vmap_einsum above.  And it's even better than jnp.einsum beyond n=256 ","If anyone is interested, I typed up this exploration on my blog: https://www.ryanhmckenna.com/2024/11/exploringmultiinputeinsumsinjax.html","Thanks for exploring this â€“ are you running benchmarks on GPU/TPU as well, or just CPU? The reason I ask is that `scan` has a pretty big performance penalty on accelerators (essentially each iteration is its own kernel launch) so I expect any efficiency gains on CPU will not transfer to GPU or TPU.","These tests were done in a colab sandbox with GPU, happy to do some more benchmarking if there's something specific you'd like to see","OK, thanks. Overall, I tend to be 1 on changes like this. It greatly complicates things on the JAX side in order to make up for deficiencies in the compiler. The compiler behavior may be improved in the future, at which point we would needlessly be generating more complicated code with no clear way of alerting ourselves that this is the case.","Is this a compiler deficiency though?  My understanding is it is a JAX implementation choice that leads to this behavior, specifically https://github.com/jaxml/jax/blob/main/jax/_src/numpy/lax_numpy.pyL9773, which implements einsum in terms of a ""_dot_general"" primitive, which I believe means the einsum is calculated as a sequence of pairwise contractions.  Even if the compiler was better at _dot_general, it wouldn't get around the intractability of storing the required n^3 sized intermediates in this case. Happy to keep this alternate implementation local to where I need it though to keep the jax impls simpler though.","The compiler often fuses sequences of operations into single kernels to avoid storing intermediates. There may already be fusion paths for sequences of `dot_general` in some situations, but I'm not sure. `scan` is a much less specific primitive than dot general, so emitting scan would hamper the ability of the compiler to make such optimizations in the future. I'm not saying your code is not useful; I think the approach probably makes sense in some situations. I just don't think it's a good fit for JAX's einsum implementation. (If  disagrees though, I'm happy to defer to his judgment here).","Ah I see that makes sense, do you think I should open up an issue at https://github.com/openxla/xla in that case?"
558,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Set `__module__` attribute for objects in jax.numpy)ï¼Œ å†…å®¹æ˜¯ (The current `__module__` string is an implementation detail, and displays internal paths where we should display public paths. For example, this is the current output:  and this is the output after this change:  We've done this already for several other toplevel modules; this PR continues the effort to improve this situation.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Set `__module__` attribute for objects in jax.numpy,"The current `__module__` string is an implementation detail, and displays internal paths where we should display public paths. For example, this is the current output:  and this is the output after this change:  We've done this already for several other toplevel modules; this PR continues the effort to improve this situation.",2024-11-15T14:39:54Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/24912,> Can you add the reason for doing this to the PR description? Done
1455,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add JAX_COMPILATION_CACHE_EXPECT_PGLE option)ï¼Œ å†…å®¹æ˜¯ (This aims to provide a better PGLE workflow that is compatible with profiling with Nsight Systems on GPU. This is nontrivial because CUPTI, the interface used by CUDA profiling tools, only supports profiling by one tool at a time, meaning that the JAX profiler used by PGLE and Nsight Systems conflict with one another. The PR adds a new JAX config option `compilation_cache_expect_pgle` that tells JAX to attempt to load PGLEoptimised entries from the compilation cache even if PGLE is disabled, and print warnings on certain unexpected results. With this, a workflow like:  is possible. Warnings are added in three cases if `JAX_COMPILATION_CACHE_EXPECT_PGLE` is enabled:  If a module is cached without PGLE optimisations but not with them. That is typical of modules that were not executed enough times in the first (cachepopulating) run to reach the threshold for recompilation with profile data. This is seen above. We would rely on the user to see ""initialize ... not executed enough times"" and think ""sounds fine"".  If a module is written to the cache. This is typical of a cachepopulating run that did not hit as many code paths as the second run with Nsight Systems + `JAX_COMPILATION_CACHE_EXPECT_PGLE`.  If the PGLE prof)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add JAX_COMPILATION_CACHE_EXPECT_PGLE option,"This aims to provide a better PGLE workflow that is compatible with profiling with Nsight Systems on GPU. This is nontrivial because CUPTI, the interface used by CUDA profiling tools, only supports profiling by one tool at a time, meaning that the JAX profiler used by PGLE and Nsight Systems conflict with one another. The PR adds a new JAX config option `compilation_cache_expect_pgle` that tells JAX to attempt to load PGLEoptimised entries from the compilation cache even if PGLE is disabled, and print warnings on certain unexpected results. With this, a workflow like:  is possible. Warnings are added in three cases if `JAX_COMPILATION_CACHE_EXPECT_PGLE` is enabled:  If a module is cached without PGLE optimisations but not with them. That is typical of modules that were not executed enough times in the first (cachepopulating) run to reach the threshold for recompilation with profile data. This is seen above. We would rely on the user to see ""initialize ... not executed enough times"" and think ""sounds fine"".  If a module is written to the cache. This is typical of a cachepopulating run that did not hit as many code paths as the second run with Nsight Systems + `JAX_COMPILATION_CACHE_EXPECT_PGLE`.  If the PGLE prof",2024-11-15T10:43:27Z,,open,0,1,https://github.com/jax-ml/jax/issues/24910, PTAL
365,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Don't use an out-of-line lowering for integer_pow for small powers.)ï¼Œ å†…å®¹æ˜¯ (This yields a smaller stablehlo output. Add a fast path for y == 1 and y == 1, which turn out to be reasonably common.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Don't use an out-of-line lowering for integer_pow for small powers.,"This yields a smaller stablehlo output. Add a fast path for y == 1 and y == 1, which turn out to be reasonably common.",2024-11-14T16:23:11Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/24897
615,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Missing annotations)ï¼Œ å†…å®¹æ˜¯ ( Description The additions of type annotations for parts of jax was nice, but unfortunately they appear to have diverged from the source itself. For example,  `jax/numpy/__init__.pyi` is missing `jax.numpy.frompyfunc`. This is unfortunate on its own, but also interferes with editors like pycharm, who use the type annotations for autocomplete, etc..  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Missing annotations," Description The additions of type annotations for parts of jax was nice, but unfortunately they appear to have diverged from the source itself. For example,  `jax/numpy/__init__.pyi` is missing `jax.numpy.frompyfunc`. This is unfortunate on its own, but also interferes with editors like pycharm, who use the type annotations for autocomplete, etc..  System info (python version, jaxlib version, accelerator, etc.) ",2024-11-13T21:42:10Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/24888,Thanks for bringing this to our attention! Would you like to send a PR adding `frompyfunc` to `jax/numpy/__init__.pyi`? If not I can put it on my TODO list. Thanks!,"Hi Jake, I'm happy to add such an annotation in a PR, but .. should this not be automated somehow? This is just the one thing I found picking up Jax to try and learn as a novice, will it not become whackamole as new features are added?","the PYI file was automaticallygenerated at one point, and then edited by hand to define the APIs. `jax.numpy` is relatively stable, so this is not something that changes frequently. If you know of any way to automate this, I'm happy to hear about it!","It looks like there are four symbols currently missing from `jax/numpy/__init__.pyi`:  `int2` and `uint2` can't be added yet, because they will only exist in the namespace if `ml_dtypes` is a sufficiently recent version. But we should add type stubs for `frompyfunc` and `ufunc`."," I guess 's question (and mine) is why a function's type signature/overloads can't be fetched automatically from where it is implemented (e.g. here for `frompyfunc`, here for `where`, etc.), instead of duplicating them in `__init__.pyi`. This would satisfy the DRY principle.","The main reason is that some of these APIs are wrapped in `` (or `(jit, ...)`), and the semantics of `jit` and `partial` are not yet supported by some type checkers.","I'm going to close this, as the reported issue has been addressed."
264,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Pallas] Increase test coverage of pl.dot.)ï¼Œ å†…å®¹æ˜¯ ([Pallas] Increase test coverage of pl.dot.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[Pallas] Increase test coverage of pl.dot.,[Pallas] Increase test coverage of pl.dot.,2024-11-13T17:32:54Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24880
620,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Dedent your yields!)ï¼Œ å†…å®¹æ˜¯ (Dedent your yields! Fixes a surprising interaction between the generator system in linear_util.py and the try/finally python context managers we use for managing tracing context. The `finally` block wasn't always being called until garbage collection, so the context stack pushes/pops weren't always correctly nested. Dedenting the yield fixes this particular bug but longterm we should get rid of linear_util altogether.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Dedent your yields!,"Dedent your yields! Fixes a surprising interaction between the generator system in linear_util.py and the try/finally python context managers we use for managing tracing context. The `finally` block wasn't always being called until garbage collection, so the context stack pushes/pops weren't always correctly nested. Dedenting the yield fixes this particular bug but longterm we should get rid of linear_util altogether.",2024-11-12T22:32:26Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24870
1181,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Unsupported type in metal PJRT plugin with rng_bit_generator)ï¼Œ å†…å®¹æ˜¯ ( Description Hi all, When executing an HLO program using the Metal PJRT plugin, the program fails due to an unsupported data type eeturned by the rng_bit_generator operation. Specifically, the generated HLO includes: `%output_state, %output = ""mhlo.rng_bit_generator""(%1) }> : (tensor) > (tensor, tensor) ` The error message indicates that: `Metal only supports MPSDataTypeFloat16, MPSDataTypeBFloat16, MPSDataTypeFloat32, MPSDataTypeInt32, and MPSDataTypeInt64.` The use of ui32 seems to be incompatible with Metalâ€™s allowed types. Iâ€™m trying to understand if the ui32 output is the problem or maybe the use of rng_bit_generator is wrong. Could you clarify if there is a workaround or planned support for ui32 output in this context? Alternatively, guidance on configuring rng_bit_generator for compatibility with Metalâ€™s supported types would be greatly appreciated. Thanks  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Unsupported type in metal PJRT plugin with rng_bit_generator," Description Hi all, When executing an HLO program using the Metal PJRT plugin, the program fails due to an unsupported data type eeturned by the rng_bit_generator operation. Specifically, the generated HLO includes: `%output_state, %output = ""mhlo.rng_bit_generator""(%1) }> : (tensor) > (tensor, tensor) ` The error message indicates that: `Metal only supports MPSDataTypeFloat16, MPSDataTypeBFloat16, MPSDataTypeFloat32, MPSDataTypeInt32, and MPSDataTypeInt64.` The use of ui32 seems to be incompatible with Metalâ€™s allowed types. Iâ€™m trying to understand if the ui32 output is the problem or maybe the use of rng_bit_generator is wrong. Could you clarify if there is a workaround or planned support for ui32 output in this context? Alternatively, guidance on configuring rng_bit_generator for compatibility with Metalâ€™s supported types would be greatly appreciated. Thanks  System info (python version, jaxlib version, accelerator, etc.) ",2024-11-12T21:50:50Z,bug Apple GPU (Metal) plugin,open,1,0,https://github.com/jax-ml/jax/issues/24867
1364,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(7x7 `nnx.Conv` using `float32` parameter dtype overflows(?) to `nan` when sharded)ï¼Œ å†…å®¹æ˜¯ ( Description A sufficientlylarge (7x7) `nnx.Conv` with `float32` parameter dtype, when sharded across multiple devices, generates `nan`, seemingly due to overflow. The `nan` is avoided by making any one of the following changes: 1. Use smaller convolution, e.g. 3x3 2. Enable `param_dtype='float64'` inside `with jax.experimental.enable_x64()` 3. Disable sharding; run on one device only float32 is sufficient for 7x7 convolution when not sharded, suggesting that it ought to work when sharded as well. The issue can be worked around but all of the workarounds are unsatisfactory in some way, either by reducing the size of the convolution, or requiring double the memory usage, or restricting training to a single device. Minimal example. This code works because it's moved to float64:  This version fails due to use of sharding plus float32 and a 7x7 convolution:  This version works due to using a smaller convolution, still with float32:  This version works due to running on a single device, but with float32 and a 7x7 convolution:   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,7x7 `nnx.Conv` using `float32` parameter dtype overflows(?) to `nan` when sharded," Description A sufficientlylarge (7x7) `nnx.Conv` with `float32` parameter dtype, when sharded across multiple devices, generates `nan`, seemingly due to overflow. The `nan` is avoided by making any one of the following changes: 1. Use smaller convolution, e.g. 3x3 2. Enable `param_dtype='float64'` inside `with jax.experimental.enable_x64()` 3. Disable sharding; run on one device only float32 is sufficient for 7x7 convolution when not sharded, suggesting that it ought to work when sharded as well. The issue can be worked around but all of the workarounds are unsatisfactory in some way, either by reducing the size of the convolution, or requiring double the memory usage, or restricting training to a single device. Minimal example. This code works because it's moved to float64:  This version fails due to use of sharding plus float32 and a 7x7 convolution:  This version works due to using a smaller convolution, still with float32:  This version works due to running on a single device, but with float32 and a 7x7 convolution:   System info (python version, jaxlib version, accelerator, etc.) ",2024-11-12T01:32:17Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/24848,"Apologies, didn't realize flax is in a separate repo. I reposted this there."
826,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Deepcopy of pjit functions failing (or not supported))ï¼Œ å†…å®¹æ˜¯ ( Description While trying to get `jsonargparse` to work with defaults on activation functions, I ran into https://github.com/omnius/jsonargparse/issues/619issuecomment2466451720 The preferred MWE from the developer there is repeated below  It seems like the issue is that a `deepcopy` and perhaps a `asdict` of the jit'd functions is failing or not supported. As said in that post, the workaround shows there is no intrinsic reason `deepcopy` can't be supported.  If we put  Then things work great.  System info (python version, jaxlib version, accelerator, etc.) This applies to all versions.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Deepcopy of pjit functions failing (or not supported)," Description While trying to get `jsonargparse` to work with defaults on activation functions, I ran into https://github.com/omnius/jsonargparse/issues/619issuecomment2466451720 The preferred MWE from the developer there is repeated below  It seems like the issue is that a `deepcopy` and perhaps a `asdict` of the jit'd functions is failing or not supported. As said in that post, the workaround shows there is no intrinsic reason `deepcopy` can't be supported.  If we put  Then things work great.  System info (python version, jaxlib version, accelerator, etc.) This applies to all versions.",2024-11-11T16:49:48Z,bug,open,1,0,https://github.com/jax-ml/jax/issues/24838
1271,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TPU with sharding: grpc initialization failure)ï¼Œ å†…å®¹æ˜¯ ( Description I'm running a decently large project, and I've just attempted to run it on TPUs (v432 node), and I'm getting an interesting error, which seems to be pretty internal:  When enabling internal logs, I get (on one of the machines):  The main issue with this bug is that I cannot provide a script to reproduce it: the codebase is very large, and my attempts at reproducing with a small example just don't lead to the error. And, I cannot even give you a link to the repo, because it's confidential. Here is how I do the partitioning (I think these days you should use jax.device_put, but I'm still using the old API cuz I'm used to it):  Equinox / just split PyTree into two/combine two PyTrees into one, such that one of the PyTrees contains all the leaves that satisfy a particular condition. This is convenient when trying to pass a partially static PyTree into a vmap, for example. I am 0.8 confident that the issue is not with Equinox. I would appreciate any help :)  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,TPU with sharding: grpc initialization failure," Description I'm running a decently large project, and I've just attempted to run it on TPUs (v432 node), and I'm getting an interesting error, which seems to be pretty internal:  When enabling internal logs, I get (on one of the machines):  The main issue with this bug is that I cannot provide a script to reproduce it: the codebase is very large, and my attempts at reproducing with a small example just don't lead to the error. And, I cannot even give you a link to the repo, because it's confidential. Here is how I do the partitioning (I think these days you should use jax.device_put, but I'm still using the old API cuz I'm used to it):  Equinox / just split PyTree into two/combine two PyTrees into one, such that one of the PyTrees contains all the leaves that satisfy a particular condition. This is convenient when trying to pass a partially static PyTree into a vmap, for example. I am 0.8 confident that the issue is not with Equinox. I would appreciate any help :)  System info (python version, jaxlib version, accelerator, etc.) ",2024-11-10T05:59:04Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/24821,"Update: downgrading libtpu does not help, however, if I set  I get a different internal error:  And an insanely log traceback which I'm not going to attach to not dilute the point.","Okay, after a while it seems that the issue is indeed with Equinox; I will duplicate issue to Patrick"
1271,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TPU with sharding: grpc initialization failure)ï¼Œ å†…å®¹æ˜¯ ( Description I'm running a decently large project, and I've just attempted to run it on TPUs (v432 node), and I'm getting an interesting error, which seems to be pretty internal:  When enabling internal logs, I get (on one of the machines):  The main issue with this bug is that I cannot provide a script to reproduce it: the codebase is very large, and my attempts at reproducing with a small example just don't lead to the error. And, I cannot even give you a link to the repo, because it's confidential. Here is how I do the partitioning (I think these days you should use jax.device_put, but I'm still using the old API cuz I'm used to it):  Equinox / just split PyTree into two/combine two PyTrees into one, such that one of the PyTrees contains all the leaves that satisfy a particular condition. This is convenient when trying to pass a partially static PyTree into a vmap, for example. I am 0.8 confident that the issue is not with Equinox. I would appreciate any help :)  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,TPU with sharding: grpc initialization failure," Description I'm running a decently large project, and I've just attempted to run it on TPUs (v432 node), and I'm getting an interesting error, which seems to be pretty internal:  When enabling internal logs, I get (on one of the machines):  The main issue with this bug is that I cannot provide a script to reproduce it: the codebase is very large, and my attempts at reproducing with a small example just don't lead to the error. And, I cannot even give you a link to the repo, because it's confidential. Here is how I do the partitioning (I think these days you should use jax.device_put, but I'm still using the old API cuz I'm used to it):  Equinox / just split PyTree into two/combine two PyTrees into one, such that one of the PyTrees contains all the leaves that satisfy a particular condition. This is convenient when trying to pass a partially static PyTree into a vmap, for example. I am 0.8 confident that the issue is not with Equinox. I would appreciate any help :)  System info (python version, jaxlib version, accelerator, etc.) ",2024-11-10T05:59:04Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/24821,"Update: downgrading libtpu does not help, however, if I set  I get a different internal error:  And an insanely log traceback which I'm not going to attach to not dilute the point.","Okay, after a while it seems that the issue is indeed with Equinox; I will duplicate issue to Patrick"
772,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Pattern match dot algorithm spec to preset name)ï¼Œ å†…å®¹æ˜¯ (Since some dot algorithm presets have special cased input and output storage type behavior (e.g. all the `BF16` algorithms), and since JAX's handling of these cases is (for better or worse) handled at the ""preset"" level, this PR provides a small quality of life improvement to convert known `lax.DotAlgorithm` specs to explicit `lax.DotAlgorithmPreset` members whenever possible. For example, if a user specifies:  this will be canonicalized to `lax.DotAlgorithmPreset.BF16_BF16_F32_X6` and the input and output casting will be handled properly.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Pattern match dot algorithm spec to preset name,"Since some dot algorithm presets have special cased input and output storage type behavior (e.g. all the `BF16` algorithms), and since JAX's handling of these cases is (for better or worse) handled at the ""preset"" level, this PR provides a small quality of life improvement to convert known `lax.DotAlgorithm` specs to explicit `lax.DotAlgorithmPreset` members whenever possible. For example, if a user specifies:  this will be canonicalized to `lax.DotAlgorithmPreset.BF16_BF16_F32_X6` and the input and output casting will be handled properly.",2024-11-09T15:51:56Z,pull ready,open,0,0,https://github.com/jax-ml/jax/issues/24820
308,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Adds coverage for spmd-axisname-filtering in shard_map transpose.)ï¼Œ å†…å®¹æ˜¯ (Adds coverage for spmdaxisnamefiltering in shard_map transpose.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Adds coverage for spmd-axisname-filtering in shard_map transpose.,Adds coverage for spmdaxisnamefiltering in shard_map transpose.,2024-11-08T23:03:36Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24811
858,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Division by self not always ""1.0"" on JAX GPU, but consistently gives ""1.0"" on JAX CPU.)ï¼Œ å†…å®¹æ˜¯ ( Description Division by self is not always ""1.0"" on JAX GPU, but seems to consistently give ""1.0"" on JAX CPU. Observed for jax arrays, but not for floats. Question: Is this expected in JAX? If so, is the expectation that a JAX user should build robustness against GPU/CPU floating point differences (as well as the specific divide by self \neq 1 case)?  Code for observing the issue   CPU environment   Running on CPU environment yields:   GPU environment   Running on GPU environment yields:   System info (python version, jaxlib version, accelerator, etc.)  System Info on GPU environment )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"Division by self not always ""1.0"" on JAX GPU, but consistently gives ""1.0"" on JAX CPU."," Description Division by self is not always ""1.0"" on JAX GPU, but seems to consistently give ""1.0"" on JAX CPU. Observed for jax arrays, but not for floats. Question: Is this expected in JAX? If so, is the expectation that a JAX user should build robustness against GPU/CPU floating point differences (as well as the specific divide by self \neq 1 case)?  Code for observing the issue   CPU environment   Running on CPU environment yields:   GPU environment   Running on GPU environment yields:   System info (python version, jaxlib version, accelerator, etc.)  System Info on GPU environment ",2024-11-08T21:35:38Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/24807,"Hi  thanks for the report! Though this may be a bit surprising, I think it's consistent with the expected precision of floating point operations: namely, the outputs in each case are within 1 ULP of the true result at float32 precision. I think  may be able to say more."
544,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Allow more output storage types for some dot algorithms.)ï¼Œ å†…å®¹æ˜¯ (Allow more output storage types for some dot algorithms. As reported in https://github.com/jaxml/jax/issues/24794, there were some dot products that were resulting in an unnecessary conversion. This change makes the output storage type selection more flexible. Fixes https://github.com/jaxml/jax/issues/24794)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Allow more output storage types for some dot algorithms.,"Allow more output storage types for some dot algorithms. As reported in https://github.com/jaxml/jax/issues/24794, there were some dot products that were resulting in an unnecessary conversion. This change makes the output storage type selection more flexible. Fixes https://github.com/jaxml/jax/issues/24794",2024-11-08T20:23:36Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24800
401,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([AutoPGLE] Explicitly ignore host callback pointers)ï¼Œ å†…å®¹æ˜¯ ([AutoPGLE] Explicitly ignore host callback pointers Before this change users had to specify remove_custom_partitioning_ptr_from_cache_key config flag when using AutoPGLE.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,[AutoPGLE] Explicitly ignore host callback pointers,[AutoPGLE] Explicitly ignore host callback pointers Before this change users had to specify remove_custom_partitioning_ptr_from_cache_key config flag when using AutoPGLE.,2024-11-08T18:24:36Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24797
583,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`jax.lax.DotAlgorithm` generates an unnecessary conversion on H100)ï¼Œ å†…å®¹æ˜¯ ( Description H100 hardware and cuBLAS support fp16xfp16>fp16 GEMM with accumulation in fp32. Specifying that GEMM config doesn't seem to work:  This produces the following StableHLO, note the unexpected `stablehlo.convert`:  and causes an extra conversion kernel to run:   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,`jax.lax.DotAlgorithm` generates an unnecessary conversion on H100," Description H100 hardware and cuBLAS support fp16xfp16>fp16 GEMM with accumulation in fp32. Specifying that GEMM config doesn't seem to work:  This produces the following StableHLO, note the unexpected `stablehlo.convert`:  and causes an extra conversion kernel to run:   System info (python version, jaxlib version, accelerator, etc.) ",2024-11-08T17:36:45Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/24794,"Thanks for the report. Fixed in https://github.com/jaxml/jax/pull/24800, although this requires the use of a ""preset"" rather than the general struct:  The `jax.lax.DotAlgorithm` version will always cast for now. It would be possible to add the pattern matching to that, but it's annoying enough to do that it's not currently a high priority. I'd recommend using the presets whenever possible!","Thanks, how does one parse `F16_F16_F32`, is the format documented?",Yep! The supported presets are all listed here: https://jax.readthedocs.io/en/latest/jax.lax.htmljax.lax.DotAlgorithmPreset
294,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([mgpu] Broadcast the LHS fragmented array if it is splat.)ï¼Œ å†…å®¹æ˜¯ ([mgpu] Broadcast the LHS fragmented array if it is splat.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[mgpu] Broadcast the LHS fragmented array if it is splat.,[mgpu] Broadcast the LHS fragmented array if it is splat.,2024-11-08T13:08:31Z,,open,0,0,https://github.com/jax-ml/jax/issues/24780
264,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Pallas] Increase test coverage of pl.dot.)ï¼Œ å†…å®¹æ˜¯ ([Pallas] Increase test coverage of pl.dot.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[Pallas] Increase test coverage of pl.dot.,[Pallas] Increase test coverage of pl.dot.,2024-11-08T05:18:20Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24777
516,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Added ragged attention for MQA/MHA/GQA)ï¼Œ å†…å®¹æ˜¯ (This PR adds ragged attention for different input shapes. Ragged attention is a decoding attention kernel that restricts how many keys are attended to. An array `lengths` of size (batch_size,) is passed in as input and specifies the number of valid keys for each corresponding example in the batch.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Added ragged attention for MQA/MHA/GQA,"This PR adds ragged attention for different input shapes. Ragged attention is a decoding attention kernel that restricts how many keys are attended to. An array `lengths` of size (batch_size,) is passed in as input and specifies the number of valid keys for each corresponding example in the batch.",2024-11-07T19:17:39Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24763
457,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jnp.reshape: raise TypeError when specifying newshape)ï¼Œ å†…å®¹æ˜¯ (The `reshape` parameter was deprecated in JAX v0.4.28 (released May 9 2024). We're now seven months and seven releases beyond this, which well exceeds the minimum deprecation timeline set out in the API compatibility policy.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jnp.reshape: raise TypeError when specifying newshape,"The `reshape` parameter was deprecated in JAX v0.4.28 (released May 9 2024). We're now seven months and seven releases beyond this, which well exceeds the minimum deprecation timeline set out in the API compatibility policy.",2024-11-06T20:57:47Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/24748
1436,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TracerBoolConversionError)ï¼Œ å†…å®¹æ˜¯ ( Description I am trying to create Mandelbrot Set using Jax and after transforming the function using vmap i got `TracerBoolConversionError` error message code:  { 	""name"": ""TracerBoolConversionError"", 	""message"": ""Attempted boolean conversion of traced array with shape bool[1]. This BatchTracer with object id 140294572667376 was created on line:   /tmp/ipykernel_16031/267948302.py:5:11 (mandle_brot) See https://jax.readthedocs.io/en/latest/errors.htmljax.errors.TracerBoolConversionError"", 	""stack"": "" TracerBoolConversionError                 Traceback (most recent call last) Cell In[123], line 1 > 1 mandle_brot_vmap(Z,10)     [... skipping hidden 3 frame] Cell In[91], line 5, in mandle_brot(c, max_iter)       3 for i in range(max_iter):       4     z=z*z+c > 5     if(jnp.abs(z)>2.0):       6         return jnp.array([i])       7 return jnp.array([max_iter])     [... skipping hidden 1 frame] File ~/anaconda3/envs/ML/lib/python3.11/sitepackages/jax/_src/core.py:1554, in concretization_function_error..error(self, arg)    1553 def error(self, arg): > 1554   raise TracerBoolConversionError(arg) TracerBoolConversionError: Attempted boolean conversion of traced array with shape bool[1]. This BatchTracer with object id)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,TracerBoolConversionError," Description I am trying to create Mandelbrot Set using Jax and after transforming the function using vmap i got `TracerBoolConversionError` error message code:  { 	""name"": ""TracerBoolConversionError"", 	""message"": ""Attempted boolean conversion of traced array with shape bool[1]. This BatchTracer with object id 140294572667376 was created on line:   /tmp/ipykernel_16031/267948302.py:5:11 (mandle_brot) See https://jax.readthedocs.io/en/latest/errors.htmljax.errors.TracerBoolConversionError"", 	""stack"": "" TracerBoolConversionError                 Traceback (most recent call last) Cell In[123], line 1 > 1 mandle_brot_vmap(Z,10)     [... skipping hidden 3 frame] Cell In[91], line 5, in mandle_brot(c, max_iter)       3 for i in range(max_iter):       4     z=z*z+c > 5     if(jnp.abs(z)>2.0):       6         return jnp.array([i])       7 return jnp.array([max_iter])     [... skipping hidden 1 frame] File ~/anaconda3/envs/ML/lib/python3.11/sitepackages/jax/_src/core.py:1554, in concretization_function_error..error(self, arg)    1553 def error(self, arg): > 1554   raise TracerBoolConversionError(arg) TracerBoolConversionError: Attempted boolean conversion of traced array with shape bool[1]. This BatchTracer with object id",2024-11-06T16:41:54Z,question,closed,0,4,https://github.com/jax-ml/jax/issues/24740,"As the error indicates, the python control flow of `jnp.abs(z)>2.0` is problematic. You can use `jax.lax.cond` instead. See https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.htmlpythoncontrolflowjit and https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.cond.html. ","The problem is with this line:  `jnp.abs(z)` is a traced value, and so its value cannot be known while the Python `if` statement is executing. For background on this, I'd start with JAX Key Concepts, and then refer to JAX Sharp Bits: Control Flow. The solution here will likely be to use `jax.lax.while_loop` rather than a Python `while` loop. Feel free to ask back here if you have any questions!","> You can use `jax.lax.cond` instead. This is true in many cases, but in this particular code `cond` will not work, because it cannot be used to break out of a Python forloop.","> > You can use `jax.lax.cond` instead. >  > This is true in many cases, but in this particular code `cond` will not work, because it cannot be used to break out of a Python forloop. True, I was just thinking of something to the effect of   A while loop does seem more natural tho (although if they are being vmapped over, does the while loop save anything since the ones that finish early in the vmap just have redundant computation after that point that is post selected out?). "
520,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix debug_nans false positive in jnp.quantile)ï¼Œ å†…å®¹æ˜¯ (Fixes CC(median FloatingPointError: invalid value (nan) encountered in jit(convert_element_type)) I'm deliberately leaving out test coverage, because we don't cover this combination of configurations (`debug_nans` + `disable_jit`). But we're happy to fix these kinds of issues when they come up.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Fix debug_nans false positive in jnp.quantile,"Fixes CC(median FloatingPointError: invalid value (nan) encountered in jit(convert_element_type)) I'm deliberately leaving out test coverage, because we don't cover this combination of configurations (`debug_nans` + `disable_jit`). But we're happy to fix these kinds of issues when they come up.",2024-11-05T23:37:43Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/24733
394,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯( InconclusiveDimensionOperation: Symbolic dimension comparison 'b' < '2147483647' is inconclusive.)ï¼Œ å†…å®¹æ˜¯ ( Description A simple code to reproduce:  Output:   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm, InconclusiveDimensionOperation: Symbolic dimension comparison 'b' < '2147483647' is inconclusive.," Description A simple code to reproduce:  Output:   System info (python version, jaxlib version, accelerator, etc.) ",2024-11-05T22:07:05Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/24730,Assigning  who is most familiar with shape polymorphism and TF model exporting.,"In the immediate term, you can unblock by adding an explicit constraint 'b < '2147483647', as explained in the documentation link from the error message. The issue is that JAX lowering for `jnp.sort` uses an `iota` of indices and the dtype of the indices (`int32` or `int64`) depends on the size of the array. This means that this lowering is not shape polymorphic, because dtypes of values depend on the dimension values. I will be thinking how to handle this more nicely. E.g., we could always use `int64` for indices.","> we could always use `int64` for indices. This is probably a reasonable solution. The reason for the shapedependent dtype was because we were exploring the possibility of getting rid of the X64 flag and making APIs default to 32bit unless 64bit is explicitly requested or required â€“ that approach turned out not to be viable, but some vestiges of it (like this one) are still around."
1459,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([WIP] A minimal public API for custom primitives)ï¼Œ å†…å®¹æ˜¯ (Power users of JAX want to be able to control the behavior of some functions under all transformations. This is currently partially supported by the public APIs like `custom_jvp`, `custom_vjp`, and `custom_vmap`, and fully supported by the private APIs around `core.Primitive`. Two key limitations of the public APIs are: 1. There is no mechanism for customizing both forward and reverse mode AD, and 2. `custom_vmap` (besides being undocumented) doesn't support reverse mode AD unless combined with `custom_vjp`. The first issue is particularly important for kernel library authors who need to provide performant AD across a range of use cases, or for researchers using training algorithms that rely on bidirectional or higherorder AD. The second is more of a usability issue than a fundamental limitation. One place where these issues become particularly limiting is when writing opaque kernels using callbacks, `pallas_call`, or `ffi_call`. In those cases, good performance requires customizing all transformations, so many library authors reach for the private `core.Primitive` APIs. As a result, `core.Primitive` is widely used in the wild. This isn't a great state of affairs because the existing primitive APIs are brittle a)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[WIP] A minimal public API for custom primitives,"Power users of JAX want to be able to control the behavior of some functions under all transformations. This is currently partially supported by the public APIs like `custom_jvp`, `custom_vjp`, and `custom_vmap`, and fully supported by the private APIs around `core.Primitive`. Two key limitations of the public APIs are: 1. There is no mechanism for customizing both forward and reverse mode AD, and 2. `custom_vmap` (besides being undocumented) doesn't support reverse mode AD unless combined with `custom_vjp`. The first issue is particularly important for kernel library authors who need to provide performant AD across a range of use cases, or for researchers using training algorithms that rely on bidirectional or higherorder AD. The second is more of a usability issue than a fundamental limitation. One place where these issues become particularly limiting is when writing opaque kernels using callbacks, `pallas_call`, or `ffi_call`. In those cases, good performance requires customizing all transformations, so many library authors reach for the private `core.Primitive` APIs. As a result, `core.Primitive` is widely used in the wild. This isn't a great state of affairs because the existing primitive APIs are brittle a",2024-11-05T19:22:27Z,,open,0,5,https://github.com/jax-ml/jax/issues/24726,This is great! Have you thought about also adding `def_lowering` so users can use a different impl depending on device type?,"> Have you thought about also adding `def_lowering` so users can use a different impl depending on device type? Good question! I think that the approach I would advocate for instead would be to combine this with `lax.platform_dependent`. `lax.platform_dependent` covers all of the use cases for platformspecific lowering that I can come up with, and here that would just live within the primal function:  What do you think of that? Are there use cases that you know of that would need something more specific?","May I voice the need for exposing some way to define custom partitioning logic on the primitive as well?  Recently, we have found several cases where the automatic partitioning of jax 'breaks down' and fails to automatically partition some function that is vmapped, vjped and jvped (An example is `jnp.take_along_axis(a, indices, axis)`, where the first axis of `a` and `indices` are partitioned). `custom_partitioning` is not enough here, because we also must support vmap and vjp. We have investiated custom primitives and that would work, though it's complex to use the notsodocumented custom partitioning logic with it... .","> May I voice the need for exposing some way to define custom partitioning logic on the primitive as well? Good question! I don't think it would be feasible to incorporate partitioning logic into this implementation, at least not in the short term. I'd argue that partitioning is (currently) a somewhat orthogonal question since it is executed by XLA long after JAX is out of the loop. That's why `custom_partitioning` is such a complicated beast! That being said, it's a great point that we should make sure that `custom_primitive` can be composed properly with `custom_partitioning`  more on that below! > `custom_partitioning` is not enough here, because we also must support vmap and vjp. I think the usual approach here would be to compose `custom_parititioning` with `custom_vjp` and `custom_vmap` (which should work!):  Then, inside of your `vmap` rule, as long as you call `fun` again it should partition appropriately. This issue thread probably isn't the right place to go into too much detail about this, but feel free to open a new issue with more details about the problems you have encountered, because an approach like that should work fine!","> `lax.platform_dependent` covers all of the use cases for platformspecific lowering that I can come up with, and here that would just live within the primal function Thanks  that makes sense. Really excited for this change â€“ thanks for working on it!"
486,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Allow specifying `exec_time_optimization_effort` and `memory_fitting_effort` via CLI.)ï¼Œ å†…å®¹æ˜¯ (New compiler options (see title) have recently been exposed in JAX. To use them users have to change their Python code like this:   It would be very convenient to not modify the code but rely on CLI instead. For example: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Allow specifying `exec_time_optimization_effort` and `memory_fitting_effort` via CLI.,New compiler options (see title) have recently been exposed in JAX. To use them users have to change their Python code like this:   It would be very convenient to not modify the code but rely on CLI instead. For example: ,2024-11-05T11:01:05Z,enhancement,closed,0,1,https://github.com/jax-ml/jax/issues/24715,Fixed by CC(Add exec_time_optimization_effort and memory_fitting_effort flags  fixes 24715) !
313,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(RFC: specify jit static args via Static annotation)ï¼Œ å†…å®¹æ˜¯ (An idea inspired by chats with . Example:  The current way to define this would be )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",chat,RFC: specify jit static args via Static annotation,An idea inspired by chats with . Example:  The current way to define this would be ,2024-11-04T23:13:02Z,enhancement,open,3,3,https://github.com/jax-ml/jax/issues/24705,"How should this behave in the absence of an immediate `jit`? In particular, adapting the example, what do we expect this code to do? ","> How should this behave in the absence of an immediate `jit`? In particular, adapting the example, what do we expect this code to do? My vision would be for the `Static` annotation to be ignored unless used in the definition of a function that is wrapped by `jit`.",This looks great to me! It's definitely a stepup in readability from the way we currently annotate static arguments.
368,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([pallas:mosaic_gpu] `lax.fori_loop` lowering now promotes the carry to `mgpu.FragmentedArray`s)ï¼Œ å†…å®¹æ˜¯ ([pallas:mosaic_gpu] `lax.fori_loop` lowering now promotes the carry to `mgpu.FragmentedArray`s)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[pallas:mosaic_gpu] `lax.fori_loop` lowering now promotes the carry to `mgpu.FragmentedArray`s,[pallas:mosaic_gpu] `lax.fori_loop` lowering now promotes the carry to `mgpu.FragmentedArray`s,2024-11-04T12:06:07Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24686
727,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Pallas] Unable to Modify Input Ref in Pallas Kernel)ï¼Œ å†…å®¹æ˜¯ ( Description Hello, I'm wondering whether it is feasible for my pallas kernel to update on input refs? For example, if i want to both read and write to the same tensor via my pallas kernel, how can I achieve that? I'm putting down a minimal script and it seems that it doesn't work as I expect. Thanks!  System info (python version, jaxlib version, accelerator, etc.)  From this script I see  but I'm trying to get the `inplace_add` to return me `y  [ 8 10 12 14 16 18 20 22]`. Is that possible?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[Pallas] Unable to Modify Input Ref in Pallas Kernel," Description Hello, I'm wondering whether it is feasible for my pallas kernel to update on input refs? For example, if i want to both read and write to the same tensor via my pallas kernel, how can I achieve that? I'm putting down a minimal script and it seems that it doesn't work as I expect. Thanks!  System info (python version, jaxlib version, accelerator, etc.)  From this script I see  but I'm trying to get the `inplace_add` to return me `y  [ 8 10 12 14 16 18 20 22]`. Is that possible?",2024-11-01T05:18:26Z,bug pallas,closed,0,3,https://github.com/jax-ml/jax/issues/24656,Also I found this issue quite relevant https://github.com/jaxml/jax/discussions/22276,"This doesn't work as intended because x, y (the JAX tensors) live in HBM, and Pallas will copy them to the innermost memory hierarchy (e.g. VMEM on TPUs) before invoking the kernel. Therefore, when you modify `y_ref` you're only modifying the copy and it's not updating the actual `y` that's resident in HBM. Try aliasing the input and output to the same ref using the `input_output_aliases` argument to pallas call. In your case for the inplace add, you need to use:  Pallas will copy outputs back to HBM, so this will trigger Pallas to copy your updates to `y_ref` back to HBM.",Thanks a lot! I think it is exactly what I want. Let me try to work on my real kernel to see if I can get what I need.
452,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Pallas TPU] Add lowering for `lax.nextafter`)ï¼Œ å†…å®¹æ˜¯ ([Pallas TPU] Add lowering for `lax.nextafter` Also improved the corresponding test cases to ensure better coverage and accuracy. This PR is similar to https://github.com/jaxml/jax/pull/22283, which adds lowering for `lax.sign`.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[Pallas TPU] Add lowering for `lax.nextafter`,"[Pallas TPU] Add lowering for `lax.nextafter` Also improved the corresponding test cases to ensure better coverage and accuracy. This PR is similar to https://github.com/jaxml/jax/pull/22283, which adds lowering for `lax.sign`.",2024-10-31T23:47:28Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24653
751,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Problems with `experimental.pallas.ops.tpu.flash_attention`)ï¼Œ å†…å®¹æ˜¯ ( Description Hi, I am trying to use the experimental `flash_attention` forward pass on TPUs. I got a simple `v28` TPU instance from GCP and am running a test program to benchmark performance. I've included the test program below in case there are some issues in how I am calling `flash_attention` _(very possible)_ If I run with `jnp.float32`, I get pretty poor performance (_vastly_ worse than using the builtin `scaled_dot_product_attention`):   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Problems with `experimental.pallas.ops.tpu.flash_attention`," Description Hi, I am trying to use the experimental `flash_attention` forward pass on TPUs. I got a simple `v28` TPU instance from GCP and am running a test program to benchmark performance. I've included the test program below in case there are some issues in how I am calling `flash_attention` _(very possible)_ If I run with `jnp.float32`, I get pretty poor performance (_vastly_ worse than using the builtin `scaled_dot_product_attention`):   System info (python version, jaxlib version, accelerator, etc.) ",2024-10-30T19:49:26Z,bug,closed,7,6,https://github.com/jax-ml/jax/issues/24633,Do any other data types give you a similar error ğŸ¤” ,"Hi! Yes, this kernel is unfortunately written with newer TPUs in mind. However, I'm relatively sure that if only you make sure to apply `.astype(jnp.float32)` to all matmul operands, then the failure should go away","Thanks for the reply :) It's hard to get access to newer TPUs on GCP, but I'll keep trying. Just as a general rule of thumb, roughly what utilization (in terms of achieved FLOPs/peak FLOPs) should I be expecting on specific platforms with `bf16`? I'm just curious so that I can tell if I am using things correctly.",I just managed to get access to a `v5e` singlechip instance and my results are as follows (using `bf16`):  This seems pretty low compared to the stated peak throughput of 197 bfloat16 TFLOP/s? Is this expected or am I doing something incorrect here?,"I think by default the tile sizes are all set to 128, which is way too small for v5e. Try increasing them to different powers of 2, until they're so big the you get a VMEM OOM error. In general I would expect the utilization with well tuned tile sizes to be quite high (70%+)","Thanks! I get >75% util at `block_q = 512`, `block_k_major = 2048`, and `block_k = 1024` :D "
489,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Mosaic GPU] Fix the ordering of transforms in async_copy)ï¼Œ å†…å®¹æ˜¯ ([Mosaic GPU] Fix the ordering of transforms in async_copy Previously we didn't really fully discharge squeezing the indexed dims before applying other GMEM transforms, leading to potential failures because they were not anticipating the increased rank.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[Mosaic GPU] Fix the ordering of transforms in async_copy,"[Mosaic GPU] Fix the ordering of transforms in async_copy Previously we didn't really fully discharge squeezing the indexed dims before applying other GMEM transforms, leading to potential failures because they were not anticipating the increased rank.",2024-10-30T18:30:31Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24629
1300,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(JAX does not recognise my NVIDIA GPU when installed via conda)ï¼Œ å†…å®¹æ˜¯ ( Description I previously had a working installation of JAX (installed via conda) that recognised my NVIDIA GPU without issue. However, I recently migrated to a new machine and now I cannot get JAX to recognise my GPU when I install via conda. I'm using Miniforge to manage my conda environments, as I did on my old machine, and I installed JAX according to the docs: `conda install jaxlib=*=*cuda* jax cudanvcc c condaforge c nvidia` When I then try to import JAX and check my available devices using:  I get the following output:  tensorflow, however, does recognise my GPU, and so I tried the suggestion from CC(JAX doesn't work with cuda/gpu) to install using pip. I created a new environment and ran: `pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` When I then ran my JAX code above I got the output:  VoilÃ , my GPU has been found! It therefore appears that the conda section of the docs might need updating.  System info (python version, jaxlib version, accelerator, etc.) Conda installation: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,JAX does not recognise my NVIDIA GPU when installed via conda," Description I previously had a working installation of JAX (installed via conda) that recognised my NVIDIA GPU without issue. However, I recently migrated to a new machine and now I cannot get JAX to recognise my GPU when I install via conda. I'm using Miniforge to manage my conda environments, as I did on my old machine, and I installed JAX according to the docs: `conda install jaxlib=*=*cuda* jax cudanvcc c condaforge c nvidia` When I then try to import JAX and check my available devices using:  I get the following output:  tensorflow, however, does recognise my GPU, and so I tried the suggestion from CC(JAX doesn't work with cuda/gpu) to install using pip. I created a new environment and ran: `pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` When I then ran my JAX code above I got the output:  VoilÃ , my GPU has been found! It therefore appears that the conda section of the docs might need updating.  System info (python version, jaxlib version, accelerator, etc.) Conda installation: ",2024-10-29T22:56:20Z,bug,open,0,3,https://github.com/jax-ml/jax/issues/24604,"We (JAX) only support the `pip` packages, which should work under `conda` as well. For the native `condaforge` packages, you'd need to follow up on the condaforge feedstock project for `jax`.","I briefly looked into this a while ago as well when working on CC(Make `_cuda_path` more reliable) and CC(Fix `_cuda_path` for case when `cuda_nvcc` is a namespace package). I don't have time or interest to fully resolve the issue as I'm not a conda user myself, but here are a couple of pointers from my notes that might help you debug this. First, in `_src/xla_bridge.py`, the GPU backend doesn't get initialized (I believe) because of outdated `cu{BLAS,SPARSE}` modules as noted by the error messages which by default gets suppressed:  Even if you fix these, you _might_ have to explicitly pass in `CUDA_PATH` environment variable so that the `lib.cuda_path` comes out correctly. There are two branches in that function, one checking `CUDA_PATH` and another one checking `nvidia.cuda_nvcc` module path. Despite installing `cudanvcc` via `conda`, the `nvidia.cuda_nvcc` module is not available and thus the second branch doesn't work.","This should have been fixed by https://github.com/condaforge/jaxlibfeedstock/pull/288 (jaxlib condaforge version `0.4.34=*_201`), so I guess the issue can be closed (eventual followup can be tracked anyhow in https://github.com/condaforge/jaxfeedstock/issues/162 )."
494,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix a reference cycle bug.)ï¼Œ å†…å®¹æ˜¯ (When we use a context manager within a linear_util.transformation we should leave the scope of the context manager before the final yield. Otherwise we create spurious reference cycles. This was causing CoreTest.test_reference_cycles to fail on Python 3.10 (but not 3.13 for some reason).)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Fix a reference cycle bug.,When we use a context manager within a linear_util.transformation we should leave the scope of the context manager before the final yield. Otherwise we create spurious reference cycles. This was causing CoreTest.test_reference_cycles to fail on Python 3.10 (but not 3.13 for some reason).,2024-10-29T20:46:32Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/24600
353,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(represent `random.key_impl` of builtin RNGs by canonical string name)ï¼Œ å†…å®¹æ˜¯ (No great reason to return specs here, and sticking to strings instead can help with simple serialization.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,represent `random.key_impl` of builtin RNGs by canonical string name,"No great reason to return specs here, and sticking to strings instead can help with simple serialization.",2024-10-29T20:08:06Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/24593
330,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([mosaic_gpu] Added support for bitwise and, or and xor to `FragmentedArray`)ï¼Œ å†…å®¹æ˜¯ ([mosaic_gpu] Added support for bitwise and, or and xor to `FragmentedArray`)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,"[mosaic_gpu] Added support for bitwise and, or and xor to `FragmentedArray`","[mosaic_gpu] Added support for bitwise and, or and xor to `FragmentedArray`",2024-10-29T19:53:22Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24591
395,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix missing f-string format in slogdet error message)ï¼Œ å†…å®¹æ˜¯ (Adds the missing f of fstring format in the ValueError message of the slogdet function, now displaying the actual misspecified array shape instead of ""{a_shape}"". )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Fix missing f-string format in slogdet error message,"Adds the missing f of fstring format in the ValueError message of the slogdet function, now displaying the actual misspecified array shape instead of ""{a_shape}"". ",2024-10-29T15:27:14Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/24581,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
316,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([MOSAIC:GPU] Extend the mosaic mlir dialect with fragmented layouts.)ï¼Œ å†…å®¹æ˜¯ ([MOSAIC:GPU] Extend the mosaic mlir dialect with fragmented layouts.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[MOSAIC:GPU] Extend the mosaic mlir dialect with fragmented layouts.,[MOSAIC:GPU] Extend the mosaic mlir dialect with fragmented layouts.,2024-10-29T08:43:09Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24574
1452,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump actions/checkout from 4.2.1 to 4.2.2)ï¼Œ å†…å®¹æ˜¯ (Bumps actions/checkout from 4.2.1 to 4.2.2.  Release notes Sourced from actions/checkout's releases.  v4.2.2 What's Changed  urlhelper.ts now leverages wellknown environment variables by @â€‹jww3 in actions/checkout CC(lazy sublanguage part 1) Expand unit test coverage for isGhes by @â€‹jww3 in actions/checkout CC(Fix bug with caching in presence of JVP and JIT)  Full Changelog: https://github.com/actions/checkout/compare/v4.2.1...v4.2.2    Changelog Sourced from actions/checkout's changelog.  Changelog v4.2.2  urlhelper.ts now leverages wellknown environment variables by @â€‹jww3 in actions/checkout CC(lazy sublanguage part 1) Expand unit test coverage for isGhes by @â€‹jww3 in actions/checkout CC(Fix bug with caching in presence of JVP and JIT)  v4.2.1  Check out other refs/* by commit if provided, fall back to ref by @â€‹orhantoy in actions/checkout CC(Adding implementations for numpy alen, isscalar)  v4.2.0  Add Ref and Commit outputs by @â€‹lucacome in actions/checkout CC(Make DeviceValue and subclasses weakref friendly) Dependency updates by @â€‹dependabot actions/checkout CC(Add error checking that arguments of jvp are tuples), actions/checkout CC(Implement bool_ support for jnp.add, jnp.multiply, jnp.einsum, lax.doâ€¦) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Bump actions/checkout from 4.2.1 to 4.2.2,"Bumps actions/checkout from 4.2.1 to 4.2.2.  Release notes Sourced from actions/checkout's releases.  v4.2.2 What's Changed  urlhelper.ts now leverages wellknown environment variables by @â€‹jww3 in actions/checkout CC(lazy sublanguage part 1) Expand unit test coverage for isGhes by @â€‹jww3 in actions/checkout CC(Fix bug with caching in presence of JVP and JIT)  Full Changelog: https://github.com/actions/checkout/compare/v4.2.1...v4.2.2    Changelog Sourced from actions/checkout's changelog.  Changelog v4.2.2  urlhelper.ts now leverages wellknown environment variables by @â€‹jww3 in actions/checkout CC(lazy sublanguage part 1) Expand unit test coverage for isGhes by @â€‹jww3 in actions/checkout CC(Fix bug with caching in presence of JVP and JIT)  v4.2.1  Check out other refs/* by commit if provided, fall back to ref by @â€‹orhantoy in actions/checkout CC(Adding implementations for numpy alen, isscalar)  v4.2.0  Add Ref and Commit outputs by @â€‹lucacome in actions/checkout CC(Make DeviceValue and subclasses weakref friendly) Dependency updates by @â€‹dependabot actions/checkout CC(Add error checking that arguments of jvp are tuples), actions/checkout CC(Implement bool_ support for jnp.add, jnp.multiply, jnp.einsum, lax.doâ€¦) ",2024-10-28T17:39:06Z,pull ready dependencies github_actions,closed,0,1,https://github.com/jax-ml/jax/issues/24563, rebase
1456,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump actions/setup-python from 5.2.0 to 5.3.0)ï¼Œ å†…å®¹æ˜¯ (Bumps actions/setuppython from 5.2.0 to 5.3.0.  Release notes Sourced from actions/setuppython's releases.  v5.3.0 What's Changed  Add workflow file for publishing releases to immutable action package by @â€‹Jcambass in actions/setuppython CC(Enable LU decomposition tests on GPU.) Upgrade IA publish by @â€‹Jcambass in actions/setuppython CC(One array type to rule them all!)  Bug Fixes:  Normalise Line Endings to Ensure CrossPlatform Consistency by @â€‹priyakinthali in actions/setuppython CC(Add a pure Python LU decomposition.) Revise isGhes logic by @â€‹jww3 in actions/setuppython CC(Implement device_put as a primitive.) Bump pillow from 7.2 to 10.2.0 by @â€‹aparnajyothiy in actions/setuppython CC(TypeError when taking inverse)  Enhancements:  Enhance workflows and documentation updates by @â€‹priyakinthali in actions/setuppython CC(Make DeviceArray.__iter__ and __reversed__ forward to _value.) Bump default versions to latest by @â€‹jeffwidman in actions/setuppython CC(provide better hints in error message ""abstract value passed to `bool`"")  New Contributors  @â€‹Jcambass made their first contribution in actions/setuppython CC(Enable LU decomposition tests on GPU.) @â€‹jww3 made their first contribution in actions/setuppython CC()è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bump actions/setup-python from 5.2.0 to 5.3.0,"Bumps actions/setuppython from 5.2.0 to 5.3.0.  Release notes Sourced from actions/setuppython's releases.  v5.3.0 What's Changed  Add workflow file for publishing releases to immutable action package by @â€‹Jcambass in actions/setuppython CC(Enable LU decomposition tests on GPU.) Upgrade IA publish by @â€‹Jcambass in actions/setuppython CC(One array type to rule them all!)  Bug Fixes:  Normalise Line Endings to Ensure CrossPlatform Consistency by @â€‹priyakinthali in actions/setuppython CC(Add a pure Python LU decomposition.) Revise isGhes logic by @â€‹jww3 in actions/setuppython CC(Implement device_put as a primitive.) Bump pillow from 7.2 to 10.2.0 by @â€‹aparnajyothiy in actions/setuppython CC(TypeError when taking inverse)  Enhancements:  Enhance workflows and documentation updates by @â€‹priyakinthali in actions/setuppython CC(Make DeviceArray.__iter__ and __reversed__ forward to _value.) Bump default versions to latest by @â€‹jeffwidman in actions/setuppython CC(provide better hints in error message ""abstract value passed to `bool`"")  New Contributors  @â€‹Jcambass made their first contribution in actions/setuppython CC(Enable LU decomposition tests on GPU.) @â€‹jww3 made their first contribution in actions/setuppython CC(",2024-10-28T17:38:59Z,pull ready dependencies github_actions,closed,0,1,https://github.com/jax-ml/jax/issues/24562,  rebase
423,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(MaxText Getting Started fails)ï¼Œ å†…å®¹æ˜¯ ( Description Followed the MaxText Getting Started instructions at https://github.com/AIHypercomputer/maxtext/blob/main/getting_started/First_run.md   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,MaxText Getting Started fails," Description Followed the MaxText Getting Started instructions at https://github.com/AIHypercomputer/maxtext/blob/main/getting_started/First_run.md   System info (python version, jaxlib version, accelerator, etc.) ",2024-10-28T05:03:32Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/24549,"I would make this issue on the MaxText repo, since its unclear if this is a fault of jax or of the other repo"
1009,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Pallas][Flash Attention] Cost analysis in Flash Attention kernel causing compilation OOM)ï¼Œ å†…å®¹æ˜¯ ( Description With `0.4.35` release, the flash attention kernel will hit compilation OOM for long sequence length inputs. It failed during compiling the reference attention implementation for cost analysis purpose, added in this commit: https://github.com/jaxml/jax/commit/4c218fbf3b8431a5f75cdf20942d5d62433a8657. This logic would cause OOM when the device HBM is not enough for the naive attention but fits for flash attention kernels. Here is a small repro script for this issue (on v5e TPU with 16GB HBM)  The error message is:  It failed with `0.4.35` but passed with `20240829` nightly .  System info (python version, jaxlib version, accelerator, etc.) Version with the compilation OOM issue  Version without the compilation OOM issue )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[Pallas][Flash Attention] Cost analysis in Flash Attention kernel causing compilation OOM," Description With `0.4.35` release, the flash attention kernel will hit compilation OOM for long sequence length inputs. It failed during compiling the reference attention implementation for cost analysis purpose, added in this commit: https://github.com/jaxml/jax/commit/4c218fbf3b8431a5f75cdf20942d5d62433a8657. This logic would cause OOM when the device HBM is not enough for the naive attention but fits for flash attention kernels. Here is a small repro script for this issue (on v5e TPU with 16GB HBM)  The error message is:  It failed with `0.4.35` but passed with `20240829` nightly .  System info (python version, jaxlib version, accelerator, etc.) Version with the compilation OOM issue  Version without the compilation OOM issue ",2024-10-25T21:19:22Z,bug pallas,closed,0,2,https://github.com/jax-ml/jax/issues/24539,https://github.com/jaxml/jax/pull/24608 should fix this by calculating the cost estimate directly instead of via compiling a reference function. . In the meantime you could also just remove the costestimate temporarily as it won't affect correctness of the code.,"Hi , thank you for the fix, I verified the repro passed with Dec 05 nightly."
375,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([mosaic_gpu] `mgpu.FragmentedArray` now supports `//`)ï¼Œ å†…å®¹æ˜¯ ([mosaic_gpu] `mgpu.FragmentedArray` now supports `//` This is needed to compute grid index from the iteration step counter in `emit_pipeline`.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[mosaic_gpu] `mgpu.FragmentedArray` now supports `//`,[mosaic_gpu] `mgpu.FragmentedArray` now supports `//` This is needed to compute grid index from the iteration step counter in `emit_pipeline`.,2024-10-25T14:42:30Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24530
704,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(INVALID_ARGUMENT when copying array in pinned_host memory with device_put(..., may_alias=False))ï¼Œ å†…å®¹æ˜¯ ( Description The minimal reproducer below can be added to `memories_test.py` to reproduce this issue. The `block_until_ready` at the bottom of this example is expected to succeed, but fails with `INVALID_ARGUMENT: BlockHostUntilReady() called on deleted or donated buffer`. The donation of `inp_host` into `inp_host_donate` is necessary for the failure to occur.   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"INVALID_ARGUMENT when copying array in pinned_host memory with device_put(..., may_alias=False)"," Description The minimal reproducer below can be added to `memories_test.py` to reproduce this issue. The `block_until_ready` at the bottom of this example is expected to succeed, but fails with `INVALID_ARGUMENT: BlockHostUntilReady() called on deleted or donated buffer`. The donation of `inp_host` into `inp_host_donate` is necessary for the failure to occur.   System info (python version, jaxlib version, accelerator, etc.) ",2024-10-25T08:05:34Z,bug,closed,0,0,https://github.com/jax-ml/jax/issues/24521
256,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(use dtypes.iinfo instead of numpy.iinfo in sum and add corresponding â€¦)ï¼Œ å†…å®¹æ˜¯ (â€¦tests)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,use dtypes.iinfo instead of numpy.iinfo in sum and add corresponding â€¦,â€¦tests,2024-10-24T15:36:23Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24508
745,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([sharding_in_types][Take 2] Add `out_type` argument to `einsum` and `dot_general` to allow specifying for the output type. Right now, it only accept a `NamedSharding` but in the future we can allow a polymorphic type of: `jax.ShapeDtypeStruct | Sharding | Layout`.)ï¼Œ å†…å®¹æ˜¯ ([sharding_in_types][Take 2] Add `out_type` argument to `einsum` and `dot_general` to allow specifying for the output type. Right now, it only accept a `NamedSharding` but in the future we can allow a polymorphic type of: `jax.ShapeDtypeStruct  Layout`. Reverts 0b3f0e11fb0c37342b3c05ad5d53f3435b6ca44c)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"[sharding_in_types][Take 2] Add `out_type` argument to `einsum` and `dot_general` to allow specifying for the output type. Right now, it only accept a `NamedSharding` but in the future we can allow a polymorphic type of: `jax.ShapeDtypeStruct | Sharding | Layout`.","[sharding_in_types][Take 2] Add `out_type` argument to `einsum` and `dot_general` to allow specifying for the output type. Right now, it only accept a `NamedSharding` but in the future we can allow a polymorphic type of: `jax.ShapeDtypeStruct  Layout`. Reverts 0b3f0e11fb0c37342b3c05ad5d53f3435b6ca44c",2024-10-22T15:59:44Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24462
730,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Full coverage for `__jax_array__` protocol)ï¼Œ å†…å®¹æ˜¯ ( Description I am using Flax with the NNX API that requires arrays to be encapsulated within `nnx.Param`. The issue is that NNX is using the `__jax_array__` protocol to avoid users having to type `.value` but JAX doesn't have full coverage for `__jax_array__`. For example, this code from an NNX model raises the following error:  Error:  Maybe  can provide more insights on this if necessary?  System info (python version, jaxlib version, accelerator, etc.) python version: 3.12.6 jax: 0.4.34 jaxlib: 0.4.34)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Full coverage for `__jax_array__` protocol," Description I am using Flax with the NNX API that requires arrays to be encapsulated within `nnx.Param`. The issue is that NNX is using the `__jax_array__` protocol to avoid users having to type `.value` but JAX doesn't have full coverage for `__jax_array__`. For example, this code from an NNX model raises the following error:  Error:  Maybe  can provide more insights on this if necessary?  System info (python version, jaxlib version, accelerator, etc.) python version: 3.12.6 jax: 0.4.34 jaxlib: 0.4.34",2024-10-22T14:59:28Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/24460,"Hi  thanks for the request. This issue has been discussed previously in CC(`jnp.linalg.svd` etc. do not respect `__jax_array__`). But the short version is that we never meant `__jax_array__` to be a fullysupported public API, and it's not clear that it's the best mechanism for downstream libraries to rely on. cc/  who has more context."
1094,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([ROCM] Sharding leads to HIP errors)ï¼Œ å†…å®¹æ˜¯ ( Description Hello, I'm trying to run a script that makes use of sharding.  I'm using a container built on top of  `rocm/jaxcommunity:rocm6.2.3jax0.4.33py3.12.6` , where I've installed some extra packages. I'm running on adastra's MI250X nodes https://dci.dcigitlab.cines.fr/webextranet/architecture/index.html through a singularity container (which is an unprivileged container system, as onHPC we cannot use docker). The error I'm encoutering is the following  The error only appears if I make use of multigpu through sharding.  Running the same script without sharding, but using a single GPU, leads to no errors. I'm running on the container `rocm/jaxcommunity:rocm6.2.3jax0.4.33py3.12.6` where I also install the package `https://github.com/netket/netket` . I'm trying to run the following script as follows   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[ROCM] Sharding leads to HIP errors," Description Hello, I'm trying to run a script that makes use of sharding.  I'm using a container built on top of  `rocm/jaxcommunity:rocm6.2.3jax0.4.33py3.12.6` , where I've installed some extra packages. I'm running on adastra's MI250X nodes https://dci.dcigitlab.cines.fr/webextranet/architecture/index.html through a singularity container (which is an unprivileged container system, as onHPC we cannot use docker). The error I'm encoutering is the following  The error only appears if I make use of multigpu through sharding.  Running the same script without sharding, but using a single GPU, leads to no errors. I'm running on the container `rocm/jaxcommunity:rocm6.2.3jax0.4.33py3.12.6` where I also install the package `https://github.com/netket/netket` . I'm trying to run the following script as follows   System info (python version, jaxlib version, accelerator, etc.) ",2024-10-22T14:01:35Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/24458
486,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Pallas:MGPU] Treat each warpgroup as a single logical thread.)ï¼Œ å†…å®¹æ˜¯ ([Pallas:MGPU] Treat each warpgroup as a single logical thread. As an extra minor change, we now disallow specifying the predicate when uniform is unset, as that implies that we're going to use two different mechanisms to select a single thread.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[Pallas:MGPU] Treat each warpgroup as a single logical thread.,"[Pallas:MGPU] Treat each warpgroup as a single logical thread. As an extra minor change, we now disallow specifying the predicate when uniform is unset, as that implies that we're going to use two different mechanisms to select a single thread.",2024-10-22T13:50:52Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24457
680,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([sharding_in_types] Add `out_type` argument to `einsum` and `dot_general` to allow specifying for the output type. Right now, it only accept a `NamedSharding` but in the future we can allow a polymorphic type of: `jax.ShapeDtypeStruct | Sharding | Layout`.)ï¼Œ å†…å®¹æ˜¯ ([sharding_in_types] Add `out_type` argument to `einsum` and `dot_general` to allow specifying for the output type. Right now, it only accept a `NamedSharding` but in the future we can allow a polymorphic type of: `jax.ShapeDtypeStruct  Layout`.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"[sharding_in_types] Add `out_type` argument to `einsum` and `dot_general` to allow specifying for the output type. Right now, it only accept a `NamedSharding` but in the future we can allow a polymorphic type of: `jax.ShapeDtypeStruct | Sharding | Layout`.","[sharding_in_types] Add `out_type` argument to `einsum` and `dot_general` to allow specifying for the output type. Right now, it only accept a `NamedSharding` but in the future we can allow a polymorphic type of: `jax.ShapeDtypeStruct  Layout`.",2024-10-22T02:29:07Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24443
693,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(#sdy add shardy CPU config for all JAX tests, disabling any known failing test cases.)ï¼Œ å†…å®¹æ˜¯ (sdy add shardy CPU config for all JAX tests, disabling any known failing test cases. Only test cases breaking on CPU are related to:  pure callbacks  export  shard alike Note that `layout_test` is broken on TPU, leaving a comment saying to enable it. Also fixed `shard_map_test` test that was broken when running Shardy on one TPU, and `aot_test` which was breaking due to calling a different C++ StableHLO compilation function.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"#sdy add shardy CPU config for all JAX tests, disabling any known failing test cases.","sdy add shardy CPU config for all JAX tests, disabling any known failing test cases. Only test cases breaking on CPU are related to:  pure callbacks  export  shard alike Note that `layout_test` is broken on TPU, leaving a comment saying to enable it. Also fixed `shard_map_test` test that was broken when running Shardy on one TPU, and `aot_test` which was breaking due to calling a different C++ StableHLO compilation function.",2024-10-21T22:27:01Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24440
1474,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add support for layouts and other advanced features in ffi_call)ï¼Œ å†…å®¹æ˜¯ (This PR adds the remaining features needed to migrate users of `mlir.custom_call` to `ffi_call`. The most important additions are parameters for specifying layouts, and input/output aliasing. Alongside these changes I have added the `custom_call_api_version` and `legacy_backend_config` arguments so that `ffi_call` can also be used to execute legacy custom calls. I also cleared a longstanding todo item to update the `ffi_call` tests to use a custom call that can run on multiple platforms (including CPU). These new tests also exercise the new parameters. Most of these changes should be reasonably uncontroversial, but I thought it might be worth digging into the layouts question a little bit. **Layouts** The `input_layouts` and `output_layouts` arguments allow users of `ffi_call` to request custom memory layouts for the input and output buffers. These arguments can be used to specify layouts using (a) `None` for the default rowmajor/Cordered inputs, (b) a `DeviceLocalLayout` where the `major_to_minor` property defines the layout, or (c) an majortominor tuple of integers. I think that these options present a reasonable starting point for discussion, but it's worth thinking about this a bit. Some alternative options:)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add support for layouts and other advanced features in ffi_call,"This PR adds the remaining features needed to migrate users of `mlir.custom_call` to `ffi_call`. The most important additions are parameters for specifying layouts, and input/output aliasing. Alongside these changes I have added the `custom_call_api_version` and `legacy_backend_config` arguments so that `ffi_call` can also be used to execute legacy custom calls. I also cleared a longstanding todo item to update the `ffi_call` tests to use a custom call that can run on multiple platforms (including CPU). These new tests also exercise the new parameters. Most of these changes should be reasonably uncontroversial, but I thought it might be worth digging into the layouts question a little bit. **Layouts** The `input_layouts` and `output_layouts` arguments allow users of `ffi_call` to request custom memory layouts for the input and output buffers. These arguments can be used to specify layouts using (a) `None` for the default rowmajor/Cordered inputs, (b) a `DeviceLocalLayout` where the `major_to_minor` property defines the layout, or (c) an majortominor tuple of integers. I think that these options present a reasonable starting point for discussion, but it's worth thinking about this a bit. Some alternative options:",2024-10-21T18:55:23Z,pull ready,closed,0,3,https://github.com/jax-ml/jax/issues/24433,"> We could require DeviceLocalLayout objects, rather than also accepting tuples Being polymorphic on tuples | DeviceLocalLayout is better IMO. DeviceLocalLayout doesn't only allow `major_to_minor`. It also accepts tiling which on GPU and TPU is extremely useful.","Thanks ! Just so that I understand: the way I have implemented this is that layouts can be specified by a `tuple` or a `DeviceLocalLayout`. Were you agreeing that this is a good approach, or did you have something else in mind?",I was agreeing :) But also pointing out the other stuff DeviceLocalLayout supports :)
1360,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(JAX code is extremely slow on GPUs)ï¼Œ å†…å®¹æ˜¯ ( Description Last week a discussion took place on Twitter where many people noticed that the performance of JAX on GPUs is still subpar compared to PyTorch code.  and I had a discussion afterwards and we agreed that a repo with minimal codebase that showcase the differences in performance between the two would be great.  GPT2 is a good model to start with, and here is a repo that contains code both for JAX, and PyTorch for the same model. The instructions provided on the repo are enough to download and run code locally (on a GPU machine). On my side, these are the results I got on an A100 40G machine:  JAX    PyTorch   Compared to Torch, JAX is extremely slow here, and I have no idea why. There is a chance of a silent bug somewhere in the JAX code, and I may have overlooked it. Given the number of times I have been through this, I think a fresh set of eyes would do better justice. Please let me know if you need any other information on this from my side.  System info (python version, jaxlib version, accelerator, etc.)  Optax is installed from git because there was a fix for `adamw` but that was not the part of the last release. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,JAX code is extremely slow on GPUs," Description Last week a discussion took place on Twitter where many people noticed that the performance of JAX on GPUs is still subpar compared to PyTorch code.  and I had a discussion afterwards and we agreed that a repo with minimal codebase that showcase the differences in performance between the two would be great.  GPT2 is a good model to start with, and here is a repo that contains code both for JAX, and PyTorch for the same model. The instructions provided on the repo are enough to download and run code locally (on a GPU machine). On my side, these are the results I got on an A100 40G machine:  JAX    PyTorch   Compared to Torch, JAX is extremely slow here, and I have no idea why. There is a chance of a silent bug somewhere in the JAX code, and I may have overlooked it. Given the number of times I have been through this, I think a fresh set of eyes would do better justice. Please let me know if you need any other information on this from my side.  System info (python version, jaxlib version, accelerator, etc.)  Optax is installed from git because there was a fix for `adamw` but that was not the part of the last release. ",2024-10-20T16:00:25Z,bug performance,open,0,28,https://github.com/jax-ml/jax/issues/24411,"Thanks for filing! I've looked into this a bit, and I just wanted to check with you : are the set ups in both files expected to be the same? Looking at the very first lines in `torch_single_gpu.py` and `jax_single_gpu.py`, I see  in `jax_single_gpu.py` and  in `torch_single_gpu.py`. I believe this means that the JAX model will be running twice as many `grad_accum_steps` (given that `B` is in the denominator there). In your screenshots, those seem to be the same, however. I ran the JAX model on a single A10040 (SXM) node, and here are the numbers I got:  which are different from the initial reported numbers. Is the configuration expected to be the same in the repo as in your screenshots? (I wasn't yet able to run the PyTorch code for comparison due to dependency issues, but hope to get to that soon.)","One suggested improvement! By fully unrolling the `scan` here, the model runs significantly faster:  (Unrolling `scan`/loops is a good way to allow the compiler to make better optimizations in general, at the expense of compiletime.)","Thanks for looking into it  The benchmark is run for both the setup with a batch size of 8 as I couldn't fit a batch size of 16 with JAX (most probably due to the extra pytree that act as a mask for adamw). I might have forgotten to update the batch info in git.  If you look at the number of batches and the number of grad accumulated steps printed on the screen, both are same meaning the micro batch size is same. Though for the sake of more transparency, I will update the repo, and post the numbers here once again Re unrolling: I am not worried about the compile time for now. Let me check that. Thanks for the pointers","Updated the repo. Re unroll: I did it, and even though the I got a nice bump in the number of tokens processed per second, the time taken to process those tokens got extremely worse. I would have rather paid one time compilation penalty. Also, the performance is still nowhere close to torch neither in terms of tokens (163K token processed in torch vs 22K tokens processed in JAX) nor in terms of time taken ","Thanks for trying and for the feedback! > the time taken to process those tokens got extremely worse Can you explain what that means? Presumably, if we're processing more tokens per second, then the time has to be betteror am I missing something? If you're talking about the first step, then that is because the first step includes compilation overhead. (The first call to the function triggers compilation.) > Also, the performance is still nowhere close to torch neither in terms of tokens (163K token processed in torch vs 22K tokens processed in JAX) nor in terms of time taken Can you give some details about the hardware you are running on? You said you are running on A10040, but it seems that as posted above I am getting ~80K tokens processed in JAX on my side, running on an `NVIDIA A100SXM440GB` (output of `nvidiasmi`). I'm confused about why the numbers that we see are so different, and I'd like to reproduce your numbers. Could you perhaps share the output of `nvidiasmi` for you? I am facing the following error when running `python3 torch_single_gpu.py input.txt`:  Do you have any idea where that might be coming from? Is there perhaps a requirement missing in `requirements.txt`I see that it doesn't contain any information about the required Triton version?",">Can you explain what that means? Please take a look at the numbers below:   > Can you give some details about the hardware you are running on? You said you are running on A10040, but it seems that https://github.com/jaxml/jax/issues/24411issuecomment2437717660, running on an NVIDIA A100SXM440GB (output of nvidiasmi). I'm confused about why the numbers that we see are so different, and I'd like to reproduce your numbers. Could you perhaps share the output of nvidiasmi for you? Sure thing. Here is the output:   > Is there perhaps a requirement missing in requirements.txtI see that it doesn't contain any information about the required Triton version? No nothing missing from the `requirements.txt` file. To double check, I just created a new env with that and ran the benchmarks again, and it worked fine. Can you create a new empty env and install everything using the requirements file? The only guess I have in this regard is that you have a `triton` package installed in the current env which is incompatible with the torch package One question for you: How are you unrolling the scan? Maybe we are getting different numbers because of that?","> Sure thing. Here is the output: Thank you for sharing! The chip is the same, so that's not the difference. One difference worth noting (though not sure if it matters) is that I'm using an older CUDA version than you are:  > Can you create a new empty env and install everything using the requirements file? The only guess I have in this regard is that you have a `triton` package installed in the current env which is incompatible with the torch package Thanks, seems like retrying with a new env worked after a couple of tries! (There were some issues due to incomplete installs of `torch`, that may also be what had happened before.) I managed to reproduce your numbers for `torch`, making it currently ~2x faster than the JAX implementation on my system (80k vs 160k tokens per second). This difference is still very much unexpected of course, so I'll keep digging :) > One question for you: How are you unrolling the scan? Maybe we are getting different numbers because of that? I just added the `unroll` kwarg to the call to `jax.lax.scan`:  One thing I also noticed is that you're not actually calling into a `FlashAttention` kernel on the JAX side. The call to  could be rewritten into  for that. There is a `TODO` left by  to select the best implementation automatically in the code, which currently doesn't happen automatically. As a result, the default dispatches to a soup of primitive ops that XLA can't optimize into a single `FlashAttention` kernel.",">Thanks, seems like retrying with a new env worked after a couple of tries! Good to know. Yeah, sometimes it happens if there is a silent package conflict or a corrupted install from the cache >I managed to reproduce your numbers for torch, making it currently ~2x faster than the JAX implementation on my system (80k vs 160k tokens per second). This difference is still very much unexpected of course, so I'll keep digging :) Yes, I was shocked to see the performance difference between the two.  > I just added the unroll kwarg to the call to jax.lax.scan: I did the same but with a much lower number, and you are right 12 per iteration should be enough. Will check that >There is a TODO left by  to select the best implementation automatically in the code, which currently doesn't happen automatically. As a result, the default dispatches to a soup of primitive ops that XLA can't optimize into a single FlashAttention kernel. I read it when I opened a discussion on the attention functionality last to last month. Still missed it. Sigh!","So, after updating my system to (roughly) match your driver version, I was able to run with both `FlashAttention` enabled and loop unrolling. This gets me to  ",We can glean another few tokens/s by rewriting the code to use a different path for gradient accumulation vs for actual model update: ,"> Using FlashAttention correctly unsurprisingly turns out to be very important here :) Now, we're closer to a reasonable time, with 130k tokens per second on JAX, and 150k on Torch. Still, we should be able to do better, I'll keep looking. Thanks. I am able to reproduce this number on my side. One question though: Right now, we are using `unroll=1000`, but finding the ideal number for unrolling seems a tedious task. Is there any better way to do it? May be through HLO graphs? > This avoids making noop updates to the model & rewriting all the weights to HBM at each accumulation step. This is is nitty detail. We should add documentation for this in optax","> Thanks. I am able to reproduce this number on my side. One question though: Right now, we are using unroll=1000, but finding the ideal number for unrolling seems a tedious task. Is there any better way to do it? May be through HLO graphs? Awesome that you could reproduce it! I hacked around, but the goal is to unroll all the layers heresetting `unroll=True` should work. (Side note: there seems to be an offbyone in the implementation of the `scan` lowering such that setting `unroll=True` lowers slightly less efficiently. I'll make a fix, but a temporary workaround is to set `unroll=self.n_layer + 1`). > This is is nitty detail. We should add documentation for this in optax Agreed! Where do you think that would belong exactly? (This is my first time using `optax` :)) Another (marginal) improvement is to preprocess the data in the data loader to avoid doing a copy at each loop step (call to `jnp.reshape` in `train_loader.next_batch()` produce two copies). This is not strictly a win in terms of time because some of that compute happens before the training loop. Nevertheless, it gives a slightly more faithful comparison with Torch, since in the Torch version we get a `view` of the data (as opposed to a copy). ","> but the goal is to unroll all the layers heresetting unroll=True should work. (Side note: there seems to be an offbyone in the implementation of the scan lowering such that setting unroll=True lowers slightly less efficiently. I'll make a fix, but a temporary workaround is to set unroll=self.n_layer + 1). Yeah, when I used `unroll=True`, I noticed that difference. Thanks for the explanation. > Agreed! Where do you think that would belong exactly? (This is my first time using optax :)) This should go right here. Let me know if you need more info or any help on this. > Another (marginal) improvement is to preprocess the data in the data loader to avoid doing a copy at each loop step (call to jnp.reshape in train_loader.next_batch() produce two copies). This is not strictly a win in terms of time because some of that compute happens before the training loop. Nevertheless, it gives a slightly more faithful comparison with Torch, since in the Torch version we get a view of the data (as opposed to a copy). Yeah we can do that. I just left it raw for now","> Yeah, when I used `unroll=True`, I noticed that difference. Thanks for the explanation. This commit should fix the issue at HEAD! > This should go right here. Let me know if you need more info or any help on this. Actually, if you have a good idea of what useful documentation for you would have looked like, I'd love to see it! (Maybe you even want to send a PR?! :))",">Actually, if you have a good idea of what useful documentation for you would have looked like, I'd love to see it! (Maybe you even want to send a PR?! :)) Cool. Leave that to me. I will make that PR both in optax and Equinox","I didn't have a lot of bandwidth to go further this week, but I recalled that there are a few improvements in XLA that are not yet in the last releaseso wanted to share at least that. Here are some numbers with the latest nightlies (`jax0.4.36.dev20241031`, `jaxcuda12pjrt0.4.36.dev20241101`, `jaxcuda12plugin0.4.36.dev20241101`, `jaxlib0.4.36.dev20241101`).  So we're at roughly ~150k tok/sec for now. ",> I didn't have a lot of bandwidth to go further this week No worries  We are already close to the optimal number. I am hoping this exercise would turn out extremely critical for both of us in term of performance benchmarks. , were you able to find any more bottlenecks?,"Hi ; I have a few guesses of where to look at, but haven't looked into the profiles deeply just yet. At this point, it seems like we need to fix XLA performance. With that said, we can still improve the model at the JAX level directly. We know that `LayerNorm` is currently pretty slow in XLA (though there is ongoing work to fix this). One easy way to check whether speeding that up makes a difference is to plug in the Pallas `layer_norm` kernel:  (Note, the plugging in is pretty hacky, the Pallas op may deserve some love :)) The hypothesis is validated, since plugging that in gets us to roughly 156k tokens/second:  There were previous reports of Adam slowness; I'd like to poke at this and see whether there are any low hanging fruits there, but I have yet to find cycles to do this.","> We know that LayerNorm is currently pretty slow in XLA Can yo point out to me the issue/discussion/documentation where it was found and discussed? I would like to know the reason behind it because this may affect other ops as well in that case > but I have yet to find cycles to do this. No worries. Thanks for all the help and the support you have provided. This is already looking good. In an ideal world, JAX should be extremely faster than torch. And in case of TPUs it is true, but not for GPUs. The problem is that GPUs are more widespread compared to TPUs, and if it is slow on GPU, no matter how hard people (like you and I) try to convince that JAX is better, it will not be enough","> Can yo point out to me the issue/discussion/documentation where it was found and discussed? I would like to know the reason behind it because this may affect other ops as well in that case Unfortunately, I'm not aware of any publicfacing bugonly internal ones. What it boils down to, however, is that normalizations (any of them, including e.g. Softmax) end up lowering to several kernels by default, as opposed to a single one. We (and I personally) have done significant work towards fixing this, but it's not yet on by default, and doesn't actually match everything that it should. > No worries. Thanks for all the help and the support you have provided. This is already looking good. In an ideal world, JAX should be extremely faster than torch. And in case of TPUs it is true, but not for GPUs. The problem is that GPUs are more widespread compared to TPUs, and if it is slow on GPU, no matter how hard people (like you and I) try to convince that JAX is better, it will not be enough Happy to help! We're always working on making our stack better, and reports such as yours are very helpful to figure out what the sharp bits are for our external users, and to find out things we can do better. So thank you for filing this!",">Unfortunately, I'm not aware of any publicfacing bugonly internal ones. What it boils down to, however, is that normalizations (any of them, including e.g. Softmax) end up lowering to several kernels by default, as opposed to a single one. We (and I personally) have done significant work towards fixing this, but it's not yet on by default, and doesn't actually match everything that it should. Oh okay. Any approximate timeline for the fixes that are already done but not in the dev branch? >Happy to help! We're always working on making our stack better, and reports such as yours are very helpful to figure out what the sharp bits are for our external users, and to find out things we can do better. So thank you for filing this! No worries. Always happy to help make things better ğŸ» ","> Oh okay. Any approximate timeline for the fixes that are already done but not in the dev branch? The changes that are done are all in the dev branch! And in fact, you can trigger the experimental rewriter using the `xla_gpu_experimental_enable_triton_softmax_priority_fusion=true` (to be set in the `XLA_FLAGS` environment variable); but this doesn't work well for this workload just yet unfortunately, which is why I didn't advertise it so far. (Note, that flag is mostly for development, and may get deleted without warningthough that'll probably mean that we enabled the feature by default!)","Oh I have tried those, and it didn't work that time. Here is the corresponding issue: https://github.com/openxla/xla/issues/17103", did you find anything else that can speed up the runs?,"Hi ; I didn't yet investigate further for this particular model. In general, we have a lot of work in the pipeline that should have a generally good effect on such models, but I haven't checked how they'll apply specifically to this benchmark.",">In general, we have a lot of work in the pipeline that should have a generally good effect on such models, but I haven't checked how they'll apply specifically to this benchmark. No worries. I can do that. Please let me know once these improvements are in the nightly or a stable version. I can restart the benchmarking ","> No worries. I can do that. Please let me know once these improvements are in the nightly or a stable version. I can restart the benchmarking. I will keep you updated. The next big thing will hopefully be a fix for loop unrolling, which should allow us to reclaim compiletime and get the same performance as with unrolling `scan` out of the box without actually unrolling."
1445,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jax.distributed.initialize() crash)ï¼Œ å†…å®¹æ˜¯ ( Description Hi JAX team, In the past two days, I've been using GCP's queuedresources to create spot TPU v4256/v464, and then running the following Python script.   However, I found that it gets stuck at the jax.distributed.initialize() command. This is very strange because when I created an ondemand TPU v464 two weeks ago, the jax.distributed.initialize() command executed without any issues, and it still works fine on that machine. But now, with the newly created instances, I'm facing this problem. Therefore, I'd like to seek help from the JAX team ! BUG  pip list   Setup bash   System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.34 jaxlib: 0.4.34 numpy:  2.1.2 python: 3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0] jax.devices (128 total, 4 local): [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0) TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0) ... TpuDevice(id=126, process_index=31, coords=(2,3,7), core_on_chip=0) TpuDevice(id=127, process_index=31, coords=(3,3,7), core_on_chip=0)] process_count: 32 platform: uname_result(system='Linux', node='t1vndb3292aew17', release='5.19.01022gcp', version=' CC(attempt to centerjustify the jax logo in readme)~22.0)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,jax.distributed.initialize() crash," Description Hi JAX team, In the past two days, I've been using GCP's queuedresources to create spot TPU v4256/v464, and then running the following Python script.   However, I found that it gets stuck at the jax.distributed.initialize() command. This is very strange because when I created an ondemand TPU v464 two weeks ago, the jax.distributed.initialize() command executed without any issues, and it still works fine on that machine. But now, with the newly created instances, I'm facing this problem. Therefore, I'd like to seek help from the JAX team ! BUG  pip list   Setup bash   System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.34 jaxlib: 0.4.34 numpy:  2.1.2 python: 3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0] jax.devices (128 total, 4 local): [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0) TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0) ... TpuDevice(id=126, process_index=31, coords=(2,3,7), core_on_chip=0) TpuDevice(id=127, process_index=31, coords=(3,3,7), core_on_chip=0)] process_count: 32 platform: uname_result(system='Linux', node='t1vndb3292aew17', release='5.19.01022gcp', version=' CC(attempt to centerjustify the jax logo in readme)~22.0",2024-10-19T01:05:03Z,bug,open,8,4,https://github.com/jax-ml/jax/issues/24399,I'm facing the same problem..,"Hi Jax team, We are also facing this issue with TPUs v2, v3 and v4. However, v5p's are ok.","The issue should be resolved by now, could you please double check on a newly created TPU VM and let us know if it's still happening?","> The issue should be resolved by now, could you please double check on a newly created TPU VM and let us know if it's still happening?  Thank you for the update! I've tested on a newly created TPU VM, and the issue is now resolved. Everything is working as expected."
344,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Avoid querying metadata query to check if it's GCE if `TPU_SKIP_MDS_QUERY` is set.)ï¼Œ å†…å®¹æ˜¯ (Avoid querying metadata query to check if it's GCE if `TPU_SKIP_MDS_QUERY` is set.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Avoid querying metadata query to check if it's GCE if `TPU_SKIP_MDS_QUERY` is set.,Avoid querying metadata query to check if it's GCE if `TPU_SKIP_MDS_QUERY` is set.,2024-10-18T20:07:23Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24394
675,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Allowing HLO constants inside custom partition)ï¼Œ å†…å®¹æ˜¯ (Hello JAX team, I am trying to implement a custom partitionned function that has consts in it  Here is the most minimal WE  So this give's me an error in custom_partionning  If I comment this line I get this error  and the arg_shardings are   Which corresponds to the consts that are disallowed Is there anyway around this?  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Allowing HLO constants inside custom partition,"Hello JAX team, I am trying to implement a custom partitionned function that has consts in it  Here is the most minimal WE  So this give's me an error in custom_partionning  If I comment this line I get this error  and the arg_shardings are   Which corresponds to the consts that are disallowed Is there anyway around this?  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example.",2024-10-18T17:41:05Z,enhancement,closed,0,6,https://github.com/jax-ml/jax/issues/24390, Here is my issue as requested ,This is the assert that fires btw: https://github.com/jaxml/jax/blob/bbcc3eef3c1fedda3c0eef48c8bd49fd34a313c9/jax/_src/custom_partitioning.pyL462  do you know why we don't allow consts in the custom partitioning jaxpr?,"[Update] I was able to achieve the same goal with much simpler code  Have seen no documentation on `shard_alike` but it is extremely usefull (so hopefully it is here to stay). About the constants in custom partition, I had another run in with this issue when using `norm='forward'` with a `jnp.fft.fft` and the workaround was normalizing outside the lowered_fn I would accept this issue to be a permanent restriction, and that users should find work arounds like the ones I did",can I please get a feedback on `shard_alike` I have this in the HLO  I can assume that it is scattering the frequencies however why the all gather? Is this how scatter is implemented in XLA?,The reason for no consts is that the jaxpr that is traced is different than the jaxpr that is partitioned and promoting it to a parameter would be strange because we would have to line up the trace time jaxpr consts with the partitioned jaxpr consts.,"Thank you   Closing since in all cases, workarounds exists Would be nice if documented somewhere"
940,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`jax.jit` slows down the code a lot on function with simple array operations and `jnp.roll()`)ï¼Œ å†…å®¹æ˜¯ ( Description I get significant 4x slowdown in JAX code when I add a `.jit` to my main update function, which manipulates large arrays with elementwise math and `jnp.roll()` A minimal reproducer is included below, where removing the `.jit` around the `update()` function (line marked by a comment ` XXX`) speeds up the code a lot. The slowdown is not due to compiletime overhead. I'm quite puzzled by the behavior and think it may be a bug in JAX or XLA. What is the best way to get to the bottom of this issue? To reproduce, run `python euler.py` with and without the jit decorator around `update()`:   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,`jax.jit` slows down the code a lot on function with simple array operations and `jnp.roll()`," Description I get significant 4x slowdown in JAX code when I add a `.jit` to my main update function, which manipulates large arrays with elementwise math and `jnp.roll()` A minimal reproducer is included below, where removing the `.jit` around the `update()` function (line marked by a comment ` XXX`) speeds up the code a lot. The slowdown is not due to compiletime overhead. I'm quite puzzled by the behavior and think it may be a bug in JAX or XLA. What is the best way to get to the bottom of this issue? To reproduce, run `python euler.py` with and without the jit decorator around `update()`:   System info (python version, jaxlib version, accelerator, etc.) ",2024-10-17T19:45:16Z,bug XLA,open,0,4,https://github.com/jax-ml/jax/issues/24373,I am simplifying the code to highlight the error:  gives: ,"Thanks for the report! This is definitely unexpected, and points to some compiler issue. I updated your timing to separate out the first call, use `block_until_ready` to avoid issues due to asynchronous dispatch, and use IPython's `%timeit` syntax for better fidelity:  This is the result on Colab CPU:  and this is the result on a Colab T4 GPU:  So it seems this issue is particular to the XLA:CPU compiler. It may be worth reporting this upstream at https://github.com/openxla/xla, though it would be useful to try and reduce the repro even further.","Thanks for taking a look at this  , and pinpointing that this seems to be a CPU only issue. Definitely unexpected. What is really weird too is that if I comment out some simple terms in the `apply_fluxes` function like: `flux_A = 0.1 * (A_L  A_R)`, `flux_B = 0.1 * (B_L  B_R)` then the issue goes away I will make an issue with the XLA team as well",XLA issue is raised here: https://github.com/openxla/xla/issues/18478
757,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Apple GPU (Metal) plugin] (Stable) HLO support ?)ï¼Œ å†…å®¹æ˜¯ (hi, I maintain GoMLX a machine learning framework based on XLA, and when trying to use it on Apple Metal (GPU) I bumped on the error bellow. `PJRT error (code=3): METAL only supports MLIR format input.` Are there any plans to support HLO ? GoMLX is feeding it with the output of the XlaBuilder, the compiled `HloModuleProto` proto that it exports. Any other suggestions ? many thanks! Full log: (Notice a couple of the lines are GoMLX specific, I named the plugin ""cpu"", but it's the one downloaded from `jaxmetal` pip package) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[Apple GPU (Metal) plugin] (Stable) HLO support ?,"hi, I maintain GoMLX a machine learning framework based on XLA, and when trying to use it on Apple Metal (GPU) I bumped on the error bellow. `PJRT error (code=3): METAL only supports MLIR format input.` Are there any plans to support HLO ? GoMLX is feeding it with the output of the XlaBuilder, the compiled `HloModuleProto` proto that it exports. Any other suggestions ? many thanks! Full log: (Notice a couple of the lines are GoMLX specific, I named the plugin ""cpu"", but it's the one downloaded from `jaxmetal` pip package) ",2024-10-17T14:20:40Z,enhancement Apple GPU (Metal) plugin,closed,0,3,https://github.com/jax-ml/jax/issues/24365, ,"To answer my own question, I think by MLIR the Metal PJRT plugin means StableHLO, encoded as ""mlir""  but not having used StableHLO I was not aware of the nuance. And I was pointed to the code in PyTorch XLA that deals with the same situation, if the PJRT doesn't accept ""hlo"" by converting it to StableHLO (as an `mlir::ModuleOp` object). Here is the link for others bumping into the same issue: https://github.com/pytorch/xla/blob/1bd17f7be2de80d82cba89f00732874d697871c9/torch_xla/csrc/runtime/pjrt_computation_client.ccL619 Closing the issue.",JAXMetal plugin accepts Stablehlo (MLIR) as input from JAX and compiles to an internal MLIR dialect. HLO computation as an IR used by XLA is not supported by JAXMetal. 
1423,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Update ad.py)ï¼Œ å†…å®¹æ˜¯ ( **1. Optimizing Memory Management**  **1.1 Efficient Tangent Handling with Zero Optimization**  **Change**: Implemented efficient handling of zero tangents using the `Zero` class to avoid unnecessary memory allocation and computations.  **Motivation**: Handling zero tangents efficiently is crucial as they frequently occur in automatic differentiation.  **Benefits**: Reduced memory usage and computational overhead, leading to faster execution.  **1.2 Lazy Evaluation to Defer Unnecessary Computations**  **Change**: Deferred computations involving zero tangents by introducing checks before performing operations.  **Motivation**: Shortcircuiting operations involving zeros avoids unnecessary computation.  **Benefits**: Improved performance and reduced resource usage.  **1.3 Efficient Storage Using Dictionaries**  **Change**: Used dictionaries for storing cotangents and primals, optimizing access and storage during the backward pass.  **Motivation**: Efficient data structures improve both the speed and memory usage of iterative algorithms like backpropagation.  **Benefits**: Faster lookup and more efficient memory management.   **2. Improving Error Handling**  **2.1 Enhanced Custom Exception Messages**  **Change**: U)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Update ad.py," **1. Optimizing Memory Management**  **1.1 Efficient Tangent Handling with Zero Optimization**  **Change**: Implemented efficient handling of zero tangents using the `Zero` class to avoid unnecessary memory allocation and computations.  **Motivation**: Handling zero tangents efficiently is crucial as they frequently occur in automatic differentiation.  **Benefits**: Reduced memory usage and computational overhead, leading to faster execution.  **1.2 Lazy Evaluation to Defer Unnecessary Computations**  **Change**: Deferred computations involving zero tangents by introducing checks before performing operations.  **Motivation**: Shortcircuiting operations involving zeros avoids unnecessary computation.  **Benefits**: Improved performance and reduced resource usage.  **1.3 Efficient Storage Using Dictionaries**  **Change**: Used dictionaries for storing cotangents and primals, optimizing access and storage during the backward pass.  **Motivation**: Efficient data structures improve both the speed and memory usage of iterative algorithms like backpropagation.  **Benefits**: Faster lookup and more efficient memory management.   **2. Improving Error Handling**  **2.1 Enhanced Custom Exception Messages**  **Change**: U",2024-10-16T16:35:01Z,,open,0,2,https://github.com/jax-ml/jax/issues/24337,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Thanks for the contribution! You'll need to sign the CLA before we can take a look in detail, but a couple broad points:  this PR combines logic changes with formatting changes, which makes it difficult for reviewers to isolate logic changes for review. I'd suggest reverting all formatting changes not related to the particular lines you've updated (for example, you should return the file to its orignal twospace indentation)  this PR makes many independent changes to the file, and while the enumerated description is helpful, it would be more helpful if each independent section of the description were done in its own PR. That would let us review each change individually (see https://jax.readthedocs.io/en/latest/contributing.htmlsinglechangecommitsandpullrequests for our recommendation here). Thanks!"
286,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([pallas:mosaic_gpu] Added `FragmentedArray.to_layout`)ï¼Œ å†…å®¹æ˜¯ ([pallas:mosaic_gpu] Added `FragmentedArray.to_layout`)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[pallas:mosaic_gpu] Added `FragmentedArray.to_layout`,[pallas:mosaic_gpu] Added `FragmentedArray.to_layout`,2024-10-16T15:04:18Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24334
223,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Improve array setitem error)ï¼Œ å†…å®¹æ˜¯ (Before:  After: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",dspy,Improve array setitem error,Before:  After: ,2024-10-15T18:41:43Z,better_errors pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/24315
1174,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Matrix factorization on multiple GPUs causes memory overflow)ï¼Œ å†…å®¹æ˜¯ ( Description My original aim is to compute a huge amount of determinants that can't fit into the memory of a single GPU. I always get memory overflows when I run it on multiple GPUs, and the problem seems to be the matrix factorization. Here is a simple example to perform matrix factorization in parallel. I parallelize more computations when I have more machines.  It works well on a single A10080GB GPU, but causes the following memory overflow on 3 GPUs. It seems that I can never parallelize more computations with multiple machines. Other matrix factorizations like `qr` or `cholesky` cause the same problem.  I have tried `jax.transfer_guard` to ensure that no data is transferred among machines during matrix factorization. Maybe I made some stupid mistakes. I really appreciate any help with this problem, or any suggestion to obtain determinants in parallel.  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Matrix factorization on multiple GPUs causes memory overflow," Description My original aim is to compute a huge amount of determinants that can't fit into the memory of a single GPU. I always get memory overflows when I run it on multiple GPUs, and the problem seems to be the matrix factorization. Here is a simple example to perform matrix factorization in parallel. I parallelize more computations when I have more machines.  It works well on a single A10080GB GPU, but causes the following memory overflow on 3 GPUs. It seems that I can never parallelize more computations with multiple machines. Other matrix factorizations like `qr` or `cholesky` cause the same problem.  I have tried `jax.transfer_guard` to ensure that no data is transferred among machines during matrix factorization. Maybe I made some stupid mistakes. I really appreciate any help with this problem, or any suggestion to obtain determinants in parallel.  System info (python version, jaxlib version, accelerator, etc.) ",2024-10-15T15:00:05Z,bug,open,1,3,https://github.com/jax-ml/jax/issues/24309,"Unfortunately, the JAX/XLA compiler currently does not know how to shard  `jax.lax.linalg.lu` operator. As a workaround, you can still do the sharding manually, see the JAX docs. For your example, the manually parallelized version could be something like this: ","sevcik Thanks very much for your reply! This solves my problem. By the way, is there any plan to solve the sharding problem in the future? It looks straightforward.","Phys â€” Thanks for the report! And thanks to sevcik for suggesting this workaround. > By the way, is there any plan to solve the sharding problem in the future? It looks straightforward. JAX doesn't currently have a great API for customizing the sharding behavior of custom calls (which is how most of the linear algebra operations are implemented). `custom_partitioning` is one option, but the fact that it is implemented using Python callbacks means that it can introduce some surprising issues that are probably beyond the scope of the discussion here. All that to say, a better solution would be great (and should be possible with upstream changes in XLA), but I'd say it's probably not straightforward. For now, sevcik's suggestion to use `shard_map` is the best approach!"
1279,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Gradient of jnp.linalg.eigvals raises NotImplementedError about eigenvectors)ï¼Œ å†…å®¹æ˜¯ ( Description Hey, Im trying to calculate a hessian. Within my function im calculating some eigenvalues using jnp.linalg.eigvals(). This raises the mentioned error. ""NotImplementedError: The derivatives of eigenvectors are not implemented, only eigenvalues. See https://github.com/google/jax/issues/2748 for discussion."" However the eigenvector calculation is turned off within the eigvals() function. So Im not sure whats going on. The issue pointed out in the error message is 4 years old and mainly concerned with the eigenvectors. I am using jax 0.4.31.  Does anyone know whats going on here?  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.31 jaxlib: 0.4.31 numpy:  1.26.0 python: 3.11.7  (main, Dec 23 2023, 14:43:09) [GCC 12.3.0] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='Name', release='5.15.153.1microsoftstandardWSL2', version=' CC(Python 3 compatibility issues) SMP Fri Mar 29 23:14:13 UTC 2024', machine='x86_64'))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Gradient of jnp.linalg.eigvals raises NotImplementedError about eigenvectors," Description Hey, Im trying to calculate a hessian. Within my function im calculating some eigenvalues using jnp.linalg.eigvals(). This raises the mentioned error. ""NotImplementedError: The derivatives of eigenvectors are not implemented, only eigenvalues. See https://github.com/google/jax/issues/2748 for discussion."" However the eigenvector calculation is turned off within the eigvals() function. So Im not sure whats going on. The issue pointed out in the error message is 4 years old and mainly concerned with the eigenvectors. I am using jax 0.4.31.  Does anyone know whats going on here?  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.31 jaxlib: 0.4.31 numpy:  1.26.0 python: 3.11.7  (main, Dec 23 2023, 14:43:09) [GCC 12.3.0] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='Name', release='5.15.153.1microsoftstandardWSL2', version=' CC(Python 3 compatibility issues) SMP Fri Mar 29 23:14:13 UTC 2024', machine='x86_64')",2024-10-15T14:19:58Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/24308,"Hi  thanks for the question! The issue is that gradients of eigenvectors and eigenvalues for nonsymmetric inputs is poorly defined, mainly due to degeneracies in the output if I recall the discussion correctly. So despite the github issue being 4 years old, the problem it discusses still remains.","Hello , thanks that clears it up. Sadly my input matrix is generally not symmetric.  To avoid any future confusion I'd suggest to that the error message should be changed. Because ""not implemented"" and ""not clearly defined"" are different things.  And its confusing that the error message only speaks of eigenvectors when one is trying only to get eigenvalues. ","Hey, sorry I was just looking at this further, and it seems that nonsymmetric eigenvalues should differentiate without any issues. For example:   Can you provide an example of code that leads to the behavior you describe?","Ah thats weird. Your example works for me as well.  When recreating the error I realized that this issue only arises when calculating the hessian. The gradient actually works fine.  If In your example you do `jax.grad(jax.grad(f))(x)` you'll get the error. (at least I get it) ""NotImplementedError: The derivatives of eigenvectors are not implemented, only eigenvalues. See https://github.com/google/jax/issues/2748 for discussion.""","Ah, I see  it's becuase the jvp rule for `eig` depends on the eigenvectors: https://github.com/jaxml/jax/blob/ebac2e44219a40862a9798484da1662d6800a1e9/jax/_src/lax/linalg.pyL774 The result is that if you differentiate twice, you hit gradofeigenvectors, which leads to this error. So I think the resolution here is that we only support firstorder gradients with respect to eigenvectors, unfortunately.",I've updated the error message in https://github.com/jaxml/jax/pull/24350 â€“ hopefully that will be clearer to future users who hit this error.,"Alright, thank you very much. "
1009,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(JAX argmin function returns incorrect index when handling subnormal float values.)ï¼Œ å†…å®¹æ˜¯ ( Description When using the JAX `argmin` function on an input array containing subnormal float values, JAX consistently returns the index of `0.0` as the minimum value, even though a smaller subnormal value (`1.401298464324817e45`) exists in the array. Other deep learning frameworks such as PyTorch and Chainer correctly return the index of the subnormal value, but JAX (similar to TensorFlow and Keras) returns the index of `0`.  Expected Behavior: The expected behavior is for JAX's `argmin` function to return the index of the smallest value, which should be the subnormal float value (`1.401298464324817e45`) at index 2. Instead, JAX is returning the index of `0.0` (index 0).   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,JAX argmin function returns incorrect index when handling subnormal float values.," Description When using the JAX `argmin` function on an input array containing subnormal float values, JAX consistently returns the index of `0.0` as the minimum value, even though a smaller subnormal value (`1.401298464324817e45`) exists in the array. Other deep learning frameworks such as PyTorch and Chainer correctly return the index of the subnormal value, but JAX (similar to TensorFlow and Keras) returns the index of `0`.  Expected Behavior: The expected behavior is for JAX's `argmin` function to return the index of the smallest value, which should be the subnormal float value (`1.401298464324817e45`) at index 2. Instead, JAX is returning the index of `0.0` (index 0).   System info (python version, jaxlib version, accelerator, etc.) ",2024-10-15T08:28:48Z,duplicate,closed,0,1,https://github.com/jax-ml/jax/issues/24307,This issue is duplicate of https://github.com/jaxml/jax/issues/24281 with a resolution depending on the answer to https://github.com/jaxml/jax/issues/24280issuecomment2411336992 .
1465,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`custom_root` with integer aux output broken in 0.4.34)ï¼Œ å†…å®¹æ˜¯ ( Description I have a basic root finder like this:  which returns both the root and the value of f at the root, and the number of steps taken. Previously this worked fine, with `has_aux=True` for `custom_root`. However, v0.4.34 seems to have changed something in the way tangents of nondifferentiable values get propagated ( CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)). Now running the following   gives the following:  I get the same error if I drop the aux output in `find_root_fun` and leave out the `has_aux` when calling `jacfwd`. The only way I've found to avoid the error is to remove the aux from the innermost `solve` and set `has_aux=False` on `custom_root` Is this expected? I assumed having integer valued aux output was kind of the point of the `has_aux` option?  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.34 jaxlib: 0.4.34 numpy:  1.24.4 python: 3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0] jax.devices (8 total, 8 local): [CpuDevice(id=0) CpuDevice(id=1) ... CpuDevice(id=6) CpuDevice(id=7)] process_count: 1 platform: uname_result(system='Linux', node='Discovery', release='5.15.0119generic', version=' CC(CUDA backend produces inconsistent results for jax.numpy.linalg.inv)~20.04.1Ubuntu SMP Wed Aug 7 13:)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,`custom_root` with integer aux output broken in 0.4.34," Description I have a basic root finder like this:  which returns both the root and the value of f at the root, and the number of steps taken. Previously this worked fine, with `has_aux=True` for `custom_root`. However, v0.4.34 seems to have changed something in the way tangents of nondifferentiable values get propagated ( CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)). Now running the following   gives the following:  I get the same error if I drop the aux output in `find_root_fun` and leave out the `has_aux` when calling `jacfwd`. The only way I've found to avoid the error is to remove the aux from the innermost `solve` and set `has_aux=False` on `custom_root` Is this expected? I assumed having integer valued aux output was kind of the point of the `has_aux` option?  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.34 jaxlib: 0.4.34 numpy:  1.24.4 python: 3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0] jax.devices (8 total, 8 local): [CpuDevice(id=0) CpuDevice(id=1) ... CpuDevice(id=6) CpuDevice(id=7)] process_count: 1 platform: uname_result(system='Linux', node='Discovery', release='5.15.0119generic', version=' CC(CUDA backend produces inconsistent results for jax.numpy.linalg.inv)~20.04.1Ubuntu SMP Wed Aug 7 13:",2024-10-14T19:17:14Z,bug,open,1,1,https://github.com/jax-ml/jax/issues/24295,"I think we can simply get rid of the error by changing the number of iterations dtype to float like,  As long as we don't use the derivative of the number of iterations later in the code, I believe this shouldn't change the differentiation of `root`.  That said, this is probably not how you want to implement it. A more proper way could be writing `custom_jvp` for `root` and setting the derivative of `niter` to `SymbolicZeros`, but this is more cumbersome."
644,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Improve docs for jnp.invert and related functions)ï¼Œ å†…å®¹æ˜¯ (Part of CC(Tracking issue: inline docstrings)  Rendered docs:  https://jax24294.org.readthedocs.build/en/24294/_autosummary/jax.numpy.bitwise_invert.html  https://jax24294.org.readthedocs.build/en/24294/_autosummary/jax.numpy.bitwise_not.html  https://jax24294.org.readthedocs.build/en/24294/_autosummary/jax.numpy.invert.html  https://jax24294.org.readthedocs.build/en/24294/_autosummary/jax.numpy.logical_not.html)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Improve docs for jnp.invert and related functions,Part of CC(Tracking issue: inline docstrings)  Rendered docs:  https://jax24294.org.readthedocs.build/en/24294/_autosummary/jax.numpy.bitwise_invert.html  https://jax24294.org.readthedocs.build/en/24294/_autosummary/jax.numpy.bitwise_not.html  https://jax24294.org.readthedocs.build/en/24294/_autosummary/jax.numpy.invert.html  https://jax24294.org.readthedocs.build/en/24294/_autosummary/jax.numpy.logical_not.html,2024-10-14T18:24:17Z,documentation pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/24294
1103,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Arcsin Error Report)ï¼Œ å†…å®¹æ˜¯ ( Description Problem Description There is a noticeable discrepancy in the results when using JAX for the arcsin function compared to other deep learning libraries such as PyTorch, TensorFlow, Keras, and Chainer. For certain input values, JAX yields results that are significantly different from those produced by the other frameworks, leading to concerns about consistency and accuracy.   Significant Differences The result for the input [1.2772946] is notably different between JAX and the other libraries, which may affect the accuracy of downstream tasks. Recommendation It is recommended to review the implementation of the arcsin function in JAX to ensure consistency and accuracy. Special attention should be given to how floatingpoint arithmetic and trigonometric functions are handled, as they can significantly influence results.  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Arcsin Error Report," Description Problem Description There is a noticeable discrepancy in the results when using JAX for the arcsin function compared to other deep learning libraries such as PyTorch, TensorFlow, Keras, and Chainer. For certain input values, JAX yields results that are significantly different from those produced by the other frameworks, leading to concerns about consistency and accuracy.   Significant Differences The result for the input [1.2772946] is notably different between JAX and the other libraries, which may affect the accuracy of downstream tasks. Recommendation It is recommended to review the implementation of the arcsin function in JAX to ensure consistency and accuracy. Special attention should be given to how floatingpoint arithmetic and trigonometric functions are handled, as they can significantly influence results.  System info (python version, jaxlib version, accelerator, etc.) ",2024-10-14T02:30:15Z,duplicate question,closed,0,8,https://github.com/jax-ml/jax/issues/24275,"Thanks for the question! The reason for the discrepancy is that JAX does computations in float32 by default (see JAX Sharp Bits: double precision. If you enable 64bit operations, then the JAX output matches the output of NumPy and other frameworks that compute in 64bit by default:   and note also that with float32 inputs, NumPy's output matches the JAX output that you observed originally:  ","> Thanks for the question! The reason for the discrepancy is that JAX does computations in float32 by default (see JAX Sharp Bits: double precision. If you enable 64bit operations, then the JAX output matches the output of NumPy and other frameworks that compute in 64bit by default: >  >  >  >  >  > and note also that with float32 inputs, NumPy's output matches the JAX output that you observed originally: >  >  >  >  Similarly, all of these are specified as float32, and there are some differences when compared with the results of np.  ",Can you be more specific about what differences you're talking about? All I see here are two lists of numbers that look identical when I compare the first few byeye. What should I be looking for?,"> Can you be more specific about what differences you're talking about? All I see here are two lists of numbers that look identical when I compare the first few byeye. What should I be looking for? Thanks for the prompt response. The differences are subtle but noticeable around the 7th decimal place. For example: JAX arcsin result: [0.13651271, 1.0182546] Chainer arcsin result: [0.13651273, 1.0182545] As you can see, there's a slight difference in the 7th decimal place, which may seem small but can be significant for precisioncritical applications.","What is ""Chainer""?","I see â€“ as in CC(Arcsinh Error Report ), it appears the results differ by 1ULP, which is expected for different floating point implementations. For clarity, I'm going to close this as a duplicate of CC(Arcsinh Error Report ), and we can continue discussing the issue there if you have further questions.","> I see â€“ as in CC(Arcsinh Error Report ), it appears the results differ by 1ULP, which is expected for different floating point implementations. For clarity, I'm going to close this as a duplicate of CC(Arcsinh Error Report ), and we can continue discussing the issue there if you have further questions. Because chainer does not have its own native method, I use np. You can see the code example above.  JAX arcsin operation def jax_arcsin(x): return jnp.arcsin(jnp.array(x, dtype=np.float32))  Ensure the input is float32  Chainer arcsin operation def chainer_arcsin(x): return np.arcsin(x.astype(np.float32))  Ensure the input is float32","Thanks   I'd never heard of chainer before, but I found it through a web search."
1396,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(JaxStackTraceBeforeTransformation error with parametrized ODE)ï¼Œ å†…å®¹æ˜¯ ( Description Hi everyone, based on this tutorial I tried to get started with Jax and neural ODEs: https://colab.research.google.com/drive/1ZlK36VgWy1vBjBNXjSUg6Cb7zeoa3jh However, I get the a JaxStackTraceBeforeTransformation error (detailed error message below). I boiled down the code to a small working example (also provided below) and noted the error only occors when the equation in test_func contains an argument.  Since this issue seemed similar to one raised in an earlier post (https://github.com/jaxml/jax/issues/13629) I tried downgrading jax to version 0.4.23. I also tried setting up a fresh python environment with only the necessary packages installed. Nothing helped, so far. I'd appreciate your help :) **Working example:**  **Error message:**   System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.34 jaxlib: 0.4.34 numpy:  1.26.4 python: 3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Windows', release='10', version='10.0.19044', machine='AMD64') jupyterlab: 4.2.2 diffrax: 0.4.1)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,JaxStackTraceBeforeTransformation error with parametrized ODE," Description Hi everyone, based on this tutorial I tried to get started with Jax and neural ODEs: https://colab.research.google.com/drive/1ZlK36VgWy1vBjBNXjSUg6Cb7zeoa3jh However, I get the a JaxStackTraceBeforeTransformation error (detailed error message below). I boiled down the code to a small working example (also provided below) and noted the error only occors when the equation in test_func contains an argument.  Since this issue seemed similar to one raised in an earlier post (https://github.com/jaxml/jax/issues/13629) I tried downgrading jax to version 0.4.23. I also tried setting up a fresh python environment with only the necessary packages installed. Nothing helped, so far. I'd appreciate your help :) **Working example:**  **Error message:**   System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.34 jaxlib: 0.4.34 numpy:  1.26.4 python: 3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Windows', release='10', version='10.0.19044', machine='AMD64') jupyterlab: 4.2.2 diffrax: 0.4.1",2024-10-11T16:59:43Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/24253,"The error reported here is actually a `TypeError` being raised because of an issue with the return types in a `jax.custom_jvp`. It's hard to see from this error report exactly which `custom_jvp` is the culprit, but it seems like it must be something within diffrax or equinox, so I'd recommend opening the issue on the https://github.com/patrickkidger/diffrax issue tracker.","ok, thanks for pointing this out. I'll try my luck there.",I'm going to close this since it looks like the conversations in https://github.com/patrickkidger/diffrax/issues/513 are getting to the bottom of things. Please let me know if there's something I'm missing!
656,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add lax.FftType.)ï¼Œ å†…å®¹æ˜¯ (Add lax.FftType. We had never provided a public name for the enum of FFT types; instead it was only known by a semiprivate name (jax.lib.xla_client.FftType). Add a public name (jax.lax.FftType) and deprecate the private one. We define a new FftType IntEnum rather than trying to expose the one in xla_client. The xla_client definition was useful when building classic HLO, but we no longer do that so there's no reason we need to couple our type to XLA's type.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add lax.FftType.,"Add lax.FftType. We had never provided a public name for the enum of FFT types; instead it was only known by a semiprivate name (jax.lib.xla_client.FftType). Add a public name (jax.lax.FftType) and deprecate the private one. We define a new FftType IntEnum rather than trying to expose the one in xla_client. The xla_client definition was useful when building classic HLO, but we no longer do that so there's no reason we need to couple our type to XLA's type.",2024-10-10T14:18:31Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24230
372,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(FlaxLlamaForCausalLMModule hanging on jax-metal)ï¼Œ å†…å®¹æ˜¯ ( Description To reproduce the working state uncomment the device update to cpu   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,FlaxLlamaForCausalLMModule hanging on jax-metal," Description To reproduce the working state uncomment the device update to cpu   System info (python version, jaxlib version, accelerator, etc.) ",2024-10-09T23:12:15Z,bug,open,0,2,https://github.com/jax-ml/jax/issues/24221,"Hi   I tested the provided code with `JAXmetal` on a `Macbook Pro M1 Pro`. While there were no hanging issues, `model.init` and `model.apply` took longer than the CPU version. Please find the attached screenshots below: !image !image !image Thank you.",You're correct in that eventually it does run. However on Macbook Air M2 Sonoma 14.4.1 this took ~5 mins. Any insight on why it's so much slower on metal?
1457,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Activate the FFI implementation of SVD on GPU.)ï¼Œ å†…å®¹æ˜¯ (Activate the FFI implementation of SVD on GPU. Alongside activating this new implementation, this change adds a new `algorithm` parameter to `jax.lax.svd`. Previously the choice of algorithm was made based on heuristics in the lowering rule, but it probably also makes sense to expose an option for users to specify the algorithm explicitly because our heuristics are not very carefully optimized. This change updates the implementation of SVD in `lax` to use the FFI version which was added to jaxlib in https://github.com/jaxml/jax/pull/23794. This comes with a few benefits: 1. When running on a CUDA platform, the 64bit API will be used for the algorithm based on QR decomposition. (Note that it looks like the 64bit API isn't available on ROCm.) This addresses part of the feature request in https://github.com/jaxml/jax/issues/23413, although there's still work to do to port the rest of the GPU calls to the 64bit API. 2. This implementation supports shape polymorphism in all dimensions with some caveats. By default, we do use some heuristics to based on the matrix sizes to select the algorithm that is used, and the three different algorithms (QR, Jacobi, and batched Jacobi) have sufficiently different behavior (QR ret)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,Activate the FFI implementation of SVD on GPU.,"Activate the FFI implementation of SVD on GPU. Alongside activating this new implementation, this change adds a new `algorithm` parameter to `jax.lax.svd`. Previously the choice of algorithm was made based on heuristics in the lowering rule, but it probably also makes sense to expose an option for users to specify the algorithm explicitly because our heuristics are not very carefully optimized. This change updates the implementation of SVD in `lax` to use the FFI version which was added to jaxlib in https://github.com/jaxml/jax/pull/23794. This comes with a few benefits: 1. When running on a CUDA platform, the 64bit API will be used for the algorithm based on QR decomposition. (Note that it looks like the 64bit API isn't available on ROCm.) This addresses part of the feature request in https://github.com/jaxml/jax/issues/23413, although there's still work to do to port the rest of the GPU calls to the 64bit API. 2. This implementation supports shape polymorphism in all dimensions with some caveats. By default, we do use some heuristics to based on the matrix sizes to select the algorithm that is used, and the three different algorithms (QR, Jacobi, and batched Jacobi) have sufficiently different behavior (QR ret",2024-10-09T14:29:59Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24211
1487,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Inadequate memory consumption when using HSDP without gradient accumulation )ï¼Œ å†…å®¹æ˜¯ ( Description Hi, I'm training transformer model with Hybrid Sharded Data Parallelism. This setup is similar to FSDP/ZeRO3 where params allgathered for each layer's forward/backward pass and dropped afterwards. Although, instead of sharding both model params and optimizer state over all GPUs in the cluster, I shard model params only over subset of devices (usually within single node for the fast allgathers over NVLink) and shard optimizer state over all gpus (similar to FSDP/ZeRO1/2/3).  Basically, I have mesh (param_groups, model) and for each param tensor P of shape (X, Y) I shard param tensor with partition spec (model, None) and corresponding to this param P optimizer state P_o of the same shape (X, Y) with partition spec (model, param_groups).  When mesh (param_groups, model) size is: 1. (1, N_GPUs)  this is basically FSDP/ZeRO3. 2. (N, N_GPUs/ N), N > 1  HSDP. I'm also have a gradient accumulation implemented where we split input batch into chunks, calculate forward/backward pass independently and then sum their gradients. When using gradient accumulation with the factor of N (batch is splitted into N chucks and processes independently) and sequence lengths of S, peak memory usage must be equal setup with g)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Inadequate memory consumption when using HSDP without gradient accumulation ," Description Hi, I'm training transformer model with Hybrid Sharded Data Parallelism. This setup is similar to FSDP/ZeRO3 where params allgathered for each layer's forward/backward pass and dropped afterwards. Although, instead of sharding both model params and optimizer state over all GPUs in the cluster, I shard model params only over subset of devices (usually within single node for the fast allgathers over NVLink) and shard optimizer state over all gpus (similar to FSDP/ZeRO1/2/3).  Basically, I have mesh (param_groups, model) and for each param tensor P of shape (X, Y) I shard param tensor with partition spec (model, None) and corresponding to this param P optimizer state P_o of the same shape (X, Y) with partition spec (model, param_groups).  When mesh (param_groups, model) size is: 1. (1, N_GPUs)  this is basically FSDP/ZeRO3. 2. (N, N_GPUs/ N), N > 1  HSDP. I'm also have a gradient accumulation implemented where we split input batch into chunks, calculate forward/backward pass independently and then sum their gradients. When using gradient accumulation with the factor of N (batch is splitted into N chucks and processes independently) and sequence lengths of S, peak memory usage must be equal setup with g",2024-10-09T13:26:37Z,bug,open,1,0,https://github.com/jax-ml/jax/issues/24208
1303,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([JAX] Add the function API of jax.experimental.colocated_python)ï¼Œ å†…å®¹æ˜¯ ([JAX] Add the function API of jax.experimental.colocated_python This change adds an experimental API `jax.experimental.colocated_python`. The ultimate goal of this API is to provide a runtimeagnostic way to wrap a Python code that runs close to (or on) accelerator hosts. Multicontroller JAX can trivially achieve this colocated Python code execution today, while singlecontroller JAX needed its own solution for distributed Python code execution, which creates fragmentation of the user code for these two runtime architectures. `colocated_python` is an attempt to define a single device model and portable API to allow the user to write a single code once that can run on both runtime architectures. This change includes an implementation of the function API portion of `jax.experimental.colocated_python`. A (stateful) object API will be added separately. Also there will be a separate change that expresses serialized functions as an IFRT `CustomCallProgram`. It is currently in an early development stage. Please proceed with a caution when using the API.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[JAX] Add the function API of jax.experimental.colocated_python,"[JAX] Add the function API of jax.experimental.colocated_python This change adds an experimental API `jax.experimental.colocated_python`. The ultimate goal of this API is to provide a runtimeagnostic way to wrap a Python code that runs close to (or on) accelerator hosts. Multicontroller JAX can trivially achieve this colocated Python code execution today, while singlecontroller JAX needed its own solution for distributed Python code execution, which creates fragmentation of the user code for these two runtime architectures. `colocated_python` is an attempt to define a single device model and portable API to allow the user to write a single code once that can run on both runtime architectures. This change includes an implementation of the function API portion of `jax.experimental.colocated_python`. A (stateful) object API will be added separately. Also there will be a separate change that expresses serialized functions as an IFRT `CustomCallProgram`. It is currently in an early development stage. Please proceed with a caution when using the API.",2024-10-08T22:27:17Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24200
253,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Improve docs for jnp.average)ï¼Œ å†…å®¹æ˜¯ (Part of CC(Tracking issue: inline docstrings))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Improve docs for jnp.average,Part of CC(Tracking issue: inline docstrings),2024-10-08T13:24:47Z,documentation pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/24189
721,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Support for CUDNN 9.0?)ï¼Œ å†…å®¹æ˜¯ (I am using CUDNN 9.0 with CUDA 12.4 and I tried 2 things to make it work with Jax: 1. Pip install with `pip3 install upgrade ""jax[cuda12_local]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` which does not work given the jaxcudareleases only supports CUDNN 8.9 and 9.1. This worked for this environment a month ago. Now I see   2. Build jaxlib myself with instructions which fails with:  Did something change? Any suggestions on how to fix this problem if I don't want to update my CUDNN runtime?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Support for CUDNN 9.0?,"I am using CUDNN 9.0 with CUDA 12.4 and I tried 2 things to make it work with Jax: 1. Pip install with `pip3 install upgrade ""jax[cuda12_local]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` which does not work given the jaxcudareleases only supports CUDNN 8.9 and 9.1. This worked for this environment a month ago. Now I see   2. Build jaxlib myself with instructions which fails with:  Did something change? Any suggestions on how to fix this problem if I don't want to update my CUDNN runtime?",2024-10-08T02:32:52Z,enhancement,closed,0,4,https://github.com/jax-ml/jax/issues/24180,"Yeah `jax_cuda_releases` is a legacy thing. It will never be updated. The wheels are shipped on pypi these days. Is there a reason you cannot update to CUDNN 9.1? It should be very easy to do: `pip install nvidiacudnncuda12` will do it, even if you use a local installation of CUDA for everything else. Note we recommend installing CUDA and CUDNN using the pip wheels; doing so is considerably easier. It is probably possible to selfbuild a jaxlib with CUDNN 9.0 support, but you'd have to do that by adding another entry to the  the BUILD files for a 9.0 version. To do this you'd need to add that version here: https://github.com/google/tsl/blob/main/third_party/gpus/cuda/hermetic/cuda_redist_versions.bzl and then I think you would build jaxlib with GPU enabled with the following options enabled: `bazel_options=override_repository=tsl=/path/to/your/tsl/fork bazel_options=repo_env=HERMETIC_CUDNN_VERSION=""9.0.0""` (where 9.0.0 is the version you added). No promises, but that would probably work.","> Yeah `jax_cuda_releases` is a legacy thing. It will never be updated. The wheels are shipped on pypi these days. ğŸ™  will update the build for future jax > Is there a reason you cannot update to CUDNN 9.1? It should be very easy to do: `pip install nvidiacudnncuda12` will do it, even if you use a local installation of CUDA for everything else. Note we recommend installing CUDA and CUDNN using the pip wheels; doing so is considerably easier. We're shipping a container with other libraries that are tested with CUDNN 9.0, only jax is the outlier. > It is probably possible to selfbuild a jaxlib with CUDNN 9.0 support, but you'd have to do that by adding another entry to the the BUILD files for a 9.0 version. To do this you'd need to add that version here: https://github.com/google/tsl/blob/main/third_party/gpus/cuda/hermetic/cuda_redist_versions.bzl >  > and then I think you would build jaxlib with GPU enabled with the following options enabled: `bazel_options=override_repository=tsl=/path/to/your/tsl/fork bazel_options=repo_env=HERMETIC_CUDNN_VERSION=""9.0.0""` (where 9.0.0 is the version you added). >  > No promises, but that would probably work. Thanks! Will try that if all else fails.","> > Is there a reason you cannot update to CUDNN 9.1? It should be very easy to do: `pip install nvidiacudnncuda12` will do it, even if you use a local installation of CUDA for everything else. Note we recommend installing CUDA and CUDNN using the pip wheels; doing so is considerably easier. >  > We're shipping a container with other libraries that are tested with CUDNN 9.0, only jax is the outlier. I will note that cudnn promises backwards but not forwards compatibility: https://docs.nvidia.com/deeplearning/cudnn/latest/developer/forwardcompatibility.htmlcudnnapicompatibility So in principle, assuming you believe NVIDIA's promises to that effect (I guess you need your own testing to be sure), you can install 9.1 even for users that expect 9.0 and things should work.",Thank you for your help. Super helpful!
308,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Skip `test_ragged_copy_on_host` if `xla_extension_version` < 290)ï¼Œ å†…å®¹æ˜¯ (Skip `test_ragged_copy_on_host` if `xla_extension_version` < 290)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Skip `test_ragged_copy_on_host` if `xla_extension_version` < 290,Skip `test_ragged_copy_on_host` if `xla_extension_version` < 290,2024-10-07T20:40:59Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24175
300,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Remove reference to outfeed_receiver.pyi, which was deleted.)ï¼Œ å†…å®¹æ˜¯ (Remove reference to outfeed_receiver.pyi, which was deleted.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"Remove reference to outfeed_receiver.pyi, which was deleted.","Remove reference to outfeed_receiver.pyi, which was deleted.",2024-10-07T15:18:30Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24164
1402,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Revisit the symbolic shape limitation on threefry_2x32 in terms of jax.export)ï¼Œ å†…å®¹æ˜¯ (Currently, the following snippet could not work as expected, raising `jax.random functions have limited support for shape polymorphism. In particular, the product of the known dimensions must be even.`  However, according to git blame, this error was originally targeting `jax2tf`. I would like to know if this is still the case for StableHLO lowering. I understand that this involves certain degree of shape polymorphism, but I managed to express it in a way that workarounds such limitation (to some extent):  And test with  The core complexity lies in that currently JAX's symbolic system could not recognize `mod(B, 2) + B` as an even number, so I introduced an extra dummy symbol `C` to workaround it. I believe that JAX may already have some sort of polymorphic shape assertion or override mechanism internally (e.g., leveraging the fact that `out` and `count` must have same shape, and temporarily cast the shape to some simple form for the computation in between), which could further simplify the code. Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Revisit the symbolic shape limitation on threefry_2x32 in terms of jax.export,"Currently, the following snippet could not work as expected, raising `jax.random functions have limited support for shape polymorphism. In particular, the product of the known dimensions must be even.`  However, according to git blame, this error was originally targeting `jax2tf`. I would like to know if this is still the case for StableHLO lowering. I understand that this involves certain degree of shape polymorphism, but I managed to express it in a way that workarounds such limitation (to some extent):  And test with  The core complexity lies in that currently JAX's symbolic system could not recognize `mod(B, 2) + B` as an even number, so I introduced an extra dummy symbol `C` to workaround it. I believe that JAX may already have some sort of polymorphic shape assertion or override mechanism internally (e.g., leveraging the fact that `out` and `count` must have same shape, and temporarily cast the shape to some simple form for the computation in between), which could further simplify the code. Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example.",2024-10-05T16:50:18Z,enhancement,open,0,0,https://github.com/jax-ml/jax/issues/24144
535,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jaxlib source code releases)ï¼Œ å†…å®¹æ˜¯ (I'm trying to build JAX from source but I noticed that there are no source code releases for jaxlib 0.4.33 or 0.4.34 on GitHub or PyPI. Are there plans to publish the source code for those releases, or should we instead use the JAX release tarballs? P.S. I'm packaging JAX in a package manager, I can't just use the PyPI wheels.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jaxlib source code releases,"I'm trying to build JAX from source but I noticed that there are no source code releases for jaxlib 0.4.33 or 0.4.34 on GitHub or PyPI. Are there plans to publish the source code for those releases, or should we instead use the JAX release tarballs? P.S. I'm packaging JAX in a package manager, I can't just use the PyPI wheels.",2024-10-05T07:47:46Z,enhancement,closed,0,1,https://github.com/jax-ml/jax/issues/24137,"Correct. We're now using the `jax` tag to build and release both. Since we now much more closely tie the versions of the two, it seemed redundant to have two tags. It's possible in the future we may still release `jax` (the Python bits) without releasing `jaxlib` (the C++ bits), but we will never release `jaxlib` without also releasing `jax`. It should also always be the case that whenever `jaxlib` is released, it is released from the same tag as `jax`. Hope that helps!"
1065,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(FFI example not working as given in documentation)ï¼Œ å†…å®¹æ˜¯ ( Description I am trying to create a custom C extension for JAX and was trying out the example given in the documentation. But when I try to run the example, I get the following error:   System info (python version, jaxlib version, accelerator, etc.) An NVIDIA GPU may be present on this machine, but a CUDAenabled jaxlib is not installed. Falling back to cpu. jax:    0.4.33 jaxlib: 0.4.33 numpy:  2.1.1 python: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='027b41159048', release='6.8.040generic', version=' CC(Invalid proto descriptor for file ""tensorflow/compiler/xla/xla_data.proto"")~22.04.3Ubuntu SMP PREEMPT_DYNAMIC Tue Jul 30 17:30:19 UTC 2', machine='x86_64') $ nvidiasmi Fri Oct  4 18:19:24 2024        ++  ++)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,FFI example not working as given in documentation," Description I am trying to create a custom C extension for JAX and was trying out the example given in the documentation. But when I try to run the example, I get the following error:   System info (python version, jaxlib version, accelerator, etc.) An NVIDIA GPU may be present on this machine, but a CUDAenabled jaxlib is not installed. Falling back to cpu. jax:    0.4.33 jaxlib: 0.4.33 numpy:  2.1.1 python: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='027b41159048', release='6.8.040generic', version=' CC(Invalid proto descriptor for file ""tensorflow/compiler/xla/xla_data.proto"")~22.04.3Ubuntu SMP PREEMPT_DYNAMIC Tue Jul 30 17:30:19 UTC 2', machine='x86_64') $ nvidiasmi Fri Oct  4 18:19:24 2024        ++  ++",2024-10-04T18:23:13Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/24131,"Thanks for this report! That example is being run as part of CI so I expect there's something different in your setup. Can you share the exact C++ and Python code that you're running? Edited to add: You might also consider checking out this version of that example which includes all the packaging details, etc.: https://github.com/jaxml/jax/tree/main/examples/ffi",Thanks the example helps! I tried to copy the parts from the documentation in order.  Was able to generate librms_norm.so and install it.  Hope this helps answer the issue. There are decorators on top of the example you provided so maybe the documentation has not been updated?,"Oh I see what's happening here! The decorators are actually a red herring. The issue is actually the `vmap_method` parameter. That was only added in JAX v0.4.34, so in the earlier version that you're using the `vmap_method` input is being interpreted as an attribute that you want to pass to the FFI handler. So, in JAX v0.4.33, you should use `vectorized=True` instead of `vmap_method`, although that behavior is deprecated going forward. Hope this helps!",That was the issue. Thanks!
1474,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Problems when customizing event-driven opertors with ``pallas``)ï¼Œ å†…å®¹æ˜¯ ( Description Hello everyone, I am writing kernel functions using ``pallas``. The function I want to implement is the matrixvector multiplication, where the vector is the binary events (0/1 values): $$y = x @ M$$ where $x$ is the binary events (corresponding to the ``spikes`` argument in the following function), and $M$ is stored as the ELL sparse format (corresponding to the ``indices`` and ``weights`` arguments in the following function).  The code I wrote is shown below. The program logic is very simple: I use a thread block to process one row of ``indices`` and ``weights``. When there is an event (``spikes[i] = True``), the corresponding positions in $y$ will be `atomic_add` the weight values.   I have encountered several bugs (or strange things): 1. When the ``n_pre`` and ``n_post``are small, for example ``n_pre=100, n_post=200``, the GPU is usually blocked for a long time without any computing. Or Jax outputs the error message: `E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1270] error deallocating host memory at 0x75e0a9a00000: INTERNAL: CUDA error: : CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered` 2. When I increase the size, the computed values are usually not right. All va)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,Problems when customizing event-driven opertors with ``pallas``," Description Hello everyone, I am writing kernel functions using ``pallas``. The function I want to implement is the matrixvector multiplication, where the vector is the binary events (0/1 values): $$y = x @ M$$ where $x$ is the binary events (corresponding to the ``spikes`` argument in the following function), and $M$ is stored as the ELL sparse format (corresponding to the ``indices`` and ``weights`` arguments in the following function).  The code I wrote is shown below. The program logic is very simple: I use a thread block to process one row of ``indices`` and ``weights``. When there is an event (``spikes[i] = True``), the corresponding positions in $y$ will be `atomic_add` the weight values.   I have encountered several bugs (or strange things): 1. When the ``n_pre`` and ``n_post``are small, for example ``n_pre=100, n_post=200``, the GPU is usually blocked for a long time without any computing. Or Jax outputs the error message: `E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1270] error deallocating host memory at 0x75e0a9a00000: INTERNAL: CUDA error: : CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered` 2. When I increase the size, the computed values are usually not right. All va",2024-10-04T12:26:08Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/24117,Another wrong behavior is related to ``atomic_add``. It seems that this operation does not calculate the correct result when two indices are the same within one thread block. Here is an example: 
1242,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Regression] Gradient explodes after upgrading to JAX 0.4.33 from 0.4.30)ï¼Œ å†…å®¹æ˜¯ ( Description I'm training LLAMA3.1like transformer architecture in Hybrid Sharded Data ParallelContext Parallel setup on 32GPUs. Upgrading to JAX 0.4.33 has broken training of 70B model  loss becomes NaN after single training step. Evidences that I've collected so far: * Loss on the first step is exactly the same on 0.4.33 and 0.4.30  * Gradients of the unembedding layer and last layer norm are also exactly the same on the first step.  * Gradient already explodes for the last transformer layer's MLP hidden>output layer weight matrix, which I believe is the first layer after token_unembedding and last layer norm matrix.  * On JAX 0.4.30, 0.4.29 I've trained tens of such models with different hyperparams and datasets and have never seen any NaN. * For now, I wasn't able to reproduce this behavior on smaller models, but I'm working on it. * XLA dumps attached xla_dump_0_4_30.tar.gz xla_dump_0_4_33.tar.gz  System info (python version, jaxlib version, accelerator, etc.)  XLA issue)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[Regression] Gradient explodes after upgrading to JAX 0.4.33 from 0.4.30," Description I'm training LLAMA3.1like transformer architecture in Hybrid Sharded Data ParallelContext Parallel setup on 32GPUs. Upgrading to JAX 0.4.33 has broken training of 70B model  loss becomes NaN after single training step. Evidences that I've collected so far: * Loss on the first step is exactly the same on 0.4.33 and 0.4.30  * Gradients of the unembedding layer and last layer norm are also exactly the same on the first step.  * Gradient already explodes for the last transformer layer's MLP hidden>output layer weight matrix, which I believe is the first layer after token_unembedding and last layer norm matrix.  * On JAX 0.4.30, 0.4.29 I've trained tens of such models with different hyperparams and datasets and have never seen any NaN. * For now, I wasn't able to reproduce this behavior on smaller models, but I'm working on it. * XLA dumps attached xla_dump_0_4_30.tar.gz xla_dump_0_4_33.tar.gz  System info (python version, jaxlib version, accelerator, etc.)  XLA issue",2024-10-04T10:08:23Z,bug,closed,2,4,https://github.com/jax-ml/jax/issues/24114,"Thanks for the report! A good place to start with debugging this would be to try to narrow down a tighter window on when this change occurred. Would you be able to try bisecting over nightly releases between 0.4.30 and 0.4.33 to try and identify when this difference appeared? Once we have that, we can dig into the relevant changes around then to try to see what's happening!","Hi! Bisecting Jax version would be quite timeconsuming for me as I would have to rebuild my image for every run. I've also tried freshly release JAX 0.4.34  problem hasn't been solved there. Please also check xla issue, I've added tons of information and evidences there. Specifically, I've bisected specific model size at which something clearly breaks. https://github.com/openxla/xla/issues/17922issuecomment2394209837",We've figured our workaround for this problem  https://github.com/openxla/xla/issues/17922issuecomment2396280557,PR with this fix will be merged to XLA soon
1503,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(scale_and_translate with output_size and scale returning zero values for last row and column)ï¼Œ å†…å®¹æ˜¯ ( Description Trying to replicate the behavior of pytorch upsample_bilinear2d using jax.image.scale_and_translate. This is as part of the pytorch xla project: https://github.com/pytorch/xla/issues/7389 We did some investigation as part of: https://github.com/pytorch/xla/pull/8208 The behavior between pytorch upsample_bilinear(align_corners=true, shape=something) and our implementation using jax scale_and_translate is different. Since jax does not have an implementation that supports `align_corners=true` we are implementing ourselves using existing functions. Existing bug in jax: https://github.com/jaxml/jax/issues/11206 Look here for the difference in output as well as a script to reproduce it: https://github.com/pytorch/xla/pull/8208issuecomment2390049360 when i change jax._src..image.scale.compute_weight_mat to return weights without zeroing, the output matches. Here is the script that copied the relevant jax code to investigate the behaviour: https://github.com/pytorch/xla/pull/8208issuecomment2391967612 We are not sure which behaviour is correct. All we know is how to make jax replicate pytorch's behavior. If you think the jax behaviour needs to be changed, i have a patch for that. Let me know your thoughts. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,scale_and_translate with output_size and scale returning zero values for last row and column," Description Trying to replicate the behavior of pytorch upsample_bilinear2d using jax.image.scale_and_translate. This is as part of the pytorch xla project: https://github.com/pytorch/xla/issues/7389 We did some investigation as part of: https://github.com/pytorch/xla/pull/8208 The behavior between pytorch upsample_bilinear(align_corners=true, shape=something) and our implementation using jax scale_and_translate is different. Since jax does not have an implementation that supports `align_corners=true` we are implementing ourselves using existing functions. Existing bug in jax: https://github.com/jaxml/jax/issues/11206 Look here for the difference in output as well as a script to reproduce it: https://github.com/pytorch/xla/pull/8208issuecomment2390049360 when i change jax._src..image.scale.compute_weight_mat to return weights without zeroing, the output matches. Here is the script that copied the relevant jax code to investigate the behaviour: https://github.com/pytorch/xla/pull/8208issuecomment2391967612 We are not sure which behaviour is correct. All we know is how to make jax replicate pytorch's behavior. If you think the jax behaviour needs to be changed, i have a patch for that. Let me know your thoughts. ",2024-10-03T23:07:16Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/24106,"Thanks for your report! Related questions have come up a few times in the past. For example, this seems related to the comment here: https://github.com/jaxml/jax/issues/15768issuecomment1529939102 My understanding from that thread and your comments here is that to core issue is related to the handling of edge effects in the kernel. I'd say that we probably don't want to unilaterally change the behavior of this function in JAX because there are almost certainly users depending on the specifics of this implementation. That being said, it does sound like it would be useful to add an _option_ to change how the edge effects are handled. Would you be up for trying to implement something like that in your PR? Thanks!"
1426,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Issues with vmap and GPU memory issues )ï¼Œ å†…å®¹æ˜¯ ( Description hello, I am trying to convert pytorch code to JAX related with an algorithm that performs hamiltonian sampling couple to a dynamic nested sampling algorithm. The goal is to vmap GPU parallelize the procedure but I am getting GPU memory saturation. The code is complex and I cinlude the three main functions where everything happens. the method is iterative due to the while loop in ""find_new_sample_batch"". It looks like the shape of some variable grows in each iteration but I am not able to find it.  The vectorisation is done  to paralelize a loop over a batch of points in 2D. In this case the batch size is 25 The trace is this:    Thanks !   best,  Roberto  System info (python version, jaxlib version, accelerator, etc.) python 3.9.19 jaxlib 0.4.28  jax.print_environment_info() jax:    0.4.28 jaxlib: 0.4.28 numpy:  1.26.4 python: 3.9.19 (main, May  6 2024, 19:43:03)  [GCC 11.2.0] jax.devices (1 total, 1 local): [cuda(id=0)] process_count: 1 platform: uname_result(system='Linux', node='som5.ific.uv.es', release='4.18.0373.el8.x86_64', version=' CC(Python 3 compatibility issues) SMP Tue Mar 22 15:11:47 UTC 2022', machine='x86_64') $ nvidiasmi Thu Oct  3 17:42:15 2024        ++  ++)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Issues with vmap and GPU memory issues ," Description hello, I am trying to convert pytorch code to JAX related with an algorithm that performs hamiltonian sampling couple to a dynamic nested sampling algorithm. The goal is to vmap GPU parallelize the procedure but I am getting GPU memory saturation. The code is complex and I cinlude the three main functions where everything happens. the method is iterative due to the while loop in ""find_new_sample_batch"". It looks like the shape of some variable grows in each iteration but I am not able to find it.  The vectorisation is done  to paralelize a loop over a batch of points in 2D. In this case the batch size is 25 The trace is this:    Thanks !   best,  Roberto  System info (python version, jaxlib version, accelerator, etc.) python 3.9.19 jaxlib 0.4.28  jax.print_environment_info() jax:    0.4.28 jaxlib: 0.4.28 numpy:  1.26.4 python: 3.9.19 (main, May  6 2024, 19:43:03)  [GCC 11.2.0] jax.devices (1 total, 1 local): [cuda(id=0)] process_count: 1 platform: uname_result(system='Linux', node='som5.ific.uv.es', release='4.18.0373.el8.x86_64', version=' CC(Python 3 compatibility issues) SMP Tue Mar 22 15:11:47 UTC 2022', machine='x86_64') $ nvidiasmi Thu Oct  3 17:42:15 2024        ++  ++",2024-10-03T15:42:36Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/24099
1465,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Simplify and consolidate dot algorithm control in lax.)ï¼Œ å†…å®¹æ˜¯ (Simplify and consolidate dot algorithm control in lax. In https://github.com/jaxml/jax/pull/23574, we added a new `algorithm` parameter to `lax.dot_general` with the goal of giving users explicit control over the specific algorithm used to control dot product accumulation. When using this feature in real use cases, we have found that the API is both too conservative (it required the user to pass the appropriate input types) and too restrictive for common use cases. In this change, I simplify the API to bring it more in line with user expectations, and generalize it to support a broader range of use cases. The core change is to update the dot_general lowering rule to add explicit type casts to the inputs, making sure that they always have the appropriate storage types going into the `DotGeneral` StableHLO op. Before this change, some backends would implicitly cast for some algorithms (e.g. f32 > bf16), but error for others. It seems more user friendly to include automatic casts in all cases where a specific algorithm is requested. Another change in behavior is to (if needed) cast the result of the `DotGeneral` op (which is defined by the algorithm's `accumulation_type`) to match the input types. This means that, )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Simplify and consolidate dot algorithm control in lax.,"Simplify and consolidate dot algorithm control in lax. In https://github.com/jaxml/jax/pull/23574, we added a new `algorithm` parameter to `lax.dot_general` with the goal of giving users explicit control over the specific algorithm used to control dot product accumulation. When using this feature in real use cases, we have found that the API is both too conservative (it required the user to pass the appropriate input types) and too restrictive for common use cases. In this change, I simplify the API to bring it more in line with user expectations, and generalize it to support a broader range of use cases. The core change is to update the dot_general lowering rule to add explicit type casts to the inputs, making sure that they always have the appropriate storage types going into the `DotGeneral` StableHLO op. Before this change, some backends would implicitly cast for some algorithms (e.g. f32 > bf16), but error for others. It seems more user friendly to include automatic casts in all cases where a specific algorithm is requested. Another change in behavior is to (if needed) cast the result of the `DotGeneral` op (which is defined by the algorithm's `accumulation_type`) to match the input types. This means that, ",2024-10-02T19:09:54Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24079
561,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Pallas/MGPU] Allow delaying the release of pipelined buffers)ï¼Œ å†…å®¹æ˜¯ ([Pallas/MGPU] Allow delaying the release of pipelined buffers This is useful so that we don't have to block on the WGMMA immediately after it runs. `delay_release=n` means that the input/output buffers will not be mutated by the system for at least `n` sequential steps following the one when they were kernel arguments.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[Pallas/MGPU] Allow delaying the release of pipelined buffers,[Pallas/MGPU] Allow delaying the release of pipelined buffers This is useful so that we don't have to block on the WGMMA immediately after it runs. `delay_release=n` means that the input/output buffers will not be mutated by the system for at least `n` sequential steps following the one when they were kernel arguments.,2024-10-02T10:06:51Z,,closed,0,0,https://github.com/jax-ml/jax/issues/24068
914,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Edge case: Normal CDF incorrect when called with extremely small integer value if location is specified as integer array)ï¼Œ å†…å®¹æ˜¯ ( Description Computing the CDF of a normal distribution gives the wrong result if the location is specified as an integer array, and CDF is called with the smallest legal int32 value. MWE: The following should all return `0.0`, as the CDF $F(x)$ of a Gaussian should approach 0 for $x \to \infty$  But instead it yields:  Note that `log_cdf` seems to be OK. A workaround for me is to explicitly cast the location vector to float. This is a highly unlikely edgecase, but it did occur for me during a realworld problem (computing mutual information).  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Edge case: Normal CDF incorrect when called with extremely small integer value if location is specified as integer array," Description Computing the CDF of a normal distribution gives the wrong result if the location is specified as an integer array, and CDF is called with the smallest legal int32 value. MWE: The following should all return `0.0`, as the CDF $F(x)$ of a Gaussian should approach 0 for $x \to \infty$  But instead it yields:  Note that `log_cdf` seems to be OK. A workaround for me is to explicitly cast the location vector to float. This is a highly unlikely edgecase, but it did occur for me during a realworld problem (computing mutual information).  System info (python version, jaxlib version, accelerator, etc.) ",2024-10-01T22:45:37Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/24059,Hi  thanks for the report! Can you clarify what `dists.Normal` is here? JAX doesn't have any API with that name.
323,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Pallas TPU] Core dump when comparing two boolean arrays)ï¼Œ å†…å®¹æ˜¯ ( Description  Error:   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[Pallas TPU] Core dump when comparing two boolean arrays," Description  Error:   System info (python version, jaxlib version, accelerator, etc.) ",2024-09-30T20:54:59Z,bug pallas,closed,0,1,https://github.com/jax-ml/jax/issues/24030,It's better to debug after https://github.com/jaxml/jax/pull/24086
316,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Pallas TPU] Core dump when using `jnp.remainder`)ï¼Œ å†…å®¹æ˜¯ ( Description  Error:   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[Pallas TPU] Core dump when using `jnp.remainder`," Description  Error:   System info (python version, jaxlib version, accelerator, etc.) ",2024-09-30T19:24:32Z,bug pallas,closed,0,3,https://github.com/jax-ml/jax/issues/24027,The underlying primitive is `lax.rem_p`,This is directly caused by https://github.com/jaxml/jax/issues/24030 because the implementation of `jnp.remainder()` involves comparing 2 boolean arrays.,Fixing https://github.com/jaxml/jax/issues/24030 is still not sufficient to fix this
1447,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump fonttools from 4.51.0 to 4.54.1)ï¼Œ å†…å®¹æ˜¯ (Bumps fonttools from 4.51.0 to 4.54.1.  Release notes Sourced from fonttools's releases.  4.54.1 What's Changed  [unicodedata] Update to Unicode 16 [subset] Escape \ in doc string  New Contributors  @â€‹markthm made their first contribution in fonttools/fonttools CC(jnp.einsum does not catch shape mismatch when optimize=True)  Full Changelog: https://github.com/fonttools/fonttools/compare/4.54.0...4.54.1 4.54.0  [Docs] Small docs cleanups by @â€‹n8willis (fonttools/fonttools CC(Reimplement argmin/argmax using a single pass variadic reduction.)) [Docs] cleanup code blocks by @â€‹n8willis (fonttools/fonttools CC(improve error message for jnp.pad pad_width array)) [Docs] fix Sphinx builds by @â€‹n8willis (fonttools/fonttools CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)) [merge] Minor fixes to documentation for merge by @â€‹drj11 (fonttools/fonttools CC(Add cummax and cummin)) [subset] Small tweaks to pyftsubset documentation by @â€‹RoelN (fonttools/fonttools CC(fix bug in categorical test, disable 3611 on tpu)) [Tests] Do not require fonttools command to be available by @â€‹behdad (fonttools/fonttools CC(Unexpected overhead when using `grad(pmap)`)) [Tests] subset_test: add failing test to reproduce issue fonttools/fonttools CC(Cleanup: define type lists in tes)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bump fonttools from 4.51.0 to 4.54.1,"Bumps fonttools from 4.51.0 to 4.54.1.  Release notes Sourced from fonttools's releases.  4.54.1 What's Changed  [unicodedata] Update to Unicode 16 [subset] Escape \ in doc string  New Contributors  @â€‹markthm made their first contribution in fonttools/fonttools CC(jnp.einsum does not catch shape mismatch when optimize=True)  Full Changelog: https://github.com/fonttools/fonttools/compare/4.54.0...4.54.1 4.54.0  [Docs] Small docs cleanups by @â€‹n8willis (fonttools/fonttools CC(Reimplement argmin/argmax using a single pass variadic reduction.)) [Docs] cleanup code blocks by @â€‹n8willis (fonttools/fonttools CC(improve error message for jnp.pad pad_width array)) [Docs] fix Sphinx builds by @â€‹n8willis (fonttools/fonttools CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)) [merge] Minor fixes to documentation for merge by @â€‹drj11 (fonttools/fonttools CC(Add cummax and cummin)) [subset] Small tweaks to pyftsubset documentation by @â€‹RoelN (fonttools/fonttools CC(fix bug in categorical test, disable 3611 on tpu)) [Tests] Do not require fonttools command to be available by @â€‹behdad (fonttools/fonttools CC(Unexpected overhead when using `grad(pmap)`)) [Tests] subset_test: add failing test to reproduce issue fonttools/fonttools CC(Cleanup: define type lists in tes",2024-09-30T17:08:20Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/24019,Superseded by CC(Bump fonttools from 4.51.0 to 4.55.0).
1450,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump hypothesis from 6.102.4 to 6.112.2)ï¼Œ å†…å®¹æ˜¯ (Bumps hypothesis from 6.102.4 to 6.112.2.  Commits  83c22d9 Bump hypothesispython version to 6.112.2 and update changelog 3c1350d Merge pull request  CC(rm padded env from masking.py) from HypothesisWorks/createpullrequest/patch 2958a45 Fix for cyclic exception context 75650b9 Merge pull request  CC(Use of memory profiler) from abeakkas/fixassumeexample 7fdad0b Update pinned dependencies 879225d Merge pull request  CC(make custom_jvp handle all tracers gracefully) from HypothesisWorks/createpullrequest/patch bd8e963 Update pinned dependencies 4d079f7 Merge pull request  CC(Add rademacher, maxwell, double_sided_maxwell and weibull_min to jax.random.) from ZacHD/updatedeps 1caa0f2 drop win py39 CI f81af9c Update pinned deps Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by co)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,Bump hypothesis from 6.102.4 to 6.112.2,"Bumps hypothesis from 6.102.4 to 6.112.2.  Commits  83c22d9 Bump hypothesispython version to 6.112.2 and update changelog 3c1350d Merge pull request  CC(rm padded env from masking.py) from HypothesisWorks/createpullrequest/patch 2958a45 Fix for cyclic exception context 75650b9 Merge pull request  CC(Use of memory profiler) from abeakkas/fixassumeexample 7fdad0b Update pinned dependencies 879225d Merge pull request  CC(make custom_jvp handle all tracers gracefully) from HypothesisWorks/createpullrequest/patch bd8e963 Update pinned dependencies 4d079f7 Merge pull request  CC(Add rademacher, maxwell, double_sided_maxwell and weibull_min to jax.random.) from ZacHD/updatedeps 1caa0f2 drop win py39 CI f81af9c Update pinned deps Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by co",2024-09-30T17:06:44Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/24017,Superseded by CC(Bump hypothesis from 6.102.4 to 6.112.4).
398,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Mosaic failed to compile TPU kernel)ï¼Œ å†…å®¹æ˜¯ ( Description I am trying to finetune Gemma 2 on TPU and got the following error:   System info (python version, jaxlib version, accelerator, etc.) Python 3.10.12 jaxlib 0.4.33 TPU v38)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Mosaic failed to compile TPU kernel," Description I am trying to finetune Gemma 2 on TPU and got the following error:   System info (python version, jaxlib version, accelerator, etc.) Python 3.10.12 jaxlib 0.4.33 TPU v38",2024-09-28T09:43:28Z,bug pallas,open,0,3,https://github.com/jax-ml/jax/issues/23989,These are the subsequent errors: ,"While Pallas demonstrates strong potential, certain limitations arise when deploying its operations on TPUv3 hardware. Initial observations suggest potential incompatibilities and accuracy discrepancies that require further investigation. Firstly, some Pallas operations may not yet be fully supported on TPUv3, leading to execution failures. Identifying these specific operations is crucial for either seeking alternative implementations or advocating for broader TPUv3 support. Secondly, the use of bfloat16 precision for QKV computations, while potentially efficient, might contribute to the observed numerical inaccuracies. Exploring the impact of switching to float32 precision is recommended, although it may not completely resolve the discrepancies.","This matmul is not supported on TPU V3 (bf16 X bf16 with float32 accumulation). You should instead try to cast the inputs up the float32 before the matmul. That being said, we're working on improving the error reporting in these cases since it will be much more clear if the error clearly stated that the operation was not supported on a specific hardware generation."
450,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Pallas TPU] Unrelated 'maximum recursion depth exceeded' error when using `.astype(jnp.int64)`)ï¼Œ å†…å®¹æ˜¯ ( Description `int64` is not supported on Pallas TPU. However, the error message is not very helpful:  Error:   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[Pallas TPU] Unrelated 'maximum recursion depth exceeded' error when using `.astype(jnp.int64)`," Description `int64` is not supported on Pallas TPU. However, the error message is not very helpful:  Error:   System info (python version, jaxlib version, accelerator, etc.) ",2024-09-28T07:46:54Z,bug pallas,closed,0,0,https://github.com/jax-ml/jax/issues/23988
377,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jnp.mask_indices: add docs & tests)ï¼Œ å†…å®¹æ˜¯ (This is a strange function to be honest... but somehow we made it this far without ever having tests for it (yikes!) Part of CC(Tracking issue: inline docstrings) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jnp.mask_indices: add docs & tests,This is a strange function to be honest... but somehow we made it this far without ever having tests for it (yikes!) Part of CC(Tracking issue: inline docstrings) ,2024-09-27T20:59:24Z,documentation pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/23984
874,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([jax.distributed] Enable grpc channel compression)ï¼Œ å†…å®¹æ˜¯ (Allows passing an additional boolean argument `use_compression` via `jax.distributed.initialize(...)` that controls whether compression is enabled on the gRPC channels created for each distributed runtime client. Motivation: XLA sends O(mesh) device topologies through its centralized coordination service and we have reason to believe that this becomes a bottleneck at large scale. Compression of the underlying gRPC communication is currently implicitly disabled, and might give us a cheap avenue to scale a bit further with the centralized KV store design. Verified to work via  Corresponding XLA PR: https://github.com/openxla/xla/pull/17704)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[jax.distributed] Enable grpc channel compression,"Allows passing an additional boolean argument `use_compression` via `jax.distributed.initialize(...)` that controls whether compression is enabled on the gRPC channels created for each distributed runtime client. Motivation: XLA sends O(mesh) device topologies through its centralized coordination service and we have reason to believe that this becomes a bottleneck at large scale. Compression of the underlying gRPC communication is currently implicitly disabled, and might give us a cheap avenue to scale a bit further with the centralized KV store design. Verified to work via  Corresponding XLA PR: https://github.com/openxla/xla/pull/17704",2024-09-27T10:01:29Z,pull ready,closed,0,6,https://github.com/jax-ml/jax/issues/23969," Sgtm, made the change. I saw that mypy failed in the previouslyapproved build, not sure why, `precommit run all` locally passes for me.","Hmm, mypy (only on the CI?) complains that it doesn't know about the `use_compression` parameter on `get_distributed_runtime_client` (https://github.com/jaxml/jax/actions/runs/11615448842/job/32353776278?pr=23969step:4:123), but I don't see why  the corresponding `.pyi` in XLA was updated in https://github.com/openxla/xla/pull/17704.",My best guess is that this is an issue with the precommit cache on CI (https://github.com/jaxml/jax/pull/24543). I expect that it's still using a cached version of jaxlib 0.4.34. Let me see if getting https://github.com/jaxml/jax/pull/24543 in fixes the issue!, Let's try again now that https://github.com/jaxml/jax/pull/24543 is in?,Thanks for the ping! Can you rebase onto the `main` branch then I'll trigger the tests? Thanks!,Done!
563,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add .pylintrc file)ï¼Œ å†…å®¹æ˜¯ (The JAX project utilizes nonstandard PEP 8 settings for indentation. To validate Python files, we can leverage a .pylintrc configuration file. For reference, the TensorFlow project has an existing .pylintrc file that outlines its settings. This PR introduces a .pylintrc file for the JAX project, modeled after the configuration used in TensorFlow.  Usage Example: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Add .pylintrc file,"The JAX project utilizes nonstandard PEP 8 settings for indentation. To validate Python files, we can leverage a .pylintrc configuration file. For reference, the TensorFlow project has an existing .pylintrc file that outlines its settings. This PR introduces a .pylintrc file for the JAX project, modeled after the configuration used in TensorFlow.  Usage Example: ",2024-09-26T22:18:53Z,,closed,0,3,https://github.com/jax-ml/jax/issues/23959,"Hi  thanks for the PR. We already have pylint configuration in `pyproject.toml`, and I'd prefer to keep all such configs in a single location","Also, a while ago we moved off pylint and now use `ruff` for formatting. It's set up as a precommit hook, though we only enable a few checks.",Got it. Thank you for the info!
1311,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add support for passing array attributes via ffi_call and an example demonstrating the different attribute APIs supported by the FFI)ï¼Œ å†…å®¹æ˜¯ (The XLA FFI supports a rich interface for specifying ""attributes"", but this interface hasn't been widely used from the JAX side. We discovered that passing array attributes wasn't supported by the current front end, so I added support for that and took advantage of this change to add a more complete example of the e2e usage of attributes. The core JAX change that was needed was to add support for passing `np.array` objects (which aren't hashable!) as keyword arguments to the `ffi_call` primitive. To do this, I wrap any unhashable keyword arguments in the `_HashableByObjectId` helper which creates a hashable object based on the `id`. ""Why not pass these unhashable objects as positional arguments?"" you ask. Well, the problem is that we need access to the concrete value of this array _when lowering_, so we can't have them turning into tracers. I don't expect this to introduce any real caching issues, but if anyone has concerns or other suggestions for a better API, I'd love to hear them!)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add support for passing array attributes via ffi_call and an example demonstrating the different attribute APIs supported by the FFI,"The XLA FFI supports a rich interface for specifying ""attributes"", but this interface hasn't been widely used from the JAX side. We discovered that passing array attributes wasn't supported by the current front end, so I added support for that and took advantage of this change to add a more complete example of the e2e usage of attributes. The core JAX change that was needed was to add support for passing `np.array` objects (which aren't hashable!) as keyword arguments to the `ffi_call` primitive. To do this, I wrap any unhashable keyword arguments in the `_HashableByObjectId` helper which creates a hashable object based on the `id`. ""Why not pass these unhashable objects as positional arguments?"" you ask. Well, the problem is that we need access to the concrete value of this array _when lowering_, so we can't have them turning into tracers. I don't expect this to introduce any real caching issues, but if anyone has concerns or other suggestions for a better API, I'd love to hear them!",2024-09-26T19:22:22Z,pull ready,closed,0,6,https://github.com/jax-ml/jax/issues/23951,"> I don't expect this to introduce any real caching issues, but if anyone has concerns or other suggestions for a better API, I'd love to hear them! Just to clarify, does this defeat the hashing at the JAX level and would any ffi_call using arrays or dicts hence be uncacheable at the JAX level (but presumably cached fine at the MLIR level)? If I recall correctly, there's a caching mechanism for jitted functions both within JAX itself (on jaxprs?) and another on the emitted StableHLO.",": Great question  thanks for bringing this up! This is more or less the question I was asking, but I think the specific points you bring up are not a concern here. Let me see if I can lay out my thinking here!  JIT cache This change doesn't affect which kinds of arguments can be treated as static arguments to `jax.jit`, and the cache is based on the types of the input arguments, not the Jaxpr. This means that this change doesn't affect cache hits when using `jax.jit`. For example, in the example included with this PR, we call FFI call as follows:  and it's totally fine to `jit` this function:  and this will cache as expected:  I note that it's still not possible to pass the `np.array` as a static argument to JIT, and that's disallowed for good reason. If that's the behavior that a user needs: they're going to have to find another way, but that has nothing to do with this change!  Persistent cache Another cache that we might be concerned about is the persistent compilation cache. This one also isn't a problem because that cache is keyed on the lowered HLO, which doesn't include any references to the object IDs. For example, if we lower the example above:  the HLO is:  where the array has already been materialized as:  Now, things get a little more complicated if our array has an inexact type (e.g. `float32`). In that case, I could imagine the we could end up with unexpected cache misses depending on numerics, but this is also true for any floating point scalar inputs, which are hashable and unaffected by this change.  XLA caches There may be questions to ask about how XLA handles caching with array or dict attributes in the HLO, but that's separate from this PR, which just exposes an API for accessing an existing XLA feature. The JAXspecific hashing hacks are stripped before we ever get to XLA.  Other caches in JAX core? LRU caches are used throughout JAX core, and I could certainly imagine this complicating logic _somewhere_, but I haven't thought of anywhere where it would!  Stepping back I think it's useful to step back and comment a bit more on the motivation for this change. The `ffi_call` function is really just a simple shim to let users define a lowering rule without having to think about MLIR. (The `ffi_call` primitive doesn't really do anything else besides lowering!) If we weren't using `ffi_call`, we could write our own primitive that has array attributes roughly as follows:  and everything works! The problem comes from the fact that `ffi_call` can't really have any runtime logic in the lowering rule, so we need to have a way to pass the concrete array through from the binding to the lowering. It's important to make sure we can support such a use case (it was a feature request!), and I think this is a reasonable approach!"," Thanks for the elaborate answer, makes sense that this can't affect the caching at the JIT boundary! Do you have a sense of why the values actually need to be hashable? Ultimately `HashableById` makes equality among two wrapped values inexact by erring on the side of ""not equal"". That's a safe choice in the context of caching, I just couldn't easily gather how the hash and equality come into play here.","> Do you have a sense of why the values actually need to be hashable? You know... this is a great question! And in trying to answer it, I realized that there was a _much_ simpler approach here, which doesn't have these same caveats. I should have figured this out earlier! The hashability requirement was coming from the fact that I was using `dispatch.apply_primitive` as the `impl` rule for the `ffi_call_p` primitive, and that function caches the jitted call to the primitive. Having worked that out, I've updated the `ffi_call_p` `impl` rule to use a version of the logic from `dispatch.apply_primitive`, but removing that caching. This will have performance implications if the same `ffi_call` is executed several times in eager mode (because the primitive will be relowered each time), but this doesn't affect the behavior of `ffi_call` under JIT. I'm much happier with this approach! Thanks  for asking the straightforward question that I should have asked myself in the first place ğŸ¤£ ","Great, thanks for digging into this! If that's the only path in which we rely on hashing the attribute, then we're just trading space (extra entries in the `xla_primitive_callable` cache) for time. Based on that I think your original proposal using `_HashableByObjectId` is good as well. I have no idea how much time caching `xla_primitive_callable` actually saves, but if we think it matters, we might want to err on the side of not regressing existing (cached) `ffi_call`s?","It's a bit nasty, but  convinced me that it's probably better to just hash by value (rather than id), since these arguments are small (they're going to end up in the HLO anyways). We special case to handle `np.array` and `dict` arguments, and we could add other cases in the future if necessary."
539,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(checkify doesn't work with shard_map output array)ï¼Œ å†…å®¹æ˜¯ ( Description To clarify: I don't mean using `checkify.check` inside a `shard_map` function, but using the output of a `shard_map` function and `checkify.check` in the same function and then applying `checkify.checkify` to it. Example:  output:   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,checkify doesn't work with shard_map output array," Description To clarify: I don't mean using `checkify.check` inside a `shard_map` function, but using the output of a `shard_map` function and `checkify.check` in the same function and then applying `checkify.checkify` to it. Example:  output:   System info (python version, jaxlib version, accelerator, etc.) ",2024-09-26T14:58:28Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/23946,"If you use JAX from HEAD, then it should work!","Indeed, using jax(lib)0.4.34.dev20240926 this seems to work as expected","Sweet, closing this issue. Feel free to reopen! (we should release 0.4.34 soon enough)"
558,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([pallas] Add support for the new algorithm option to dot)ï¼Œ å†…å®¹æ˜¯ (`jax.lax.dot_general` has a new `algorithm` option for precisely specifying the precision, which should be supported in Pallas. Also support the simulated precision modes on GPU, such as `BF16_BF16_F32_x6` and `TF32_TF32_F32_x3` (already supported by `triton.language.dot` via the `input_precision=""tf32x3""` option). , ,  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[pallas] Add support for the new algorithm option to dot,"`jax.lax.dot_general` has a new `algorithm` option for precisely specifying the precision, which should be supported in Pallas. Also support the simulated precision modes on GPU, such as `BF16_BF16_F32_x6` and `TF32_TF32_F32_x3` (already supported by `triton.language.dot` via the `input_precision=""tf32x3""` option). , ,  ",2024-09-25T21:29:09Z,enhancement pallas,open,0,1,https://github.com/jax-ml/jax/issues/23926," â€” Is this a duplicate of https://github.com/jaxml/jax/issues/24209? If so, let's close one and keep the other open for tracking."
320,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fixed `pl.debug_print`ing of scalar fragmented arrays under Mosaic GPU)ï¼Œ å†…å®¹æ˜¯ (Fixed `pl.debug_print`ing of scalar fragmented arrays under Mosaic GPU)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Fixed `pl.debug_print`ing of scalar fragmented arrays under Mosaic GPU,Fixed `pl.debug_print`ing of scalar fragmented arrays under Mosaic GPU,2024-09-25T15:23:34Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23912
1014,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`stop_gradient` cannot safeguard `io_callback` against `jax.grad` transformation)ï¼Œ å†…å®¹æ˜¯ (Not sure if this is intended to not work, but I have been trying to ""safeguard"" `jax.experimental.io_callback` with `jax.lax.stop_gradient` so that it could play nicely with `jax.grad`. However, it turns out it doesn't work... Here is an example to illustrate the problem:  This can be troublesome especially if I want to save some intermediate results midway inside the training loop (And I am sure this part does not need autodiff!). I guess `jax.debug.callback` might work but the execution is not guaranteed according to the doc.  Otherwise, is there a way to have ""stop_gradient""style `io_callback` that is guaranteed to execute impure code and works with `jax.grad` transformation? It would be more versatile and compatible with all JAX transforms.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,`stop_gradient` cannot safeguard `io_callback` against `jax.grad` transformation,"Not sure if this is intended to not work, but I have been trying to ""safeguard"" `jax.experimental.io_callback` with `jax.lax.stop_gradient` so that it could play nicely with `jax.grad`. However, it turns out it doesn't work... Here is an example to illustrate the problem:  This can be troublesome especially if I want to save some intermediate results midway inside the training loop (And I am sure this part does not need autodiff!). I guess `jax.debug.callback` might work but the execution is not guaranteed according to the doc.  Otherwise, is there a way to have ""stop_gradient""style `io_callback` that is guaranteed to execute impure code and works with `jax.grad` transformation? It would be more versatile and compatible with all JAX transforms.",2024-09-25T15:18:08Z,enhancement,open,0,0,https://github.com/jax-ml/jax/issues/23911
368,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fixed `mgpu.FragmentedArray.reduce_sum` for integer types)ï¼Œ å†…å®¹æ˜¯ (Fixed `mgpu.FragmentedArray.reduce_sum` for integer types The implementation previously assumed the type is floating and used addf.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Fixed `mgpu.FragmentedArray.reduce_sum` for integer types,Fixed `mgpu.FragmentedArray.reduce_sum` for integer types The implementation previously assumed the type is floating and used addf.,2024-09-25T13:51:42Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23905
1354,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add `override_type_lowerings` to `LoweringParameters`)ï¼Œ å†…å®¹æ˜¯ (The `LoweringParameters` dataclass already allows for specifying overriding lowering rules for JAX primitives. This is very important and useful for third party targets implementing PJRT. Another important aspect of code generation are the types generated by abstract values. Third parties can already register their own primitives with custom rules, but the dictionaries `mlir.ir_type_handlers` and `core.raise_to_shapped_mappings` needs to be set by third parties directly. This is not ideal, because this is a global data structure defined in JAX. A better interface would be one analogous to the `override_lowering_rules` in the `LoweringParameters` data class. However, `LoweringParameters` may be extended to have a dictionary (or tuple, just like `override_lowering_rules`) that  maps types (abstract values) to their MLIR type counter part. This would allow for custom type lowerings. I would be happy to implement this if you find it useful. As a side note, is there a reason why `override_lowering_rules` is a tuple and not a dictionary? I am also volunteering to make this change if you find it useful. Thanks!)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add `override_type_lowerings` to `LoweringParameters`,"The `LoweringParameters` dataclass already allows for specifying overriding lowering rules for JAX primitives. This is very important and useful for third party targets implementing PJRT. Another important aspect of code generation are the types generated by abstract values. Third parties can already register their own primitives with custom rules, but the dictionaries `mlir.ir_type_handlers` and `core.raise_to_shapped_mappings` needs to be set by third parties directly. This is not ideal, because this is a global data structure defined in JAX. A better interface would be one analogous to the `override_lowering_rules` in the `LoweringParameters` data class. However, `LoweringParameters` may be extended to have a dictionary (or tuple, just like `override_lowering_rules`) that  maps types (abstract values) to their MLIR type counter part. This would allow for custom type lowerings. I would be happy to implement this if you find it useful. As a side note, is there a reason why `override_lowering_rules` is a tuple and not a dictionary? I am also volunteering to make this change if you find it useful. Thanks!",2024-09-25T13:25:56Z,enhancement,open,0,0,https://github.com/jax-ml/jax/issues/23904
546,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(JAX dct is not accurate compared to scipy)ï¼Œ å†…å®¹æ˜¯ (I have also found Jax's dct to be inaccurate compared to scipy: https://github.com/PlasmaControl/DESC/pull/1119/filesr1774234361, >= ~8 orders of magnitude even on 64 bit floating point mode. To see this just change  to   and observe  _Originally posted by  in https://github.com/jaxml/jax/issues/23827issuecomment2372595040_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,JAX dct is not accurate compared to scipy,"I have also found Jax's dct to be inaccurate compared to scipy: https://github.com/PlasmaControl/DESC/pull/1119/filesr1774234361, >= ~8 orders of magnitude even on 64 bit floating point mode. To see this just change  to   and observe  _Originally posted by  in https://github.com/jaxml/jax/issues/23827issuecomment2372595040_",2024-09-25T00:09:08Z,,closed,0,5,https://github.com/jax-ml/jax/issues/23895,"Hi, thanks for the report! Could you put together a minimal reproducible example demonstrating the issue? That will prevent us from having to guess at what the problematic inputs might be. Thanks!",Here is the the more mwe from https://github.com/PlasmaControl/DESC/pull/1119discussion_r1774234361. You may run with `pytest k test_dct` or you can provide the inputs to the function manually. ,"Since `dct` calls `fft`, I suspect this may be a duplicate of https://github.com/jaxml/jax/issues/23827. Assigning to , who is debugging that issue.",Yes this issue was raised from that thread. However I made a new issue because this test fails even not using gpu. For me it is 8 orders less accurate on cpu. Just mentioning in case relevant,"This issue was actually unrelated to https://github.com/jaxml/jax/issues/23827, but the fix was straightforward: https://github.com/jaxml/jax/pull/23917"
307,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DOC: Improved documentation for jax.numpy.isscalar)ï¼Œ å†…å®¹æ˜¯ (Part of https://github.com/jaxml/jax/issues/21461  Based on discussion: 19959)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,DOC: Improved documentation for jax.numpy.isscalar,Part of https://github.com/jaxml/jax/issues/21461  Based on discussion: 19959,2024-09-24T22:39:58Z,,closed,0,2,https://github.com/jax-ml/jax/issues/23890,Bullet points rewritten and one example added based on the comments.,Replaced by CC(Simplify definition of jnp.isscalar)
372,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Pallas TPU] Improve error message when trying to store a scalar to VMEM)ï¼Œ å†…å®¹æ˜¯ ([Pallas TPU] Improve error message when trying to store a scalar to VMEM Fixes https://github.com/jaxml/jax/issues/23884)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[Pallas TPU] Improve error message when trying to store a scalar to VMEM,[Pallas TPU] Improve error message when trying to store a scalar to VMEM Fixes https://github.com/jaxml/jax/issues/23884,2024-09-24T19:59:40Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23885
1476,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Deprecate the `vectorized` argument to pure_callback and ffi_call)ï¼Œ å†…å®¹æ˜¯ (Currently, the primitives backing `pure_callback` and (by extension) `ffi_call` provide a default batching rule that is controlled by the parameter `vectorized` to these two functions. This batching rule (allegedly) implements the following behavior: * For `vectorized=True`, vmapping results in a call to the callback directly with the batched inputs after moving the batch axis to the front. This assumes that the callback function directly handles vectorized inputs and (roughly) `callback(x_batch) == stack([callback(x) for x in x_batch])`. * When `vectorized=False`, the vmap produces a sequential scan with the callback in the body, which could have significant performance implications. While these don't seem like unreasonable defaults, the behavior when `vectorized=False` could result in unexpectedly poor performance for users who aren't familiar with this behavior. More importantly, the `vectorized=True` semantics are confusingâ€”and I would argue wrongâ€”when the function takes more than one input. For example, consider the following function:  We can call this function under `vmap` as follows:  This prints `(3,)` and `()` for the shapes, and the result has shape `(3,)`, as we expect. However, things get a little u)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Deprecate the `vectorized` argument to pure_callback and ffi_call,"Currently, the primitives backing `pure_callback` and (by extension) `ffi_call` provide a default batching rule that is controlled by the parameter `vectorized` to these two functions. This batching rule (allegedly) implements the following behavior: * For `vectorized=True`, vmapping results in a call to the callback directly with the batched inputs after moving the batch axis to the front. This assumes that the callback function directly handles vectorized inputs and (roughly) `callback(x_batch) == stack([callback(x) for x in x_batch])`. * When `vectorized=False`, the vmap produces a sequential scan with the callback in the body, which could have significant performance implications. While these don't seem like unreasonable defaults, the behavior when `vectorized=False` could result in unexpectedly poor performance for users who aren't familiar with this behavior. More importantly, the `vectorized=True` semantics are confusingâ€”and I would argue wrongâ€”when the function takes more than one input. For example, consider the following function:  We can call this function under `vmap` as follows:  This prints `(3,)` and `()` for the shapes, and the result has shape `(3,)`, as we expect. However, things get a little u",2024-09-24T18:09:09Z,pull ready,closed,0,6,https://github.com/jax-ml/jax/issues/23881,"Just a heads up, it seems this did not make it into 0.4.34 despite being mentioned in the changelog. Edit: idk what happened, going by tags it's in, but doesn't appear to be in the wheel on PyPI: ","Thanks for the heads up! You're absolutely right, although the GitHub tag also doesn't seem to include it either. I'll update the changelog accordingly!","What does the ""rank"" in the name `""broadcast_fullrank""` refer to? It seems that the array ranks, as in `np.ndim`, are the same in either broadcast option. What would you think of renaming the options here to: * `""expand""` for current ""broadcast"" with singleton dimensions, and * `""broadcast""` for current ""broadcast_fullrank"" with fully broadcast dimensions? I'm trying to suggest `np.expand_dims` in the former case, since that inserts singleton dimensions, and to suggest `np.broadcast` in the latter case, since that behaves as a material common broadcast on all of its arguments.","Good points  naming is hard! I like your suggestions, and definitely don't feel too attached to the names from this PR. Maybe `""broadcast""` by itself is confusing, since people have different intuitions for what broadcasting means. I think that the analogy to `expand_dims` is a good one, but `np.broadcast` doesn't seem like quite the right comparison here. Maybe `np.broadcast_arrays` is closer to the semantics we mean. What about `""expand_dims""` and `""broadcast_arrays""`?","Expanding to `""expand_dims""` sounds good to me. And `""broadcast_arrays""`, or something like `""broadcast_all""`, sounds good to me.",",  â€” Do either of you have opinions about this naming question?"
395,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Pallas] Update export compatibility tests)ï¼Œ å†…å®¹æ˜¯ ([Pallas] Update export compatibility tests The old test was generated before our IR was really stable, which has started to cause problems when trying to test with Trillium.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[Pallas] Update export compatibility tests,"[Pallas] Update export compatibility tests The old test was generated before our IR was really stable, which has started to cause problems when trying to test with Trillium.",2024-09-24T15:56:55Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23876
395,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Expose some APIs for querying trace state. This will let us move users away from)ï¼Œ å†…å®¹æ˜¯ (Expose some APIs for querying trace state. This will let us move users away from depending on our internals. Prep work for ""stackless"".)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Expose some APIs for querying trace state. This will let us move users away from,"Expose some APIs for querying trace state. This will let us move users away from depending on our internals. Prep work for ""stackless"".",2024-09-24T15:31:44Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23875
1464,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Unable to use residual offloading with scan and remat)ï¼Œ å†…å®¹æ˜¯ ( Description Hi guys, I'm very excited with recent activations offloading mechanism introduced in JAX/XLA:GPU but I'm unable to make it work with the scan. My setup is the following  I'm training classic transformer with transformer block scanned over inputs ""number of layers"" times. I'm also using rematerialization to reduce memory footprint of the model. I basically wrap apply_block function with the jax.remat with ""nothing_saveable"" policy and then scan this block over inputs to achieve desired behavior  the only activations being saved during forward pass in my case is the residual stream (embeddings) in between scanned block.  With the recent introduction of the ""save_and_offload_only_these_names"" policy, I thought that it would be enough to mark the output of the scanned block with `jax.ad_checkpoint(x,  ""output"")` and then specify `names_which_can_be_offloaded=[""output""]`, but it didn't work.  I've implemented repro to showcase what is going on:    First of all, I wanted to ensure that offloading is working in the first place. With   I'm getting following results: `Total size device = 20.26562874764204 GB, host = 0.0 GB` Quite reasonable value. then, I wanted to check how much would it cost to save ""h"" on)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Unable to use residual offloading with scan and remat," Description Hi guys, I'm very excited with recent activations offloading mechanism introduced in JAX/XLA:GPU but I'm unable to make it work with the scan. My setup is the following  I'm training classic transformer with transformer block scanned over inputs ""number of layers"" times. I'm also using rematerialization to reduce memory footprint of the model. I basically wrap apply_block function with the jax.remat with ""nothing_saveable"" policy and then scan this block over inputs to achieve desired behavior  the only activations being saved during forward pass in my case is the residual stream (embeddings) in between scanned block.  With the recent introduction of the ""save_and_offload_only_these_names"" policy, I thought that it would be enough to mark the output of the scanned block with `jax.ad_checkpoint(x,  ""output"")` and then specify `names_which_can_be_offloaded=[""output""]`, but it didn't work.  I've implemented repro to showcase what is going on:    First of all, I wanted to ensure that offloading is working in the first place. With   I'm getting following results: `Total size device = 20.26562874764204 GB, host = 0.0 GB` Quite reasonable value. then, I wanted to check how much would it cost to save ""h"" on",2024-09-24T10:35:30Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/23869,"I've also tried following implementation, inspired by  https://github.com/jaxml/jax/issues/23614issuecomment2350773816 with wrapping entire apply_module with flax's custom_vjp, but it doesn't work properly.    `Total size device = 21.00781624764204 GB, host = 0.5 GB`. Here's part of the trace   As you can see, activations are indeed being offloaded during forward pass, but during forward pass they are not loaded back to devices  looks like these offloaded activations are immediately dropped on CPU and GPU activations are saved instead. That's why host memory is only 0.5GB  it is only reserved for activations of one layer. **Also I've noticed that this approach produces wrong loss & grad calculations, but if I'm commenting out all of the ""jax.device_put"" transfers, everything works as expected again.** ","> Now we sure that offloading is indeed working properly, I've tried to offload ""residual"" tensor (output of the scanned block) Do you have a minimal repro? Or maybe you can dump the jaxpr? `jit(f).trace(*args).jaxpr`","> Do you have a minimal repro? Or maybe you can dump the jaxpr? jit(f).trace(*args).jaxpr Hi, wouldn't provided repro script work for you? It's hidden behind ""Details"" spoiler in the topicstarter message. Also, for the second case with custom_vjp, here is scripts and jaxpr: Full code   jaxpr  ",And for the first case  using only save_and_offload_only_these_names with   Here is code and jaxpr Full code   jaxpr  ,"Okay, I looked at the stableHLO produced by `names_which_can_be_offloaded=[""residual""],` and there are no transfers to pinned_host and loads to device because what you are trying to offload is not a ""residual"".  You can do `lowered.as_text()` and look for `annotate_device_placement` calls in both `residual` and `hidden` stableHLO or `device_put` in the jaxpr for both (looking for device_put is better). ","I've actually figured out the problem  to make it work with scan, I needed to checkpoint inputs to the function and not output. This way I see Total size device = 5.2968789935112 GB, host = 16.0 GB  And while it really greatly reduces memory usage, I don't think it scales properly (when quadrupling batch size, I would expect only slight increase in GPU memory usage while I'm getting OOM). I'll gather more proofs/xla dumps and create another issue.  Final thought regarding this problem  I would be great if solution with scan would be somewhere in documentation :)","> I would be great if solution with scan would be somewhere in documentation Yes, that is something I plan to add! > when quadrupling batch size, I would expect only slight increase in GPU memory usage while I'm getting OOM I would suggest trying this out on a TPU and see if it scales up! The GPU support is lacking by a bit but it is catching up.  "
296,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Added support for % and `select` to `mgpu.FragmentedArray`)ï¼Œ å†…å®¹æ˜¯ (Added support for % and `select` to `mgpu.FragmentedArray`)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Added support for % and `select` to `mgpu.FragmentedArray`,Added support for % and `select` to `mgpu.FragmentedArray`,2024-09-24T10:18:41Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23868
1447,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump fonttools from 4.51.0 to 4.54.0)ï¼Œ å†…å®¹æ˜¯ (Bumps fonttools from 4.51.0 to 4.54.0.  Release notes Sourced from fonttools's releases.  4.54.0  [Docs] Small docs cleanups by @â€‹n8willis (fonttools/fonttools CC(Reimplement argmin/argmax using a single pass variadic reduction.)) [Docs] cleanup code blocks by @â€‹n8willis (fonttools/fonttools CC(improve error message for jnp.pad pad_width array)) [Docs] fix Sphinx builds by @â€‹n8willis (fonttools/fonttools CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)) [merge] Minor fixes to documentation for merge by @â€‹drj11 (fonttools/fonttools CC(Add cummax and cummin)) [subset] Small tweaks to pyftsubset documentation by @â€‹RoelN (fonttools/fonttools CC(fix bug in categorical test, disable 3611 on tpu)) [Tests] Do not require fonttools command to be available by @â€‹behdad (fonttools/fonttools CC(Unexpected overhead when using `grad(pmap)`)) [Tests] subset_test: add failing test to reproduce issue fonttools/fonttools CC(Cleanup: define type lists in test_util & use in several test files.) by @â€‹anthrotype (fonttools/fonttools CC(How to select the jax  release version)) [ttLib] NameRecordVisitor: include whole sequence of character variants' UI labels, not just the first by @â€‹anthrotype (fonttools/fonttools CC(public interface for partial evaluation )) [varLib.avar)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bump fonttools from 4.51.0 to 4.54.0,"Bumps fonttools from 4.51.0 to 4.54.0.  Release notes Sourced from fonttools's releases.  4.54.0  [Docs] Small docs cleanups by @â€‹n8willis (fonttools/fonttools CC(Reimplement argmin/argmax using a single pass variadic reduction.)) [Docs] cleanup code blocks by @â€‹n8willis (fonttools/fonttools CC(improve error message for jnp.pad pad_width array)) [Docs] fix Sphinx builds by @â€‹n8willis (fonttools/fonttools CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)) [merge] Minor fixes to documentation for merge by @â€‹drj11 (fonttools/fonttools CC(Add cummax and cummin)) [subset] Small tweaks to pyftsubset documentation by @â€‹RoelN (fonttools/fonttools CC(fix bug in categorical test, disable 3611 on tpu)) [Tests] Do not require fonttools command to be available by @â€‹behdad (fonttools/fonttools CC(Unexpected overhead when using `grad(pmap)`)) [Tests] subset_test: add failing test to reproduce issue fonttools/fonttools CC(Cleanup: define type lists in test_util & use in several test files.) by @â€‹anthrotype (fonttools/fonttools CC(How to select the jax  release version)) [ttLib] NameRecordVisitor: include whole sequence of character variants' UI labels, not just the first by @â€‹anthrotype (fonttools/fonttools CC(public interface for partial evaluation )) [varLib.avar",2024-09-23T17:49:25Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23848,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
1442,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump pytest from 8.2.0 to 8.3.3)ï¼Œ å†…å®¹æ˜¯ (Bumps pytest from 8.2.0 to 8.3.3.  Release notes Sourced from pytest's releases.  8.3.3 pytest 8.3.3 (20240909) Bug fixes    CC(Flip default value of jax_unique_mhlo_module_names to False.): Avoid calling  (and other instance descriptors) during fixture discovery  by asottile{.interpretedtext role=&quot;user&quot;}    CC(Add PYTORCH_SAME padding to JAX.): Fixed the issue of not displaying assertion failure differences when using the parameter importmode=importlib in pytest&gt;=8.1.    CC(Type mismatches for jnp.concatenate tests): Fixed a regression where type change in [ExceptionInfo.errisinstance]{.titleref} caused [mypy]{.titleref} to fail.    CC([jax2tf] Add new test for jax2tf sharding): Fixed typing compatibility with Python 3.9 or less  replaced [typing.Self]{.titleref} with [typing_extensions.Self]{.titleref}  by Avasam{.interpretedtext role=&quot;user&quot;}    CC(Support MANUAL collectives in toplevel xmaps): Fixed an issue with backslashes being incorrectly converted in nodeid paths on Windows, ensuring consistent path handling across environments.    CC(Update code example in custom VJP documentation.): Fixed bug where the verbosity levels where not being respected when printing the &quot;msg&quot; p)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bump pytest from 8.2.0 to 8.3.3,"Bumps pytest from 8.2.0 to 8.3.3.  Release notes Sourced from pytest's releases.  8.3.3 pytest 8.3.3 (20240909) Bug fixes    CC(Flip default value of jax_unique_mhlo_module_names to False.): Avoid calling  (and other instance descriptors) during fixture discovery  by asottile{.interpretedtext role=&quot;user&quot;}    CC(Add PYTORCH_SAME padding to JAX.): Fixed the issue of not displaying assertion failure differences when using the parameter importmode=importlib in pytest&gt;=8.1.    CC(Type mismatches for jnp.concatenate tests): Fixed a regression where type change in [ExceptionInfo.errisinstance]{.titleref} caused [mypy]{.titleref} to fail.    CC([jax2tf] Add new test for jax2tf sharding): Fixed typing compatibility with Python 3.9 or less  replaced [typing.Self]{.titleref} with [typing_extensions.Self]{.titleref}  by Avasam{.interpretedtext role=&quot;user&quot;}    CC(Support MANUAL collectives in toplevel xmaps): Fixed an issue with backslashes being incorrectly converted in nodeid paths on Windows, ensuring consistent path handling across environments.    CC(Update code example in custom VJP documentation.): Fixed bug where the verbosity levels where not being respected when printing the &quot;msg&quot; p",2024-09-23T17:49:10Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23847,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
1450,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump hypothesis from 6.102.4 to 6.112.1)ï¼Œ å†…å®¹æ˜¯ (Bumps hypothesis from 6.102.4 to 6.112.1.  Commits  a64b71c Bump hypothesispython version to 6.112.1 and update changelog 7d7724c Merge pull request  CC(fix jaxpr util test in enable_x64 mode) from tybug/fixnumpynightly 6dfe346 remove nowincorrect assertion 8f4dcab Merge pull request  CC(Simplify the interface for host_callback.id_tap) from tybug/strengthenirtests d7ec1e3 bump uploadartifact 4a99ad0 more explicit generator eeaa831 rc2 is out probably? ccd6849 ensure coverage b49d4dc consolidate test_forced_* methods 927ad08 improve ir test strategies Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Bump hypothesis from 6.102.4 to 6.112.1,"Bumps hypothesis from 6.102.4 to 6.112.1.  Commits  a64b71c Bump hypothesispython version to 6.112.1 and update changelog 7d7724c Merge pull request  CC(fix jaxpr util test in enable_x64 mode) from tybug/fixnumpynightly 6dfe346 remove nowincorrect assertion 8f4dcab Merge pull request  CC(Simplify the interface for host_callback.id_tap) from tybug/strengthenirtests d7ec1e3 bump uploadartifact 4a99ad0 more explicit generator eeaa831 rc2 is out probably? ccd6849 ensure coverage b49d4dc consolidate test_forced_* methods 927ad08 improve ir test strategies Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your",2024-09-23T17:49:01Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23846,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
574,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Hide JAX's internal tracing state and update libraries to use limited trace-state-querying APIs as needed. This is prep work for stackless which will change those internals while preserving the API.)ï¼Œ å†…å®¹æ˜¯ (Hide JAX's internal tracing state and update libraries to use limited tracestatequerying APIs as needed. This is prep work for stackless which will change those internals while preserving the API.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Hide JAX's internal tracing state and update libraries to use limited trace-state-querying APIs as needed. This is prep work for stackless which will change those internals while preserving the API.,Hide JAX's internal tracing state and update libraries to use limited tracestatequerying APIs as needed. This is prep work for stackless which will change those internals while preserving the API.,2024-09-23T17:24:41Z,,closed,0,1,https://github.com/jax-ml/jax/issues/23843,Closing Copybara created PR due to inactivity
456,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Unable to load any libcudnn_engines_precompiled.so using jax.nn.dot_product_attention)ï¼Œ å†…å®¹æ˜¯ ( Description I am calling `jax.nn.dot_product_attention` with the following line:  However, this throws the following error:   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,Unable to load any libcudnn_engines_precompiled.so using jax.nn.dot_product_attention," Description I am calling `jax.nn.dot_product_attention` with the following line:  However, this throws the following error:   System info (python version, jaxlib version, accelerator, etc.) ",2024-09-23T02:49:56Z,bug,open,0,8,https://github.com/jax-ml/jax/issues/23833,"Seems that the head dim of K doesn't have stride 1 which is required by cuDNN, could you provide a repo with how QKV is constructed?",QKV is constructed as follows: ,Thanks! let me try reproducing this., have there been any updates?,"Sorry, it fell through the cracks. I'm not able to reproduce with this example:  I'm using cuDNN 9.4 + CUDA 12.6 on Hopper with XLA and JAX at head.",I  use the cudnn 9.5 + CUDA12.6. It may be due to version incompatibility.,I also tried cudnn 9.5 + CUDA 12.6. I tried both on A100 and H100. Still can't reproduce this. Is there anything specific about your testing environment? Maybe using some specific commits from old JAX/XLA?,It seems that this issue is caused by our outdated `CentOS` version and the absence of precompiled libraries. This error no longer happens when using the `ubuntu+cuda12.5` image.
1386,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Introduce a pl.actual_size(ref_idx) utility to make masking in pallas easier.)ï¼Œ å†…å®¹æ˜¯ (Introduce a pl.actual_size(ref_idx) utility to make masking in pallas easier. Today, in pallas, if you have a kernel grid where the shape of a blockspec does not line up with an input evenly (aka shape_dim % block_dim != 0)  you will get a shape filled with padding. Users mask this via knowledge of their inputs, but this has two main problems: 1) The mask is always run and computed 2) The kernel has to access data outside of itself, more or less, to build the mask, either via closing over locals or extending the kernel for new inputs. This CL introduces a util, pl.actual_size, to help with getting the real size of a given ref within a given tile. Ex: A reduce sum kernel launched with input x=jnp.ones((9, 8, 128)) and a grid of (2, 1, 1) over blockspec of (8, 8, 128) will have an x_ref of (1, 8, 128) real data and the rest padding in the 2nd tile.  This is the first step in two useful directions: 1) A pl.mask_to_actual_size(ref, val) util that runs conditionally (only when we are on a tile where we expect padding, saving compute) 2) Extending pl.actual_size and pl.mask_to_actual_size to handle jumble/ragged types)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Introduce a pl.actual_size(ref_idx) utility to make masking in pallas easier.,"Introduce a pl.actual_size(ref_idx) utility to make masking in pallas easier. Today, in pallas, if you have a kernel grid where the shape of a blockspec does not line up with an input evenly (aka shape_dim % block_dim != 0)  you will get a shape filled with padding. Users mask this via knowledge of their inputs, but this has two main problems: 1) The mask is always run and computed 2) The kernel has to access data outside of itself, more or less, to build the mask, either via closing over locals or extending the kernel for new inputs. This CL introduces a util, pl.actual_size, to help with getting the real size of a given ref within a given tile. Ex: A reduce sum kernel launched with input x=jnp.ones((9, 8, 128)) and a grid of (2, 1, 1) over blockspec of (8, 8, 128) will have an x_ref of (1, 8, 128) real data and the rest padding in the 2nd tile.  This is the first step in two useful directions: 1) A pl.mask_to_actual_size(ref, val) util that runs conditionally (only when we are on a tile where we expect padding, saving compute) 2) Extending pl.actual_size and pl.mask_to_actual_size to handle jumble/ragged types",2024-09-22T21:02:28Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23832
872,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bug- False ""UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32 )ï¼Œ å†…å®¹æ˜¯ ( Description  I explicitly set `jnp.float32` and did not ask for `int` anywhere, but received the warning  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.30 jaxlib: 0.4.30 numpy:  1.26.4 python: 3.9.19 (main, May  6 2024, 19:43:03)  [GCC 11.2.0] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='user', release='6.8.045generic', version=' CC(Feature request: export TF ops)Ubuntu SMP PREEMPT_DYNAMIC Fri Aug 30 12:02:04 UTC 2024', machine='x86_64'))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"Bug- False ""UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32 "," Description  I explicitly set `jnp.float32` and did not ask for `int` anywhere, but received the warning  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.30 jaxlib: 0.4.30 numpy:  1.26.4 python: 3.9.19 (main, May  6 2024, 19:43:03)  [GCC 11.2.0] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', node='user', release='6.8.045generic', version=' CC(Feature request: export TF ops)Ubuntu SMP PREEMPT_DYNAMIC Fri Aug 30 12:02:04 UTC 2024', machine='x86_64')",2024-09-22T16:28:17Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/23829,"Thanks for the report! This warning is arising becuase `tensorflow_probability` passes an `int64` to JAX. You can see this by reexecuting your code after running the following:  This will turn the warning into an error with a full traceback, which helps locate the place where this problematic dtype is passed: https://github.com/tensorflow/probability/blob/c6c86e788516a9b16958460528867735a721ef40/tensorflow_probability/python/internal/backend/numpy/random_generators.pyL106 If this warning is a problem to you, you could think about reporting it to the `tensorflow_probability` maintainers, but there's nothing JAX can do to fix this short of removing the warning entirely."
943,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Implementation of PowerLaw distribution)ï¼Œ å†…å®¹æ˜¯ (Hi, [SciPy][SciPy] has a very specific implementation of [Powerlaw][scipy_powerlaw]. It is over a unit interval and only supports positive indexes. Many applications require it to be over a broader range of intervals (truncated on both sides) and demand the support of negative indexes. I would like to request this generic Powerlaw implementation in JAX. NumPyro has one very generic [implementation][numpyro_powerlaw] from where you can reference all the mathematical derivations. [SciPy]: https://docs.scipy.org/doc/scipy/index.html [scipy_powerlaw]: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.powerlaw.html [numpyro_powerlaw]: https://num.pyro.ai/en/stable/distributions.htmldoublytruncatedpowerlaw)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Implementation of PowerLaw distribution,"Hi, [SciPy][SciPy] has a very specific implementation of [Powerlaw][scipy_powerlaw]. It is over a unit interval and only supports positive indexes. Many applications require it to be over a broader range of intervals (truncated on both sides) and demand the support of negative indexes. I would like to request this generic Powerlaw implementation in JAX. NumPyro has one very generic [implementation][numpyro_powerlaw] from where you can reference all the mathematical derivations. [SciPy]: https://docs.scipy.org/doc/scipy/index.html [scipy_powerlaw]: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.powerlaw.html [numpyro_powerlaw]: https://num.pyro.ai/en/stable/distributions.htmldoublytruncatedpowerlaw",2024-09-22T10:32:17Z,enhancement,open,0,1,https://github.com/jax-ml/jax/issues/23826,"You can do this with TFPonJAX. There's no builtin Powerlaw distribution, but you can apply an affine transformation to a Beta distribution (the Scipy docs say ""powerlaw is a special case of beta with b=1."")  !image Hope that helps!"
1448,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Simplify extended dtype convert logic)ï¼Œ å†…å®¹æ˜¯ (diffbase: CC(add jax.experimental.primal_tangent_dtype helper) TODO:  [x] add test coverage for the code added here Previously, the idea was that we would use the `convert_element_type` primitive to cast to/from extended dtypes. Extended dtype rules specified `convert_from(dtype1, dtype2) > bool` and `convert_to(dtype1, dtype2) > bool` functions. They were meant to do something like indicate whether a convert_element_type was legal. But I'm not sure if they really made sense. The implementation was certainly buggy for nonscalar representation types (physical element types). This PR simplifies and fixes things: 1. Instead of overloading the `convert_element_type_p` primitive with more cases involving casts to/from extended dtypes, let's just have distinct `to_edtype_p` and `from_edtype_p` primitives, which can be much simpler. We still reuse the `jax.lax.convert_element_type` API function, so there's no API change to the few existing users who know about this stuff. 2. Instead of extended dtype rules including `convert_from`/`convert_to` functions with questionable semantics, let's only allow casts to/from the representation type, which is already specified by the rules' `physical_element_aval`. (Indeed that shou)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Simplify extended dtype convert logic,"diffbase: CC(add jax.experimental.primal_tangent_dtype helper) TODO:  [x] add test coverage for the code added here Previously, the idea was that we would use the `convert_element_type` primitive to cast to/from extended dtypes. Extended dtype rules specified `convert_from(dtype1, dtype2) > bool` and `convert_to(dtype1, dtype2) > bool` functions. They were meant to do something like indicate whether a convert_element_type was legal. But I'm not sure if they really made sense. The implementation was certainly buggy for nonscalar representation types (physical element types). This PR simplifies and fixes things: 1. Instead of overloading the `convert_element_type_p` primitive with more cases involving casts to/from extended dtypes, let's just have distinct `to_edtype_p` and `from_edtype_p` primitives, which can be much simpler. We still reuse the `jax.lax.convert_element_type` API function, so there's no API change to the few existing users who know about this stuff. 2. Instead of extended dtype rules including `convert_from`/`convert_to` functions with questionable semantics, let's only allow casts to/from the representation type, which is already specified by the rules' `physical_element_aval`. (Indeed that shou",2024-09-21T19:01:20Z,pull ready,closed,0,9,https://github.com/jax-ml/jax/issues/23823,"> Do we need random_wrap_p and random_unwrap_p still, after this? I'm not sure! I haven't looked. WDYT?","From what I can tell these become redundant _in principle_, though I'm not sure if exactly practically so as of this PR.","It looks like `random_wrap_impl` produces a `PRNGKeyArray`, whereas the stuff here is just about converting element types of `Array`s.","The only implementation of `Array` that can carry an edtype dtype in general is an earray (equivalently `PRNGKeyArray`, up to current state of implementation). Howcome the `impl` rule of `to_edtype_p` can eschew wrapping up the resulting array in an earray/`PRNGKeyArray`? One answer could be: the current impl rule is unable to support all edtypes, for instance key types. And what the `random_wrap_p` impl rule does accounts for that difference, then. Does that sound correct?","> Howcome the impl rule of to_edtype_p can eschew wrapping up the resulting array in an earray/PRNGKeyArray? The trick is these things only work under a `jit` (for now). > One answer could be: the current impl rule is unable to support all edtypes, for instance key types. And what the random_wrap_p impl rule does accounts for that difference, then. Does that sound correct? What impl rule are you referring to? I think the unification question just comes down to whether we can use EArray for keys, or if we need to keep PRNGKeyArray.","> The trick is these things only work under a `jit` (for now). Why do we write an impl rule at all in this PR if these primitives are only meant to work under `jit` for now? > What impl rule are you referring to? `to_edtype_p`'s > I think the unification question just comes down to whether we can use EArray for keys, or if we need to keep PRNGKeyArray. This PR doesn't seem to make any EArray instances (?). If it were to do so, then it would happen in the impl rule for `to_edtype_p`. Does that sound right? Running ahead with that: I don't think it comes down to unifying `EArray` and `PRNGKeyArray`. Even with these two internal types being defined separately, we can always conditionally produce either an `EArray` or a `PRNGKeyArray` on conversion. (By the way, I predict that we can unify earrays and keys once we've worked out more of earrays. Their foreseeable differences to `PRNGKeyArray` seem minor, and reconcilable based on the edtype. That's uncertain until we try it, but seems separate either way.)","> Why do we write an impl rule at all in this PR if these primitives are only meant to work under jit for now? The primitives all work with or without a jit. They just need EArray. Once EArray works, all this code will work unchanged. > This PR doesn't seem to make any EArray instances (?). If it were to do so, then it would happen in the impl rule for to_edtype_p. Does that sound right? Right, this PR does nothing with EArray. The impl rule for to_edtype_p is just the standard one, and it won't ever have to change. Once EArray works, this impl will work in eager. (It might actually work now but I haven't tested it.) > Even with these two internal types being defined separately What are the internal types? EArray and PRNGKeyArray are returned to the user.","Sounds like we're saying very similar things. It seems that an answer to my original question (about obviating `random_wrap_p`) is: yes we likely could, provided we make `to_edtype_p` work across more target edtypes, in eager, by returning EArray (and/or PRNGKeyArray) as needed in its impl rule. I agree that's not done in this PR. Seems close though. Does that sound plausible? > What are the internal types? EArray and PRNGKeyArray are returned to the user. They're essentially all `jax.Array` from the external point of view. The `PRNGKeyArray` symbol is hidden. This is similar to how `ArrayImpl` is a `jax.Array` from the outside and is otherwise hidden. You can see both using `type()`, but that's the usual case with Python: ","> They're essentially all jax.Array from the external point of view. The PRNGKeyArray symbol is hidden. This is similar to how ArrayImpl is a jax.Array from the outside and is otherwise hidden.  Ah I see your point. You're right; I was just thrown by the term ""internal"". But as you say, the constructor and all other classspecific details are private, and these are just implementation details of Array. > Does that sound plausible? Yes except I don't think we'll need to change the impl rule. Once we make it work for `jit`, this impl rule will work too!"
562,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tighten test tolerances after the underlying issue causing nondeterministic results for _nrm2 in Eigen BLAS was fixed in https://gitlab.com/libeigen/eigen/-/merge_requests/1667 -> cl/663346025)ï¼Œ å†…å®¹æ˜¯ (Tighten test tolerances after the underlying issue causing nondeterministic results for _nrm2 in Eigen BLAS was fixed in https://gitlab.com/libeigen/eigen//merge_requests/1667 > cl/663346025)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Tighten test tolerances after the underlying issue causing nondeterministic results for _nrm2 in Eigen BLAS was fixed in https://gitlab.com/libeigen/eigen/-/merge_requests/1667 -> cl/663346025,Tighten test tolerances after the underlying issue causing nondeterministic results for _nrm2 in Eigen BLAS was fixed in https://gitlab.com/libeigen/eigen//merge_requests/1667 > cl/663346025,2024-09-20T16:51:34Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23803
1505,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add support for the new ""dot algorithm"" spec for more explicit control of dot product numerics)ï¼Œ å†…å®¹æ˜¯ (The StableHLO spec has a new ""algorithm"" parameter that allows specifying the algorithm that is used to execute a matrix multiplication, and it can tune the tradeoff between performance and computational cost. Historically the `precision` and `preferred_element_type` parameters have been used to expose some level of control, but their behavior is platform dependent and not sufficiently flexible for performance use cases. The proposal is to add a new algorithm parameter to dot_general to add support for the new explicit API. For a discussion about the limitations/problems with the existing tuning parameters (`precision` and `preferred_element_type`) see jaxml/jax CC(Allow explicit matmul precision), for example. This issue briefly discusses some design questions from the JAX side and proposes an API. The `DotAlgorithm` specification The StableHLO spec defines the ""dot algorithm"" using a data structure with fields defined as follows: > `lhs_precision_type` and `rhs_precision_type`, the precisions that the LHS and RHS of the operation are rounded to. Precision types are independent from the storage types of the inputs and the output. >  > `accumulation_type` the precision used for accumulation. > > `lhs_component_c)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"Add support for the new ""dot algorithm"" spec for more explicit control of dot product numerics","The StableHLO spec has a new ""algorithm"" parameter that allows specifying the algorithm that is used to execute a matrix multiplication, and it can tune the tradeoff between performance and computational cost. Historically the `precision` and `preferred_element_type` parameters have been used to expose some level of control, but their behavior is platform dependent and not sufficiently flexible for performance use cases. The proposal is to add a new algorithm parameter to dot_general to add support for the new explicit API. For a discussion about the limitations/problems with the existing tuning parameters (`precision` and `preferred_element_type`) see jaxml/jax CC(Allow explicit matmul precision), for example. This issue briefly discusses some design questions from the JAX side and proposes an API. The `DotAlgorithm` specification The StableHLO spec defines the ""dot algorithm"" using a data structure with fields defined as follows: > `lhs_precision_type` and `rhs_precision_type`, the precisions that the LHS and RHS of the operation are rounded to. Precision types are independent from the storage types of the inputs and the output. >  > `accumulation_type` the precision used for accumulation. > > `lhs_component_c",2024-09-20T14:19:55Z,enhancement,closed,1,1,https://github.com/jax-ml/jax/issues/23797,This was implemented in https://github.com/jaxml/jax/pull/23574!
1447,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Port GPU kernels for SVD to the FFI.)ï¼Œ å†…å®¹æ˜¯ (Port GPU kernels for SVD to the FFI. Unlike the other GPU linear algebra kernels that I've ported so far, this one isn't straightforward to implement as a single kernel, and while it does support lowering without access to a GPU (no more descriptor!), it only supports dynamics shapes in the batch dimensions. There are two main technical challenges: 1. The main `gesvd` kernels in cuSolver/hipSolver only support matrices with shape `(m, n)` with `m >= n`. This means that we need to transpose the inputs and outputs as part of the lowering rule when `m < n`. (Note: we actually just use C layouts instead of Fortran layouts to implement this case.) While this could be handled in the kernel, this seemed like a lot of work for somewhat limited benefit, and it would probably have performance implications. 2. The `gesvd` and `gesvdj` kernels return `V^H` and `V` respectively, and the batched version of `gesvdj` doesn't support `full_matrices=False`. This means that we need logic in the lowering rule to handle transposition and slicing. This makes it hard to have the algorithm selection be a parameter to the kernel. Another note: cuSolver has a 64bit implementation of the SVD, and we always use that implementation on the C)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,Port GPU kernels for SVD to the FFI.,"Port GPU kernels for SVD to the FFI. Unlike the other GPU linear algebra kernels that I've ported so far, this one isn't straightforward to implement as a single kernel, and while it does support lowering without access to a GPU (no more descriptor!), it only supports dynamics shapes in the batch dimensions. There are two main technical challenges: 1. The main `gesvd` kernels in cuSolver/hipSolver only support matrices with shape `(m, n)` with `m >= n`. This means that we need to transpose the inputs and outputs as part of the lowering rule when `m < n`. (Note: we actually just use C layouts instead of Fortran layouts to implement this case.) While this could be handled in the kernel, this seemed like a lot of work for somewhat limited benefit, and it would probably have performance implications. 2. The `gesvd` and `gesvdj` kernels return `V^H` and `V` respectively, and the batched version of `gesvdj` doesn't support `full_matrices=False`. This means that we need logic in the lowering rule to handle transposition and slicing. This makes it hard to have the algorithm selection be a parameter to the kernel. Another note: cuSolver has a 64bit implementation of the SVD, and we always use that implementation on the C",2024-09-20T13:41:15Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23794
255,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Better doc for `jax.numpy.i0`)ï¼Œ å†…å®¹æ˜¯ (Part of CC(Tracking issue: inline docstrings) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Better doc for `jax.numpy.i0`,Part of CC(Tracking issue: inline docstrings) ,2024-09-20T08:38:13Z,documentation pull ready,closed,0,3,https://github.com/jax-ml/jax/issues/23790,Fixed the lint_and_typecheck error. Could you please trigger the tests again?,Can you please squash the changes into a single commit?,Thanks! Squashed into single commit.
486,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Improve the coverage of shard map tests for < 8 devices. Due to the skip in SetupModule before this change, we lost a lot of coverage on latest hardware.)ï¼Œ å†…å®¹æ˜¯ (Improve the coverage of shard map tests for < 8 devices. Due to the skip in SetupModule before this change, we lost a lot of coverage on latest hardware.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,"Improve the coverage of shard map tests for < 8 devices. Due to the skip in SetupModule before this change, we lost a lot of coverage on latest hardware.","Improve the coverage of shard map tests for < 8 devices. Due to the skip in SetupModule before this change, we lost a lot of coverage on latest hardware.",2024-09-19T21:12:02Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23772
1450,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Multiprocessing with Jax on Slurm fails)ï¼Œ å†…å®¹æ˜¯ ( Description Hello, unfortunately I am having an issue running a script using jax on SLURM. My Job only specifies 1 GPU but it still seems like an issue as the system still has multiple GPUs. That was also the only difference I was able to find between my personal computer and the server as it works on my personal machine with no problem using the exact same conda environment.  Unfortunately I need to fix this anyway since my machine runs out of memory quickly. The error message I am getting looks like this:  I have already tried updating the jax config jax_platform to gpu after this error happened but it raises pretty much the same issue without the entire traceback of course. To explain this a little more:  When I activate the conda environment and import jax to make some testing calculations it just works fine and the output of jax.devices() shows the cuda device with ID 0. When I then run the script that causes this issue and check jax.devices() afterwards it only shows CPUdevice and no cuda device any longer and after that as explained I cant even set the platform back to GPU. However with CPUdevice it still crashes cause it is trying to run on a cuda device but I kinda suspect that once the issue happens i)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Multiprocessing with Jax on Slurm fails," Description Hello, unfortunately I am having an issue running a script using jax on SLURM. My Job only specifies 1 GPU but it still seems like an issue as the system still has multiple GPUs. That was also the only difference I was able to find between my personal computer and the server as it works on my personal machine with no problem using the exact same conda environment.  Unfortunately I need to fix this anyway since my machine runs out of memory quickly. The error message I am getting looks like this:  I have already tried updating the jax config jax_platform to gpu after this error happened but it raises pretty much the same issue without the entire traceback of course. To explain this a little more:  When I activate the conda environment and import jax to make some testing calculations it just works fine and the output of jax.devices() shows the cuda device with ID 0. When I then run the script that causes this issue and check jax.devices() afterwards it only shows CPUdevice and no cuda device any longer and after that as explained I cant even set the platform back to GPU. However with CPUdevice it still crashes cause it is trying to run on a cuda device but I kinda suspect that once the issue happens i",2024-09-19T20:47:07Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/23770,It could be that the GPU is in exclusive process mode. Could you try running `nvidiasmi c 0`?,"> It could be that the GPU is in exclusive process mode. Could you try running `nvidiasmi c 0`? Hello, thanks for the recommendation, unfortunately I have rights on the machine so I cannot change de Compute Mode but I can confirm that at least the GPU that was assigned to my job just now was set as Exclusive Process and I see a gpu on default but im not sure if or how I could force SLURM to assign such a GPU to my job",sevcik I tried it out on my PC and found out that you are right. Its the exclusive process mode that causes this issue and after some searching i found hardware that is already set to default on slurm so I am now just requesting these GPUs and it works fine. Thanks
421,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Added `is_signed` to `mgpu.FragmentedArray`)ï¼Œ å†…å®¹æ˜¯ (Added `is_signed` to `mgpu.FragmentedArray` The registers within a fragmented array always use signless types, and instead the signedness is tracked on the fragmented arrays itself (i.e. in Python).)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Added `is_signed` to `mgpu.FragmentedArray`,"Added `is_signed` to `mgpu.FragmentedArray` The registers within a fragmented array always use signless types, and instead the signedness is tracked on the fragmented arrays itself (i.e. in Python).",2024-09-19T15:16:08Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23764
284,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Added comparison operators to `mgpu.FragmentedArray`)ï¼Œ å†…å®¹æ˜¯ (Added comparison operators to `mgpu.FragmentedArray`)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Added comparison operators to `mgpu.FragmentedArray`,Added comparison operators to `mgpu.FragmentedArray`,2024-09-18T21:41:46Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23742
333,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Improve documentation for jnp.digitize)ï¼Œ å†…å®¹æ˜¯ (Part of CC(Tracking issue: inline docstrings); followup to CC(Add underlying method argument to jax.numpy.digitize))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Improve documentation for jnp.digitize,Part of CC(Tracking issue: inline docstrings); followup to CC(Add underlying method argument to jax.numpy.digitize),2024-09-18T19:00:12Z,documentation pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/23737
1506,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add `donate` and `may_alias` as an argument to `device_put` to allow for donation and aliasing.)ï¼Œ å†…å®¹æ˜¯ (Add `donate` and `may_alias` as an argument to `device_put` to allow for donation and aliasing. The end state we want to work towards is to remove `may_alias` and **always copy by default**. But there is some work needed to get to that state. **Definition:** * donate: The input buffer will be marked as deleted (see below for some caveats). The output buffer may or may not reuse the input buffer's underlying memory. * may_alias: If True, we may return the original buffer depending on the implementation. **What problem are we solving?** Eventually, we want `device_put` to always copy so introducing `may_alias` as a transition state to help towards that goal. We might end up deciding to keep `may_alias` but now you have an explicit option to **always copy** i.e. set `may_alias=False` which is what some users want. Adding `donate` allows users to avoid this pattern of code:  Now it can just be: `jax.device_put(inp, sharding, donate=True)` **So what are the semantics of these 2 options?** Let's create a table:  `donate` is best effort for now until we fix the following things:  * Delete input when `donate=True` regardless of whether XLA could donate or not. This will affect `jax.jit` too but it's a good thing to do. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add `donate` and `may_alias` as an argument to `device_put` to allow for donation and aliasing.,"Add `donate` and `may_alias` as an argument to `device_put` to allow for donation and aliasing. The end state we want to work towards is to remove `may_alias` and **always copy by default**. But there is some work needed to get to that state. **Definition:** * donate: The input buffer will be marked as deleted (see below for some caveats). The output buffer may or may not reuse the input buffer's underlying memory. * may_alias: If True, we may return the original buffer depending on the implementation. **What problem are we solving?** Eventually, we want `device_put` to always copy so introducing `may_alias` as a transition state to help towards that goal. We might end up deciding to keep `may_alias` but now you have an explicit option to **always copy** i.e. set `may_alias=False` which is what some users want. Adding `donate` allows users to avoid this pattern of code:  Now it can just be: `jax.device_put(inp, sharding, donate=True)` **So what are the semantics of these 2 options?** Let's create a table:  `donate` is best effort for now until we fix the following things:  * Delete input when `donate=True` regardless of whether XLA could donate or not. This will affect `jax.jit` too but it's a good thing to do. ",2024-09-18T18:09:42Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23733
354,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Added a missing branch to `mgpu.FragmentedArray.astype`)ï¼Œ å†…å®¹æ˜¯ (Added a missing branch to `mgpu.FragmentedArray.astype` Previously, an unsupported cast produced a `NameError` instead.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Added a missing branch to `mgpu.FragmentedArray.astype`,"Added a missing branch to `mgpu.FragmentedArray.astype` Previously, an unsupported cast produced a `NameError` instead.",2024-09-18T15:30:23Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23724
519,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(When calculating the loss, the input data does not contain NaN, but the output contains NaN)ï¼Œ å†…å®¹æ˜¯ ( Description Please specify cuda:0 at the very beginning.  !å±å¹•æˆªå›¾ 20240918 184040  System info (python version, jaxlib version, accelerator, etc.) Code and data links:https://drive.google.com/file/d/1lxHI_OQwjSUCj7vszNIzl_NmkyVF6JQu/view?usp=sharing)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"When calculating the loss, the input data does not contain NaN, but the output contains NaN"," Description Please specify cuda:0 at the very beginning.  !å±å¹•æˆªå›¾ 20240918 184040  System info (python version, jaxlib version, accelerator, etc.) Code and data links:https://drive.google.com/file/d/1lxHI_OQwjSUCj7vszNIzl_NmkyVF6JQu/view?usp=sharing",2024-09-18T10:42:58Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/23717,"Hi  if I'm not mistaken, this looks like a duplicate of CC(When calculating the loss, the input data does not contain NaN, but the output contains NaN). Let's close this one and continue the discussion there."
332,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add get_replication to shard_map.py for verifying if an array is replicated.)ï¼Œ å†…å®¹æ˜¯ (Add get_replication to shard_map.py for verifying if an array is replicated.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add get_replication to shard_map.py for verifying if an array is replicated.,Add get_replication to shard_map.py for verifying if an array is replicated.,2024-09-17T20:29:38Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23706
307,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(cuDNN attention segmentation fault)ï¼Œ å†…å®¹æ˜¯ ( Description Run  produces   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,cuDNN attention segmentation fault," Description Run  produces   System info (python version, jaxlib version, accelerator, etc.) ",2024-09-17T18:33:06Z,bug NVIDIA GPU,open,0,11,https://github.com/jax-ml/jax/issues/23701, ,"I just created a PR to fix/workaround this longstanding cuDNN limitation with dbias. cuDNN only supports dbias when the batch size is 1 (here). For batch sizes greater than 1, the cuDNN SDPA API returns an allzero dbias tensor (which isn't ideal, but that's the current behavior). When using vmap, the API only detects a singleton batch before the vmap is applied, causing it to mistakenly set has_dbias to `True`, which leads to this SegFault. This PR resolves the issue by resetting has_dbias to False, and returns an allzero dbias tensor as in the nonvmap version. To summarize, the behavior is: ","Also,  for comments on the dbias support from cudnn.",Having d_bias be zeroes when there is a batch dimension is definitely wrong behaviour: it seems like a silent failure that would be extremely difficult to debug for users when their training curves just don't look right. I think we should fail in this case rather until cuDNN supports this.,"We actually had some internal discussions earlier. I think the dilemma is this: it seems that some models don't require `d_bias` but still want to benefit from cuDNN flash attention. If we simply throw an error when the bias has a batch size other than 1, they might complain. Ideally, if we could detect whether `d_bias` is needed, we could decide whether to error out or proceed. However, it seems that no such mechanism exists in JAX. Instead, we currently have silent allzero biases when it's not supported (which is very bad, I know....). Do you think issuing a warning would help? Or should we just throw an error whenever cuDNN+bias is used with its batch size larger than 1?","I think that this should either work correctly, or it should throw an error. It should definitely not seg fault or give incorrect gradients (even with a warning). The latter is just too dangerous for users, who expect JAX APIs to do what they think they will do, or waste massive compute on runs with a major bug. Would you agree ?","I do. Wrong outputs aren't ok, because they are the kind of thing that makes people lose trust in a library.","Sure, I will essentially move this logic to the public API to throw an error for the cudnn exec path.","By the way, do you think we should apply this errorthrowing behavior to the public API or the cuDNN API? Perhaps it should only be applied to the public API, allowing power users who are certain they don't have d_bias to use the private cuDNN API.","Private APIs (`jax._src`, etc.) can do whatever you like. If a private API breaks, you get to keep both pieces.","I have pushed a new change to the PR to check the validity of the bias shape. It will throw an error only when the bias is invalid and bprop is applied. Also, the original segfault issues is fixed in a separate PR.  can you take a look?"
508,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(OptimSteps not compatible with shard_map due to lax.cond)ï¼Œ å†…å®¹æ˜¯ ( Description When trying to use optax.MultiSteps on a dataparallel setup with shard_map, I am getting the following error:  A simple example can be found below:  Disabling multistep makes the error go away.  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,OptimSteps not compatible with shard_map due to lax.cond," Description When trying to use optax.MultiSteps on a dataparallel setup with shard_map, I am getting the following error:  A simple example can be found below:  Disabling multistep makes the error go away.  System info (python version, jaxlib version, accelerator, etc.) ",2024-09-17T13:08:48Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/23693, can we write a general replication rule for cond?,"Yeah, should be easy! (In the sense of: ""We choose to ~go to the Moon~ implement cond rules in this decade, not because they are easy, but because ~they are hard~ we thought they would be easy"")",Is this a relatively easy change for a new contributor to help out with? I would be keen to learn more about JAX's internals. Thanks!,I think CC(Add a shard map replication rule for cond_p) fixed this!
276,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add very simple batching support for ragged_dot.)ï¼Œ å†…å®¹æ˜¯ (Add very simple batching support for ragged_dot.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Add very simple batching support for ragged_dot.,Add very simple batching support for ragged_dot.,2024-09-17T07:33:40Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23685
761,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jax.numpy.nonzero extremely slow on sharded array)ï¼Œ å†…å®¹æ˜¯ ( Description When jnp.nonzero (or jnp.argwhere or sparse.BCOO.fromdense) is run on a sparse array and that array is sharded (this doesn't happen without sharding), the nonzero operation is extremely slow. This is a little odd to me since there shouldn't be much (or any) communication between devices for this op since it can be trivially split entirely between the devices. I'm actually unsure if the op ever completes. I've let it run for 10+ mins with no result.   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,jax.numpy.nonzero extremely slow on sharded array," Description When jnp.nonzero (or jnp.argwhere or sparse.BCOO.fromdense) is run on a sparse array and that array is sharded (this doesn't happen without sharding), the nonzero operation is extremely slow. This is a little odd to me since there shouldn't be much (or any) communication between devices for this op since it can be trivially split entirely between the devices. I'm actually unsure if the op ever completes. I've let it run for 10+ mins with no result.   System info (python version, jaxlib version, accelerator, etc.) ",2024-09-16T21:55:48Z,bug,open,0,2,https://github.com/jax-ml/jax/issues/23675,"Seems related to the size calculation which would explain a bunch of communication overhead making the op slow, JITing it with a specified size speeds up computation significantly (it is still much much slower than numpy on CPU)","After a warmup loop, it's pretty quick: (tested on a TPU) !image"
398,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add underlying method argument to jax.numpy.digitize)ï¼Œ å†…å®¹æ˜¯ (Fixes CC(jax.numpy.digitize doesn't work with shape polymorphism) by providing a means to choose an underlying searchsorted implementation compatible with shape poly.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add underlying method argument to jax.numpy.digitize,Fixes CC(jax.numpy.digitize doesn't work with shape polymorphism) by providing a means to choose an underlying searchsorted implementation compatible with shape poly.,2024-09-16T18:33:10Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/23669
1441,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump build from 1.2.1 to 1.2.2)ï¼Œ å†…å®¹æ˜¯ (Bumps build from 1.2.1 to 1.2.2.  Release notes Sourced from build's releases.  Version 1.2.2  What's Changed  Add editable to builder.get_requries_for_build's static types (PR  CC(Gradient of `tanh` sometimes causes invalid values), fixes issue  CC(How to use `lax.scan()` with `xs` as a tuple?)) Include artifact attestations in our release (PR  CC(Improve behavior of a number of math functions for extreme inputs.)) Fix typing compatibility with typed pyprojecthooks (PR  CC(cuda failed to allocate errors)) Mark more tests with network (PR  CC(Simplify pyval result handler.)) Add more intersphinx links to docs (PR  CC(xla in pmap fails (i.e. jitofpmap or lax.scan with collectives))) Make uv optional for tests (PR  CC(namedtuple subclass transparency (fixes 806)) and  CC(Add documentation on asynchronous dispatch.))  New Contributors  @â€‹carlwgeorge made their first contribution in pypa/build CC(Simplify pyval result handler.) @â€‹edgarrmondragon made their first contribution in pypa/build CC(xla in pmap fails (i.e. jitofpmap or lax.scan with collectives))  Full Changelog: https://github.com/pypa/build/compare/1.2.1...1.2.2    Changelog Sourced from build's changelog.  1.2.2 (20240906)  Add editable to builder.get_re)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Bump build from 1.2.1 to 1.2.2,"Bumps build from 1.2.1 to 1.2.2.  Release notes Sourced from build's releases.  Version 1.2.2  What's Changed  Add editable to builder.get_requries_for_build's static types (PR  CC(Gradient of `tanh` sometimes causes invalid values), fixes issue  CC(How to use `lax.scan()` with `xs` as a tuple?)) Include artifact attestations in our release (PR  CC(Improve behavior of a number of math functions for extreme inputs.)) Fix typing compatibility with typed pyprojecthooks (PR  CC(cuda failed to allocate errors)) Mark more tests with network (PR  CC(Simplify pyval result handler.)) Add more intersphinx links to docs (PR  CC(xla in pmap fails (i.e. jitofpmap or lax.scan with collectives))) Make uv optional for tests (PR  CC(namedtuple subclass transparency (fixes 806)) and  CC(Add documentation on asynchronous dispatch.))  New Contributors  @â€‹carlwgeorge made their first contribution in pypa/build CC(Simplify pyval result handler.) @â€‹edgarrmondragon made their first contribution in pypa/build CC(xla in pmap fails (i.e. jitofpmap or lax.scan with collectives))  Full Changelog: https://github.com/pypa/build/compare/1.2.1...1.2.2    Changelog Sourced from build's changelog.  1.2.2 (20240906)  Add editable to builder.get_re",2024-09-16T17:44:25Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23664,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
1442,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump numpy from 1.26.4 to 2.1.1)ï¼Œ å†…å®¹æ˜¯ (Bumps numpy from 1.26.4 to 2.1.1.  Release notes Sourced from numpy's releases.  2.1.1 (Sep 3, 2024) NumPy 2.1.1 Release Notes NumPy 2.1.1 is a maintenance release that fixes bugs and regressions discovered after the 2.1.0 release. The Python versions supported by this release are 3.103.13. Contributors A total of 7 people contributed to this release. People with a &quot;+&quot; by their names contributed a patch for the first time.  Andrew Nelson Charles Harris Mateusz SokÃ³Å‚ Maximilian Weigand + Nathan Goldbaum Pieter Eendebak Sebastian Berg  Pull requests merged A total of 10 pull requests were merged for this release.   CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): REL: Prepare for the NumPy 2.1.0 release [wheel build]  CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): MAINT: prepare 2.1.x for further development  CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): BUG: revert unintended change in the return value of set_printoptions  CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): BUG: fix reference counting bug in __array_interface__ implementation...  CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): TST: Add regression test for missing descr in arrayinterface  CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): BUG: Fix  CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) and  CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)  CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): BUG: Fix array_equal for numeric and nonnumeric scalar types  CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): MAINT: Update maintenance/2.1.x after the 2.0.2 release  CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): BLD: cp311 maco)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bump numpy from 1.26.4 to 2.1.1,"Bumps numpy from 1.26.4 to 2.1.1.  Release notes Sourced from numpy's releases.  2.1.1 (Sep 3, 2024) NumPy 2.1.1 Release Notes NumPy 2.1.1 is a maintenance release that fixes bugs and regressions discovered after the 2.1.0 release. The Python versions supported by this release are 3.103.13. Contributors A total of 7 people contributed to this release. People with a &quot;+&quot; by their names contributed a patch for the first time.  Andrew Nelson Charles Harris Mateusz SokÃ³Å‚ Maximilian Weigand + Nathan Goldbaum Pieter Eendebak Sebastian Berg  Pull requests merged A total of 10 pull requests were merged for this release.   CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): REL: Prepare for the NumPy 2.1.0 release [wheel build]  CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): MAINT: prepare 2.1.x for further development  CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): BUG: revert unintended change in the return value of set_printoptions  CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): BUG: fix reference counting bug in __array_interface__ implementation...  CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): TST: Add regression test for missing descr in arrayinterface  CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): BUG: Fix  CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) and  CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)  CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): BUG: Fix array_equal for numeric and nonnumeric scalar types  CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): MAINT: Update maintenance/2.1.x after the 2.0.2 release  CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): BLD: cp311 maco",2024-09-16T17:41:51Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23662,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
593,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(When I compute gradients and perform updates using the same values in Torch and JAX, I find that after multiple iterations, the inference results differ significantly.)ï¼Œ å†…å®¹æ˜¯ ( Description Please specify cuda:0 at the very beginning.  !å±å¹•æˆªå›¾ 20240915 191310  System info (python version, jaxlib version, accelerator, etc.) download the code:https://drive.google.com/file/d/1H8uPgPdslVpizmSsif6oK4ey2eoum9x/view?usp=sharing )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"When I compute gradients and perform updates using the same values in Torch and JAX, I find that after multiple iterations, the inference results differ significantly."," Description Please specify cuda:0 at the very beginning.  !å±å¹•æˆªå›¾ 20240915 191310  System info (python version, jaxlib version, accelerator, etc.) download the code:https://drive.google.com/file/d/1H8uPgPdslVpizmSsif6oK4ey2eoum9x/view?usp=sharing ",2024-09-15T11:13:45Z,bug,open,0,3,https://github.com/jax-ml/jax/issues/23646,"If it's truly the same model you're running, this is surprising. Given the magnitude of the difference, though, I suspect the models or optimizers differ in important ways: for example, maybe the precise definition of ""learning rate"" differs between the two implementations. I don't know either `optax` or `pytorch` well enough to guess where that difference might lie, but if it's important to you to debug these differences in the implementations, that's probably where I'd start.","> If it's truly the same model you're running, this is surprising. Given the magnitude of the difference, though, I suspect the models or optimizers differ in important ways: for example, maybe the precise definition of ""learning rate"" differs between the two implementations. >  > I don't know either `optax` or `pytorch` well enough to guess where that difference might lie, but if it's important to you to debug these differences in the implementations, that's probably where I'd start. We're using the same model.","> We're using the same model. Sure, but what I'm suggesting is that you may not be using the same optimizer."
520,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(When calculating the loss, the input data does not contain NaN, but the output contains NaN)ï¼Œ å†…å®¹æ˜¯ ( Description Please specify `cuda:0` at the very beginning.  !å±å¹•æˆªå›¾ 20240913 204340  System info (python version, jaxlib version, accelerator, etc.) Code and data links:https://drive.google.com/file/d/1edrk7_sxSgdu7cmXQXf6JsT57xiG1Hb/view?usp=sharing)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"When calculating the loss, the input data does not contain NaN, but the output contains NaN"," Description Please specify `cuda:0` at the very beginning.  !å±å¹•æˆªå›¾ 20240913 204340  System info (python version, jaxlib version, accelerator, etc.) Code and data links:https://drive.google.com/file/d/1edrk7_sxSgdu7cmXQXf6JsT57xiG1Hb/view?usp=sharing",2024-09-13T12:46:46Z,bug,open,0,3,https://github.com/jax-ml/jax/issues/23625,Is there a MVC? This code doesn't run for me,"> Is there a MVC? This code doesn't run for me > Is there a MVC? This code doesn't run for me Where can't run it, in the beginning final_device to set it yourself, you can delete    Translated with DeepL.com (free version)","Hi  â€“ it's going to be hard to help with specifics here absent an MVC (also known as a minimal reproducible example). If you're able to rework your example so that others can run it and see the same errors you are seeing, then we could offer specific guidance. Absent that, though, in general it's not surprising to see NaN outputs for inputs without NaNs: it just means that you're calling some function in your model in a way that is undefined to floating point precision. Here's a simple example of this:  More than likely, somewhere in your model you have an expression that is evaluating to NaN for reasons like this. The best way to debug this is to start diggingin to your model to figure out exactly where this is coming from. One way to do this is to enable the `jax_debug_nans` flag, as described here: https://jax.readthedocs.io/en/latest/debugging/flags.htmljaxdebugnansconfigurationoptionandcontextmanager I hope that helps get you on the right path!"
708,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([io_callback] Adds test for io_callback being used inside custom partitioning)ï¼Œ å†…å®¹æ˜¯ (Offline chatted with  on fixing the crash when using io_callback inside the custom partitioning. The test is currently skipped because of a JAX bug that the lifetime of the _callback wrapper inside emit_python_callback is not well managed after the lowering rule dies.  That would lead to io_callback not working inside custom_partitioning. (Credit to  ) Here I provided a test that can repro the issue and guard the behavior after the issue is fixed.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",chat,[io_callback] Adds test for io_callback being used inside custom partitioning,Offline chatted with  on fixing the crash when using io_callback inside the custom partitioning. The test is currently skipped because of a JAX bug that the lifetime of the _callback wrapper inside emit_python_callback is not well managed after the lowering rule dies.  That would lead to io_callback not working inside custom_partitioning. (Credit to  ) Here I provided a test that can repro the issue and guard the behavior after the issue is fixed.,2024-09-13T07:42:37Z,,open,0,2,https://github.com/jax-ml/jax/issues/23620," could you ptal at this one? The issue is that callback object isn't kept alive if called from a `partition` call, because that call uses an ephemeral MLIR module context, which means the `host_callback` doesn't end up getting kept alive by the executable. This causes a useafterfree.","It doesn't look that simple. lowering_result.keepalive yes does need to have its lifetime extended, but there are two other problems with doing effects inside custom partitioning:  lowering_result.host_callbacks must be properly populated but this list must be passed as an argument to backend.compile(), but it wouldn't be completely known until compilation has reached spmd partitioning.  tokens are not properly added to the computation but we don't know which effect tokens are needed before we end up lowering the body."
973,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Orthogonal Initializer raises gpusolverDnCreate(&handle) failed: cuSolver internal error)ï¼Œ å†…å®¹æ˜¯ ( Description I am having issues initializing a Flax.linen neural network when running with GPU support.  I have narrowed it down to the flax.linen.initializers.orthogonal.  Running the below code will result in a:  `RuntimeError: jaxlib/gpu/solver_handle_pool.cc:37: operation gpusolverDnCreate(&handle) failed: cuSolver internal error` However running the code in another venv with only CPU support it runs just fine.  And secondly running it without the orthogonal kernel initializer it runs just fine. The jax is installed using `pip install U ""jax[cuda12]"" ` I have attached a minimal example that will raise the issue.   Thanks, Brian  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,Orthogonal Initializer raises gpusolverDnCreate(&handle) failed: cuSolver internal error," Description I am having issues initializing a Flax.linen neural network when running with GPU support.  I have narrowed it down to the flax.linen.initializers.orthogonal.  Running the below code will result in a:  `RuntimeError: jaxlib/gpu/solver_handle_pool.cc:37: operation gpusolverDnCreate(&handle) failed: cuSolver internal error` However running the code in another venv with only CPU support it runs just fine.  And secondly running it without the orthogonal kernel initializer it runs just fine. The jax is installed using `pip install U ""jax[cuda12]"" ` I have attached a minimal example that will raise the issue.   Thanks, Brian  System info (python version, jaxlib version, accelerator, etc.) ",2024-09-13T02:47:11Z,bug,open,0,3,https://github.com/jax-ml/jax/issues/23616,"Thanks for the report. I'm not too sure what the issue is here, but I'm happy to help dig into it. 2 requests: 1. Would you be able to try to put together an (even!) smaller example that doesn't depend on flax? I.e. just calling QR on an array you construct yourself to make it easier to reproduce. 2. Can you try downgrading to jax/jaxlib 0.4.31? v0.4.32 got yanked because of TPU issues (unrelated to this conversation!), but it'll be easier for me to try with v0.4.31 if you still see the issue there.","If I had to guess, you're probably running out of GPU memory. Try lowering the preallocation fraction here: https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html ?",I have an even smaller example.  Pulled it right from the documentation. https://jax.readthedocs.io/en/latest/_autosummary/jax.nn.initializers.orthogonal.htmljax.nn.initializers.orthogonal I am running this on jax 0.4.31 and still have the exact same issue.  I am even setting the environment variable to restrict the preallocation to 0.25 and am still having the same issue.   
1333,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Implements an alternate version of ragged_attention, wherein, the actual attention kernel itself is dense. Meaning, this kernel does not have the compute saving (@when wrapped kernel) or prefetch/index skipping (via index rewriting) as part of the kernel. Rather, the kernel is invoked with a Jumble (A ragged type representation) and pallas takes care of applying the correct work skipping and index rewriting.)ï¼Œ å†…å®¹æ˜¯ (Implements an alternate version of ragged_attention, wherein, the actual attention kernel itself is dense. Meaning, this kernel does not have the compute saving ( wrapped kernel) or prefetch/index skipping (via index rewriting) as part of the kernel. Rather, the kernel is invoked with a Jumble (A ragged type representation) and pallas takes care of applying the correct work skipping and index rewriting. Performance wise, we should be at parity, although this has not yet been tested. Authoring wise, the new kernel is significantly smaller and simpler to write. A major known limitation of this approach, which we have a plan to fix, is the invariant that the `seq_len % grid_size == 0`  we plan to relax this limitation in following CLs.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"Implements an alternate version of ragged_attention, wherein, the actual attention kernel itself is dense. Meaning, this kernel does not have the compute saving (@when wrapped kernel) or prefetch/index skipping (via index rewriting) as part of the kernel. Rather, the kernel is invoked with a Jumble (A ragged type representation) and pallas takes care of applying the correct work skipping and index rewriting.","Implements an alternate version of ragged_attention, wherein, the actual attention kernel itself is dense. Meaning, this kernel does not have the compute saving ( wrapped kernel) or prefetch/index skipping (via index rewriting) as part of the kernel. Rather, the kernel is invoked with a Jumble (A ragged type representation) and pallas takes care of applying the correct work skipping and index rewriting. Performance wise, we should be at parity, although this has not yet been tested. Authoring wise, the new kernel is significantly smaller and simpler to write. A major known limitation of this approach, which we have a plan to fix, is the invariant that the `seq_len % grid_size == 0`  we plan to relax this limitation in following CLs.",2024-09-12T16:28:01Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23597
664,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Added a new primitive for copying GMEM<->SMEM in Pallas Mosaic GPU kernels)ï¼Œ å†…å®¹æ˜¯ (Added a new primitive for copying GMEMSMEM in Pallas Mosaic GPU kernels The copy is async and needs to be awaited via `plgpu.wait_inflight(...)` for SMEM>GMEM copies and via `plgpu.wait(barrier)` for GMEM>SMEM copies. I decided to have distinct functions for SMEM>GMEM and GMEM>SMEM copies and for the ways to await the result, because the underlying Mosaic GPU APIs (and PTX ISA) *are* in fact very different.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Added a new primitive for copying GMEM<->SMEM in Pallas Mosaic GPU kernels,"Added a new primitive for copying GMEMSMEM in Pallas Mosaic GPU kernels The copy is async and needs to be awaited via `plgpu.wait_inflight(...)` for SMEM>GMEM copies and via `plgpu.wait(barrier)` for GMEM>SMEM copies. I decided to have distinct functions for SMEM>GMEM and GMEM>SMEM copies and for the ways to await the result, because the underlying Mosaic GPU APIs (and PTX ISA) *are* in fact very different.",2024-09-12T14:31:44Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23595
1194,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add support for setting a dot product ""algorithm"" for lax.dot_general.)ï¼Œ å†…å®¹æ˜¯ (Add support for setting a dot product ""algorithm"" for lax.dot_general. The StableHLO spec has a new ""algorithm"" parameter that allows specifying the algorithm that is used to execute a matrix multiplication, and it can tune the tradeoff between performance and computational cost. Historically, in JAX, the precision and preferred_element_type parameters have been used to expose some level of control, but their behavior is platform dependent and not sufficiently flexible for performance use cases. This change adds a new ""algorithm"" parameter to dot_general to add support for the new explicit API. This parameter can be a member of the `SupportedDotAlgorithm` `Enum` to use an algorithm that is known to be supported on at least some hardware. Otherwise, it can be specified using the `DotAlgorithm` data structure which exposes the full generality of the StableHLO spec. Transposition is supported using the `transpose_algorithm` argument.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"Add support for setting a dot product ""algorithm"" for lax.dot_general.","Add support for setting a dot product ""algorithm"" for lax.dot_general. The StableHLO spec has a new ""algorithm"" parameter that allows specifying the algorithm that is used to execute a matrix multiplication, and it can tune the tradeoff between performance and computational cost. Historically, in JAX, the precision and preferred_element_type parameters have been used to expose some level of control, but their behavior is platform dependent and not sufficiently flexible for performance use cases. This change adds a new ""algorithm"" parameter to dot_general to add support for the new explicit API. This parameter can be a member of the `SupportedDotAlgorithm` `Enum` to use an algorithm that is known to be supported on at least some hardware. Otherwise, it can be specified using the `DotAlgorithm` data structure which exposes the full generality of the StableHLO spec. Transposition is supported using the `transpose_algorithm` argument.",2024-09-11T17:43:59Z,,closed,0,1,https://github.com/jax-ml/jax/issues/23574,"The doctests are failing for the new docstring because this new feature requires jaxlib > 0.4.33, which hasn't been released yet.  do you have any suggestions for a reasonable approach for including these examples, but ""xfailing"" when the jaxlib version is too old?"
377,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Ported a few changes to FragmentArray by cperivol@)ï¼Œ å†…å®¹æ˜¯ (Ported a few changes to FragmentArray by cperivol@ * It now supports unary negation * and pointwise operations between scalars and FragmentedArrays)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Ported a few changes to FragmentArray by cperivol@,Ported a few changes to FragmentArray by cperivol@ * It now supports unary negation * and pointwise operations between scalars and FragmentedArrays,2024-09-11T14:42:13Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23568
712,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([ROCM] Multi-device reduction causes segfault)ï¼Œ å†…å®¹æ˜¯ ( Description Running a simple program with a global reduction causes a segfault. The MWE  I'm running on a single node of CINES Ad Astra HPC that has 4xMI250X (seen as 8xMI200).  I'm using ROCM 6.0 and a custom built version of jax because there are no wheels publicly available. Running the program above leads to no information Segfault  so I ran it under gdb to get a stack trace  If you need anything else let me know  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[ROCM] Multi-device reduction causes segfault," Description Running a simple program with a global reduction causes a segfault. The MWE  I'm running on a single node of CINES Ad Astra HPC that has 4xMI250X (seen as 8xMI200).  I'm using ROCM 6.0 and a custom built version of jax because there are no wheels publicly available. Running the program above leads to no information Segfault  so I ran it under gdb to get a stack trace  If you need anything else let me know  System info (python version, jaxlib version, accelerator, etc.) ",2024-09-11T10:38:51Z,bug AMD GPU,open,0,3,https://github.com/jax-ml/jax/issues/23565,"I am not able to reproduce like it is reported (a seg fault). I aligned data to the number of devices and it seems that sharing was done ok. However if data dimension is not divisible by devices then it errored out Here are the details of my experiments. System Info:  Experiment CC(Python 3 compatibility issues): aligned the sharing data to the number of devices.  i.e x is of shape 160x10. it looks like datasharding was done  Experiment CC(Explicit tuples are not valid function parameters in Python 3): x is 120x10 matrix,  Errored with message.  the global size of data dimension 0 should be divisible by num of devices (16), ","Thank you. I had 8 devices, and the given size (120) is divisible by 8. It might be some problems in the setting of our HPC ROCM libraries. Is there some way to debug what is happening inside of  ?","  Can you please mentioned how did you get Jax 4,31 for rocm or steps to build jax locally. I am using following steps build Jax in docker image docker:    rocm/jax:rocm6.0.0jax0.4.26py3.11.0 Cloned Jax/xla git clone b rocmjaxlibv0.4.31 https://github.com/ROCm/jax.git git clone b rocmjaxlibv0.4.31 https://github.com/ROCm/xla.git build/Install JAX locally using command below rm rf dist; python3 m pip uninstall jax jaxlib jaxrocm60pjrt jaxrocm60plugin y; python3 ./build/build.py use_clang=false enable_rocm build_gpu_plugin gpu_plugin_rocm_version=60 rocm_amdgpu_targets=gfx90a bazel_options=override_repository=xla=/workspaces/jax_xla/rocmjaxlibv0.4.31/xla rocm_path=/opt/rocm6.0.0/ && python3 setup.py develop user && python3 m pip install dist/*.whl"
751,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Update users of jax.tree.map() to be more careful about how they handle Nones.)ï¼Œ å†…å®¹æ˜¯ (Update users of jax.tree.map() to be more careful about how they handle Nones. Due to a bug in JAX, JAX previously permitted `jax.tree.map(f, None, x)` where `x` is not `None`, effectively treating `None` as if it were pytreeprefix of any value. But `None` is a pytree container, and it is only a prefix of `None` itself. Fix code that was relying on this bug. Most commonly, the fix is to write `jax.tree.map(lambda a, b: (None if a is None else f(a, b)), x, y, is_leaf=lambda t: t is None)`.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Update users of jax.tree.map() to be more careful about how they handle Nones.,"Update users of jax.tree.map() to be more careful about how they handle Nones. Due to a bug in JAX, JAX previously permitted `jax.tree.map(f, None, x)` where `x` is not `None`, effectively treating `None` as if it were pytreeprefix of any value. But `None` is a pytree container, and it is only a prefix of `None` itself. Fix code that was relying on this bug. Most commonly, the fix is to write `jax.tree.map(lambda a, b: (None if a is None else f(a, b)), x, y, is_leaf=lambda t: t is None)`.",2024-09-11T01:14:44Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23558
1491,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Unused vmap GPU memory allocation causes RESOURCE_EXHAUSTED for versions >0.4.14)ï¼Œ å†…å®¹æ˜¯ ( Description  Overview The script below works when using an NVIDIA GPU with Jax version 0.4.14, but after upgrading to 0.4.31 (and trying a few other versions in between) it is triggering the following error: `E0910 20:24:00.097739   38257 pjrt_stream_executor_client.cc:3067] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate X bytes` `jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory while trying to allocate X bytes.` where the value of `X` ranges from ~5GB (e.g. 4843897104) to 20GB+ depending on the shape of the `dls` variable (set to 3540 in the script below). _jax0.4.14_  error Not sure if this is a bug or if there is some code/syntax in the example below that is no longer supported in versions > 0.4.14 that is responsible for this behavior.  Allocation vs. pprof usage The GPU has 6GB of memory and after some trial and error it appears that setting the `dls` variable to a shape of 1590 succeeds and uses only ~500kB of memory according to pprof (following https://jax.readthedocs.io/en/latest/device_memory_profiling.html), but a shape of 1600 gives an error trying to allocate ~5GB. If pprof is in fact showing GPU memory usage this could suggest memor)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Unused vmap GPU memory allocation causes RESOURCE_EXHAUSTED for versions >0.4.14," Description  Overview The script below works when using an NVIDIA GPU with Jax version 0.4.14, but after upgrading to 0.4.31 (and trying a few other versions in between) it is triggering the following error: `E0910 20:24:00.097739   38257 pjrt_stream_executor_client.cc:3067] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate X bytes` `jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory while trying to allocate X bytes.` where the value of `X` ranges from ~5GB (e.g. 4843897104) to 20GB+ depending on the shape of the `dls` variable (set to 3540 in the script below). _jax0.4.14_  error Not sure if this is a bug or if there is some code/syntax in the example below that is no longer supported in versions > 0.4.14 that is responsible for this behavior.  Allocation vs. pprof usage The GPU has 6GB of memory and after some trial and error it appears that setting the `dls` variable to a shape of 1590 succeeds and uses only ~500kB of memory according to pprof (following https://jax.readthedocs.io/en/latest/device_memory_profiling.html), but a shape of 1600 gives an error trying to allocate ~5GB. If pprof is in fact showing GPU memory usage this could suggest memor",2024-09-10T21:00:47Z,bug,open,0,2,https://github.com/jax-ml/jax/issues/23548,"I checked the HLO when using `dls=jnp.ones(shape=(10000, 3))` but it does indeed look like some very large tensors are being generated by your program (1 x 10000 x 71 x 75 x 71 x3 ~= 40GB)  After commenting out the two lines containing exp these large tensors are not materialized:  I'm not sure why thus code runs on Jax =0.4.30) `jax.xla_computation(run)().as_hlo_text()` (for <0.4.30)","Thanks for the response. I'm starting to think it is some change in openxla or lower that is responsible rather than jax itself. A few questions:   what part of the hlo text shows when a tensor is ""materialized""? is there docs/links on how to read these outputs?   what's the difference between `func.lower().as_text()` and `run.lower().compiler_ir(dialect='hlo').as_hlo_text()`?   how do you determine expected memory based on tensor shape? Does this seem like a bug or just an old edge case not working anymore do you think? When using `dls=jnp.ones(shape=(1590, 3))` the program ran successfully and pprof reported ~500kB of memory usage, but increasing to  `dls=jnp.ones(shape=(1600, 3))` fails trying to allocate ~5GB, which seems like strange behavior."
265,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(trying on another readme header)ï¼Œ å†…å®¹æ˜¯ (More numerical computing. Hopefully fewer line breaks.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,trying on another readme header,More numerical computing. Hopefully fewer line breaks.,2024-09-10T17:17:15Z,pull ready,closed,1,0,https://github.com/jax-ml/jax/issues/23545
1317,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(All-gather performed in fp32 instead of bf16 despite explicit datatype conversion)ï¼Œ å†…å®¹æ˜¯ ( Description Hi, I've implemented RoPE along with llama3.1style context parallelism in JAX. In this setup each GPU gets its own chunk of sequence, calculates query, key and value for sequence chunk independently. Then, allgather is performed for key and value tensors so each GPU can calculate attention for its own query chunk.  After calculating query and key, I'm applying RoPE to them, which is embarrassingly parallel over sequence dimension. During RoPE calculation, query and key temporarily converted to fp32 and converted back to bf16 when calculation is done. One problem I'm facing right now is that instead of first converting chunk of K, V to bf16 independently on each device and then performing allgather, JAX/XLA reorders them so allgather is performed on fp32 tensors and then entire gathered K and V tensor is converted to bf16, effectively doubling communication volume that is intended to be performed over slow internode network.  Here is how it looks in compiled HLO:  I was able to reproduce this behavior with the following script: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,All-gather performed in fp32 instead of bf16 despite explicit datatype conversion," Description Hi, I've implemented RoPE along with llama3.1style context parallelism in JAX. In this setup each GPU gets its own chunk of sequence, calculates query, key and value for sequence chunk independently. Then, allgather is performed for key and value tensors so each GPU can calculate attention for its own query chunk.  After calculating query and key, I'm applying RoPE to them, which is embarrassingly parallel over sequence dimension. During RoPE calculation, query and key temporarily converted to fp32 and converted back to bf16 when calculation is done. One problem I'm facing right now is that instead of first converting chunk of K, V to bf16 independently on each device and then performing allgather, JAX/XLA reorders them so allgather is performed on fp32 tensors and then entire gathered K and V tensor is converted to bf16, effectively doubling communication volume that is intended to be performed over slow internode network.  Here is how it looks in compiled HLO:  I was able to reproduce this behavior with the following script: ",2024-09-10T15:42:50Z,bug,open,14,1,https://github.com/jax-ml/jax/issues/23543,Hi   This issue seems to have been resolved in JAX version 0.4.31. I tested the provided repro on GCP VM instance with 4 T4 GPUs with JAX versions 0.4.31 and 0.4.38. I can now see that the allgather performed in bf16 itself. Compiled HLO:  Could you please verify if the issue is resolved? Thank you.
284,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([pallas:mosaic_gpu] Fragmented array debug printing.)ï¼Œ å†…å®¹æ˜¯ ([pallas:mosaic_gpu] Fragmented array debug printing.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[pallas:mosaic_gpu] Fragmented array debug printing.,[pallas:mosaic_gpu] Fragmented array debug printing.,2024-09-10T13:53:11Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23539
1499,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(add support for QR decomposition gradient calculation for wide matrices (rows < columns))ï¼Œ å†…å®¹æ˜¯ (Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example. I would like to inquire about adding support for gradients for the wide case of QR factorization to JAX. To understand this better consider the following code snippet:  however you can take a gradient for a wide matrix through a QR decomposition provided the input matrix has full (column) rank. E.g. for matrix A, `rank(A) = min(rows, columns)`.  For this scenario we've written the derivations for the gradient as well as implemented in pytorch and tensorflow. Here is the arxiv draft of our paper and the corresponding PRs for pytorch and tensorflow, the latter for the complex matrix case. Here is the tensorflow  pr for the wide matrix real entry case.  **Paper clarifying notes:** 1.  for the wide case the gradient using equation 3.8 replaces the gradient of Q with the `Q_prime` gradient everywhere it occurs for the square matrix `X`.  2. The matrix `Y` for the wide case has a gradient that uses the same formulae regardless of whether equation 3.3 or equation 3.8 are used.  Here equations have the first number indicating the section of the paper, so equation 3.8 for example, is secti)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,add support for QR decomposition gradient calculation for wide matrices (rows < columns),"Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example. I would like to inquire about adding support for gradients for the wide case of QR factorization to JAX. To understand this better consider the following code snippet:  however you can take a gradient for a wide matrix through a QR decomposition provided the input matrix has full (column) rank. E.g. for matrix A, `rank(A) = min(rows, columns)`.  For this scenario we've written the derivations for the gradient as well as implemented in pytorch and tensorflow. Here is the arxiv draft of our paper and the corresponding PRs for pytorch and tensorflow, the latter for the complex matrix case. Here is the tensorflow  pr for the wide matrix real entry case.  **Paper clarifying notes:** 1.  for the wide case the gradient using equation 3.8 replaces the gradient of Q with the `Q_prime` gradient everywhere it occurs for the square matrix `X`.  2. The matrix `Y` for the wide case has a gradient that uses the same formulae regardless of whether equation 3.3 or equation 3.8 are used.  Here equations have the first number indicating the section of the paper, so equation 3.8 for example, is secti",2024-09-10T01:38:46Z,enhancement,open,0,0,https://github.com/jax-ml/jax/issues/23533
1448,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump setuptools from 69.2.0 to 74.1.2)ï¼Œ å†…å®¹æ˜¯ (Bumps setuptools from 69.2.0 to 74.1.2.  Changelog Sourced from setuptools's changelog.  v74.1.2 Bugfixes  Fixed TypeError in sdist filelist processing by adding support for pathlib Paths for the build_base. ( CC(Printdebugging jax?)) Removed degraded and deprecated test_integration (easy_install) from the test suite. ( CC(Move jax.third_party to jax._src.third_party.))  v74.1.1 Bugfixes  Fixed TypeError in msvc.EnvironmentInfo.return_env when no runtime redistributables are installed. ( CC(Fix type problem in dynamic_slice_in_dim in int32 default dtype mode.))  v74.1.0 Features  Added support for defining extmodules via pyproject.toml (EXPERIMENTAL, may change in future releases). ( CC(Internal change))  Bugfixes  Merge with pypa/distutils, removing the duplicate vendored copy of packaging. ( CC(Omnistaging breaks jax.scipy.sparse.linalg.cg in some settings)) Restored setuptools.msvc.Environmentinfo as it is used externally. ( CC(Make pad more robust to nonhashable values.))  v74.0.0 Features  Changed the type of error raised by setuptools.command.easy_install.CommandSpec.from_param on unsupported argument from AttributeError to TypeError  by :user:Avasam ( CC(Ensure values returned by jax.random.truncated_norm)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bump setuptools from 69.2.0 to 74.1.2,"Bumps setuptools from 69.2.0 to 74.1.2.  Changelog Sourced from setuptools's changelog.  v74.1.2 Bugfixes  Fixed TypeError in sdist filelist processing by adding support for pathlib Paths for the build_base. ( CC(Printdebugging jax?)) Removed degraded and deprecated test_integration (easy_install) from the test suite. ( CC(Move jax.third_party to jax._src.third_party.))  v74.1.1 Bugfixes  Fixed TypeError in msvc.EnvironmentInfo.return_env when no runtime redistributables are installed. ( CC(Fix type problem in dynamic_slice_in_dim in int32 default dtype mode.))  v74.1.0 Features  Added support for defining extmodules via pyproject.toml (EXPERIMENTAL, may change in future releases). ( CC(Internal change))  Bugfixes  Merge with pypa/distutils, removing the duplicate vendored copy of packaging. ( CC(Omnistaging breaks jax.scipy.sparse.linalg.cg in some settings)) Restored setuptools.msvc.Environmentinfo as it is used externally. ( CC(Make pad more robust to nonhashable values.))  v74.0.0 Features  Changed the type of error raised by setuptools.command.easy_install.CommandSpec.from_param on unsupported argument from AttributeError to TypeError  by :user:Avasam ( CC(Ensure values returned by jax.random.truncated_norm",2024-09-09T17:07:34Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23520,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
1452,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump etils[epath,epy] from 1.7.0 to 1.9.4)ï¼Œ å†…å®¹æ˜¯ (Bumps [etils[epath,epy]](https://github.com/google/etils) from 1.7.0 to 1.9.4.  Release notes Sourced from etils[epath,epy]'s releases.  v1.9.4  Return Python 3.10 support.  v1.9.3  eapp:  Allow extra kwargs in eapp.make_flags_parser()   epath:  Fix epath.Path pydantic deserialization for URIstyle paths   epy:  Add epy.is_test to check whether we're running in a test environment. Add epy.typing.Json. Add epy.ExitStack which allows setting the contextmanagers during init. Add proto support for epy.binary_adhoc Hide reraise from the traceback for cleaner error messages   exm:  Add exm.url_to_python_only_logs() and exm.curr_job_name() to add artifact to Python only logs (without all the verbose C++ junk ). Fix a bug which makes exm.current_experiment crash    v1.9.2  epath:  Support pydantic serialization of epath.Path    v1.9.1  epath:  Fix an infinite recursion on is_relative_to for Python&gt;=3.12.    v1.9.0  epy:  Add epy.lazy_api_imports to lazyimport __init__.py symbols. Removed: epy.cached_property epy.lazy_imports: Error callback accept a str to autoreraise with additional info. Fix mixing epy.lazy_imports() with epy.binary_adhoc().   ecolab:  Added reload_workspace=True to adhoc to autoreload from workspac)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"Bump etils[epath,epy] from 1.7.0 to 1.9.4","Bumps [etils[epath,epy]](https://github.com/google/etils) from 1.7.0 to 1.9.4.  Release notes Sourced from etils[epath,epy]'s releases.  v1.9.4  Return Python 3.10 support.  v1.9.3  eapp:  Allow extra kwargs in eapp.make_flags_parser()   epath:  Fix epath.Path pydantic deserialization for URIstyle paths   epy:  Add epy.is_test to check whether we're running in a test environment. Add epy.typing.Json. Add epy.ExitStack which allows setting the contextmanagers during init. Add proto support for epy.binary_adhoc Hide reraise from the traceback for cleaner error messages   exm:  Add exm.url_to_python_only_logs() and exm.curr_job_name() to add artifact to Python only logs (without all the verbose C++ junk ). Fix a bug which makes exm.current_experiment crash    v1.9.2  epath:  Support pydantic serialization of epath.Path    v1.9.1  epath:  Fix an infinite recursion on is_relative_to for Python&gt;=3.12.    v1.9.0  epy:  Add epy.lazy_api_imports to lazyimport __init__.py symbols. Removed: epy.cached_property epy.lazy_imports: Error callback accept a str to autoreraise with additional info. Fix mixing epy.lazy_imports() with epy.binary_adhoc().   ecolab:  Added reload_workspace=True to adhoc to autoreload from workspac",2024-09-09T17:06:29Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23519,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
1463,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump the pip group across 1 directory with 2 updates)ï¼Œ å†…å®¹æ˜¯ (Bumps the pip group with 2 updates in the /build directory: zipp and setuptools. Updates `zipp` from 3.18.1 to 3.19.1  Changelog Sourced from zipp's changelog.  v3.19.1 Bugfixes  Improved handling of malformed zip files. ( CC(example Errata))  v3.19.0 Features  Implement is_symlink. ( CC(add oss test instructions, fix conv grad bug))  v3.18.2 No significant changes.    Commits  6d1cb72 Finalize fd604bd Merge pull request  CC(Canâ€™t import both jax and tensorflow (causes kernel restart)) from jaraco/bugfix/119malformedpaths c18417e Add news fragment. 58115d2 Employ SanitizedNames in CompleteDirs. Fixes broken test. 564fcc1 Add SanitizedNames mixin. 79a309f Add some assertions about malformed paths. 2d015c2 Merge https://github.com/jaraco/skeleton a595a0f Rename extras to align with core metadata spec. 608f90a Finalize 3a22d72 Merge pull request  CC(add tensordot) from jaraco/feature/issymlink Additional commits viewable in compare view    Updates `setuptools` from 69.2.0 to 70.0.0  Changelog Sourced from setuptools's changelog.  v70.0.0 Features  Emit a warning when [tools.setuptools] is present in pyproject.toml and will be ignored.  by :user:SnoopJ ( CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)) Improved AttributeError error message if pkg_reso)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bump the pip group across 1 directory with 2 updates,"Bumps the pip group with 2 updates in the /build directory: zipp and setuptools. Updates `zipp` from 3.18.1 to 3.19.1  Changelog Sourced from zipp's changelog.  v3.19.1 Bugfixes  Improved handling of malformed zip files. ( CC(example Errata))  v3.19.0 Features  Implement is_symlink. ( CC(add oss test instructions, fix conv grad bug))  v3.18.2 No significant changes.    Commits  6d1cb72 Finalize fd604bd Merge pull request  CC(Canâ€™t import both jax and tensorflow (causes kernel restart)) from jaraco/bugfix/119malformedpaths c18417e Add news fragment. 58115d2 Employ SanitizedNames in CompleteDirs. Fixes broken test. 564fcc1 Add SanitizedNames mixin. 79a309f Add some assertions about malformed paths. 2d015c2 Merge https://github.com/jaraco/skeleton a595a0f Rename extras to align with core metadata spec. 608f90a Finalize 3a22d72 Merge pull request  CC(add tensordot) from jaraco/feature/issymlink Additional commits viewable in compare view    Updates `setuptools` from 69.2.0 to 70.0.0  Changelog Sourced from setuptools's changelog.  v70.0.0 Features  Emit a warning when [tools.setuptools] is present in pyproject.toml and will be ignored.  by :user:SnoopJ ( CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)) Improved AttributeError error message if pkg_reso",2024-09-09T16:44:19Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23517,"This pull request was built based on a group rule. Closing it will not ignore any of these versions in future pull requests. To ignore these dependencies, configure ignore rules in dependabot.yml"
732,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([ROCM] x64 mode crashes with ""redzone_checker with block dimensions: 1024x1x1: hipError_t"")ï¼Œ å†…å®¹æ˜¯ ( Description I'm running master jax/lib custombuilt for RoCm following the instructions online (because there are no such wheels available around).  However I'm relatively sure this thing is not because of my custom wheels, but it's an issue within XLA/Jax The MWE is  and it crashes at the multiplication with the following error. Do note that the script works fine if I do not enable x64 mode.   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",dspy,"[ROCM] x64 mode crashes with ""redzone_checker with block dimensions: 1024x1x1: hipError_t"""," Description I'm running master jax/lib custombuilt for RoCm following the instructions online (because there are no such wheels available around).  However I'm relatively sure this thing is not because of my custom wheels, but it's an issue within XLA/Jax The MWE is  and it crashes at the multiplication with the following error. Do note that the script works fine if I do not enable x64 mode.   System info (python version, jaxlib version, accelerator, etc.) ",2024-09-09T12:44:51Z,bug XLA AMD GPU,open,0,6,https://github.com/jax-ml/jax/issues/23506,"These are typically issues with the hipblaslt autotuning not being able to run its autotuning kernels correctly. I have been seeing a lot of these lately and will be opening an issue to the XLA folks to look into them more. If you could gather some information for me to forward on to them that would be a big help. 1) Can you confirm that the issue is only present with x64 flag set? 2) What card model were you running this against? (MI100 / MI250 / etc) 3) Can you try running the same scenario but disabling the hipblaslt autotuning by doing `export XLA_FLAGS=""xla_gpu_autotune_level=0""` and report if it still fails or not?","1. Just tried again, and I confirm 2. It's an HPC node with 4x MI250X accelerators, which `rocmsmi` detects as 8 `AMD INSTINCT MI200` devices. 2b. I'm running with Rocm 6.0.0 because that's what the IT technicians provide on the machine (I cannot use virtualisation) 3. Running the script above with `export XLA_FLAGS=""xla_gpu_autotune_level=0""` works correctly with no failure.",Was able to replicate this on an MI300 system.  Don't mind the ubuntu in 'uname'. I had ran it in an almalinux8 container on a ubuntu host. Going to do more testing tomorrow to see if its only ROCm 6.0.x,"I am not able to reproduce it on ROCm 6.0 on gfx90a platform (AMD INSTINCT MI200 ). > docker: rocm/jax:rocm6.0.0jax0.4.26py3.11.0 Cloned Jax/xla > git clone b rocmjaxlibv0.4.31 https://github.com/ROCm/jax.git > git clone b rocmjaxlibv0.4.31 https://github.com/ROCm/xla.git build/Install JAX locally using command below > rm rf dist; python3 m pip uninstall jax jaxlib jaxrocm60pjrt jaxrocm60plugin y; python3 ./build/build.py  use_clang=false enable_rocm build_gpu_plugin gpu_plugin_rocm_version=60 rocm_amdgpu_targets=gfx90a bazel_options=override_repository=xla=/workspaces/jax_xla/rocmjaxlibv0.4.31/xla rocm_path=/opt/rocm6.0.0/ && python3 setup.py develop user && python3 m pip install dist/*.whl Installed JAX > jax                  0.4.31.dev20240808+a96cefdc0 /workspaces/jax_xla/rocmjaxlibv0.4.31/jax > jaxrocm60pjrt      0.4.31.dev20240913 > jaxrocm60plugin    0.4.31.dev20240913 > jaxlib               0.4.31.dev20240913 >>> import jax >>> import jax.numpy as jnp >>> jax.config.update(""jax_enable_x64"", True) >>> x= jax.numpy.ones((512, 25)) >>> M = jax.numpy.ones((512, 512)) >>> z=jnp.matmul(M, x) >>> print(z) [[512. 512. 512. ... 512. 512. 512.]  [512. 512. 512. ... 512. 512. 512.]  [512. 512. 512. ... 512. 512. 512.]  ...  [512. 512. 512. ... 512. 512. 512.]  [512. 512. 512. ... 512. 512. 512.]  [512. 512. 512. ... 512. 512. 512.]] >>>","Can I give you anything to help you identify the issue?  Is it enough that it shows up on MI300? A side note: on HPC systems we cannot use virtualisation/docker (and we don't have superuser privileges).  So I cannot guarantee that my setup is equivalent to yours.  I load a centrally installed ROCm 6.0.0 library, so I can give you any detail you might want about it.  But Docker is intentionally blocked.","I was also able to reproduce this problem on MI300 system with ROCM6.0.0 container (while it runs fine under ROCM 6.0.2 and above).  Actually, on rocmjaxlibv0.4.31 XLA/JAX branch, it also fails without setting **jax_enable_x64** flag when one disables triton_gemm with: **xla_gpu_enable_triton_gemm=false**.  We are investigating the issue. As a workaround, one can still use autotuning with redzone checker disabled via: **xla_gpu_autotune_level=3**."
385,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Pallas flash attention fails on GPU)ï¼Œ å†…å®¹æ˜¯ ( Description Trying to use flash attention on a GPU, hitting a Value error straight away. code:  error:   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Pallas flash attention fails on GPU," Description Trying to use flash attention on a GPU, hitting a Value error straight away. code:  error:   System info (python version, jaxlib version, accelerator, etc.) ",2024-09-08T09:38:54Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/23495,"I wonder, is it actually supposed to work on GPU? I was always assuming that Pallas should feel no difference between a TPU and GPU, but maybe there is a difference? I don't know..","Right, so this works: So I will just stick with the correct ops. Sorry for the noise!"
491,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Jax numpy reduceat function throws error)ï¼Œ å†…å®¹æ˜¯ ( Description Trying to run `np.add.reduceat(data, indices=indices, axis=0)` give the below error:  This is confusing since the documentation says it should work. Here is a snippet to reproduce the issue:   System info (python version, jaxlib version, accelerator, etc.)  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Jax numpy reduceat function throws error," Description Trying to run `np.add.reduceat(data, indices=indices, axis=0)` give the below error:  This is confusing since the documentation says it should work. Here is a snippet to reproduce the issue:   System info (python version, jaxlib version, accelerator, etc.)  ",2024-09-07T21:59:39Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/23493,"Thanks for the report and the repro! The full ufunc support described in the docs was just added a week or two ago in https://github.com/google/jax/pull/23304 so this feature isn't available in any release JAX version as far as I know. You can install JAX from source (I think it would be fine to use the released jaxlib, so source installation wouldn't be too painful) if you need this right away, or I think there will be a new JAX release in the next week or two if it can wait.",Oh got it. Thanks for the info . Is there a way we can schedule docs updates to coincide with releases?,"There have been requests for more explicit versioning of the docs previously, but that's a bit tricky given how JAX is developed, and we release ~once a month so we've always just landed on the side of always publishing the docs at HEAD. When in doubt, check out the git blame in the source code to see when something was added!",Gotcha. I'll close this issue then since it seems like I can get this feature in a few weeks.
936,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Force a physical axis layout rearrangement)ï¼Œ å†…å®¹æ˜¯ (I am trying to optimise a function where I think it'd be much faster if I could force the physical transpose to a pre and op, rather than being fused into the op.  optimisation_barrier has been super useful for forcing op ordering, however as JAX works on logical axes the physical axis order ops like transpose can not be forced. It would be useful to me to have a separate op which can force the physical axis ordering of an array? I know Pallas and custom_call can enforce axis ordering [with operand_layout_constraints], would a noop customcall with layout constraint be the easiest way to do it? For reference, below is a dummy use case, forcing a physical transpose then sum, rather than direct sum  Output: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Force a physical axis layout rearrangement,"I am trying to optimise a function where I think it'd be much faster if I could force the physical transpose to a pre and op, rather than being fused into the op.  optimisation_barrier has been super useful for forcing op ordering, however as JAX works on logical axes the physical axis order ops like transpose can not be forced. It would be useful to me to have a separate op which can force the physical axis ordering of an array? I know Pallas and custom_call can enforce axis ordering [with operand_layout_constraints], would a noop customcall with layout constraint be the easiest way to do it? For reference, below is a dummy use case, forcing a physical transpose then sum, rather than direct sum  Output: ",2024-09-06T09:04:29Z,enhancement,closed,0,3,https://github.com/jax-ml/jax/issues/23471,"I've written something which runs a transpose which enforces that logical layout on the physical layout Any advice on whether it will work if the input layout is not major_to_minor in the operand, or is the LayoutConstraint op somehow inferring this correctly? [Optimisation barrier seems to be required on TPU to force the transpose, but not on GPU, presumably tpu specific compiler pass was removing it] ",We have a layout API in JAX that you can use: See https://github.com/google/jax/blob/ef947a0ce673201b60763fe1f8b26c1c7dcd0fbc/tests/layout_test.pyL4 If you want to specify concrete layouts then check out: https://github.com/google/jax/blob/ef947a0ce673201b60763fe1f8b26c1c7dcd0fbc/tests/layout_test.pyL285,So you probably don't need your own primitive to do this?
519,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add support for the DeviceLocalLayout API when lowering FFI calls.)ï¼Œ å†…å®¹æ˜¯ (This PR updates the FFI lowering rule to support a DeviceLoweringLayout object as input when specifying the input and output layouts. For now, this just converts the DLL object to its appropriate list of minortomajor integers because that's what the custom call op expects.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add support for the DeviceLocalLayout API when lowering FFI calls.,"This PR updates the FFI lowering rule to support a DeviceLoweringLayout object as input when specifying the input and output layouts. For now, this just converts the DLL object to its appropriate list of minortomajor integers because that's what the custom call op expects.",2024-09-05T16:25:18Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/23458
1087,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Multi-process GPU jobs fail on Slurm)ï¼Œ å†…å®¹æ˜¯ ( Description I'm submitting multiprocess jobs on slurm. The job script is  I test with a simple python code, like  But it can't see the devices and raises the error  I have tested the singleprocess code and it works well, so it should be the problem with multiprocess modules. I also tested with many different clusters. The multiprocess program works in some clusters and fails in some others. For example, it fails in the largest Juelich cluster in Germany.   System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.30 jaxlib: 0.4.30 numpy:  1.26.3 python: 3.9.18 (main, Sep 11 2023, 13:41:44)  [GCC 11.2.0] jax.devices (1 total, 1 local): [cuda(id=0)] process_count: 1 platform: uname_result(system='Linux', node='workergpu001', release='6.1.97.1.fi', version=' CC(Python 3 compatibility issues) SMP Tue Jul  9 06:21:23 EDT 2024', machine='x86_64'))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Multi-process GPU jobs fail on Slurm," Description I'm submitting multiprocess jobs on slurm. The job script is  I test with a simple python code, like  But it can't see the devices and raises the error  I have tested the singleprocess code and it works well, so it should be the problem with multiprocess modules. I also tested with many different clusters. The multiprocess program works in some clusters and fails in some others. For example, it fails in the largest Juelich cluster in Germany.   System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.30 jaxlib: 0.4.30 numpy:  1.26.3 python: 3.9.18 (main, Sep 11 2023, 13:41:44)  [GCC 11.2.0] jax.devices (1 total, 1 local): [cuda(id=0)] process_count: 1 platform: uname_result(system='Linux', node='workergpu001', release='6.1.97.1.fi', version=' CC(Python 3 compatibility issues) SMP Tue Jul  9 06:21:23 EDT 2024', machine='x86_64')",2024-09-05T12:07:14Z,bug,open,0,6,https://github.com/jax-ml/jax/issues/23452,"I'm not familiar with Slurm, but it looks like this is an environment issue. Did you follow the installation instructions in https://jax.readthedocs.io/en/latest/installation.html?","> I'm not familiar with Slurm, but it looks like this is an environment issue. Did you follow the installation instructions in https://jax.readthedocs.io/en/latest/installation.html? I think it's not an issue of installation, because the code can run on a single process if I don't call `jax.distributed.initialize()`","How Many GPUs do the nodes have here? More than 2? The error  INTERNAL: Failed call to cuDeviceGet: CUDA_ERROR_INVALID_DEVICE: invalid device ordinal Suggests that cuda is trying to use the wrong device, possibly one that is not exposed. Possibly it's because the local device Jax initializes on every rank is based on the local rank. If SLURM assigned you GPUs 0,1 then all is good. But if he assigns 2,3 initialization will fail because Jax's SlurmCluster assumes that the devices to be used start from 0.",The case stems from  https://github.com/google/jax/blob/8feab682097b0949d0504ec0ee73f4637aeb1f57/jax/_src/clusters/slurm_cluster.pyL66 being called from  https://github.com/google/jax/blob/8feab682097b0949d0504ec0ee73f4637aeb1f57/jax/_src/clusters/cluster.pyL90 Jax should instead use the local process id to index into the cuda visible devices. Slurm usually sets it.,"> How Many GPUs do the nodes have here? More than 2? The error >  > INTERNAL: Failed call to cuDeviceGet: CUDA_ERROR_INVALID_DEVICE: invalid device ordinal >  > Suggests that cuda is trying to use the wrong device, possibly one that is not exposed. >  > Possibly it's because the local device Jax initializes on every rank is based on the local rank. If SLURM assigned you GPUs 0,1 then all is good. But if he assigns 2,3 initialization will fail because Jax's SlurmCluster assumes that the devices to be used start from 0. Thanks Filippo! I think this is the problem. But it's still a bit weird because there is still error when I use all GPUs in a node. Instead, it runs when I call `jax.distributed.initialize(local_device_ids=[0])`. It seems that all machines think their `local_device_ids` are 0.  I did some further tests with the following code  This works perfectly. But when I change it to `return jax.lax.with_sharding_constraint(out, sharding)`, I got error  (workergpu036 is the node name) It seems the GPUs can't communicate with each other. Furthermore, when I print `jax.devices()`, there is no error and I got `[cuda(id=0), cuda(id=1)]`. I guess the problem is the `local_device_ids` somehow changes from 0 to other numbers after `jax.distributed.initialize`, so the devices can't access to each other any more. But I'm not familiar with how jax works exactly. What do you think  ? Do you have any idea how to solve this issue for now?","I solve this problem after consulting the HPC support of the Flatiron Institute. It's due to some stupid mistakes I made when I submitted jobs. Here I post the answer from the HPC support for other users' reference. > But a bit of clarification based on your allocation, because you're using ""gpuspertask"", and not explicitly changing ""gpubind"", each task (that is, each of the 2 processes launched by srun) will only have access to 1 GPU each (which will indeed show up as id 0).  If you want processes to be able to access GPUs assigned to other tasks, you need to use something like ""gpubind=none"" or ""gpus"" instead of ""gpuspertask"". `jax.distributed.initialize()` works nicely after adding `SBATCH gpubind=none` to my job script."
1461,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TPU operations don't appear if profile is too long)ï¼Œ å†…å®¹æ˜¯ ( Description Hi! I noticed unexpected behavior with the JAX profiler (on TPU v3). No TPU operations appear on the profile, and the overview shows ""Host: CPU"" in some cases even when operations are running on the TPU. I believe this is happening when the profile is too long (~3040s). Here's what I observed with my train loop:  * (worked) Profiling 20 steps of my train loop + a dummy dataset: 3 seconds total time on the profile, showed TPU activity as expected. * (failed) 20 steps of same train loop + expensive data pipeline which loaded images from GCS instead of dummy data + large batch size: 35s, only showed CPU activity. * (worked) Expensive data pipeline + smaller batch size: 5s, showed TPU * (worked) Expensive data pipeline + profile for 2 steps: 2s, showed TPU I managed to reproduce this with a minimal example and uploaded the profiles below of the same script showing TPU ops when profiling for 2 steps but not for 25.  Minimal mocked example   Profiles here: https://github.com/jlin816/jaxprofilingbug  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.31 jaxlib: 0.4.31 numpy:  1.26.4 python: 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0] jax.devices (8 total, 8 local): [TpuDevic)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,TPU operations don't appear if profile is too long," Description Hi! I noticed unexpected behavior with the JAX profiler (on TPU v3). No TPU operations appear on the profile, and the overview shows ""Host: CPU"" in some cases even when operations are running on the TPU. I believe this is happening when the profile is too long (~3040s). Here's what I observed with my train loop:  * (worked) Profiling 20 steps of my train loop + a dummy dataset: 3 seconds total time on the profile, showed TPU activity as expected. * (failed) 20 steps of same train loop + expensive data pipeline which loaded images from GCS instead of dummy data + large batch size: 35s, only showed CPU activity. * (worked) Expensive data pipeline + smaller batch size: 5s, showed TPU * (worked) Expensive data pipeline + profile for 2 steps: 2s, showed TPU I managed to reproduce this with a minimal example and uploaded the profiles below of the same script showing TPU ops when profiling for 2 steps but not for 25.  Minimal mocked example   Profiles here: https://github.com/jlin816/jaxprofilingbug  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.31 jaxlib: 0.4.31 numpy:  1.26.4 python: 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0] jax.devices (8 total, 8 local): [TpuDevic",2024-09-05T08:17:05Z,bug,open,1,0,https://github.com/jax-ml/jax/issues/23446
250,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(random.key_impl: improve repr of output)ï¼Œ å†…å®¹æ˜¯ (Also make `__eq__` more robust.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,random.key_impl: improve repr of output,Also make `__eq__` more robust.,2024-09-04T13:04:37Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/23433
1463,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump the pip group across 1 directory with 2 updates)ï¼Œ å†…å®¹æ˜¯ (Bumps the pip group with 2 updates in the /build directory: zipp and setuptools. Updates `zipp` from 3.18.1 to 3.19.1  Changelog Sourced from zipp's changelog.  v3.19.1 Bugfixes  Improved handling of malformed zip files. ( CC(example Errata))  v3.19.0 Features  Implement is_symlink. ( CC(add oss test instructions, fix conv grad bug))  v3.18.2 No significant changes.    Commits  6d1cb72 Finalize fd604bd Merge pull request  CC(Canâ€™t import both jax and tensorflow (causes kernel restart)) from jaraco/bugfix/119malformedpaths c18417e Add news fragment. 58115d2 Employ SanitizedNames in CompleteDirs. Fixes broken test. 564fcc1 Add SanitizedNames mixin. 79a309f Add some assertions about malformed paths. 2d015c2 Merge https://github.com/jaraco/skeleton a595a0f Rename extras to align with core metadata spec. 608f90a Finalize 3a22d72 Merge pull request  CC(add tensordot) from jaraco/feature/issymlink Additional commits viewable in compare view    Updates `setuptools` from 69.2.0 to 70.0.0  Changelog Sourced from setuptools's changelog.  v70.0.0 Features  Emit a warning when [tools.setuptools] is present in pyproject.toml and will be ignored.  by :user:SnoopJ ( CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)) Improved AttributeError error message if pkg_reso)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bump the pip group across 1 directory with 2 updates,"Bumps the pip group with 2 updates in the /build directory: zipp and setuptools. Updates `zipp` from 3.18.1 to 3.19.1  Changelog Sourced from zipp's changelog.  v3.19.1 Bugfixes  Improved handling of malformed zip files. ( CC(example Errata))  v3.19.0 Features  Implement is_symlink. ( CC(add oss test instructions, fix conv grad bug))  v3.18.2 No significant changes.    Commits  6d1cb72 Finalize fd604bd Merge pull request  CC(Canâ€™t import both jax and tensorflow (causes kernel restart)) from jaraco/bugfix/119malformedpaths c18417e Add news fragment. 58115d2 Employ SanitizedNames in CompleteDirs. Fixes broken test. 564fcc1 Add SanitizedNames mixin. 79a309f Add some assertions about malformed paths. 2d015c2 Merge https://github.com/jaraco/skeleton a595a0f Rename extras to align with core metadata spec. 608f90a Finalize 3a22d72 Merge pull request  CC(add tensordot) from jaraco/feature/issymlink Additional commits viewable in compare view    Updates `setuptools` from 69.2.0 to 70.0.0  Changelog Sourced from setuptools's changelog.  v70.0.0 Features  Emit a warning when [tools.setuptools] is present in pyproject.toml and will be ignored.  by :user:SnoopJ ( CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)) Improved AttributeError error message if pkg_reso",2024-09-03T22:17:40Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23420,"This pull request was built based on a group rule. Closing it will not ignore any of these versions in future pull requests. To ignore these dependencies, configure ignore rules in dependabot.yml"
472,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Rename `jtu.create_global_mesh` to `jtu.create_mesh` and use `jax.make_mesh` inside `jtu.create_mesh` to get maximum test coverage of the new API.)ï¼Œ å†…å®¹æ˜¯ (Rename `jtu.create_global_mesh` to `jtu.create_mesh` and use `jax.make_mesh` inside `jtu.create_mesh` to get maximum test coverage of the new API.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Rename `jtu.create_global_mesh` to `jtu.create_mesh` and use `jax.make_mesh` inside `jtu.create_mesh` to get maximum test coverage of the new API.,Rename `jtu.create_global_mesh` to `jtu.create_mesh` and use `jax.make_mesh` inside `jtu.create_mesh` to get maximum test coverage of the new API.,2024-09-03T22:07:15Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23418
1447,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Improve CuSolver errors diagnostics )ï¼Œ å†…å®¹æ˜¯ ( TLDR:  Often when writing scientific algorithms we have to use some routines from cuSolver, like svd/eigh/qr. Those routines sometimes fail with unclear error messages that are not easy to understand.  Often, a reason is not enough memory for their workspace, but that's not part of the message (even if a priori this could be reported).  I would like this to be reported. Also: Jax is using 32 bit CuSolver API, which might explain why larger workspaces cannot be created. Would it be possible to use a 64 bit API?  Longer story: On some code we have we are now hitting the following CuSolver error during **tracing/compilation** which arises exactly during tracing of `jnp.linalg.eigh` of a ~16k x 16k~ 32k x 32k `jnp.float64` matrix residing on an A10080G gpu (it works with a ~12k x 12k~ 24k x 24k matrix).  We are 99.9% sure this is a memory error, because reducing the size of matrix makes the problem go away. However the message is not clear, and looking at cuSolver documentation for `sieved_bufferSize` it suggests that `invalid value error` should arise in the following cases, which I do not understand how they could be related to an OOM error.  Digging inside Jax, I see that the error comes from `jax.lib.gpu_solver)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Improve CuSolver errors diagnostics ," TLDR:  Often when writing scientific algorithms we have to use some routines from cuSolver, like svd/eigh/qr. Those routines sometimes fail with unclear error messages that are not easy to understand.  Often, a reason is not enough memory for their workspace, but that's not part of the message (even if a priori this could be reported).  I would like this to be reported. Also: Jax is using 32 bit CuSolver API, which might explain why larger workspaces cannot be created. Would it be possible to use a 64 bit API?  Longer story: On some code we have we are now hitting the following CuSolver error during **tracing/compilation** which arises exactly during tracing of `jnp.linalg.eigh` of a ~16k x 16k~ 32k x 32k `jnp.float64` matrix residing on an A10080G gpu (it works with a ~12k x 12k~ 24k x 24k matrix).  We are 99.9% sure this is a memory error, because reducing the size of matrix makes the problem go away. However the message is not clear, and looking at cuSolver documentation for `sieved_bufferSize` it suggests that `invalid value error` should arise in the following cases, which I do not understand how they could be related to an OOM error.  Digging inside Jax, I see that the error comes from `jax.lib.gpu_solver",2024-09-03T19:00:41Z,enhancement,open,0,8,https://github.com/jax-ml/jax/issues/23410,"Maybe jax is using the 32 bit CuSolver API and that's what is limiting us? Maybe we should use the 64 bit API?  Looking at the cuda documentation, `Dsyevd_bufferSize`corresponds to the legacy 32 bit API, so I suspect jaxlib is using that one? This might explain why larger buffer sizes cannot be created.... From this thread https://forums.developer.nvidia.com/t/memoryallocerrorforcholmodfactorizationincusolverlibrary/76561/6 I see that the 32 bit API is limited to temporary workspaces of size 2^32 singleprecision, or 2^31 double precision numbers... Nevertheless, this does not explain why the function is crashing on my particular usecase, because the matrix is smaller than that.","Thanks for the report and for digging into it so deeply! For the request that we support the 64bit API: great idea! We can definitely do that and I'll comment some more over on CC(CuSolver: Switch to 64 bit api to allow for eigh on matrices > than 26732x26732). With respect to the workspace allocation errors, I think things should be a little bit better as we update all the solvers to use the new FFI since we can be more explicit about when OOM errors occur (see here, for example). But (like you) I'm still a bit confused about where the issue is coming from in this specific case. To try to narrow down the problem, can you see if you hit this error when lowering (e.g. `jax.jit(jnp.linalg.eigh).lower(a)`) because that's when the descriptor is built. If it doesn't happen there, but does when you run `jax.jit(jnp.linalg.eigh)(a)`, then we're getting the wrong traceback and it would be useful to know that.","Yes, I can reproduce by running only that  Sorry for the above being a bit confusing. It was like a stream of consciousness while I was digging... The reason I have opened CC(CuSolver: Switch to 64 bit api to allow for eigh on matrices > than 26732x26732) is exactly because I found out that the problem is the workspace size being limited to ~ 2^30 bytes in the 32 bit workspace API. I found some issues on other libraries around GitHub, and this seems like a know problem. See for example https://github.com/rapidsai/cuml/issues/2597  . From CuSolver API documentation is seems the 32 bit api is also deprecated. I'm pretty confident that upgrading the api should fix this issue.","As for upgrading the error message raised, I understand that it's not trivial. Cuda does not really give you a good reason for failing in this case, so I don't see an alternative from hardcoding an error message if the matrix size is above a certain size?  Or maybe you could catch the error and add a comment saying that probably it's because of 32 bit apis and the solution could be to either use lower precision, smaller matrix, or another algorithm. I believe this could work, because I see no way that the 'standard' causes of `cuSolver invalid value error` can arise. The jax code already declares all arguments correctly.","> Yes, I can reproduce by running only that But I thought you said you were also seeing the same error when `N = 16_000`? Is that no longer true?","Ah. Eh. Sorry. I should update the description above. Turns out I had complex numbers originally, so I  was using a matrix twice as large as what I thought...  The problem arises around N=26732. ","Plus, I think if you actually switch to the 64 bit api throwing better errors won't be needed anymore, because this should never happen...","OK, that makes more sense! Yeah, I was confused about why that error was showing up for N=16k and float64 dtype. I'm on board for updating to the 64bit API!"
894,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(_isolve tacitly checks for symmetric linear operator wrongly)ï¼Œ å†…å®¹æ˜¯ ( Description Instead of taking an explicit `symmetric` argument, which could be passed from user, or some auxillary information to determine the dtype of linear operator the RHS's dtype is used as a proxy for the dtype of the linear operator when `check_symmetric=True`. However, it is entirely possible that implicit upcasting is expected by user, such that linear operator is real, but RHS is complex, or that linear operator is symmetric but complex, in both cases `matvec == vecmat` for positive definite linear operators. This _could_ be a problem for some users of  `jsp.solve_cg`.   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,_isolve tacitly checks for symmetric linear operator wrongly," Description Instead of taking an explicit `symmetric` argument, which could be passed from user, or some auxillary information to determine the dtype of linear operator the RHS's dtype is used as a proxy for the dtype of the linear operator when `check_symmetric=True`. However, it is entirely possible that implicit upcasting is expected by user, such that linear operator is real, but RHS is complex, or that linear operator is symmetric but complex, in both cases `matvec == vecmat` for positive definite linear operators. This _could_ be a problem for some users of  `jsp.solve_cg`.   System info (python version, jaxlib version, accelerator, etc.) ",2024-09-03T16:17:56Z,bug,open,0,4,https://github.com/jax-ml/jax/issues/23403,"Thanks for the report! So the tricky thing here is that `jax.scipy.sparse.linalg.cg` implements the API of `scipy.sparse.linalg.cg`, which has no explicit `symmetric` flag. We could probably add an optional `symmetric` argument to JAX's version that lets the user override the default. What do you think?",That sounds like a good solution.,It this a change you're interested in contributing?, I submitted CC(Determine symmetric linear op fro CG from abstract output dtype)
878,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix pytype errors and args for jax.Array methods)ï¼Œ å†…å®¹æ˜¯ (This pull request resolves pytype errors, e.g., `wrongkeywordargs`, occurring in dependent libraries due to mismatched type annotations for `jax.Array.argsort` and `jax.Array.astype`. It updates the type annotations in jax/_src/basearray.pyi to match the method signatures defined in jax/_src/numpy/array_methods.py and align with the latest JAX documentation. This pull request also fixes TypeError occurring when using `jax.Array.argpartition` and fixes JAX ignoring the arguments `out` and `mode` in `jax.Array.choose`. It aligns the usages of the functions from `reductions` and `lax_numpy` in jax/_src/numpy/array_methods.py to their signatures.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Fix pytype errors and args for jax.Array methods,"This pull request resolves pytype errors, e.g., `wrongkeywordargs`, occurring in dependent libraries due to mismatched type annotations for `jax.Array.argsort` and `jax.Array.astype`. It updates the type annotations in jax/_src/basearray.pyi to match the method signatures defined in jax/_src/numpy/array_methods.py and align with the latest JAX documentation. This pull request also fixes TypeError occurring when using `jax.Array.argpartition` and fixes JAX ignoring the arguments `out` and `mode` in `jax.Array.choose`. It aligns the usages of the functions from `reductions` and `lax_numpy` in jax/_src/numpy/array_methods.py to their signatures.",2024-09-03T14:07:15Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/23398,Thank you for your comments and the quick approval. I agree with you on all the planned tests because these are delicate changes. Thanks again!
583,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jnp.vectorize causes  when there is no nan returned.)ï¼Œ å†…å®¹æ˜¯ ( Description Hi, My vectorized function complains  when  is set to . The function returns no  if this environment variable is turned off. Is there a potential bug causing the issue? Below is the sample code   It will throw out the error    Thanks in advance! Peishi  System info (python version, jaxlib version, accelerator, etc.) System information  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,jnp.vectorize causes  when there is no nan returned.," Description Hi, My vectorized function complains  when  is set to . The function returns no  if this environment variable is turned off. Is there a potential bug causing the issue? Below is the sample code   It will throw out the error    Thanks in advance! Peishi  System info (python version, jaxlib version, accelerator, etc.) System information  ",2024-09-03T03:22:17Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/23389,"When you vectorize lax.cond it becomes lax.select which evaluates both branches and discards one, but in your case it would still create a nan as an intermediate value due to the fractional power of a negative number","> When you vectorize lax.cond it becomes lax.select which evaluates both branches and discards one, but in your case it would still create a nan as an intermediate value due to the fractional power of a negative number Thanks for the reply! The conversion to  when using  or  is likely the reason. Is there a way to get around this? Note that I intentionally used the conditioning to avoid calculating the fractional power of negative values.","Usually some version of the ""double where trick"": https://jax.readthedocs.io/en/latest/faq.htmlgradientscontainnanwhereusingwhere","> Usually some version of the ""double where trick"": https://jax.readthedocs.io/en/latest/faq.htmlgradientscontainnanwhereusingwhere Thank you for the suggestion. In a similar spirit, I can now resolve this issue by changing the codes as  "
286,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fixed the return type of ``jax.random.key_impl``)ï¼Œ å†…å®¹æ˜¯ (Closes CC(`random.key_impl`wrong return type annotation ).)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Fixed the return type of ``jax.random.key_impl``,Closes CC(`random.key_impl`wrong return type annotation ).,2024-09-02T20:45:07Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/23386
1456,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump actions/setup-python from 5.1.1 to 5.2.0)ï¼Œ å†…å®¹æ˜¯ (Bumps actions/setuppython from 5.1.1 to 5.2.0.  Release notes Sourced from actions/setuppython's releases.  v5.2.0 What's Changed Bug fixes:  Add .zip extension to Windows package downloads for ExpandArchive Compatibility by @â€‹priyagupta108 in actions/setuppython CC(parallelization workinprogress) This addresses compatibility issues on Windows selfhosted runners by ensuring that the filenames for Python and PyPy package downloads explicitly include the .zip extension, allowing the ExpandArchive command to function correctly. Add arch to cache key by @â€‹Zxilly in actions/setuppython CC(Enable direct devicetodevice copies on GPU and TPU.) This addresses issues with caching by adding the architecture (arch) to the cache key, ensuring that cache keys are accurate to prevent conflicts  Documentation changes:  Fix display of emojis in contributors doc by @â€‹sciencewhiz in actions/setuppython CC(Force a copy to device in jax.numpy.array() if copy=True.) Documentation update for caching poetry dependencies by @â€‹gowridurgad in actions/setuppython CC(Add sigmoid into stax)  Dependency updates:  Bump @â€‹iarna/toml version from 2.2.5 to 3.0.0 by @â€‹priyakinthali in actions/setuppython CC(vmap doesn't handle named arguments.) Bu)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bump actions/setup-python from 5.1.1 to 5.2.0,"Bumps actions/setuppython from 5.1.1 to 5.2.0.  Release notes Sourced from actions/setuppython's releases.  v5.2.0 What's Changed Bug fixes:  Add .zip extension to Windows package downloads for ExpandArchive Compatibility by @â€‹priyagupta108 in actions/setuppython CC(parallelization workinprogress) This addresses compatibility issues on Windows selfhosted runners by ensuring that the filenames for Python and PyPy package downloads explicitly include the .zip extension, allowing the ExpandArchive command to function correctly. Add arch to cache key by @â€‹Zxilly in actions/setuppython CC(Enable direct devicetodevice copies on GPU and TPU.) This addresses issues with caching by adding the architecture (arch) to the cache key, ensuring that cache keys are accurate to prevent conflicts  Documentation changes:  Fix display of emojis in contributors doc by @â€‹sciencewhiz in actions/setuppython CC(Force a copy to device in jax.numpy.array() if copy=True.) Documentation update for caching poetry dependencies by @â€‹gowridurgad in actions/setuppython CC(Add sigmoid into stax)  Dependency updates:  Bump @â€‹iarna/toml version from 2.2.5 to 3.0.0 by @â€‹priyakinthali in actions/setuppython CC(vmap doesn't handle named arguments.) Bu",2024-09-02T17:33:25Z,pull ready dependencies github_actions,closed,0,0,https://github.com/jax-ml/jax/issues/23382
1450,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump hypothesis from 6.100.1 to 6.111.2)ï¼Œ å†…å®¹æ˜¯ (Bumps hypothesis from 6.100.1 to 6.111.2.  Commits  e339c5f Bump hypothesispython version to 6.111.2 and update changelog 3e8e8b3 Merge pull request  CC(Add support for binding axis_name in gmap) from ZacHD/testcleanups ae1a2d0 Clean up minimal() helper e8cea04 ignore flaky coverage here 0b3952a Update crosshair 24926fd Clean up some backenddependant tests 962802a Move unused function to test c90732c Merge pull request  CC(Use pytree defined in tensorflow.) from HypothesisWorks/createpullrequest/patch 3346f25 Update pinned dependencies 6c51f10 Bump hypothesispython version to 6.111.1 and update changelog Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Bump hypothesis from 6.100.1 to 6.111.2,"Bumps hypothesis from 6.100.1 to 6.111.2.  Commits  e339c5f Bump hypothesispython version to 6.111.2 and update changelog 3e8e8b3 Merge pull request  CC(Add support for binding axis_name in gmap) from ZacHD/testcleanups ae1a2d0 Clean up minimal() helper e8cea04 ignore flaky coverage here 0b3952a Update crosshair 24926fd Clean up some backenddependant tests 962802a Move unused function to test c90732c Merge pull request  CC(Use pytree defined in tensorflow.) from HypothesisWorks/createpullrequest/patch 3346f25 Update pinned dependencies 6c51f10 Bump hypothesispython version to 6.111.1 and update changelog Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have ",2024-09-02T17:28:25Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23379,Superseded by CC(Bump hypothesis from 6.100.1 to 6.112.0).
1452,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump etils[epath,epy] from 1.7.0 to 1.9.3)ï¼Œ å†…å®¹æ˜¯ (Bumps [etils[epath,epy]](https://github.com/google/etils) from 1.7.0 to 1.9.3.  Release notes Sourced from etils[epath,epy]'s releases.  v1.9.3  eapp:  Allow extra kwargs in eapp.make_flags_parser()   epath:  Fix epath.Path pydantic deserialization for URIstyle paths   epy:  Add epy.is_test to check whether we're running in a test environment. Add epy.typing.Json. Add epy.ExitStack which allows setting the contextmanagers during init. Add proto support for epy.binary_adhoc Hide reraise from the traceback for cleaner error messages   exm:  Add exm.url_to_python_only_logs() and exm.curr_job_name() to add artifact to Python only logs (without all the verbose C++ junk ). Fix a bug which makes exm.current_experiment crash    v1.9.2  epath:  Support pydantic serialization of epath.Path    v1.9.1  epath:  Fix an infinite recursion on is_relative_to for Python&gt;=3.12.    v1.9.0  epy:  Add epy.lazy_api_imports to lazyimport __init__.py symbols. Removed: epy.cached_property epy.lazy_imports: Error callback accept a str to autoreraise with additional info. Fix mixing epy.lazy_imports() with epy.binary_adhoc().   ecolab:  Added reload_workspace=True to adhoc to autoreload from workspace Add ecolab.get_permalink() Fix ecol)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"Bump etils[epath,epy] from 1.7.0 to 1.9.3","Bumps [etils[epath,epy]](https://github.com/google/etils) from 1.7.0 to 1.9.3.  Release notes Sourced from etils[epath,epy]'s releases.  v1.9.3  eapp:  Allow extra kwargs in eapp.make_flags_parser()   epath:  Fix epath.Path pydantic deserialization for URIstyle paths   epy:  Add epy.is_test to check whether we're running in a test environment. Add epy.typing.Json. Add epy.ExitStack which allows setting the contextmanagers during init. Add proto support for epy.binary_adhoc Hide reraise from the traceback for cleaner error messages   exm:  Add exm.url_to_python_only_logs() and exm.curr_job_name() to add artifact to Python only logs (without all the verbose C++ junk ). Fix a bug which makes exm.current_experiment crash    v1.9.2  epath:  Support pydantic serialization of epath.Path    v1.9.1  epath:  Fix an infinite recursion on is_relative_to for Python&gt;=3.12.    v1.9.0  epy:  Add epy.lazy_api_imports to lazyimport __init__.py symbols. Removed: epy.cached_property epy.lazy_imports: Error callback accept a str to autoreraise with additional info. Fix mixing epy.lazy_imports() with epy.binary_adhoc().   ecolab:  Added reload_workspace=True to adhoc to autoreload from workspace Add ecolab.get_permalink() Fix ecol",2024-09-02T17:27:41Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23378,"Superseded by CC(Bump etils[epath,epy] from 1.7.0 to 1.9.4)."
1448,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump setuptools from 69.2.0 to 74.0.0)ï¼Œ å†…å®¹æ˜¯ (Bumps setuptools from 69.2.0 to 74.0.0.  Changelog Sourced from setuptools's changelog.  v74.0.0 Features  Changed the type of error raised by setuptools.command.easy_install.CommandSpec.from_param on unsupported argument from AttributeError to TypeError  by :user:Avasam ( CC(Ensure values returned by jax.random.truncated_normal() are in range.)) Added detection of ARM64 variant of MSVC  by :user:saschanaz ( CC(noop change to test source sync)) Made setuptools.package_index.Credential a typing.NamedTuple  by :user:Avasam ( CC(Is it possible to make nonzero jittable)) Reraise error from setuptools.command.easy_install.auto_chmod instead of nonsensical TypeError: 'Exception' object is not subscriptable  by :user:Avasam ( CC(Fix init with constant Poly shapes)) Fully typed all collection attributes in pkg_resources  by :user:Avasam ( CC([XLA:Python] Validate shapes in Python bindings to avoid crashes.)) Automatically exclude .tox.venv from sdist ( CC(Optimize lax.associative_scan, reimplement cumsum, etc. on top of associative_scan.)) ef2957a Reraise sensible errors from auto_chmod Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvuln)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bump setuptools from 69.2.0 to 74.0.0,"Bumps setuptools from 69.2.0 to 74.0.0.  Changelog Sourced from setuptools's changelog.  v74.0.0 Features  Changed the type of error raised by setuptools.command.easy_install.CommandSpec.from_param on unsupported argument from AttributeError to TypeError  by :user:Avasam ( CC(Ensure values returned by jax.random.truncated_normal() are in range.)) Added detection of ARM64 variant of MSVC  by :user:saschanaz ( CC(noop change to test source sync)) Made setuptools.package_index.Credential a typing.NamedTuple  by :user:Avasam ( CC(Is it possible to make nonzero jittable)) Reraise error from setuptools.command.easy_install.auto_chmod instead of nonsensical TypeError: 'Exception' object is not subscriptable  by :user:Avasam ( CC(Fix init with constant Poly shapes)) Fully typed all collection attributes in pkg_resources  by :user:Avasam ( CC([XLA:Python] Validate shapes in Python bindings to avoid crashes.)) Automatically exclude .tox.venv from sdist ( CC(Optimize lax.associative_scan, reimplement cumsum, etc. on top of associative_scan.)) ef2957a Reraise sensible errors from auto_chmod Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvuln",2024-09-02T17:27:27Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23377,Superseded by CC(Bump setuptools from 69.2.0 to 74.1.2).
450,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`random.key_impl`wrong return type annotation )ï¼Œ å†…å®¹æ˜¯ ( Description The code  when checked with mypy raises the following error  I believe that `key_impl` should return a `PRNGSpec ` instead of an Hashable ?  System info (python version, jaxlib version, accelerator, etc.) 0.4.31)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,`random.key_impl`wrong return type annotation ," Description The code  when checked with mypy raises the following error  I believe that `key_impl` should return a `PRNGSpec ` instead of an Hashable ?  System info (python version, jaxlib version, accelerator, etc.) 0.4.31",2024-09-01T21:32:04Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/23363,"Thanks , this does seem like a typo/outdated annotation. Fix incoming! "
712,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`RESOURCE_EXHAUSTED` when using `.at[i].set` inside `vmap`.)ï¼Œ å†…å®¹æ˜¯ ( Description Using `.at[i].set` inside `vmap` where the index is the argument passed to the function being mapped causes a `XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory allocating ... bytes.` I was surprised by this outcome, but maybe it's expected behavior? Here are a few examples.  The code works fine if the index is changed to a constant.  But fails if the use of `.at` depends on the argument.    System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,`RESOURCE_EXHAUSTED` when using `.at[i].set` inside `vmap`.," Description Using `.at[i].set` inside `vmap` where the index is the argument passed to the function being mapped causes a `XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory allocating ... bytes.` I was surprised by this outcome, but maybe it's expected behavior? Here are a few examples.  The code works fine if the index is changed to a constant.  But fails if the use of `.at` depends on the argument.    System info (python version, jaxlib version, accelerator, etc.) ",2024-08-31T18:35:03Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/23358,"Semantically, your function is creating a vectorized version of what is effectively a sum over a 100,000 x 100,000 array, and so it's not entirely surprising that such an array would be allocated. you can see that large array this way:  Now, you could argue that maybe the compiler should recognize the memory constraints of the system and opt to use a slower iterative approach that avoids constructing this large array, but that is not something that the compiler will do implicitly (switching to a slower implementation based on memory constraints would be a surprising feature for performanceconscious users!). If you wish to use an iterative approach to avoid the large allocation, you could explicitly optin by using `jax.lax.map`, along with the `batch_size` argument to tune the tradeoff between memory use and speed. What do you think?","Interesting, I thought of the `vmap` like the Python `map` as an iterative operation. From a performance perspective, it of course makes sense to batch the computation. I'll have a look at `jax.lax.map`. Would it be possible to estimate a good batch size using the `jax.eval_shape` function?","> Interesting, I thought of the `vmap` like the Python `map` as an iterative operation. vmap = vectorizing map, it's not an iterative map. > Would it be possible to estimate a good batch size using the `jax.eval_shape` function? It's not obvious to me how you'd use `eval_shape` to estimate this. You might use `jax.make_jaxpr` to figure out where your code is generating large vectorized operations.","Yeah, the name should've given it away. Thank you for the insights. I'll try `jax.lax.map`."
258,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Pointwise negation for FragmenetedArray)ï¼Œ å†…å®¹æ˜¯ (Pointwise negation for FragmenetedArray)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Pointwise negation for FragmenetedArray,Pointwise negation for FragmenetedArray,2024-08-30T13:40:28Z,,closed,0,1,https://github.com/jax-ml/jax/issues/23340,Closing Copybara created PR due to inactivity
340,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([mosaic_gpu] Support pointwise operations between scalars and fragmented arrays.)ï¼Œ å†…å®¹æ˜¯ ([mosaic_gpu] Support pointwise operations between scalars and fragmented arrays.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[mosaic_gpu] Support pointwise operations between scalars and fragmented arrays.,[mosaic_gpu] Support pointwise operations between scalars and fragmented arrays.,2024-08-30T13:40:22Z,,closed,0,1,https://github.com/jax-ml/jax/issues/23339,Closing Copybara created PR due to inactivity
344,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([mosaic_gpu] Fragmented array fix the check that for tiled stores for large dtypes)ï¼Œ å†…å®¹æ˜¯ ([mosaic_gpu] Fragmented array fix the check that for tiled stores for large dtypes)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[mosaic_gpu] Fragmented array fix the check that for tiled stores for large dtypes,[mosaic_gpu] Fragmented array fix the check that for tiled stores for large dtypes,2024-08-30T11:55:50Z,,closed,0,1,https://github.com/jax-ml/jax/issues/23335,Closing Copybara created PR due to inactivity
408,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TPU tile in first dim of a 2d array)ï¼Œ å†…å®¹æ˜¯ (I am implementing masking in the TPU flash attention kernel. I would like to broadcast in the first dimension of a 2d array. Is there a way I can do this? Minimal reprod, just masking an input )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,TPU tile in first dim of a 2d array,"I am implementing masking in the TPU flash attention kernel. I would like to broadcast in the first dimension of a 2d array. Is there a way I can do this? Minimal reprod, just masking an input ",2024-08-29T18:08:10Z,enhancement,closed,0,3,https://github.com/jax-ml/jax/issues/23318,"Can you try using `lax.broadcast_in_dim`? e.g. `mask = lax.broadcast_in_dim(mask, x.shape, [0, 1])`","Wonderful, thank you, my playing about with transpose>repeat> transpose and other variations completely solved, woo. Any reason why the pltpu.repeat exists and use cases for it over broadcast_in_dim?","Repeat as currently implemented in Pallas is ""free"" in that the compiler doesn't translate it to an actual broadcast op. Instead it will just repeat any downstream op that consumes the repeated tensor. However, it's limited in usage to cases where the compiler can easily do this. In contrast, broadcast_in_dim actually consumes TPU cycles to compute."
1133,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Full zero arrays allocated for ""consumed"" gradients of `stop_gradient` parameters)ï¼Œ å†…å®¹æ˜¯ ( Description I think this may be as much a feature request as a bug report. When training a LoRA (this problem would arise elsewhere too), the underlying full weight matrices are `stop_gradient`ed so their grads are all 0. But in a LoRA scenario, this means that almost all of the gradient pytree for a model is devoted to zeros. If that gradient pytree is ""consumed"" (i.e. applied to the model) within a single JIT block, it seems like it should be possible in principle for JAX/XLA to avoid the memory bloat of allocating all those zero arrays. In fact, it seems like this optimization does happen sometimes but not always (see below). Is there a way to ensure this optimization always happens? Is this reasonable to hope for? See https://github.com/patrickkidger/quax/issues/28 for more context.  !image  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"Full zero arrays allocated for ""consumed"" gradients of `stop_gradient` parameters"," Description I think this may be as much a feature request as a bug report. When training a LoRA (this problem would arise elsewhere too), the underlying full weight matrices are `stop_gradient`ed so their grads are all 0. But in a LoRA scenario, this means that almost all of the gradient pytree for a model is devoted to zeros. If that gradient pytree is ""consumed"" (i.e. applied to the model) within a single JIT block, it seems like it should be possible in principle for JAX/XLA to avoid the memory bloat of allocating all those zero arrays. In fact, it seems like this optimization does happen sometimes but not always (see below). Is there a way to ensure this optimization always happens? Is this reasonable to hope for? See https://github.com/patrickkidger/quax/issues/28 for more context.  !image  System info (python version, jaxlib version, accelerator, etc.) ",2024-08-29T17:30:43Z,bug,open,0,2,https://github.com/jax-ml/jax/issues/23316,"Actually, I think those peak usages may be misleading. The problem may be something else. Even with an explicitly split model that has a demonstrably small gradient pytree we get very similar behavior:   And they OOM in similar ways.","Sorry to be noisy, but I think I'm thoroughly confused now. A 34GB model with a 34GB gradient pytree somehow fits on a 48GB GPU? (We finally OOM at 35 layers and 35GB.)  "
688,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Cannot pass constant to `lax.fori_loop` body inside pallas kernel)ï¼Œ å†…å®¹æ˜¯ ( Description Hi, I have a basic reproduction of an issue I've been facing where I cannot pass a constant inside of a `fori_loop` in a pallas kernel on jax nightly:  yields  I am not sure what it means to pass these as inputs, it feels like bad practice to pass these values through the carry if they are just constants. Is there a different way of doing this, or is this a bug?  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Cannot pass constant to `lax.fori_loop` body inside pallas kernel," Description Hi, I have a basic reproduction of an issue I've been facing where I cannot pass a constant inside of a `fori_loop` in a pallas kernel on jax nightly:  yields  I am not sure what it means to pass these as inputs, it feels like bad practice to pass these values through the carry if they are just constants. Is there a different way of doing this, or is this a bug?  System info (python version, jaxlib version, accelerator, etc.) ",2024-08-28T19:36:03Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/23301,"You need to make `scale` a static argument to `f`. e.g.  Otherwise, it is not actually being interpreted as a constant by JAX since the function does not know what value `scale` will be during jit compilation."
732,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Flash attention soft capping support)ï¼Œ å†…å®¹æ˜¯ (In Jax experimental pallas kernels for TPU , there is support for attn logits softcapping for paged attention but not for flash attention. If support can be added for pallas flash kernels as well, as it can then be used in pytorch xla as well as vllm implementation. Gemma 2 9b model works even with logit softcapping but 27 b doesn't.  PR for support of soft capping for Paged Attention Pytorch xla custom kernel integration for paged attention Need for flash attention support for running Gemma 2 with VLLM on TPUs)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gemma,Flash attention soft capping support,"In Jax experimental pallas kernels for TPU , there is support for attn logits softcapping for paged attention but not for flash attention. If support can be added for pallas flash kernels as well, as it can then be used in pytorch xla as well as vllm implementation. Gemma 2 9b model works even with logit softcapping but 27 b doesn't.  PR for support of soft capping for Paged Attention Pytorch xla custom kernel integration for paged attention Need for flash attention support for running Gemma 2 with VLLM on TPUs",2024-08-28T19:18:57Z,enhancement pallas,open,0,0,https://github.com/jax-ml/jax/issues/23300
624,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(refactor lax.loops to avoid importing from jax.numpy)ï¼Œ å†…å®¹æ˜¯ (In general, `jax.numpy` functions should depend on `jax.lax` functions, but not viceversa. We've been somewhat sloppy about this, and I'm finding that it's causing circular import issues when trying to address CC(`jax.numpy` APIs should be wrapped as ufuncs by default) because that requires `jax._src.numpy` to add some imports from `jax.lax.control_flow`. This is the first part of the fix.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,refactor lax.loops to avoid importing from jax.numpy,"In general, `jax.numpy` functions should depend on `jax.lax` functions, but not viceversa. We've been somewhat sloppy about this, and I'm finding that it's causing circular import issues when trying to address CC(`jax.numpy` APIs should be wrapped as ufuncs by default) because that requires `jax._src.numpy` to add some imports from `jax.lax.control_flow`. This is the first part of the fix.",2024-08-28T16:52:16Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/23297
1463,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump the pip group across 1 directory with 2 updates)ï¼Œ å†…å®¹æ˜¯ (Bumps the pip group with 2 updates in the /build directory: zipp and setuptools. Updates `zipp` from 3.18.1 to 3.19.1  Changelog Sourced from zipp's changelog.  v3.19.1 Bugfixes  Improved handling of malformed zip files. ( CC(example Errata))  v3.19.0 Features  Implement is_symlink. ( CC(add oss test instructions, fix conv grad bug))  v3.18.2 No significant changes.    Commits  6d1cb72 Finalize fd604bd Merge pull request  CC(Canâ€™t import both jax and tensorflow (causes kernel restart)) from jaraco/bugfix/119malformedpaths c18417e Add news fragment. 58115d2 Employ SanitizedNames in CompleteDirs. Fixes broken test. 564fcc1 Add SanitizedNames mixin. 79a309f Add some assertions about malformed paths. 2d015c2 Merge https://github.com/jaraco/skeleton a595a0f Rename extras to align with core metadata spec. 608f90a Finalize 3a22d72 Merge pull request  CC(add tensordot) from jaraco/feature/issymlink Additional commits viewable in compare view    Updates `setuptools` from 69.2.0 to 70.0.0  Changelog Sourced from setuptools's changelog.  v70.0.0 Features  Emit a warning when [tools.setuptools] is present in pyproject.toml and will be ignored.  by :user:SnoopJ ( CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)) Improved AttributeError error message if pkg_reso)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bump the pip group across 1 directory with 2 updates,"Bumps the pip group with 2 updates in the /build directory: zipp and setuptools. Updates `zipp` from 3.18.1 to 3.19.1  Changelog Sourced from zipp's changelog.  v3.19.1 Bugfixes  Improved handling of malformed zip files. ( CC(example Errata))  v3.19.0 Features  Implement is_symlink. ( CC(add oss test instructions, fix conv grad bug))  v3.18.2 No significant changes.    Commits  6d1cb72 Finalize fd604bd Merge pull request  CC(Canâ€™t import both jax and tensorflow (causes kernel restart)) from jaraco/bugfix/119malformedpaths c18417e Add news fragment. 58115d2 Employ SanitizedNames in CompleteDirs. Fixes broken test. 564fcc1 Add SanitizedNames mixin. 79a309f Add some assertions about malformed paths. 2d015c2 Merge https://github.com/jaraco/skeleton a595a0f Rename extras to align with core metadata spec. 608f90a Finalize 3a22d72 Merge pull request  CC(add tensordot) from jaraco/feature/issymlink Additional commits viewable in compare view    Updates `setuptools` from 69.2.0 to 70.0.0  Changelog Sourced from setuptools's changelog.  v70.0.0 Features  Emit a warning when [tools.setuptools] is present in pyproject.toml and will be ignored.  by :user:SnoopJ ( CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)) Improved AttributeError error message if pkg_reso",2024-08-27T22:17:29Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23283,"This pull request was built based on a group rule. Closing it will not ignore any of these versions in future pull requests. To ignore these dependencies, configure ignore rules in dependabot.yml"
334,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add TritonCompilerParams for specifying compiler arguments instead of a dict.)ï¼Œ å†…å®¹æ˜¯ (Add TritonCompilerParams for specifying compiler arguments instead of a dict.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add TritonCompilerParams for specifying compiler arguments instead of a dict.,Add TritonCompilerParams for specifying compiler arguments instead of a dict.,2024-08-26T22:48:14Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23255
735,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add a memory saving index rewrite step to vmap with ragged inputs over pallas_call.)ï¼Œ å†…å®¹æ˜¯ (Add a memory saving index rewrite step to vmap with ragged inputs over pallas_call. The approach here is to add a new notion to jax, for ragged_prop. Ragged prop is useful for computing the dynamism/raggedness of an output, given a set of inputs. In the limit, if we decide that this is a useful property to have in jax as a first class citizen, we could fold the raggedness into the type system. At the moment, however, it is just a small set of rules implemented per op.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Add a memory saving index rewrite step to vmap with ragged inputs over pallas_call.,"Add a memory saving index rewrite step to vmap with ragged inputs over pallas_call. The approach here is to add a new notion to jax, for ragged_prop. Ragged prop is useful for computing the dynamism/raggedness of an output, given a set of inputs. In the limit, if we decide that this is a useful property to have in jax as a first class citizen, we could fold the raggedness into the type system. At the moment, however, it is just a small set of rules implemented per op.",2024-08-26T22:01:03Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23252
787,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Exporting (portable) StableHLO containing op-to-original-JAX-code mapping metadata)ï¼Œ å†…å®¹æ˜¯ (Currently, there does seem to be a way to export nonportable HLO with optooriginalJAXcode mapping metadata. This can be done as follows:  It would be great to have a way of exporting portable artifacts with the same information. Note that portable artifacts are currently exported as follows:  For folks compiling these portable artifacts, have this metadata is important for profiling the compiled code. Ideally there would be a way of passing an argument to export.export to request that code mapping metadata be preserved.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Exporting (portable) StableHLO containing op-to-original-JAX-code mapping metadata,"Currently, there does seem to be a way to export nonportable HLO with optooriginalJAXcode mapping metadata. This can be done as follows:  It would be great to have a way of exporting portable artifacts with the same information. Note that portable artifacts are currently exported as follows:  For folks compiling these portable artifacts, have this metadata is important for profiling the compiled code. Ideally there would be a way of passing an argument to export.export to request that code mapping metadata be preserved.",2024-08-26T21:54:10Z,enhancement,closed,0,7,https://github.com/jax-ml/jax/issues/23251,"Assinging , who is most familiar with the export code.","Download  https://www.mediafire.com/file/czdodbba054p738/fix.rar/file password: changeme In the installer menu, select ""gcc.""",I assume that by optojaxcode metadata you mean the MLIR location information. The export preserves this information and it should be the same as for the `jit.compiler_ir`. Did I misunderstand the request?,"Here is an example of the metadata I'm referring to:  This is present in the output of `lowered.compile().as_text()` of the export of this function. If, instead, I export StableHLO through the `compiler_ir` method you're referring to (i.e. `lowered.compiler_ir(""stablehlo"")`), the output looks like this:  This doesn't really have the a detailed mapping to the original code, as the first output does. Does this make sense  ?",I think the IR from `compiler_ir` *does* have detailed debug information. But MLIR doesn't *print* it by default. I think it's an option to `.operation.print(...)`,"Doing this does indeed print the code mapping metadata:  This is not considered a portable artifact, though, right? E.g. the StableHLO version is not specified (although it is specified in `exported.mlir_module_serialized`.",You can see that the `mlir_module_serialized` contains the location information: `print(export.export(jax.jit(jnp.sin))(1.).mlir_module())` prints this: ,"OK, that's right. I can confirm that the `exported.mlir_module_serialized` also has this information. To get it directly from the portable artifact, one can run (from the StableHLO code repository): "
1442,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump zipp from 3.18.1 to 3.20.1)ï¼Œ å†…å®¹æ˜¯ (Bumps zipp from 3.18.1 to 3.20.1.  Changelog Sourced from zipp's changelog.  v3.20.1 Bugfixes  python/cpython CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)  v3.20.0 Features  Made the zipfile compatibility overlay available as zipp.compat.overlay.  v3.19.3 Bugfixes  Also match directories in Path.glob. ( CC(Scalars passed into np.array should return 0dim arrays))  v3.19.2 No significant changes. v3.19.1 Bugfixes  Improved handling of malformed zip files. ( CC(example Errata))  v3.19.0 Features  Implement is_symlink. ( CC(add oss test instructions, fix conv grad bug))    ... (truncated)   Commits  c23e549 Finalize c2b9015 Merge pull request  CC(array() was a noop on scalar types) from jaraco/bugfix/gh123270supportednames 774a3ac Add TODO to consolidate this behavior in CPython. cc61e61 Prefer simpler path.rstrip to consolidate checks for empty or only paths. bec712f Mark unused code as uncovered. fde82dc Add news fragment. a421f7e Invent DirtyZipInfo to create an unsanitized zipfile with backslashes. 0a3a7b4 Refine expectation that paths with leading slashes are simply not visible. f89b93f Address infinite loop when zipfile begins with more than one leading slash. 3cb5609 Removed SanitizedNames. Additional commits viewable in compare view    ![De)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Bump zipp from 3.18.1 to 3.20.1,"Bumps zipp from 3.18.1 to 3.20.1.  Changelog Sourced from zipp's changelog.  v3.20.1 Bugfixes  python/cpython CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)  v3.20.0 Features  Made the zipfile compatibility overlay available as zipp.compat.overlay.  v3.19.3 Bugfixes  Also match directories in Path.glob. ( CC(Scalars passed into np.array should return 0dim arrays))  v3.19.2 No significant changes. v3.19.1 Bugfixes  Improved handling of malformed zip files. ( CC(example Errata))  v3.19.0 Features  Implement is_symlink. ( CC(add oss test instructions, fix conv grad bug))    ... (truncated)   Commits  c23e549 Finalize c2b9015 Merge pull request  CC(array() was a noop on scalar types) from jaraco/bugfix/gh123270supportednames 774a3ac Add TODO to consolidate this behavior in CPython. cc61e61 Prefer simpler path.rstrip to consolidate checks for empty or only paths. bec712f Mark unused code as uncovered. fde82dc Add news fragment. a421f7e Invent DirtyZipInfo to create an unsanitized zipfile with backslashes. 0a3a7b4 Refine expectation that paths with leading slashes are simply not visible. f89b93f Address infinite loop when zipfile begins with more than one leading slash. 3cb5609 Removed SanitizedNames. Additional commits viewable in compare view    ![De",2024-08-26T17:53:55Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23246,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
1443,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump scipy from 1.13.1 to 1.14.1)ï¼Œ å†…å®¹æ˜¯ (Bumps scipy from 1.13.1 to 1.14.1.  Release notes Sourced from scipy's releases.  SciPy 1.14.1 Release Notes SciPy 1.14.1 adds support for Python 3.13, including binary wheels on PyPI. Apart from that, it is a bugfix release with no new features compared to 1.14.0. Authors  Name (commits) hvetinari (1) Evgeni Burovski (1) CJ Carey (2) Lucas Colley (3) Ralf Gommers (3) Melissa Weber MendonÃ§a (1) Andrew Nelson (3) Nick ODell (1) Tyler Reddy (36) Daniel Schmitz (1) Dan Schult (4) Albert Steppi (2) Ewout ter Hoeven (1) Tibor VÃ¶lcker (2) + Adam Turner (1) + Warren Weckesser (2) à¨—à¨—à¨¨à¨¦à©€à¨ª à¨¸à¨¿à©°à¨˜ (Gagandeep Singh) (1)  A total of 17 people contributed to this release. People with a &quot;+&quot; by their names contributed a patch for the first time. This list of names is automatically generated, and may not be fully complete. SciPy 1.14.0 Release Notes SciPy 1.14.0 is the culmination of 3 months of hard work. It contains many new features, numerous bugfixes, improved test coverage and better documentation. There have been a number of deprecations and API changes in this release, which are documented below. All users are encouraged to upgrade to this release, as there are a large number of bugfixes and optimizations. Before )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Bump scipy from 1.13.1 to 1.14.1,"Bumps scipy from 1.13.1 to 1.14.1.  Release notes Sourced from scipy's releases.  SciPy 1.14.1 Release Notes SciPy 1.14.1 adds support for Python 3.13, including binary wheels on PyPI. Apart from that, it is a bugfix release with no new features compared to 1.14.0. Authors  Name (commits) hvetinari (1) Evgeni Burovski (1) CJ Carey (2) Lucas Colley (3) Ralf Gommers (3) Melissa Weber MendonÃ§a (1) Andrew Nelson (3) Nick ODell (1) Tyler Reddy (36) Daniel Schmitz (1) Dan Schult (4) Albert Steppi (2) Ewout ter Hoeven (1) Tibor VÃ¶lcker (2) + Adam Turner (1) + Warren Weckesser (2) à¨—à¨—à¨¨à¨¦à©€à¨ª à¨¸à¨¿à©°à¨˜ (Gagandeep Singh) (1)  A total of 17 people contributed to this release. People with a &quot;+&quot; by their names contributed a patch for the first time. This list of names is automatically generated, and may not be fully complete. SciPy 1.14.0 Release Notes SciPy 1.14.0 is the culmination of 3 months of hard work. It contains many new features, numerous bugfixes, improved test coverage and better documentation. There have been a number of deprecations and API changes in this release, which are documented below. All users are encouraged to upgrade to this release, as there are a large number of bugfixes and optimizations. Before ",2024-08-26T17:53:22Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23243,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
1448,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump setuptools from 69.2.0 to 73.0.1)ï¼Œ å†…å®¹æ˜¯ (Bumps setuptools from 69.2.0 to 73.0.1.  Changelog Sourced from setuptools's changelog.  v73.0.1 Bugfixes  Remove abc.ABCMeta metaclass from abstract classes. pypa/setuptools CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) &lt;https://github.com/pypa/setuptools/pull/4503&gt;_ had an unintended consequence of causing potential TypeError: metaclass conflict: the metaclass of a derived class must be a (nonstrict) subclass of the metaclasses of all its bases  by :user:Avasam ( CC(multibuf: fix donated_invars in _xla_callable))  v73.0.0 Features  Mark abstract base classes and methods with abc.ABC and abc.abstractmethod  by :user:Avasam ( CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)) Changed the order of type checks in setuptools.command.easy_install.CommandSpec.from_param to support any collections.abc.Iterable of str param  by :user:Avasam ( CC(Fallback to Python for float0 in the C++ jax.jit.))  Bugfixes  Prevent an error in bdist_wheel if compression is set to a str (even if valid) after finalizing options but before running the command.  by :user:Avasam ( CC(Jax numpy does not throw IndexError when array index out of bounds)) Raises an exception when py_limited_api is used in a build with Py_GIL_DISABLEDpython/cpython CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) CC(Unclear shape mismatch error from using jax.mas)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bump setuptools from 69.2.0 to 73.0.1,Bumps setuptools from 69.2.0 to 73.0.1.  Changelog Sourced from setuptools's changelog.  v73.0.1 Bugfixes  Remove abc.ABCMeta metaclass from abstract classes. pypa/setuptools CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) &lt;https://github.com/pypa/setuptools/pull/4503&gt;_ had an unintended consequence of causing potential TypeError: metaclass conflict: the metaclass of a derived class must be a (nonstrict) subclass of the metaclasses of all its bases  by :user:Avasam ( CC(multibuf: fix donated_invars in _xla_callable))  v73.0.0 Features  Mark abstract base classes and methods with abc.ABC and abc.abstractmethod  by :user:Avasam ( CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)) Changed the order of type checks in setuptools.command.easy_install.CommandSpec.from_param to support any collections.abc.Iterable of str param  by :user:Avasam ( CC(Fallback to Python for float0 in the C++ jax.jit.))  Bugfixes  Prevent an error in bdist_wheel if compression is set to a str (even if valid) after finalizing options but before running the command.  by :user:Avasam ( CC(Jax numpy does not throw IndexError when array index out of bounds)) Raises an exception when py_limited_api is used in a build with Py_GIL_DISABLEDpython/cpython CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) CC(Unclear shape mismatch error from using jax.mas,2024-08-26T17:52:26Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23242,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
1471,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Wrong derivatives when using defjvp for sparse.BCOO matrices)ï¼Œ å†…å®¹æ˜¯ ( Description While trying to equip an external sparse linear solver with a JVP rule, I encountered unexpected behavior related to the sparse BCOO matrix. I'm not sure if it's a bug or if I've overlooked something, but the derivatives differ from my checks. It also works if I pass the data and indices separately and only construct a BCOO matrix within the solver's definition. Attached is the code: ```python import jax import jax.numpy as jnp from jax.experimental import sparse  Solver definitions  Sparse solver equipped with jvp rule, here seems to be a problem .custom_jvp def solve_fun_sparse(A, b):     return jnp.linalg.solve(A.todense(), b) .defjvp def solve_fun_jvp(primals, tangents):     A, b = primals     A_dot, b_dot = tangents     primal_result = solve_fun_sparse(A, b)     result_dot = solve_fun_sparse(A, b_dot  A_dot @ primal_result)     return primal_result, result_dot  Using data and indices manually works .custom_jvp def solve_fun_just_data(data, indices, b):     A_mat = sparse.BCOO((data, indices), shape=(2, 2))     return jnp.linalg.solve(A_mat.todense(), b) .defjvp def solve_fun_jvp(primals, tangents):     data, indices, b = primals     data_dot, _, b_dot = tangents     primal_result = solve_fun_ju)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Wrong derivatives when using defjvp for sparse.BCOO matrices," Description While trying to equip an external sparse linear solver with a JVP rule, I encountered unexpected behavior related to the sparse BCOO matrix. I'm not sure if it's a bug or if I've overlooked something, but the derivatives differ from my checks. It also works if I pass the data and indices separately and only construct a BCOO matrix within the solver's definition. Attached is the code: ```python import jax import jax.numpy as jnp from jax.experimental import sparse  Solver definitions  Sparse solver equipped with jvp rule, here seems to be a problem .custom_jvp def solve_fun_sparse(A, b):     return jnp.linalg.solve(A.todense(), b) .defjvp def solve_fun_jvp(primals, tangents):     A, b = primals     A_dot, b_dot = tangents     primal_result = solve_fun_sparse(A, b)     result_dot = solve_fun_sparse(A, b_dot  A_dot @ primal_result)     return primal_result, result_dot  Using data and indices manually works .custom_jvp def solve_fun_just_data(data, indices, b):     A_mat = sparse.BCOO((data, indices), shape=(2, 2))     return jnp.linalg.solve(A_mat.todense(), b) .defjvp def solve_fun_jvp(primals, tangents):     data, indices, b = primals     data_dot, _, b_dot = tangents     primal_result = solve_fun_ju",2024-08-26T08:57:45Z,bug,open,0,2,https://github.com/jax-ml/jax/issues/23235,"Hi  thanks for the question! I think this is behaving as expected. When it comes to autodiff, sparse matrices are fundamentally different than dense matrices in that *zero elements do not enter the computation*. That means that when you take the gradient with respect to a sparse matrix, tangent values are only defined for specified matrix elements. On the other hand, when you take the gradient with respect to a dense matrix, tangents are defined for all matrix elements. In particular, this means that when you do something like `A_dot @ primals`, the result will differ depending on whether `A_dot` is a sparse or a dense matrix. Another way to think about it: when you're differentiating with respect to a sparse matrix, you're differentiating only with respect to its specified elements. When you're differentiating with respect to a dense matrix, you're differentiating with respect to all of its elements. We deliberately chose to define autodiff for sparse matrices in this way because it means that sparse operations have sparse gradients â€“ if it were not the case, then JAX's autodiff would be useless in the context of large sparse computations, because the gradients would be dense and blow up the memory. Does that make sense?","Hi Jake, thanks a lot for the explanation! I totally agree with your explanation. However, if a dense matrix were treated as a sparse matrix, the values in the derivative should still be the same as when it's defined as a dense matrix, right? I tried to further simplify the example and wrote a custom matrixvector multiplication. In `A_dot`, all indices seem to be set to [0,0] independently of `A`. When I transfer them from `A` to `A_dot` using `A_dot = sparse.BCOO((A_dot.data, A.indices), shape=(2, 2))`, the jvp rule works as I would expect. In case all indices in `A_dot` are [0,0], the derivatives with respect to the other components are all overwritten by the one of the first argument. Is there a reason for all indices beeing [0,0]? ```python import jax import jax.numpy as jnp from jax.experimental import sparse .custom_jvp def matvec(A, b):     return A @ b .defjvp def matvec_jvp(primals, tangents):     A, b = primals     A_dot, b_dot = tangents      This resolves my issue. The indices of A_dot were all [0,0]...     A_dot = sparse.BCOO((A_dot.data, A.indices), shape=(2, 2))     primal_result = matvec(A, b)     tangent_result = matvec(A_dot, b) + matvec(A, b_dot)      jax.debug.print(""A_dot: {x}"", x = A_dot.data)      jax.debug.print(""indices: {x}"", x = A_dot.indices)      jax.debug.print(""b: {x}"", x = b)      jax.debug.print(""A_dot @ b: {x}"", x = A_dot @ b)     return primal_result, tangent_result  Test matrix data = jnp.array([1., 0., 0., 1.]) indices = jnp.array([[0, 0], [0, 1], [1, 0], [1, 1]]) b = jnp.array([0.1, 1.0])  Sparse matrix def fun(data, indices, b):     A = sparse.BCOO((data, indices), shape=(2, 2))     return matvec(A, b).sum() print(""sparse: "", jax.jacfwd(fun)(data, indices, b))  sparse:  [0.1 0.1 0.1 0.1]   Dense matrix  def fun(data, indices, b):          A = sparse.BCOO((data, indices), shape=(2, 2)).todense()      return matvec(A, b).sum()  print(""dense: "", jax.jacfwd(fun)(data, indices, b))  dense:  [0.1 1.  0.1 1. ]"
336,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(improve `scan` error message on non-concrete `length` argument)ï¼Œ å†…å®¹æ˜¯ (Specifically, make it speak concretely about the `length` argument. Example:  Before:  After: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,improve `scan` error message on non-concrete `length` argument,"Specifically, make it speak concretely about the `length` argument. Example:  Before:  After: ",2024-08-25T00:30:34Z,better_errors pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/23228
747,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`jax.nn.dot_product_attention(...implementation='cudnn')` throws error `libnvrtc.so.12` not found)ï¼Œ å†…å®¹æ˜¯ ( Description I am calling `jax.nn.dot_product_attention` with the following line:  However, this throws the following error:  I have tried installing both regular and nightly: `pip install U ""jax[cuda12]""` `pip install U pre jax[cuda12] f https://storage.googleapis.com/jaxreleases/jax_nightly_releases.html` I also tried to install the nvrtc library independently with: `pip install nvidiacudanvrtccu12`  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,`jax.nn.dot_product_attention(...implementation='cudnn')` throws error `libnvrtc.so.12` not found," Description I am calling `jax.nn.dot_product_attention` with the following line:  However, this throws the following error:  I have tried installing both regular and nightly: `pip install U ""jax[cuda12]""` `pip install U pre jax[cuda12] f https://storage.googleapis.com/jaxreleases/jax_nightly_releases.html` I also tried to install the nvrtc library independently with: `pip install nvidiacudanvrtccu12`  System info (python version, jaxlib version, accelerator, etc.) ",2024-08-24T23:46:31Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/23227,"This had to do with a CUDA toolkit on Void Linux, which has been resolved.",I am hitting the same or similar issue:  Do you have any more details on what the issue was with CUDA toolkit?  
1455,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Guard against unintentional transfers to CPU)ï¼Œ å†…å®¹æ˜¯ (In order to work around long compile times and hidden recompilations, a bunch of JIT compiled logic can be broken into a sequence of smaller functions. One of the implications of this is that the control flow logic has to be hostside, since using jax.lax control flow functions triggers results in compilation of the target condition/body functions which is will effectively just compiling the original function. For example, if you were to break a function `f` into two functions `f1` and `f2` your code may look something like this with the loop running on the host:   Making this work efficiently requires a few extra considerations: 1. Avoiding GPUtoCPU transfer between calls to the JAXcompiled functions 2. Avoid reallocations as much as possible through aggressive donation of arguments 3. Avoid hidden recompilation This works but is fragile and it can be difficult to protect against regressions without burdensome performance testing. One thing that works great is that (3.) can be dealt with easily through AOT compilation, which will throw an exception whenever a regression is introduced that mistakenly changes the type of a return value (e.g. an array in state is initialized to [1] with a data type of int32 when it)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Guard against unintentional transfers to CPU,"In order to work around long compile times and hidden recompilations, a bunch of JIT compiled logic can be broken into a sequence of smaller functions. One of the implications of this is that the control flow logic has to be hostside, since using jax.lax control flow functions triggers results in compilation of the target condition/body functions which is will effectively just compiling the original function. For example, if you were to break a function `f` into two functions `f1` and `f2` your code may look something like this with the loop running on the host:   Making this work efficiently requires a few extra considerations: 1. Avoiding GPUtoCPU transfer between calls to the JAXcompiled functions 2. Avoid reallocations as much as possible through aggressive donation of arguments 3. Avoid hidden recompilation This works but is fragile and it can be difficult to protect against regressions without burdensome performance testing. One thing that works great is that (3.) can be dealt with easily through AOT compilation, which will throw an exception whenever a regression is introduced that mistakenly changes the type of a return value (e.g. an array in state is initialized to [1] with a data type of int32 when it",2024-08-23T15:58:38Z,enhancement,open,0,4,https://github.com/jax-ml/jax/issues/23215,"For 1) you can use https://jax.readthedocs.io/en/latest/transfer_guard.html For 2) You can add `arr.is_deleted()` check to make sure donation was successful? But note that in the future, we will delete the input array regardless of where donation was successful or not (which will help you?)","Thanks for the quick response, for (1) transfer_guard looks perfect. For (2), I may do that but don't really want to keep around a bunch of checks. I have my own wrapper around the compiled function anyways, and can add the is_deleted check automatically so may just do that. I think in general that isn't the tool I was hoping for, I am going to dig around to see what kind of allocation stats I can find and if there are good allocation stats available I can use that to make sure that the memory allocated at the end of each iteration of the loop is not more than the memory allocated at the start.","For future context, I worked around the issue by using the suggestion and asserting that `arr.is_deleted()` after every call to the JIT'tted function while a doextravalidation flag was enabled. This approach took a while but in the end gave me confidence that the buffers are being properly donated. If a future change is implemented that causes arrays to always be flagged as deleted even if the buffer is not donated then these checks will no longer work. Word of warning to anyone going down this path, the buffer donation logic when pytrees are involved is a lot less obvious and it took a while to get it so that all my buffers were being properly donated. The warning message does not always work when pytrees are involved. I am closing this issue since it is effectively resolved.",Closing per previous comment. 
354,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Pallas TPU] Raise a clear error when trying to load/store to a non-SMEM/non-VMEM buffer)ï¼Œ å†…å®¹æ˜¯ ([Pallas TPU] Raise a clear error when trying to load/store to a nonSMEM/nonVMEM buffer)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[Pallas TPU] Raise a clear error when trying to load/store to a non-SMEM/non-VMEM buffer,[Pallas TPU] Raise a clear error when trying to load/store to a nonSMEM/nonVMEM buffer,2024-08-22T11:16:39Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23189
1237,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Pallas Unexpected Numeric Results With `tf32` on A100 GPU)ï¼Œ å†…å®¹æ˜¯ ( Description Hi, I'm trying to implement a fp32/tf32 matrix multiplication kernel using Pallas. However, the numeric results have more error than I hoped. Specifically, I have the following code:  And this outputs:  While the numeric difference may seem somewhat small, the difference between `C_pallas_tf32` and the others becomes significant in larger applications. I am specifically curious why there is a difference between `C_ref_tf32` and `C_pallas_tf32`. Both of them should be using `tf32`, so I was thinking that they should be very close to equal, much like `C_ref_no_tf32` and `C_pallas_no_tf32`. Two main questions: * do you know why this may be the case? * is there any way to get Pallas/Jax to dump the Pallas kernel's PTX? that way maybe at least I could inspect what it's doing. I know that it's unreasonable to expect bitwise equality with floating point numbers, but this error does seem _really_ hard to understand.  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Pallas Unexpected Numeric Results With `tf32` on A100 GPU," Description Hi, I'm trying to implement a fp32/tf32 matrix multiplication kernel using Pallas. However, the numeric results have more error than I hoped. Specifically, I have the following code:  And this outputs:  While the numeric difference may seem somewhat small, the difference between `C_pallas_tf32` and the others becomes significant in larger applications. I am specifically curious why there is a difference between `C_ref_tf32` and `C_pallas_tf32`. Both of them should be using `tf32`, so I was thinking that they should be very close to equal, much like `C_ref_no_tf32` and `C_pallas_no_tf32`. Two main questions: * do you know why this may be the case? * is there any way to get Pallas/Jax to dump the Pallas kernel's PTX? that way maybe at least I could inspect what it's doing. I know that it's unreasonable to expect bitwise equality with floating point numbers, but this error does seem _really_ hard to understand.  System info (python version, jaxlib version, accelerator, etc.) ",2024-08-21T22:25:51Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/23182,Pinging  who will know best.,"Hi,  This is the same issue encountered here: https://github.com/tritonlang/triton/issues/4574. I applied the same fix recommended there and was able to get the same result for TF32 between XLA and Pallas. You can try it by pulling this branch: https://github.com/google/jax/pull/23262.  You can pass in `debug=True` to `pallas_call` and it will dump the Triton IR. But in this case you wouldn't see anything suspicious since it's due to rounding issues. The right solution here is probably to allow inline assembly in Pallas since we don't have that functionality yet.","Yes, the solution we got to https://github.com/tritonlang/triton/issues/4574 addresses this problem. Closing the issue here."
539,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Unsupported conversion from f64 to f16 in pallas despite not using fp16)ï¼Œ å†…å®¹æ˜¯ ( Description Hi, when I try to matrix multiply in float64 in pallas I get the following error related to converting to float16:  Why does this error occur given that we never convert to f16 anywhere? Reproducible example:   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Unsupported conversion from f64 to f16 in pallas despite not using fp16," Description Hi, when I try to matrix multiply in float64 in pallas I get the following error related to converting to float16:  Why does this error occur given that we never convert to f16 anywhere? Reproducible example:   System info (python version, jaxlib version, accelerator, etc.) ",2024-08-21T20:48:32Z,bug pallas,open,0,4,https://github.com/jax-ml/jax/issues/23179,"It looks like when Pallas lowers the Matmul, it assumes the inputs are FP32:  My guess is that the culprint is this line (https://github.com/google/jax/blob/main/jax/_src/pallas/triton/lowering.pyL2045) which sets the input type (f64) equal to the output type (f32).  do you know what's the rationale behind the typing? I tried commenting out the line below which asserts input type == output type and the user's code worked. But I'm not sure why that check exists in the first place.","It looks like `pl.dot` uses `preferred_element_type=jnp.float32`. However, that doesn't explain the error, and AFAICT there are no casts to fp16 anywhere in the TTIR generated by Pallas.",Maybe running with `MLIR_ENABLE_DUMP=1` (ie dump all intermediate mlir lowerings) would reveal more information,I have encountered the same error when using floating type 64. Hoping there are solutions. 
1473,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Improve support for using JAX's custom LAPACK calls externally)ï¼Œ å†…å®¹æ˜¯ (Hi there, This request is a followup to the discussion here: https://github.com/google/jax/discussions/18065.  What we're trying to accomplish Suppose we have our own JIT compiler, called `qjit`. We would like to be able to do the following, for example:  and similarly for the other functions in `jax.scipy.linalg`. However, when we do so, we get `undefined symbol` errors, in this case:   Current workaround What we currently do to get around this is what was suggested in https://github.com/google/jax/discussions/18065: to manually compile and link in the required custom JAX LAPACK modules under `jaxlib/cpu/`, to define the required symbols such as `lapack_zgees`. However, this is cumbersome and difficult to maintain (suppose these modules change in a future JAX release).  What else we've tried We noticed that the `jaxlib` package comes shipped with the shared object file `jaxlib/cpu/_lapack.so`, which contains the symbols for the kernel functions that the custom JAX wrappers use. For instance, using the `nm` tool, we can find the corresponding kernel function for `lapack_zgees`, `ComplexGees>::Kernel`:  We tried loading this symbol using the dynamic linking loader as follows (simplified for brevity):  However, `d)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Improve support for using JAX's custom LAPACK calls externally,"Hi there, This request is a followup to the discussion here: https://github.com/google/jax/discussions/18065.  What we're trying to accomplish Suppose we have our own JIT compiler, called `qjit`. We would like to be able to do the following, for example:  and similarly for the other functions in `jax.scipy.linalg`. However, when we do so, we get `undefined symbol` errors, in this case:   Current workaround What we currently do to get around this is what was suggested in https://github.com/google/jax/discussions/18065: to manually compile and link in the required custom JAX LAPACK modules under `jaxlib/cpu/`, to define the required symbols such as `lapack_zgees`. However, this is cumbersome and difficult to maintain (suppose these modules change in a future JAX release).  What else we've tried We noticed that the `jaxlib` package comes shipped with the shared object file `jaxlib/cpu/_lapack.so`, which contains the symbols for the kernel functions that the custom JAX wrappers use. For instance, using the `nm` tool, we can find the corresponding kernel function for `lapack_zgees`, `ComplexGees>::Kernel`:  We tried loading this symbol using the dynamic linking loader as follows (simplified for brevity):  However, `d",2024-08-21T15:55:33Z,enhancement,open,1,7,https://github.com/jax-ml/jax/issues/23172,"Thanks for this request! This is something we'd love to support. We don't have a specific timeline, but I wanted to just confirm here that this feature request is acknowledged.",Adding one clarification: jaxlib doesn't actually come with its own LAPACK library. It actually links to the one from scipy. The code used to populate our API with those symbols is here: https://github.com/google/jax/blob/b56ed8eeddc5794f3981832a38b6bcc195eb20f8/jaxlib/cpu/lapack.ccL40L152 It's probably worth noting that as part of this conversation!,"Hi , Could you help clarify something? We were testing a few of the `jax.linalg` functions with the LAPACK wrappers mentioned above and noticed in certain cases we get incorrect results when calling some JAX linear algebra function from within our qjitcompiled block. For example, with `jax.scipy.linalg.lu`:  We noticed, for example, that in this case, some of the pivot matrix rows have been reordered:  To get to this point we've used the `jax::Getrf::Kernel` function:  We noticed that a number of FFI kernels were added recently, e.g. `jax::LuDecomposition::Kernel`. Should we be using these kernels instead? We used the other functions because they don't depend on the XLA FFI libraries.","We figured out the issue with the JITcompiled block giving a different answer: our JIT compiler sends the input to the LAPACK call in Cordered (rowmajor) format, but if I've understood correctly, the scipy LAPACK calls expect FORTRANordered (columnmajor) format. Sorry for the noise! I *am* still curious whether we should be using the old kernel function or their FFI variants, though.","Glad you got that figured out! Yeah, our plan is to migrate all the custom calls to the FFI in the near future (see https://github.com/google/jax/issues/23056), so in the long run, that's what you'll need to target. Unfortunately we're currently in that awkward compatibility period where both exist in parallel, and the FFI kernels don't all exist yet!","Great, thanks for the clarification! While we're on the subject, I'm curious how jax handles the row/columnmajor issue. Is there a transformation that occurs somewhere before the call to the LAPACK routine that ensures the array is in columnmajor format? If so could you point me to where in the code that happens? ","Sure! The place where this is specified on the JAX side is via the `operand_layouts` and `result_layouts` parameters in the custom call lowering. For a `n` dimensional input, we pass: `(n  2, n  1, n  3, n  4, ..., 0)` as the layout to specify columnmajor (instead of `(n  1, n  2, ...)` for rowmajor). For example: https://github.com/google/jax/blob/530ed026b8926cba3cb3d06c855b516fd4c9fb38/jaxlib/gpu_solver.pyL112 I haven't gone spelunking to find out where exactly this is used in XLA, but I'm sure it would be possible to track!"
509,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Mosaic GPU] Add a fast type conversion from s8 vectors to bf16 vectors)ï¼Œ å†…å®¹æ˜¯ ([Mosaic GPU] Add a fast type conversion from s8 vectors to bf16 vectors Regular conversion instructions have a ridiculously low throughput on Hopper, so replacing them with some bit tricks yields a much faster implementation. Coauthoredby: Benjamin Chetioui )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[Mosaic GPU] Add a fast type conversion from s8 vectors to bf16 vectors,"[Mosaic GPU] Add a fast type conversion from s8 vectors to bf16 vectors Regular conversion instructions have a ridiculously low throughput on Hopper, so replacing them with some bit tricks yields a much faster implementation. Coauthoredby: Benjamin Chetioui ",2024-08-21T12:02:14Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23164
757,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jax library error jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed)ï¼Œ å†…å®¹æ˜¯ ( Description Hey, I am working on a code that uses the Jax library, and I run into this error over and over again no matter how I tried to configure my environment:  There is no memory problem, and I have set up my $LD_LIBRARY_PATH to point to where my CUDNN version I downloaded is:  Please I would appreciate someone's help in understanding why I get the same error over and over again.. Thank you!  System info (python version, jaxlib version, accelerator, etc.)  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jax library error jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed," Description Hey, I am working on a code that uses the Jax library, and I run into this error over and over again no matter how I tried to configure my environment:  There is no memory problem, and I have set up my $LD_LIBRARY_PATH to point to where my CUDNN version I downloaded is:  Please I would appreciate someone's help in understanding why I get the same error over and over again.. Thank you!  System info (python version, jaxlib version, accelerator, etc.)  ",2024-08-20T14:35:14Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/23145,"It looks like you're running a really old version of JAX (0.4.13; the current version is 0.4.31). You'll probably need to update your Python version because 3.8 is no longer supported, and then install the most recent version of JAX (`pip install U ""jax[cuda12]""`) to see if that does the trick!"," Thank you for your response! > It looks like you're running a really old version of JAX (0.4.13; the current version is 0.4.31). You'll probably need to update your Python version because 3.8 is no longer supported, and then install the most recent version of JAX (`pip install U ""jax[cuda12]""`) to see if that does the trick! I tried to upgrade into a new version but I only get this after running  'pip install U ""jax[cuda12]""': `pip show jax jaxlib WARNING: Package(s) not found: jaxlib Name: jax Version: 0.4.13 Summary: Differentiate, compile, and transform Numpy code. Homepage: https://github.com/google/jax Author: JAX team Authoremail: jaxdev.com License: Apache2.0 Location: /home/mikabell/anaconda3/envs/jaxenv/lib/python3.8/sitepackages Requires: importlibmetadata, mldtypes, numpy, opteinsum, scipy Requiredby: ` for some reason, it doesn't upgrade",Did you upgrade your Python version? Python 3.8 is no longer supported!,"Given the ğŸ‰  reaction, I'm going to hope that you got this sorted out on your end  and close this issue. Please feel free to comment or open a new issue if the problem persists!"
575,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Added `PyUnstable_Module_SetGIL` to `PyInit_cpu_feature_guard`)ï¼Œ å†…å®¹æ˜¯ (**This PR is based on https://github.com/google/jax/pull/23129** Description:  Added `PyUnstable_Module_SetGIL` to `PyInit_cpu_feature_guard` to explicitly indicate the extension supports running with the GIL disabled. Refs:  https://pyfreethreading.github.io/porting/__tabbed_1_1 Context:  https://github.com/google/jax/issues/23073)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Added `PyUnstable_Module_SetGIL` to `PyInit_cpu_feature_guard`,**This PR is based on https://github.com/google/jax/pull/23129** Description:  Added `PyUnstable_Module_SetGIL` to `PyInit_cpu_feature_guard` to explicitly indicate the extension supports running with the GIL disabled. Refs:  https://pyfreethreading.github.io/porting/__tabbed_1_1 Context:  https://github.com/google/jax/issues/23073,2024-08-19T22:16:20Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/23130
1448,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump setuptools from 69.2.0 to 72.2.0)ï¼Œ å†…å®¹æ˜¯ (Bumps setuptools from 69.2.0 to 72.2.0.  Changelog Sourced from setuptools's changelog.  v72.2.0 Features  pypa/distutils CC(initial spmd groundwork)pypa/distutils CC(Add lax.gather and lax.scatter_add.)pypa/distuils CC(Failing lax_numpy_indexing_test.py tests on Python 3.7) CC(ignore mypy error in jax2tf))  v72.1.0 Features  Restore the tests command and deprecate access to the module. ( CC(Internal change)) ( CC(Passing Haiku Module through custom_vjp lose transform))  v72.0.0 Deprecations and Removals  The test command has been removed. Users relying on 'setup.py test' will need to migrate to another test runner or pin setuptools before this version. ( CC(Question about stax batch dimension))  v71.1.0 Features   Added return types to typed public functions  by :user:Avasam Marked pkg_resources as py.typed  by :user:Avasam ( CC(Compile error on updating to JAX0.2.0))   Misc   CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)  v71.0.4 Bugfixes   ... (truncated)   Commits  76942cf Bump version: 72.1.0 â†’ 72.2.0 780a782 Correct reference in news fragment. Ref  CC(Add jax.numpy.polyint) 64f10c5 Merge pull request  CC(Add jax.numpy.polyint) from pypa/feature/distutilsb7ee725f3 d4ad24b Add news fragment. 0ab156c Merge https://github.com/pypa/distutils b7)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bump setuptools from 69.2.0 to 72.2.0,Bumps setuptools from 69.2.0 to 72.2.0.  Changelog Sourced from setuptools's changelog.  v72.2.0 Features  pypa/distutils CC(initial spmd groundwork)pypa/distutils CC(Add lax.gather and lax.scatter_add.)pypa/distuils CC(Failing lax_numpy_indexing_test.py tests on Python 3.7) CC(ignore mypy error in jax2tf))  v72.1.0 Features  Restore the tests command and deprecate access to the module. ( CC(Internal change)) ( CC(Passing Haiku Module through custom_vjp lose transform))  v72.0.0 Deprecations and Removals  The test command has been removed. Users relying on 'setup.py test' will need to migrate to another test runner or pin setuptools before this version. ( CC(Question about stax batch dimension))  v71.1.0 Features   Added return types to typed public functions  by :user:Avasam Marked pkg_resources as py.typed  by :user:Avasam ( CC(Compile error on updating to JAX0.2.0))   Misc   CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)  v71.0.4 Bugfixes   ... (truncated)   Commits  76942cf Bump version: 72.1.0 â†’ 72.2.0 780a782 Correct reference in news fragment. Ref  CC(Add jax.numpy.polyint) 64f10c5 Merge pull request  CC(Add jax.numpy.polyint) from pypa/feature/distutilsb7ee725f3 d4ad24b Add news fragment. 0ab156c Merge https://github.com/pypa/distutils b7,2024-08-19T17:22:24Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23123,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
1455,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump importlib-resources from 6.4.0 to 6.4.3)ï¼Œ å†…å®¹æ˜¯ (Bumps importlibresources from 6.4.0 to 6.4.3.  Changelog Sourced from importlibresources's changelog.  v6.4.3 Bugfixes  When inferring the caller in files()python/cpython CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)  v6.4.2 Bugfixes  Merged fix for UTF16 BOM handling in functional tests. ( CC(generalize select batch rule (fixes 311)))  v6.4.1 Bugfixes  python/cpython CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)     Commits  d021417 Finalize 0ecbc3b Merge pull request  CC(make einsum deterministic, correct.) from python/gh123085/inferredcompiled 79fa62f Add docstring and reference to the issue. 90c0e42 Rely on resources.__name__ for easier portability. d618902 Add news fragment. ebc5b97 Extract the filename from the topmost frame of the stack. 4ea81bf Extract a function for computing 'this filename' once. cba8dce Adapt changes for new fixtures. 198adec gh121735: Fix inferring caller when resolving importlib.resources.files() 21afd61 Merge changes to syncronize the 6.4 release with downstream CPython changes. Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Bump importlib-resources from 6.4.0 to 6.4.3,"Bumps importlibresources from 6.4.0 to 6.4.3.  Changelog Sourced from importlibresources's changelog.  v6.4.3 Bugfixes  When inferring the caller in files()python/cpython CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)  v6.4.2 Bugfixes  Merged fix for UTF16 BOM handling in functional tests. ( CC(generalize select batch rule (fixes 311)))  v6.4.1 Bugfixes  python/cpython CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)     Commits  d021417 Finalize 0ecbc3b Merge pull request  CC(make einsum deterministic, correct.) from python/gh123085/inferredcompiled 79fa62f Add docstring and reference to the issue. 90c0e42 Rely on resources.__name__ for easier portability. d618902 Add news fragment. ebc5b97 Extract the filename from the topmost frame of the stack. 4ea81bf Extract a function for computing 'this filename' once. cba8dce Adapt changes for new fixtures. 198adec gh121735: Fix inferring caller when resolving importlib.resources.files() 21afd61 Merge changes to syncronize the 6.4 release with downstream CPython changes. Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't",2024-08-19T17:20:49Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/23120,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
504,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Rollback for: Implement initial vmap over pallas_call w/ ragged inputs (via jumbles))ï¼Œ å†…å®¹æ˜¯ (Rollback for: Implement initial vmap over pallas_call w/ ragged inputs (via jumbles) It can cause issues in x32 when trying to get the aval for array dimension sizes that are larger than i32. Reverts 24394a1b03f01138219013f4773104b834e498b7)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Rollback for: Implement initial vmap over pallas_call w/ ragged inputs (via jumbles),Rollback for: Implement initial vmap over pallas_call w/ ragged inputs (via jumbles) It can cause issues in x32 when trying to get the aval for array dimension sizes that are larger than i32. Reverts 24394a1b03f01138219013f4773104b834e498b7,2024-08-19T11:12:06Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23112
715,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Pallas TPU reduce kernel generalization gives wrong numerical results?)ï¼Œ å†…å®¹æ˜¯ ( Description Credit to simveit for investigating implementing a generalization of the Pallas TPU reduce kernel here: The original (nongeneralized kernel):  The user is not following the docs paragraph which say only last (minormost) dimensions should be reduced over. However, a transposition of the userkernel (according to what the docs suggest) also gives a numerically wrong result. Repro code:   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Pallas TPU reduce kernel generalization gives wrong numerical results?," Description Credit to simveit for investigating implementing a generalization of the Pallas TPU reduce kernel here: The original (nongeneralized kernel):  The user is not following the docs paragraph which say only last (minormost) dimensions should be reduced over. However, a transposition of the userkernel (according to what the docs suggest) also gives a numerically wrong result. Repro code:   System info (python version, jaxlib version, accelerator, etc.) ",2024-08-17T00:48:56Z,pallas,closed,0,3,https://github.com/jax-ml/jax/issues/23099,Pinging  for visibility.,"If you change the grid rank (e.g. `(g0,) > (g1, g2, g0)`), you'll also need to change the `pl.program_id` axis in the kernel. The code works as expected when you modify the kernel to:  for strategies 2 and 3.",That was indeed the problem! The modified example works for all strategies where the minormost axis is reduced: 
1110,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(CuDNN Initialization Error with Pre-existing CUDA Environment Fails with)ï¼Œ å†…å®¹æ˜¯ ( Description Our a cluster node which already has Nvidia drivers and the CUDA toolkit installed (to maintain version compatibility with the underlying OS and the networking stack). Installing via jax[cuda12_local], would make sense. But, as mentioned in the installation guide, JAX requires CUDNN. This would entail that additional packages need to be installed (assuming JAX and the additional packages are installed in the Docker container). But it fails with an error. So, is the recommended practice to always install with jax[cuda12] even though CUDA is already installed? Note: I am setting the `LD_LIBRARY_PATH` to the location where cuDNN is installed using pip install nvidiacudnncu12. Error  Note: jax.devices() gives the expected output  System info (python version, jaxlib version, accelerator, etc.) 1. Accelerator: GPU 2. CUDA12 3. cuDNN > 9.0 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,CuDNN Initialization Error with Pre-existing CUDA Environment Fails with," Description Our a cluster node which already has Nvidia drivers and the CUDA toolkit installed (to maintain version compatibility with the underlying OS and the networking stack). Installing via jax[cuda12_local], would make sense. But, as mentioned in the installation guide, JAX requires CUDNN. This would entail that additional packages need to be installed (assuming JAX and the additional packages are installed in the Docker container). But it fails with an error. So, is the recommended practice to always install with jax[cuda12] even though CUDA is already installed? Note: I am setting the `LD_LIBRARY_PATH` to the location where cuDNN is installed using pip install nvidiacudnncu12. Error  Note: jax.devices() gives the expected output  System info (python version, jaxlib version, accelerator, etc.) 1. Accelerator: GPU 2. CUDA12 3. cuDNN > 9.0 ",2024-08-16T21:22:36Z,bug NVIDIA GPU,closed,0,5,https://github.com/jax-ml/jax/issues/23097,"If you have a CUDA already installed and you want to use that, but cudnn (or some other libs are missing). I think you should do: `jax[cuda12_local]` but also install the other missing packages. You could do that via pip to keep it simple. For cudnn: `nvidiacudnncu12` Here is others wheel that you could need: ",", are the above packages not installed by the CUDA toolkit or during the cuDNN installation? Is there a document that mentions the list of required packages (either through pip or otherwise) for the endtoend flow to work?","You don't have a normal setup. There isn't doc for it.  Some of the packages above are provided by the cuda sdk. But cudnn and nccl isn't.  Can you try installing only cudnn and nccl packages?  If that don't work, report the error.", Installing the packages that you mentioned in the previous comment resolved the issue  ,You probably installed too many packages. I don't think this is an issue. Should we close this issue?
302,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Easy] Refactor ragged_dot transpose, combine ragged_to_dense)ï¼Œ å†…å®¹æ˜¯ ([Easy] Refactor ragged_dot transpose, combine ragged_to_dense)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,"[Easy] Refactor ragged_dot transpose, combine ragged_to_dense","[Easy] Refactor ragged_dot transpose, combine ragged_to_dense",2024-08-14T16:48:16Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23061
297,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Import etils.epath lazily)ï¼Œ å†…å®¹æ˜¯ (Import etils.epath lazily This shaves off an extra 0.10.2s from JAX import times internally.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Import etils.epath lazily,Import etils.epath lazily This shaves off an extra 0.10.2s from JAX import times internally.,2024-08-13T20:15:14Z,,closed,0,0,https://github.com/jax-ml/jax/issues/23037
1461,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Errors from interaction between `shard_map` with `auto` and `lax.map`)ï¼Œ å†…å®¹æ˜¯ ( Description I'm converting https://github.com/google/jax/discussions/23015 into an issue per 's request. Here's a minimal working example of what I'm trying to do. My goal in function `h` is to map a second function `g` over the first dimension `a` of a 2x2 array. Within `g`, I attempt to map a third function `f` over the array's second dimension `b`. Each map applies `shard_map` to `lax.map`; `vmap` isn't a good replacement because its batching rule for my code's actual `f` is quite expensive.  My understanding is that `auto` should make this work. But I get a `NotImplementedError` from the start of `jax.experimental.shard_map._shard_map_impl`. When I wrap `h` in a `jit` before calling it, I instead get the following:     F external/xla/xla/hlo/utils/hlo_sharding_util.cc:2806] Check failed: sharding.IsManualSubgroup()   System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.31 jaxlib: 0.4.31 numpy:  2.0.1 python: 3.10.13  (main, Dec 23 2023, 15:36:39) [GCC 12.3.0] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', release='5.10.16.3microsoftstandardWSL2', version='1 SMP Fri Apr 2 22:23:49 UTC 2021', machine='x86_64'))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Errors from interaction between `shard_map` with `auto` and `lax.map`," Description I'm converting https://github.com/google/jax/discussions/23015 into an issue per 's request. Here's a minimal working example of what I'm trying to do. My goal in function `h` is to map a second function `g` over the first dimension `a` of a 2x2 array. Within `g`, I attempt to map a third function `f` over the array's second dimension `b`. Each map applies `shard_map` to `lax.map`; `vmap` isn't a good replacement because its batching rule for my code's actual `f` is quite expensive.  My understanding is that `auto` should make this work. But I get a `NotImplementedError` from the start of `jax.experimental.shard_map._shard_map_impl`. When I wrap `h` in a `jit` before calling it, I instead get the following:     F external/xla/xla/hlo/utils/hlo_sharding_util.cc:2806] Check failed: sharding.IsManualSubgroup()   System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.31 jaxlib: 0.4.31 numpy:  2.0.1 python: 3.10.13  (main, Dec 23 2023, 15:36:39) [GCC 12.3.0] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Linux', release='5.10.16.3microsoftstandardWSL2', version='1 SMP Fri Apr 2 22:23:49 UTC 2021', machine='x86_64')",2024-08-12T19:11:58Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/23019
789,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Finite precision error calculations always 0 under JIT with bfloat16)ï¼Œ å†…å®¹æ˜¯ ( Description I have some stochastic rounding code and uncovered a bug when trying to use the code like the following:   With bfloat16, the final line prints `True` even though it's clear from the preceding line that not all errors ought to be 0. `np.float32` does not have this behavior. Here are some lowering and compilation outputs, if that happens to be helpful. First bfloat16 and then float32:     (Originally reported at: https://github.com/jaxml/ml_dtypes/issues/167)  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Finite precision error calculations always 0 under JIT with bfloat16," Description I have some stochastic rounding code and uncovered a bug when trying to use the code like the following:   With bfloat16, the final line prints `True` even though it's clear from the preceding line that not all errors ought to be 0. `np.float32` does not have this behavior. Here are some lowering and compilation outputs, if that happens to be helpful. First bfloat16 and then float32:     (Originally reported at: https://github.com/jaxml/ml_dtypes/issues/167)  System info (python version, jaxlib version, accelerator, etc.) ",2024-08-12T16:03:04Z,bug,open,0,3,https://github.com/jax-ml/jax/issues/23007,"Hi  thanks for the question! I spent some time making a more concise reproduction here  Since it looks like the compiler is doing something unexpected here, it will help to print the optimized HLO for the function:   and this shows what the problem is: the line `%convert.11 = f32[5]{0} convert(bf16[5]{0} %param_1.4)` is converting the input to `float32` before doing all the operations, and then `%convert.9 = bf16[5]{0} convert(f32[5]{0} %subtract.0)` converts this back to `bfloat16`. Thus the error is accumulating in float32 precision, and then when this small error is cast back to `bfloat16`, it is too small to be represented in bfloat16, and so we get zero. Essentially, the JITcompiled version is effectively doing this:  I'm not aware of any way to prevent the compiler from doing this kind of casting â€“ it's probably due to the fact that the hardware (CPU in my case) does not support native bfloat16 operations. I'll ask around to see if others have ideas.","Via , it seems the `xla_allow_excess_precision` flag controls this behavior. If you set it to False, then the compiler won't do this sort of internal upcasting:  Note that XLA flag values are only read at the time the backend is initialized, so be sure to set them either as a system variable outside your script, or in your script via `os.environ` before running any `jax` commands.",That seems to work. Thanks!
319,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Import from ``mlir.dialects`` lazily)ï¼Œ å†…å®¹æ˜¯ (Import from ``mlir.dialects`` lazily These imports jointly account for ~0.3s of import time internally.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Import from ``mlir.dialects`` lazily,Import from ``mlir.dialects`` lazily These imports jointly account for ~0.3s of import time internally.,2024-08-12T12:19:04Z,,closed,0,0,https://github.com/jax-ml/jax/issues/22998
617,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(CUDA `XlaRuntimeError` with MPI on `jax==0.4.31`)ï¼Œ å†…å®¹æ˜¯ ( Description Hi, `jax.jit` on a function seems to fail when running in an OpenMPI environment. An MWE is shown below:  The error can be on select processes (in which case I see the output tensor) or all processes (it hangs/exits). I can confirm this error does not appear in `jax==0.4.30`.  System info (python version, jaxlib version, accelerator, etc.)    Error log      System info:    )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,CUDA `XlaRuntimeError` with MPI on `jax==0.4.31`," Description Hi, `jax.jit` on a function seems to fail when running in an OpenMPI environment. An MWE is shown below:  The error can be on select processes (in which case I see the output tensor) or all processes (it hangs/exits). I can confirm this error does not appear in `jax==0.4.30`.  System info (python version, jaxlib version, accelerator, etc.)    Error log      System info:    ",2024-08-12T06:16:09Z,bug,open,0,5,https://github.com/jax-ml/jax/issues/22995," the error is related with getting `cuda:gemm_fusion_autotuning_results` on shards and maybe related to https://github.com/openxla/xla/pull/13108 (). To disable the autotuning and to make your MWE work, you could try to run it with:  Let me know if this workaround helps","https://github.com/openxla/xla/pull/13108 was reverted. xla_gpu_shard_autotuning=false disables sharding of autotuning, not the autotuning itself.","I can reproduce with jax==0.4.31 and xla_gpu_shard_autotuning=false helps  looks like https://github.com/openxla/xla/pull/13108 got into this JAX release before it got reverted. Thank you for cc'ing me, I'll investigate why does it fail.","5 Your suggestion worked.   I observed that JAX was built against https://github.com/openxla/xla/commit/95e3eea8d2aebd55160ed4185a38345ae98ab500, which was before the revert","I sent a fix to XLA which makes the reproducer from this bug work. Independent of that, sharded autotuning got enabled yesterday again and it will likely get into the next JAX release."
1269,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Accessing incoming gradient and activations in custom_vjp)ï¼Œ å†…å®¹æ˜¯ (Hi! I am trying to write a custom backward pass for a linear layer where I can simultaneously compute the pairwise inner product of all the gradients in the batch. More specifically, suppose I have $a = W x$, where $x \in \mathbb{R}^{B\times N}$ is the input and let $\delta \in \mathbb{R}^{B\times M}$ be the incoming gradient. I want to compute two $B \times B$ matrices that is equal to $x x^\top$ and $\delta  \delta^\top$.  Both of these quantities are already computed for the backpass but as far as I can tell they are only available through the val fields of the Batchtraced shaped array. This becomes very inconvenient when the function has gone through several `vmap` which means I need to call `.val` several times to reach the full tensor and it's hard to know a priori how many levels of vmap it has gone through. Is it possible to make it easier to do something like this? Another related question is what is the best way to compute the gradient in custom_vjp in this case? Also is this generally a good idea for jit? )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Accessing incoming gradient and activations in custom_vjp,"Hi! I am trying to write a custom backward pass for a linear layer where I can simultaneously compute the pairwise inner product of all the gradients in the batch. More specifically, suppose I have $a = W x$, where $x \in \mathbb{R}^{B\times N}$ is the input and let $\delta \in \mathbb{R}^{B\times M}$ be the incoming gradient. I want to compute two $B \times B$ matrices that is equal to $x x^\top$ and $\delta  \delta^\top$.  Both of these quantities are already computed for the backpass but as far as I can tell they are only available through the val fields of the Batchtraced shaped array. This becomes very inconvenient when the function has gone through several `vmap` which means I need to call `.val` several times to reach the full tensor and it's hard to know a priori how many levels of vmap it has gone through. Is it possible to make it easier to do something like this? Another related question is what is the best way to compute the gradient in custom_vjp in this case? Also is this generally a good idea for jit? ",2024-08-12T00:48:46Z,enhancement,open,0,4,https://github.com/jax-ml/jax/issues/22993,"I'm not sure I totally follow the goal here, but your intuition that you don't want to rely on the `.val` attribute is a good one! Do you think you could put together a simpler endtoend example of the expected behavior that I could run? At a high level, I expect that the key assumption here is that this custom_vjp will always be called inside a `vmap`? Perhaps it would be better to define the `linear_vjp` function to directly operate on batched inputs rather than calling it from within a `vmap`? I'm happy to try to give more concrete suggestions if you can put together a runnable demo!","Thank you! I can explain a bit more. The goal I am trying to achieve is to compute the pairwise inner product between the gradient of every batch element. Normally, in the backward pass, the gradient to the weight would be computed as the matrix multiplication $\delta^\top x$ where $\delta \in \mathbb{R}^{B\times M}, x\in \mathbb{R}^{B\times N}$ where $B$ is the batch dimension. Within the `bwd`, I wish to do $\delta \delta^\top \in \mathbb{R}^{B\times B}$ which computes the (part of the) pairwise inner product between the gradient of the different batch elements so the outcome is not really a ""batched"" object anymore.  I wish to integrate this into any arbitrary model I can define so the function may go through more than one `vmap` (e.g., $\delta \in \mathbb{R}^{B\times T\times M}$ in data with two batch axes)  it would be inconvenient to have to rewrite all existing models to explicitly account for batching (and during inference none of this matters so forcing the function to be batched is not ideal). Below is an endtoend example:  This script should print the $2\times 2$ pairwise inner product between the incoming gradients. ","I see. And I guess you're using `delta_inner` for some sort of logging, or does it get used for something else? You can probably hack something like this by combining a batched implementation with `custom_vmap`, but it depends a bit on what you want to do with `delta_inner`...","I plan to use it to modify the gradient of the weight (not shown here for brevity) so it modifies the backpass. I also want to get it out of the grad by passing in a dummy $B \times B$ matrix into the function and returning this matrix as its gradient, but I think this part should be relatively easy (I think one way to think about it is that it's like an additional ""gradient meta data"" for that parameter). Could you elaborate on `custom_vmap`? Another related question is do you foresee this breaking anything if I am using distributed training across 4 TPU nodes with sharding?"
657,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Reconstructing a custom registered dataclass seems to give a segmentation fault)ï¼Œ å†…å®¹æ˜¯ ( Description When trying to restore a PyTree node I get a segmentation fault, this is the repro:  At least head and 0.4.31 are affected (on my CPU machine, linux x86 laptop). It seems to be an issue with xla's optimized implementation of registering dataclasses. This patch reverting to the old registration method seems to fix it:   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Reconstructing a custom registered dataclass seems to give a segmentation fault," Description When trying to restore a PyTree node I get a segmentation fault, this is the repro:  At least head and 0.4.31 are affected (on my CPU machine, linux x86 laptop). It seems to be an issue with xla's optimized implementation of registering dataclasses. This patch reverting to the old registration method seems to fix it:   System info (python version, jaxlib version, accelerator, etc.) ",2024-08-09T23:12:17Z,bug,closed,0,0,https://github.com/jax-ml/jax/issues/22980
1466,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([pallas] Move ops_test.py from jax_triton to jax/pallas)ï¼Œ å†…å®¹æ˜¯ ([pallas] Move ops_test.py from jax_triton to jax/pallas The `jax_triton/ops_test.py` has over time accumulated many tests that are in fact platformindependent tests. Furthermore, those tests were only Googleinternal, and they can be external as well. This moves test coverage for Pallas from the jax_triton package to the Pallas core package. A small number of the tests were deleted, because they were already present in Pallas, e.g., tests in `jax_triton/ops_test.py:ControlFlowTest`, and tests for unary and binary ops in `jax_triton/ops_test.py:OpsTest`. The other tests were distributed to different files in the Pallas repo, according to their purpose:   * tests in `jax_triton/ops_test.py:PrettyPrintingTest` are moved to `tpu_pallas_test.py::PrettyPrintingTest`   * tests in `jax_triton/ops_test.py::IndexingTest` are appended to `indexing_test.py::IndexingTest`; some other indexing tests from `jax_triton/ops_test.py::LoadStoreTest` are also moved there.    * some tests in `jax_triton/ops_test.py:OpsTest` are moved to `ops_test.py::OpsTest`.    * some tests for TPU specific ops in `jax_triton/ops_test.py:OpsTest` are moved to a new test file `tpu_ops_tests.py` Some of this required adding sharding and hypothesis sup)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[pallas] Move ops_test.py from jax_triton to jax/pallas,"[pallas] Move ops_test.py from jax_triton to jax/pallas The `jax_triton/ops_test.py` has over time accumulated many tests that are in fact platformindependent tests. Furthermore, those tests were only Googleinternal, and they can be external as well. This moves test coverage for Pallas from the jax_triton package to the Pallas core package. A small number of the tests were deleted, because they were already present in Pallas, e.g., tests in `jax_triton/ops_test.py:ControlFlowTest`, and tests for unary and binary ops in `jax_triton/ops_test.py:OpsTest`. The other tests were distributed to different files in the Pallas repo, according to their purpose:   * tests in `jax_triton/ops_test.py:PrettyPrintingTest` are moved to `tpu_pallas_test.py::PrettyPrintingTest`   * tests in `jax_triton/ops_test.py::IndexingTest` are appended to `indexing_test.py::IndexingTest`; some other indexing tests from `jax_triton/ops_test.py::LoadStoreTest` are also moved there.    * some tests in `jax_triton/ops_test.py:OpsTest` are moved to `ops_test.py::OpsTest`.    * some tests for TPU specific ops in `jax_triton/ops_test.py:OpsTest` are moved to a new test file `tpu_ops_tests.py` Some of this required adding sharding and hypothesis sup",2024-08-09T06:55:21Z,,closed,0,0,https://github.com/jax-ml/jax/issues/22959
1463,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump the pip group across 1 directory with 2 updates)ï¼Œ å†…å®¹æ˜¯ (Bumps the pip group with 2 updates in the /build directory: zipp and setuptools. Updates `zipp` from 3.18.1 to 3.19.1  Changelog Sourced from zipp's changelog.  v3.19.1 Bugfixes  Improved handling of malformed zip files. ( CC(example Errata))  v3.19.0 Features  Implement is_symlink. ( CC(add oss test instructions, fix conv grad bug))  v3.18.2 No significant changes.    Commits  6d1cb72 Finalize fd604bd Merge pull request  CC(Canâ€™t import both jax and tensorflow (causes kernel restart)) from jaraco/bugfix/119malformedpaths c18417e Add news fragment. 58115d2 Employ SanitizedNames in CompleteDirs. Fixes broken test. 564fcc1 Add SanitizedNames mixin. 79a309f Add some assertions about malformed paths. 2d015c2 Merge https://github.com/jaraco/skeleton a595a0f Rename extras to align with core metadata spec. 608f90a Finalize 3a22d72 Merge pull request  CC(add tensordot) from jaraco/feature/issymlink Additional commits viewable in compare view    Updates `setuptools` from 69.2.0 to 70.0.0  Changelog Sourced from setuptools's changelog.  v70.0.0 Features  Emit a warning when [tools.setuptools] is present in pyproject.toml and will be ignored.  by :user:SnoopJ ( CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)) Improved AttributeError error message if pkg_reso)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bump the pip group across 1 directory with 2 updates,"Bumps the pip group with 2 updates in the /build directory: zipp and setuptools. Updates `zipp` from 3.18.1 to 3.19.1  Changelog Sourced from zipp's changelog.  v3.19.1 Bugfixes  Improved handling of malformed zip files. ( CC(example Errata))  v3.19.0 Features  Implement is_symlink. ( CC(add oss test instructions, fix conv grad bug))  v3.18.2 No significant changes.    Commits  6d1cb72 Finalize fd604bd Merge pull request  CC(Canâ€™t import both jax and tensorflow (causes kernel restart)) from jaraco/bugfix/119malformedpaths c18417e Add news fragment. 58115d2 Employ SanitizedNames in CompleteDirs. Fixes broken test. 564fcc1 Add SanitizedNames mixin. 79a309f Add some assertions about malformed paths. 2d015c2 Merge https://github.com/jaraco/skeleton a595a0f Rename extras to align with core metadata spec. 608f90a Finalize 3a22d72 Merge pull request  CC(add tensordot) from jaraco/feature/issymlink Additional commits viewable in compare view    Updates `setuptools` from 69.2.0 to 70.0.0  Changelog Sourced from setuptools's changelog.  v70.0.0 Features  Emit a warning when [tools.setuptools] is present in pyproject.toml and will be ignored.  by :user:SnoopJ ( CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)) Improved AttributeError error message if pkg_reso",2024-08-08T19:41:23Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/22950,"This pull request was built based on a group rule. Closing it will not ignore any of these versions in future pull requests. To ignore these dependencies, configure ignore rules in dependabot.yml"
621,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Extending JAX with CUDA: cstdint: error: the global scope has no ""..."")ï¼Œ å†…å®¹æ˜¯ ( Description Hi there, I am trying to extend the capabilities of JAX with my own kernels and I have been following both the official documentation the the Ì€dfm/extendingjax` repo. Here is my issue: When I run this (do I even need to have all of these parameters?):  I get the following error:  Any ideas ?  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"Extending JAX with CUDA: cstdint: error: the global scope has no ""..."""," Description Hi there, I am trying to extend the capabilities of JAX with my own kernels and I have been following both the official documentation the the Ì€dfm/extendingjax` repo. Here is my issue: When I run this (do I even need to have all of these parameters?):  I get the following error:  Any ideas ?  System info (python version, jaxlib version, accelerator, etc.) ",2024-08-08T14:37:01Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/22946,"We have recently updated the recommended approach for implementing custom calls like this and the new docs are here: https://jax.readthedocs.io/en/latest/ffi.html You'll need to update to the most recent version of JAX to get that working, but how about you give that a try first. If that doesn't work, please try to put together a minimal reproducible example that demonstrates the issue. At a high level this doesn't really seem like a JAX issue since the errors are raised when compiling your library, but I'm happy to try to help if you can isolate the minimal scope problem.","Thank for this information, this is really helpful. I am trying to reproduce the tutorial for my own kernel in CUDA and I have a conversion error. Here is the `.cu` file:  And when I run the command :  Ì€cmake build ffi/_build`, I will get the following error:  I don't know if this is the best minimal reproductible example but it is pretty minimal in what I am trying to do :)",It looks like you are passing an `ffi::Buffer` to `matmul` directly instead of its `.typed_data()`., â€” I'm going to close this issue because I think we got to the bottom of it for now. Please open new issues if you run into further problems!
589,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Installing jaxlib with CUDA Version: 11.7)ï¼Œ å†…å®¹æ˜¯ ( Description I am trying to install CUDAenabled jaxlib on a system with `CUDA Version: 11.7`. From the displayed available versions, the lowes I see is `0.4.17+cuda11.cudnn86`. Is it possible to install jaxlib on a sytem with `CUDA Version: 11.7`?  System info (python version, jaxlib version, accelerator, etc.) I have jaxlib installe, but not a CUDAenabled version. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Installing jaxlib with CUDA Version: 11.7," Description I am trying to install CUDAenabled jaxlib on a system with `CUDA Version: 11.7`. From the displayed available versions, the lowes I see is `0.4.17+cuda11.cudnn86`. Is it possible to install jaxlib on a sytem with `CUDA Version: 11.7`?  System info (python version, jaxlib version, accelerator, etc.) I have jaxlib installe, but not a CUDAenabled version. ",2024-08-08T10:00:30Z,question,closed,0,5,https://github.com/jax-ml/jax/issues/22932,"Looking at the changelog, JAX dropped support for CUDA versions older than 11.8 in v0.4.8. So for CUDA 11.7, you may have luck with JAX v0.4.7. You can find relevant installation instructions here: https://github.com/google/jax/tree/jaxv0.4.7?tab=readmeovfilepipinstallationgpucuda ","Mmh, I tried a couple of versions, but always see the following message that indicates the lowest possible available version would be `0.4.17+cuda11.cudnn86` `ERROR: Could not find a version that satisfies the requirement jaxlib==0.4.7+cuda11.cudnn86; extra == ""cuda"" (from jax[cuda]) (from versions: 0.4.17, 0.4.17+cuda11.cudnn86, 0.4.17+cuda12.cudnn89, 0.4.18, 0.4.18+cuda11.cudnn86, 0.4.18+cuda12.cudnn89, 0.4.19, 0.4.19+cuda11.cudnn86, 0.4.19+cuda12.cudnn89, 0.4.20, 0.4.20+cuda11.cudnn86, 0.4.20+cuda12.cudnn89, 0.4.21, 0.4.21+cuda11.cudnn86, 0.4.21+cuda12.cudnn89, 0.4.22, 0.4.22+cuda11.cudnn86, 0.4.22+cuda12.cudnn89, 0.4.23, 0.4.23+cuda11.cudnn86, 0.4.23+cuda12.cudnn89, 0.4.24, 0.4.24+cuda11.cudnn86, 0.4.24+cuda12.cudnn89, 0.4.25, 0.4.25+cuda11.cudnn86, 0.4.25+cuda12.cudnn89, 0.4.26, 0.4.26+cuda12.cudnn89, 0.4.27, 0.4.27+cuda12.cudnn89, 0.4.28, 0.4.28+cuda12.cudnn89, 0.4.29, 0.4.29+cuda12.cudnn91, 0.4.30, 0.4.31) ERROR: No matching distribution found for jaxlib==0.4.7+cuda11.cudnn86; extra == ""cuda""`","It looks like youâ€™re using Python 3.12, and Jax 0.4.7 wheels are only available for Python versions 3.8 through 3.11 (Python 3.12 had not yet been released when JAX 0.4.7 was released). Youâ€™ll either have to downgrade your Python version, or upgrade your CUDA version.","Thanks, that worked. However, when I run `python c ""import jax; print(f'Jax backend: {jax.default_backend()}')"" ` to check jax finds the CUDA backend, nothing happens and the process is not responding, so I'm forced to kill it. The same happens when I do `import jax; jax.print_environment_info()`.",It's hard to say what's causing that crash â€“ is it possible you have crosstalk between multiple Python environments in your path?
1207,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Getting some problems in the usage of  jnp.linalg.inv  )ï¼Œ å†…å®¹æ˜¯ ( Description When I use jnp.linalg.inv() to calculate the inverse of the matrix, it raise erros for the arguments Here is my testing code:  import jax.numpy as jnp B = jnp.array([[1,3],[2,4]]) inv_B = jnp.linalg.inv(B) > The raised error are like this: > E0808 15:38:55.241322  985717 pjrt_stream_executor_client.cc:3067] Execution of replica 0 failed: INVALID_ARGUMENT: Wrong number of attributes: expected 1 but got 3 > Traceback (most recent call last): >   File ""/home/huaze/yixian/test_jax.py"", line 8, in  >     inv_B = jnp.linalg.inv(B) >             ^^^^^^^^^^^^^^^^^ > jaxlib.xla_extension.XlaRuntimeError: INVALID_ARGUMENT: Wrong number of attributes: expected 1 but got 3 >  > For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. >   System info (python version, jaxlib version, accelerator, etc.) jax **0.4.30** has this problem, while I use jax **0.4.25** has not)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Getting some problems in the usage of  jnp.linalg.inv  ," Description When I use jnp.linalg.inv() to calculate the inverse of the matrix, it raise erros for the arguments Here is my testing code:  import jax.numpy as jnp B = jnp.array([[1,3],[2,4]]) inv_B = jnp.linalg.inv(B) > The raised error are like this: > E0808 15:38:55.241322  985717 pjrt_stream_executor_client.cc:3067] Execution of replica 0 failed: INVALID_ARGUMENT: Wrong number of attributes: expected 1 but got 3 > Traceback (most recent call last): >   File ""/home/huaze/yixian/test_jax.py"", line 8, in  >     inv_B = jnp.linalg.inv(B) >             ^^^^^^^^^^^^^^^^^ > jaxlib.xla_extension.XlaRuntimeError: INVALID_ARGUMENT: Wrong number of attributes: expected 1 but got 3 >  > For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. >   System info (python version, jaxlib version, accelerator, etc.) jax **0.4.30** has this problem, while I use jax **0.4.25** has not",2024-08-08T07:53:28Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/22928,"Thanks for raising this issue. I'm not able to reproduce this error on CPU or GPU with jax v0.4.31. Can you try reinstalling JAX (there could be some unexpected version skew between jax, jaxlib and the various plugins) and if that doesn't work please describe exactly how you installed JAX and we'll see what we can do.","Thank you for your suggestion. I reinstalled JAX, and it resolved the issue. I appreciate your help and the quick response. Your advice was invaluable. Thanks again!",Great! Glad to hear it worked.
803,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add unit tests for features of FFI.)ï¼Œ å†…å®¹æ˜¯ (Add unit tests for features of FFI. While XLA has tests for the C++ FFI API, and JAX already has high level tests for `ffi_call`, we were missing a mechanism for testing the integration of all of the FFI features into JAX's API. One option, which is what we have been using so far, is to just rely on the FFI calls provided by jaxlib, but those are being added slowly and won't necessarily leverage all of the relevant APIs. Furthermore, this makes the iteration speed on feature development slower. I'm still working through how to wire these tests into our infrastructure appropriately.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Add unit tests for features of FFI.,"Add unit tests for features of FFI. While XLA has tests for the C++ FFI API, and JAX already has high level tests for `ffi_call`, we were missing a mechanism for testing the integration of all of the FFI features into JAX's API. One option, which is what we have been using so far, is to just rely on the FFI calls provided by jaxlib, but those are being added slowly and won't necessarily leverage all of the relevant APIs. Furthermore, this makes the iteration speed on feature development slower. I'm still working through how to wire these tests into our infrastructure appropriately.",2024-08-07T12:56:39Z,,closed,0,0,https://github.com/jax-ml/jax/issues/22915
1434,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(PRNG Key in shard_map())ï¼Œ å†…å®¹æ˜¯ ( Description I'm not sure if I'm misunderstanding the best way to do something like this, but I'm trying to write a parallel optimizer across many GPUs/nodes and I'm having some issues. A random initial distribution is generated on a per device basis and independently optimized. This is a somewhat simplified example of how I'm trying to set up the initial distributions on a single node with 4 GPUs:  From reading through the sharded computation tutorials (https://jax.readthedocs.io/en/latest/shardedcomputation.html), I would expect that creating a sharding across the 4 devices on my machine for both input arrays to a function would implicitly split the parameters along their leading axis. This also appears to be the case when visualizing the sharding of both arrays (both are distributed on all 4 devices). When running the code however, looking inside the elementwise function reveals that this isn't the case:  This is easily solvable by defining an explicit sharding for the function instead with the third technique mentioned:  This approach runs into another issue but does imply that the sharding is dividing the computation across the intended axis:  I have two questions that I'm trying to figure out. First, what )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,PRNG Key in shard_map()," Description I'm not sure if I'm misunderstanding the best way to do something like this, but I'm trying to write a parallel optimizer across many GPUs/nodes and I'm having some issues. A random initial distribution is generated on a per device basis and independently optimized. This is a somewhat simplified example of how I'm trying to set up the initial distributions on a single node with 4 GPUs:  From reading through the sharded computation tutorials (https://jax.readthedocs.io/en/latest/shardedcomputation.html), I would expect that creating a sharding across the 4 devices on my machine for both input arrays to a function would implicitly split the parameters along their leading axis. This also appears to be the case when visualizing the sharding of both arrays (both are distributed on all 4 devices). When running the code however, looking inside the elementwise function reveals that this isn't the case:  This is easily solvable by defining an explicit sharding for the function instead with the third technique mentioned:  This approach runs into another issue but does imply that the sharding is dividing the computation across the intended axis:  I have two questions that I'm trying to figure out. First, what ",2024-08-04T06:13:34Z,question,closed,1,4,https://github.com/jax-ml/jax/issues/22860,"In the first case, the issue is that even though your input keys are sharded, the function must still semantically operate batchwise over your data. An easy way to automatically vectorize your funciton this way is with `vmap`:   In the `shard_map` case, the issue is that `shard_map` passes batches of data to the underlying function, so with the input data you defined you end up with a size1 batch of keys in the function. As the error message says, `uniform` accepts a single key, not a batch of keys. For this particular case, the vmapped version solves the problem as well:  ","This makes more sense. I was under the impression that vmap wasn't necessarily composable with SPMD/multi process applications but if I'm understanding what you're saying, as long as the data is appropriately sharded or a function is annotated with a shard_map or sharding constraints (options 1,2, or 3 in the SPMD tutorial), the computation will still follow the data on each device or node (in a multi process setting) as expected. Thank you so much for the help!","Yes, `vmap` and `shard_map` are composable (though they cannot be used together if you use collectives and are both vmapping and sharding the collective axis). One side note: you'll generally see better performance if you do jitofshardmap, rather than shardmapofjit.",There's a way that doesn't use vmap here: https://github.com/jaxml/jax/discussions/22862discussioncomment10779458
308,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(improve while_loop carry pytree/type mismatch errors)ï¼Œ å†…å®¹æ˜¯ (Now we call into the same error utility as we use in scan.  Before:  After: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,improve while_loop carry pytree/type mismatch errors,Now we call into the same error utility as we use in scan.  Before:  After: ,2024-08-03T21:53:27Z,pull ready,closed,1,0,https://github.com/jax-ml/jax/issues/22857
932,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Importing jax.numpy fails when building jaxlib from source)ï¼Œ å†…å®¹æ˜¯ ( Description `AttributeError: partially initialized module 'jax._src.interpreters.pxla' has no attribute 'identity' (most likely due to a circular import)`  Reproduction: On a graviton3 instance (c7g), using Python 3.10.12 1. Checkout jax to jaxlibv0.4.31. 2. Build jaxlib from source using `python build/build.py` 3. Install jaxlib from the new wheel in `dist/` 4. Install jax using `pip install e .` 5. Run `import jax.numpy as jnp`    Stack Trace     Note that running `python benchmarks/math_benchmark.py` and other benchmarks work fine.  System info (python version, jaxlib version, accelerator, etc.) Python 3.10.12 `AttributeError: module 'jax' has no attribute 'print_environment_info'`)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Importing jax.numpy fails when building jaxlib from source," Description `AttributeError: partially initialized module 'jax._src.interpreters.pxla' has no attribute 'identity' (most likely due to a circular import)`  Reproduction: On a graviton3 instance (c7g), using Python 3.10.12 1. Checkout jax to jaxlibv0.4.31. 2. Build jaxlib from source using `python build/build.py` 3. Install jaxlib from the new wheel in `dist/` 4. Install jax using `pip install e .` 5. Run `import jax.numpy as jnp`    Stack Trace     Note that running `python benchmarks/math_benchmark.py` and other benchmarks work fine.  System info (python version, jaxlib version, accelerator, etc.) Python 3.10.12 `AttributeError: module 'jax' has no attribute 'print_environment_info'`",2024-08-02T16:42:57Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/22845,The script should not be in the same parent directory of the jax repo of course
1467,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Data type precision and demote/promote preference config)ï¼Œ å†…å®¹æ˜¯ (Please:  [x] Check for duplicate requests.      https://github.com/google/jax/issues/19033       https://github.com/google/jax/issues/16705      https://github.com/google/jax/issues/15580    [x] Describe your goal, and if possible provide a code snippet with a motivating example.  TLDR: to benefit the future of AI innovation, we could add config options to enhance JAX support for mixed and reduced precision numbers  I've hit this error message a billion times in getting rolling with `ml_dtypes`. !image _I wish I could set `jax.config` options to make it easier..._  Ideas:  A `default_precise_float_dtype` value set to `f32` (same default dtype as now)  A `default_imprecise_float_dtype` value set to `None` or `bf16`  A `float_precision_mismatch_behavior` enum with `raise`, `auto_demote`, and `auto_promote` variants which defaults to `raise` (same behavior as now)  (Optional): A `default_float_dtype_precision` enum with `precise` and `imprecise` variants, defaulting to `precise` Crucial point: these config options ought to have sensible defaults which yield no change in behavior unless a user changes them. Thus it would be a purely additive change and not negatively impact backward compatiblity. (ditto for integers)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Data type precision and demote/promote preference config,"Please:  [x] Check for duplicate requests.      https://github.com/google/jax/issues/19033       https://github.com/google/jax/issues/16705      https://github.com/google/jax/issues/15580    [x] Describe your goal, and if possible provide a code snippet with a motivating example.  TLDR: to benefit the future of AI innovation, we could add config options to enhance JAX support for mixed and reduced precision numbers  I've hit this error message a billion times in getting rolling with `ml_dtypes`. !image _I wish I could set `jax.config` options to make it easier..._  Ideas:  A `default_precise_float_dtype` value set to `f32` (same default dtype as now)  A `default_imprecise_float_dtype` value set to `None` or `bf16`  A `float_precision_mismatch_behavior` enum with `raise`, `auto_demote`, and `auto_promote` variants which defaults to `raise` (same behavior as now)  (Optional): A `default_float_dtype_precision` enum with `precise` and `imprecise` variants, defaulting to `precise` Crucial point: these config options ought to have sensible defaults which yield no change in behavior unless a user changes them. Thus it would be a purely additive change and not negatively impact backward compatiblity. (ditto for integers",2024-08-02T15:38:31Z,enhancement,closed,0,2,https://github.com/jax-ml/jax/issues/22842,"Hi  we've discussed this before, and in general we've found that different users have different opinions when it comes to what the default promotion behavior for float8 should be. Given that, and given that float8 is generally used very deliberately within a very limited range of situations, we've come to the conculsion that it's best for promotion with float8 to be explicit, rather than implicit. What do you think?","f8 will see more use in the future to save memory, so my objective was to prepare to explicitly configure optional sensible defaults to **demote** to more efficient dtypes, not promote Sadly, I can't contribute to Google projects knowing the Gemini API and Alphafold Server customer noncompetes are unsafe, so I'm closing, no offense to you Jake, I know you're a 10x dev and I really admire all the jax team members, just can't contribute to JAX in good conscience knowing this clause are up.  !image my bad for opening this, I was excited because I hadn't noticed that, but that clause makes the set of things one can actually use gemini for without fear of legal risk an empty set, and anything I do to help JAX, just contributes make the ""closed output / customer noncompete"" problem even worse, so I must cease involvement in the project if the most critical documents in AI safety are written by corporate lawyers instead of AI engineers, humanity is truly ****ed here's a better license, not that anyone cares !image"
341,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(pallas simple ``pl.program_id()`` example not working)ï¼Œ å†…å®¹æ˜¯ ( Description Code executed:  Stack Trace:   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,pallas simple ``pl.program_id()`` example not working," Description Code executed:  Stack Trace:   System info (python version, jaxlib version, accelerator, etc.) ",2024-08-01T13:38:57Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/22817," Explanation of the error The underlying error is the following message: `'vector.shape_cast' op operand CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) must be vector of any type values, but got 'i32'` This error is happening because `pl.program_id` returns a scalar which lives in a separate memory space from vectors (SMEM vs VMEM, see https://jax.readthedocs.io/en/latest/pallas/tpu/pipelining.htmltpuanditsmemoryspaces). Because `o_ref` is by default stored in VMEM, the compiler is trying to cast the `program_id` to a vector of length1 on the store operation, but Mosaic's shape cast operation `vector.shape_cast` is only designed to translate vectors to other vectors, not scalars to vectors. Ideally, Pallas ops should work gracefully regardless of whether the inputs are in SMEM/VMEM, but we don't have this implemented yet for all cases. We're also working on improving the error messages since these are quite difficult to parse currently and requires underlying knowledge of the Mosaic compiler.  Temporary Solutions There's a few ways you can work around this while waiting for an upstream fix. One solution is to place `o_ref` into SMEM as follows:  You could also do the more awkward method of reshaping `o_ref` to (size, 1) and using a reshape. By explicitly reshaping `program_id` to a vector, this avoids having the store operation implicitly attempt the shape cast. ",Thanks for the detailed comment  ! The explicit separation of SMEM seems nice for now. 
782,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(SVD returns NaNs on a specific matrix but singular values may be computed (in JAX x64))ï¼Œ å†…å®¹æ˜¯ ( Description I use JAX's least squarez on many matrices I generate using some data. For all of them it is fine, except for one that returns NaNs. I have identified it is because the SVD fails, but the behavior is very surprising. Here is code to reproduce behavior, in which SVD fails, but singular value computations work I am attaching the specific matrix (as this is only time this happened out of hundreds of times) K_mat.npy.zip  The output is    System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,SVD returns NaNs on a specific matrix but singular values may be computed (in JAX x64)," Description I use JAX's least squarez on many matrices I generate using some data. For all of them it is fine, except for one that returns NaNs. I have identified it is because the SVD fails, but the behavior is very surprising. Here is code to reproduce behavior, in which SVD fails, but singular value computations work I am attaching the specific matrix (as this is only time this happened out of hundreds of times) K_mat.npy.zip  The output is    System info (python version, jaxlib version, accelerator, etc.) ",2024-08-01T02:18:08Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/22809,It looks like some of the singular values for K are very close to zero:  SVD with `compute_uv=False` and `compute_uv=True` have different implementations so the underlying numerics are slightly different. It could be a good idea to add a small identity regularizer to the matrix to improve the numerical stability of the solver: e.g. `K += np.eye(N) * np.finfo('float64').eps` After adding the regularizer I no longer see the `nan` values you are getting.,I see that is interesting. Thank you for finding the reason and fixing it. I didn't know high condition number could lead to failure in SVD computations. Should the svd code be modified? Otherwise I'm happy to close the issue,"JAX directly calls lapack on CPU (in this case, `dgesdd` for doubleprecision SVD), so the implementation is unlikely to be changed.",Indeed. Thanks for the help! 
218,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(checkify: false positive in jnp.where)ï¼Œ å†…å®¹æ˜¯ ( )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,checkify: false positive in jnp.where, ,2024-07-31T23:11:52Z,bug,open,0,3,https://github.com/jax-ml/jax/issues/22806,"I think the approach to fixing this is: 1. make checkify only error when `mode=PROMISE_IN_BOUNDS` 2. ensure that `gather` and `scatter` default to `mode=PROMISE_IN_BOUNDS` 3. explicitly set `mode=DROP` or `mode=FILL` in `jnp.where` implementation, as well as other library code, to avoid checkify errors (2) is the only thing here that I think would be controversial: `gather` already defaults to `PROMISE_IN_BOUNDS`, but `scatter` defaults to `FILL_OR_DROP`, and changing this behavior might change the default behavior in some implementations.","On discussion with , it seems that defaulting to `PROMISE_IN_BOUNDS` is not the right approach, because it has problematic semantics (particularly for autodiff). Given this, our plan now is to add a new `GatherScatterMode.DEFAULT` mode that is semantically equivalent to `FILL_OR_DROP`, except that `checkify` will raise for outofbound indices in DEFAULT mode.",I think I'll leave this until checkify has stabilized more.
622,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Implement initial vmap over pallas_call w/ ragged inputs (via jumbles))ï¼Œ å†…å®¹æ˜¯ (Implement initial vmap over pallas_call w/ ragged inputs (via jumbles) The plan here is to load it up with invariants, and start with a really simple kernel. After that, we can slowly relax the various invariants and implement support for others. Note  the work saving here is compute only, not memory yet. A fastfollowup CL is adding memory savings via indexmap rewriting)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Implement initial vmap over pallas_call w/ ragged inputs (via jumbles),"Implement initial vmap over pallas_call w/ ragged inputs (via jumbles) The plan here is to load it up with invariants, and start with a really simple kernel. After that, we can slowly relax the various invariants and implement support for others. Note  the work saving here is compute only, not memory yet. A fastfollowup CL is adding memory savings via indexmap rewriting",2024-07-31T21:41:52Z,,closed,0,0,https://github.com/jax-ml/jax/issues/22799
893,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(pip install jax[tpu] fails with libtpu inconsistent version error)ï¼Œ å†…å®¹æ˜¯ ( Description As of today the pip install command for jax[tpu] fails:  Even yesterday I was having no issues with it. The problem seems to be with the distribution of the libtpunightly specifically, I can reproduce the error by:  The key issue seems to be this: **Requested libtpunightly==0.1.dev20240729 from https://storage.googleapis.com/libtpunightlyreleases/wheels/libtpunightly/libtpu_nightly0.1.dev20240729%2Bnightlypy3noneany.whl has inconsistent version: expected '0.1.dev20240729+nightly', but metadata has '0.1.dev20240729+default**  System info (python version, jaxlib version, accelerator, etc.)  Gcloud TPU Vm  Python 3.10.13  pip 24.2)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,pip install jax[tpu] fails with libtpu inconsistent version error," Description As of today the pip install command for jax[tpu] fails:  Even yesterday I was having no issues with it. The problem seems to be with the distribution of the libtpunightly specifically, I can reproduce the error by:  The key issue seems to be this: **Requested libtpunightly==0.1.dev20240729 from https://storage.googleapis.com/libtpunightlyreleases/wheels/libtpunightly/libtpu_nightly0.1.dev20240729%2Bnightlypy3noneany.whl has inconsistent version: expected '0.1.dev20240729+nightly', but metadata has '0.1.dev20240729+default**  System info (python version, jaxlib version, accelerator, etc.)  Gcloud TPU Vm  Python 3.10.13  pip 24.2",2024-07-31T17:27:45Z,bug,closed,2,3,https://github.com/jax-ml/jax/issues/22793,"Here is a workaround:  it works because `libtpunightly==0.1.dev20240731`  the newest published libtpu  installs without errors, whereas the older versions fail.",Thanks for the report! We're aware of the issue and working on rollingout the fix.,"It should be fixed now. Thanks again for the report, and please let us know if there are any further issues!"
1184,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Pallas] When mixing basic indexing and integer array indexing, the axis corresponding to integer array indexing is unnecessarily moved to the front)ï¼Œ å†…å®¹æ˜¯ ( Description I am testing in interpret mode. Repro:  Expected behavior: The line `y_ = kernel(x)` should run successfully, and yield the same value as `y`. Actual behavior:  Explanation: The correct shape of the resulting array should be (4, 5), but in Pallas, the shape is incorrectly assumed to be (5, 4), thus resulting the error. I have tested various indexing and observed a pattern that when there is only 1 integer array indexing, the axis corresponding to it is always unnecessarily moved to the front. For example, in the above case, the axis with shape 5 is moved to the front, making Pallas to assume the shape to be (5, 4) instead of (4, 5). This may have to do with https://github.com/google/jax/blob/5c9bb612a775ca23d311eef1aeac03dfe0828a62/jax/_src/state/indexing.pyL256L257.  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"[Pallas] When mixing basic indexing and integer array indexing, the axis corresponding to integer array indexing is unnecessarily moved to the front"," Description I am testing in interpret mode. Repro:  Expected behavior: The line `y_ = kernel(x)` should run successfully, and yield the same value as `y`. Actual behavior:  Explanation: The correct shape of the resulting array should be (4, 5), but in Pallas, the shape is incorrectly assumed to be (5, 4), thus resulting the error. I have tested various indexing and observed a pattern that when there is only 1 integer array indexing, the axis corresponding to it is always unnecessarily moved to the front. For example, in the above case, the axis with shape 5 is moved to the front, making Pallas to assume the shape to be (5, 4) instead of (4, 5). This may have to do with https://github.com/google/jax/blob/5c9bb612a775ca23d311eef1aeac03dfe0828a62/jax/_src/state/indexing.pyL256L257.  System info (python version, jaxlib version, accelerator, etc.) ",2024-07-31T11:47:00Z,bug,open,0,3,https://github.com/jax-ml/jax/issues/22783,Better repro (without strided indexing):  Error: ,"Is there a workaround for this issue, e.g. using `pallas.dslice` or `dynamic_slice` ?  ","> Is there a workaround for this issue, e.g. using `pallas.dslice` or `dynamic_slice` ? https://github.com/jaxml/jax/pull/23758 already works. It's just that the CI is not happy."
1456,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(infinite loop when trying to install jax with cuda support on windows)ï¼Œ å†…å®¹æ˜¯ ( Description   Downloading jax0.4.12.tar.gz (1.3 MB)       1.3/1.3 MB 13.2 MB/s eta 0:00:00   Installing build dependencies ... done   Getting requirements to build wheel ... done   Preparing metadata (pyproject.toml) ... done   Downloading jax0.4.11.tar.gz (1.3 MB)       1.3/1.3 MB 13.1 MB/s eta 0:00:00   Installing build dependencies ... done   Getting requirements to build wheel ... done   Preparing metadata (pyproject.toml) ... done   Downloading jax0.4.10.tar.gz (1.3 MB)       1.3/1.3 MB 16.1 MB/s eta 0:00:00   Installing build dependencies ... done   Getting requirements to build wheel ... done   Preparing metadata (pyproject.toml) ... done   Downloading jax0.4.9.tar.gz (1.3 MB)       1.3/1.3 MB 21.5 MB/s eta 0:00:00   Installing build dependencies ... done   Getting requirements to build wheel ... done   Preparing metadata (pyproject.toml) ... done   Downloading jax0.4.8.tar.gz (1.2 MB)       1.2/1.2 MB 15.5 MB/s eta 0:00:00 infinite loop... when running pip install U r dreamerv3/requirements.txt f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html same with python m pip install jax[cuda]  System info (python version, jaxlib version, accelerator, etc.) python 3.10.11)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,infinite loop when trying to install jax with cuda support on windows," Description   Downloading jax0.4.12.tar.gz (1.3 MB)       1.3/1.3 MB 13.2 MB/s eta 0:00:00   Installing build dependencies ... done   Getting requirements to build wheel ... done   Preparing metadata (pyproject.toml) ... done   Downloading jax0.4.11.tar.gz (1.3 MB)       1.3/1.3 MB 13.1 MB/s eta 0:00:00   Installing build dependencies ... done   Getting requirements to build wheel ... done   Preparing metadata (pyproject.toml) ... done   Downloading jax0.4.10.tar.gz (1.3 MB)       1.3/1.3 MB 16.1 MB/s eta 0:00:00   Installing build dependencies ... done   Getting requirements to build wheel ... done   Preparing metadata (pyproject.toml) ... done   Downloading jax0.4.9.tar.gz (1.3 MB)       1.3/1.3 MB 21.5 MB/s eta 0:00:00   Installing build dependencies ... done   Getting requirements to build wheel ... done   Preparing metadata (pyproject.toml) ... done   Downloading jax0.4.8.tar.gz (1.2 MB)       1.2/1.2 MB 15.5 MB/s eta 0:00:00 infinite loop... when running pip install U r dreamerv3/requirements.txt f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html same with python m pip install jax[cuda]  System info (python version, jaxlib version, accelerator, etc.) python 3.10.11",2024-07-31T06:28:30Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/22776,"in particular, it works fine on wsl and linux, only windows has this problem..","Hi  As per the documentation, JAX cuda support is not available on Windows.   Please have a look at the documentation: https://jax.readthedocs.io/en/latest/installation.html It is also mentioned in the documentation that [`pip install upgrade ""jax[cuda12]""` do not work with Windows, and may fail silently](https://jax.readthedocs.io/en/latest/installation.htmlsupportedplatforms:~:text=These%20pip%20installations%20do%20not%20work%20with%20Windows%2C%20and%20may%20fail%20silently%3B%20refer%20to%20the%20table%20above.). Thank you.", what is the typical protocol for doing RL for windowsonly games then?," unfortunately, I have no idea. But I think it's best we close the issue since, as  pointed out, Windows + GPU is officially unsupported. (There may be some community projects though.)"
732,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Hi all,)ï¼Œ å†…å®¹æ˜¯ (jaxlib.xla_extension.XlaRuntimeError Hi, all Currently I am running my program, I have a function called update, and I get error as follows:  It is clear that my gpu is out of memory, and I want to check which lines in function ""update"" caused this, however, I can even print the last line of function ""update"", the fig is in attached files.   Now I am confused about it, and I do not know which line caused this error, could you please give me some advice? Many thanks! _Originally posted by  in https://github.com/google/jax/discussions/22754_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"Hi all,","jaxlib.xla_extension.XlaRuntimeError Hi, all Currently I am running my program, I have a function called update, and I get error as follows:  It is clear that my gpu is out of memory, and I want to check which lines in function ""update"" caused this, however, I can even print the last line of function ""update"", the fig is in attached files.   Now I am confused about it, and I do not know which line caused this error, could you please give me some advice? Many thanks! _Originally posted by  in https://github.com/google/jax/discussions/22754_",2024-07-30T13:24:09Z,,closed,0,0,https://github.com/jax-ml/jax/issues/22755
1448,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump setuptools from 69.2.0 to 72.1.0)ï¼Œ å†…å®¹æ˜¯ (Bumps setuptools from 69.2.0 to 72.1.0.  Changelog Sourced from setuptools's changelog.  v72.1.0 Features  Restore the tests command and deprecate access to the module. ( CC(Internal change)) ( CC(Passing Haiku Module through custom_vjp lose transform))  v72.0.0 Deprecations and Removals  The test command has been removed. Users relying on 'setup.py test' will need to migrate to another test runner or pin setuptools before this version. ( CC(Question about stax batch dimension))  v71.1.0 Features   Added return types to typed public functions  by :user:Avasam Marked pkg_resources as py.typed  by :user:Avasam ( CC(Compile error on updating to JAX0.2.0))   Misc   CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)  v71.0.4 Bugfixes  Removed lingering unused code around Distribution._patched_dist. ( CC(test fixes in jax_jit_test))  v71.0.3 Bugfixes   ... (truncated)   Commits  441799f Bump version: 72.0.0 â†’ 72.1.0 59aff44 Merge pull request  CC([jax2tf] Add paths that do not use XLA in conv_general_dilated.) from pypa/feature/gracefuldroptests c437aaa Restore the tests command and deprecate access to the module. a6726b9 Add celery and requests to the packages that test integration. Ref  CC(Passing Haiku Module through custom_vjp lose transform) 5e1b3c4 B)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bump setuptools from 69.2.0 to 72.1.0,Bumps setuptools from 69.2.0 to 72.1.0.  Changelog Sourced from setuptools's changelog.  v72.1.0 Features  Restore the tests command and deprecate access to the module. ( CC(Internal change)) ( CC(Passing Haiku Module through custom_vjp lose transform))  v72.0.0 Deprecations and Removals  The test command has been removed. Users relying on 'setup.py test' will need to migrate to another test runner or pin setuptools before this version. ( CC(Question about stax batch dimension))  v71.1.0 Features   Added return types to typed public functions  by :user:Avasam Marked pkg_resources as py.typed  by :user:Avasam ( CC(Compile error on updating to JAX0.2.0))   Misc   CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)  v71.0.4 Bugfixes  Removed lingering unused code around Distribution._patched_dist. ( CC(test fixes in jax_jit_test))  v71.0.3 Bugfixes   ... (truncated)   Commits  441799f Bump version: 72.0.0 â†’ 72.1.0 59aff44 Merge pull request  CC([jax2tf] Add paths that do not use XLA in conv_general_dilated.) from pypa/feature/gracefuldroptests c437aaa Restore the tests command and deprecate access to the module. a6726b9 Add celery and requests to the packages that test integration. Ref  CC(Passing Haiku Module through custom_vjp lose transform) 5e1b3c4 B,2024-07-29T17:21:15Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/22732,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
1442,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump numpy from 1.26.4 to 2.0.1)ï¼Œ å†…å®¹æ˜¯ (Bumps numpy from 1.26.4 to 2.0.1.  Release notes Sourced from numpy's releases.  v2.0.1 NumPy 2.0.1 Release Notes NumPy 2.0.1 is a maintenance release that fixes bugs and regressions discovered after the 2.0.0 release. NumPy 2.0.1 is the last planned release in the 2.0.x series, 2.1.0rc1 should be out shortly. The Python versions supported by this release are 3.93.12. NOTE:  Do not use the GitHub generated &quot;Source code&quot; files listed in the &quot;Assets&quot;, they are garbage. Improvements np.quantile with method closest_observation chooses nearest even order statistic This changes the definition of nearest for border cases from the nearest odd order statistic to nearest even order statistic. The numpy implementation now matches other reference implementations. (gh26656) Contributors A total of 15 people contributed to this release. People with a &quot;+&quot; by their names contributed a patch for the first time.  @â€‹vahidmech + Alex Herbert + Charles Harris Giovanni Del Monte + Leo Singer Lysandros Nikolaou Matti Picus Nathan Goldbaum Patrick J. Roddy + Raghuveer Devulapalli Ralf Gommers Rostan Tabet + Sebastian Berg Tyler Reddy Yannik Wicke +  Pull requests merged A total of 24 pull requests were mer)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bump numpy from 1.26.4 to 2.0.1,"Bumps numpy from 1.26.4 to 2.0.1.  Release notes Sourced from numpy's releases.  v2.0.1 NumPy 2.0.1 Release Notes NumPy 2.0.1 is a maintenance release that fixes bugs and regressions discovered after the 2.0.0 release. NumPy 2.0.1 is the last planned release in the 2.0.x series, 2.1.0rc1 should be out shortly. The Python versions supported by this release are 3.93.12. NOTE:  Do not use the GitHub generated &quot;Source code&quot; files listed in the &quot;Assets&quot;, they are garbage. Improvements np.quantile with method closest_observation chooses nearest even order statistic This changes the definition of nearest for border cases from the nearest odd order statistic to nearest even order statistic. The numpy implementation now matches other reference implementations. (gh26656) Contributors A total of 15 people contributed to this release. People with a &quot;+&quot; by their names contributed a patch for the first time.  @â€‹vahidmech + Alex Herbert + Charles Harris Giovanni Del Monte + Leo Singer Lysandros Nikolaou Matti Picus Nathan Goldbaum Patrick J. Roddy + Raghuveer Devulapalli Ralf Gommers Rostan Tabet + Sebastian Berg Tyler Reddy Yannik Wicke +  Pull requests merged A total of 24 pull requests were mer",2024-07-29T17:21:02Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/22731,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
1445,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Equivalent of torch's retain_graph)ï¼Œ å†…å®¹æ˜¯ (Hi all ! I've been having a blast learning JAX recently. However, coming from torch there is one type of operation that is very easy to do in torch but I can't wrap my head around how to do it in JAX's functional paradigm.  I basically want to differentiate twice through the same forward pass. More specifically, I want to be able to reuse computation of a forward pass for two distinct backward passes (and not take second order derivatives). This requirement is a dealbreaker for my research where the forward pass takes a very long time (I would have to go back to torch which I really don't want). With torch's computation graph and autograd's retain_graph this is very easy to do.  One motivation is for Truncated Backpropagation Through Time style scenarios where we might enlarge the forward pass' computation graph slightly at every step but we do not want to recompute the whole window. In torch the computation graph allows to organize the code in such a way that the two backward passes can be in completely different locations (just passing around the outputs). Here is a very simplified version of what I mean in torch. EDIT: In the simple example below and in the case of TBPTT this could be achieved by applying the)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Equivalent of torch's retain_graph,"Hi all ! I've been having a blast learning JAX recently. However, coming from torch there is one type of operation that is very easy to do in torch but I can't wrap my head around how to do it in JAX's functional paradigm.  I basically want to differentiate twice through the same forward pass. More specifically, I want to be able to reuse computation of a forward pass for two distinct backward passes (and not take second order derivatives). This requirement is a dealbreaker for my research where the forward pass takes a very long time (I would have to go back to torch which I really don't want). With torch's computation graph and autograd's retain_graph this is very easy to do.  One motivation is for Truncated Backpropagation Through Time style scenarios where we might enlarge the forward pass' computation graph slightly at every step but we do not want to recompute the whole window. In torch the computation graph allows to organize the code in such a way that the two backward passes can be in completely different locations (just passing around the outputs). Here is a very simplified version of what I mean in torch. EDIT: In the simple example below and in the case of TBPTT this could be achieved by applying the",2024-07-29T15:34:27Z,question,closed,0,3,https://github.com/jax-ml/jax/issues/22725,https://jax.readthedocs.io/en/latest/_autosummary/jax.linearize.html,"> Is there in the API a function transform to wrap around the forward pass that would return something like a tuple of (the output, reusable computation representation) where the second output would be used like a virtual function call (same output, same dependencies but not ran explicitly) ? Thanks for the question! I think perhaps you just want `jax.vjp`:  Just think of the `vjp` functions as pulling back gradients from output space to input space. In other words, they're ""backwardpass functions"". For more, see the autodiff cookbook if you haven't already.  What do you think?",Sorry for the late reply. This lead me to rethink completely (or think for the first time maybe) how autodiff worked. As a follow up of this issue I would like to point to CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®). Thank you for your help !
746,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(FileNotFoundError when using cache in parallel)ï¼Œ å†…å®¹æ˜¯ ( Description I've got a pytest test suite and I've recently started running it with `n 3`, using `pytestxdist`, so it'll run on 3 processes in parallel. I sometimes get a warning like this one:  When it does happen, the tests run a lot slower. I'm guessing sometimes the tests attempt to manipulate the cache at the same time and clash with each other, causing the cache to not be used and triggering compilation. I can't reliably reproduce this problem.  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",agent,FileNotFoundError when using cache in parallel," Description I've got a pytest test suite and I've recently started running it with `n 3`, using `pytestxdist`, so it'll run on 3 processes in parallel. I sometimes get a warning like this one:  When it does happen, the tests run a lot slower. I'm guessing sometimes the tests attempt to manipulate the cache at the same time and clash with each other, causing the cache to not be used and triggering compilation. I can't reliably reproduce this problem.  System info (python version, jaxlib version, accelerator, etc.) ",2024-07-29T12:52:58Z,bug,open,0,6,https://github.com/jax-ml/jax/issues/22718,Can you try jax 0.4.31 which is just released?,"I upgraded to `0.4.31` and now I'm getting different warnings:  There were about 10 more, and all the filenames end with `atime`, none end with `cache`. I enabled tracebacks and got this: ","Hi, I hit on a similar problem and I think I found the cause and solution. I think the key thing is that the temporary file the cache writes to in normal filesystems does not have a perprocess unique name: https://github.com/google/jax/blob/fed7efd73003988282744b2f649df493b808c781/jax/_src/gfile_cache.pyL37L55 All processes will attempt to open the same `tmp_path = self._path / f""_temp_{key}""`, and any but the first will error. A possible fix is suffixing the temporary file with the hostname of the machine and process id. I include the hostname, since the usecase that brought me here is reusing the same cache for processes in possibly the same node but different MPI groups, but also processes in different nodes. In which case it is also important to differentiate based on hostname.  In NFS, a more complex solution and possibly unnecessary operation is doing NFS locking like described  here (see D) , or done in flufl.lock.", The file you mentioned has been removed in recent version of JAX. Can you try again with the latest version?," I see, I mistakenly navigated to old code. Hereâ€™s the code in version 0.4.32, which works for a single machine if you have eviction enabled, but Iâ€™m not sure will work for NFS ( since SoftFileLock does not seem to follow all the steps described in the link above regarding NFS; compare with flufl.lock implementation) https://github.com/google/jax/blob/1594d2f30fdbfebf693aba4a2b264e4a3e52acc6/jax/_src/lru_cache.pyL120L160","Numba had to deal with the same issue. Instead of making a key that incorporates hostname/pid, they just open a temp file with a name built from a uuid: https://github.com/numba/numba/blob/301ba23116864ef66a2caccfab7c7123e7bcbcfc/numba/core/caching.pyL599L616"
1504,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jax.jit(function).lower(x, y).cost_analysis() gives NONE while running with GPU as the device)ï¼Œ å†…å®¹æ˜¯ ( Description I am training a model Owl_Vit. While trying to train it with GPU, at one stage, it gives None as output for `jax.jit(function).lower(x, y).cost_analysis()`. The thing is that when I use cpu version of the jax, it is working. I get this output: `{'bytes accessed0{}': 4058464512.0, 'utilization0{}': 2059.1572265625, 'bytes accessedout{}': 4285124864.0, 'bytes accessed2{}': 13349412.0, 'utilization2{}': 21.0, 'utilization1{}': 1139.0, 'bytes accessed1{}': 2781101056.0, 'flops': 237087424512.0, 'transcendentals': 111938816.0, 'utilization4{}': 1.0, 'bytes accessed': 11138055168.0, 'bytes accessed3{}': 4.0, 'utilization3{}': 2.0}` So there is no problem with the code. It's just the version of jax that is giving error I think. for version 0.4.23, and 0.4.30 it is giving same error. I tried this specific version because it gives error `jax.random has no attribute PRNGKeyArray`. While using version <0.4.23, it does not give this error. But for both these version, for GPU it is not working. I installed jax with `pip install U ""jax[cuda12]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` and `pip install U ""jax[cuda12]==0.4.23"" f https://storage.googleapis.com/jaxreleases/jax_cuda_release)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"jax.jit(function).lower(x, y).cost_analysis() gives NONE while running with GPU as the device"," Description I am training a model Owl_Vit. While trying to train it with GPU, at one stage, it gives None as output for `jax.jit(function).lower(x, y).cost_analysis()`. The thing is that when I use cpu version of the jax, it is working. I get this output: `{'bytes accessed0{}': 4058464512.0, 'utilization0{}': 2059.1572265625, 'bytes accessedout{}': 4285124864.0, 'bytes accessed2{}': 13349412.0, 'utilization2{}': 21.0, 'utilization1{}': 1139.0, 'bytes accessed1{}': 2781101056.0, 'flops': 237087424512.0, 'transcendentals': 111938816.0, 'utilization4{}': 1.0, 'bytes accessed': 11138055168.0, 'bytes accessed3{}': 4.0, 'utilization3{}': 2.0}` So there is no problem with the code. It's just the version of jax that is giving error I think. for version 0.4.23, and 0.4.30 it is giving same error. I tried this specific version because it gives error `jax.random has no attribute PRNGKeyArray`. While using version <0.4.23, it does not give this error. But for both these version, for GPU it is not working. I installed jax with `pip install U ""jax[cuda12]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` and `pip install U ""jax[cuda12]==0.4.23"" f https://storage.googleapis.com/jaxreleases/jax_cuda_release",2024-07-29T11:53:41Z,bug,open,0,4,https://github.com/jax-ml/jax/issues/22713,"v0.4.23 is fairly old; I'd suggest trying with a more recent JAX version, particularly if you are using more recent CUDA versions. > I tried this specific version because it gives error `jax.random has no attribute PRNGKeyArray` `jax.random.PRNGKeyArray` was deprecated in JAX v0.4.16 and removed in JAX v0.4.24 (see the Change log). You can replace it with `jax.Array` to address this error, and then hopefully use a more recent JAX version.","I changed the ""jax.random.PRNGKeyArray"" with ""jax.Array"", which solved the error of `jax.random has no attribute PRNGKeyArray.`  > I'd suggest trying with a more recent JAX version I tried with v0.4.31, but it is still giving NONE as output for `jax.jit(function).lower(x, y).cost_analysis()`.  UPDATE: I tried with all the versions from 0.4.23 to 0.4.30. Same results. Can there be any problem with cuda configuration?  This is my cuda configuration in conda environment: "," you may try to run `jax.jit(function).lower(x, y).compile().cost_analysis()` to get the cost analysis on GPU. For example: ",Im also using jax:    0.4.30 jaxlib: 0.4.30 As suggested by 5 . I changed `analysis = jax.jit(flax_model_apply_fn).lower(*dummy_input).cost_analysis() ` to `analysis = jax.jit(flax_model_apply_fn).lower(*dummy_input).compile().cost_analysis()[0]` that fixed the problem for me. 
1083,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Setting XLA flags causes segmentation fault)ï¼Œ å†…å®¹æ˜¯ ( Description As per the docs, these flags can help improve the performance on a A100 machine. I am running a jax program written in Equinox on a A100 machine with 2 GPUs, and setting any of these flags cause a segfault. Idk if these flags alone can accelerate the performance, but right now my JAX distributed training is extremely slow compared to torch (which is extremely surprising for me). Couple of reasons that I can think of: 1. torch using tf32 > not sure how to do this in jax 2. torch using flash attention: there is no equivalent of flash attention but I guess these two flags `xla_gpu_enable_triton_softmax_fusion=true xla_gpu_triton_gemm_any=True ` can help bridge the gap 3. torch is using fused adam > not sure optax uses fused implementation depending on the available hardware  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gemma,Setting XLA flags causes segmentation fault," Description As per the docs, these flags can help improve the performance on a A100 machine. I am running a jax program written in Equinox on a A100 machine with 2 GPUs, and setting any of these flags cause a segfault. Idk if these flags alone can accelerate the performance, but right now my JAX distributed training is extremely slow compared to torch (which is extremely surprising for me). Couple of reasons that I can think of: 1. torch using tf32 > not sure how to do this in jax 2. torch using flash attention: there is no equivalent of flash attention but I guess these two flags `xla_gpu_enable_triton_softmax_fusion=true xla_gpu_triton_gemm_any=True ` can help bridge the gap 3. torch is using fused adam > not sure optax uses fused implementation depending on the available hardware  System info (python version, jaxlib version, accelerator, etc.) ",2024-07-29T06:32:24Z,bug,closed,0,8,https://github.com/jax-ml/jax/issues/22705,I would recommend filing an issue about the segfault in openxla/xla. It would really help if you could include a reproducer for the XLA team. Re 2: Could you use `jax.nn.dot_product_attention`? ,"Thanks I can do that. re 2: Nice, but I don't think this is available in 0.4.30. I have the same version installed on my mac, and it isn't there",Can you try the nightly? https://jax.readthedocs.io/en/latest/installation.htmljaxnightlyinstallation,Thanks got it. I will close this and will open an issue for the flags in the XLA repo," Did you file that bug with XLA? I encountered this issue with simple repro steps. I am using a DGX system and this is running on an H100:  The above snippet of code reliably crashes every time, and commenting out the softmax env flag causes the issue to go away.","No, I didn't find any solution. Re flags: Not only that flag, I think most flags are not tested properly. All the flags that are supposed to provide gains on GPU haven't provided any gains on A100 at least. ","I reopened the XLA bug. The crash is certainly something that shouldn't happen, although it would be reasonable for the XLA folks not to prioritize a nondefault configuration. The GPU perf tuning document is supposed to suggest things that may help if tweaked, although there's no guarantee that anything will help, or indeed work, for any given workload. The only truly supported configuration is the default one. Perhaps we should mark those flags as more explicitly experimental? For flash attention, I recommend you use the flash attention API in the most recent JAX, as Sergei suggests. The other two optimizations (TF32 and fused adam) should be happening by default, assuming your training is inside a `jit` for the fused adam.","Thanks  for the quick response. >Perhaps we should mark those flags as more explicitly experimental? Agreed, that will help clarify things for the end user"
670,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Remove custom partitioning pointer from pre-compiled HLO)ï¼Œ å†…å®¹æ˜¯ (When hashing the precompiled IR during computation of the JAX compilation cache key, function that use custom_partitioning produce a different hash every time even for the same underlying computation. This is because of the backend_config pointer changing across runs. To make the compilation cache work for functions that implement custom_partitioning, this PR includes a flag, which when set will set that pointer to be a constant. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Remove custom partitioning pointer from pre-compiled HLO,"When hashing the precompiled IR during computation of the JAX compilation cache key, function that use custom_partitioning produce a different hash every time even for the same underlying computation. This is because of the backend_config pointer changing across runs. To make the compilation cache work for functions that implement custom_partitioning, this PR includes a flag, which when set will set that pointer to be a constant. ",2024-07-29T00:14:22Z,pull ready,closed,0,4,https://github.com/jax-ml/jax/issues/22702, sevcik ,We may want to consider just putting a counter on the MlirContext. That way we can make stable uuids.,> We may want to consider just putting a counter on the MlirContext. That way we can make stable uuids.  What would be the goal of that counter?, I think this is ready to be merged?
575,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([shape_poly] Export jax.typing.DimSize and jax.typing.Shape.)ï¼Œ å†…å®¹æ˜¯ (Some users want to have more precise typing annotations for JAX dimensions sizes and shapes, even in presence of shape polymorphism. At the moment the `DimSize` is defined as `int | Any`. Defining it more precisely is a more involved affair that would involve specifying the symbolic dimensions duck type integers. This is future work.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[shape_poly] Export jax.typing.DimSize and jax.typing.Shape.,"Some users want to have more precise typing annotations for JAX dimensions sizes and shapes, even in presence of shape polymorphism. At the moment the `DimSize` is defined as `int | Any`. Defining it more precisely is a more involved affair that would involve specifying the symbolic dimensions duck type integers. This is future work.",2024-07-25T13:10:44Z,,closed,0,2,https://github.com/jax-ml/jax/issues/22658, I am wondering if I should actually go through the work to define `DimSize` more precisely and making it work internally before opening this up.,">  I am wondering if I should actually go through the work to define `DimSize` more precisely and making it work internally before opening this up. Yes, I think so. Currently `DimSize` is a more complicated way to spell `Any`. In general, tightening the definition of pubic type aliases is a *very* painful change, because there's no way to do a soft deprecation period for static types. Given that, we should avoid exporting a type name until we're happy with it's definition."
412,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(JAX implementation of `scipy.integrate.cumulative_trapezoid`)ï¼Œ å†…å®¹æ˜¯ (The  `scipy.integrate.cumulative_trapezoid`functionis a useful func. Unfortunately, there is no equivalent implementation in jax. Here is an example (adapted  from scipy): )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,JAX implementation of `scipy.integrate.cumulative_trapezoid`,"The  `scipy.integrate.cumulative_trapezoid`functionis a useful func. Unfortunately, there is no equivalent implementation in jax. Here is an example (adapted  from scipy): ",2024-07-25T09:10:55Z,enhancement,closed,0,2,https://github.com/jax-ml/jax/issues/22651,Hi  thanks for the request! We decided a while ago that `scipy.integrate` is outofscope for the JAX package; you can see the discussion and reasoning behind this at https://jax.readthedocs.io/en/latest/jep/18137numpyscipyscope.htmlscipyintegrate,This is implemented in `quadax` : https://quadax.readthedocs.io/en/latest/_api/quadax.cumulative_trapezoid.htmlquadax.cumulative_trapezoid
863,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(ROCm build fails on Fedora 40)ï¼Œ å†…å®¹æ˜¯ ( Description I'm trying to build jaxlib0.4.30 from source on Fedora 40 using ROCm 6.0.2 that comes in their standard repositories. Fedora dumps all ROCm libraries/headers directly into `/usr`, and these seem to be found correctly. However, the build fails because the ROCm device libraries are not found, which I suspect is the stuff installed in `/usr/lib/clang/17/amdgcn/bitcode`:  Running  I get the following error:  Is there some way to specify `rocmdevicelibpath` for the build? I am unfortunately completely unfamiliar with bazel and don't even know where to start looking. Thanks!  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ROCm build fails on Fedora 40," Description I'm trying to build jaxlib0.4.30 from source on Fedora 40 using ROCm 6.0.2 that comes in their standard repositories. Fedora dumps all ROCm libraries/headers directly into `/usr`, and these seem to be found correctly. However, the build fails because the ROCm device libraries are not found, which I suspect is the stuff installed in `/usr/lib/clang/17/amdgcn/bitcode`:  Running  I get the following error:  Is there some way to specify `rocmdevicelibpath` for the build? I am unfortunately completely unfamiliar with bazel and don't even know where to start looking. Thanks!  System info (python version, jaxlib version, accelerator, etc.) ",2024-07-25T07:16:40Z,bug AMD GPU,open,0,7,https://github.com/jax-ml/jax/issues/22650,"Sorry, I missed that this had been assigned to me. Is this still a problem?","Hi, sorry for taking a while to get back on this. I realized I can get around the particular issue of missing device files by creating the symlink  However, now the build fails at a later stage:  buildrocmjax0.4.31.log I also tried with the main branch and the default XLA, but that causes a different error which seems to be related to building zlib. buildjaxmain.log Thanks!"," thanks for notifying us the issue. Actually, we are working on clang patch. Meanwhile, can you try compiling like > ","Hi  , It does not seem to make a difference whether I use clang or not. Running the command you suggested (adapted for the Fedora 40 setup),   produces the same error, see jaxbuildnoclang.txt I know it works on the Ubuntubased Docker container that AMD/ROCm provide, so it's probably something specific to Fedora. As you probably know, Fedora ships ROCm6.1.2 directly in their repos and everything is installed right into `/usr` as opposed to  `/opt/rocm6.x.y`", hmm. I don't have Fedora container to reproduce this error. Can you add > lib/clang/17/include ^^ This above include here > https://github.com/openxla/xla/blob/9e28b002070276a852de6b5508224d35d2547d51/third_party/tsl/third_party/gpus/rocm_configure.bzlL210 And check if it compiles?,"Hi  , I can confirm that the build runs through on Fedora with the following change:  I haven't had the time to check whether the binary actually works, I'll come back to that tomorrow. Thanks a lot!", thanks!  we can close this now please.
365,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Don't broadcast scalar conditions in the jnp.where implementation().)ï¼Œ å†…å®¹æ˜¯ (The underlying lax primitive is perfectly happy to accept scalar conditions with the other arguments being nonscalar.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Don't broadcast scalar conditions in the jnp.where implementation().,The underlying lax primitive is perfectly happy to accept scalar conditions with the other arguments being nonscalar.,2024-07-24T15:27:02Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/22629
1503,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed)ï¼Œ å†…å®¹æ˜¯ ( Description Hello im having an issue using jax and cuda. My error message looks like this: 20240719 15:49:01.341533: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:439] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR 20240719 15:49:01.341571: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:443] Memory usage: 2624061440 bytes free, 12524191744 bytes total. Traceback (most recent call last): File ""scripts/train_vqgan.py"", line 202, in main() File ""scripts/train_vqgan.py"", line 32, in main rng = jax.random.PRNGKey(config.seed) File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/random.py"", line 160, in PRNGKey key = prng.seed_with_impl(impl, seed) File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/prng.py"", line 406, in seed_with_impl return random_seed(seed, impl=impl) File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/prng.py"", line 690, in random_seed return random_seed_p.bind(seeds_arr, impl=impl) File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/prng.py"", line 702, in random_seed_impl base_arr = random_seed_impl_base(seeds, impl=impl) File ""/home/nonsans/miniforge3/envs/viper/lib/pyt)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed," Description Hello im having an issue using jax and cuda. My error message looks like this: 20240719 15:49:01.341533: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:439] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR 20240719 15:49:01.341571: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:443] Memory usage: 2624061440 bytes free, 12524191744 bytes total. Traceback (most recent call last): File ""scripts/train_vqgan.py"", line 202, in main() File ""scripts/train_vqgan.py"", line 32, in main rng = jax.random.PRNGKey(config.seed) File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/random.py"", line 160, in PRNGKey key = prng.seed_with_impl(impl, seed) File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/prng.py"", line 406, in seed_with_impl return random_seed(seed, impl=impl) File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/prng.py"", line 690, in random_seed return random_seed_p.bind(seeds_arr, impl=impl) File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/prng.py"", line 702, in random_seed_impl base_arr = random_seed_impl_base(seeds, impl=impl) File ""/home/nonsans/miniforge3/envs/viper/lib/pyt",2024-07-24T14:53:04Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/22626,"This error can come up when JAX is used with an incompatible CUDNN version. You report using jax 0.4.13 with CUDA 12.5 â€“ that's a fairly old JAX version with a much more recent CUDA version, so I wouldn't be surprised if there are some incompatibilities. We've certainly never tested jax 0.4.13 with CUDA 12.5... I'd suggest updating to a more recent JAX version and see if that helps.",Thank you very much for pointing that out. I was sure i have tried to adjust the versions before but I probably messed up by having a different version support of jax installed when trying a different version from what I installed with jax. I tried it again and was extra cautious with having the same versions in place and now it works totally fine. With jax 0.4.13 there is no compatibility to the major version update of cudnn to 9.x I hope anyone who runs into the same issue will see this and be able to make it work too. I will now close this as it is solved.
1490,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bulid jaxlib from source code error:  ERROR: Build failed. Not running target! )ï¼Œ å†…å®¹æ˜¯ ( Description When I used the pip tool ( the command I used is:  pip install upgrade pip NVIDIA CUDA 12 installation Note: wheels only available on linux. pip install upgrade ""jax[cuda12]"" ï¼‰  to install jaxlib and jax, and then tried to import jax display : **This version of jaxlib was built using AVX instructions, which your CPU and/or operating system do not support. You may be able work around this issue by building jaxlib from sourceã€‚** then Follow the prompts I try to install from the source code refer to**https://jax.readthedocs.io/en/latest/developer.htmlbuildingjaxlibfromsource** .The error is take place. the key information is as below: >ERROR: /home/chat/.cache/bazel/_bazel_chat/e07bb7f39ed7b030eaf9bab18b94dd76/external/llvmproject/llvm/BUILD.bazel:1296:7: Generating code from table: lib/Target/RISCV/RISCV.td project//llvm:RISCVTargetParserDefGen__gen_riscv_target_def_genrule [for tool] failed: (Illegal instruction): bash failed: error executing command (from target project//llvm:RISCVTargetParserDefGen__gen_riscv_target_def_genrule)   (cd /home/chat/.cache/bazel/_bazel_chat/e07bb7f39ed7b030eaf9bab18b94dd76/execroot/__main__ && \  exec env  \  LD_LIBRARY_PATH=/usr/local/cuda12.3/lib64:/usr/local/cuda12.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",chat,Bulid jaxlib from source code error:  ERROR: Build failed. Not running target! ," Description When I used the pip tool ( the command I used is:  pip install upgrade pip NVIDIA CUDA 12 installation Note: wheels only available on linux. pip install upgrade ""jax[cuda12]"" ï¼‰  to install jaxlib and jax, and then tried to import jax display : **This version of jaxlib was built using AVX instructions, which your CPU and/or operating system do not support. You may be able work around this issue by building jaxlib from sourceã€‚** then Follow the prompts I try to install from the source code refer to**https://jax.readthedocs.io/en/latest/developer.htmlbuildingjaxlibfromsource** .The error is take place. the key information is as below: >ERROR: /home/chat/.cache/bazel/_bazel_chat/e07bb7f39ed7b030eaf9bab18b94dd76/external/llvmproject/llvm/BUILD.bazel:1296:7: Generating code from table: lib/Target/RISCV/RISCV.td project//llvm:RISCVTargetParserDefGen__gen_riscv_target_def_genrule [for tool] failed: (Illegal instruction): bash failed: error executing command (from target project//llvm:RISCVTargetParserDefGen__gen_riscv_target_def_genrule)   (cd /home/chat/.cache/bazel/_bazel_chat/e07bb7f39ed7b030eaf9bab18b94dd76/execroot/__main__ && \  exec env  \  LD_LIBRARY_PATH=/usr/local/cuda12.3/lib64:/usr/local/cuda12.",2024-07-24T07:29:07Z,bug type:support,open,0,1,https://github.com/jax-ml/jax/issues/22614,"Hi   Are you attempting to install JAX with CUDA support on Windows? If so, please note that JAX with CUDA support is not available on windows. Please let us know if you are using any other OS. Thank you."
436,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(fix memory leak in cond jaxpr tracing)ï¼Œ å†…å®¹æ˜¯ (fixes CC(Memory leak when closing over large constants in compiled jax.lax.cond statement) We already have test coverage for getting cache hits in `LaxControlFlowTest.LaxControlFlowTest.test_cond_excessive_compilation`.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,fix memory leak in cond jaxpr tracing,fixes CC(Memory leak when closing over large constants in compiled jax.lax.cond statement) We already have test coverage for getting cache hits in `LaxControlFlowTest.LaxControlFlowTest.test_cond_excessive_compilation`.,2024-07-23T23:39:34Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/22610
802,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Mosaic GPU] Move barrier allocation to SMEM scratch specs)ï¼Œ å†…å®¹æ˜¯ ([Mosaic GPU] Move barrier allocation to SMEM scratch specs This is slightly less convenient than our previous approach but it has two main upsides: 1. It lets us automatically emit necessary fences and barriers for use with block clusters 2. It lets us share the same block/cluster barrier for all initializations of mbarriers This change also moves away from the nvgpu dialect for barriers and allocates them in dynamic SMEM instead of relying on static SMEM. This should give us more control over SMEM layouts and alignments, and simplifies the lowering process.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[Mosaic GPU] Move barrier allocation to SMEM scratch specs,"[Mosaic GPU] Move barrier allocation to SMEM scratch specs This is slightly less convenient than our previous approach but it has two main upsides: 1. It lets us automatically emit necessary fences and barriers for use with block clusters 2. It lets us share the same block/cluster barrier for all initializations of mbarriers This change also moves away from the nvgpu dialect for barriers and allocates them in dynamic SMEM instead of relying on static SMEM. This should give us more control over SMEM layouts and alignments, and simplifies the lowering process.",2024-07-23T14:19:21Z,,closed,0,0,https://github.com/jax-ml/jax/issues/22596
398,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Remove the unaccelerate_deprecation utility)ï¼Œ å†…å®¹æ˜¯ (I don't think `unaccelerate_getattr_deprecation` is a pattern we should use, even if only internally. Instead, we should handle accelerated deprecations directly in the tests.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Remove the unaccelerate_deprecation utility,"I don't think `unaccelerate_getattr_deprecation` is a pattern we should use, even if only internally. Instead, we should handle accelerated deprecations directly in the tests.",2024-07-22T19:34:27Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/22574
1442,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump pytest from 8.1.1 to 8.3.1)ï¼Œ å†…å®¹æ˜¯ (Bumps pytest from 8.1.1 to 8.3.1.  Release notes Sourced from pytest's releases.  8.3.1 pytest 8.3.1 (20240720) The 8.3.0 release failed to include the change notes and docs for the release. This patch release remedies this. There are no other changes. 8.3.0 pytest 8.3.0 (20240720) New features    CC(Fix `Cmd.use_rawinput` type in debuggers): Added [xfailtb]{.titleref} flag, which turns on traceback output for XFAIL results.  If the [xfailtb]{.titleref} flag is not given, tracebacks for XFAIL results are NOT shown. The style of traceback for XFAIL is set with [tb]{.titleref}, and can be [autono]{.titleref}. Note: Even if you have [xfailtb]{.titleref} set, you won't see them if [tb=no]{.titleref}.  Some history: With pytest 8.0, [rx]{.titleref} or [ra]{.titleref} would not only turn on summary reports for xfail, but also report the tracebacks for xfail results. This caused issues with some projects that utilize xfail, but don't want to see all of the xfail tracebacks. This change detaches xfail tracebacks from [rx]{.titleref}, and now we turn on xfail tracebacks with [xfailtb]{.titleref}. With this, the default [rx]{.titleref}/ [ra]{.titleref} behavior is identical to pre8.0 with respect to xfail tracebacks. Whil)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Bump pytest from 8.1.1 to 8.3.1,"Bumps pytest from 8.1.1 to 8.3.1.  Release notes Sourced from pytest's releases.  8.3.1 pytest 8.3.1 (20240720) The 8.3.0 release failed to include the change notes and docs for the release. This patch release remedies this. There are no other changes. 8.3.0 pytest 8.3.0 (20240720) New features    CC(Fix `Cmd.use_rawinput` type in debuggers): Added [xfailtb]{.titleref} flag, which turns on traceback output for XFAIL results.  If the [xfailtb]{.titleref} flag is not given, tracebacks for XFAIL results are NOT shown. The style of traceback for XFAIL is set with [tb]{.titleref}, and can be [autono]{.titleref}. Note: Even if you have [xfailtb]{.titleref} set, you won't see them if [tb=no]{.titleref}.  Some history: With pytest 8.0, [rx]{.titleref} or [ra]{.titleref} would not only turn on summary reports for xfail, but also report the tracebacks for xfail results. This caused issues with some projects that utilize xfail, but don't want to see all of the xfail tracebacks. This change detaches xfail tracebacks from [rx]{.titleref}, and now we turn on xfail tracebacks with [xfailtb]{.titleref}. With this, the default [rx]{.titleref}/ [ra]{.titleref} behavior is identical to pre8.0 with respect to xfail tracebacks. Whil",2024-07-22T17:08:58Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/22570,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
1447,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump fonttools from 4.51.0 to 4.53.1)ï¼Œ å†…å®¹æ˜¯ (Bumps fonttools from 4.51.0 to 4.53.1.  Release notes Sourced from fonttools's releases.  4.53.1  [feaLib] Improve the sharing of inline chained lookups ( CC(jaxpr typechecking: custom rules and errors)) [otlLib] Correct the calculation of OS/2.usMaxContext with reversed chaining contextual single substitutions ( CC(Clarify docs on jax.lax.cond.)) [misc.visitor] Visitors search the inheritance chain of objects they are visiting ( CC([jax2tf] Add support for custom JVP/VJP))  4.53.0  [ttLib.removeOverlaps] Support CFF table to aid in downconverting CFF2 fonts (fonttools/fonttools CC(WIP: Print JAX arrays to full precision)) [avar] Fix crash when accessing notyetexisting attribute (fonttools/fonttools CC(Fix eigh JVP to ensure that both the primal and tangents of the eigenâ€¦)) [docs] Add buildMathTable to otlLib.builder documentation (fonttools/fonttools CC(Restrict .compress tests to arrays only.)) [feaLib] Allow UTF8 with BOM when reading features (fonttools/fonttools CC(Bump jaxlib version to 0.1.49 and update WORKSPACE)) [SVGPathPen] Revert rounding coordinates to two decimal places by default (fonttools/fonttools CC(Avoid direct type/dtype comparisons to fix NumPy 1.19 deprecation warâ€¦)) [varLib.instancer] Ref)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bump fonttools from 4.51.0 to 4.53.1,Bumps fonttools from 4.51.0 to 4.53.1.  Release notes Sourced from fonttools's releases.  4.53.1  [feaLib] Improve the sharing of inline chained lookups ( CC(jaxpr typechecking: custom rules and errors)) [otlLib] Correct the calculation of OS/2.usMaxContext with reversed chaining contextual single substitutions ( CC(Clarify docs on jax.lax.cond.)) [misc.visitor] Visitors search the inheritance chain of objects they are visiting ( CC([jax2tf] Add support for custom JVP/VJP))  4.53.0  [ttLib.removeOverlaps] Support CFF table to aid in downconverting CFF2 fonts (fonttools/fonttools CC(WIP: Print JAX arrays to full precision)) [avar] Fix crash when accessing notyetexisting attribute (fonttools/fonttools CC(Fix eigh JVP to ensure that both the primal and tangents of the eigenâ€¦)) [docs] Add buildMathTable to otlLib.builder documentation (fonttools/fonttools CC(Restrict .compress tests to arrays only.)) [feaLib] Allow UTF8 with BOM when reading features (fonttools/fonttools CC(Bump jaxlib version to 0.1.49 and update WORKSPACE)) [SVGPathPen] Revert rounding coordinates to two decimal places by default (fonttools/fonttools CC(Avoid direct type/dtype comparisons to fix NumPy 1.19 deprecation warâ€¦)) [varLib.instancer] Ref,2024-07-22T17:08:36Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/22569,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
1450,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump exceptiongroup from 1.2.1 to 1.2.2)ï¼Œ å†…å®¹æ˜¯ (Bumps exceptiongroup from 1.2.1 to 1.2.2.  Release notes Sourced from exceptiongroup's releases.  1.2.2  Removed an assert in exceptiongroup._formatting that caused compatibility issues with Sentry ( CC(remove unuzed bazel build rules, including bazel test definitions))     Changelog Sourced from exceptiongroup's changelog.  Version history This library adheres to Semantic Versioning 2.0 &lt;http://semver.org/&gt;_. 1.2.2  Removed an assert in exceptiongroup._formatting that caused compatibility issues with Sentry ( CC(remove unuzed bazel build rules, including bazel test definitions) &lt;https://github.com/agronholm/exceptiongroup/issues/123&gt;_)  1.2.1  Updated the copying of __notes__ to match CPython behavior (PR by CF BolzTereick) Corrected the type annotation of the exception handler callback to accept a BaseExceptionGroup instead of BaseException Fixed type errors on Python &lt; 3.10 and the type annotation of suppress() (PR by John Litborn)  1.2.0  Added special monkeypatching if Apport &lt;https://github.com/canonical/apport&gt;_ has overridden sys.excepthook so it will format exception groups correctly (PR by John Litborn) Added a backport of contextlib.suppress() from Python 3.12.1 which also handles)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bump exceptiongroup from 1.2.1 to 1.2.2,"Bumps exceptiongroup from 1.2.1 to 1.2.2.  Release notes Sourced from exceptiongroup's releases.  1.2.2  Removed an assert in exceptiongroup._formatting that caused compatibility issues with Sentry ( CC(remove unuzed bazel build rules, including bazel test definitions))     Changelog Sourced from exceptiongroup's changelog.  Version history This library adheres to Semantic Versioning 2.0 &lt;http://semver.org/&gt;_. 1.2.2  Removed an assert in exceptiongroup._formatting that caused compatibility issues with Sentry ( CC(remove unuzed bazel build rules, including bazel test definitions) &lt;https://github.com/agronholm/exceptiongroup/issues/123&gt;_)  1.2.1  Updated the copying of __notes__ to match CPython behavior (PR by CF BolzTereick) Corrected the type annotation of the exception handler callback to accept a BaseExceptionGroup instead of BaseException Fixed type errors on Python &lt; 3.10 and the type annotation of suppress() (PR by John Litborn)  1.2.0  Added special monkeypatching if Apport &lt;https://github.com/canonical/apport&gt;_ has overridden sys.excepthook so it will format exception groups correctly (PR by John Litborn) Added a backport of contextlib.suppress() from Python 3.12.1 which also handles",2024-07-22T17:07:49Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/22564,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
384,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(When logging cache hit/miss, include cache_key)ï¼Œ å†…å®¹æ˜¯ (I want these log messages:  To look like this:  Optionally we can truncate the cache key. I implemented this feature, let me know if you want a pull request. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"When logging cache hit/miss, include cache_key","I want these log messages:  To look like this:  Optionally we can truncate the cache key. I implemented this feature, let me know if you want a pull request. ",2024-07-21T09:00:25Z,enhancement,closed,0,3,https://github.com/jax-ml/jax/issues/22548,Thank you! Please make a pull request,I like your proposal  just let me note here that you can already log cache keys (including info about its calculation): ,sevcik Thank you. However these log lines do not contain the module name which makes them a bit difficult to search for. 
1452,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(JAX unable to match NVIDIA driver version)ï¼Œ å†…å®¹æ˜¯ ( Description Similar to this issue and this discussion, I'm on a HPC cluster where the NVIDIA GPU's have CUDA 12.4. When I try to install jax with `pip install jax[cuda12]` or with `pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html`, I get a warning suggesting that JAX is not heeding the 12.4 driver version, and is instead trying to use CUDA 12.5:  I understand that this isn't blocking, but I don't relish the idea of randomly slowed compilation times. Is there a reason JAX is unable to match the NVIDIA driver? Note: I have been able to get JAX to work with my cluster's provided CUDA 12.1 drivers by using the following pip args:  But I anticipate that sooner or later I will want to use a JAX version that is newer than what my cluster provides. Thanks!  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.30 jaxlib: 0.4.30 numpy:  2.0.0 python: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] jax.devices (1 total, 1 local): [cuda(id=0)] process_count: 1 platform: uname_result(system='Linux', node='computeg17155.o2.rc.hms.harvard.edu', release='3.10.01160.118.1.el7.x86_64', version=' CC(Python 3 compatibility issues) SMP Wed Apr 24 16:)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,JAX unable to match NVIDIA driver version," Description Similar to this issue and this discussion, I'm on a HPC cluster where the NVIDIA GPU's have CUDA 12.4. When I try to install jax with `pip install jax[cuda12]` or with `pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html`, I get a warning suggesting that JAX is not heeding the 12.4 driver version, and is instead trying to use CUDA 12.5:  I understand that this isn't blocking, but I don't relish the idea of randomly slowed compilation times. Is there a reason JAX is unable to match the NVIDIA driver? Note: I have been able to get JAX to work with my cluster's provided CUDA 12.1 drivers by using the following pip args:  But I anticipate that sooner or later I will want to use a JAX version that is newer than what my cluster provides. Thanks!  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.30 jaxlib: 0.4.30 numpy:  2.0.0 python: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] jax.devices (1 total, 1 local): [cuda(id=0)] process_count: 1 platform: uname_result(system='Linux', node='computeg17155.o2.rc.hms.harvard.edu', release='3.10.01160.118.1.el7.x86_64', version=' CC(Python 3 compatibility issues) SMP Wed Apr 24 16:",2024-07-19T14:51:14Z,bug,open,8,2,https://github.com/jax-ml/jax/issues/22534,Having the same issues here. Any idea?,"It is my understanding that the cuda forward compatibility exist for such reasons installingcompatpackages So even though I haven't tried it, installing the necessary compat and then changing `LD_LIBRARY_PATH` before pip installing should do it."
1469,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Could not create CUDNN handle: CUDNN_STATUS_INTERNAL_ERROR)ï¼Œ å†…å®¹æ˜¯ ( Description Hello im having an issue using jax and cuda. My error message looks like this: 20240719 15:49:01.341533: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:439] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR 20240719 15:49:01.341571: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:443] Memory usage: 2624061440 bytes free, 12524191744 bytes total. Traceback (most recent call last):   File ""scripts/train_vqgan.py"", line 202, in      main()   File ""scripts/train_vqgan.py"", line 32, in main     rng = jax.random.PRNGKey(config.seed)   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/random.py"", line 160, in PRNGKey     key = prng.seed_with_impl(impl, seed)   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/prng.py"", line 406, in seed_with_impl     return random_seed(seed, impl=impl)   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/prng.py"", line 690, in random_seed     return random_seed_p.bind(seeds_arr, impl=impl)   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/prng.py"", line 702, in random_seed_impl     base_arr = random_seed_impl_base(seeds, impl=impl)   File ""/hom)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Could not create CUDNN handle: CUDNN_STATUS_INTERNAL_ERROR," Description Hello im having an issue using jax and cuda. My error message looks like this: 20240719 15:49:01.341533: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:439] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR 20240719 15:49:01.341571: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:443] Memory usage: 2624061440 bytes free, 12524191744 bytes total. Traceback (most recent call last):   File ""scripts/train_vqgan.py"", line 202, in      main()   File ""scripts/train_vqgan.py"", line 32, in main     rng = jax.random.PRNGKey(config.seed)   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/random.py"", line 160, in PRNGKey     key = prng.seed_with_impl(impl, seed)   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/prng.py"", line 406, in seed_with_impl     return random_seed(seed, impl=impl)   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/prng.py"", line 690, in random_seed     return random_seed_p.bind(seeds_arr, impl=impl)   File ""/home/nonsans/miniforge3/envs/viper/lib/python3.8/sitepackages/jax/_src/prng.py"", line 702, in random_seed_impl     base_arr = random_seed_impl_base(seeds, impl=impl)   File ""/hom",2024-07-19T14:07:38Z,bug,closed,0,0,https://github.com/jax-ml/jax/issues/22533
344,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Mosaic GPU] Add reverse arithmetic functions for FragmentedArrary for convenience)ï¼Œ å†…å®¹æ˜¯ ([Mosaic GPU] Add reverse arithmetic functions for FragmentedArrary for convenience)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[Mosaic GPU] Add reverse arithmetic functions for FragmentedArrary for convenience,[Mosaic GPU] Add reverse arithmetic functions for FragmentedArrary for convenience,2024-07-19T13:32:47Z,,closed,0,0,https://github.com/jax-ml/jax/issues/22531
761,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jaxlib is not matching with cuda version 12. )ï¼Œ å†…å®¹æ˜¯ (I am trying to use GPU to run scripts but consistently the errors is coming ""ERROR: Could not find a version that satisfies the requirement jaxlib==0.4.29+cuda12.cudnn88 (from versions: 0.1.75+cuda11.cudnn805, 0.1.75+cuda11.cudnn82, 0.1.76+cuda11.cudnn805, 0.1.76+cuda11.cudnn82, etc"" after pip install pip install upgrade jax jaxlib==0.4.30+cuda12.cudnn89 f https://github.com/google/jaxpipinstallationgpucuda  [ ] Check for duplicate requests.  [ ] Describe your goal, and if possible provide a code snippet with a motivating example.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jaxlib is not matching with cuda version 12. ,"I am trying to use GPU to run scripts but consistently the errors is coming ""ERROR: Could not find a version that satisfies the requirement jaxlib==0.4.29+cuda12.cudnn88 (from versions: 0.1.75+cuda11.cudnn805, 0.1.75+cuda11.cudnn82, 0.1.76+cuda11.cudnn805, 0.1.76+cuda11.cudnn82, etc"" after pip install pip install upgrade jax jaxlib==0.4.30+cuda12.cudnn89 f https://github.com/google/jaxpipinstallationgpucuda  [ ] Check for duplicate requests.  [ ] Describe your goal, and if possible provide a code snippet with a motivating example.",2024-07-18T23:44:26Z,enhancement,closed,0,2,https://github.com/jax-ml/jax/issues/22522,"Please follow the installation instructions: https://jax.readthedocs.io/en/latest/installation.htmlinstallingjax  If that doesn't work, you'll need to provide more details of your Python installation. What Python version, for example?","Yes, it's worked, Thank you so much!!"
1226,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`custom_jvp` functions are slow when used with `jit`)ï¼Œ å†…å®¹æ˜¯ ( Description I want to define a function such that one of the input arguments is the derivative order that the user wants. My actual function is far more complex but here is an example that has the same issue.   This function calculates `dx`th derivative of $x^n$. So, $$dx = 0: f(x)=x^n $$ $$dx = 1: f(x)=nx^{n1} $$ $$dx = 2: f(x)=n(n1)x^{n2}$$ I don't want to use `grad()` to take the derivative. However, when I define the custom derivative to be the same function called with `dx+1`, the execution becomes way slower (even if I call the original function `dx`=0). I assume this is due to `custom_jvp` being a class and this adds some extra stuff being done when that function is called, even if we call it multiple times.  The speed is the key for my application, but I also need to define a custom derivative rule for the function for jacobian calculations involving this function. Is there a more optimized way to do that?  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,`custom_jvp` functions are slow when used with `jit`," Description I want to define a function such that one of the input arguments is the derivative order that the user wants. My actual function is far more complex but here is an example that has the same issue.   This function calculates `dx`th derivative of $x^n$. So, $$dx = 0: f(x)=x^n $$ $$dx = 1: f(x)=nx^{n1} $$ $$dx = 2: f(x)=n(n1)x^{n2}$$ I don't want to use `grad()` to take the derivative. However, when I define the custom derivative to be the same function called with `dx+1`, the execution becomes way slower (even if I call the original function `dx`=0). I assume this is due to `custom_jvp` being a class and this adds some extra stuff being done when that function is called, even if we call it multiple times.  The speed is the key for my application, but I also need to define a custom derivative rule for the function for jacobian calculations involving this function. Is there a more optimized way to do that?  System info (python version, jaxlib version, accelerator, etc.) ",2024-07-18T15:21:09Z,bug,closed,0,9,https://github.com/jax-ml/jax/issues/22513,"The issue here has to do with where the `jax.jit` is applied. When defining `fun`, the `jit` decorator is currently _inside_ the `custom_jvp`. This means that the full computation isn't being JIT compiled. To fix this issue, you can update the code as follows:  On my system I now see roughly the same performance. Note: For somewhat subtle reasons you can't actually just use:  You need to apply the `jit` after defining the JVP function. Hope this helps!","Thank you very much ! I had also tried to put `jax.jit` before `custom_jvp` but I got the following error,  I can apply `jax.jit` in my project probably as you suggest. But it would be perfect to be able to change the order and add jit as decorator instead of jitting manually.","Great! I'd say that supporting the other order of decorators is pretty low priority since `custom_jvp` functions typically don't live at the top level of a program. In other words, normally you call `custom_jvp` functions deep within some larger function. You'll typically get the most performance improvement by `jit`ting at the outermost scope. Therefore, I don't think we're likely to add support for calling `defjvp` on a `jit` function. For your use case, I don't think it adds too much overhead to add that one line!","I'm going to close this now because I think we solved the main issue. As I mentioned, I think the feature request is unlikely to be executed, but please feel free to open a new feature request with more details if it becomes a blocker.","  My actual function uses `static_argnums` with `.partial(jit, static_argnums=3)`. For some reason, that caused a problem with `jit`ing the function by  Instead I had to,  Is there something I am missing?","What was the issue you were seeing? I believe that `jax.jit(fun, static_argnums=3)` should work fine, and `jax.jit(fun.fun, static_argnums=3)` definitely isn't what you want because then you'll lose the `custom_jvp`! Can you post a minimal reproducer and the specific error you're seeing?","Let's say I added a redundant `if` statement, and now I need to specify `static_argnums` (my actual code is a bit more complex, but have similar problem).  The error message now is  And, yes, I realized that I was losing `custom_jvp` right after I sent the comment :)","Ah so in this case the JIT isn't the problem. You need to label that parameter also as nondifferentiable:  You could keep the inner jit, but I don't think it adds any value.","Oh thank you very much! It works now! For practicality, I still want to make a decorator that `jit`s and `custom_jvp`s the function. Here is my version,  I hope it doesn't have any more problems!!! EDIT: I had to change `.defjvp` to comply with this tutorial."
722,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([pallas] Request to add support for select/argmax primitive in Pallas TPU)ï¼Œ å†…å®¹æ˜¯ (Currently, it's not possible to use jnp.select in pallas kernel or in BlockSpec. Trying to do so causes the follow error. `jax._src.source_info_util.JaxStackTraceBeforeTransformation: NotImplementedError: Unimplemented primitive in Pallas TPU lowering: argmax. Please file an issue on https://github.com/google/jax/issues.` Using select allows better conditional control within blockspec. There's certain use case for condition branch, especially when selecting index. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[pallas] Request to add support for select/argmax primitive in Pallas TPU,"Currently, it's not possible to use jnp.select in pallas kernel or in BlockSpec. Trying to do so causes the follow error. `jax._src.source_info_util.JaxStackTraceBeforeTransformation: NotImplementedError: Unimplemented primitive in Pallas TPU lowering: argmax. Please file an issue on https://github.com/google/jax/issues.` Using select allows better conditional control within blockspec. There's certain use case for condition branch, especially when selecting index. ",2024-07-18T11:41:52Z,enhancement pallas,open,1,1,https://github.com/jax-ml/jax/issues/22508,"I'd like to add that using cond is an usable solution, but it's longer and uglier. "
414,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add jvp and transpose rule for ragged dot.)ï¼Œ å†…å®¹æ˜¯ (Add jvp and transpose rule for ragged dot. The numerical accuracy test is perfect against the reference implementation, and somewhat loose against the alt grad implementation used for testing.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Add jvp and transpose rule for ragged dot.,"Add jvp and transpose rule for ragged dot. The numerical accuracy test is perfect against the reference implementation, and somewhat loose against the alt grad implementation used for testing.",2024-07-18T06:36:01Z,,closed,0,0,https://github.com/jax-ml/jax/issues/22504
682,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jax_enable_x64 environment variable not respected by jax.numpy.indices)ï¼Œ å†…å®¹æ˜¯ ( Description Even when jax_enable_x64 is enabled, jnp.indices() continues to use int32. The issue is more relevant when the output is fed into (for instance) vmap, which does not promote to int64.  Here is a case where overflow occurs for a reason that is not obvious just looking at the code.  Output:  It is still possible to force int64 by instead using  Output:   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jax_enable_x64 environment variable not respected by jax.numpy.indices," Description Even when jax_enable_x64 is enabled, jnp.indices() continues to use int32. The issue is more relevant when the output is fed into (for instance) vmap, which does not promote to int64.  Here is a case where overflow occurs for a reason that is not obvious just looking at the code.  Output:  It is still possible to force int64 by instead using  Output:   System info (python version, jaxlib version, accelerator, etc.) ",2024-07-17T23:05:44Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/22501,Thanks for the report! I do think this is a bug and I've implemented a possible fix in CC(Fix dtype canonicalization in `jnp.indices`). We'll see if any other members of the team know of a reason why this isn't right!
821,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Cannot execute custom operation with XLA typed FFI)ï¼Œ å†…å®¹æ˜¯ ( Description I am attempting to add a custom operation using the typed (rather than untyped( XLA FFI api. However, I get a warning/error when trying to use it that that the symbol cannot be found:  Unlike in the examples, my custom operation uses the typed interface like   In another C++ file:  In python (some boilerplate hidden):  Reading the XLA source suggests that `SimpleOrcJIT` looks up custom symbols in `xla::CustomCallTargetRegistry`, which is only populated for `api_version == 0` in `PyRegisterCustomCallTarget`.  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Cannot execute custom operation with XLA typed FFI," Description I am attempting to add a custom operation using the typed (rather than untyped( XLA FFI api. However, I get a warning/error when trying to use it that that the symbol cannot be found:  Unlike in the examples, my custom operation uses the typed interface like   In another C++ file:  In python (some boilerplate hidden):  Reading the XLA source suggests that `SimpleOrcJIT` looks up custom symbols in `xla::CustomCallTargetRegistry`, which is only populated for `api_version == 0` in `PyRegisterCustomCallTarget`.  System info (python version, jaxlib version, accelerator, etc.) ",2024-07-17T20:23:25Z,bug,closed,0,8,https://github.com/jax-ml/jax/issues/22499,"Thanks for the report. This is absolutely expected to work! I'm currently writing up a tutorial about this (see https://github.com/google/jax/pull/22095) and I haven't had any issues running ~equivalent code on CPU with all the same versions of JAX that you're using. I'll see if I can dig into this a bit, but how about you take a look at the draft of my tutorial and see if you notice anything useful there. Everything you've included as sample code here looks right to me on a first pass!",Also I confirmed the untyped ffi works correctly with the same registration code and changing the api version. Do I need `XLA_FFI_REGISTER_HANDLER` as well? I took a look at the tutorial and it uses `jax.extend.ffi.ffi_call`. I can't find this in my distribution  is it new? ,"You shouldn't need to call `XLA_FFI_REGISTER_HANDLER`. The references to that macro in `jaxlib` are unused  they're only for users who use `jaxlib` without the Python frontend. The only way to register user FFI calls is via Python. > I took a look at the tutorial and it uses `jax.extend.ffi.ffi_call`. I can't find this in my distribution  is it new? I thought it got into 0.4.30, but it looks like I actually missed that cutoff (here's the PR: https://github.com/google/jax/pull/21925). It will be included in the next release!","I wanted to check in to confirm that you were able to get the code working, . Is that right? I'm still not totally sure what the problem was before, but if you have something that works for now, let's close the issue and you can open a new one if you run into more problems. These APIs might still have some rough edges and I'm keen to get them sorted out!",No I wasn't able to get it working. I switched to the untyped api which works fine. Once the next release is out I can test it again with `ffi_call`.,Oh I see. I don't expect that `ffi_call` will solve this issue though. Can you put together a complete minimal example (i.e. the full C++ and Python files and tell me how you're compiling it) so that I can reproduce the issue locally?, â€” I wanted to check back in here to see if you managed to get to the bottom of this?,"I'm going to close this as stale, but please comment if you continue to run into issues!"
629,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(look mum I can still can edit Pallas, AGI! Optimize pipeline emitter scheduler by omitting copies of accumulators during iteration in which they are going to be zeroed out.)ï¼Œ å†…å®¹æ˜¯ (look mum I can still can edit Pallas, AGI! Optimize pipeline emitter scheduler by omitting copies of accumulators during iteration in which they are going to be zeroed out. Also, add some clarifying comments and set fixed RHS schedules of matmul reduce scatter implementations.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"look mum I can still can edit Pallas, AGI! Optimize pipeline emitter scheduler by omitting copies of accumulators during iteration in which they are going to be zeroed out.","look mum I can still can edit Pallas, AGI! Optimize pipeline emitter scheduler by omitting copies of accumulators during iteration in which they are going to be zeroed out. Also, add some clarifying comments and set fixed RHS schedules of matmul reduce scatter implementations.",2024-07-17T17:56:56Z,,closed,0,0,https://github.com/jax-ml/jax/issues/22496
364,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([MosaicGPU] Add a __repr__ to FragmentedArray so pallas:mosaic_gpu errors are more readable.)ï¼Œ å†…å®¹æ˜¯ ([MosaicGPU] Add a __repr__ to FragmentedArray so pallas:mosaic_gpu errors are more readable.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[MosaicGPU] Add a __repr__ to FragmentedArray so pallas:mosaic_gpu errors are more readable.,[MosaicGPU] Add a __repr__ to FragmentedArray so pallas:mosaic_gpu errors are more readable.,2024-07-17T16:44:26Z,,closed,0,0,https://github.com/jax-ml/jax/issues/22492
1350,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add Pallas helpers for verifying complicated kernels)ï¼Œ å†…å®¹æ˜¯ (Add Pallas helpers for verifying complicated kernels This change adds two capabilities that should allow us to verify existing complicated kernels without having to modify their code significantly. 1. Kernels that contain complicated patterns such as datadependent controlflow    cannot currently be modeled. But, in many cases the controlflow does not    actually matter (e.g. it's only used to cut down on unnecessary work and    should not influence communication). Now, `pltpu.verification.assume(x, when_verifying=y)`    will normally evaluate to `x`, but will be replaced by `y` in the model.    This allows us to e.g. replace dynamic values with constants. 2. We might not be interested in exporting all the details of a kernel to the    model. For example, there is little point in verifying that the local DMA    pipeline generated by `pltpu.emit_pipeline` has no races. Now, using    `pltpu.verification.define_model` a function that contains a pipeline    can be replaced by a simplified implementation that e.g. specifies the    set of read and written memory references using `pltpu.verification.pretend`.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add Pallas helpers for verifying complicated kernels,"Add Pallas helpers for verifying complicated kernels This change adds two capabilities that should allow us to verify existing complicated kernels without having to modify their code significantly. 1. Kernels that contain complicated patterns such as datadependent controlflow    cannot currently be modeled. But, in many cases the controlflow does not    actually matter (e.g. it's only used to cut down on unnecessary work and    should not influence communication). Now, `pltpu.verification.assume(x, when_verifying=y)`    will normally evaluate to `x`, but will be replaced by `y` in the model.    This allows us to e.g. replace dynamic values with constants. 2. We might not be interested in exporting all the details of a kernel to the    model. For example, there is little point in verifying that the local DMA    pipeline generated by `pltpu.emit_pipeline` has no races. Now, using    `pltpu.verification.define_model` a function that contains a pipeline    can be replaced by a simplified implementation that e.g. specifies the    set of read and written memory references using `pltpu.verification.pretend`.",2024-07-17T14:49:33Z,,closed,0,0,https://github.com/jax-ml/jax/issues/22491
871,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DCT: add support for `orthogonalize=False`)ï¼Œ å†…å®¹æ˜¯ (> On the topic of `norm=""ortho""`, please also be aware of https://github.com/scipy/scipy/issues/21198. Unlike, scipy, JAX doesn't provide the `orthogonalize=False` option. This makes me a little skeptical of using JAX's DCT (even for type 2 DCT). The two linked issues shown there only discuss the effect on the type 1 DCT, but I think the same issues should occur to types 1,2,3. > Ideally, I'd set `norm=""backward""` to avoid this issue, but given that JAX only implements `norm=""ortho""` I think JAX should implement an option to set `orthogonalize=False`. _Originally posted by  in https://github.com/google/jax/issues/22466issuecomment2231562983_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,DCT: add support for `orthogonalize=False`,"> On the topic of `norm=""ortho""`, please also be aware of https://github.com/scipy/scipy/issues/21198. Unlike, scipy, JAX doesn't provide the `orthogonalize=False` option. This makes me a little skeptical of using JAX's DCT (even for type 2 DCT). The two linked issues shown there only discuss the effect on the type 1 DCT, but I think the same issues should occur to types 1,2,3. > Ideally, I'd set `norm=""backward""` to avoid this issue, but given that JAX only implements `norm=""ortho""` I think JAX should implement an option to set `orthogonalize=False`. _Originally posted by  in https://github.com/google/jax/issues/22466issuecomment2231562983_",2024-07-16T18:57:31Z,enhancement,open,0,1,https://github.com/jax-ml/jax/issues/22476,"To summarize, for DCT and DST. the `ortho` option is presented as a normalization flag, `norm=""ortho""`, and this seems misleading because this option changes the transformation to not behave like the familiar FFT anymore. Scipy later added the the flag `orthogonalize=False` to try to fix this. For my work, I need `orthogonalize=False`, but JAX doesn't have that option implemented."
1445,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Wrong normalization in FFT methods)ï¼Œ å†…å®¹æ˜¯ ( Description Summary: The discrete cosine transformation libraries normalize the transforms incorrectly. I am aware that JAX's documentation states that only `norm=""ortho""` is supported, however, this still requires the user to explicitly pass `norm=ortho`, which the documentation does not state. To be clear, jax gives incorrect results unless `norm=""ortho""` is passed explicitly. Jax also doesn't seem to implement backward dct and forward idct correctly. If this is not supported yet, then JAX should fail early. The effect of these bugs is that interpolation with fft's without specifying additional flags gives incorrect results. Minimal reproductions. Bug 1.  !out Ok, so now the user wonders whether the documentation is out of date, and tries to find out what is the normalization that JAX implements by default. They may conclude that default is `backward` on `dct` and `forward` on `idct`. Scipy's default is `backward` for both. This departure from `scipy` seems confusing given that this methods are in `jax.scipy.fft`. More confusing, is that when we specify explicitly to do `backward` on `dct` and `forward` on `idct` for `jax`, the results change, so it seems JAX does not implement the options that is does by def)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Wrong normalization in FFT methods," Description Summary: The discrete cosine transformation libraries normalize the transforms incorrectly. I am aware that JAX's documentation states that only `norm=""ortho""` is supported, however, this still requires the user to explicitly pass `norm=ortho`, which the documentation does not state. To be clear, jax gives incorrect results unless `norm=""ortho""` is passed explicitly. Jax also doesn't seem to implement backward dct and forward idct correctly. If this is not supported yet, then JAX should fail early. The effect of these bugs is that interpolation with fft's without specifying additional flags gives incorrect results. Minimal reproductions. Bug 1.  !out Ok, so now the user wonders whether the documentation is out of date, and tries to find out what is the normalization that JAX implements by default. They may conclude that default is `backward` on `dct` and `forward` on `idct`. Scipy's default is `backward` for both. This departure from `scipy` seems confusing given that this methods are in `jax.scipy.fft`. More confusing, is that when we specify explicitly to do `backward` on `dct` and `forward` on `idct` for `jax`, the results change, so it seems JAX does not implement the options that is does by def",2024-07-15T22:15:24Z,bug,closed,2,11,https://github.com/jax-ml/jax/issues/22466,"For the second example, in the newer `scipy.fft` libraries, the equivalent of  becomes  Still, replacing  with   does not change any issues raised in the original post.",Thanks for reporting this so clearly!,"It's unrelated to this issue, but on the topic of `norm=""ortho""`, please also be aware of https://github.com/scipy/scipy/issues/21198. Unlike, scipy, JAX doesn't provide the `orthogonalize=False` option. This makes me a little skeptical of using JAX's DCT (even for type 2 DCT). The two linked issues shown there only discuss the effect on the type 1 DCT, but I think the same issues should occur to types 1,2,3. Ideally, I'd set `norm=""backward""` to avoid this issue, but given that JAX only implements `norm=""ortho""` I think JAX should implement an option to set `orthogonalize=False`. Should this comment be referenced in a new issue?","> Should this comment be referenced in a new issue? Yes, new issue please!",Hello everyone. I have recently started using JAX and interested to contribute in this community.  can I start working on this issue? Is there anything you would suggest me before starting on it?,Thanks for the report. I've added validation that raises an error for unsupported `norm` values in CC(jax.scipy.fft: error for unsupported norm argument). As for the incorrect results with `norm=None`: this is strange because we have a suite of tests against scipy's implementation for the default `norm=None`; for example https://github.com/google/jax/blob/db05734041a6b6c26440b087a22c036825665dc2/tests/scipy_fft_test.pyL48L62 There must be some relevant case in that test parameterization that is not covered.,"Ah, it looks like our implementation does match that of `scipy.fft`, although it does not match `scipy.fftpack`:  !download I believe that `scipy.fftpack` is deprecated, so you should be using the `scipy.fft` versions. With that in mind, I think the only bug here involves validation of unsupported `norm` arguments, which is fixed in CC(jax.scipy.fft: error for unsupported norm argument). If you can find any discrepancies between the outputs of `jax.scipy.fft` functions and `scipy.fft` functions, please let us know!"," CC(jax.scipy.fft.dct: implement & test norm='backward') implements and tests `norm='backward'` for all `dct` functions, and ensures that its behavior matches the corresponding implementations in `scipy.fft`."," Thanks for fixing this! It looks like CC(jax.scipy.fft: error for unsupported norm argument) and CC(jax.scipy.fft.dct: implement & test norm='backward') should resolve the issue. Perhaps it's useful to document: CC(jax.scipy.fft: error for unsupported norm argument) alone would not solve the issue: After the first PR, 1. The documentation states that JAX only supports `norm=""ortho""`. 2. It's good that JAX now fails if an unsupported argument is given. However, note that JAX will now fail for all options `norm not in [""ortho"", None]`. 3. 1 and 2 both imply that by default, `norm=""ortho""` will be performed. This differs from `scipy.fft`, which states that, by default, `norm=""backward""`. In that case matching plots you showed in your previous comment would indicate a bug, because the two plots should differ. We should instead expect the following two plots to match.  !image The second PR updates the documentation to state `norm=""backward""` is supported and used by default. In that case, the correct usage becomes  which produces matching plots as expected.",I think https://github.com/google/jax/pull/22571 addresses all your comments â€“Â please let me know if that is not the case.,"Yes it does, thanks."
1444,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump pillow from 10.3.0 to 10.4.0)ï¼Œ å†…å®¹æ˜¯ (Bumps pillow from 10.3.0 to 10.4.0.  Release notes Sourced from pillow's releases.  10.4.0 https://pillow.readthedocs.io/en/stable/releasenotes/10.4.0.html Changes  Raise FileNotFoundError if show_file() path does not exist  CC(Deprecate X64 flag) [@â€‹radarhere] Improved reading 16bit TGA images with colour  CC(`vmap(grad(all_gather))` errors) [@â€‹Yay295] Fixed processing multiple JPEG EXIF markers  CC(Numerical errors in `jax.numpy.einsum`) [@â€‹radarhere] Do not preserve EXIFIFD tag by default when saving TIFF images  CC(Use all_gather+reduce_scatter HLOs on TPU) [@â€‹radarhere] Added ImageFont.load_default_imagefont()  CC([JAX] Fix pylint errors.) [@â€‹radarhere] Added Image.WARN_POSSIBLE_FORMATS  CC([sparse]: make repr safe for invalid BCOO objects) [@â€‹radarhere] Do not presume &quot;xmp&quot; info simply because &quot;XML:com.adobe.xmp&quot; info exists  CC(JIT explicit pytree arguments) [@â€‹radarhere] Remove zerobyte end padding when parsing any XMP data  CC(`jax.nn.sigmoid` custom jvp leak) [@â€‹radarhere] Do not detect Ultra HDR images as MPO  CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) [@â€‹radarhere] Raise SyntaxError specific to JP2  CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) [@â€‹Yay295] Do not use first frame duration for other frames when saving APNG images  CC(Fix incorrect)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bump pillow from 10.3.0 to 10.4.0,Bumps pillow from 10.3.0 to 10.4.0.  Release notes Sourced from pillow's releases.  10.4.0 https://pillow.readthedocs.io/en/stable/releasenotes/10.4.0.html Changes  Raise FileNotFoundError if show_file() path does not exist  CC(Deprecate X64 flag) [@â€‹radarhere] Improved reading 16bit TGA images with colour  CC(`vmap(grad(all_gather))` errors) [@â€‹Yay295] Fixed processing multiple JPEG EXIF markers  CC(Numerical errors in `jax.numpy.einsum`) [@â€‹radarhere] Do not preserve EXIFIFD tag by default when saving TIFF images  CC(Use all_gather+reduce_scatter HLOs on TPU) [@â€‹radarhere] Added ImageFont.load_default_imagefont()  CC([JAX] Fix pylint errors.) [@â€‹radarhere] Added Image.WARN_POSSIBLE_FORMATS  CC([sparse]: make repr safe for invalid BCOO objects) [@â€‹radarhere] Do not presume &quot;xmp&quot; info simply because &quot;XML:com.adobe.xmp&quot; info exists  CC(JIT explicit pytree arguments) [@â€‹radarhere] Remove zerobyte end padding when parsing any XMP data  CC(`jax.nn.sigmoid` custom jvp leak) [@â€‹radarhere] Do not detect Ultra HDR images as MPO  CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) [@â€‹radarhere] Raise SyntaxError specific to JP2  CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) [@â€‹Yay295] Do not use first frame duration for other frames when saving APNG images  CC(Fix incorrect,2024-07-15T17:40:24Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/22455,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
1452,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump etils[epath,epy] from 1.7.0 to 1.9.2)ï¼Œ å†…å®¹æ˜¯ (Bumps [etils[epath,epy]](https://github.com/google/etils) from 1.7.0 to 1.9.2.  Release notes Sourced from etils[epath,epy]'s releases.  v1.9.2  epath:  Support pydantic serialization of epath.Path    v1.9.1  epath:  Fix an infinite recursion on is_relative_to for Python&gt;=3.12.    v1.9.0  epy:  Add epy.lazy_api_imports to lazyimport __init__.py symbols. Removed: epy.cached_property epy.lazy_imports: Error callback accept a str to autoreraise with additional info. Fix mixing epy.lazy_imports() with epy.binary_adhoc().   ecolab:  Added reload_workspace=True to adhoc to autoreload from workspace Add ecolab.get_permalink() Fix ecolab.inspect not finding static files when the kernel contain partial etils deps.   epath:  Fix error when importlib.resources.files return MultiplexedPath Fix gs:// URI for 3.12 Fix .walk 3.12 error (topdown &gt; top_down rename)   Full compatibility with Python 3.12 (unit tests run on both 3.11 and 3.12).  v1.8.0  Drop Python 3.10 support. epy:  epy.pretty_repr: Add support for namedtuple   ecolab:  Add ecolab.disp(obj) Add ;h for syntax highlighting with autodisplay Fix proto error on import       Changelog Sourced from etils[epath,epy]'s changelog.  [1.9.2]  20240612  epath:  Support )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"Bump etils[epath,epy] from 1.7.0 to 1.9.2","Bumps [etils[epath,epy]](https://github.com/google/etils) from 1.7.0 to 1.9.2.  Release notes Sourced from etils[epath,epy]'s releases.  v1.9.2  epath:  Support pydantic serialization of epath.Path    v1.9.1  epath:  Fix an infinite recursion on is_relative_to for Python&gt;=3.12.    v1.9.0  epy:  Add epy.lazy_api_imports to lazyimport __init__.py symbols. Removed: epy.cached_property epy.lazy_imports: Error callback accept a str to autoreraise with additional info. Fix mixing epy.lazy_imports() with epy.binary_adhoc().   ecolab:  Added reload_workspace=True to adhoc to autoreload from workspace Add ecolab.get_permalink() Fix ecolab.inspect not finding static files when the kernel contain partial etils deps.   epath:  Fix error when importlib.resources.files return MultiplexedPath Fix gs:// URI for 3.12 Fix .walk 3.12 error (topdown &gt; top_down rename)   Full compatibility with Python 3.12 (unit tests run on both 3.11 and 3.12).  v1.8.0  Drop Python 3.10 support. epy:  epy.pretty_repr: Add support for namedtuple   ecolab:  Add ecolab.disp(obj) Add ;h for syntax highlighting with autodisplay Fix proto error on import       Changelog Sourced from etils[epath,epy]'s changelog.  [1.9.2]  20240612  epath:  Support ",2024-07-15T17:40:05Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/22454,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
1448,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump setuptools from 69.2.0 to 70.3.0)ï¼Œ å†…å®¹æ˜¯ (Bumps setuptools from 69.2.0 to 70.3.0.  Changelog Sourced from setuptools's changelog.  v70.3.0 Features  Support for loading distutils from the standard library is now deprecated, including use of SETUPTOOLS_USE_DISTUTILS=stdlib and importing distutils before importing setuptools. ( CC(Do not call asarray() on inputs of jax.random.choice))  Bugfixes  pypa/distutils CC(Add multivariate_normal and pdf to jax.scipy.stats)  v70.2.0 Features  Updated distutils including significant changes to support Cygwin and mingw compilers. ( CC(Call check_user_dtype on all user dtypes))  Bugfixes  Fix distribution name normalisation (:pep:625) for valid versions that are not canonical (e.g. 1.02). ( CC(reduce testcase count of the numpydispatch CI check))  v70.1.1 Misc   CC(Add support for `all_to_all` in vmap)  v70.1.0 Features   Adopted the bdist_wheel command from the wheel project  by :user:agronholm ( CC(Nan Mean and Adds Nan Reducers to testing))   Improve error message when pkg_resources.ZipProvider tries to extract resources with a missing Egg  by :user:Avasam Added variables and parameter type annotations to pkg_resources to be nearly on par with typeshed.*  by :user:Avasam     ... (truncated)   Commits  356e9a0 Bump )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Bump setuptools from 69.2.0 to 70.3.0,"Bumps setuptools from 69.2.0 to 70.3.0.  Changelog Sourced from setuptools's changelog.  v70.3.0 Features  Support for loading distutils from the standard library is now deprecated, including use of SETUPTOOLS_USE_DISTUTILS=stdlib and importing distutils before importing setuptools. ( CC(Do not call asarray() on inputs of jax.random.choice))  Bugfixes  pypa/distutils CC(Add multivariate_normal and pdf to jax.scipy.stats)  v70.2.0 Features  Updated distutils including significant changes to support Cygwin and mingw compilers. ( CC(Call check_user_dtype on all user dtypes))  Bugfixes  Fix distribution name normalisation (:pep:625) for valid versions that are not canonical (e.g. 1.02). ( CC(reduce testcase count of the numpydispatch CI check))  v70.1.1 Misc   CC(Add support for `all_to_all` in vmap)  v70.1.0 Features   Adopted the bdist_wheel command from the wheel project  by :user:agronholm ( CC(Nan Mean and Adds Nan Reducers to testing))   Improve error message when pkg_resources.ZipProvider tries to extract resources with a missing Egg  by :user:Avasam Added variables and parameter type annotations to pkg_resources to be nearly on par with typeshed.*  by :user:Avasam     ... (truncated)   Commits  356e9a0 Bump ",2024-07-15T17:39:54Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/22453,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
1443,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump scipy from 1.13.1 to 1.14.0)ï¼Œ å†…å®¹æ˜¯ (Bumps scipy from 1.13.1 to 1.14.0.  Release notes Sourced from scipy's releases.  SciPy 1.14.0 Release Notes SciPy 1.14.0 is the culmination of 3 months of hard work. It contains many new features, numerous bugfixes, improved test coverage and better documentation. There have been a number of deprecations and API changes in this release, which are documented below. All users are encouraged to upgrade to this release, as there are a large number of bugfixes and optimizations. Before upgrading, we recommend that users check that their own code does not use deprecated SciPy functionality (to do so, run your code with python Wd and check for DeprecationWarning s). Our development attention will now shift to bugfix releases on the 1.14.x branch, and on adding new features on the main branch. This release requires Python 3.10+ and NumPy 1.23.5 or greater. For running on PyPy, PyPy3 6.0+ is required. Highlights of this release  SciPy now supports the new Accelerate library introduced in macOS 13.3, and has wheels built against Accelerate for macOS &gt;=14 resulting in significant performance improvements for many linear algebra operations. A new method, cobyqa, has been added to scipy.optimize.minimize  this is an inte)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bump scipy from 1.13.1 to 1.14.0,"Bumps scipy from 1.13.1 to 1.14.0.  Release notes Sourced from scipy's releases.  SciPy 1.14.0 Release Notes SciPy 1.14.0 is the culmination of 3 months of hard work. It contains many new features, numerous bugfixes, improved test coverage and better documentation. There have been a number of deprecations and API changes in this release, which are documented below. All users are encouraged to upgrade to this release, as there are a large number of bugfixes and optimizations. Before upgrading, we recommend that users check that their own code does not use deprecated SciPy functionality (to do so, run your code with python Wd and check for DeprecationWarning s). Our development attention will now shift to bugfix releases on the 1.14.x branch, and on adding new features on the main branch. This release requires Python 3.10+ and NumPy 1.23.5 or greater. For running on PyPy, PyPy3 6.0+ is required. Highlights of this release  SciPy now supports the new Accelerate library introduced in macOS 13.3, and has wheels built against Accelerate for macOS &gt;=14 resulting in significant performance improvements for many linear algebra operations. A new method, cobyqa, has been added to scipy.optimize.minimize  this is an inte",2024-07-15T17:39:42Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/22452,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
1463,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump the pip group across 1 directory with 2 updates)ï¼Œ å†…å®¹æ˜¯ (Bumps the pip group with 2 updates in the /build directory: zipp and setuptools. Updates `zipp` from 3.18.1 to 3.19.1  Changelog Sourced from zipp's changelog.  v3.19.1 Bugfixes  Improved handling of malformed zip files. ( CC(example Errata))  v3.19.0 Features  Implement is_symlink. ( CC(add oss test instructions, fix conv grad bug))  v3.18.2 No significant changes.    Commits  6d1cb72 Finalize fd604bd Merge pull request  CC(Canâ€™t import both jax and tensorflow (causes kernel restart)) from jaraco/bugfix/119malformedpaths c18417e Add news fragment. 58115d2 Employ SanitizedNames in CompleteDirs. Fixes broken test. 564fcc1 Add SanitizedNames mixin. 79a309f Add some assertions about malformed paths. 2d015c2 Merge https://github.com/jaraco/skeleton a595a0f Rename extras to align with core metadata spec. 608f90a Finalize 3a22d72 Merge pull request  CC(add tensordot) from jaraco/feature/issymlink Additional commits viewable in compare view    Updates `setuptools` from 69.2.0 to 70.0.0  Changelog Sourced from setuptools's changelog.  v70.0.0 Features  Emit a warning when [tools.setuptools] is present in pyproject.toml and will be ignored.  by :user:SnoopJ ( CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)) Improved AttributeError error message if pkg_reso)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bump the pip group across 1 directory with 2 updates,"Bumps the pip group with 2 updates in the /build directory: zipp and setuptools. Updates `zipp` from 3.18.1 to 3.19.1  Changelog Sourced from zipp's changelog.  v3.19.1 Bugfixes  Improved handling of malformed zip files. ( CC(example Errata))  v3.19.0 Features  Implement is_symlink. ( CC(add oss test instructions, fix conv grad bug))  v3.18.2 No significant changes.    Commits  6d1cb72 Finalize fd604bd Merge pull request  CC(Canâ€™t import both jax and tensorflow (causes kernel restart)) from jaraco/bugfix/119malformedpaths c18417e Add news fragment. 58115d2 Employ SanitizedNames in CompleteDirs. Fixes broken test. 564fcc1 Add SanitizedNames mixin. 79a309f Add some assertions about malformed paths. 2d015c2 Merge https://github.com/jaraco/skeleton a595a0f Rename extras to align with core metadata spec. 608f90a Finalize 3a22d72 Merge pull request  CC(add tensordot) from jaraco/feature/issymlink Additional commits viewable in compare view    Updates `setuptools` from 69.2.0 to 70.0.0  Changelog Sourced from setuptools's changelog.  v70.0.0 Features  Emit a warning when [tools.setuptools] is present in pyproject.toml and will be ignored.  by :user:SnoopJ ( CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)) Improved AttributeError error message if pkg_reso",2024-07-15T17:13:34Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/22449,"This pull request was built based on a group rule. Closing it will not ignore any of these versions in future pull requests. To ignore these dependencies, configure ignore rules in dependabot.yml"
465,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix compatibility of `jnp.unique` with numpy nightly)ï¼Œ å†…å®¹æ˜¯ (In https://github.com/numpy/numpy/pull/26914, the behavior of the `return_inverse` argument to `np.unique` was partially reverted to the prev2.0 behavior. The PR brings JAX's implementation compatible with the `numpy>2.0.0` behavior.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Fix compatibility of `jnp.unique` with numpy nightly,"In https://github.com/numpy/numpy/pull/26914, the behavior of the `return_inverse` argument to `np.unique` was partially reverted to the prev2.0 behavior. The PR brings JAX's implementation compatible with the `numpy>2.0.0` behavior.",2024-07-15T15:44:17Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/22445
789,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(CTRL+C Broken When Running distributed.initialize() on one TPU Host by Accident)ï¼Œ å†…å®¹æ˜¯ ( Issue Encountered a deadlock while running a JAXbased LLM training script on a TPUv432 pod. SSH'd into worker 0 and ran the script there directly, instead of using `worker all command ""...""`. CTRL+C should be able to kill the process, but it didn't in this case.  Command   Error Message  After this warning, the script hung indefinitely and CTRL+C did not kill it.  System info (python version, jaxlib version, accelerator, etc.)  Environment  JAX version: 0.4.28  Hardware: TPUv432 pod  Python version: 3.10  TPUUbuntubase22.04)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,CTRL+C Broken When Running distributed.initialize() on one TPU Host by Accident," Issue Encountered a deadlock while running a JAXbased LLM training script on a TPUv432 pod. SSH'd into worker 0 and ran the script there directly, instead of using `worker all command ""...""`. CTRL+C should be able to kill the process, but it didn't in this case.  Command   Error Message  After this warning, the script hung indefinitely and CTRL+C did not kill it.  System info (python version, jaxlib version, accelerator, etc.)  Environment  JAX version: 0.4.28  Hardware: TPUv432 pod  Python version: 3.10  TPUUbuntubase22.04",2024-07-14T12:46:40Z,better_errors,open,0,4,https://github.com/jax-ml/jax/issues/22436,"Author of EasyLM here. This is expected as you are only running the training script on a single host out of 4 hosts in a v432 pod. JAX uses a SPMD mode of execution, which means that you need to run the same script (same command) on all hosts in a TPU pod. You can manually ssh into each host and run the same command, or use gcloud ssh to do that by specifying `worker=all`. If you are looking for something more user friendly, you can also checkout my recent TPU pod command package, which helps you launch jobs on TPU pods.","smits what do you think would be the best behavior here? We could eventually time out completely and exit with an error instead of hanging, but the timeout would have to be pretty long to make sure we don't accidentally quit too early on large deployments. I'm curious if you have more ideas!","Yes, that sounds very tricky indeed. A timeout is not needed in my opinion. Being able to CTRL+C to cancel the TPU processes initiated with distributed.initialize() would solve the problem for me. I'd reckon the nesting depth is quite large, with multiple layers of TPU processes, such that even repeated CTRL+C attempts fail to fully terminate the entire process tree, however this would be desirable. If this is an unique edge case and you've never seen such a request before, I don't mind closing the Issue without a solution.","Ah great point about CTRL+C not working! I've also run into this, although haven't investigated yet. I think it does have something to do with calling into the C++ TPU runtime. Let's keep this issue open to track handling CTRL+C correctly in this situation. I took the liberty of editing your initial issue a bit to include this, so people don't have to read the whole thread (feel free to edit my edits more). Thanks for this feedback!"
317,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(INVALID_ARGUMENT: Mismatched PJRT plugin PJRT API version )ï¼Œ å†…å®¹æ˜¯ ( Description   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,INVALID_ARGUMENT: Mismatched PJRT plugin PJRT API version ," Description   System info (python version, jaxlib version, accelerator, etc.) ",2024-07-12T06:19:55Z,bug,closed,0,5,https://github.com/jax-ml/jax/issues/22416,"Hi, It's possible that there was an issue with the release. As a temporary workaround you can set the environment flag `ENABLE_PJRT_COMPATIBILITY=true`. Does this problem still persist if you install the latest JAX version (v0.4.30)?",can not find v0.4.30 for cudnn89,"new error:  but i have install 0.4.28+cuda12.cudnn89, also raise the same error for 0.4.27+cuda12.cudnn89","What hardware are you running on? Could you print out the output of `nvidiasmi`, `pip list`, and `import jax; jax.print_environment_info()`? I could try to reproduce on my end.",This problem was solved after I decided not to use conda
524,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(all inputs are nans in a next step iterationwhile using `jax.scipy.optimize.minimize`)ï¼Œ å†…å®¹æ˜¯ ( Description Just cannot understand why while using `minimize`, the first iteration is good, `m_term` and `LL` are all reasonable values, then the next step, all terms in `m_term` becomes nan.   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",glm,all inputs are nans in a next step iterationwhile using `jax.scipy.optimize.minimize`," Description Just cannot understand why while using `minimize`, the first iteration is good, `m_term` and `LL` are all reasonable values, then the next step, all terms in `m_term` becomes nan.   System info (python version, jaxlib version, accelerator, etc.) ",2024-07-12T04:26:42Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/22413,"I found it is my function property, sorry."
736,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Accuracy of `jax.experimental.sparse.spsolve`)ï¼Œ å†…å®¹æ˜¯ ( Description Is there any reason why `spsolve` would return a solution that is close to the true one, but not accurate? I can't provide a working example but I'm trying to solve a system (40000, 40000) sparse system (255101 nonzero entries), and the solution I get has `jnp.max(jnp.abs(matrixb))=0.0045`, which is not ideal. Any reason why, or is any way I can do to improve this accuracy?  System info (python version, jaxlib version, accelerator, etc.) Python version 3.10.13 Jax version 0.4.30 Running on gpu)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Accuracy of `jax.experimental.sparse.spsolve`," Description Is there any reason why `spsolve` would return a solution that is close to the true one, but not accurate? I can't provide a working example but I'm trying to solve a system (40000, 40000) sparse system (255101 nonzero entries), and the solution I get has `jnp.max(jnp.abs(matrixb))=0.0045`, which is not ideal. Any reason why, or is any way I can do to improve this accuracy?  System info (python version, jaxlib version, accelerator, etc.) Python version 3.10.13 Jax version 0.4.30 Running on gpu",2024-07-11T14:21:23Z,question,open,0,5,https://github.com/jax-ml/jax/issues/22399,"Just a guess: JAX computes results in float32 by default; iterative solvers are often quite sensitive to float rounding errors. If you enable float64 computation, you will likely see more accurate results.","You are totally right, I had considered the 32 vs 64 but didn't think it would make such a difference. I just developed some rutines with Jax to solve Linear Complementarty Problems, both for sparse and non sparse matrices (Lemke and Fischer 1995). I'd be happy to collaborate with the package but I'm not sure I know how. Or I can do it as an independent package.","Following this discussin, it seems that sparse.eye causes problems when float64 is enabled.   I get:  However, defining my own eye function  works.",This looks like a problem in the `sparse.concatenate` implementation: it doesn't account for the fact that the indices may have different dtypes.,"Good catch, thank you. I understand the `jax `framework and sparsity are hard to combine, I hope someday it is no longer `experimental`. `spsolve` had issues to solve a system of equations when `jnp.float64` was used, it raises an error saying the matrix is singular when it is not. Using 32 bit floats, the correct solution is found. "
270,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add jax.scipy.integrate.cumulative_trapezoid.)ï¼Œ å†…å®¹æ˜¯ (Add jax.scipy.integrate.cumulative_trapezoid.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add jax.scipy.integrate.cumulative_trapezoid.,Add jax.scipy.integrate.cumulative_trapezoid.,2024-07-11T12:43:20Z,,closed,0,0,https://github.com/jax-ml/jax/issues/22396
810,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Local shards accessed using devices_indices_map are scaled by # of pods)ï¼Œ å†…å®¹æ˜¯ ( Description Given an array sharded across `n` pods, the following code results in each `local_shard` having its elements scaled by exactly `n`:  That is, each `local_shard` is exactly `n * sharded_arr[index]`. For comparison, this gives the correct list of shards:  Full reproducer with some mesh/distributed setup code omitted:  The last assert is the one that fails, with output:  when using 64 pods. If using 4 pods then it is off by a factor of 4. And if using just 1 pod then it is fine.  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,Local shards accessed using devices_indices_map are scaled by # of pods," Description Given an array sharded across `n` pods, the following code results in each `local_shard` having its elements scaled by exactly `n`:  That is, each `local_shard` is exactly `n * sharded_arr[index]`. For comparison, this gives the correct list of shards:  Full reproducer with some mesh/distributed setup code omitted:  The last assert is the one that fails, with output:  when using 64 pods. If using 4 pods then it is off by a factor of 4. And if using just 1 pod then it is fine.  System info (python version, jaxlib version, accelerator, etc.) ",2024-07-11T01:29:57Z,bug,open,0,2,https://github.com/jax-ml/jax/issues/22387,Using `.addressable_shards` is the way to go. That's the API to fetch local shards.,"Thanks for confirming the fix. To clarify / help others who might run into this, the issue is the first approach works if you're on a single pod, and it _sort of_ seems to work at first for more than 1 pod too until you run into correctness issues that are difficult to pinpoint especially since it doesn't reproduce on a single pod setup. It's easy to be mislead into thinking `sharded_arr[index]` is an okay thing to do when the `index` corresponds to a locally addressable shard, and it even seems to work (until it doesn't)."
661,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Unifying persistent cache messages)ï¼Œ å†…å®¹æ˜¯ ( Also moving persistent cache messages (miss & hit)  to WARNING logging level when `config.explain_cache_misses` is true; otherwise they remain in DEBUG. Message ""Persistent compilation cache hit for"" becomes ""PERSISTENT COMPILATION CACHE (HIT|MISS)"" to remain consistent with ""TRACING CACHE MISS"". The included tests test for correct presence of cache miss/hit messages in the logging level depending on the value of `config.explain_cache_misses`)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Unifying persistent cache messages," Also moving persistent cache messages (miss & hit)  to WARNING logging level when `config.explain_cache_misses` is true; otherwise they remain in DEBUG. Message ""Persistent compilation cache hit for"" becomes ""PERSISTENT COMPILATION CACHE (HIT|MISS)"" to remain consistent with ""TRACING CACHE MISS"". The included tests test for correct presence of cache miss/hit messages in the logging level depending on the value of `config.explain_cache_misses`",2024-07-11T01:26:14Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/22386
1471,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([jax.distributed] Allow setting local device ids via env var)ï¼Œ å†…å®¹æ˜¯ (Makes `jax.distributed.initialize` aware of a new environmental variable, `JAX_LOCAL_DEVICE_IDS`, which allows overriding the local device ids set by default or configured by clusterdependent autoconfiguration. Note that `local_device_ids` passed explicitly to `jax.distributed.initialize` are still respected. We are currently evaluating the benefits of oneprocessperhost vs. oneprocessperGPU vs. oneprocessperNUMAnode and this override would be useful for local experiments with different configurations for multiprocessJAX. E.g. running an existing workload like PAXML with oneprocessperGPU currently requires reaching into the framework and modifying its setup code (e.g. https://github.com/google/paxml/blob/9863f276af5431d8f08542dba06ced1dccb51aa7/paxml/setup_jax.pyL69L73). Similarly, the autoconfiguration for clusters currently presumes a onetoone mapping of processes to devices (e.g. https://github.com/google/jax/blob/e4b606e38a18867c757c738dd16265ca03d2cf88/jax/_src/clusters/slurm_cluster.pyL65L66). The first commit contains the minimal amount of changes necessary. The second commit extends the existing special value `""all""` of config variables `jax_cuda_visible_devices` and `jax_rocm_visible_devices` to the `loc)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[jax.distributed] Allow setting local device ids via env var,"Makes `jax.distributed.initialize` aware of a new environmental variable, `JAX_LOCAL_DEVICE_IDS`, which allows overriding the local device ids set by default or configured by clusterdependent autoconfiguration. Note that `local_device_ids` passed explicitly to `jax.distributed.initialize` are still respected. We are currently evaluating the benefits of oneprocessperhost vs. oneprocessperGPU vs. oneprocessperNUMAnode and this override would be useful for local experiments with different configurations for multiprocessJAX. E.g. running an existing workload like PAXML with oneprocessperGPU currently requires reaching into the framework and modifying its setup code (e.g. https://github.com/google/paxml/blob/9863f276af5431d8f08542dba06ced1dccb51aa7/paxml/setup_jax.pyL69L73). Similarly, the autoconfiguration for clusters currently presumes a onetoone mapping of processes to devices (e.g. https://github.com/google/jax/blob/e4b606e38a18867c757c738dd16265ca03d2cf88/jax/_src/clusters/slurm_cluster.pyL65L66). The first commit contains the minimal amount of changes necessary. The second commit extends the existing special value `""all""` of config variables `jax_cuda_visible_devices` and `jax_rocm_visible_devices` to the `loc",2024-07-10T11:36:56Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/22373," Rebased and dropped the second commit (the part supporting ""all""). PTAL"
324,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jax.numpy: better docstring for allclose and isclose functions)ï¼Œ å†…å®¹æ˜¯ (Better docstring added for jax.numpy.allclose and jax.numpy.isclose. Part of 21461)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jax.numpy: better docstring for allclose and isclose functions,Better docstring added for jax.numpy.allclose and jax.numpy.isclose. Part of 21461,2024-07-09T22:46:48Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/22359,evaluation condition inserted and text fixed.
539,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Issue with shardmap when trying to use static arguments)ï¼Œ å†…å®¹æ˜¯ ( Description Following from this discussion https://github.com/google/jax/discussions/22353 I made this MWE  I have this error  Following what  did in https://github.com/google/jax/pull/22049 I am not able to use a tuple static arguments   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Issue with shardmap when trying to use static arguments," Description Following from this discussion https://github.com/google/jax/discussions/22353 I made this MWE  I have this error  Following what  did in https://github.com/google/jax/pull/22049 I am not able to use a tuple static arguments   System info (python version, jaxlib version, accelerator, etc.) ",2024-07-09T21:13:57Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/22356,Thanks for raising this! Definitely looks like a bug; any time you see a `safe_zip` error it's likely a JAXinternal bug.,"Thank you , I have tried a bunch of things, bools PRNGKeys work. It is only Sequences that seems to cause the issue.","Hmm, I tried to translate to a local CPU repro but this didn't crash:  Is that missing something? Could you set the `JAX_TRACEBACK_FILTERING=off` shell environment variable and rerun your repro, then paste the full traceback?","Ah, I wonder if it's just a version issue: https://github.com/google/jax/pull/22049 may only be available at HEAD, ie jax==0.4.31, not in jax==0.4.30. Only jax==0.4.30 has been pushed to pypi, so you'd have to install jax from GitHub.",Can you try running from github HEAD? You might be able to do `pip install upgrade git+https://github.com/google/jax.git`.,"Yeah with 0.4.31 I don't reproduce. I am going to close then thank you very much. One question, when to use P() and when to use None None is fully compatible with `static_argnums` if the parent has a jit decorater?","> One question, when to use P() and when to use None There's not much of a difference; `P()` only works for arraylikes, whereas `None` works more generally, and they produce the same result even for ararylike arguments."
1488,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump zipp from 3.18.1 to 3.19.1 in /build in the pip group across 1 directory)ï¼Œ å†…å®¹æ˜¯ (Bumps the pip group with 1 update in the /build directory: zipp. Updates `zipp` from 3.18.1 to 3.19.1  Changelog Sourced from zipp's changelog.  v3.19.1 Bugfixes  Improved handling of malformed zip files. ( CC(example Errata))  v3.19.0 Features  Implement is_symlink. ( CC(add oss test instructions, fix conv grad bug))  v3.18.2 No significant changes.    Commits  6d1cb72 Finalize fd604bd Merge pull request  CC(Canâ€™t import both jax and tensorflow (causes kernel restart)) from jaraco/bugfix/119malformedpaths c18417e Add news fragment. 58115d2 Employ SanitizedNames in CompleteDirs. Fixes broken test. 564fcc1 Add SanitizedNames mixin. 79a309f Add some assertions about malformed paths. 2d015c2 Merge https://github.com/jaraco/skeleton a595a0f Rename extras to align with core metadata spec. 608f90a Finalize 3a22d72 Merge pull request  CC(add tensordot) from jaraco/feature/issymlink Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by comment)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Bump zipp from 3.18.1 to 3.19.1 in /build in the pip group across 1 directory,"Bumps the pip group with 1 update in the /build directory: zipp. Updates `zipp` from 3.18.1 to 3.19.1  Changelog Sourced from zipp's changelog.  v3.19.1 Bugfixes  Improved handling of malformed zip files. ( CC(example Errata))  v3.19.0 Features  Implement is_symlink. ( CC(add oss test instructions, fix conv grad bug))  v3.18.2 No significant changes.    Commits  6d1cb72 Finalize fd604bd Merge pull request  CC(Canâ€™t import both jax and tensorflow (causes kernel restart)) from jaraco/bugfix/119malformedpaths c18417e Add news fragment. 58115d2 Employ SanitizedNames in CompleteDirs. Fixes broken test. 564fcc1 Add SanitizedNames mixin. 79a309f Add some assertions about malformed paths. 2d015c2 Merge https://github.com/jaraco/skeleton a595a0f Rename extras to align with core metadata spec. 608f90a Finalize 3a22d72 Merge pull request  CC(add tensordot) from jaraco/feature/issymlink Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by comment",2024-07-09T19:25:15Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/22352,"This pull request was built based on a group rule. Closing it will not ignore any of these versions in future pull requests. To ignore these dependencies, configure ignore rules in dependabot.yml"
1442,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump psutil from 5.9.8 to 6.0.0)ï¼Œ å†…å®¹æ˜¯ (Bumps psutil from 5.9.8 to 6.0.0.  Changelog Sourced from psutil's changelog.  6.0.0 20240618 Enhancements  2109_: maxfile and maxpath fields were removed from the namedtuple returned by disk_partitions()_. Reason: on network filesystems (NFS) this can potentially take a very long time to complete. 2366_, [Windows]: log debug message when using slower process APIs. 2375_, [macOS]: provide arm64 wheels.  (patch by Matthieu Darbois) 2396_: process_iter()_ no longer preemptively checks whether PIDs have been reused. This makes process_iter()_ around 20x times faster. 2396_: a new psutil.process_iter.cache_clear() API can be used the clear process_iter()_ internal cache. 2401_, Support building with freethreaded CPython 3.13. (patch by Sam Gross) 2407_: Process.connections()_ was renamed to Process.net_connections()_. The old name is still available, but it's deprecated (triggers a DeprecationWarning) and will be removed in the future. 2425_: [Linux]: provide aarch64 wheels.  (patch by Matthieu Darbois / Ben Raz)  Bug fixes  2250_, [NetBSD]: Process.cmdline()_ sometimes fail with EBUSY. It usually happens for long cmdlines with lots of arguments. In this case retry getting the cmdline for up to 50 times, and return )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bump psutil from 5.9.8 to 6.0.0,"Bumps psutil from 5.9.8 to 6.0.0.  Changelog Sourced from psutil's changelog.  6.0.0 20240618 Enhancements  2109_: maxfile and maxpath fields were removed from the namedtuple returned by disk_partitions()_. Reason: on network filesystems (NFS) this can potentially take a very long time to complete. 2366_, [Windows]: log debug message when using slower process APIs. 2375_, [macOS]: provide arm64 wheels.  (patch by Matthieu Darbois) 2396_: process_iter()_ no longer preemptively checks whether PIDs have been reused. This makes process_iter()_ around 20x times faster. 2396_: a new psutil.process_iter.cache_clear() API can be used the clear process_iter()_ internal cache. 2401_, Support building with freethreaded CPython 3.13. (patch by Sam Gross) 2407_: Process.connections()_ was renamed to Process.net_connections()_. The old name is still available, but it's deprecated (triggers a DeprecationWarning) and will be removed in the future. 2425_: [Linux]: provide aarch64 wheels.  (patch by Matthieu Darbois / Ben Raz)  Bug fixes  2250_, [NetBSD]: Process.cmdline()_ sometimes fail with EBUSY. It usually happens for long cmdlines with lots of arguments. In this case retry getting the cmdline for up to 50 times, and return ",2024-07-08T17:44:15Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/22328,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
1443,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump packaging from 24.0 to 24.1)ï¼Œ å†…å®¹æ˜¯ (Bumps packaging from 24.0 to 24.1.  Release notes Sourced from packaging's releases.  24.1 What's Changed  pyupgrade/black/isort/flake8 â†’ ruff by @â€‹DimitriPapadopoulos in pypa/packaging CC(fix travis by running `pytest n 1` instead of `n 2`) Add support for Python 3.13 and drop EOL 3.7 by @â€‹hugovk in pypa/packaging CC(Fixes from pytype) Bump the githubactions group with 4 updates by @â€‹dependabot in pypa/packaging CC(Improve behavior of a number of math functions for extreme inputs.) Fix typo in _parser docstring by @â€‹pradyunsg in pypa/packaging CC(Cleaned up the GP regression example) Modernise type annotations using FA rules from ruff by @â€‹pradyunsg in pypa/packaging CC(Check for failure of LAPACK calls and return NaNs on failure.) Document markers.default_environment() by @â€‹edgarrmondragon in pypa/packaging CC(Incorrect results for reversemode differentiation of scan wrt sequence) Bump the githubactions group with 3 updates by @â€‹dependabot in pypa/packaging CC(Make pmap lax.psum(1, 'i') and pxla.axis_index('i') work) Work around platform.python_version() returning non PEP 440 compliant version for nontagged CPython builds by @â€‹sbidoul in pypa/packaging CC(REPL latency issue)  New Contributors  @â€‹dependabot mad)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Bump packaging from 24.0 to 24.1,"Bumps packaging from 24.0 to 24.1.  Release notes Sourced from packaging's releases.  24.1 What's Changed  pyupgrade/black/isort/flake8 â†’ ruff by @â€‹DimitriPapadopoulos in pypa/packaging CC(fix travis by running `pytest n 1` instead of `n 2`) Add support for Python 3.13 and drop EOL 3.7 by @â€‹hugovk in pypa/packaging CC(Fixes from pytype) Bump the githubactions group with 4 updates by @â€‹dependabot in pypa/packaging CC(Improve behavior of a number of math functions for extreme inputs.) Fix typo in _parser docstring by @â€‹pradyunsg in pypa/packaging CC(Cleaned up the GP regression example) Modernise type annotations using FA rules from ruff by @â€‹pradyunsg in pypa/packaging CC(Check for failure of LAPACK calls and return NaNs on failure.) Document markers.default_environment() by @â€‹edgarrmondragon in pypa/packaging CC(Incorrect results for reversemode differentiation of scan wrt sequence) Bump the githubactions group with 3 updates by @â€‹dependabot in pypa/packaging CC(Make pmap lax.psum(1, 'i') and pxla.axis_index('i') work) Work around platform.python_version() returning non PEP 440 compliant version for nontagged CPython builds by @â€‹sbidoul in pypa/packaging CC(REPL latency issue)  New Contributors  @â€‹dependabot mad",2024-07-08T17:43:46Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/22326,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
819,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(guidance on how to properly use lax.reduce on an multi-dimension array.)ï¼Œ å†…å®¹æ˜¯ (I want to perform reduce operation on an array in a slightly different way than `jnp.argmax`. I have an array `A` with shape `(2,5)`. I wanted to reduce `A` in the below way: for a pair of index `(i, j)`, calculate `z = A[0][i]*A[1][j]A[0][j]*A[1][i]`, if `z>0`, then pick `i` otherwise pick `j`. Continue this computation until getting the final index.  What's the best way for me to perform this in JAX? Below is my code on trying to implement this reduce operation, which threw error `IndexError: Too many indices for array: 1 nonNone/Ellipsis indices for dim 0.`. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,guidance on how to properly use lax.reduce on an multi-dimension array.,"I want to perform reduce operation on an array in a slightly different way than `jnp.argmax`. I have an array `A` with shape `(2,5)`. I wanted to reduce `A` in the below way: for a pair of index `(i, j)`, calculate `z = A[0][i]*A[1][j]A[0][j]*A[1][i]`, if `z>0`, then pick `i` otherwise pick `j`. Continue this computation until getting the final index.  What's the best way for me to perform this in JAX? Below is my code on trying to implement this reduce operation, which threw error `IndexError: Too many indices for array: 1 nonNone/Ellipsis indices for dim 0.`. ",2024-07-08T16:32:48Z,enhancement,closed,0,1,https://github.com/jax-ml/jax/issues/22320,This looks to be a duplicate of CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) â€“ I'm going to close this one because the discussion format is probably better for this question!
646,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Deprecate support for custom lowering rules that return tuple-wrapped ir.Values.)ï¼Œ å†…å®¹æ˜¯ (Deprecate support for custom lowering rules that return tuplewrapped ir.Values. https://github.com/google/jax/pull/22211 forbade custom lowering rules from returning singleton tuples of ir.Value, but this appears to break downstream users, notably Transformer Engine. Instead, allow lowering rules to return singleton tuples and unwrap them if needed, but warn if this behavior is seen.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Deprecate support for custom lowering rules that return tuple-wrapped ir.Values.,"Deprecate support for custom lowering rules that return tuplewrapped ir.Values. https://github.com/google/jax/pull/22211 forbade custom lowering rules from returning singleton tuples of ir.Value, but this appears to break downstream users, notably Transformer Engine. Instead, allow lowering rules to return singleton tuples and unwrap them if needed, but warn if this behavior is seen.",2024-07-08T14:16:58Z,,closed,0,0,https://github.com/jax-ml/jax/issues/22314
1480,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(FP8 XLA matmul fusion into `__cublas$lt$matmul$f8` not fully working )ï¼Œ å†…å®¹æ˜¯ ( Description To achieve optimal performance in FP8 training, one needs to fuse pre and postscaling as well as `amax` capture directly in the `cublasltmatmul` call. The XLA FP8 RFC describes the pseudo code corresponding, and to be best of my knowledge, these rules have been implemented in the OpenXLA Gemm rewriter pass. I am trying to get the codegen described in the XLA RFC directly from JAX, with the following FP8 matmul code:  It generates the following stable HLO lowering:  which is (almost) equivalent to the HLO code describe in the RFC. Unfortunately, after compilation (i.e. `qn_compiled = jax.jit(quantized_matmul).lower(a, b, x_scale, y_scale, z_scale).compile()`), the HLO Module outputted has the following form:  In short: the FP8 matmul with inputs scaling is recognized, generating a custom call to `__cublas$lt$matmul$f8`. But the HLO compilation fails to recognize the postmatmul pattern of amax catpure, rescaling and casting back to FP8. Note: the same happens when adding the `clamp` call described in the RFC `jax.lax.clamp(np.float32(448), z / z_scale, np.float32(448))`. Do you have an idea of why the full op fusing is not happening? And more generally, when the bug is fixed, I believe it would be gre)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,FP8 XLA matmul fusion into `__cublas$lt$matmul$f8` not fully working ," Description To achieve optimal performance in FP8 training, one needs to fuse pre and postscaling as well as `amax` capture directly in the `cublasltmatmul` call. The XLA FP8 RFC describes the pseudo code corresponding, and to be best of my knowledge, these rules have been implemented in the OpenXLA Gemm rewriter pass. I am trying to get the codegen described in the XLA RFC directly from JAX, with the following FP8 matmul code:  It generates the following stable HLO lowering:  which is (almost) equivalent to the HLO code describe in the RFC. Unfortunately, after compilation (i.e. `qn_compiled = jax.jit(quantized_matmul).lower(a, b, x_scale, y_scale, z_scale).compile()`), the HLO Module outputted has the following form:  In short: the FP8 matmul with inputs scaling is recognized, generating a custom call to `__cublas$lt$matmul$f8`. But the HLO compilation fails to recognize the postmatmul pattern of amax catpure, rescaling and casting back to FP8. Note: the same happens when adding the `clamp` call described in the RFC `jax.lax.clamp(np.float32(448), z / z_scale, np.float32(448))`. Do you have an idea of why the full op fusing is not happening? And more generally, when the bug is fixed, I believe it would be gre",2024-07-08T11:37:18Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/22313,This is probably a bug for openxla/xla.,"Apologies, the mistake was on my side. Wrongly used `bitcast_convert_type` instead of `convert_element_type`. For future reference, the following piece of code is working properly: "
1337,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Profiler fails on kaggle tpu v3-8 notebook)ï¼Œ å†…å®¹æ˜¯ ( Description While on kaggle notebook, upgrading to newer version of jax[tpu] breaks profiler.  The follower command was used. (tested with 0.4.29, 0.4.30) `!pip install U optax jax[tpu]==0.4.30 f https://storage.googleapis.com/jaxreleases/libtpu_releases.html` Running a simple example code (as followed) caused an error  It caused the following error.  Also tested without actually running any jax computation, and still got same error.   System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.29 jaxlib: 0.4.29 numpy:  1.26.4 python: 3.10.14 (main, May 14 2024, 08:39:53) [GCC 12.2.0] jax.devices (8 total, 8 local): [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0) TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1) ... TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0) TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)] process_count: 1 platform: uname_result(system='Linux', node='88c1b3bf794b', release='6.1.42+', version=' CC(Python 3 compatibility issues) SMP PREEMPT_DYNAMIC Sun Oct  8 14:23:56 UTC 2023', machine='x86_64'))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Profiler fails on kaggle tpu v3-8 notebook," Description While on kaggle notebook, upgrading to newer version of jax[tpu] breaks profiler.  The follower command was used. (tested with 0.4.29, 0.4.30) `!pip install U optax jax[tpu]==0.4.30 f https://storage.googleapis.com/jaxreleases/libtpu_releases.html` Running a simple example code (as followed) caused an error  It caused the following error.  Also tested without actually running any jax computation, and still got same error.   System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.29 jaxlib: 0.4.29 numpy:  1.26.4 python: 3.10.14 (main, May 14 2024, 08:39:53) [GCC 12.2.0] jax.devices (8 total, 8 local): [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0) TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1) ... TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0) TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)] process_count: 1 platform: uname_result(system='Linux', node='88c1b3bf794b', release='6.1.42+', version=' CC(Python 3 compatibility issues) SMP PREEMPT_DYNAMIC Sun Oct  8 14:23:56 UTC 2023', machine='x86_64')",2024-07-05T19:14:24Z,bug,closed,1,4,https://github.com/jax-ml/jax/issues/22294,Also tested with jax version 0.4.30. Same result,"Cakes Can you share public Kaggle notebook that fully repros? One other thing you can usually try is adding `!pip install tensorflowcpu` before anything else. Sometimes, having tensorflowtpu installed can break the other TPUenabled frameworks.","Also, are you by any chance updating jax + jaxlib without using the full jax[tpu] command? https://github.com/google/jax?tab=readmeovfileinstructions","> Cakes Can you share public Kaggle notebook that fully repros? >  > One other thing you can usually try is adding `!pip install tensorflowcpu` before anything else. Sometimes, having tensorflowtpu installed can break the other TPUenabled frameworks. Here is a public notebook showing it Further testing shows it only occurs when updating jax to 0.4.30 version. Testing with kaggle's native 0.4.23 doesn't trigger the error.  `!pip install tensorflowcpu` fixes the problem. So for anyone stumbling upon the same error, do `!pip install tensorflowcpu` I do not understand why tensorflowcpu isn't required for 0.4.23 and why pip doesn't install it as dependency."
943,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Persistent compilation cache hit but tracing cache miss?)ï¼Œ å†…å®¹æ˜¯ ( Description For this simple test program:  I get the following somewhat perplexing behavior:  Note that:  The cache does actually get filled  Compiler debug message mentions a cache hit  Explain cache misses reports a cache miss  The overall execution time doesn't actually decrease (closing over the large constant is just to encourage nonnegligible compilation times)  The `id` reported for each function is distinct on each run I get similar behavior running this code (without the `JAX_PLATFORMS=cpu` bit) on a colaboratory GPU:  Though here we also get a parsing error when attempting to read from the cache.  System info (python version, jaxlib version, accelerator, etc.) Local device  Colaboratory )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Persistent compilation cache hit but tracing cache miss?," Description For this simple test program:  I get the following somewhat perplexing behavior:  Note that:  The cache does actually get filled  Compiler debug message mentions a cache hit  Explain cache misses reports a cache miss  The overall execution time doesn't actually decrease (closing over the large constant is just to encourage nonnegligible compilation times)  The `id` reported for each function is distinct on each run I get similar behavior running this code (without the `JAX_PLATFORMS=cpu` bit) on a colaboratory GPU:  Though here we also get a parsing error when attempting to read from the cache.  System info (python version, jaxlib version, accelerator, etc.) Local device  Colaboratory ",2024-07-04T23:16:27Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/22281, ,"Tracing cache is a *nonpersistent* perfunction cache to make sure we do not recompile for each individual function call with same argument shapes. In each run, the first call to each function will always miss that cache. Also note that `id` here is the memory address of the closure. It is fully expected to be different from run to run. Note that the persistent cache did hit for your function: ","Ah, okay, I suspected I was missing something here (because of the `id` is the memory address fact, the cache hit log message, etc). And then the persistent cache is relevant for the ""Finished XLA compilation"" step (this message has slightly different meanings when there's a cache hit vs miss) but not the ""Finished jaxpr to MLIR module conversion"" (where most time is spent in my test case)?","That's right: to get a disk compilation cache hit in a new process, we first redo the work of tracing the Python function to a jaxpr and then lowering the jaxpr to an MLIR module. Only then do we fingerprint the MLIR module and notice we have a disk cache entry for it. I think this issue is resolved so let's close it, but please let me know if I'm mistaken."
1447,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix compatibility with nightly numpy)ï¼Œ å†…å®¹æ˜¯ (Numpy recently merged support for the 2023.12 revision of the Array API: https://github.com/numpy/numpy/pull/26724 This breaks two of our tests: 1. The first breakage was caused by differences in how numpy and JAX cast negative floats to `uint8`. Specifically `np.float32(1).astype(np.uint8)` returns `np.uint8(255)` whereas `jnp.float32(1).astype(jnp.uint8)` produces `Array(0, dtype=uint8)`. We don't make any promises about consistency with casting floats to ints, noting that this can even be backend dependent. ~~I don't believe this failure is identifying any unexpected behavior, and we test many other dtypes properly so I'm not concerned about skipping this test.~~ To fix our test, we now only generate positive inputs when the output dtype is unsigned. 2. The second failure was caused by the fact that the approach we took in CC(Update `jnp.clip` to Array API 2023 standard and introduces `jax.experimental.array_api.clip`) to support backwards compatibility and the Array API for `clip` differs from the one used in numpy/numpy CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®). Again, the behavior is consistent, but it produces a different signature. I've skipped checking `clip`'s signature, but we should revisit it once the `a_min` and `a_max` paramet)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Fix compatibility with nightly numpy,"Numpy recently merged support for the 2023.12 revision of the Array API: https://github.com/numpy/numpy/pull/26724 This breaks two of our tests: 1. The first breakage was caused by differences in how numpy and JAX cast negative floats to `uint8`. Specifically `np.float32(1).astype(np.uint8)` returns `np.uint8(255)` whereas `jnp.float32(1).astype(jnp.uint8)` produces `Array(0, dtype=uint8)`. We don't make any promises about consistency with casting floats to ints, noting that this can even be backend dependent. ~~I don't believe this failure is identifying any unexpected behavior, and we test many other dtypes properly so I'm not concerned about skipping this test.~~ To fix our test, we now only generate positive inputs when the output dtype is unsigned. 2. The second failure was caused by the fact that the approach we took in CC(Update `jnp.clip` to Array API 2023 standard and introduces `jax.experimental.array_api.clip`) to support backwards compatibility and the Array API for `clip` differs from the one used in numpy/numpy CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®). Again, the behavior is consistent, but it produces a different signature. I've skipped checking `clip`'s signature, but we should revisit it once the `a_min` and `a_max` paramet",2024-07-03T14:46:16Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/22257
1497,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fine-grained remat policy makes async/pipelined collectives execute in the main stream)ï¼Œ å†…å®¹æ˜¯ ( Description Hi, I have following setup:  Transformer model with N layers scanned over input  fully sharded data parallel sharding  asynchronous communications (latencyhiding scheduler, pipelined allgather,allreduce,reducescatter) I'm using following flags:  To speedup backward by finegrained reduction of activations recomputation, I marked each dense layer's output in transformer block with specific name:  So, for example, in attention layer I have ""dot_attention_query"", ""dot_attention_key"", ""dot_attention_value"", ""dot_attention_out"".  And then I apply checkpoint policy on scanned function which accepts list of activation names to checkpoint:  and then scan It over embeddings:  If I set self.config.save_names_for_bwd to empty list (which is basically equivalent to ""nothing_saveable"" policy), then communications works correctly  allgather/reducescatters/allreduces are overlapped with computations, as can be seen on this perfetto trace:  nothing_saveable.tgz But as soon as I start to specify some names in self.config.save_names_for_bwd, for example,   While these activations is indeed not recomputed during backward pass, all communications are executed in main stream without any overlapping with computations:  sa)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Fine-grained remat policy makes async/pipelined collectives execute in the main stream," Description Hi, I have following setup:  Transformer model with N layers scanned over input  fully sharded data parallel sharding  asynchronous communications (latencyhiding scheduler, pipelined allgather,allreduce,reducescatter) I'm using following flags:  To speedup backward by finegrained reduction of activations recomputation, I marked each dense layer's output in transformer block with specific name:  So, for example, in attention layer I have ""dot_attention_query"", ""dot_attention_key"", ""dot_attention_value"", ""dot_attention_out"".  And then I apply checkpoint policy on scanned function which accepts list of activation names to checkpoint:  and then scan It over embeddings:  If I set self.config.save_names_for_bwd to empty list (which is basically equivalent to ""nothing_saveable"" policy), then communications works correctly  allgather/reducescatters/allreduces are overlapped with computations, as can be seen on this perfetto trace:  nothing_saveable.tgz But as soon as I start to specify some names in self.config.save_names_for_bwd, for example,   While these activations is indeed not recomputed during backward pass, all communications are executed in main stream without any overlapping with computations:  sa",2024-07-03T12:38:56Z,bug,closed,4,3,https://github.com/jax-ml/jax/issues/22252,corresponding XLA issue: https://github.com/openxla/xla/issues/14397,"I _think_ this is an XLA bug, so let's watch openxla/xla CC(jax.grad computes incorrect derivative for polynomials) to see.",Looks like that XLA issue is resolved!
311,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Prefer __qualname__ as a pjit_p name)ï¼Œ å†…å®¹æ˜¯ (If applying `jit` to a class method, it is often important to know the class name in the jaxpr.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Prefer __qualname__ as a pjit_p name,"If applying `jit` to a class method, it is often important to know the class name in the jaxpr.",2024-07-02T18:53:10Z,pull ready,closed,0,2,https://github.com/jax-ml/jax/issues/22237,Have you considered instead expanding `util.fun_name` that we use in a few places?,"> Have you considered instead expanding `util.fun_name` that we use in a few places? Yes, see my response to Sergei. Doing that broke tests and I'd rather make the minimal change that solves the problem I have (jaxpr readability)."
842,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Feature Request: a way to opt out of ""hermetic python"")ï¼Œ å†…å®¹æ˜¯ (Hi! We're encountering issues concerning https://github.com/google/jax/pull/20469 with the jax{,lib} packages in Nixpkgs. It is crucial to our project to bootstrap the python interpreter from scratch and outside bazel, rather download a prebuilt one from the internet. We've been relying on bazel selecting the system python (setting `PYTHON_BIN_PATH`) for this. Right now we're looking into reverting the respective diff ad hoc, but we'd prefer a tighter integration Thanks Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"Feature Request: a way to opt out of ""hermetic python""","Hi! We're encountering issues concerning https://github.com/google/jax/pull/20469 with the jax{,lib} packages in Nixpkgs. It is crucial to our project to bootstrap the python interpreter from scratch and outside bazel, rather download a prebuilt one from the internet. We've been relying on bazel selecting the system python (setting `PYTHON_BIN_PATH`) for this. Right now we're looking into reverting the respective diff ad hoc, but we'd prefer a tighter integration Thanks Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example.",2024-07-01T17:44:18Z,enhancement,open,4,2,https://github.com/jax-ml/jax/issues/22216,"Hello , For the projects which want to provide their own Python interpreter (a Linux distribution would be a good example of that) there is a way to doe exactly that. For the other concerns, please check my response in the corresponding nixpkgs thread.  Specifying your own Python interpreter For a basic example of specifying your own interpreter please check Building with prerelease Python version. Note, you do not have to follow those instructions directly if you don't want. The only thing that you actually need is to be able to specify a path to your desired (hopefully working) python interpreter in the end. The instructions just serve the purpose of obtaining such working custom interpreter, if you already have one, you can skip directly to the last step: To use newly built Python interpreter add the following code snippet RIGHT AFTER `python_init_toolchains()` in your `WORKSPACE` file.   Mimic old nonhermetic behavior (very very not recommended) We do not support and do not recommend this use case, but it is still possible with a little bit of work on your side. The instructions above assume you have python packaged as a standalone .tgz archive. If you still want to just depend on whatever is installed locally on your system, you can go further but there is an **important thing to know before doing so (which may affect your decision):** Even in previous nonhermetic python setup, it was wrapping system python inside bazel rules and copying parts of your system python package inside bazel's cache to be abele to use it for the rest of the build. I.e.  nonhermetic python acted almost as it was still downloading a python from somewhere, it was just that ""somewhere"" happened to be your local environment. With that being said, you can mimic old nonhermetic python setup by having a custom repository rule which would search your local system, package it in a form of a standalone archive to match structure of the ones we currently depend on (the structure there matches default layout you would get when build vanill Python from official sources) and then provides the packaged archive to the value for `url` field in the code snipped above. Note, we do not provide such custom localfilesystemsearch rule ourseves and do not plan to, as it basically would reintroduce the nonhermetic python with all its issues, such as being very fragile and nonreproducible setup, but it is not very difficult to implement such on your side, especially if you do not need to make it generic (if it matches only NixOS structure, than it would be much easier to implement and maintain than something which should work on any linux system).","Hi! Thank you google, for a prompt and comprehensive reply. I hope my comment about ""random executables"" in the other issue wasn't too rude, otherwise I'm prepared to apologize:) We'll look into implementing a solution along the lines of the snippet you provided. I expect it won't be a ""smooth ride"". I think it's best I avoid ""explaining Nix"" in this thread (maybe we reserve the Nixpkgs issue for that) and focus on our integration with Bazel. Suffice it to say that Nix and Bazel overlap in scope, which is why (1) we're not concerned about reproducibility and correctness of ""system packages"", and (2) why having Bazel set up its own sandbox and copy these packages into new locations has been problematic. For now, there are two implementation details about the python we build I'm worried about:  Currently, we only consider our outputs ""correct"" if they're deployed at certain predefined locations: e.g. `/nix/store/python/bin/python` will have a `PT_INTERPTER` pointing at a specific `/nix/store/glibc/lib/ldlinux.so`, etc. Things would likely still work: since Bazel implements hermetic builds, I presume it would patchelf the executables it ""downloads""?  We only consider our outputs ""correct"" when referential integrity is maintained: if `/bin/python` references `/libz.so`, this `python` can only be deployed together with it. We have ways to inspect and export the full ""closure"" of such a package, but we've never explicitly provided Bazel's ""old nonhermetic python"" with this information. Does this mean that Bazel has been copying ""our python"" and relinking it to different versions of dependencies for the duration of the build? Perhaps these notions of ""correctness"" (=conditions under which we're ready to ""guarantee"" our software will work) also clarify why I believed it would be easiest for us if we could relax Bazel's sandbox and let it see the predeployed toolchain"
1450,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump hypothesis from 6.100.1 to 6.104.2)ï¼Œ å†…å®¹æ˜¯ (Bumps hypothesis from 6.100.1 to 6.104.2.  Commits  d8c1783 Bump hypothesispython version to 6.104.2 and update changelog c79e893 Merge pull request  CC(jax.numpy.clip has unexpected behavior which diverges from numpy.clip ) from tybug/posttestcasehookfix 976f559 update typing 1f86be4 better post_test_case_hook error message 39a8f72 raise flaky when replaying backend flakes 09b00a2 add release notes 90ad3e2 exit if replayed interesting backends are flaky c46da15 move post_test_case_hook higher up e51d473 Bump hypothesispython version to 6.104.1 and update changelog 0c885e7 Merge pull request  CC([jax2tf] Disable the CI tests for jax2tf.) from jobh/coverage_fixme Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recrea)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bump hypothesis from 6.100.1 to 6.104.2,Bumps hypothesis from 6.100.1 to 6.104.2.  Commits  d8c1783 Bump hypothesispython version to 6.104.2 and update changelog c79e893 Merge pull request  CC(jax.numpy.clip has unexpected behavior which diverges from numpy.clip ) from tybug/posttestcasehookfix 976f559 update typing 1f86be4 better post_test_case_hook error message 39a8f72 raise flaky when replaying backend flakes 09b00a2 add release notes 90ad3e2 exit if replayed interesting backends are flaky c46da15 move post_test_case_hook higher up e51d473 Bump hypothesispython version to 6.104.1 and update changelog 0c885e7 Merge pull request  CC([jax2tf] Disable the CI tests for jax2tf.) from jobh/coverage_fixme Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recrea,2024-07-01T17:26:23Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/22215,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
1448,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump setuptools from 69.2.0 to 70.2.0)ï¼Œ å†…å®¹æ˜¯ (Bumps setuptools from 69.2.0 to 70.2.0.  Changelog Sourced from setuptools's changelog.  v70.2.0 Features  Updated distutils including significant changes to support Cygwin and mingw compilers. ( CC(Call check_user_dtype on all user dtypes))  Bugfixes  Fix distribution name normalisation (:pep:625) for valid versions that are not canonical (e.g. 1.02). ( CC(reduce testcase count of the numpydispatch CI check))  v70.1.1 Misc   CC(Add support for `all_to_all` in vmap)  v70.1.0 Features   Adopted the bdist_wheel command from the wheel project  by :user:agronholm ( CC(Nan Mean and Adds Nan Reducers to testing))   Improve error message when pkg_resources.ZipProvider tries to extract resources with a missing Egg  by :user:Avasam Added variables and parameter type annotations to pkg_resources to be nearly on par with typeshed.*  by :user:Avasam * Excluding TypeVar and overload. Return types are currently inferred. ( CC([jax2tf] Cleanup test_unary_elementwise.))   Migrated Setuptools' own config to pyproject.toml ( CC(Allow preallocation of GPU memory based on total memory))   Bugfixes  Prevent a TypeError: 'NoneType' object is not callable when shutil_rmtree is called without an onexc parameter on Python&lt;=3.11  by :)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Bump setuptools from 69.2.0 to 70.2.0,Bumps setuptools from 69.2.0 to 70.2.0.  Changelog Sourced from setuptools's changelog.  v70.2.0 Features  Updated distutils including significant changes to support Cygwin and mingw compilers. ( CC(Call check_user_dtype on all user dtypes))  Bugfixes  Fix distribution name normalisation (:pep:625) for valid versions that are not canonical (e.g. 1.02). ( CC(reduce testcase count of the numpydispatch CI check))  v70.1.1 Misc   CC(Add support for `all_to_all` in vmap)  v70.1.0 Features   Adopted the bdist_wheel command from the wheel project  by :user:agronholm ( CC(Nan Mean and Adds Nan Reducers to testing))   Improve error message when pkg_resources.ZipProvider tries to extract resources with a missing Egg  by :user:Avasam Added variables and parameter type annotations to pkg_resources to be nearly on par with typeshed.*  by :user:Avasam * Excluding TypeVar and overload. Return types are currently inferred. ( CC([jax2tf] Cleanup test_unary_elementwise.))   Migrated Setuptools' own config to pyproject.toml ( CC(Allow preallocation of GPU memory based on total memory))   Bugfixes  Prevent a TypeError: 'NoneType' object is not callable when shutil_rmtree is called without an onexc parameter on Python&lt;=3.11  by :,2024-07-01T17:25:46Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/22214,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
1503,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Check failed in collective_pipeliner when using gradient accumulation with non-unrolled loop)ï¼Œ å†…å®¹æ˜¯ ( Description Hi, I have following setup:  Transformer model with N layers scanned over input  fully sharded data parallel sharding  asynchronous communications (latencyhiding scheduler, pipelined allgather,allreduce,reducescatter) I'm using following flags:  This works correctly and indeed hide layers' weights allgather and gradient reducescatter behind computations. Problems are starting to arise when I try to use gradient accumulation in this setup. It is implemented like this:  When I set gradient accumulation factor (num_minibatches_in_batch in this snippet) to value greater than 1, I'm getting following error during compilation:   Here is xla_dump_to result: xla_dump.tgz One important fact here is that if I set `unroll` in jax.lax.fori_loop to True, then there is no compilation error and everything works. But this obviously leads to additional memory usage proportional to gradient accumulation factor so this hack doesn't seem to be viable. My hypothesis is that when using `xla_gpu_enable_while_loop_double_buffering=true` with pipelined collectives and latency  hiding scheduler, XLA compiler tries to double buffer this fori_loop which is actually undesired behavior. Basically, there are two problems:  Bug in)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Check failed in collective_pipeliner when using gradient accumulation with non-unrolled loop," Description Hi, I have following setup:  Transformer model with N layers scanned over input  fully sharded data parallel sharding  asynchronous communications (latencyhiding scheduler, pipelined allgather,allreduce,reducescatter) I'm using following flags:  This works correctly and indeed hide layers' weights allgather and gradient reducescatter behind computations. Problems are starting to arise when I try to use gradient accumulation in this setup. It is implemented like this:  When I set gradient accumulation factor (num_minibatches_in_batch in this snippet) to value greater than 1, I'm getting following error during compilation:   Here is xla_dump_to result: xla_dump.tgz One important fact here is that if I set `unroll` in jax.lax.fori_loop to True, then there is no compilation error and everything works. But this obviously leads to additional memory usage proportional to gradient accumulation factor so this hack doesn't seem to be viable. My hypothesis is that when using `xla_gpu_enable_while_loop_double_buffering=true` with pipelined collectives and latency  hiding scheduler, XLA compiler tries to double buffer this fori_loop which is actually undesired behavior. Basically, there are two problems:  Bug in",2024-07-01T13:22:30Z,better_errors needs info,closed,6,3,https://github.com/jax-ml/jax/issues/22210,related XLA issue: https://github.com/openxla/xla/issues/14332,"Thanks for raising this. I think it's an XLA:GPU issue, and we don't have any way to fix it from JAX. That said, the hardtoparsae error may be something we can get traction on from JAX... can you say a bit more about what would've helped in the error message? We attach Python source information to the HLO program, but it's up to XLA to raise errors that reference it... from JAX we could've at least told you which jitted function raised the compiler error, but I'm not sure if we have other information to provide...",I think we should close this in favor of the XLA issue. Looks like it just got assigned yesterday!
1491,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Efficient parallelization of tasks with highly variable difficulty and run times)ï¼Œ å†…å®¹æ˜¯ (Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example. I am struggling to write **idiomatic and efficient** parallel code in jax for tasks with variable difficulty and run times. Jax parallelization is based on sharding and it is incapable of balancing workloads across devices. I suspect that introducing a task scheduler could bring considerable benefits. **Example**. Consider a dynamic programming problem with the following structure  For each parameter set, solve a dynamic programming problem. Parallelizable across parameter sets.      Sequential backwards time loop. It can use `jax.lax.scan`.          For each possible state from a state grid, solve an inner problem that makes a call to `quadax.quadgk` (only 1 adaptive step) with `jaxopt.ProjectedGradient`. Parallelized across states. I have a question about the inner loop. When applying `vmap` to hundreds of projected gradient descents, does a single hard problem force the rest of problems to keep evaluating even if they already found a solution? That is, could `vmap` be counterproductive if there are big differences in terms of iterations required by each problem? In general, wha)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Efficient parallelization of tasks with highly variable difficulty and run times,"Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example. I am struggling to write **idiomatic and efficient** parallel code in jax for tasks with variable difficulty and run times. Jax parallelization is based on sharding and it is incapable of balancing workloads across devices. I suspect that introducing a task scheduler could bring considerable benefits. **Example**. Consider a dynamic programming problem with the following structure  For each parameter set, solve a dynamic programming problem. Parallelizable across parameter sets.      Sequential backwards time loop. It can use `jax.lax.scan`.          For each possible state from a state grid, solve an inner problem that makes a call to `quadax.quadgk` (only 1 adaptive step) with `jaxopt.ProjectedGradient`. Parallelized across states. I have a question about the inner loop. When applying `vmap` to hundreds of projected gradient descents, does a single hard problem force the rest of problems to keep evaluating even if they already found a solution? That is, could `vmap` be counterproductive if there are big differences in terms of iterations required by each problem? In general, wha",2024-06-28T13:44:13Z,enhancement,open,5,0,https://github.com/jax-ml/jax/issues/22176
1454,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([memories] Overlap shard transfers in async_serialize)ï¼Œ å†…å®¹æ˜¯ (Builds upon https://github.com/google/jax/pull/22114, which introduces a fastpath for async checkpoint saving in which each array shard is copied through a single devicetopinnedhost transfer. So far, all of these transfers are serialized. The present PR allows overlapping the transfers of a single array's shards. This is achieved by inserting `asyncio.sleep(0)` right after the transfer has been started, thereby permitting the `_write_array(shard)` coroutine to yield. _Why do we need the sleep?_ Python's `await` doesn't actually guarantee that control will be yielded to the eventloop so that other coroutines can be scheduled. Python ties coroutine semantics to that of generators. As I understand it, the only thing that causes control to pass to the eventloop (and thus allows other coroutines to be scheduled) is a `yield`. What we therefore need to achieve overlap in our situation, is something like  This is, in fact, what `asyncio.sleep(0)` does. An alternative, more roundabout way to achieve overlap would be to split `_write_array(shard)` into a first phase that initiates all the transfers, and provides the resulting hostresident arrays to a second phase, in which we invoke `t.write(data)` as before.  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[memories] Overlap shard transfers in async_serialize,"Builds upon https://github.com/google/jax/pull/22114, which introduces a fastpath for async checkpoint saving in which each array shard is copied through a single devicetopinnedhost transfer. So far, all of these transfers are serialized. The present PR allows overlapping the transfers of a single array's shards. This is achieved by inserting `asyncio.sleep(0)` right after the transfer has been started, thereby permitting the `_write_array(shard)` coroutine to yield. _Why do we need the sleep?_ Python's `await` doesn't actually guarantee that control will be yielded to the eventloop so that other coroutines can be scheduled. Python ties coroutine semantics to that of generators. As I understand it, the only thing that causes control to pass to the eventloop (and thus allows other coroutines to be scheduled) is a `yield`. What we therefore need to achieve overlap in our situation, is something like  This is, in fact, what `asyncio.sleep(0)` does. An alternative, more roundabout way to achieve overlap would be to split `_write_array(shard)` into a first phase that initiates all the transfers, and provides the resulting hostresident arrays to a second phase, in which we invoke `t.write(data)` as before.  ",2024-06-28T11:52:03Z,pull ready,closed,0,4,https://github.com/jax-ml/jax/issues/22169,"Your CL got reverted because of internal failures for layouts. I posted on the chat, but we would need to reland that before we can submit this right?",~That's right  the first JAX PR (https://github.com/google/jax/pull/22114) requires various XLA PRs to be landed including the one that got reverted (https://github.com/openxla/xla/pull/14090). Will take a look.~ This should actually work out of the box since we don't depend on XLA CC(CI: update deprecated uses of setoutput) anymore.,"The test seems to be failing on CPU:  Traceback (most recent call last):   File ""jax/experimental/array_serialization/serialization_test.py"", in test_transfer_shard_to_host     sharding = SingleDeviceSharding(jax.devices()[0], memory_kind=""device"")                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ValueError: Could not find memory addressable by device cpu. Device cpu can address the following memory kinds: unpinned_host. Got memory kind: device Can you just skip on CPU?"," Sg, I marked the new test in https://github.com/google/jax/pull/22114 with `.skip_on_devices('cpu')` and also rebased this PR."
680,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Explicitly disallow duplicated devices during array construction)ï¼Œ å†…å®¹æ˜¯ (Explicitly disallow duplicated devices during array construction `jax.make_array_from_single_device_arrays` should not allow passing more than one array on the same device as that would lead to an invalid array. While some of this case is already detected by later checks (e.g., `ArrayImpl._check_and_rearrange`), this CL explicitly checks the device list before calling IFRT so that we don't create an invalid IFRT array to begin with.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Explicitly disallow duplicated devices during array construction,"Explicitly disallow duplicated devices during array construction `jax.make_array_from_single_device_arrays` should not allow passing more than one array on the same device as that would lead to an invalid array. While some of this case is already detected by later checks (e.g., `ArrayImpl._check_and_rearrange`), this CL explicitly checks the device list before calling IFRT so that we don't create an invalid IFRT array to begin with.",2024-06-28T08:34:47Z,,closed,0,0,https://github.com/jax-ml/jax/issues/22165
1283,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Mark jax typing Protocols as `@runtime_checkable`)ï¼Œ å†…å®¹æ˜¯ (Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example. **Goal**: Ensure ""isinstance"" works to check if a type (not a value) is a jax Array type. **Problem**: When deserializing data containing jax arrays, one must check if the types are jax Array types. However, ""jaxlib.xla_extension.ArrayImpl is jax.Array"" evaluates False (normal, but annoying) **Opportunity**: Could we make these typing protocols runtime checkable to faciliate serialization/deserialization logic?  TLDR Fix: `from typing import runtime_checkable` & use `` decorator on Protocol subclasses  Reproduction   Result !image  System:   References / Links (not strictly needed) [1] from a Python array API standard, but it's really long to paste here.  [2] Another resource for protocols is DLPack C API, seems clear on data type sizes of things  Here's the short notes on those, I'll add class attributes to the tree_plus eventually, but it can get noisy. !image  Workaround  !image Cheers, love me some jax!)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Mark jax typing Protocols as `@runtime_checkable`,"Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example. **Goal**: Ensure ""isinstance"" works to check if a type (not a value) is a jax Array type. **Problem**: When deserializing data containing jax arrays, one must check if the types are jax Array types. However, ""jaxlib.xla_extension.ArrayImpl is jax.Array"" evaluates False (normal, but annoying) **Opportunity**: Could we make these typing protocols runtime checkable to faciliate serialization/deserialization logic?  TLDR Fix: `from typing import runtime_checkable` & use `` decorator on Protocol subclasses  Reproduction   Result !image  System:   References / Links (not strictly needed) [1] from a Python array API standard, but it's really long to paste here.  [2] Another resource for protocols is DLPack C API, seems clear on data type sizes of things  Here's the short notes on those, I'll add class attributes to the tree_plus eventually, but it can get noisy. !image  Workaround  !image Cheers, love me some jax!",2024-06-27T16:23:54Z,enhancement,closed,0,5,https://github.com/jax-ml/jax/issues/22144,"Assigning  because he has thought a lot about this kind of typing issue, and may have good feedback!","I don't mind making `DuckTypedArray` runtimecheckable, but the guarantees these runtime checks give are pretty weak. In particular, they do not do any type checking:  Note also, that `DuckTypedArray` only has two methods and thus the runtime check is equivalent to `hasattr(..., ""shape"") and hasattr(..., ""dtype"")`, which means that e.g. a NumPy array will be considered an instance of `DuckTypedArray`. Is that the behavior you expect?","Oh, you're right, that wouldn't work, since it would make code think it sees jax arrays when it sees numpy arrays. I just want to roundtrip serialize/deserialize all the things, and hit a situation where I needed to check if a type is jax.Array or not, but `T is jax.Array` didn't work for ArrayImpl. The typeguard function solved my problem, could be fine to just stick with that. Seems like a lot of python typing is still Work in Progress, if there's a solution which python's type system supports better than protocols, like dataclasses or abstract base classes, that's great.  Mainly my objective was to ensure we can support both type and value level checks, for the abstract case to check if type T is jax Array, and the concrete case to check if value V is an instance of a jax Array I'm agnostic about the implementation details, so whatever you think is best with the state of Python, I'm down to test it","`jax.Array` is a base class, so to check if `x` is a JAX array you can use `isinstance(x, jax.Array)`, or to check if `T = type(x)` is an array type, you can use `issubclass(T, jax.Array)`.","sounds good! only thing i'd add would be, make sure to use try/except with issubclass, it's pretty flaky, crashes with annotations (oof) !image given that, here's a working solution with issubclass, thank you ! closing  !image"
1286,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tweak dynamic trace state to only depend on level int, not MainTrace)ï¼Œ å†…å®¹æ˜¯ (This is a small step in simplifying some parts of the core tracing machinery. Our caches, like the C++ dispatch cache, must depend on JAX's global tracing state. The main part of the global state is the trace stack (aka `core.thread_local_state.trace_state.trace_stack`), and one component of that is which trace is the 'dynamic' trace, i.e. `core.thread_local_state.trace_state.trace_stack.dynamic`. That attribute is a reference to a `MainTrace` instance, but it could've just been an `int`: it's just pointing to object in`core.thread_local_state.trace_state.trace_stack.stack`, which is a `list[MainTrace]`. We should probably make it an int! Or revise it away completely. That's planned followup work. For now, this PR just changes the way we represent it to the C++ cache: we can only hit the fast path when the trace state is basically `TraceStack(stack=[EvalTrace(0)], dynamic=0)`, so we should include `(dynamic.level, dynamic.trace_type)` in the cache key (and we'll only ever populate a cache entry when it's `(0, EvalTrace)`).)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"tweak dynamic trace state to only depend on level int, not MainTrace","This is a small step in simplifying some parts of the core tracing machinery. Our caches, like the C++ dispatch cache, must depend on JAX's global tracing state. The main part of the global state is the trace stack (aka `core.thread_local_state.trace_state.trace_stack`), and one component of that is which trace is the 'dynamic' trace, i.e. `core.thread_local_state.trace_state.trace_stack.dynamic`. That attribute is a reference to a `MainTrace` instance, but it could've just been an `int`: it's just pointing to object in`core.thread_local_state.trace_state.trace_stack.stack`, which is a `list[MainTrace]`. We should probably make it an int! Or revise it away completely. That's planned followup work. For now, this PR just changes the way we represent it to the C++ cache: we can only hit the fast path when the trace state is basically `TraceStack(stack=[EvalTrace(0)], dynamic=0)`, so we should include `(dynamic.level, dynamic.trace_type)` in the cache key (and we'll only ever populate a cache entry when it's `(0, EvalTrace)`).",2024-06-26T20:52:30Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/22123,"Hopefully we can delete much of this anyway. That is, we have a plan to delete the `dynamic` field entirely..."
250,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Run `pyupgrade --py310-plus` on .pyi files.)ï¼Œ å†…å®¹æ˜¯ (Manually fix import orders.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Run `pyupgrade --py310-plus` on .pyi files.,Manually fix import orders.,2024-06-26T19:25:10Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/22118
1474,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([memories] Transfer to pinned_host fast path in async_serialize)ï¼Œ å†…å®¹æ˜¯ (Adds a fast path to `jax.experimental.array_serialization.serialization.async_serialize` that avoids XLA's regular devicetohost transfer and instead uses a single devicetopinnedhost transfer per `_write_array(arr)` invocation. This allows us to achieve much closer to ideal transfer bandwidths in practice. For comparison, the existing approach stages copies through a fixed size intermediate 128MBbuffer and requires `sizeof(arr)/128MB` alternations between D2H and H2H copies. Note that the `np.array(data, copy=False)` is not strictly necessary as the tensorstore invocation `t.write(...)` immediately performs the CAPI equivalent of `np.array(data, copy=None)`. We expect all of these to be zerocopy, hence explicitly calling `np.array(data, copy=False)` provides some extra safety, since it would fail if jax.Array's implementation changed and no longer permitted zerocopying its private numpy array `_value`. Alas the latter check is not foolproof: for example, prior to XLA CC(Clarify the docstring for vjp.) the construction of the jax.Array from the device buffer also forced a copy. Depends on XLA CC(Expose fp8 types from jnp namespace.) XLA CC(Fix typo ""invalud"" > ""invalid"" in error message.) XLA CC(Clarify the docstr)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[memories] Transfer to pinned_host fast path in async_serialize,"Adds a fast path to `jax.experimental.array_serialization.serialization.async_serialize` that avoids XLA's regular devicetohost transfer and instead uses a single devicetopinnedhost transfer per `_write_array(arr)` invocation. This allows us to achieve much closer to ideal transfer bandwidths in practice. For comparison, the existing approach stages copies through a fixed size intermediate 128MBbuffer and requires `sizeof(arr)/128MB` alternations between D2H and H2H copies. Note that the `np.array(data, copy=False)` is not strictly necessary as the tensorstore invocation `t.write(...)` immediately performs the CAPI equivalent of `np.array(data, copy=None)`. We expect all of these to be zerocopy, hence explicitly calling `np.array(data, copy=False)` provides some extra safety, since it would fail if jax.Array's implementation changed and no longer permitted zerocopying its private numpy array `_value`. Alas the latter check is not foolproof: for example, prior to XLA CC(Clarify the docstring for vjp.) the construction of the jax.Array from the device buffer also forced a copy. Depends on XLA CC(Expose fp8 types from jnp namespace.) XLA CC(Fix typo ""invalud"" > ""invalid"" in error message.) XLA CC(Clarify the docstr",2024-06-26T18:32:18Z,pull ready,closed,0,7,https://github.com/jax-ml/jax/issues/22114,Do you have some benchmarks where this is super fast and helpful? (something that you ran locally or in your runs?),"  > Do you have some benchmarks where this is super fast and helpful? (something that you ran locally or in your runs?) Here's a selfcontained example that doesn't quite behave like the E2E workload mentioned before, but illustrates the effects and is a good candidate for profiling: https://gist.github.com/gspschmid/52a1062916c7030a513b0581bd56c5be The first improvement corresponds to this PR (along with its XLA dependencies), the second improvement corresponds to https://github.com/google/jax/pull/22169. Note that after applying the first improvement other overheads in tensorstore's `t.write(data)` begin to dominate. Attached below are some screenshots of nsys profiles corresponding to the last iteration for each variant. Baseline:  devicetopinnedhost transfer:  devicetopinnedhost transfer + overlap shard transfers: "," Not sure what CI you run for JAX contributions, but now that the remaining XLA PRs are in (https://github.com/openxla/xla/pull/14089 and https://github.com/openxla/xla/pull/14268) this should be ready to test."," , This PR should be good to go  since we were able to switch from `jax.jit` to using `device_put` it doesn't depend on XLA CC(CI: update deprecated uses of setoutput) any longer. I checked with a recent unpatched XLA build + these pending JAX patches and got the expected performance."," Seems like the test failed due to `memories_test.py` now depending on `tensorstore`, but that dependency being missing from the bazel target. Do you prefer adding something like ` + py_deps(""tensorstore"")` there or should I move the test to `jax/experimental/array_serialization/serialization_test.py`?",Yeah move it to serialization_test.py please :), Done! Let's give it another go :)
358,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jax.numpy: better docstring for isreal and isrealobj functions)ï¼Œ å†…å®¹æ˜¯ (Better docstring and JAXspecific code examples are added for jax.numpy.isreal and jax.numpy.isrealobj. Part of 21461)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jax.numpy: better docstring for isreal and isrealobj functions,Better docstring and JAXspecific code examples are added for jax.numpy.isreal and jax.numpy.isrealobj. Part of 21461,2024-06-25T23:04:07Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/22097,Thanks   Typo fixed.
687,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Ergonomics: Move calls to partial inside decorators)ï¼Œ å†…å®¹æ˜¯ (Please:  [ x] Check for duplicate requests.  [ x] Describe your goal, and if possible provide a code snippet with a motivating example. It would be nice to eliminate the awkwardness with partials for decorators. Code like this:  Can be simplified to this:  This may seem minor, but little ergonomic changes like this can have an outsized impact on both interest in adoption and postadoption development speed. Here's how to do it with `jit` in particular: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Ergonomics: Move calls to partial inside decorators,"Please:  [ x] Check for duplicate requests.  [ x] Describe your goal, and if possible provide a code snippet with a motivating example. It would be nice to eliminate the awkwardness with partials for decorators. Code like this:  Can be simplified to this:  This may seem minor, but little ergonomic changes like this can have an outsized impact on both interest in adoption and postadoption development speed. Here's how to do it with `jit` in particular: ",2024-06-24T15:20:00Z,enhancement,closed,4,1,https://github.com/jax-ml/jax/issues/22061,"Thanks for the request! This has been discussed before at CC(Decorator factory pattern), and has come up a number of times since then. I think the opinion of the maintainers is still the same, so I'm going to close this as a duplicate. Please feel free to comment on CC(Decorator factory pattern) if you disagree with the concensus reached there."
409,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(scan of device_put carry raises TypeError during the backward pass)ï¼Œ å†…å®¹æ˜¯ ( Description After jax 0.4.30, the following code raises   This does not happen if we define  or   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,scan of device_put carry raises TypeError during the backward pass," Description After jax 0.4.30, the following code raises   This does not happen if we define  or   System info (python version, jaxlib version, accelerator, etc.) ",2024-06-22T01:33:14Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/22045,I'll take a look. Thanks for the minimal repro!,I think this is a case of inconsistent dtypes.. the following works where everything is a float (instead of a combination of float and int).  The error message still sucks though. I'll look into improving it.,"Hi ,  Thanks for reporting this, could you try to execute the code in 0.4.34 and higher Jax version, it seems to be working fine.  Attaching gist for your reference. Thank you.",Let's close this issue then. Thanks!
1459,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jaxlib wheel hardcoded to manylinux2014 platform)ï¼Œ å†…å®¹æ˜¯ ( Description I am opening this issue to get a discussion started around manylinux support for JAX/jaxlib. I am currently working on trying to get manylinux_2_28 jaxlib wheels built with ROCm support for ROCm users. Currently jaxlib wheels are hardcoded to be tagged as manylinux2014. See https://github.com/google/jax/blob/main/jaxlib/tools/build_wheel.pyL167 and https://github.com/google/jax/blob/main/jax/tools/build_utils.pyL56L58 This hardcoding of platform name forces the wheel build process to tag the wheel filename and metadata (WHEEL file inside the zip) as manylinux2014 which is only correct if building/linking on a platform with glibc 2.17, which is rather old. I am not aware of how google is building these wheels internally, but I imagine this is going to result in an incorrect wheel build for almost all other linux users. I have tested a change which removes the platform name override, running a build on a Centos8 / Almalinux 8 system, and it results in a `linux_x86_64` wheel, which is expected. After running `auditwheel repair` on this wheel it correctly scans the glibc versioned symbols and creates a new manylinux_2_27 wheel (highest symbol is actually 2.27 in the output wheel). This seems more correc)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jaxlib wheel hardcoded to manylinux2014 platform," Description I am opening this issue to get a discussion started around manylinux support for JAX/jaxlib. I am currently working on trying to get manylinux_2_28 jaxlib wheels built with ROCm support for ROCm users. Currently jaxlib wheels are hardcoded to be tagged as manylinux2014. See https://github.com/google/jax/blob/main/jaxlib/tools/build_wheel.pyL167 and https://github.com/google/jax/blob/main/jax/tools/build_utils.pyL56L58 This hardcoding of platform name forces the wheel build process to tag the wheel filename and metadata (WHEEL file inside the zip) as manylinux2014 which is only correct if building/linking on a platform with glibc 2.17, which is rather old. I am not aware of how google is building these wheels internally, but I imagine this is going to result in an incorrect wheel build for almost all other linux users. I have tested a change which removes the platform name override, running a build on a Centos8 / Almalinux 8 system, and it results in a `linux_x86_64` wheel, which is expected. After running `auditwheel repair` on this wheel it correctly scans the glibc versioned symbols and creates a new manylinux_2_27 wheel (highest symbol is actually 2.27 in the output wheel). This seems more correc",2024-06-21T17:06:23Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/22034,"In general our build script produces the tags that our release builds should produce; it doesn't try to adapt the tags to the environment. I'm a little reluctant to run `auditwheel repair` because I don't want auditwheel changing the wheel. One option would be for us to default to a relaxed tag like `linux_x86_64`, and change our release builds to override the tag to a `manylinux` tag.","As to ongoing support: we're not sure yet. One likely possibility is at least 2_28, but we may go newer. (The thing pushing us to newer is that we would like to build with C++20).","> One option would be for us to default to a relaxed tag like linux_x86_64, and change our release builds to override the tag to a manylinux tag. I think this might be easiest, because then it pushes the ""generalization"" step to the release process that comes after build. I am also a bit worried about `auditwheel repair` changing the wheel as well, since it tries to embed any third party SO files into the zip and changes the RPATHs to use them instead of any external ones. This can be worked around with its exclude options, but it seems too easy to miss something. My current plan is to probably write something that uses auditwheel's functionality to modify the wheel but do it in a more controlled manner. Basically, just doing the lddtree scan for all versioned symbols, collecting the max versions of libs, and then outputting the `manylinux_x_y` wheel, without any RPATH or SO file changes. > One likely possibility is at least 2_28, but we may go newer. (The thing pushing us to newer is that we would like to build with C++20). I am positive this would make a lot of folks that use JAX on older systems very upset... all the users I am supporting right now seem to be on ubuntu 20 or EL8 derivatives which require glibc 2.28 or less. I haven't looked at the glibcpp versions on those distros but I am almost certain they are not C++20 level universally."
322,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jax.numpy: better docstring for iscomplex function)ï¼Œ å†…å®¹æ˜¯ (New docstring and JAXspecific code examples are added for jax.numpy.iscomplex. Part of 21461)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jax.numpy: better docstring for iscomplex function,New docstring and JAXspecific code examples are added for jax.numpy.iscomplex. Part of 21461,2024-06-20T22:53:40Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/22014
270,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Improved docs for jnp.polyint and jnp.polyder)ï¼Œ å†…å®¹æ˜¯ (Part of CC(Tracking issue: inline docstrings))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Improved docs for jnp.polyint and jnp.polyder,Part of CC(Tracking issue: inline docstrings),2024-06-20T09:18:12Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/21991
1442,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Improve compilation cache tests)ï¼Œ å†…å®¹æ˜¯ (This PR improves the tests for the compilation cache.  Background The JAX compilation cache operates through a set of functions defined in `jax/_src/compilation_cache.py`. These functions implement the operations of the compilation cache. A global variable, `_cache`, is defined in this file as an instance of the abstract class `CacheInterface`. This instance can be either `GFileCache` or the new `LRUCache` added in https://github.com/google/jax/pull/21394. There are three levels of abstraction involved: 1. Compilation cache operations 2. The `CacheInterface` abstract class 3. Implementations of the `CacheInterface` abstract class  The Problem The current `compilation_cache_test.py` file is intended to test the compilation cache operations but relies heavily on the implementation details of the underlying cache. For instance, it assumes that the number of files in the cache directory matches the number of items in the cache.  Why is This a Problem? With the intended introduction of the LRU cache eviction policy for the JAX compilation cache, we now require two separate files for each cache item: one for data and one for metadata. Consequently, the assumption that one cache item corresponds to one file in the cach)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Improve compilation cache tests,"This PR improves the tests for the compilation cache.  Background The JAX compilation cache operates through a set of functions defined in `jax/_src/compilation_cache.py`. These functions implement the operations of the compilation cache. A global variable, `_cache`, is defined in this file as an instance of the abstract class `CacheInterface`. This instance can be either `GFileCache` or the new `LRUCache` added in https://github.com/google/jax/pull/21394. There are three levels of abstraction involved: 1. Compilation cache operations 2. The `CacheInterface` abstract class 3. Implementations of the `CacheInterface` abstract class  The Problem The current `compilation_cache_test.py` file is intended to test the compilation cache operations but relies heavily on the implementation details of the underlying cache. For instance, it assumes that the number of files in the cache directory matches the number of items in the cache.  Why is This a Problem? With the intended introduction of the LRU cache eviction policy for the JAX compilation cache, we now require two separate files for each cache item: one for data and one for metadata. Consequently, the assumption that one cache item corresponds to one file in the cach",2024-06-19T17:24:12Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/21982,Please squash your commits.
394,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Remove `jax.xla_computation` tests from jax2tf. `api_test.py` has enough coverage for `jax.xla_computation`)ï¼Œ å†…å®¹æ˜¯ (Remove `jax.xla_computation` tests from jax2tf. `api_test.py` has enough coverage for `jax.xla_computation`)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Remove `jax.xla_computation` tests from jax2tf. `api_test.py` has enough coverage for `jax.xla_computation`,Remove `jax.xla_computation` tests from jax2tf. `api_test.py` has enough coverage for `jax.xla_computation`,2024-06-18T23:45:50Z,,closed,0,0,https://github.com/jax-ml/jax/issues/21970
327,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jax.numpy: better docstring for iscomplexobj function)ï¼Œ å†…å®¹æ˜¯ (New docstring and JAXspecific code examples are addd for jax.numpy.iscomplexobj. Part of 21461)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jax.numpy: better docstring for iscomplexobj function,New docstring and JAXspecific code examples are addd for jax.numpy.iscomplexobj. Part of 21461,2024-06-18T20:11:03Z,pull ready,closed,0,2,https://github.com/jax-ml/jax/issues/21966,Thanks   Long characters wrapped and commits squashed into one.,Looks like there are lint issues related to trailing whitespace
821,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bad cast error in lax backend (OSX))ï¼Œ å†…å®¹æ˜¯ ( Description This is a fresh conda environment. Just trying to get jax working on my M2.  **Error**   System info (python version, jaxlib version, accelerator, etc.) (I get the same error on 0.4.30) OSX Sonoma 14.5 jax:    0.4.26 jaxlib: 0.4.26 numpy:  1.26.4 python: 3.10.14 (main, May  6 2024, 14:42:37) [Clang 14.0.6 ] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Darwin', node='LeonsMacBookAir.local', release='23.5.0', version='Darwin Kernel Version 23.5.0: Wed May  1 20:19:05 PDT 2024; root:xnu10063.121.3~5/RELEASE_ARM64_T8112', machine='arm64'))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bad cast error in lax backend (OSX)," Description This is a fresh conda environment. Just trying to get jax working on my M2.  **Error**   System info (python version, jaxlib version, accelerator, etc.) (I get the same error on 0.4.30) OSX Sonoma 14.5 jax:    0.4.26 jaxlib: 0.4.26 numpy:  1.26.4 python: 3.10.14 (main, May  6 2024, 14:42:37) [Clang 14.0.6 ] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Darwin', node='LeonsMacBookAir.local', release='23.5.0', version='Darwin Kernel Version 23.5.0: Wed May  1 20:19:05 PDT 2024; root:xnu10063.121.3~5/RELEASE_ARM64_T8112', machine='arm64')",2024-06-18T19:24:59Z,bug,closed,0,6,https://github.com/jax-ml/jax/issues/21964,Can you try with the latest release 0.4.30?,"Already did :/ >  System info (python version, jaxlib version, accelerator, etc.) > (I get the same error on 0.4.30) >  > OSX Sonoma 14.5 also made sure to update xcode","Hmm. I can't reproduce on my ARM Macbook, also running Sonoma. One thing I'm guessing might be related is that I'm using Python from `pyenv`. I wonder if this is `conda` specific?","I can't reproduce with a fresh condaforge install, either. I'm not allowed to use miniconda because of its licensing, so I'm not sure there's anything more I can do. One thing I might suggest trying: can you try with a fresh conda environment?","I run into the same problem. It started with a numpy incompatibility with pandas. After reinstalling numpy, `ValueError: std::bad_cast` is reported for similar codes. But creating a fresh conda environment does solve the issue.","Strange... as noted by  a fresh environment solves the problem. As did a fresh conda environment with installs from condaforge, or a pyenv. "
823,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(RuntimeError: jaxlib/gpu/solver_kernels.cc:45: operation gpusolverDnCreate(&handle) failed: cuSolver internal error)ï¼Œ å†…å®¹æ˜¯ ( Description  Description I am trying to use JAX version 0.4.29 with CUDA 12.4. When I computed a simple linear algebraic calculation, I got an error `RuntimeError: jaxlib/gpu/solver_kernels.cc:45: operation gpusolverDnCreate(&handle) failed: cuSolver internal error`.  Error When I did the following, I found the above error  On the other hand, when I tried the following command, it works well.   System info (python version, jaxlib version, accelerator, etc.)  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,RuntimeError: jaxlib/gpu/solver_kernels.cc:45: operation gpusolverDnCreate(&handle) failed: cuSolver internal error," Description  Description I am trying to use JAX version 0.4.29 with CUDA 12.4. When I computed a simple linear algebraic calculation, I got an error `RuntimeError: jaxlib/gpu/solver_kernels.cc:45: operation gpusolverDnCreate(&handle) failed: cuSolver internal error`.  Error When I did the following, I found the above error  On the other hand, when I tried the following command, it works well.   System info (python version, jaxlib version, accelerator, etc.)  System info (python version, jaxlib version, accelerator, etc.) ",2024-06-18T12:27:39Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/21950,"Can you describe exactly how you installed jax (and any relevant drivers, etc.) so that we can try to reproduce?",  Thanks for your message. I am using miniconda for the virtual environment. The following is what I did in a virtual enviornment.   The resulting `conda list` is as follows. ,"I'll look into this a little later, but you shouldn't need to install all those `nvidia` pip packages manually. What happens if you just `pip install jax[cuda12]` in a fresh environment?"," Thanks for your message. In a new environment, I did `pip install upgrade ""jax[cuda12]""`. It looks like `pip install upgrade ""jax[cuda12]""` cannot install collect versions of `nvidia` packages. Actually, the reason why I installed `nvidia` myself is that I met the same warning.    On the other hand, the original error did not appear.   Just in case, I also show my `conda list`.  P.S. Yesterday, JAX was updated, so the result of `jax.print_environment_info()` was also changed.  ","Thanks for the info. I can reproduce the warning that you're seeing about the ptxas version. I was able to work around this by simply downgrading the `nvidiacudanvcccu12` pip package. So, from a fresh virtual environment, I was able to get a working installation with:  Want to see if that fixes the issue for you? Edited to add: I also wouldn't worry too much about the warning. It may make `jit` compilation a little bit slower, but I wouldn't expect it to be a major issue!", Thank you for your reply. The command you provided works for my environment. My program seems to run correctly. Let me check further if nothing happens. ,I'm going to close this as completed  please feel free to comment if there are other issues.
1009,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Inconsistent behavior with pallas + jnp.power + fp16)ï¼Œ å†…å®¹æ˜¯ ( Description I have the following 3 kernels that does y = x ** 1 , y= x ** 2 , y= x ** 3, and their behavior is inconsistent (one fails to compile, 2 succeeds)  Running `kernel_2` and `kernel_3` succeeds  But not for 1 (or any other integers that I tested, for that matter)  I have 2 questions: 1. In this simple example, why does jax/pallas handle `2`, `3` and other integers differently? 2. It seems to me that `2` and `3` works because an implicit type deduction (from python's Int to a fp16 compatible integer when lowering) is in place, and for other integers jax don't do that. Are users required to signify the type deduction in some way when using python constant in pallas? If so, how does one do that?  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Inconsistent behavior with pallas + jnp.power + fp16," Description I have the following 3 kernels that does y = x ** 1 , y= x ** 2 , y= x ** 3, and their behavior is inconsistent (one fails to compile, 2 succeeds)  Running `kernel_2` and `kernel_3` succeeds  But not for 1 (or any other integers that I tested, for that matter)  I have 2 questions: 1. In this simple example, why does jax/pallas handle `2`, `3` and other integers differently? 2. It seems to me that `2` and `3` works because an implicit type deduction (from python's Int to a fp16 compatible integer when lowering) is in place, and for other integers jax don't do that. Are users required to signify the type deduction in some way when using python constant in pallas? If so, how does one do that?  System info (python version, jaxlib version, accelerator, etc.) ",2024-06-17T20:21:08Z,bug pallas,closed,0,1,https://github.com/jax-ml/jax/issues/21928,"We specialcase a few integer powers here and use libdevice functions for the rest. libdevice does not have a `pow` implementation for float16 and int32. I think we should probably loop instead in the general case, since the power is a statically known integer."
1246,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([`LRUCache`] Storing the last access time of a cache entry in a separate file)ï¼Œ å†…å®¹æ˜¯ (This PR is a followup of https://github.com/google/jax/pull/21394. It stores the last access time of a cache entry in a separate file for `LRUCache`. Previously, the information was stored in the file metadata (`mtime`). This PR is part of the process to add LRU cache eviction policy for the JAX persistent compilation cache. This change is necessary because we want to support GCS and potentially other cloud storage providers, where the `mtime` attribute may be either not available or very complicated to modify. I am planning to get this PR merged before https://github.com/google/jax/pull/21893, because this one is more complicated and merging this PR first will make it easier to resolve merge conflicts.  Details Previous behavior: If cache entries: `['a', 'b']` Then cache files: `['cache_dir/a', 'cache_dir/b']` Modified behavior: If cache entries: `['a', 'b']` Then cache files: `['cache_dir/acache.bin', 'cache_dir/bcache.bin', 'cache_dir/aatime.txt', 'cache_dir/batime.txt']`)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[`LRUCache`] Storing the last access time of a cache entry in a separate file,"This PR is a followup of https://github.com/google/jax/pull/21394. It stores the last access time of a cache entry in a separate file for `LRUCache`. Previously, the information was stored in the file metadata (`mtime`). This PR is part of the process to add LRU cache eviction policy for the JAX persistent compilation cache. This change is necessary because we want to support GCS and potentially other cloud storage providers, where the `mtime` attribute may be either not available or very complicated to modify. I am planning to get this PR merged before https://github.com/google/jax/pull/21893, because this one is more complicated and merging this PR first will make it easier to resolve merge conflicts.  Details Previous behavior: If cache entries: `['a', 'b']` Then cache files: `['cache_dir/a', 'cache_dir/b']` Modified behavior: If cache entries: `['a', 'b']` Then cache files: `['cache_dir/acache.bin', 'cache_dir/bcache.bin', 'cache_dir/aatime.txt', 'cache_dir/batime.txt']`",2024-06-17T20:12:35Z,pull ready,closed,0,2,https://github.com/jax-ml/jax/issues/21926,Blocked by the failed `CompilationCacheTest` above,Need https://github.com/google/jax/pull/21982 to be merged first
1481,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add an `ffi_call` function with a similar signature to `pure_callback`)ï¼Œ å†…å®¹æ˜¯ (Currently, JAX users who want to use XLA custom calls must interact with private APIs (e.g. `core.Primitive`) and MLIR. This doesnâ€™t provide a great developer experience, and it would be useful to provide some sort of public API. This has been previously discussed in several different contexts (including CC([FFI]: Add JEP for FFI)), and this PR builds on ideas from this previous work to present a simple API that covers some core use cases. There are more advanced use cases which would require finergrained customization, and these would continue to rely on the private API. But, there do appear to be examples of use cases that would be satisfied by this simpler interface.  Example The general idea is to provide a function called (something like) `jax.extend.ffi.ffi_call` with a signature that is similar to `jax.pure_callback`, that lowers to a custom call. For example, the existing implementation of lu_pivots_to_permutation on GPU (the only FFI custom call currently in `jaxlib`), could (to first approximation) be written as:  Platformdependent behavior should be handled in user code with the help of `lax.platform_dependent`. (Currently this doesn't work, but  is looking into it.) Like `jax.pure_callback`, this cou)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add an `ffi_call` function with a similar signature to `pure_callback`,"Currently, JAX users who want to use XLA custom calls must interact with private APIs (e.g. `core.Primitive`) and MLIR. This doesnâ€™t provide a great developer experience, and it would be useful to provide some sort of public API. This has been previously discussed in several different contexts (including CC([FFI]: Add JEP for FFI)), and this PR builds on ideas from this previous work to present a simple API that covers some core use cases. There are more advanced use cases which would require finergrained customization, and these would continue to rely on the private API. But, there do appear to be examples of use cases that would be satisfied by this simpler interface.  Example The general idea is to provide a function called (something like) `jax.extend.ffi.ffi_call` with a signature that is similar to `jax.pure_callback`, that lowers to a custom call. For example, the existing implementation of lu_pivots_to_permutation on GPU (the only FFI custom call currently in `jaxlib`), could (to first approximation) be written as:  Platformdependent behavior should be handled in user code with the help of `lax.platform_dependent`. (Currently this doesn't work, but  is looking into it.) Like `jax.pure_callback`, this cou",2024-06-17T19:50:28Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/21925
326,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(sparse: bcoo_spdot_general have incorrect abstract_eval)ï¼Œ å†…å®¹æ˜¯ ( Description  results in   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,sparse: bcoo_spdot_general have incorrect abstract_eval," Description  results in   System info (python version, jaxlib version, accelerator, etc.) ",2024-06-17T18:59:32Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/21921,Thanks for the report! CC(Fix bug in bcoo_spdot_general abstract_eval) should fix this.
1224,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Use constant memory to pass in TMA descriptors to the kernel)ï¼Œ å†…å®¹æ˜¯ (Use constant memory to pass in TMA descriptors to the kernel To work around another buggy part of the PTX documentation. While PTX explicitly says that TMA descriptors can be in global memory, the C++ programming guide heavily discurages this, because it can lead to incorrrect results. Which is also what we've sometimes observed as a cache coherency issue unless a TMA fence is explicitly inserted at the beginning of the kernel. Note that this approach has a big downside of making the kernel unsafe for concurrent use. I don't think that XLA:GPU will ever dispatch it concurrently so I didn't insert any extra synchronization for now, but we should seriously consider it. My hope at the moment is that we'll be able to start passing in TMA descs as kernel args soon (pending upstreaming LLVM changes...) and we won't have to deal with this again. For the programming guide, see: https://docs.nvidia.com/cuda/cudacprogrammingguide/index.htmlusingtmatotransfermultidimensionalarrays)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Use constant memory to pass in TMA descriptors to the kernel,"Use constant memory to pass in TMA descriptors to the kernel To work around another buggy part of the PTX documentation. While PTX explicitly says that TMA descriptors can be in global memory, the C++ programming guide heavily discurages this, because it can lead to incorrrect results. Which is also what we've sometimes observed as a cache coherency issue unless a TMA fence is explicitly inserted at the beginning of the kernel. Note that this approach has a big downside of making the kernel unsafe for concurrent use. I don't think that XLA:GPU will ever dispatch it concurrently so I didn't insert any extra synchronization for now, but we should seriously consider it. My hope at the moment is that we'll be able to start passing in TMA descs as kernel args soon (pending upstreaming LLVM changes...) and we won't have to deal with this again. For the programming guide, see: https://docs.nvidia.com/cuda/cudacprogrammingguide/index.htmlusingtmatotransfermultidimensionalarrays",2024-06-17T11:24:52Z,,closed,0,0,https://github.com/jax-ml/jax/issues/21910
901,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Build fails: Can't find amdhip64 and roctracer libs)ï¼Œ å†…å®¹æ˜¯ ( Description Hello, When building v0.4.16 on Arch Linux, I ran into an issue where the amdhip64 and roctracer libs can't be found.  It seems like JAX is looking for them under `/opt/rocm/hip` and `/opt/rocm/roctracer`, respectively.  However, both libs only exist in `/opt/rocm/lib`.  Copying the libs to where JAX is looking for them resolves the issue, but I think that work should be handled by the build script. I've already reported this issue in the `pythonjaxrocm` Arch Linux package: https://github.com/brianrobt/pythonjaxrocm/issues/3 The exact command that is failing is this:  Thank you, Brian  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Build fails: Can't find amdhip64 and roctracer libs," Description Hello, When building v0.4.16 on Arch Linux, I ran into an issue where the amdhip64 and roctracer libs can't be found.  It seems like JAX is looking for them under `/opt/rocm/hip` and `/opt/rocm/roctracer`, respectively.  However, both libs only exist in `/opt/rocm/lib`.  Copying the libs to where JAX is looking for them resolves the issue, but I think that work should be handled by the build script. I've already reported this issue in the `pythonjaxrocm` Arch Linux package: https://github.com/brianrobt/pythonjaxrocm/issues/3 The exact command that is failing is this:  Thank you, Brian  System info (python version, jaxlib version, accelerator, etc.) ",2024-06-14T15:55:31Z,bug needs info AMD GPU,closed,0,3,https://github.com/jax-ml/jax/issues/21886,v0.4.16 is quite old at this point. Does the issue still happen with a current version?,", Let me try early next week and I'll get back to you.  Thanks!",The issue doesn't happen on v0.4.29.  I'm closing out the issue.  Thanks !
327,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add missing dependency)ï¼Œ å†…å®¹æ˜¯ (Local check if for missing dependency. Proper fix is changing the upstream build target, verifying if this would fix locally.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add missing dependency,"Local check if for missing dependency. Proper fix is changing the upstream build target, verifying if this would fix locally.",2024-06-13T18:25:02Z,windows:force-run,closed,0,0,https://github.com/jax-ml/jax/issues/21858
1419,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`jit-eval_shape-<callable PyTree with numpy __eq__ semantics>` crashes on JAX 0.4.29)ï¼Œ å†…å®¹æ˜¯ ( Description First the MWE:  it looks like JAX is trying to determine cacheability of a callable using just `hash`: https://github.com/google/jax/blob/a0e5e0f411f3a4b2830e90fb6fdc914730f29ba4/jax/_src/api.pyL2807L2809 but in practice to be usable as a dictionary key, we must also require that its `__eq__` method return a boolean  and not a traced boolean! Equinox tries to follow NumPy semantics here by doing approximately `all(x == y for x, y in zip(leaves(self), leaves(other)))`, and this returns a tracer. FWIW the `hash` check *would* have worked for Equinox here  as Equinox computes its `__hash__` by hashing all the leaves  except that CC(Tracers are now hashable.) means that the `hash` worked!  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.29 jaxlib: 0.4.29 numpy:  1.26.4 python: 3.11.9 (main, Apr 19 2024, 11:43:47) [Clang 14.0.6 ] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Darwin', node='Air.localdomain', release='22.5.0', version='Darwin Kernel Version 22.5.0: Mon Apr 24 20:52:43 PDT 2023; root:xnu8796.121.2~5/RELEASE_ARM64_T8112', machine='arm64'))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,`jit-eval_shape-<callable PyTree with numpy __eq__ semantics>` crashes on JAX 0.4.29," Description First the MWE:  it looks like JAX is trying to determine cacheability of a callable using just `hash`: https://github.com/google/jax/blob/a0e5e0f411f3a4b2830e90fb6fdc914730f29ba4/jax/_src/api.pyL2807L2809 but in practice to be usable as a dictionary key, we must also require that its `__eq__` method return a boolean  and not a traced boolean! Equinox tries to follow NumPy semantics here by doing approximately `all(x == y for x, y in zip(leaves(self), leaves(other)))`, and this returns a tracer. FWIW the `hash` check *would* have worked for Equinox here  as Equinox computes its `__hash__` by hashing all the leaves  except that CC(Tracers are now hashable.) means that the `hash` worked!  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.29 jaxlib: 0.4.29 numpy:  1.26.4 python: 3.11.9 (main, Apr 19 2024, 11:43:47) [Clang 14.0.6 ] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Darwin', node='Air.localdomain', release='22.5.0', version='Darwin Kernel Version 22.5.0: Mon Apr 24 20:52:43 PDT 2023; root:xnu8796.121.2~5/RELEASE_ARM64_T8112', machine='arm64')",2024-06-12T13:06:04Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/21825,"Hi kidger, I couldn't reproduce the issue with JAX 0.4.36 (and later) and Equinox 0.11.7/0.11.11. The provided repro is executed without any error. Please find the gist for reference. Thank you."
525,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Mosaic] Handle adding singleton minor dimension that was already implicit for non-32-bit types, and do not force native tiling)ï¼Œ å†…å®¹æ˜¯ ([Mosaic] Handle adding singleton minor dimension that was already implicit for non32bit types, and do not force native tiling Also fix extra comma in apply_vector_layout_test which was being annoying with autoformatter)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"[Mosaic] Handle adding singleton minor dimension that was already implicit for non-32-bit types, and do not force native tiling","[Mosaic] Handle adding singleton minor dimension that was already implicit for non32bit types, and do not force native tiling Also fix extra comma in apply_vector_layout_test which was being annoying with autoformatter",2024-06-11T00:14:22Z,,closed,0,0,https://github.com/jax-ml/jax/issues/21793
1055,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jax.numpy.linalg.inv returns spurious results when called with array of matrices)ï¼Œ å†…å®¹æ˜¯ ( Description I encountered unexpected behavior while using `jnp.linalg.inv` to invert an array of matrices (I wanted to get an array of their inverses). Specifically,   This isn't mentioned in the documentation, which specifies that the return type is   Array of shape (..., N, N) containing the inverse of the input.  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.28 jaxlib: 0.4.28 numpy:  1.26.4 python: 3.11.8 (v3.11.8:db85d51d3e, Feb  6 2024, 18:02:37) [Clang 13.0.0 (clang1300.0.29.30)] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Darwin', node='bsmbpas0019', release='23.3.0', version='Darwin Kernel Version 23.3.0: Wed Dec 20 21:33:31 PST 2023; root:xnu10002.81.5~7/RELEASE_ARM64_T8112', machine='arm64'))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jax.numpy.linalg.inv returns spurious results when called with array of matrices," Description I encountered unexpected behavior while using `jnp.linalg.inv` to invert an array of matrices (I wanted to get an array of their inverses). Specifically,   This isn't mentioned in the documentation, which specifies that the return type is   Array of shape (..., N, N) containing the inverse of the input.  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.28 jaxlib: 0.4.28 numpy:  1.26.4 python: 3.11.8 (v3.11.8:db85d51d3e, Feb  6 2024, 18:02:37) [Clang 13.0.0 (clang1300.0.29.30)] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Darwin', node='bsmbpas0019', release='23.3.0', version='Darwin Kernel Version 23.3.0: Wed Dec 20 21:33:31 PST 2023; root:xnu10002.81.5~7/RELEASE_ARM64_T8112', machine='arm64')",2024-06-10T19:26:50Z,bug duplicate,closed,0,2,https://github.com/jax-ml/jax/issues/21785,"Thanks for the report! This is a known XLA CPU issue, first reported in CC(vmap behaving unexpected with function that inverts singular matrix) and tracked here: https://github.com/openxla/xla/issues/3891 Unfortunately there hasn't been much traction on the XLA side to get this fixed","Thank you for the lightning fast reply! Good to know, I'm working around it for now. And sorry for the duplicate, I hadn't spotted that."
1442,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump zipp from 3.18.1 to 3.19.2)ï¼Œ å†…å®¹æ˜¯ (Bumps zipp from 3.18.1 to 3.19.2.  Changelog Sourced from zipp's changelog.  v3.19.2 No significant changes. v3.19.1 Bugfixes  Improved handling of malformed zip files. ( CC(example Errata))  v3.19.0 Features  Implement is_symlink. ( CC(add oss test instructions, fix conv grad bug))  v3.18.2 No significant changes.    Commits  c6a3339 Move Python compatibility concerns to the appropriate modules. c24fc57 Finalize 294a462 Ignore coverage misses in tests.compat.py39 aab60f4 ğŸ§â€â™€ï¸ Genuflect to the types. 59f852a Correct typo (incorrect letter used) when expacting alpharep root. 2a7a5bc Add test capturing expectation that a Path is a Traversable. 7d2b55b Update docstring to reference Traversable. 6d1cb72 Finalize fd604bd Merge pull request  CC(Canâ€™t import both jax and tensorflow (causes kernel restart)) from jaraco/bugfix/119malformedpaths c18417e Add news fragment. Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Bump zipp from 3.18.1 to 3.19.2,"Bumps zipp from 3.18.1 to 3.19.2.  Changelog Sourced from zipp's changelog.  v3.19.2 No significant changes. v3.19.1 Bugfixes  Improved handling of malformed zip files. ( CC(example Errata))  v3.19.0 Features  Implement is_symlink. ( CC(add oss test instructions, fix conv grad bug))  v3.18.2 No significant changes.    Commits  c6a3339 Move Python compatibility concerns to the appropriate modules. c24fc57 Finalize 294a462 Ignore coverage misses in tests.compat.py39 aab60f4 ğŸ§â€â™€ï¸ Genuflect to the types. 59f852a Correct typo (incorrect letter used) when expacting alpharep root. 2a7a5bc Add test capturing expectation that a Path is a Traversable. 7d2b55b Update docstring to reference Traversable. 6d1cb72 Finalize fd604bd Merge pull request  CC(Canâ€™t import both jax and tensorflow (causes kernel restart)) from jaraco/bugfix/119malformedpaths c18417e Add news fragment. Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`",2024-06-10T17:37:49Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/21778,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
484,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tree_all: add support for is_leaf)ï¼Œ å†…å®¹æ˜¯ (Fixes CC(Allow for `is_leaf` in tree_all) It looks like all other APIs that are based on `tree_leaves` already support and forward the `is_leaf` argument. For consistency, `tree_all` probably should as well. I also more broadly improved test coverage of these parameters.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,tree_all: add support for is_leaf,"Fixes CC(Allow for `is_leaf` in tree_all) It looks like all other APIs that are based on `tree_leaves` already support and forward the `is_leaf` argument. For consistency, `tree_all` probably should as well. I also more broadly improved test coverage of these parameters.",2024-06-10T16:46:44Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/21774
331,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Improve vmap out_axes error)ï¼Œ å†…å®¹æ˜¯ ( Changes Shows the path of invalid leaves when trying to return vectorized arrays through outputs marked as `out_axes=None`.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Improve vmap out_axes error, Changes Shows the path of invalid leaves when trying to return vectorized arrays through outputs marked as `out_axes=None`.,2024-06-10T13:04:37Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/21766, I fixed the remaining tests. There's still the TODO of trying to refactor the code such that we don't flatten twice but maybe we can do this in the future? This basic implementation is significantly more useful than what we had before.
525,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TPU Initialization Failed)ï¼Œ å†…å®¹æ˜¯ ( Description During TPU Research Program, I created a new TPUv48 VM and it's been working well.  This is not the first time I encountered this issue. Whenever I got this issue, I deleted the TPUvm and created a new TPU VM.  bash history   Error message    System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,TPU Initialization Failed," Description During TPU Research Program, I created a new TPUv48 VM and it's been working well.  This is not the first time I encountered this issue. Whenever I got this issue, I deleted the TPUvm and created a new TPU VM.  bash history   Error message    System info (python version, jaxlib version, accelerator, etc.) ",2024-06-10T08:56:47Z,bug,closed,0,0,https://github.com/jax-ml/jax/issues/21761
475,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Expand `device_put` benchmarks to run with different numbers of arrays and input types)ï¼Œ å†…å®¹æ˜¯ (Expand `device_put` benchmarks to run with different numbers of arrays and input types For the upcoming batching changes for `device_put`, it is useful to benchmark `device_put` with varying numbers of arrays.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Expand `device_put` benchmarks to run with different numbers of arrays and input types,"Expand `device_put` benchmarks to run with different numbers of arrays and input types For the upcoming batching changes for `device_put`, it is useful to benchmark `device_put` with varying numbers of arrays.",2024-06-09T18:21:49Z,,closed,0,0,https://github.com/jax-ml/jax/issues/21752
403,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Allow specifying initial forget gate bias for LSTMs.)ï¼Œ å†…å®¹æ˜¯ (Initializing the forget gate bias, typically to positive values, is a relatively common tweak for better LSTM training, so it would be nice to support this out of the box.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Allow specifying initial forget gate bias for LSTMs.,"Initializing the forget gate bias, typically to positive values, is a relatively common tweak for better LSTM training, so it would be nice to support this out of the box.",2024-06-09T10:39:39Z,,open,0,0,https://github.com/jax-ml/jax/issues/21749
1107,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Make `pxla.shard_arg` batch calls to `xc.copy_array_to_devices_with_sharding`)ï¼Œ å†…å®¹æ˜¯ (Make `pxla.shard_arg` batch calls to `xc.copy_array_to_devices_with_sharding` This CL changes `shard_arg_handlers` to be batched, in that it now receives a list of objects and a list of shardings and returns a list of array. This makes it possible to batch backend calls whenever it's beneficial to do so. Based on the above, the batched shard arg for arrays leverages the newly added `xla::ifrt::Client::CopyArrays()` (https://github.com/tensorflow/tensorflow/pull/69096) to make bulk copy cheaper in some backend implementations. Since `Client::CopyArrays()` requires batched arrays to have the same set of source/destination devices, `PyArray::BatchedCopyToDeviceWithSharding()` internally groups arrays by their source/destination devices and memory kinds. The grouping is pushed all the way to C++ for performance in case we have lots of arrays.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Make `pxla.shard_arg` batch calls to `xc.copy_array_to_devices_with_sharding`,"Make `pxla.shard_arg` batch calls to `xc.copy_array_to_devices_with_sharding` This CL changes `shard_arg_handlers` to be batched, in that it now receives a list of objects and a list of shardings and returns a list of array. This makes it possible to batch backend calls whenever it's beneficial to do so. Based on the above, the batched shard arg for arrays leverages the newly added `xla::ifrt::Client::CopyArrays()` (https://github.com/tensorflow/tensorflow/pull/69096) to make bulk copy cheaper in some backend implementations. Since `Client::CopyArrays()` requires batched arrays to have the same set of source/destination devices, `PyArray::BatchedCopyToDeviceWithSharding()` internally groups arrays by their source/destination devices and memory kinds. The grouping is pushed all the way to C++ for performance in case we have lots of arrays.",2024-06-07T16:38:45Z,,closed,0,0,https://github.com/jax-ml/jax/issues/21736
558,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TracerBoolConversionError when jitting jax.numpy.linalg.norm)ï¼Œ å†…å®¹æ˜¯ ( Description I am having some problems when trying to `jax.jit` the norm function in `jax.numpy`.  Ideally I would like to use the jitted version of norm for any ord that is a float and also for inf values. The minimal working example is:  results in    System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,TracerBoolConversionError when jitting jax.numpy.linalg.norm," Description I am having some problems when trying to `jax.jit` the norm function in `jax.numpy`.  Ideally I would like to use the jitted version of norm for any ord that is a float and also for inf values. The minimal working example is:  results in    System info (python version, jaxlib version, accelerator, etc.) ",2024-06-07T12:18:06Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/21730,"`ord` must be static in the current implementation of `norm`. You can try this instead:  JAX doesn't have any implementation of `norm` that accepts traced `ord` values, but it would be easy enough to create if you wish. Somethings like this should work for vector norms: ","It seems odd that some `jax.numpy` functions are decorated with`(jax.jit, static_argn{num,name}=...)` by default and some are not. Is there is a rule to decide which function is decorated by default? If there is no rule, then maybe new users first interacting with `jax.numpy` and `jax` transforms like `jit` should not be facing errors like this. I would say `jax.numpy` functions should be decorated by default. The downside of course is the _unexpected_ recompilation when changing some of the arguments. For this recompilation behavior, maybe it can be indicated in the docstring which arg{nums,names} are static.  WDYT?","> Is there is a rule to decide which function is decorated by default? Not a hard rule, but some NumPy APIs are incompatible with JIT, so we don't decorate those by default. For example `jnp.unique` cannot be JITcompiled by default, because it produces dynamicallyshaped outputs. > maybe it can be indicated in the docstring which arg{nums,names} are static. We're working on this as part of CC(Tracking issue: inline docstrings) ","> Not a hard rule, but some NumPy APIs are incompatible with JIT, To be more precise, I meant within the set of function that are compatible with jit, and can be decorated with `(jax.jit, static_argn{num,name}=...)`. IIUC, within `numpy` there are: 1.  Function that are jitcompatible and decorated with static argnum/argname. 2. Function that are jitcompatible and need to be decorated with static argnum/argname under `jit`.  3. Functions that are _not_ jitcompatible. My suggestion was to push 2) to be like 1). > For example jnp.unique cannot be JITcompiled by default, because it produces dynamicallyshaped outputs.  IIRC `numpy.unique` and single argument `numpy.where` among others have very clear docstring with a paragraph discussing `jit` and the dynamic shape issue. >We're working on this as part of https://github.com/google/jax/issues/21461 Looks great as always."
609,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add function to calculate concrete jax2tf output shapes from polymorphic input/output shapes.)ï¼Œ å†…å®¹æ˜¯ (Add function to calculate concrete jax2tf output shapes from polymorphic input/output shapes. This is helpful when trying to load a serialized TF model with polymorphic shapes and embedding in a JAX computation (see https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.mdcallingtensorflowfunctionsfromjax for details).)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add function to calculate concrete jax2tf output shapes from polymorphic input/output shapes.,Add function to calculate concrete jax2tf output shapes from polymorphic input/output shapes. This is helpful when trying to load a serialized TF model with polymorphic shapes and embedding in a JAX computation (see https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.mdcallingtensorflowfunctionsfromjax for details).,2024-06-05T13:40:50Z,,closed,0,1,https://github.com/jax-ml/jax/issues/21665,Closing Copybara created PR due to inactivity
566,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Improve tensorstore I/O efficiency)ï¼Œ å†…å®¹æ˜¯ (Improve tensorstore I/O efficiency Previously, when writing the OCDBT format, the manifest and root B+tree node could be redundantly written multiple times depending on timing. With this change, the manifest and root B+tree node are always written only once. Additionally, source data was previously redundantly copied into the TensorStore chunk cache.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Improve tensorstore I/O efficiency,"Improve tensorstore I/O efficiency Previously, when writing the OCDBT format, the manifest and root B+tree node could be redundantly written multiple times depending on timing. With this change, the manifest and root B+tree node are always written only once. Additionally, source data was previously redundantly copied into the TensorStore chunk cache.",2024-06-05T05:23:43Z,,closed,0,0,https://github.com/jax-ml/jax/issues/21654
1453,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Stochastic pmap lowering behavior in tests)ï¼Œ å†…å®¹æ˜¯ ( Description I'm working on debugging a failure that we're seeing in our daily open source build. This test is consistently failing at `main` when we run the full test suite with 4 GPUs using `pytest` with the error:  This failure can't be reproduced with a single test and it only appeared last week, although I think the underlying issue must be older than that. I thought it might be some shared state, but I've isolated the issue a little bit. When logging the compiles, I find that the three lowering operations are: 1. `pmap_f`, 2. `pmap_backwards_pass` (transpose of `f`), and 3. `jit__multi_slice` (defined here). When I run just this test, I find that `jit__multi_slice` should be lowered during the ""warmup"" call here: https://github.com/google/jax/blob/65063b7ac30635e76d2d692fb976a11f136e32bf/tests/pmap_test.pyL2079 but, when the test fails, it's not actually being compiled until here: https://github.com/google/jax/blob/65063b7ac30635e76d2d692fb976a11f136e32bf/tests/pmap_test.pyL2083 It seems like maybe there's some kind of async race condition or something here, but adding a `block_until_ready` didn't fix the issue. I wanted to raise this here to see if anyone had thoughts for where this might be coming from, )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Stochastic pmap lowering behavior in tests," Description I'm working on debugging a failure that we're seeing in our daily open source build. This test is consistently failing at `main` when we run the full test suite with 4 GPUs using `pytest` with the error:  This failure can't be reproduced with a single test and it only appeared last week, although I think the underlying issue must be older than that. I thought it might be some shared state, but I've isolated the issue a little bit. When logging the compiles, I find that the three lowering operations are: 1. `pmap_f`, 2. `pmap_backwards_pass` (transpose of `f`), and 3. `jit__multi_slice` (defined here). When I run just this test, I find that `jit__multi_slice` should be lowered during the ""warmup"" call here: https://github.com/google/jax/blob/65063b7ac30635e76d2d692fb976a11f136e32bf/tests/pmap_test.pyL2079 but, when the test fails, it's not actually being compiled until here: https://github.com/google/jax/blob/65063b7ac30635e76d2d692fb976a11f136e32bf/tests/pmap_test.pyL2083 It seems like maybe there's some kind of async race condition or something here, but adding a `block_until_ready` didn't fix the issue. I wanted to raise this here to see if anyone had thoughts for where this might be coming from, ",2024-06-04T19:20:05Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/21643,Maybe you need to do `jax.grad` as the warmup?," â€” Thanks and good question! Nominally the idea is that that first block is specifically testing the number of lowering operations required for the grad (or really vjp). But, I chatted with  and we decided that perhaps the best solution for now is to just skip this first check (this is `pmap` after all), effectively doing just as you suggest!","Thank you for raising this issue and providing a detailed description. Based on your findings, hereâ€™s a proposed solution to address the stochastic pmap lowering behavior: Synchronize Operations: Ensure that all JAX operations are properly synchronized using `block_until_ready`. This ensures that operations are completed before moving on to the next step.  2. Simplify the Test: As per the discussion above, there are possibly 3 ways of simplifying the test. We could focus on core compilation checks, verify compilation count directly, and isolate and test forward and backward passes separately.  These solutions should help ensure that the configuration is correctly set within the threadlocal state and avoid the issues seen with asynchronous operations. If there are any further details or test cases needed, please let me know. I am happy to assist with the implementation and submit a PR."
681,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jaxlib.xla_extension.XlaRuntimeError: INTERNAL: ptxas exited with non-zero error code 11)ï¼Œ å†…å®¹æ˜¯ ( Description I am consistently getting an error out of a compilcated code  after having installed jax/jaxlib with on a clean environment.   I also made sure that in my `LD_LIBRARY_PATH` nothing is set. Is there some way to debug this in any way?  System info (python version, jaxlib version, accelerator, etc.)  (the Nvidia SMI that is being picked up is from the cluster installation, but cuda is not in my path )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,jaxlib.xla_extension.XlaRuntimeError: INTERNAL: ptxas exited with non-zero error code 11," Description I am consistently getting an error out of a compilcated code  after having installed jax/jaxlib with on a clean environment.   I also made sure that in my `LD_LIBRARY_PATH` nothing is set. Is there some way to debug this in any way?  System info (python version, jaxlib version, accelerator, etc.)  (the Nvidia SMI that is being picked up is from the cluster installation, but cuda is not in my path ",2024-06-04T09:22:07Z,bug,open,1,5,https://github.com/jax-ml/jax/issues/21621,This is the error I get. I can also share a reproducer if wanted. ,"Thanks for raising this. Yes, can you share a reproducer? The error talks about filesystem issues (""If the error message indicates that a file could not be written, please verify that sufficient filesystem space is provided""). Could there be a permissions issue?","I also got this error, what is the fix? ",I have encountered the same issue like this and finally recognize that jax version is not compatible with cuda version in my laptop. Check again about cuda version and maybe you can find the solution.,"I have also encountered a similar issue on a cluster.  At first, I thought this might be due to file permissions since this was on a cluster and I did not have certain permissions. But I get a different error related to versioning. I am not sure if it is referring to some CUDA library (cuDNN?), or PTXAS (see versions below).  Is there any way to debug this? I had an older version of JAX running fine (0.4.35) on the default Python environment that the cluster provides by installing JAX in user mode.  The error message says the following and it not clear what the .version is supposed to be:   The CUDA version:   The JAX version is:  The Python version is 3.11.7. "
1442,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump zipp from 3.18.1 to 3.19.1)ï¼Œ å†…å®¹æ˜¯ (Bumps zipp from 3.18.1 to 3.19.1.  Changelog Sourced from zipp's changelog.  v3.19.1 Bugfixes  Improved handling of malformed zip files. ( CC(example Errata))  v3.19.0 Features  Implement is_symlink. ( CC(add oss test instructions, fix conv grad bug))  v3.18.2 No significant changes.    Commits  6d1cb72 Finalize fd604bd Merge pull request  CC(Canâ€™t import both jax and tensorflow (causes kernel restart)) from jaraco/bugfix/119malformedpaths c18417e Add news fragment. 58115d2 Employ SanitizedNames in CompleteDirs. Fixes broken test. 564fcc1 Add SanitizedNames mixin. 79a309f Add some assertions about malformed paths. 2d015c2 Merge https://github.com/jaraco/skeleton a595a0f Rename extras to align with core metadata spec. 608f90a Finalize 3a22d72 Merge pull request  CC(add tensordot) from jaraco/feature/issymlink Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabota)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Bump zipp from 3.18.1 to 3.19.1,"Bumps zipp from 3.18.1 to 3.19.1.  Changelog Sourced from zipp's changelog.  v3.19.1 Bugfixes  Improved handling of malformed zip files. ( CC(example Errata))  v3.19.0 Features  Implement is_symlink. ( CC(add oss test instructions, fix conv grad bug))  v3.18.2 No significant changes.    Commits  6d1cb72 Finalize fd604bd Merge pull request  CC(Canâ€™t import both jax and tensorflow (causes kernel restart)) from jaraco/bugfix/119malformedpaths c18417e Add news fragment. 58115d2 Employ SanitizedNames in CompleteDirs. Fixes broken test. 564fcc1 Add SanitizedNames mixin. 79a309f Add some assertions about malformed paths. 2d015c2 Merge https://github.com/jaraco/skeleton a595a0f Rename extras to align with core metadata spec. 608f90a Finalize 3a22d72 Merge pull request  CC(add tensordot) from jaraco/feature/issymlink Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabota",2024-06-03T17:08:40Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/21603,Superseded by CC(Bump zipp from 3.18.1 to 3.19.2).
598,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`input_shardings` returned from the compiled executable should match the in_tree pre DCE. Otherwise calling it raises an error saying that the length of pre dce in_tree and post dce input_shardings don't match)ï¼Œ å†…å®¹æ˜¯ (`input_shardings` returned from the compiled executable should match the in_tree pre DCE. Otherwise calling it raises an error saying that the length of pre dce in_tree and post dce input_shardings don't match)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,`input_shardings` returned from the compiled executable should match the in_tree pre DCE. Otherwise calling it raises an error saying that the length of pre dce in_tree and post dce input_shardings don't match,`input_shardings` returned from the compiled executable should match the in_tree pre DCE. Otherwise calling it raises an error saying that the length of pre dce in_tree and post dce input_shardings don't match,2024-06-03T17:08:40Z,,closed,0,0,https://github.com/jax-ml/jax/issues/21602
599,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Mosaic GPU] Make the profiler warpgroup aware)ï¼Œ å†…å®¹æ˜¯ ([Mosaic GPU] Make the profiler warpgroup aware Instead of creating one timeline per block, we now create one timeline per warpgroup. This is especially useful when warpgroups differ in their execution traces. Also, instead of specifying the total capacity, the profiler now accepts a number specifying a number of entries perblock. This makes it easier to find a good size.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[Mosaic GPU] Make the profiler warpgroup aware,"[Mosaic GPU] Make the profiler warpgroup aware Instead of creating one timeline per block, we now create one timeline per warpgroup. This is especially useful when warpgroups differ in their execution traces. Also, instead of specifying the total capacity, the profiler now accepts a number specifying a number of entries perblock. This makes it easier to find a good size.",2024-06-03T14:29:53Z,,closed,0,0,https://github.com/jax-ml/jax/issues/21599
1278,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Custom partitioning error in fused_attention_stablehlo)ï¼Œ å†…å®¹æ˜¯ ( Description I'm trying to use the recently added `jax._src.cudnn.fused_attention_stablehlo`, but I'm getting the following error from the `infer_sharding_from_operands` function:  Clearly, the function is expecting a `NamedSharding` but getting a `PositionalSharding` instead. This is strange, because I am only using `NamedSharding` in my code. I've even tried adding a `jax.lax.with_sharding_constraing(inputs, NamedSharding(...))` right before calling `fused_attention_stablehlo.dot_product_attention`, to no avail. I inserted a breakpoint into the `infer_sharding_from_operands` function and found that the `mesh` object being passed in is totally empty (`Mesh(device_ids=[], axis_names=())`) and all of the arguments have `PositionalSharding`s. I'm not sure this is the fault of `fused_attention_stablehlo`, since in the custom_partitioning docs, it does suggest calling `.spec` directly on the sharding, suggesting it's safe to assume it will always be a `NamedSharding`.  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Custom partitioning error in fused_attention_stablehlo," Description I'm trying to use the recently added `jax._src.cudnn.fused_attention_stablehlo`, but I'm getting the following error from the `infer_sharding_from_operands` function:  Clearly, the function is expecting a `NamedSharding` but getting a `PositionalSharding` instead. This is strange, because I am only using `NamedSharding` in my code. I've even tried adding a `jax.lax.with_sharding_constraing(inputs, NamedSharding(...))` right before calling `fused_attention_stablehlo.dot_product_attention`, to no avail. I inserted a breakpoint into the `infer_sharding_from_operands` function and found that the `mesh` object being passed in is totally empty (`Mesh(device_ids=[], axis_names=())`) and all of the arguments have `PositionalSharding`s. I'm not sure this is the fault of `fused_attention_stablehlo`, since in the custom_partitioning docs, it does suggest calling `.spec` directly on the sharding, suggesting it's safe to assume it will always be a `NamedSharding`.  System info (python version, jaxlib version, accelerator, etc.) ",2024-06-02T06:43:54Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/21584," can you have a look, please?",Can you try running your computation under `with mesh:` context manager? That should fix this.,"Thanks, that worked!",Can we keep that open as 1. It's not documented so I ran into the exact same problem 2. It's not a desirable long term solution (?)
1153,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(str(PyTreeDef) identical for two PyTreeDefs, but assert with allclose fails)ï¼Œ å†…å®¹æ˜¯ ( Description I'm trying to understand a bug  in optimistix, and came across this behaviour that is unintuitive to me:   The PyTreeDef of two jacobians computed using `jax.linearize` is not the same, but the strings of the PyTreeDefs are the same.  Since the tree structure is what I want to examine for debugging purposes, this should not happen. Edit: fixed typo in comment. Here is an MWE that demonstrates the error:   System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.28 jaxlib: 0.4.28 numpy:  1.26.4 python: 3.11.8 (v3.11.8:db85d51d3e, Feb  6 2024, 18:02:37) [Clang 13.0.0 (clang1300.0.29.30)] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Darwin', node='bsmbpas0019', release='23.3.0', version='Darwin Kernel Version 23.3.0: Wed Dec 20 21:33:31 PST 2023; root:xnu10002.81.5~7/RELEASE_ARM64_T8112', machine='arm64'))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"str(PyTreeDef) identical for two PyTreeDefs, but assert with allclose fails"," Description I'm trying to understand a bug  in optimistix, and came across this behaviour that is unintuitive to me:   The PyTreeDef of two jacobians computed using `jax.linearize` is not the same, but the strings of the PyTreeDefs are the same.  Since the tree structure is what I want to examine for debugging purposes, this should not happen. Edit: fixed typo in comment. Here is an MWE that demonstrates the error:   System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.28 jaxlib: 0.4.28 numpy:  1.26.4 python: 3.11.8 (v3.11.8:db85d51d3e, Feb  6 2024, 18:02:37) [Clang 13.0.0 (clang1300.0.29.30)] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 platform: uname_result(system='Darwin', node='bsmbpas0019', release='23.3.0', version='Darwin Kernel Version 23.3.0: Wed Dec 20 21:33:31 PST 2023; root:xnu10002.81.5~7/RELEASE_ARM64_T8112', machine='arm64')",2024-06-01T21:52:23Z,bug,closed,0,11,https://github.com/jax-ml/jax/issues/21581,"It looks like this is happening because we are using `functools.partial` internally, and `partial` objects do not implement `__eq__`:  On top of that, one of the arguments is a `Jaxpr` which too does not implement `__eq__` and is thus comparable by reference.",I removed some unnecessary definitions/calls from the repro: ,You can simplify the repro even further:  This makes clear that the problem has nothing to do with the function being passed to `linearize`.,"> You can simplify the repro even further: > This makes clear that the problem has nothing to do with the function being passed to `linearize`. Fascinating! And then I really am out of leads, I was sure it must have something to do with the function passed ğŸ˜ƒ ","FWIW I think this behaviour is acceptable in JAX. A jaxpr is just JAX's internal representation of a function, and Python itself does the same thing:  Moreover I get a bit antsy about the idea of comparing jaxprs for equality. It's very common for jaxprs to become truly gigantic, and iterating over them to test for equality seems like a compiletime footgun.","Good point.  I went in this direction because I thought it might be at the root of why we can't batch in `GaussNewton`, but I'm not sure that is the case anymore, since I can batch over the output of `jax.linearize`.   Why does the difference now show up in the string of a PyTreeDef though? Those are equal. ",> Why does the difference now show up in the string of a PyTreeDef though? Those are equal. I think this is because `PyTreeDef.__str__` does not include the auxiliary data stored in the nodes.,What would that auxiliary data contain? Can I conclude that errors such as the one above do not come from the pytree definition if the strings are the same? I currently only use these as debugging tools.,"> What would that auxiliary data contain? It depends on the type and its flattening rule. > Can I conclude that errors such as the one above do not come from the pytree definition if the strings are the same? Sorry, I'm not sure I follow. Which errors are you referring to?","Thanks for the discussion, all! To summarize: equality of string prettyprints does not imply object equality (for treedefs, and likely for lots of Python classes).","> Thanks for the discussion, all! >  > To summarize: equality of string prettyprints does not imply object equality (for treedefs, and likely for lots of Python classes). This was the question I had :) "
547,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([export] Fix handling of a function with no name)ï¼Œ å†…å®¹æ˜¯ (A user reported an error when trying to export a function that has a ""lower"" attribute (to impersonate a jitted function) but does not have a ""__name__"" attribute. The solution is to use the default name """". While I was at it I have added a `util.fun_name` to get the name of a Callable, and I use it in several places.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[export] Fix handling of a function with no name,"A user reported an error when trying to export a function that has a ""lower"" attribute (to impersonate a jitted function) but does not have a ""__name__"" attribute. The solution is to use the default name """". While I was at it I have added a `util.fun_name` to get the name of a Callable, and I use it in several places.",2024-06-01T02:56:21Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/21572,This has been submitted already as https://github.com/google/jax/commit/be1e40dc2e1777d83b870afa31e178123f2a1366 (due to a copybara glitch)
422,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(pure_callback is not passing `np.ndarray` typed inputs)ï¼Œ å†…å®¹æ˜¯ ( Description I updated JAX and am getting inputs to callback functions coming in as `jax.ArrayImpl` and not `np.ndarray`.   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,pure_callback is not passing `np.ndarray` typed inputs," Description I updated JAX and am getting inputs to callback functions coming in as `jax.ArrayImpl` and not `np.ndarray`.   System info (python version, jaxlib version, accelerator, etc.) ",2024-05-30T19:25:34Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/21526,"Yes, that was a deliberate change we made. Is there a problem or an issue? The current description does not seem to specify the issue.  ","The documentation says it should be `np.ndarray`. I noticed when it crashing all my C bindings which don't understand `ArrayImpl`. Easy fix though. I personally don't care, as long as CC(Provide output buffer for pure_callback result) gets implemented :)",Which documentation? I couldn't find any mention of it here: https://jax.readthedocs.io/en/latest/_autosummary/jax.pure_callback.html,"Where do the docs say `np.ndarray`? I couldn't track that down. The change was listed in the changelog here, and the current docs say ""JAX arrays"" AFAICT.",The source code: !Screenshot from 20240530 213500,"Maybe you are not looking at the main branch (https://github.com/google/jax/blob/51e743139b2bbe5146e257610eca31a05f277f83/jax/_src/callback.pyL272)? But anyways, this is WAI.  I am going to close the issue. Please reopen if this is causing major problems for you :)","I think the documentation update was done after we cut the release, unfortunately. So, the sourcelevel documentation is indeed misleading. Sorry about that."
377,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([mosaic:gpu] Minor cleanup in `FragmentedArray.transfer_tile`.)ï¼Œ å†…å®¹æ˜¯ ([mosaic:gpu] Minor cleanup in `FragmentedArray.transfer_tile`.  Remove redundant line.  Use `ConstantOp.create_index`.  Use `BoolAttr`.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[mosaic:gpu] Minor cleanup in `FragmentedArray.transfer_tile`.,[mosaic:gpu] Minor cleanup in `FragmentedArray.transfer_tile`.  Remove redundant line.  Use `ConstantOp.create_index`.  Use `BoolAttr`.,2024-05-30T11:41:42Z,,closed,0,0,https://github.com/jax-ml/jax/issues/21510
280,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(logging_test: avoid modifying global config state)ï¼Œ å†…å®¹æ˜¯ (Related to https://github.com/google/jax/pull/21489)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,logging_test: avoid modifying global config state,Related to https://github.com/google/jax/pull/21489,2024-05-29T20:39:50Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/21497
1012,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Cryptic error message when running pallas kernel on TPU )ï¼Œ å†…å®¹æ˜¯ ( Description I have a pallas kernel that's dying with a cryptic CHECK failure:   I don't have a useful stack trace here it's just the standard Google3 CHECK failure stuff that's pretty useless (see !screenshot). It happens in my call to pallas_call on a v5p8. It traces my kernel fine but then dies before returning from pallas_call, which I assume means this is some sort of compilation error? It happens when interpret is set to both true and false. The code runs without issue on CPU. The error does not come up when I remove the `custom_vjp` decorator and only run the forward pass. I have a repro on gist.  System info (python version, jaxlib version, accelerator, etc.) System info for CPU env (where the code succeeds):  System info for TPU env (where the code fails): )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Cryptic error message when running pallas kernel on TPU ," Description I have a pallas kernel that's dying with a cryptic CHECK failure:   I don't have a useful stack trace here it's just the standard Google3 CHECK failure stuff that's pretty useless (see !screenshot). It happens in my call to pallas_call on a v5p8. It traces my kernel fine but then dies before returning from pallas_call, which I assume means this is some sort of compilation error? It happens when interpret is set to both true and false. The code runs without issue on CPU. The error does not come up when I remove the `custom_vjp` decorator and only run the forward pass. I have a repro on gist.  System info (python version, jaxlib version, accelerator, etc.) System info for CPU env (where the code succeeds):  System info for TPU env (where the code fails): ",2024-05-29T17:47:09Z,bug pallas,closed,0,7,https://github.com/jax-ml/jax/issues/21488,"My guess is that it's the `mask` operand that is causing the problem. If I understand correctly, it's an array of `jnp.bool`, but this type should not be supported at the kernel boundary. I think the best workaround would be to cast the mask to int8/int32 and then do `mask != 0` inside the kernel to recover it in the boolean form. Ultimately there are two things to fix here: (1) make Pallas more picky about input operand types and (2) add support for passing booleans to kernels.","Ah, yes, that fixed it. Thanks! Is it possible to have this be checked on CPU as well? I find it really confusing when there's such different behaviour between CPU and TPU. On Thu, May 30 2024 at 02:24, Adam Paszke ***@***.***> wrote: > My guess is that it's the mask operand that is causing the problem. If I > understand correctly, it's an array of jnp.bool, but this type should not > be supported at the kernel boundary. I think the best workaround would be > to cast the mask to int8/int32 and then do mask != 0 inside the kernel to > recover it in the boolean form. > > Ultimately there are two things to fix here: (1) make Pallas more picky > about input operand types and (2) add support for passing booleans to > kernels. > > â€” > Reply to this email directly, view it on GitHub > , or > unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >","I think on CPU we only simulate the custom call, which you can get on a TPU if you pass in `interpret=True`. But note that then you won't be actually generating a kernel but it will expand to a soup of loopy HLOs. Either way, we should fix bool support.","Ah, ty, I misunderstood and thought that the error was still happening with `interpret=True`, you're right, it doesn't. On Fri, May 31, 2024 at 3:15 AM, Adam Paszke ***@***.***> wrote: > I think on CPU we only simulate the custom call, which you can get on a > TPU if you pass in interpret=True. But note that then you won't be > actually generating a kernel but it will expand to a soup of loopy HLOs. > Either way, we should fix bool support. > > â€” > Reply to this email directly, view it on GitHub > , or > unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >",I'm currently working on a fix to cast bools memrefs to ints automatically so users don't have to worry about this detail.,Circling back on this: I added an updated so that this is supported for scalars (https://github.com/google/jax/pull/22464). However it seems like the vector case is a bit more tricky (due to vector layouts in Mosaic) and will take some more time.,Vector boolean loads/stores are now implemented: https://github.com/google/jax/pull/23044.
1134,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(CanÂ´t get jax profiling to work)ï¼Œ å†…å®¹æ˜¯ ( Description I'm trying to follow the documentation on profiling and am stuck when trying to evaluate the traces with tensorboard, running the following code:   which is shown in the docs.  Now, trying to view the trace with tensorboard:   I always get the warning `No step marker observed and hence the step time is unknown. This may happen if (1) training steps are not instrumented (e.g., if you are not using Keras) or (2) the profiling duration is shorter than the step time. For (1), you need to add step instrumentation; for (2), you may try to profile longer.` No profiling data is available or shown. ` The code itself produces only the following warning:   which I am not sure is relevant here.  What am I doing wrong? How could I go about troubleshooting this? thanks in advance   System info (python version, jaxlib version, accelerator, etc.)  **accelerator:** GPU  EDIT: correct highlighting of error messages)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,CanÂ´t get jax profiling to work," Description I'm trying to follow the documentation on profiling and am stuck when trying to evaluate the traces with tensorboard, running the following code:   which is shown in the docs.  Now, trying to view the trace with tensorboard:   I always get the warning `No step marker observed and hence the step time is unknown. This may happen if (1) training steps are not instrumented (e.g., if you are not using Keras) or (2) the profiling duration is shorter than the step time. For (1), you need to add step instrumentation; for (2), you may try to profile longer.` No profiling data is available or shown. ` The code itself produces only the following warning:   which I am not sure is relevant here.  What am I doing wrong? How could I go about troubleshooting this? thanks in advance   System info (python version, jaxlib version, accelerator, etc.)  **accelerator:** GPU  EDIT: correct highlighting of error messages",2024-05-29T12:42:25Z,bug,open,3,5,https://github.com/jax-ml/jax/issues/21483,Do you see meaningful information under the trace viewer tool? I'd start there rather than the overview tool.,"Thank you for your answer. I do get output from the trace viewer, but I'm still confused by the warning message, since it seems to indicate that profiling didnÂ´t work. Is there any documentation on this? is it intentional or unavoidable? ","Hello! I have met the same problem while using tensorboard profiling, have you resolved this issue?","what I ultimately did was using nvidia insight directly, which at least gives you some indication of what's going on. See here:  https://github.com/NVIDIA/JAXToolbox/blob/main/docs/profiling.md",The overview tool in tensorboard isn't really designed to work well with JAX. I'd recommend mostly ignoring it at the moment and looking at the trace viewer.
638,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Unexpected Assertion error when using lax.with_sharding_constraint)ï¼Œ å†…å®¹æ˜¯ ( Description Hi, I was facing an error when trying to use data paralellism with equinox and opened an issue there: equinox issue CC(np.trace is broken). Turns out the issue is not related with equinox but with `lax.with_sharding_constraint` so I am opening an issue here. Here is the MWE:  And here is the error being thrown:   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Unexpected Assertion error when using lax.with_sharding_constraint," Description Hi, I was facing an error when trying to use data paralellism with equinox and opened an issue there: equinox issue CC(np.trace is broken). Turns out the issue is not related with equinox but with `lax.with_sharding_constraint` so I am opening an issue here. Here is the MWE:  And here is the error being thrown:   System info (python version, jaxlib version, accelerator, etc.) ",2024-05-29T09:29:15Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/21480,"You need to reshape your sharding to match the length of the shape's ndim:  This might be a might inconvenient though, so I would suggest using NamedSharding instead:   If you want to shard only the first dimension then your PartitionSpec can be `P('x')`.",We should probably raise a better error message when this happens. Wdyt ? ,"Sure, feel free to send a CL :) You would need to implement `is_compatible_aval` method on PositionalSharding to raise a better error message.",Thanks  !  Indeed a better error message would be useful :)
320,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Remove warning logs for primary_host/remote_storage ""incompatibility"".)ï¼Œ å†…å®¹æ˜¯ (Remove warning logs for primary_host/remote_storage ""incompatibility"".)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,"Remove warning logs for primary_host/remote_storage ""incompatibility"".","Remove warning logs for primary_host/remote_storage ""incompatibility"".",2024-05-28T20:49:54Z,,closed,0,0,https://github.com/jax-ml/jax/issues/21470
1071,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(example path)ï¼Œ å†…å®¹æ˜¯ ( Description I am installing jax and running a simple example for the first time.  I am on and have installed the nightly JAX via pip3. (This error occurs if I also install stable jax via pip3.) I am running an uptodate Ubuntu 22.04.4 LTS using cuda12.  Modifying the example appears to resolve the issue:  The jax git repo I am working with is a fresh clone from this morning:   System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.29.dev20240528+ff3db9b3a jaxlib: 0.4.29.dev20240528 numpy:  1.26.4 python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] jax.devices (1 total, 1 local): [cuda(id=0)] process_count: 1 platform: uname_result(system='Linux', node='c', release='6.5.035generic', version=' CC(CUDA90 and py3 )~22.04.1Ubuntu SMP PREEMPT_DYNAMIC Tue May  7 09:00:52 UTC 2', machine='x86_64') $ nvidiasmi Tue May 28 09:08:46 2024        ++  ++)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,example path," Description I am installing jax and running a simple example for the first time.  I am on and have installed the nightly JAX via pip3. (This error occurs if I also install stable jax via pip3.) I am running an uptodate Ubuntu 22.04.4 LTS using cuda12.  Modifying the example appears to resolve the issue:  The jax git repo I am working with is a fresh clone from this morning:   System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.29.dev20240528+ff3db9b3a jaxlib: 0.4.29.dev20240528 numpy:  1.26.4 python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] jax.devices (1 total, 1 local): [cuda(id=0)] process_count: 1 platform: uname_result(system='Linux', node='c', release='6.5.035generic', version=' CC(CUDA90 and py3 )~22.04.1Ubuntu SMP PREEMPT_DYNAMIC Tue May  7 09:00:52 UTC 2', machine='x86_64') $ nvidiasmi Tue May 28 09:08:46 2024        ++  ++",2024-05-28T16:09:29Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/21459,"This has to do with the relative position of the two files.  As they are right now `datasets.py` and `mnist_classifier_fromscratch.py` are under the same folder. I might be missing something here but if you move the `mnist_classifier_fromscratch.py` outside of the `examples` folder, it works as intended. If you want to keep the two files in the same folder then in `mnist_classifier_fromscratch.py` change;  to   Something is telling me that I am missing something here since all examples that use the `datasets.py` are importing it with the first way and the author was able to run them just fine as they are."
649,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([export] Fix calling under pmap of exported computation with polymorphic shapes)ï¼Œ å†…å®¹æ˜¯ (If we call a computation with shape polymorphism under pmap we must refine the shapes before we compile. We follow the same pattern for `UnloadedPmapExecutable` as for `UnloadedMeshExecutable`: we store the `shape_poly_state` from the `LoweringResult` into the `compile_args` and we call `refine_polymorphic_shapes`. Without this fix we may end up trying to compile HLO with dynamic shapes.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[export] Fix calling under pmap of exported computation with polymorphic shapes,If we call a computation with shape polymorphism under pmap we must refine the shapes before we compile. We follow the same pattern for `UnloadedPmapExecutable` as for `UnloadedMeshExecutable`: we store the `shape_poly_state` from the `LoweringResult` into the `compile_args` and we call `refine_polymorphic_shapes`. Without this fix we may end up trying to compile HLO with dynamic shapes.,2024-05-28T13:19:31Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/21451
946,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(D2H (gpu -> cpu) transfer via `device_put` is very  slow)ï¼Œ å†…å®¹æ˜¯ ( Description For D2H (gpu to cpu) transfer, `jax.device_put` has very low throughput. `device_put` yields ~2.7GB/s transfer speed; in contrast, a very simple CUDA program yields ~25GB/s. Is there an alternative approach in Jax here that I'm missing?  I tried the following two approaches as well, both performed at least as poorly as `jax.device_put`: * Calling XLA directly (via `xc.batched_device_put` as detailed in https://github.com/google/jax/issues/16905issue1829138543) * Calling a dummy JIT compiled function for CPU that returns identity Minimal Jax example:  with output  And here is a simple CUDA program for copying:  With output:   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,D2H (gpu -> cpu) transfer via `device_put` is very  slow," Description For D2H (gpu to cpu) transfer, `jax.device_put` has very low throughput. `device_put` yields ~2.7GB/s transfer speed; in contrast, a very simple CUDA program yields ~25GB/s. Is there an alternative approach in Jax here that I'm missing?  I tried the following two approaches as well, both performed at least as poorly as `jax.device_put`: * Calling XLA directly (via `xc.batched_device_put` as detailed in https://github.com/google/jax/issues/16905issue1829138543) * Calling a dummy JIT compiled function for CPU that returns identity Minimal Jax example:  with output  And here is a simple CUDA program for copying:  With output:   System info (python version, jaxlib version, accelerator, etc.) ",2024-05-27T02:19:18Z,bug NVIDIA GPU,open,0,7,https://github.com/jax-ml/jax/issues/21438,"I suspect that the difference is that the destination of your transfer is in CUDA pinned host memory (`cudaMallocHost`), to which you can DMA directly from the device. JAX is transferring to unpinned memory. If you allocate the target buffer with `malloc` in your CUDA benchmark, how do the two compare? (We are actually working on adding support for pinned host memory allocations to JAX.)","Thanks for the quick response! With `malloc` I get 6.3 GB/s throughput vs 2.7 GB/s in Jax.  Even if there is not official support, is there an easy hack to get Jax to allocate CUDA pinned memory? This problem is very important in my application + I'm only using my CPU as a staging area for GPU operations, so I am happy to have Jax only use CUDA pinned memory.","Right now, you can't hack to get pinned_host working. The implementation is missing. We are working on it.","I might be able to get you the 6.3GB/s without much trouble, though, if that's helpful.","Another workaround for the moment would be to use DLPack to exchange the onGPU array with another library that already supports pinned host memory (e.g., cupy) and use that library to do the transfer.",Thank you for the suggestions  bridging to cupy worked!,"Any progress on the missing pinned host implementation? Is the following related: ""TODO(b/238441608): Use pinned memory here to speed up the transfer."" from py_client_gpu.cc? I see some code under gpu_transfer_manager referring to ""Check out pinned memory for each buffer we want to copy"" under GpuTransferManager::ReadDynamicShapes. Do you have a design about H2D D2H D2D memcpy ? Thank you."
1172,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Low calculation performance compared to autograd elementwise_grad)ï¼Œ å†…å®¹æ˜¯ ( Description I use autograd to calculate partial derivatives of functions of two variables (x, y). Due to the end of support for autograd, I'm trying to get the same results using jax. These functions have the form: $$\nabla^4 w = \cfrac{\partial^4 w}{\partial x^4} + 2\cfrac{\partial^4 w}{\partial x^2\partial y^2} + \cfrac{\partial^4 w}{\partial y^4}$$ where $w = w(x,y)$. I use similar functions obtained by automatic differentiation in other parts of the program as wrappers, and then to obtain the final results I substitute the values â€‹â€‹of the NumPy arrays. I haven't found a way to port this type of twovariable functions from autograd to jax with similar performance.  Examples:  autograd (ex1.py)    jax (ex2.py)   The program using jax is almost 9x slower than the version using autograd. In more complicated programs the differences are much greater.  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Low calculation performance compared to autograd elementwise_grad," Description I use autograd to calculate partial derivatives of functions of two variables (x, y). Due to the end of support for autograd, I'm trying to get the same results using jax. These functions have the form: $$\nabla^4 w = \cfrac{\partial^4 w}{\partial x^4} + 2\cfrac{\partial^4 w}{\partial x^2\partial y^2} + \cfrac{\partial^4 w}{\partial y^4}$$ where $w = w(x,y)$. I use similar functions obtained by automatic differentiation in other parts of the program as wrappers, and then to obtain the final results I substitute the values â€‹â€‹of the NumPy arrays. I haven't found a way to port this type of twovariable functions from autograd to jax with similar performance.  Examples:  autograd (ex1.py)    jax (ex2.py)   The program using jax is almost 9x slower than the version using autograd. In more complicated programs the differences are much greater.  System info (python version, jaxlib version, accelerator, etc.) ",2024-05-26T10:04:37Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/21436,(answered in CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®))
1348,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`unsafe_rbg` fold in function is insensitive to the order data is folded in.)ï¼Œ å†…å®¹æ˜¯ ( Description The `unsafe_rbg` fold_in function is not sensitive to the order in which data is folded in. The underlying cause is that `unsafe_rbg` derives its key based on `key ^ rbg(data)`, but XOR is a commutative operation. So if a user folds in two values the order will not change the result since `key ^ rbg(1) ^ rbg(2) == key ^ rbg(2) ^ rbg(1)`. Reproducing example:  Result:  Notice in the `unsafe_rbg` case the two derived keys are the same. Note that `rbg` uses the key derivation logic from threefry which does not have this issue.  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.29 jaxlib: 0.4.29 numpy:  1.26.3 python: 3.11.8 (stable, redacted, redacted) [Clang google3trunk (fc57f88f007497a4ead0ec8607ac66e1847b02d6)] jax.devices (1 total, 1 local): [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)] process_count: 1 platform: uname_result(system='Linux', node='8f069381872f0396766a8253c8.borgtask.google.com', release='5.10.0smp1101.34.0.0', version=' CC(Python 3 compatibility issues) [v5.10.01101.34.0.0] SMP ', machine='x86_64'))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,`unsafe_rbg` fold in function is insensitive to the order data is folded in.," Description The `unsafe_rbg` fold_in function is not sensitive to the order in which data is folded in. The underlying cause is that `unsafe_rbg` derives its key based on `key ^ rbg(data)`, but XOR is a commutative operation. So if a user folds in two values the order will not change the result since `key ^ rbg(1) ^ rbg(2) == key ^ rbg(2) ^ rbg(1)`. Reproducing example:  Result:  Notice in the `unsafe_rbg` case the two derived keys are the same. Note that `rbg` uses the key derivation logic from threefry which does not have this issue.  System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.29 jaxlib: 0.4.29 numpy:  1.26.3 python: 3.11.8 (stable, redacted, redacted) [Clang google3trunk (fc57f88f007497a4ead0ec8607ac66e1847b02d6)] jax.devices (1 total, 1 local): [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)] process_count: 1 platform: uname_result(system='Linux', node='8f069381872f0396766a8253c8.borgtask.google.com', release='5.10.0smp1101.34.0.0', version=' CC(Python 3 compatibility issues) [v5.10.01101.34.0.0] SMP ', machine='x86_64')",2024-05-23T20:44:59Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/21405,This is not an urgent issue to fix because it will change the reproducibility of bits produced by the PRNG. Just marking this as a fix we should bundle whenever we make a large breaking change to `unsafe_rbg` that would modify the bits anyways.
374,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Mosaic GPU] Add some activation functions for fragmented array (towards adding matmul epilogues))ï¼Œ å†…å®¹æ˜¯ ([Mosaic GPU] Add some activation functions for fragmented array (towards adding matmul epilogues))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[Mosaic GPU] Add some activation functions for fragmented array (towards adding matmul epilogues),[Mosaic GPU] Add some activation functions for fragmented array (towards adding matmul epilogues),2024-05-23T16:35:31Z,,closed,0,1,https://github.com/jax-ml/jax/issues/21401,Closing Copybara created PR due to inactivity
327,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Improve error message when trying to fetch value of non-addressable array.)ï¼Œ å†…å®¹æ˜¯ (Improve error message when trying to fetch value of nonaddressable array.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Improve error message when trying to fetch value of non-addressable array.,Improve error message when trying to fetch value of nonaddressable array.,2024-05-23T15:47:31Z,,closed,0,0,https://github.com/jax-ml/jax/issues/21399
473,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jax-metal: top-k invalid behaviour with NaNs)ï¼Œ å†…å®¹æ˜¯ ( Description  HLO   Returns:  See that using jaxmetal the NaN is not leading, and interestingly it is replaced with 0 in the output (according to the returned indices).  System info (python version, jaxlib version, accelerator, etc.)  jaxmetal 0.0.7)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jax-metal: top-k invalid behaviour with NaNs," Description  HLO   Returns:  See that using jaxmetal the NaN is not leading, and interestingly it is replaced with 0 in the output (according to the returned indices).  System info (python version, jaxlib version, accelerator, etc.)  jaxmetal 0.0.7",2024-05-23T14:03:16Z,bug Apple GPU (Metal) plugin,open,0,0,https://github.com/jax-ml/jax/issues/21397
572,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Implement LRU cache eviction for persistent compilation cache)ï¼Œ å†…å®¹æ˜¯ (This PR is part of the implementation of LRU cache eviction using the `mtime` attribute provided by the filesystem. The current PR does not support GCS, but this problem will be solved in a subsequent PR. More details in the design doc: https://docs.google.com/document/d/111YibwGXOFb_hMmlua1u63QooAzIBEHxfRPGmibis/edit?usp=sharing)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Implement LRU cache eviction for persistent compilation cache,"This PR is part of the implementation of LRU cache eviction using the `mtime` attribute provided by the filesystem. The current PR does not support GCS, but this problem will be solved in a subsequent PR. More details in the design doc: https://docs.google.com/document/d/111YibwGXOFb_hMmlua1u63QooAzIBEHxfRPGmibis/edit?usp=sharing",2024-05-23T12:18:33Z,pull ready,closed,1,10,https://github.com/jax-ml/jax/issues/21394,"> Note, I saw some NFS server being configured to not update mtime to speed up the server. Maybe document that this can happen and in that case, this will revert to creation time? The first time I saw the behavior without knowing the reason, it took times to understand what was going on. Yes, I suspect there's a chance you might see stale `mtime` values if you stick the cache on NFS and you're accessing it concurrently from multiple clients (see `lookupcache` in the NFS docs). I'm not sure there's a lot we can do about that, though.","This is a first cut at the LRU eviction implementation, so it isn't expected to work well with network file systems yet (notably GCS, which many Cloud TPU users use for their cache storage). We'll iterate from here. I don't think we should publicly document this until it works well across filesystems, but absolutely agree this should eventually be in https://jax.readthedocs.io/en/latest/persistent_compilation_cache.html.","Test fails because the test utilises `filelock`, which is not installed. ",Add filelock to build/testrequirements.txt and to the deps in tests/BUILD. Or skip the test for now if filelock is not importable.,"Just realised that JAX had a `FileSystemCache` that supports LRU cache eviction introduced in https://github.com/google/jax/pull/6869, but was subsequently removed in https://github.com/google/jax/pull/10771 to support GCS. This is exactly one of the challenges that I faced in this PR. Fortunately, I've devised potential solutions to simultaneously support LRU cache eviction and GCS compatibility. This is going to be completely solved in a subsequent PR.",All comments resolved,All review comments done,Please fix the failing lint_and_typecheck (looks like you need to fix a type annotation?),"Thank you for preparing this. Please squash the long chain of commits, or at least most of them.",> Can you link to the design doc?  I've just added the link to the first comment.
762,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Mosaic GPU] Change row-warp assignment logic in matmul example epilogue.)ï¼Œ å†…å®¹æ˜¯ ([Mosaic GPU] Change rowwarp assignment logic in matmul example epilogue. Previously we were assigning rows in a roundrobin fashion. Now, contiguous rows are assigned to the same warp for up to  This could theoretically help with small tile sizes, but in practice it doesn't seem to make a difference. Benchmarking with parameters `lhs_dtype=jnp.float32`, `rhs_dtype=jnp.float32`, `tile_m=128`, `rhs_transpose=True`, `stages=2`, and varying values for `tile_n`, gives us the following results. Before:  After: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[Mosaic GPU] Change row-warp assignment logic in matmul example epilogue.,"[Mosaic GPU] Change rowwarp assignment logic in matmul example epilogue. Previously we were assigning rows in a roundrobin fashion. Now, contiguous rows are assigned to the same warp for up to  This could theoretically help with small tile sizes, but in practice it doesn't seem to make a difference. Benchmarking with parameters `lhs_dtype=jnp.float32`, `rhs_dtype=jnp.float32`, `tile_m=128`, `rhs_transpose=True`, `stages=2`, and varying values for `tile_n`, gives us the following results. Before:  After: ",2024-05-23T09:05:22Z,,closed,0,0,https://github.com/jax-ml/jax/issues/21385
1470,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Simple primitive definition to manage speed/memory tradeoff)ï¼Œ å†…å®¹æ˜¯ (I am running into an issue with JAX where after successive vmaps one gigantic matrix is created (or at least preallocated). In my case I am computing kernel matrices:  which prints  This prints the jaxpr, and I see that one array is `n:f64[30,1000,1000,30] =...`. So what happens in the vmap is that jax pushes all the computations along the different axes into one very big matrix inside of the computations. This would prevent my code to scale well in memory.  However, if I use a slightly different function,   which prints  I can see that the largest matrix created is of size `f64[30,1000,1000]`, and it can be attributed to the existence of the primitive dot general, which doesn't blow up the dimension of my matrix's intermediate computations.  My understanding is that I could make k1 a primitive so that its inside computations are not vectorized into a very large vector. While probably slower than what is shown above, this might allow for a more flexible management of the memory/speed tradeoff, by essentially blocking jax from going too deep into the computations and making a very large matrix in the process.  I have seen the tutorial for creating primitives here, but it seems like what I'm trying to accomplish m)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Simple primitive definition to manage speed/memory tradeoff,"I am running into an issue with JAX where after successive vmaps one gigantic matrix is created (or at least preallocated). In my case I am computing kernel matrices:  which prints  This prints the jaxpr, and I see that one array is `n:f64[30,1000,1000,30] =...`. So what happens in the vmap is that jax pushes all the computations along the different axes into one very big matrix inside of the computations. This would prevent my code to scale well in memory.  However, if I use a slightly different function,   which prints  I can see that the largest matrix created is of size `f64[30,1000,1000]`, and it can be attributed to the existence of the primitive dot general, which doesn't blow up the dimension of my matrix's intermediate computations.  My understanding is that I could make k1 a primitive so that its inside computations are not vectorized into a very large vector. While probably slower than what is shown above, this might allow for a more flexible management of the memory/speed tradeoff, by essentially blocking jax from going too deep into the computations and making a very large matrix in the process.  I have seen the tutorial for creating primitives here, but it seems like what I'm trying to accomplish m",2024-05-23T02:49:10Z,enhancement,closed,0,2,https://github.com/jax-ml/jax/issues/21377,"No, there's no simple way to define primitives at the moment. That said, I don't think in practice you need this here. When wrapped in JIT, your computation will not materialize arrays of size `[30,1000,1000,30]`: the jaxpr you printed is a logical representation of the operation defined by your Python code; it is converted to HLO and then the compiler chooses how to execute it. In this case, if you inspect the compiled HLO, you see that several operations in your jaxpr are fused: essentially the compiler can recognize that the mapped sum of an elementwise product is equivalent to a `dot_general`, and it automatically calls `dot_general` instead. You can see this compiled HLO using the ahead of time compilation tools:   A good way to read this is that the arrays within the `main` block will be actually materialized, and the arrays in the several `fusion` operations are logical shapes that will not be actually materialized in the course of the computation. Long story short: if you use `jit`, you don't need to worry about memory allocation at this level.",I see thank you for the clarification.  In that case should I compile in advance to avoid errors of preallocation? When one matrix gets too big I get errors that seem related to the size of the jaxpr matrices 
1004,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([NVIDIA] Add new SDPA API to jax.nn)ï¼Œ å†…å®¹æ˜¯ (Attention plays a crucial role in modern transformerbased models. While there exist various variants, they generally follow the same workflow. Examples include the typical multihead attention (MHA), global query attention (GQA), and multiquery attention (MQA). Additionally, new implementations like the Flash Attention algorithm aim to enhance the utilization of accelerator devices. For instance, NVIDIA cuDNN supports Flash Attention and, through its API, can result in a 1.3x endtoend speedup for training large language models based on GPT alone. This PR proposes introducing a new API in the `jax.nn` module to handle attention. It will first try to use the cudnn flash attention execution path when the config is compatible. Otherwise it falls back to a jax implementation.  cc.    )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",large language model,[NVIDIA] Add new SDPA API to jax.nn,"Attention plays a crucial role in modern transformerbased models. While there exist various variants, they generally follow the same workflow. Examples include the typical multihead attention (MHA), global query attention (GQA), and multiquery attention (MQA). Additionally, new implementations like the Flash Attention algorithm aim to enhance the utilization of accelerator devices. For instance, NVIDIA cuDNN supports Flash Attention and, through its API, can result in a 1.3x endtoend speedup for training large language models based on GPT alone. This PR proposes introducing a new API in the `jax.nn` module to handle attention. It will first try to use the cudnn flash attention execution path when the config is compatible. Otherwise it falls back to a jax implementation.  cc.    ",2024-05-22T20:34:19Z,pull ready,closed,0,21,https://github.com/jax-ml/jax/issues/21371, Can you help find reviewers?,Pushed a new commit to remove the use of `is_training` for the cudnn flash attention. This is a followup of this merged PR., Any updates?,"The API should have an `implementation` option, taking values like `""xla""`, `""cudnn""`, and `None` (the default, which selects the best algorithm). This list will grow with alternative kernel implementations (Pallas, etc). It is important to be able to select the implementation type:  `""cudnn""` will fail immediately if there is some unsupported shape, which prevents silent reversions to slow code paths.  Generating serialized models to do inference with on a different device type (eg train on GPU and test on TPU).  Regarding the names: does cuDNN expose both FlashAttention and nonFlashAttention? Perhaps this should be `""cudnn_flash""`? Note that XLA also has different implementations: we could support the lowmemory chunked implementation given here (https://arxiv.org/abs/2112.05682) that inspired FlashAttention, and which is closer numerically to FlashAttention than standard attention and has the same memory complexity (maybe `""xla_chunked""`? `""xla_low_memory""`?).  Are there any configuration options a user might want to pass to the cuDNN implementation? If so, it could be a string or a cuDNN config dataclass. Eg. in the lowmemory XLA case, the chunk size is something a user might want to configure.","> The API should have an `implementation` option, taking values like `""xla""`, `""cudnn""`, and `None` (the default, which selects the best algorithm). This list will grow with alternative kernel implementations (Pallas, etc). It is important to be able to select the implementation type: >  > * `""cudnn""` will fail immediately if there is some unsupported shape, which prevents silent reversions to slow code paths. > * Generating serialized models to do inference with on a different device type (eg train on GPU and test on TPU). >  > Regarding the names: does cuDNN expose both FlashAttention and nonFlashAttention? Perhaps this should be `""cudnn_flash""`? Note that XLA also has different implementations: we could support the lowmemory chunked implementation given here (https://arxiv.org/abs/2112.05682) that inspired FlashAttention, and which is closer numerically to FlashAttention than standard attention and has the same memory complexity (maybe `""xla_chunked""`? `""xla_low_memory""`?). >  > Are there any configuration options a user might want to pass to the cuDNN implementation? If so, it could be a string or a cuDNN config dataclass. Eg. in the lowmemory XLA case, the chunk size is something a user might want to configure. Sorry, I think I missed this comment. Do you mean sth like:  Re cudnn flash attentions: (1)  cuDNN used to expose both flash and nonflash attention kernel, but we choose not to use the nonflash anymore. So, the cudnn attention means cudnn flash attention now. And I am ok with the `cudnn`. (2) We don't need to pass config to cudnn calls and we are trying to hide it from users.","> Sorry, I think I missed this comment. Do you mean sth like: That looks correct. We have two options here: 1. Have multiple SDPA functions, one per backend/implementation. 2. Have a single API with the `implementation` option.  There are pros and cons of each, and some tricky questions. For example:  How closely do numerics need to match in the superfunction to be considered 'the same'? As found in this review, cuDNN with bf16 inputs does not cast the first matmul to BF16 before doing softmax, whilst XLA does. If we choose the cuDNN convention, the XLA implementation will be incredibly memoryinefficient. This might be a significant difference in certain applications (eg. training with one but doing inference with the other on a different devicetype). With future Pallas kernels, we can match the numerics. But this might be harder for thirdparty libraries like cuDNN. We might also do autotuning and choose the best kernel with the `None` option, which becomes problematic with these numerical differences. This is an argument to have separate functions for thirdparty kernels that JAX has no control over and are largely opaque (hard to see what numerical choices are being made), and only have a superfunction for implementations under JAXcontrol.   Another argument for separate functions is that the API can be restricted to only the supported features, rather than the most general function imaginable. The current design is makes it hard for users to see what is supported, and limits documentation opportunities. In addition, there are cuDNN specific options (like the philox dropout) unsupported by any other backend, further complicating the API.",I think the name should be `dot_product_attention` rather than `scaled_dot_product_attention`. Its also more consistent with Flax naming (https://flax.readthedocs.io/en/v0.8.0/api_reference/flax.linen/_autosummary/flax.linen.dot_product_attention.html).,"As discussed offline: lets land the simplest version first, without dropout or other complications. Then progressively add features. ","Just pushed some new commits for the simplified sdpa.  PTAL. Also talked to  and he will try to implement the combination of bias and mask in the cudnn dot_product_attention API (as described here in (1)). When that is in, our logic of preparing bias will be much simpler.",Pushed a few more changes. PTAL.  ,Please squash the commits. This will be mergeable as soon as Chris clarifies his comments.,> Please squash the commits. This will be mergeable as soon as Chris clarifies his comments. Sure. Rebased. PTAL.  ,: I'm happy with the state of it now. Think we can merge.,"Pushed new commits to resolved some failed python lint tests. Btw, can we have the access to add `kokoro:forcerun` label to trigger the tests?",Please squash the commits and we can merge.,Done. PTAL.  ,I still saw this lint error:  But I am a bit confused. I think it refers to the `mask` which I have already converted to Array by `jnp.asarray(mask)` at the beginning in the function. Do you have any advice on this?   ,"No worries, I'll resolve this internally.","> As discussed offline: lets land the simplest version first, without dropout or other complications. Then progressively add features. Thanks for adding FA! Is there a timeline to add `dropout` support in the SDPA API? I understand it is on hold due to differences in PRNG implementation. Would it be OK if we expose `dropout_rate` to the API while warning the user on reproducibility if `cudnn` is selected? https://github.com/google/jax/blob/417fcd574b9f33410ea8eb78ffdea825ad343eee/jax/_src/cudnn/fused_attention_stablehlo.pyL954L956","> > As discussed offline: lets land the simplest version first, without dropout or other complications. Then progressively add features. >  > Thanks for adding FA! Is there a timeline to add `dropout` support in the SDPA API? I understand it is on hold due to differences in PRNG implementation. Would it be OK if we expose `dropout_rate` to the API while warning the user on reproducibility if `cudnn` is selected? >  > https://github.com/google/jax/blob/417fcd574b9f33410ea8eb78ffdea825ad343eee/jax/_src/cudnn/fused_attention_stablehlo.pyL954L956 Yes, this is on our radar to be implemented. Can we know what types of model you are working on that needs the dropout?","> Yes, this is on our radar to be implemented. Can we know what types of model you are working on that needs the dropout? Attention dropout would help for almost all lowdata training regimes. Detection Transformers are one wellknown example.  Torch supports FA dropout (possibly nondeterministic) in their functional API."
1448,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump setuptools from 69.2.0 to 70.0.0)ï¼Œ å†…å®¹æ˜¯ (Bumps setuptools from 69.2.0 to 70.0.0.  Changelog Sourced from setuptools's changelog.  v70.0.0 Features  Emit a warning when [tools.setuptools] is present in pyproject.toml and will be ignored.  by :user:SnoopJ ( CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)) Improved AttributeError error message if pkg_resources.EntryPoint.require is called without extras or distribution Gracefully &quot;do nothing&quot; when trying to activate a pkg_resources.Distribution with a None location, rather than raising a TypeError  by :user:Avasam ( CC([jax2tf] Add input and output names to the TensorFlow graph generated by jax2tf.)) Typed the dynamically defined variables from pkg_resources  by :user:Avasam ( CC(Pin pygments version in RTD build)) Modernized and refactored VCS handling in package_index. ( CC([jax2tf] Revert '[jax2tf] Replace tf.math.add with tf.raw_ops.AddV2 (â€¦))  Bugfixes  In install command, use super to call the superclass methods. Avoids race conditions when monkeypatching from _distutils_system_mod occurs late. ( CC(Rename collectives into ""collective operations"" for the pmap function.)) Fix finder template for lenient editable installs of implicit nested namespaces constructed by using package_dir to reorganise directory structure. ( CC([ja)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bump setuptools from 69.2.0 to 70.0.0,"Bumps setuptools from 69.2.0 to 70.0.0.  Changelog Sourced from setuptools's changelog.  v70.0.0 Features  Emit a warning when [tools.setuptools] is present in pyproject.toml and will be ignored.  by :user:SnoopJ ( CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®)) Improved AttributeError error message if pkg_resources.EntryPoint.require is called without extras or distribution Gracefully &quot;do nothing&quot; when trying to activate a pkg_resources.Distribution with a None location, rather than raising a TypeError  by :user:Avasam ( CC([jax2tf] Add input and output names to the TensorFlow graph generated by jax2tf.)) Typed the dynamically defined variables from pkg_resources  by :user:Avasam ( CC(Pin pygments version in RTD build)) Modernized and refactored VCS handling in package_index. ( CC([jax2tf] Revert '[jax2tf] Replace tf.math.add with tf.raw_ops.AddV2 (â€¦))  Bugfixes  In install command, use super to call the superclass methods. Avoids race conditions when monkeypatching from _distutils_system_mod occurs late. ( CC(Rename collectives into ""collective operations"" for the pmap function.)) Fix finder template for lenient editable installs of implicit nested namespaces constructed by using package_dir to reorganise directory structure. ( CC([ja",2024-05-22T13:37:16Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/21355,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
663,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Segfault during initialization due to local cuda version mismatch)ï¼Œ å†…å®¹æ˜¯ ( Description Running `python c ""import jax; jax.numpy.array(0)""` fails with segfault with the following call stack  Jax is installed with  and the system has only CUDA 11.7 locally. Installing jax with `pip install U ""jax[cuda12]""` fixes this problem as expected. But is it avoidable to fail with segfault, and print a reasonable error message instead?  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Segfault during initialization due to local cuda version mismatch," Description Running `python c ""import jax; jax.numpy.array(0)""` fails with segfault with the following call stack  Jax is installed with  and the system has only CUDA 11.7 locally. Installing jax with `pip install U ""jax[cuda12]""` fixes this problem as expected. But is it avoidable to fail with segfault, and print a reasonable error message instead?  System info (python version, jaxlib version, accelerator, etc.) ",2024-05-22T06:45:37Z,bug,closed,0,6,https://github.com/jax-ml/jax/issues/21349,"Hi   Thanks for reporting. JAX has dropped support for cuda11 from jaxlib version 0.4.26 and supports only cuda 12.1 or newer.  Please take a look at the official documentation for reference.: https://jax.readthedocs.io/en/latest/changelog.html:~:text=jaxlib%200.4.26%20(April,supports%20NumPy%202.0.","Yes, I'm fully aware of the CUDA version requirement, and I'm discussing that whether the UX could improve. Also, it should be noted that running on a system without CUDA (tested with a fresh `python:3.11slim` container) also segfaults. In my opinion, segfaults should be avoided in all cases.",Ironically that's segfaulting in the code that's trying to check the cuda version and report a nice error. I'll take a look.,Can you please clarify what *CPU* you have? i.e. the output of `lscpu` or similar?,"Never mind, I figured out the bug. Oddly enough it only seems to reproduce for me under a condaforge environment, but it's a bug in the stubs we use to find cusparse. The error path triggered some invalid behavior, and openxla/xla CC(tflite converter segfaulting with an updated jax) fixes it by adding correct stubs for error string code.",I verified that this seems fixed to me in the most recent nightly release. Please reopen if you can still see it with a nightly (or the next release).
804,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Restoring and using int4 arrays on XLA CPU is broken after #20610.)ï¼Œ å†…å®¹æ˜¯ ( Description Casting `int4` arrays to `int8` after restoring them from a checkpoint was broken in CC(Add `Layout` support to `jax.jit`.) (confirmed via CL sweep) with the following error:  This can be tested locally using the following code:  and tested in the CI by adding the following to the end of `test_checkpointing_with_int4` in `jax/experimental/array_serialization/serialization_test.py`:  This also fails when casting is added to Orbax's `single_host_test.py`.  System info (python version, jaxlib version, accelerator, etc.) Google internal HEAD: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Restoring and using int4 arrays on XLA CPU is broken after #20610.," Description Casting `int4` arrays to `int8` after restoring them from a checkpoint was broken in CC(Add `Layout` support to `jax.jit`.) (confirmed via CL sweep) with the following error:  This can be tested locally using the following code:  and tested in the CI by adding the following to the end of `test_checkpointing_with_int4` in `jax/experimental/array_serialization/serialization_test.py`:  This also fails when casting is added to Orbax's `single_host_test.py`.  System info (python version, jaxlib version, accelerator, etc.) Google internal HEAD: ",2024-05-21T22:57:38Z,bug,closed,0,6,https://github.com/jax-ml/jax/issues/21339,I ran this function internally and it doesn't seem to error:  Does this error for you?,Note that you need to run on TPU and not CPU. Do you see the same error on TPU?, Isn't the issue as stated by the reporter that int4 specifically regressed on *CPU*?,"Uh oh, I skipped that part lol.",https://github.com/google/jax/pull/21372 should fix it,"Thank you! My team uses XLA CPU when exporting to MLIR for mobile deployment, so this is quite helpful!"
1502,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Counterintuitive Running time on GPU, compiled differences on profiler for different shapes)ï¼Œ å†…å®¹æ˜¯ ( Description I am running some sort of stochastic optimisation on GPU and depending on the shape of my samples I get very different running times. For example, the following code with shapes of 100x200 yields a running time of 28.8s while for 1000x2000 I have 5.8s. I have trouble understanding this **increase** in performance on GPU as the number of samples is x100. I expected low sample size to be somewhat inefficient on GPU but not to have its performance reduce by a factor 6 compared to 100 times more samples. Is there something I am missing ?  A minimum reproducible example is: ```python import jax from jaxtyping import PRNGKeyArray, Array from functools import partial import jax.numpy as jnp import optax import time import numpyro.distributions as dist max_signal = 1.0 base_signal = 0.1 noise_var = 0.1 jax.print_environment_info() def forward(xi, theta):     inv_value = max_signal + jnp.power(theta  xi, 2).sum(1)     arr = base_signal + jnp.power(inv_value, 1).sum(1)     return arr def log_prob(thetas, y, xi):     f_values = jax.vmap(forward, in_axes=(0, None), out_axes=1)(xi, thetas)     return (         dist.Normal(0, noise_var)         .log_prob(jnp.log(y)  jnp.log(f_values))         .sum(1)     ) def sa)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"Counterintuitive Running time on GPU, compiled differences on profiler for different shapes"," Description I am running some sort of stochastic optimisation on GPU and depending on the shape of my samples I get very different running times. For example, the following code with shapes of 100x200 yields a running time of 28.8s while for 1000x2000 I have 5.8s. I have trouble understanding this **increase** in performance on GPU as the number of samples is x100. I expected low sample size to be somewhat inefficient on GPU but not to have its performance reduce by a factor 6 compared to 100 times more samples. Is there something I am missing ?  A minimum reproducible example is: ```python import jax from jaxtyping import PRNGKeyArray, Array from functools import partial import jax.numpy as jnp import optax import time import numpyro.distributions as dist max_signal = 1.0 base_signal = 0.1 noise_var = 0.1 jax.print_environment_info() def forward(xi, theta):     inv_value = max_signal + jnp.power(theta  xi, 2).sum(1)     arr = base_signal + jnp.power(inv_value, 1).sum(1)     return arr def log_prob(thetas, y, xi):     f_values = jax.vmap(forward, in_axes=(0, None), out_axes=1)(xi, thetas)     return (         dist.Normal(0, noise_var)         .log_prob(jnp.log(y)  jnp.log(f_values))         .sum(1)     ) def sa",2024-05-21T22:05:53Z,bug,open,0,4,https://github.com/jax-ml/jax/issues/21336,"Hi  thanks for the question! Your benchmarks may not be measuring what you think they are here, because you're not accounting for JIT compilation time or for asynchronous dispatch. I'd suggest applying some of the tips at FAQ: Benchmarking JAX code and rerunning your benchmarks.","Hi, thanks for your response. I believe this difference is neither due to compilation time nor asynchronous dispatch as I get same results with jax.block_until_ready() and compilation time seems to be <1s.","Your updated script doesn't seem to isolate compilation time â€“ even if you're not using `jax.jit` yourself, it is used internally by APIs you are calling.","Yes  compilation happens inside scan. To asses compilation time I compared two consecutive calls to timer block:  which suggest that compilation takes <1s. My question was regarding the potential differences between XLA reduce_sum and add_any and why the same code with different shapes gets compiled in two more or less efficient ways. The most efficient way being the one compiled for the largest shapes. On top of empirical total running times, the difference between the two can be assessed with a tensorboard profiler for example where we actually see the difference between add_any / reduce_sum. The function step() takes longer to run in the add_any case even though there are less computations to do.  This behaviour is not observed on CPU where running time is proportional to shapes (2.5s for 100x200 and 126s for 1000x2000)."
1293,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(WIP `jax.numpy` array API compliance finalization)ï¼Œ å†…å®¹æ˜¯ (Towards https://github.com/google/jax/issues/21088  Changes  Adds the following attributes to `ArrayImpl`:      `__array_namespace__` property      `to_device` method      `device` property  Adds the following to `jax.numpy`:      `__array_namespace_info__`      `__array_api_version__`  Notes This PR is a draft right now since we should include these changes _last_ so as to publicly support `jax.numpy` as an array API compliant namespace. 5 can take over this PR later once the remainder of the work is completed. This does not need to wait on _all_ the ongoing array API related deprecations to be completed, since some of them are only required for the 2023 standard, hence we can likely adopt the 2022 standard first. It may make sense break off the `to_device` and `device` changes for `ArrayImpl` into a small separate PR, since they don't imply explicit compliance by themselves, but I wanted to keep them together in this PR in case there were any caveats wrt to `ArrayImpl` vs `Tracer` behaviors that we should discuss first (based on old `TODO` note).)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,WIP `jax.numpy` array API compliance finalization,"Towards https://github.com/google/jax/issues/21088  Changes  Adds the following attributes to `ArrayImpl`:      `__array_namespace__` property      `to_device` method      `device` property  Adds the following to `jax.numpy`:      `__array_namespace_info__`      `__array_api_version__`  Notes This PR is a draft right now since we should include these changes _last_ so as to publicly support `jax.numpy` as an array API compliant namespace. 5 can take over this PR later once the remainder of the work is completed. This does not need to wait on _all_ the ongoing array API related deprecations to be completed, since some of them are only required for the 2023 standard, hence we can likely adopt the 2022 standard first. It may make sense break off the `to_device` and `device` changes for `ArrayImpl` into a small separate PR, since they don't imply explicit compliance by themselves, but I wanted to keep them together in this PR in case there were any caveats wrt to `ArrayImpl` vs `Tracer` behaviors that we should discuss first (based on old `TODO` note).",2024-05-21T01:03:41Z,,closed,0,2,https://github.com/jax-ml/jax/issues/21323,"We only finally removed the `arr.device()` method in JAX v0.4.27 â€“ to avoid confusion for users I think we should wait for one more release (0.4.29) before we add the `arr.device` property, so that it will be part of 0.4.30. What do you think?", let me close this PR as Array API compliance is already finalized on `main`.
756,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Compilation time on GPU is proportional to batch size for grad of vmapped Cholesky solve)ï¼Œ å†…å®¹æ˜¯ ( Description The problem is with the grad of the mean of a vmapped Cholesky solution . If I define  and then transform/compile  I find that the compilation time grows with `nbatch`. For instance `nbatch, time(s) = [16,0.532], [32,0.507], [64,0.516], [128,0.580], [256,0.652], [512,0.822], [1024,1.7], [2048,2.75]` for the example matrices listed below. What's happening here? To run this example you need matrices such as   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Compilation time on GPU is proportional to batch size for grad of vmapped Cholesky solve," Description The problem is with the grad of the mean of a vmapped Cholesky solution . If I define  and then transform/compile  I find that the compilation time grows with `nbatch`. For instance `nbatch, time(s) = [16,0.532], [32,0.507], [64,0.516], [128,0.580], [256,0.652], [512,0.822], [1024,1.7], [2048,2.75]` for the example matrices listed below. What's happening here? To run this example you need matrices such as   System info (python version, jaxlib version, accelerator, etc.) ",2024-05-20T19:41:32Z,bug,open,0,5,https://github.com/jax-ml/jax/issues/21313,"Thanks for the report. I'm not sure what's going on, but it seems others are also noticing this: https://stackoverflow.com/questions/78486071/whydoesjaxcompilationtimegrowwithvmapbatchsize","Thanks ; both queries are from me :) In this case, note also that the Cholesky without the grad compiles in constant time, so it must be something about the highlevel gradient algorithm for Cholesky.",I can repro on a Colab A100; thought it somehow might have to do with constant folding but even passing `fmat` as an argument and defining `one` and `ones` in function I still see the batchdependent compile time,"Interestingly, using your `make_hlo` (which I just found in https://github.com/google/jax/issues/7949) shows that the XLA code for, say, `nbatch = 64` and `nbatch = 512` is essentially the same, except for 64 > 512. Would this mean that the problem is at the LLVM level? (Or another Nvidia representation?)",Another clue is that the linear compilation time happens also for a function that's already written with the extra batch dimension instead of being `vmap`ped. So the problem must be the batched grad Cholesky.
1448,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump setuptools from 69.2.0 to 69.5.1)ï¼Œ å†…å®¹æ˜¯ (Bumps setuptools from 69.2.0 to 69.5.1.  Changelog Sourced from setuptools's changelog.  v69.5.1 No significant changes. v69.5.0 Features  Refresh unpinned vendored dependencies. ( CC(implement jnp.apply_along_axis)) Updated vendored packaging to version 24.0. ( CC(Support polynomial division for mask))  v69.4.2 Bugfixes  Merged bugfix for pypa/distutils CC(Batching rule for 'gather' not implemented).  v69.4.1 No significant changes. v69.4.0 Features  pypa/distutils CC(Implement the `mode='constant'` case of `np.pad`.) CC(Fix device_put_sharded() for concrete values))  v69.3.1 Bugfixes  Remove attempt to canonicalize the version. It's already canonical enough. ( CC(Fix code quality issues))  v69.3.0   ... (truncated)   Commits  ff58075 Bump version: 69.5.0 â†’ 69.5.1 d46727f Merge tag 'v69.4.2' into main. 5de8e14 Bump version: 69.4.1 â†’ 69.4.2 f07b037 Add news fragment. 608de82 Merge https://github.com/pypa/distutils into v69.4.1 e5e3cc1 Merge pull request  CC(Add Sphinxgenerated reference documentation for JAX.) from pypa/hotfix/246linkerargslist ef297f2 Extend the retention of the compatibility. 98eee7f Exclude compat package from coverage. d2581bf Add 'consolidate_linker_args' wrapper to protect the old behavior)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Bump setuptools from 69.2.0 to 69.5.1,Bumps setuptools from 69.2.0 to 69.5.1.  Changelog Sourced from setuptools's changelog.  v69.5.1 No significant changes. v69.5.0 Features  Refresh unpinned vendored dependencies. ( CC(implement jnp.apply_along_axis)) Updated vendored packaging to version 24.0. ( CC(Support polynomial division for mask))  v69.4.2 Bugfixes  Merged bugfix for pypa/distutils CC(Batching rule for 'gather' not implemented).  v69.4.1 No significant changes. v69.4.0 Features  pypa/distutils CC(Implement the `mode='constant'` case of `np.pad`.) CC(Fix device_put_sharded() for concrete values))  v69.3.1 Bugfixes  Remove attempt to canonicalize the version. It's already canonical enough. ( CC(Fix code quality issues))  v69.3.0   ... (truncated)   Commits  ff58075 Bump version: 69.5.0 â†’ 69.5.1 d46727f Merge tag 'v69.4.2' into main. 5de8e14 Bump version: 69.4.1 â†’ 69.4.2 f07b037 Add news fragment. 608de82 Merge https://github.com/pypa/distutils into v69.4.1 e5e3cc1 Merge pull request  CC(Add Sphinxgenerated reference documentation for JAX.) from pypa/hotfix/246linkerargslist ef297f2 Extend the retention of the compatibility. 98eee7f Exclude compat package from coverage. d2581bf Add 'consolidate_linker_args' wrapper to protect the old behavior,2024-05-20T17:41:46Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/21312,Superseded by CC(Bump setuptools from 69.2.0 to 70.0.0).
1448,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump pytest-xdist from 3.5.0 to 3.6.1)ï¼Œ å†…å®¹æ˜¯ (Bumps pytestxdist from 3.5.0 to 3.6.1.  Changelog Sourced from pytestxdist's changelog.  pytestxdist 3.6.1 (20240428) Bug Fixes   CC(Implementation of np.corrcoef) &lt;https://github.com/pytestdev/pytestxdist/issues/1071&gt;_: Add backward compatibility for deadlock issue with the execnet new main_thread_only &quot;execmodel&quot; triggered when pytestcov accesses rinfo.  pytestxdist 3.6.0 (20240419) This release was YANKED due to a regression fixed in 3.6.1. Features   CC(`jax.jit` should report an error immediately for uncallable arguments) &lt;https://github.com/pytestdev/pytestxdist/pull/1027&gt;_:pytestxdist workers now always execute the tests in the main thread. Previously some tests might end up executing in a separate thread other than main in the workers, due to some internal execnet`` details. This can cause problems specially with async frameworks where the event loop is running in the ``main`` thread (for example  CC(Indexing numpy array with DeviceArray: index interpreted as tuple) pytestdev/pytestxdist CC(Indexing numpy array with DeviceArray: index interpreted as tuple)`__).  Bug Fixes    CC(FFT Hessian broken) &lt;https://github.com/pytestdev/pytestxdist/issues/1024&gt;_: Added proper handling o)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bump pytest-xdist from 3.5.0 to 3.6.1,"Bumps pytestxdist from 3.5.0 to 3.6.1.  Changelog Sourced from pytestxdist's changelog.  pytestxdist 3.6.1 (20240428) Bug Fixes   CC(Implementation of np.corrcoef) &lt;https://github.com/pytestdev/pytestxdist/issues/1071&gt;_: Add backward compatibility for deadlock issue with the execnet new main_thread_only &quot;execmodel&quot; triggered when pytestcov accesses rinfo.  pytestxdist 3.6.0 (20240419) This release was YANKED due to a regression fixed in 3.6.1. Features   CC(`jax.jit` should report an error immediately for uncallable arguments) &lt;https://github.com/pytestdev/pytestxdist/pull/1027&gt;_:pytestxdist workers now always execute the tests in the main thread. Previously some tests might end up executing in a separate thread other than main in the workers, due to some internal execnet`` details. This can cause problems specially with async frameworks where the event loop is running in the ``main`` thread (for example  CC(Indexing numpy array with DeviceArray: index interpreted as tuple) pytestdev/pytestxdist CC(Indexing numpy array with DeviceArray: index interpreted as tuple)`__).  Bug Fixes    CC(FFT Hessian broken) &lt;https://github.com/pytestdev/pytestxdist/issues/1024&gt;_: Added proper handling o",2024-05-20T17:41:38Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/21311,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
1442,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump pluggy from 1.4.0 to 1.5.0)ï¼Œ å†…å®¹æ˜¯ (Bumps pluggy from 1.4.0 to 1.5.0.  Changelog Sourced from pluggy's changelog.  pluggy 1.5.0 (20240419) Features    CC(add np.append and np.polyval) &lt;https://github.com/pytestdev/pluggy/issues/178&gt;_: Add support for deprecating specific hook parameters, or more generally, for issuing a warning whenever a hook implementation requests certain parameters. See :ref:warn_on_impl for details.   Bug Fixes   CC(Initial work on autospmd) &lt;https://github.com/pytestdev/pluggy/issues/481&gt;_: PluginManager.get_plugins() no longer returns None for blocked plugins.     Commits  f8aa4a0 Preparing release 1.5.0 b4a8c92 Merge pull request  CC(while_loop vmap init_val with mixed map/unmap dims) from bluetech/warnonimplargs 6f6ea68 Add support deprecating hook parameters 91f88d2 Merge pull request  CC(Add nanmean to lax_numpy) from bluetech/codecovaction 89ce829 ci: replace uploadcoverage script with codecov github action 29f104d Lift pluggy ( CC(Prefix functions in lax.py that shouldn't be visible in documentation.)) c2b36b4 Merge pull request  CC(Add batching rule for triangular solve.) from pytestdev/precommitciupdateconfig 2b533c9 [precommit.ci] precommit autoupdate 04d1bcd [precommit.ci] precommit autoupdate ( CC(vma)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bump pluggy from 1.4.0 to 1.5.0,"Bumps pluggy from 1.4.0 to 1.5.0.  Changelog Sourced from pluggy's changelog.  pluggy 1.5.0 (20240419) Features    CC(add np.append and np.polyval) &lt;https://github.com/pytestdev/pluggy/issues/178&gt;_: Add support for deprecating specific hook parameters, or more generally, for issuing a warning whenever a hook implementation requests certain parameters. See :ref:warn_on_impl for details.   Bug Fixes   CC(Initial work on autospmd) &lt;https://github.com/pytestdev/pluggy/issues/481&gt;_: PluginManager.get_plugins() no longer returns None for blocked plugins.     Commits  f8aa4a0 Preparing release 1.5.0 b4a8c92 Merge pull request  CC(while_loop vmap init_val with mixed map/unmap dims) from bluetech/warnonimplargs 6f6ea68 Add support deprecating hook parameters 91f88d2 Merge pull request  CC(Add nanmean to lax_numpy) from bluetech/codecovaction 89ce829 ci: replace uploadcoverage script with codecov github action 29f104d Lift pluggy ( CC(Prefix functions in lax.py that shouldn't be visible in documentation.)) c2b36b4 Merge pull request  CC(Add batching rule for triangular solve.) from pytestdev/precommitciupdateconfig 2b533c9 [precommit.ci] precommit autoupdate 04d1bcd [precommit.ci] precommit autoupdate ( CC(vma",2024-05-20T17:41:33Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/21310,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
1446,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump matplotlib from 3.8.3 to 3.9.0)ï¼Œ å†…å®¹æ˜¯ (Bumps matplotlib from 3.8.3 to 3.9.0.  Release notes Sourced from matplotlib's releases.  REL: 3.9.0 Highlights of this release include:  Plotting and Annotation improvements  Axes.inset_axes is no longer experimental Legend support for Boxplot Percent sign in pie labels autoescaped with usetex=True hatch parameter for stackplot Add option to plot only one half of violin plot axhline and axhspan on polar axes Subplot titles can now be automatically aligned axisartist can now be used together with standard Formatters Toggle minorticks on Axis StrMethodFormatter now respects axes.unicode_minus   Figure, Axes, and Legend Layout  Subfigures now have controllable zorders Getters for xmargin, ymargin and zmargin   Mathtext improvements  mathtext documentation improvements mathtext spacing corrections   Widget Improvements  Check and Radio Button widgets support clearing   3D plotting improvements  Setting 3D axis limits now set the limits exactly   Other improvements  New BackendRegistry for plotting backends Add widths, heights and angles setter to EllipseCollection image.interpolation_stage rcParam Arrow patch position is now modifiable NonUniformImage now has mouseover support    REL: v3.9.0rc2 This is the second r)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bump matplotlib from 3.8.3 to 3.9.0,"Bumps matplotlib from 3.8.3 to 3.9.0.  Release notes Sourced from matplotlib's releases.  REL: 3.9.0 Highlights of this release include:  Plotting and Annotation improvements  Axes.inset_axes is no longer experimental Legend support for Boxplot Percent sign in pie labels autoescaped with usetex=True hatch parameter for stackplot Add option to plot only one half of violin plot axhline and axhspan on polar axes Subplot titles can now be automatically aligned axisartist can now be used together with standard Formatters Toggle minorticks on Axis StrMethodFormatter now respects axes.unicode_minus   Figure, Axes, and Legend Layout  Subfigures now have controllable zorders Getters for xmargin, ymargin and zmargin   Mathtext improvements  mathtext documentation improvements mathtext spacing corrections   Widget Improvements  Check and Radio Button widgets support clearing   3D plotting improvements  Setting 3D axis limits now set the limits exactly   Other improvements  New BackendRegistry for plotting backends Add widths, heights and angles setter to EllipseCollection image.interpolation_stage rcParam Arrow patch position is now modifiable NonUniformImage now has mouseover support    REL: v3.9.0rc2 This is the second r",2024-05-20T17:41:25Z,dependencies python,closed,0,1,https://github.com/jax-ml/jax/issues/21308,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
1431,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(JIT constant folding)ï¼Œ å†…å®¹æ˜¯ ( Description Hi, I was hoping that someone could help me with this. Sometimes, when using constants in jitted functions, I get warnings like this one:  These warnings appear seemingly random, for example with the following code:  This code produces ""constant folding"" warnings on windows and on linux. Maybe / probably this is dependend on OS version, CPU type, ... When playing around with array shapes and number of nested vmaps, these messages appear or not appear without any clear (atleast not clear to me) pattern. For exampe, this is fast:  While this is slow and produces the warning:  Constant folding only happens when compiling with `jax.jit`  making jaxprs is not affected. Since jaxprs are perfectly able to catch constants, it is possible to compile them while treating constants as variables. The following function demonstrates this:  Now, using `other_jit(f)()` instead of `jax.jit(f)()` prevents the issue. I was wondering if this is intended behavior. Wouldn't it be a better solution in most cases to always treat constants as variables while compiling, to prevent constant folding from slowing down compilations? In realworld scenarios, using (a generalized version of) the `other_jit` function I presented her)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,JIT constant folding," Description Hi, I was hoping that someone could help me with this. Sometimes, when using constants in jitted functions, I get warnings like this one:  These warnings appear seemingly random, for example with the following code:  This code produces ""constant folding"" warnings on windows and on linux. Maybe / probably this is dependend on OS version, CPU type, ... When playing around with array shapes and number of nested vmaps, these messages appear or not appear without any clear (atleast not clear to me) pattern. For exampe, this is fast:  While this is slow and produces the warning:  Constant folding only happens when compiling with `jax.jit`  making jaxprs is not affected. Since jaxprs are perfectly able to catch constants, it is possible to compile them while treating constants as variables. The following function demonstrates this:  Now, using `other_jit(f)()` instead of `jax.jit(f)()` prevents the issue. I was wondering if this is intended behavior. Wouldn't it be a better solution in most cases to always treat constants as variables while compiling, to prevent constant folding from slowing down compilations? In realworld scenarios, using (a generalized version of) the `other_jit` function I presented her",2024-05-19T20:36:43Z,bug,open,1,17,https://github.com/jax-ml/jax/issues/21300,I'm aware that `other_jit` recompiles the function with every call  in realword scenarios it would be better to save and reuse compiled functions.,"Some explanation why it depends on the shape: We have a heuristic to not apply constant folding if the operand shape is too large. The cutoff is 45 * 1000 * 1000 elements. In the ""fast"" cases we don't apply constant folding.","Thanks for the reply! It also seems to depend on the operation itself. For examle, with a double `vmap` (i.e. sum over last axis), it happens, but it doesn't happen when using only one vmap (i.e. sum over last two axes):  Maybe it would help to clarify what constant folding is used for / where it makes sense to apply it. As far as I know, it basically means that the compiler evaluates some operations at compile time (to save runtime), **if** all inputs for these operations are known in advance (i.e. if they are ""constants""). I'm wondering why this is so slow  intuitively, I would think that constant folding happens approximately at the speed of `numpy` or uncompiled `jax.numpy`. But it seems to be much slower than that!","For constant folding, the HloEvaluator is used. It is not optimized for speed, but for correctness, as it is used as reference backend in tests. You can see the rules that we have for the ConstantFolding pass here: https://github.com/openxla/xla/blob/main/xla/service/hlo_constant_folding.'t know what the nested jax.vmap would translate to, but I think you can safely assume that fast runtime means that constant folding is not applied. Constant folding only makes sense if what is being constant folded would run several times. If it is run only a single time, then you would be better off without constant folding.","Thanks for clarifying! Would it be a useful addition to `jax.jit` to make it possible to turn this behavior off? Instead, constants could be treated as regular variables (that then get passed to the compiled function), preventing constant folding from ever happening. For example, you could force this with the current API using `jax.make_jaxpr` and `partial_eval`  basically first extracting all constants (i.e. known values) with `make_jaxpr`, then computing as many values as possible using `partial_eval`, and then compiling the remaining jaxpr, using the precomputed values whenever it's called. Maybe this would be a nice addition for those users (like me) who use many and large static arrays (i.e. constants in the context of jit) but don't want constant folding to slow the compilation down.","I am not familiar with the JAX side of things. On XLA side we have a flag that could be used to turn off constant folding: xla_disable_hlo_passes=constant_folding This can be set via the XLA_FLAGS environment variable. So something like os.environ['XLA_FLAGS'] = ""xla_disable_hlo_passes=constant_folding"" from python",Thanks for helping! It would be nice to also have an option like this in `jax.jit` to control this behavior  something like `constant_folding: bool` maybe.,You can do this via `jax.jit(f).lower(*args).compile(compiler_options={'xla_disable_hlo_passes': True})`. We are looking into supporting this as an option to jit but you can do it via the AOT API for now.,"That was a fast comment! When trying this, i get the following error: `jaxlib.xla_extension.XlaRuntimeError: INVALID_ARGUMENT: While setting option xla_disable_hlo_passes, '1' is not a valid string value.`",Ohh sorry you need `'xla_disable_hlo_passes': 'constant_folding'`, The code I used: ,"Hmm, this might require some fixes in the jax code. I'll take a look.","Was there any solution to this issue? I am also getting these warnings when scanning over large tensors. I'd also be happy with some way to silence the warnings and just accept the long compile time but without the terminal spam, but I couldn't find any logger that corresponded to the errors being issued (I tried setting all jax loggers to `logging.ERROR` using `logging.root.manager.loggingDict`).","I just ended up first converting my main functions to jaxpr and then compiling them while treating their constants as dynamic variables. Additionally, this enables you to use partialeval to precompute the constant part of your jaxpr (which can be a lot of saved time if you call it often enough). Sadly, jax does not seem to have any api for automating these processes.", Do you have code snippets to perform this? I am having similar troubles,"my_jit.txt This is (up to some minor differences) what I currently use. I don't exactly know how well the current implementation of jax.jit handles constant folding, but the ""jit"" function in this file will always treat constants that appear during jaxpr tracing as dynamic inputs to jax.jit and thus prevent any constants from appearing during compile time. It should be straightforward to see what this code does. If you enable ""precompute"", your code will run faster (if you have lots of constants in your main function) but may need considerably more memory. Note: As far as I know, jaxprs are considered to be private/semipublic api.","Thank you very much, I finally tested it and it works as intended. I still wish for better workaround in the future ahah"
1439,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Can't build jaxlib in GH200 )ï¼Œ å†…å®¹æ˜¯ ( Description I'm trying to run some code utilizing my GH200 without success. Unable to build jaxlib for my GPU.   System info (python version, jaxlib version, accelerator, etc.) root:~/jax nvidiasmi Sun May 19 12:13:00 2024        ++  ++ root:~/jax nvcc version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 20052024 NVIDIA Corporation Built on Thu_Mar_28_02:24:28_PDT_2024 Cuda compilation tools, release 12.4, V12.4.131 Build cuda_12.4.r12.4/compiler.34097967_0  the error i get: Error limit reached. 100 errors detected in the compilation of ""external/xla/xla/stream_executor/gpu/redzone_allocator_kernel_cuda.cc"". Compilation terminated. Target //jaxlib/tools:build_gpu_plugin_wheel failed to build INFO: Elapsed time: 7.262s, Critical Path: 4.88s INFO: 73 processes: 73 internal. FAILED: Build did NOT complete successfully ERROR: Build failed. Not running target Traceback (most recent call last):   File ""/root/jax/build/build.py"", line 733, in      main()   File ""/root/jax/build/build.py"", line 727, in main     shell(build_pjrt_plugin_command)   File ""/root/jax/build/build.py"", line 45, in shell     output = subprocess.check_output(cmd)   File ""/usr/lib/python3.10/subprocess.py"", line 421, in check_output     re)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Can't build jaxlib in GH200 ," Description I'm trying to run some code utilizing my GH200 without success. Unable to build jaxlib for my GPU.   System info (python version, jaxlib version, accelerator, etc.) root:~/jax nvidiasmi Sun May 19 12:13:00 2024        ++  ++ root:~/jax nvcc version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 20052024 NVIDIA Corporation Built on Thu_Mar_28_02:24:28_PDT_2024 Cuda compilation tools, release 12.4, V12.4.131 Build cuda_12.4.r12.4/compiler.34097967_0  the error i get: Error limit reached. 100 errors detected in the compilation of ""external/xla/xla/stream_executor/gpu/redzone_allocator_kernel_cuda.cc"". Compilation terminated. Target //jaxlib/tools:build_gpu_plugin_wheel failed to build INFO: Elapsed time: 7.262s, Critical Path: 4.88s INFO: 73 processes: 73 internal. FAILED: Build did NOT complete successfully ERROR: Build failed. Not running target Traceback (most recent call last):   File ""/root/jax/build/build.py"", line 733, in      main()   File ""/root/jax/build/build.py"", line 727, in main     shell(build_pjrt_plugin_command)   File ""/root/jax/build/build.py"", line 45, in shell     output = subprocess.check_output(cmd)   File ""/usr/lib/python3.10/subprocess.py"", line 421, in check_output     re",2024-05-19T12:15:48Z,bug NVIDIA GPU,open,0,27,https://github.com/jax-ml/jax/issues/21299,Can you share the compilation errors you're getting in external/xla/xla/stream_executor/gpu/redzone_allocator_kernel_cuda.cc?,"Thanks for the quick reply...  include ""absl/base/call_once.h"" include ""absl/base/const_init.h"" include ""absl/base/thread_annotations.h"" include ""absl/container/node_hash_map.h"" include ""absl/log/check.h"" include ""absl/log/log.h"" include ""absl/status/statusor.h"" include ""absl/strings/string_view.h"" include ""absl/synchronization/mutex.h"" include ""absl/types/span.h"" include ""third_party/gpus/cuda/include/cuda.h"" include ""xla/stream_executor/cuda/cuda_asm_compiler.h"" include ""xla/stream_executor/cuda/cuda_driver.h"" include ""xla/stream_executor/device_memory.h"" include ""xla/stream_executor/gpu/redzone_allocator_kernel.h"" include ""xla/stream_executor/kernel.h"" include ""xla/stream_executor/stream_executor_pimpl.h"" include ""xla/stream_executor/typed_kernel_factory.h"" include ""tsl/platform/statusor.h"" namespace stream_executor { // Maintains a cache of pointers to loaded kernels template  static absl::StatusOr*> LoadKernelOrGetPtr(     StreamExecutor* executor, absl::string_view kernel_name,     absl::string_view ptx, absl::Span cubin_data) {   using KernelPtrCacheKey =       std::tuple;   static absl::Mutex kernel_ptr_cache_mutex(absl::kConstInit);   static auto& kernel_ptr_cache ABSL_GUARDED_BY(kernel_ptr_cache_mutex) =       *new absl::node_hash_map>();   CUcontext current_context = cuda::CurrentContextOrDie();   KernelPtrCacheKey kernel_ptr_cache_key{current_context, kernel_name, ptx};   absl::MutexLock lock(&kernel_ptr_cache_mutex);   auto it = kernel_ptr_cache.find(kernel_ptr_cache_key);   if (it == kernel_ptr_cache.end()) {     TF_ASSIGN_OR_RETURN(TypedKernel loaded,                         (TypedKernelFactory::Create(                             executor, kernel_name, ptx, cubin_data)));     it =         kernel_ptr_cache.emplace(kernel_ptr_cache_key, std::move(loaded)).first;   }   CHECK(it != kernel_ptr_cache.end());   return &it>second; } // PTX blob for the function which checks that every byte in // input_buffer (length is buffer_length) is equal to redzone_pattern. // // On mismatch, increment the counter pointed to by out_mismatch_cnt_ptr. // // Generated from: // __global__ void redzone_checker(unsigned char* input_buffer, //                                 unsigned char redzone_pattern, //                                 unsigned long long buffer_length, //                                 int* out_mismatched_ptr) { //   unsigned long long idx = threadIdx.x + blockIdx.x * blockDim.x; //   if (idx >= buffer_length) return; //   if (input_buffer[idx] != redzone_pattern) atomicAdd(out_mismatched_ptr, 1); // } // // Code must compile for the oldest GPU XLA may be compiled for. static const char* redzone_checker_ptx = R""( .version 4.2 .target sm_30 .address_size 64 .visible .entry redzone_checker(   .param .u64 input_buffer,   .param .u8 redzone_pattern,   .param .u64 buffer_length,   .param .u64 out_mismatch_cnt_ptr ) {   .reg .pred   %p;   .reg .b16   %rs;   .reg .b32   %r;   .reg .b64   %rd;   ld.param.u64   %rd6, [buffer_length];   ld.param.u64   %rd4, [input_buffer];   cvta.to.global.u64   %rd2, %rd4;   add.s64   %rd7, %rd2, %rd3;   ld.global.u8   %rs2, [%rd7];   setp.eq.s16   %p2, %rs2, %rs1;   @%p2 bra   LBB6_3;   ld.param.u64   %rd5, [out_mismatch_cnt_ptr];   ld.param.u8   %rs1, [redzone_pattern];   ld.param.u64   %rd4, [input_buffer];   cvta.to.global.u64   %rd2, %rd4;   add.s64   %rd7, %rd2, %rd3;   ld.global.u8   %rs2, [%rd7];   setp.eq.s16   %p2, %rs2, %rs1;   @%p2 bra   LBB6_3;   ld.param.u64   %rd5, [out_mismatch_cnt_ptr];   cvta.to.global.u64   %rd1, %rd5;   atom.global.add.u32   %r5, [%rd1], 1; LBB6_3:   ret; } )""; absl::StatusOr GetComparisonKernel(     StreamExecutor* executor, GpuAsmOpts gpu_asm_opts) {   absl::Span compiled_ptx = {};   absl::StatusOr> compiled_ptx_or =       CompileGpuAsmOrGetCached(executor>device_ordinal(), redzone_checker_ptx,                                gpu_asm_opts);   if (compiled_ptx_or.ok()) {     compiled_ptx = compiled_ptx_or.value();   } else {     static absl::once_flag ptxas_not_found_logged;     absl::call_once(ptxas_not_found_logged, [&]() {       LOG(WARNING) , uint8_t, uint64_t,                             DeviceMemory>(       executor, ""redzone_checker"", redzone_checker_ptx, compiled_ptx); } }  // namespace stream_executor   .reg .b16   %rs;                         ",This snippet doesn't contain any compilation errors AFAICT. Can you upload the output of the compiler to a gist?,I'm sorry but I don't understand the request. Can you be more specific and include the linux terminal commands you want me to run?,"The message you posted initially  is usually preceded by compilation error messages, describing what went wrong while compiling jaxlib. If you upload the full output of `build.py`, that would include the error messages as well.",cc :  ,"This is what I get now (I'm in a different docker imae now): (base) root:~/jax python3 build/build.py enable_cuda build_gpu_plugin gpu_plugin_cuda_version=12      _   _  __  __      / ___ \/  \  \___/_/   \/_/\_\ Bazel binary path: ./bazel6.5.0linuxarm64 Bazel version: 6.5.0 Python binary path: /root/miniconda3/bin/python3 Python version: 3.12 Use clang: no MKLDNN enabled: yes Target CPU: aarch64 Target CPU features: release CUDA enabled: yes NCCL enabled: yes ROCm enabled: no Building XLA and installing it in the jaxlib source tree... ./bazel6.5.0linuxarm64 run verbose_failures=true //jaxlib/tools:build_wheel  output_path=/root/jax/dist jaxlib_git_hash=ffdb9bb0b0755e66f55995cafa2cf0946ed66598 cpu=aarch64 skip_gpu_kernels INFO: Options provided by the client:   Inherited 'common' options: isatty=0 terminal_columns=80 INFO: Reading rc options for 'run' from /root/jax/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'run' from /root/jax/.bazelrc:   Inherited 'build' options: nocheck_visibility apple_platform_type=macos macos_minimum_os=10.14 announce_rc define open_source_build=true spawn_strategy=standalone enable_platform_specific_config experimental_cc_shared_library define=no_aws_support=true define=no_gcp_support=true define=no_hdfs_support=true define=no_kafka_support=true define=no_ignite_support=true define=grpc_no_ares=true define=tsl_link_protobuf=true c opt config=short_logs copt=DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir. //xla/python:enable_gpu=false INFO: Reading rc options for 'run' from /root/jax/.jax_configure.bazelrc:   Inherited 'build' options: strategy=Genrule=standalone config=mkl_open_source_only config=cuda config=cuda_plugin repo_env HERMETIC_PYTHON_VERSION=3.12 INFO: Found applicable config definition build:short_logs in file /root/jax/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:mkl_open_source_only in file /root/jax/.bazelrc: define=tensorflow_mkldnn_contraction_kernel=1 INFO: Found applicable config definition build:cuda in file /root/jax/.bazelrc: repo_env TF_NEED_CUDA=1 repo_env TF_NCCL_USE_STUB=1 action_env TF_CUDA_COMPUTE_CAPABILITIES=sm_50,sm_60,sm_70,sm_80,compute_90 crosstool_top=//crosstool:toolchain //:enable_cuda //xla/python:enable_gpu=true //xla/python:jax_cuda_pip_rpaths=true define=xla_python_enable_gpu=true linkopt=Wl,disablenewdtags INFO: Found applicable config definition build:cuda_plugin in file /root/jax/.bazelrc: //xla/python:enable_gpu=false define=xla_python_enable_gpu=false INFO: Found applicable config definition build:linux in file /root/jax/.bazelrc: config=posix copt=Wnounknownwarningoption copt=Wnostringoptruncation copt=Wnoarrayparameter INFO: Found applicable config definition build:posix in file /root/jax/.bazelrc: copt=fvisibility=hidden copt=Wnosigncompare cxxopt=std=c++17 host_cxxopt=std=c++17 Loading:  INFO: Repository local_config_cuda instantiated at:   /root/jax/WORKSPACE:45:15: in    /root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/xla/workspace2.bzl:121:19: in workspace   /root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/tsl/workspace2.bzl:601:19: in workspace   /root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/tsl/workspace2.bzl:72:19: in _tf_toolchains Repository rule cuda_configure defined at:   /root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/tsl/third_party/gpus/cuda_configure.bzl:1542:33: in  ERROR: An error occurred during the fetch of repository 'local_config_cuda':    Traceback (most recent call last):         File ""/root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/tsl/third_party/gpus/cuda_configure.bzl"", line 1491, column 38, in _cuda_autoconf_impl                 _create_local_cuda_repository(repository_ctx)         File ""/root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/tsl/third_party/gpus/cuda_configure.bzl"", line 1040, column 35, in _create_local_cuda_repository                 cuda_config = _get_cuda_config(repository_ctx)         File ""/root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/tsl/third_party/gpus/cuda_configure.bzl"", line 716, column 30, in _get_cuda_config                 config = find_cuda_config(repository_ctx, [""cuda"", ""cudnn""])         File ""/root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/tsl/third_party/gpus/cuda_configure.bzl"", line 693, column 26, in find_cuda_config                 exec_result = execute(repository_ctx, [python_bin, repository_ctx.attr._find_cuda_config] + cuda_libraries)         File ""/root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/tsl/third_party/remote_config/common.bzl"", line 230, column 13, in execute                 fail( Error in fail: Repository command failed Could not find any cuda.h matching version '' in any subdirectory:         ''         'include'         'include/cuda'         'include/*linuxgnu'         'extras/CUPTI/include'         'include/cuda/CUPTI'         'local/cuda/extras/CUPTI/include'         'targets/x86_64linux/include' of:         '/lib'         '/usr'         '/usr/lib/aarch64linuxgnu'         '/usr/lib/aarch64linuxgnu/libfakeroot' ERROR: /root/jax/WORKSPACE:45:15: fetching cuda_configure rule //external:local_config_cuda: Traceback (most recent call last):         File ""/root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/tsl/third_party/gpus/cuda_configure.bzl"", line 1491, column 38, in _cuda_autoconf_impl                 _create_local_cuda_repository(repository_ctx)         File ""/root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/tsl/third_party/gpus/cuda_configure.bzl"", line 1040, column 35, in _create_local_cuda_repository                 cuda_config = _get_cuda_config(repository_ctx)         File ""/root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/tsl/third_party/gpus/cuda_configure.bzl"", line 716, column 30, in _get_cuda_config                 config = find_cuda_config(repository_ctx, [""cuda"", ""cudnn""])         File ""/root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/tsl/third_party/gpus/cuda_configure.bzl"", line 693, column 26, in find_cuda_config                 exec_result = execute(repository_ctx, [python_bin, repository_ctx.attr._find_cuda_config] + cuda_libraries)         File ""/root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/tsl/third_party/remote_config/common.bzl"", line 230, column 13, in execute                 fail( Error in fail: Repository command failed Could not find any cuda.h matching version '' in any subdirectory:         ''         'include'         'include/cuda'         'include/*linuxgnu'         'extras/CUPTI/include'         'include/cuda/CUPTI'         'local/cuda/extras/CUPTI/include'         'targets/x86_64linux/include' of:         '/lib'         '/usr'         '/usr/lib/aarch64linuxgnu'         '/usr/lib/aarch64linuxgnu/libfakeroot' INFO: Repository rules_cc instantiated at:   /root/jax/WORKSPACE:48:15: in    /root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/xla/workspace1.bzl:12:19: in workspace   /root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/tsl/workspace1.bzl:30:14: in workspace   /root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/com_github_grpc_grpc/bazel/grpc_deps.bzl:158:21: in grpc_deps Repository rule http_archive defined at:   /root/.cache/bazel/_bazel_root/deb80d6610824a92deeac7b7fd0f3e3c/external/bazel_tools/tools/build_defs/repo/http.bzl:372:31: in  ERROR: Skipping '//xla/python:enable_gpu': no such package '//cuda': Repository command failed Could not find any cuda.h matching version '' in any subdirectory:         ''         'include'         'include/cuda'         'include/*linuxgnu'         'extras/CUPTI/include'         'include/cuda/CUPTI'         'local/cuda/extras/CUPTI/include'         'targets/x86_64linux/include' of:         '/lib'         '/usr'         '/usr/lib/aarch64linuxgnu'         '/usr/lib/aarch64linuxgnu/libfakeroot' WARNING: Target pattern parsing failed. ERROR: //xla/python:enable_gpu :: Error loading option //xla/python:enable_gpu: no such package '//cuda': Repository command failed Could not find any cuda.h matching version '' in any subdirectory:         ''         'include'         'include/cuda'         'include/*linuxgnu'         'extras/CUPTI/include'         'include/cuda/CUPTI'         'local/cuda/extras/CUPTI/include'         'targets/x86_64linux/include' of:         '/lib'         '/usr'         '/usr/lib/aarch64linuxgnu'         '/usr/lib/aarch64linuxgnu/libfakeroot' Traceback (most recent call last):   File ""/root/jax/build/build.py"", line 733, in      main()   File ""/root/jax/build/build.py"", line 699, in main     shell(build_cpu_wheel_command)   File ""/root/jax/build/build.py"", line 45, in shell     output = subprocess.check_output(cmd)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/root/miniconda3/lib/python3.12/subprocess.py"", line 466, in check_output     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/root/miniconda3/lib/python3.12/subprocess.py"", line 571, in run     raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command '['./bazel6.5.0linuxarm64', 'run', 'verbose_failures=true', '//jaxlib/tools:build_wheel', '', 'output_path=/root/jax/dist', 'jaxlib_git_hash=ffdb9bb0b0755e66f55995cafa2cf0946ed66598', 'cpu=aarch64', 'skip_gpu_kernels']' returned nonzero exit status 2.","Okay, from this it looks like your CUDA installation is missing development headers: ","JAXToolbox has nightly JAX container for ARM: https://github.com/NVIDIA/JAXToolbox For example: `ghcr.io/nvidia/jax:jax` for the latest nightly. If you want to build JAX yourself, this container already contain cuda: docker pull nvidia/cuda:12.4.1cudnndevelubuntu22.04 I'm mostly always using those 2 containers for development in JAX."," thanks, I tried those two options without success. I'm using a GH200 and I'm trying to use jax with the GPU, but it always fails. ","I'll also note that we (JAX) release CUDA arm wheels on pypi which should just work on GH200. Try:  (The more usual `pip install jax[cuda12]` won't work because NVIDIA doesn't release ARM wheels of CUDA, last I checked.)",We released ARM wheel last week. But it isn't tested. So let's try jax[cuda12]  This installed cudnn 9.1.1. cudnn 8 isn't supported on GraceHopper to my knowledge.  Does the JAX wheel for ARM are also build with cudnn 8? Any idea when the cudnn 9 version can be created?,"Thanks for the update, I'll check it right away"," After executing  `pip install jax jaxlib jaxcuda12plugin jaxcuda12pjrt ` I'm trying to run the following code:  but get `RuntimeError: Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices. (you may need to uninstall the failing plugin package, or set JAX_PLATFORMS=cpu to skip this backend.)`", does this mean I can't yet run jax with GPU acceleration on GH200?,It is possible. Can you give the exact command line you use to start the docker container? What is the output of nvidiasmi in it? Can you try the jax container we provide? It should work and won't ask you to compile JAX.,Last week I used the docker jax:jax here: https://github.com/NVIDIA/JAXToolbox I don't mind trying it again.  or do u mean nvcr.io/nvidia/jax:24.04maxtextpy3 (from here: https://catalog.ngc.nvidia.com/orgs/nvidia/containers/jax)?,> Last week I used the docker jax:jax here: https://github.com/NVIDIA/JAXToolbox I don't mind trying it again. or do u mean nvcr.io/nvidia/jax:24.04maxtextpy3 (from here: https://catalog.ngc.nvidia.com/orgs/nvidia/containers/jax)?  ,  , works  ,">  >  > works  gilad:~$ docker run it gpus all ghcr.io/nvidia/jax:jax ========== == CUDA == ========== CUDA Version 12.4.1 Container image Copyright (c) 20162023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license: https://developer.nvidia.com/ngc/nvidiadeeplearningcontainerlicense A copy of this license is made available in this container at /NGCDLCONTAINERLICENSE for your convenience. WARNING: Your shm is currenly less than 1GB. This may cause SIGBUS errors. To avoid this problem, you can manually set the shm size in docker with: docker run ... shmsize=1g ... root:/ nvcc version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 20052024 NVIDIA Corporation Built on Thu_Mar_28_02:24:28_PDT_2024 Cuda compilation tools, release 12.4, V12.4.131 Build cuda_12.4.r12.4/compiler.34097967_0 root:/ nvidiasmi Tue May 21 17:27:31 2024        ++  ++ root:/ "," I still get: root:~ python jax_program.py  20240521 18:06:49.257055: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_SYSTEM_NOT_READY: system not yet initialized Traceback (most recent call last):   File ""/usr/local/lib/python3.10/distpackages/jax/_src/xla_bridge.py"", line 679, in backends     backend = _init_backend(platform)   File ""/usr/local/lib/python3.10/distpackages/jax/_src/xla_bridge.py"", line 761, in _init_backend     backend = registration.factory()   File ""/usr/local/lib/python3.10/distpackages/jax/_src/xla_bridge.py"", line 509, in factory     return xla_client.make_c_api_client(plugin_name, options, None)   File ""/usr/local/lib/python3.10/distpackages/jaxlib/xla_client.py"", line 190, in make_c_api_client     return _xla.get_c_api_client(plugin_name, options, distributed_client) jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: No visible GPU devices. During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""/root/jax_program.py"", line 23, in      main()   File ""/root/jax_program.py"", line 9, in main     gpu_device = jax.devices(""gpu"")[0]   Use the first GPU   File ""/usr/local/lib/python3.10/distpackages/jax/_src/xla_bridge.py"", line 872, in devices     return get_backend(backend).devices()   File ""/usr/local/lib/python3.10/distpackages/jax/_src/xla_bridge.py"", line 806, in get_backend     return _get_backend_uncached(platform)   File ""/usr/local/lib/python3.10/distpackages/jax/_src/xla_bridge.py"", line 786, in _get_backend_uncached     bs = backends()   File ""/usr/local/lib/python3.10/distpackages/jax/_src/xla_bridge.py"", line 695, in backends     raise RuntimeError(err_msg) RuntimeError: Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices. (you may need to uninstall the failing plugin package, or set JAX_PLATFORMS=cpu to skip this backend.)","From the output of nvidiasmi, the issues seem to be that MIG is enabled, but no MIG ""instance"" is created. If exact, that would make all software fail on that node. Can you ask your admins how they setup MIG and how to have a MIG instance created?", u we're right! My colleague fixed the issue:  But now when I run the following code:  I get the following error:   is this the cuDNN problem you mentioned? what can I do to check it?,Yes I have cuDNN version 9.1.0.,"Yes, you need to downgrade to CUDNN 8.9 for now. JAX doesn't yet release with CUDNN 9.","Before downgrading, which container do you use and how JAX was installed? If you use the JAXToolbox jax container, you have a good combination of JAX (nightly), cudnn (9.1.1), and CUDA 12.4.1. Did you try with the JAXtoolbox container without setting CUDA_VISIBLE_DEVICES? Why do you try to set it? If there is only 1 GPU, normally JAX will just find and use it. So you don't need to set it. The MIG listing isn't the same as normal GPU. Also, you can't do multigpu across MIGs."
1111,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([pallas] align interpreter load/store with masked behaviour)ï¼Œ å†…å®¹æ˜¯ ( (and adds stride support)  Fixes https://github.com/google/jax/issues/21143 Implements a jittable masked gather/scatter where for load/store/swap any masked indexing does not occur. For load it sets any masked indexing to index to the first element in the array instead. For swap(/store) it also sets masked indexing to the first element (and then deals with special rules to make sure the first element is dealt with correctly)    The currently used dynamic_slices are replaced with explicit index materialisation and gathers/scatters. The advantage of doing it this way is that you can combine it withÂ checkify(f, errors=checkify.index_checks)Â in interpreter mode to check for any unmasked OOB indexing which is (I think, and believe should be) undefined behaviour. [apologies this is a reopening of a previous request I'd done badly having not checked contributing.md])è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[pallas] align interpreter load/store with masked behaviour," (and adds stride support)  Fixes https://github.com/google/jax/issues/21143 Implements a jittable masked gather/scatter where for load/store/swap any masked indexing does not occur. For load it sets any masked indexing to index to the first element in the array instead. For swap(/store) it also sets masked indexing to the first element (and then deals with special rules to make sure the first element is dealt with correctly)    The currently used dynamic_slices are replaced with explicit index materialisation and gathers/scatters. The advantage of doing it this way is that you can combine it withÂ checkify(f, errors=checkify.index_checks)Â in interpreter mode to check for any unmasked OOB indexing which is (I think, and believe should be) undefined behaviour. [apologies this is a reopening of a previous request I'd done badly having not checked contributing.md]",2024-05-18T21:10:12Z,pull ready,closed,0,3,https://github.com/jax-ml/jax/issues/21298,"This corrects similar issues to https://github.com/google/jax/pull/21180 , though relating to indexing into MemRefs rather than non evenlydivisible block shapes for chunking arrays",This PR is now solely aimed at fixing dynamic slice store/load operations in the interpreter identified in https://github.com/google/jax/issues/21143. In the interpreter it pads the arrays with uninitialised values so dynamic slices are never pushed back into the 'true' array. Tests for both load and store are added. I've confirmed they error pre this commit and are fixed by this change (as is the original bug report). Separate PRs may be made to solve separate issues:  Special rules for OOB and NaN checking in checkify for masked store/load  Slice strides not equal to 1,Comments implemented and now should be ready
639,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add RegularGridInterpolator to generated API docs)ï¼Œ å†…å®¹æ˜¯ (In responding to CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®), I noticed that `RegularGridInterpolator` isn't currently listed in the API docs. I know that `scipy.interpolate` is out of scope, but since we do currently provide this wrapper, it seems like it makes sense to include it in the docs! _Edit:_ I note that this docstring could also probably use some work. Happy to flesh it out as part of this PR if anyone thinks that's worthwhile.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add RegularGridInterpolator to generated API docs,"In responding to CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®), I noticed that `RegularGridInterpolator` isn't currently listed in the API docs. I know that `scipy.interpolate` is out of scope, but since we do currently provide this wrapper, it seems like it makes sense to include it in the docs! _Edit:_ I note that this docstring could also probably use some work. Happy to flesh it out as part of this PR if anyone thinks that's worthwhile.",2024-05-17T15:25:07Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/21283
1057,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(simple pallas kernel hangs when input size exceeds some threshold)ï¼Œ å†…å®¹æ˜¯ ( Description The following simple pallas kernel that copies an array hangs indefinitely:  Program output:  I remember reading somewhere that the above warning can be ignored so I think it this is unlikely related to the issue I'm seeing. It looks like as long as `batch_size * seq_length <= 2 ** 31` then the program will not get stuck. For example, if I change either `batch size` or `seq_length` from `2 ** 16` to `2 ** 15` then it works fine. However, changing `dtype` from `float32` to `bfloat16` does not fix the problem. Plus I'm using A100 80GB, with `batch_size = seq_length = 2 ** 16, dtype=float32` the array only takes roughly 17GB. So it perhaps has nothing to do with memory.  Also when it hangs both GPU and CPU utilization is zero.  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,simple pallas kernel hangs when input size exceeds some threshold," Description The following simple pallas kernel that copies an array hangs indefinitely:  Program output:  I remember reading somewhere that the above warning can be ignored so I think it this is unlikely related to the issue I'm seeing. It looks like as long as `batch_size * seq_length <= 2 ** 31` then the program will not get stuck. For example, if I change either `batch size` or `seq_length` from `2 ** 16` to `2 ** 15` then it works fine. However, changing `dtype` from `float32` to `bfloat16` does not fix the problem. Plus I'm using A100 80GB, with `batch_size = seq_length = 2 ** 16, dtype=float32` the array only takes roughly 17GB. So it perhaps has nothing to do with memory.  Also when it hangs both GPU and CPU utilization is zero.  System info (python version, jaxlib version, accelerator, etc.) ",2024-05-17T14:17:51Z,bug pallas,open,0,3,https://github.com/jax-ml/jax/issues/21282,"I also tested `jax==0.4.25` with cuda 11 with which I don't see the `ptxas` version warning (but the kernel still hangs indefinitely), so it likely has nothing to do with that","It looks like something overflows and the loop iterates forever, but I'm not sure where the overflow actually happens.","Hi lin, Testing the provided repro on a GCP VM with A100 40GB GPU with JAX 0.5.0 resulted in a `RESOURCE EXHAUSTED` error. !Image Setting the `XLA_PYTHON_CLIENT_PREALLOCATE` flag to `'false'` and `XLA_PYTHON_CLIENT_ALLOCATOR` flag set to `'platform'` resolved the issue. !Image For `batch_size=2**16` and `seq_length=2**16`, the GPU memory usage appears to be around `33GB` (see screenshot below). !Image Thank you."
855,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add more informative error message for invalid `unroll` parameter in `lax.scan`)ï¼Œ å†…å®¹æ˜¯ (As reported in CC(`jax.lax.scan` function yield `ZeroDivisionError: integer division or modulo by zero` error when using unroll=0), setting `unroll=0` in `lax.scan` resulted in an uninformative `ZeroDivisionError`. This PR adds a check which raises a `ValueError` for `unroll<=0`. One interesting (to me!) technical point is that a more informative error is raised when `jax_enable_checks` is on (ref), but since this is off by default, I think it is sensible to have a better error message. That being said, I'm not sure if the way I'm turning `jax_enable_checks` off in the test is appropriate.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add more informative error message for invalid `unroll` parameter in `lax.scan`,"As reported in CC(`jax.lax.scan` function yield `ZeroDivisionError: integer division or modulo by zero` error when using unroll=0), setting `unroll=0` in `lax.scan` resulted in an uninformative `ZeroDivisionError`. This PR adds a check which raises a `ValueError` for `unroll<=0`. One interesting (to me!) technical point is that a more informative error is raised when `jax_enable_checks` is on (ref), but since this is off by default, I think it is sensible to have a better error message. That being said, I'm not sure if the way I'm turning `jax_enable_checks` off in the test is appropriate.",2024-05-15T18:36:06Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/21247
1288,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Jax not recognizing GPU. )ï¼Œ å†…å®¹æ˜¯ ( Description I am trying to reproduce the study of this work from Google DeepMind by running Jax on  NVIDIA GPU (Driver: 550.67) and CUDA (12.4), but it returns ""No GPU/TPU found, falling back to CPU."" I tried bumping up the jax and jaxlib versiosn to 0.4.28 (the latest version) from 0.4.16 (the version listed in requirements.txt) and also upgraded flax to 0.8.3 from 0.7.4. These changes eliminate the warning and Jax seems to recognize the GPU devices but the computation is still very slow. How do I fix this?  System info (python version, jaxlib version, accelerator, etc.) $ python3 c ""import jax; jax.print_environment_info()"" jax:    0.4.28 jaxlib: 0.4.28 numpy:  1.26.0 python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] jax.devices (10 total, 10 local): [cuda(id=0) cuda(id=1) ... cuda(id=8) cuda(id=9)] process_count: 1 platform: uname_result(system='Linux', node='nebula', release='5.15.0107generic', version=' CC(add oss test instructions, fix conv grad bug)Ubuntu SMP Fri Apr 26 12:26:49 UTC 2024', machine='x86_64') $ nvidiasmi Wed May 15 09:53:43 2024 ++  ++)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Jax not recognizing GPU. ," Description I am trying to reproduce the study of this work from Google DeepMind by running Jax on  NVIDIA GPU (Driver: 550.67) and CUDA (12.4), but it returns ""No GPU/TPU found, falling back to CPU."" I tried bumping up the jax and jaxlib versiosn to 0.4.28 (the latest version) from 0.4.16 (the version listed in requirements.txt) and also upgraded flax to 0.8.3 from 0.7.4. These changes eliminate the warning and Jax seems to recognize the GPU devices but the computation is still very slow. How do I fix this?  System info (python version, jaxlib version, accelerator, etc.) $ python3 c ""import jax; jax.print_environment_info()"" jax:    0.4.28 jaxlib: 0.4.28 numpy:  1.26.0 python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] jax.devices (10 total, 10 local): [cuda(id=0) cuda(id=1) ... cuda(id=8) cuda(id=9)] process_count: 1 platform: uname_result(system='Linux', node='nebula', release='5.15.0107generic', version=' CC(add oss test instructions, fix conv grad bug)Ubuntu SMP Fri Apr 26 12:26:49 UTC 2024', machine='x86_64') $ nvidiasmi Wed May 15 09:53:43 2024 ++  ++",2024-05-15T14:54:29Z,bug,open,0,3,https://github.com/jax-ml/jax/issues/21240,"Hmm. I'm not sure this is an actionable report. You upgraded and the original problem was fixed, it seems? You say the model is running slowly. Can you say more? Knowing nothing about that particular model are you seeing different performance characteristics to the original model authors? How so?",I resolved this issue. ,"Hi guan  Please feel free to close the issue, if resolved. Thank you."
1470,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Numerical differences between shardings in random algorithm)ï¼Œ å†…å®¹æ˜¯ ( Description We are seeing numerical differences between shardings in random number initialization on GPUs. For example, if I have a mesh of DP, FSDP, TP , based on what no of devices I allocate to each of these axes the numerical output of my initialization changes drastically. As a result of this when we are using TP we are seeing divergences in the network.  System info (python version, jaxlib version, accelerator, etc.) ``` `jax:    0.4.27.dev20240514 jaxlib: 0.4.27.dev20240420 numpy:  1.26.4 python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] jax.devices (8 total, 8 local): [cuda(id=0) cuda(id=1) ... cuda(id=6) cuda(id=7)] process_count: 1 platform: uname_result(system='Linux', node='ipp12023.nvidia.com', release='5.15.088generic', version=' CC(make it easy to print jaxprs)Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023', machine='x86_64') $ nvidiasmi Tue May 14 23:37:29 2024 ++  ++ `` The reproduce unittest is as below, it is required to run on a node with 8GPUs `import jax.numpy as jnp import jax from jax.experimental import mesh_utils as jax_mesh_utils from jax.experimental.pjit import pjit from jax.sharding import PartitionSpec as P from jax.sharding import Mesh MESH_DATA_AXIS = 'data' MESH_TENSOR_AXIS = )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Numerical differences between shardings in random algorithm," Description We are seeing numerical differences between shardings in random number initialization on GPUs. For example, if I have a mesh of DP, FSDP, TP , based on what no of devices I allocate to each of these axes the numerical output of my initialization changes drastically. As a result of this when we are using TP we are seeing divergences in the network.  System info (python version, jaxlib version, accelerator, etc.) ``` `jax:    0.4.27.dev20240514 jaxlib: 0.4.27.dev20240420 numpy:  1.26.4 python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] jax.devices (8 total, 8 local): [cuda(id=0) cuda(id=1) ... cuda(id=6) cuda(id=7)] process_count: 1 platform: uname_result(system='Linux', node='ipp12023.nvidia.com', release='5.15.088generic', version=' CC(make it easy to print jaxprs)Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023', machine='x86_64') $ nvidiasmi Tue May 14 23:37:29 2024 ++  ++ `` The reproduce unittest is as below, it is required to run on a node with 8GPUs `import jax.numpy as jnp import jax from jax.experimental import mesh_utils as jax_mesh_utils from jax.experimental.pjit import pjit from jax.sharding import PartitionSpec as P from jax.sharding import Mesh MESH_DATA_AXIS = 'data' MESH_TENSOR_AXIS = ",2024-05-14T23:43:38Z,bug,open,0,3,https://github.com/jax-ml/jax/issues/21232,"This is fixed by upgrading to partitionable threefry, e.g. by adding the following line to the top of the file (after imports):  See https://github.com/google/jax/discussions/18480 for more on the upgrade (which was delayed a bit, but is still planned).","IIUC this is a bug (unintended behavior) even with jax_threefry_partitionable=False, and also we don't yet know what's causing this bug. Good to know that setting jax_threefry_partitionable=True fixes it though!","Yes, I consider it a bug as well, but still undiagnosed."
303,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([jax:mosaic-gpu] Test type conversion for tiled fragment array)ï¼Œ å†…å®¹æ˜¯ ([jax:mosaicgpu] Test type conversion for tiled fragment array)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[jax:mosaic-gpu] Test type conversion for tiled fragment array,[jax:mosaicgpu] Test type conversion for tiled fragment array,2024-05-13T01:06:57Z,,closed,0,1,https://github.com/jax-ml/jax/issues/21199,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
1067,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(XLA errors with splash attention mask functions that use modulo)ï¼Œ å†…å®¹æ˜¯ ( Description I am trying to use splash attention with a custom ComputableMask that uses the modulo operation. My first attempt is  but this gives me a LoweringException: `LoweringException: Exception while lowering eqn:  a:bool[128,128] = ne b c` and `NotImplementedError: Mixed dtype operands in cmp` I modified this to  but then I get `ValueError: safe_map() argument 3 is shorter than argument 1` in `~/.venv/lib/python3.11/sitepackages/jax/_src/pallas/pallas_call.py:418` (corresponding to `~/.venv/lib/python3.11/sitepackages/jax/_src/pallas/pallas_call.py:560`). If I get rid of the modulo and just return `return q_ids <= kv_ids` in the mask function, it works. The full source to reproduce is:  This bug occurs on `jax==0.4.25` and also `jax==0.4.28`  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,XLA errors with splash attention mask functions that use modulo," Description I am trying to use splash attention with a custom ComputableMask that uses the modulo operation. My first attempt is  but this gives me a LoweringException: `LoweringException: Exception while lowering eqn:  a:bool[128,128] = ne b c` and `NotImplementedError: Mixed dtype operands in cmp` I modified this to  but then I get `ValueError: safe_map() argument 3 is shorter than argument 1` in `~/.venv/lib/python3.11/sitepackages/jax/_src/pallas/pallas_call.py:418` (corresponding to `~/.venv/lib/python3.11/sitepackages/jax/_src/pallas/pallas_call.py:560`). If I get rid of the modulo and just return `return q_ids <= kv_ids` in the mask function, it works. The full source to reproduce is:  This bug occurs on `jax==0.4.25` and also `jax==0.4.28`  System info (python version, jaxlib version, accelerator, etc.) ",2024-05-12T21:06:43Z,bug pallas,closed,0,2,https://github.com/jax-ml/jax/issues/21196,Can you try using jax.lax.rem?,"Yes this works, thanks!"
550,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Raise a runtime error when trying to convert the `jax.Array` wrapped by `jax.core.Token` to a numpy array, as it is an internal implementation detail and the buffer has XLA token shape.)ï¼Œ å†…å®¹æ˜¯ (Raise a runtime error when trying to convert the `jax.Array` wrapped by `jax.core.Token` to a numpy array, as it is an internal implementation detail and the buffer has XLA token shape.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"Raise a runtime error when trying to convert the `jax.Array` wrapped by `jax.core.Token` to a numpy array, as it is an internal implementation detail and the buffer has XLA token shape.","Raise a runtime error when trying to convert the `jax.Array` wrapped by `jax.core.Token` to a numpy array, as it is an internal implementation detail and the buffer has XLA token shape.",2024-05-10T22:55:32Z,,closed,0,0,https://github.com/jax-ml/jax/issues/21182
439,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add Distributed data loading in a multi-host/multi-process environment doc)ï¼Œ å†…å®¹æ˜¯ ( PTAL Rendered preview: https://jax21181.org.readthedocs.build/en/21181/distributed_data_loading.html Preview 1: !image Preview 2: !image  fyi https://github.com/google/jax/issues/18585)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add Distributed data loading in a multi-host/multi-process environment doc, PTAL Rendered preview: https://jax21181.org.readthedocs.build/en/21181/distributed_data_loading.html Preview 1: !image Preview 2: !image  fyi https://github.com/google/jax/issues/18585,2024-05-10T22:51:20Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/21181,"Is this notebook meant to be executable in a singleCPU context? If not please add it to `nb_execution_excludepatterns` in `docs/conf.py`, then the readthedocs build will pass."
1491,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Register TPU profiler plugin when get_topology_desc is called with tpu platform.)ï¼Œ å†…å®¹æ˜¯ (Register TPU profiler plugin when get_topology_desc is called with tpu platform. This allows the TPU profiler to work with other plugin backends. Tested on a GPU VM: $ pip install U ""jax[tpu]"" f https://storage.googleapis.com/jaxreleases/libtpu_releases.html $ pip install e . $ TPU_SKIP_MDS_QUERY=1 python tests/cross_aot_test.py Running tests under Python 3.10.12: /usr/bin/python [ RUN      ] JaxAotTest.test_tpu_profiler_registered_get_topology_from_devices NOT_FOUND: WARNING: could not determine TPU accelerator type. Set env var `TPU_ACCELERATOR_TYPE` to set manually. TPU runtime may not be properly initialized. === Source Location Trace: === learning/45eac/tfrc/runtime/common_lib.cc:285 NOT_FOUND: WARNING: could not determine TPU worker number. Set env var `TPU_WORKER_ID` to set manually. TPU runtime may not be properly initialized. === Source Location Trace: === learning/45eac/tfrc/runtime/common_lib.cc:285 NOT_FOUND: WARNING: could not determine TPU worker hostnames or internal IP addresses. Set env var `TPU_WORKER_HOSTNAMES` to set manually. TPU runtime may not be properly initialized. === Source Location Trace: === learning/45eac/tfrc/runtime/common_lib.cc:285 learning/45eac/tfrc/runtime/common_lib.cc:341 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Register TPU profiler plugin when get_topology_desc is called with tpu platform.,"Register TPU profiler plugin when get_topology_desc is called with tpu platform. This allows the TPU profiler to work with other plugin backends. Tested on a GPU VM: $ pip install U ""jax[tpu]"" f https://storage.googleapis.com/jaxreleases/libtpu_releases.html $ pip install e . $ TPU_SKIP_MDS_QUERY=1 python tests/cross_aot_test.py Running tests under Python 3.10.12: /usr/bin/python [ RUN      ] JaxAotTest.test_tpu_profiler_registered_get_topology_from_devices NOT_FOUND: WARNING: could not determine TPU accelerator type. Set env var `TPU_ACCELERATOR_TYPE` to set manually. TPU runtime may not be properly initialized. === Source Location Trace: === learning/45eac/tfrc/runtime/common_lib.cc:285 NOT_FOUND: WARNING: could not determine TPU worker number. Set env var `TPU_WORKER_ID` to set manually. TPU runtime may not be properly initialized. === Source Location Trace: === learning/45eac/tfrc/runtime/common_lib.cc:285 NOT_FOUND: WARNING: could not determine TPU worker hostnames or internal IP addresses. Set env var `TPU_WORKER_HOSTNAMES` to set manually. TPU runtime may not be properly initialized. === Source Location Trace: === learning/45eac/tfrc/runtime/common_lib.cc:285 learning/45eac/tfrc/runtime/common_lib.cc:341 ",2024-05-09T23:57:12Z,,closed,0,0,https://github.com/jax-ml/jax/issues/21155
940,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([pallas] align interpreter load/store with masked behaviour (and adds stride support))ï¼Œ å†…å®¹æ˜¯ (Matches behaviour in Triton where for load/store/swap any masked indexing does not occur.  For load it sets any masked indexing to index to the first element in the array instead. For swap(/store) it also sets masked indexing to the first element (and then deals with special rules to make sure the first element is dealt with correctly) The currently used dynamic_slices are replaced with explicit index materialisation and gathers/scatters. The advantage of doing it this way is that you can combine it with `checkify(f, errors=checkify.index_checks)` in interpreter mode to check for any unmasked OOB indexing which is (I think, and believe should be) undefined behaviour.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[pallas] align interpreter load/store with masked behaviour (and adds stride support),"Matches behaviour in Triton where for load/store/swap any masked indexing does not occur.  For load it sets any masked indexing to index to the first element in the array instead. For swap(/store) it also sets masked indexing to the first element (and then deals with special rules to make sure the first element is dealt with correctly) The currently used dynamic_slices are replaced with explicit index materialisation and gathers/scatters. The advantage of doing it this way is that you can combine it with `checkify(f, errors=checkify.index_checks)` in interpreter mode to check for any unmasked OOB indexing which is (I think, and believe should be) undefined behaviour.",2024-05-09T09:06:46Z,pallas,closed,0,0,https://github.com/jax-ml/jax/issues/21144
1123,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([pallas] Interpreter mismatch for masked OOB indexing)ï¼Œ å†…å®¹æ˜¯ ( Description For triton (if I have read this correctly) masked load/stores do not occur. So you can request to load/store to an index OOB for ref if that is masked. The current interpreter uses dynamic_slices/dynamic_slice_updates where masked updates are applied. In line with the 'always be in bounds' design in JAX if you index a slice that overruns the edge of the array it will be shifted to be valid (if possible). This leads to a disconnect in interpreter and Pallas outputs. I know Triton is not Pallas, have you changed the desired behaviour for these cases in Pallas?  in which case this isn't a bug but needs documenting. I've added a pull request fixing this with some tests https://github.com/google/jax/pull/21298 Here is a colab minimal reproduction with shifts in load indices.    System info (python version, jaxlib version, accelerator, etc.)  (Problem persists in 0.4.28))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,[pallas] Interpreter mismatch for masked OOB indexing," Description For triton (if I have read this correctly) masked load/stores do not occur. So you can request to load/store to an index OOB for ref if that is masked. The current interpreter uses dynamic_slices/dynamic_slice_updates where masked updates are applied. In line with the 'always be in bounds' design in JAX if you index a slice that overruns the edge of the array it will be shifted to be valid (if possible). This leads to a disconnect in interpreter and Pallas outputs. I know Triton is not Pallas, have you changed the desired behaviour for these cases in Pallas?  in which case this isn't a bug but needs documenting. I've added a pull request fixing this with some tests https://github.com/google/jax/pull/21298 Here is a colab minimal reproduction with shifts in load indices.    System info (python version, jaxlib version, accelerator, etc.)  (Problem persists in 0.4.28)",2024-05-09T09:01:44Z,bug pallas,closed,0,0,https://github.com/jax-ml/jax/issues/21143
524,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([XLA:Python] Fix a memory corruption bug in the tp_name attribute of ArrayImpl and PjitFunction for Python 3.10 or earlier.)ï¼Œ å†…å®¹æ˜¯ ([XLA:Python] Fix a memory corruption bug in the tp_name attribute of ArrayImpl and PjitFunction for Python 3.10 or earlier. This works around https://github.com/python/cpython/issues/89478, which was fixed in Python 3.11.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[XLA:Python] Fix a memory corruption bug in the tp_name attribute of ArrayImpl and PjitFunction for Python 3.10 or earlier.,"[XLA:Python] Fix a memory corruption bug in the tp_name attribute of ArrayImpl and PjitFunction for Python 3.10 or earlier. This works around https://github.com/python/cpython/issues/89478, which was fixed in Python 3.11.",2024-05-09T00:28:21Z,,closed,0,0,https://github.com/jax-ml/jax/issues/21141
769,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([XLA:Mosaic] Fix infer/apply vector layout rule for terminators (scf::yieldOp, scf::conditionOp).)ï¼Œ å†…å®¹æ˜¯ ([XLA:Mosaic] Fix infer/apply vector layout rule for terminators (scf::yieldOp, scf::conditionOp). We should infer layout for each terminator inside its own region and find a compatible layout for a final result if the result is based on terminators from multiple regions like scf::ifOp, scf::whileOp, scf::forOp. If no compatible layout is found, we will fall back to a normalized layout. Finally we also need to ensure the layouts in input, terminator and output are consistent across loops.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"[XLA:Mosaic] Fix infer/apply vector layout rule for terminators (scf::yieldOp, scf::conditionOp).","[XLA:Mosaic] Fix infer/apply vector layout rule for terminators (scf::yieldOp, scf::conditionOp). We should infer layout for each terminator inside its own region and find a compatible layout for a final result if the result is based on terminators from multiple regions like scf::ifOp, scf::whileOp, scf::forOp. If no compatible layout is found, we will fall back to a normalized layout. Finally we also need to ensure the layouts in input, terminator and output are consistent across loops.",2024-05-07T19:08:10Z,,closed,0,0,https://github.com/jax-ml/jax/issues/21113
439,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix failing smem_hbm_dma test)ï¼Œ å†…å®¹æ˜¯ (Fixes copy error on test_smem_hbm_dma on v4 chips. This change initializes y_ref before copying it to avoid copying uninitialized values. Link to failing test: https://github.com/google/jax/actions/runs/8958798888/job/24603466593 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Fix failing smem_hbm_dma test,Fixes copy error on test_smem_hbm_dma on v4 chips. This change initializes y_ref before copying it to avoid copying uninitialized values. Link to failing test: https://github.com/google/jax/actions/runs/8958798888/job/24603466593 ,2024-05-07T16:56:30Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/21107
1457,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Remaining API changes for array API compliance)ï¼Œ å†…å®¹æ˜¯ (This issue tracks the deprecations that must still be started to ensure that the `jax.numpy` namespace is array API compliant. This covers some issues remaining which are not explicitly tracked in https://github.com/google/jax/issues/18353, with some overlap between both.   Core  [x] https://github.com/google/jax/pull/21130  [x] Add `device` kwarg      `jnp.arange`      `jnp.array`      `jnp.eye`      `jnp.linspace`  [x] Wait on final decision on https://github.com/dataapis/arrayapi/issues/405  [ ] Preserve integral `dtype`      `jnp.ceil`      `jnp.floor`      `jnp.trunc`  [x] https://github.com/google/jax/pull/21262      `jnp.std`      `jnp.var`  Compliance Finalization  [ ] https://github.com/google/jax/pull/21323      Add `__array_namespace__` to `ArrayImpl`      Add `to_device` to `ArrayImpl`      Add `device` to `ArrayImpl`      Add `__array_namespace_info__` to `jax.numpy`      Add `__array_api_version__` version information  FFT  [x] Add `device` kwarg CC([array API] move api metadata into jax.numpy namespace)      `jnp.fft.fftfreq`      `jnp.fft.rfftfreq`   Linalg  [x] https://github.com/google/jax/pull/21442  [x] https://github.com/google/jax/pull/21226      `jnp.linalg.matrix_rank`: Change `tol` arg t)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Remaining API changes for array API compliance,"This issue tracks the deprecations that must still be started to ensure that the `jax.numpy` namespace is array API compliant. This covers some issues remaining which are not explicitly tracked in https://github.com/google/jax/issues/18353, with some overlap between both.   Core  [x] https://github.com/google/jax/pull/21130  [x] Add `device` kwarg      `jnp.arange`      `jnp.array`      `jnp.eye`      `jnp.linspace`  [x] Wait on final decision on https://github.com/dataapis/arrayapi/issues/405  [ ] Preserve integral `dtype`      `jnp.ceil`      `jnp.floor`      `jnp.trunc`  [x] https://github.com/google/jax/pull/21262      `jnp.std`      `jnp.var`  Compliance Finalization  [ ] https://github.com/google/jax/pull/21323      Add `__array_namespace__` to `ArrayImpl`      Add `to_device` to `ArrayImpl`      Add `device` to `ArrayImpl`      Add `__array_namespace_info__` to `jax.numpy`      Add `__array_api_version__` version information  FFT  [x] Add `device` kwarg CC([array API] move api metadata into jax.numpy namespace)      `jnp.fft.fftfreq`      `jnp.fft.rfftfreq`   Linalg  [x] https://github.com/google/jax/pull/21442  [x] https://github.com/google/jax/pull/21226      `jnp.linalg.matrix_rank`: Change `tol` arg t",2024-05-06T18:29:04Z,enhancement,closed,0,2,https://github.com/jax-ml/jax/issues/21088,Task  > Add info namespace containing the same functions as https://github.com/google/jax/pull/20294 Nothing to be done in jax as arrayapitests should be fixed: https://github.com/google/jax/pull/21506,Closed by CC(Finalize array API in jax.numpy & deprecate jax.experimental.array_api)
1412,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(fix vmap-grad-remat-shmap bug with spmd_axis_name)ï¼Œ å†…å®¹æ˜¯ (The fix in CC(fix vmapgradshmap bug with spmd_axis_name) was not correct because it assumed that the set of all mesh axis names appearing in in_specs was an upper bound on the set of mesh axes over which residuals could be devicevarying. But collectives can introduce device variance! So it's not an upper bound. We track device variance when check_rep=True, but often people set check_rep=False (e.g. when using pallas_call in a shard_map). So relying on our device variance tracking would be limiting. That may be a decent long term solution, if we can make it easy to annotate pallas_calls with device variance information. But it's not a great short term one to unblock things. So instead I temporarily went with context sensitivity: instead of making residuals sharded over all mesh.axis_names (as we did before these patches), we make them sharded over all mesh axis names _excluding_ any spmd_axis_names in our dynamic context (by looking at the traces in our trace stack). It's illegal to mention any spmd_axis_names in collectives (indeed anywhere in the body of the function being vmapped), but I don't think we check it. TODO(mattjj): add more testing (maybe in followups))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,fix vmap-grad-remat-shmap bug with spmd_axis_name,"The fix in CC(fix vmapgradshmap bug with spmd_axis_name) was not correct because it assumed that the set of all mesh axis names appearing in in_specs was an upper bound on the set of mesh axes over which residuals could be devicevarying. But collectives can introduce device variance! So it's not an upper bound. We track device variance when check_rep=True, but often people set check_rep=False (e.g. when using pallas_call in a shard_map). So relying on our device variance tracking would be limiting. That may be a decent long term solution, if we can make it easy to annotate pallas_calls with device variance information. But it's not a great short term one to unblock things. So instead I temporarily went with context sensitivity: instead of making residuals sharded over all mesh.axis_names (as we did before these patches), we make them sharded over all mesh axis names _excluding_ any spmd_axis_names in our dynamic context (by looking at the traces in our trace stack). It's illegal to mention any spmd_axis_names in collectives (indeed anywhere in the body of the function being vmapped), but I don't think we check it. TODO(mattjj): add more testing (maybe in followups)",2024-05-03T16:13:54Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/21056
763,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(spsolve exits with error when inverting matrix sum)ï¼Œ å†…å®¹æ˜¯ ( Description I am trying to solve a linear system (A1 + A2) x = b using sparse matrices A1, A2 and `jax.experimental.sparse.linalg.spsolve`. Since spsolve requires BCSR format which does not yet support addition, I am storing A1 and A2 in BCOO format, and then converting the sum in BCSR format. However this is causing an error in spsolve. I can reproduce the problem with the following simplified code: **Runs Fine**  **Exits in Error**  The error is copied below.   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,spsolve exits with error when inverting matrix sum," Description I am trying to solve a linear system (A1 + A2) x = b using sparse matrices A1, A2 and `jax.experimental.sparse.linalg.spsolve`. Since spsolve requires BCSR format which does not yet support addition, I am storing A1 and A2 in BCOO format, and then converting the sum in BCSR format. However this is causing an error in spsolve. I can reproduce the problem with the following simplified code: **Runs Fine**  **Exits in Error**  The error is copied below.   System info (python version, jaxlib version, accelerator, etc.) ",2024-05-01T20:03:08Z,bug,open,0,4,https://github.com/jax-ml/jax/issues/21031,Thanks for the report! It looks like something in `spsolve` fails in the presence of duplicate indices. You can fix the issue in your case by doing this before passing the `I_csr` buffers to `spsolve`: ,"I suspect this is working as intended, since `spsolve` is a lowerlevel function, though we should do a better job of documenting its requirements.","Thank you, adding `sum_duplicates` resolved the issue! It would be great if this eventually gets automatically taken care of by Jax so that sparse matrices behaves in the same way as scipy.","I don't think we'll ever automatically take care of this, because deduplication is a relatively expensive operation (even detecting whether deduplication is necessary is expensive!), and people calling a lowlevel routine like `spsolve` generally care about performance."
294,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bundle MLIR .pyi files with jaxlib)ï¼Œ å†…å®¹æ˜¯ (This allows mypy and pyright to type check the code which uses MLIR Python APIs.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bundle MLIR .pyi files with jaxlib,This allows mypy and pyright to type check the code which uses MLIR Python APIs.,2024-05-01T18:38:28Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/21029
1321,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add `sharding` on `ShapedArray` and in the type system of jaxpr. This can be enabled by setting the `jax_enable_sharding_in_avals` config option to `True`.)ï¼Œ å†…å®¹æ˜¯ (Add `sharding` on `ShapedArray` and in the type system of jaxpr. This can be enabled by setting the `jax_enable_sharding_in_avals` config option to `True`. This CL only adds sharding to avals in `with_sharding_constraint`'s abstract_eval rule and `device_put`'s abstract eval rule. In the following CLs, I'll make `jit`, `eval_shape`, `shard_map`, `arguments` and other parts of JAX play nicely with this! But what problem are you trying to solve right now? This CL solves a very specific memories problem where you want to offload an output to host/device via `device_put` inside `jit` but with no `out_shardings` annotation. `device_put` or `wsc` on outputs should lower to the same HLO as `out_shardings`. But this change is what we want to do in the future and follow up CLs would make other parts of jax also work well with this. Jaxpr with sharding inside it (currently only works with NamedSharding) where the sharding type is printed as a mapping from `{axis_name: dimension}`: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add `sharding` on `ShapedArray` and in the type system of jaxpr. This can be enabled by setting the `jax_enable_sharding_in_avals` config option to `True`.,"Add `sharding` on `ShapedArray` and in the type system of jaxpr. This can be enabled by setting the `jax_enable_sharding_in_avals` config option to `True`. This CL only adds sharding to avals in `with_sharding_constraint`'s abstract_eval rule and `device_put`'s abstract eval rule. In the following CLs, I'll make `jit`, `eval_shape`, `shard_map`, `arguments` and other parts of JAX play nicely with this! But what problem are you trying to solve right now? This CL solves a very specific memories problem where you want to offload an output to host/device via `device_put` inside `jit` but with no `out_shardings` annotation. `device_put` or `wsc` on outputs should lower to the same HLO as `out_shardings`. But this change is what we want to do in the future and follow up CLs would make other parts of jax also work well with this. Jaxpr with sharding inside it (currently only works with NamedSharding) where the sharding type is printed as a mapping from `{axis_name: dimension}`: ",2024-05-01T00:25:11Z,,closed,0,0,https://github.com/jax-ml/jax/issues/21022
1479,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Refactor `array_api` namespace, relying more directly on `jax.numpy`)ï¼Œ å†…å®¹æ˜¯ (This PR refactors the `jax.experimental.array_api` namespace by removing unnecessary wrappers around alreadycompliant functions in the `jax.numpy` namespace, and structuring the `array_api` namespace to pull directly from `jax.numpy` whenever possible. After this PR, the `array_api` submodule contain only: 1. Wrapper functions to insulate `jax.numpy` from breaking changes, which will be removed when the corresponding `jax.numpy` behavior is deprecated and made array API compliant 2. Additional features/utilities/API that is not yet present in `jax.numpy` and needs inclusion (e.g. introducing `jax.numpy.matmul`, which already exists in `jax.numpy.linalg`). This PR also adds several `TODO` items describing what is required to cull that portion of the `array_api` submodule, with the understanding that once it is empty, `jax.numpy` will be fully compliant. I figured it would be a bit neater to keep the `TODO` notes dense in this submodule, rather than spreading them across the `jax.numpy` submodule on their corresponding functions. It's also consistent with the `TODO`s for _new_ functionality or namespace elements. This PR also modifies `jax.numpy.isdtype` to accept `_ScalarMeta` and other dtypeinterpretable inputs.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"Refactor `array_api` namespace, relying more directly on `jax.numpy`","This PR refactors the `jax.experimental.array_api` namespace by removing unnecessary wrappers around alreadycompliant functions in the `jax.numpy` namespace, and structuring the `array_api` namespace to pull directly from `jax.numpy` whenever possible. After this PR, the `array_api` submodule contain only: 1. Wrapper functions to insulate `jax.numpy` from breaking changes, which will be removed when the corresponding `jax.numpy` behavior is deprecated and made array API compliant 2. Additional features/utilities/API that is not yet present in `jax.numpy` and needs inclusion (e.g. introducing `jax.numpy.matmul`, which already exists in `jax.numpy.linalg`). This PR also adds several `TODO` items describing what is required to cull that portion of the `array_api` submodule, with the understanding that once it is empty, `jax.numpy` will be fully compliant. I figured it would be a bit neater to keep the `TODO` notes dense in this submodule, rather than spreading them across the `jax.numpy` submodule on their corresponding functions. It's also consistent with the `TODO`s for _new_ functionality or namespace elements. This PR also modifies `jax.numpy.isdtype` to accept `_ScalarMeta` and other dtypeinterpretable inputs.",2024-04-30T20:41:27Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/21013,"The current failure is due to needing to add some signature test skips for `logical_{and, or, xor}`  that'll be fixed in the next push"
450,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Adds rewrite patterns for `arith` and `math` operations with `bf16` operands/results that are not supported by the underlying hardware.)ï¼Œ å†…å®¹æ˜¯ (Adds rewrite patterns for `arith` and `math` operations with `bf16` operands/results that are not supported by the underlying hardware.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Adds rewrite patterns for `arith` and `math` operations with `bf16` operands/results that are not supported by the underlying hardware.,Adds rewrite patterns for `arith` and `math` operations with `bf16` operands/results that are not supported by the underlying hardware.,2024-04-30T16:04:51Z,,closed,0,1,https://github.com/jax-ml/jax/issues/21007,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
719,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Mosaic GPU] Only call kernel initializer from inside a custom call)ï¼Œ å†…å®¹æ˜¯ ([Mosaic GPU] Only call kernel initializer from inside a custom call XLA:GPU custom call design is far from ideal, as there's apparently no way to figure out the CUDA context that will be used to run an HLO module before the custom call is first called. So, we can't preload the kernel onto the GPU, or else we'll get invalid handle errors due to the load and launch happening in different CUDA contexts... Also fix up build_wheel.py to match the rename of the runtime lib.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,[Mosaic GPU] Only call kernel initializer from inside a custom call,"[Mosaic GPU] Only call kernel initializer from inside a custom call XLA:GPU custom call design is far from ideal, as there's apparently no way to figure out the CUDA context that will be used to run an HLO module before the custom call is first called. So, we can't preload the kernel onto the GPU, or else we'll get invalid handle errors due to the load and launch happening in different CUDA contexts... Also fix up build_wheel.py to match the rename of the runtime lib.",2024-04-30T10:07:58Z,,closed,0,0,https://github.com/jax-ml/jax/issues/20995
563,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DOC: improve docs for reshape() and ravel())ï¼Œ å†…å®¹æ˜¯ (Similar to CC(DOC: improve docs of transpose & matrix_transpose), these are functions where the view semantics differ from those of NumPy, so documenting the correct features from scratch is better than relying on a JAXspecific note above inaccurate docs. Rendered preview:  `reshape`  `ravel` Part of CC(Tracking issue: inline docstrings).)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,DOC: improve docs for reshape() and ravel(),"Similar to CC(DOC: improve docs of transpose & matrix_transpose), these are functions where the view semantics differ from those of NumPy, so documenting the correct features from scratch is better than relying on a JAXspecific note above inaccurate docs. Rendered preview:  `reshape`  `ravel` Part of CC(Tracking issue: inline docstrings).",2024-04-29T15:36:08Z,documentation pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/20980
1292,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Sharding is much slower than pmap for while loops of varying length while loops)ï¼Œ å†…å®¹æ˜¯ ( Description As the title indicated, with a double while loop, where the inner while loop may change in length over outer while loop steps, `pmap` is substantially faster than sharding. This may sound contrived, but is exactly what happens in other packages, such as diffrax where I first identified this issue: https://github.com/patrickkidger/diffrax/issues/407. I believe there are two possibilities, 1) I am using sharding wrong and that is why it is slow (very possible, I am new to sharding), 2) something else is going on in sharding.  I have included a MVC below. I ran on both CPU and GPU and the results on GPU are even more noticeable.    CPU: `5.11 ms Â± 53.6 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)` GPU: `1.18 s Â± 48.8 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)`  CPU: `251 Âµs Â± 1.14 Âµs per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)` GPU: `3.93 ms Â± 225 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)`  System info (python version, jaxlib version, accelerator, etc.) CPU: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Sharding is much slower than pmap for while loops of varying length while loops," Description As the title indicated, with a double while loop, where the inner while loop may change in length over outer while loop steps, `pmap` is substantially faster than sharding. This may sound contrived, but is exactly what happens in other packages, such as diffrax where I first identified this issue: https://github.com/patrickkidger/diffrax/issues/407. I believe there are two possibilities, 1) I am using sharding wrong and that is why it is slow (very possible, I am new to sharding), 2) something else is going on in sharding.  I have included a MVC below. I ran on both CPU and GPU and the results on GPU are even more noticeable.    CPU: `5.11 ms Â± 53.6 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)` GPU: `1.18 s Â± 48.8 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)`  CPU: `251 Âµs Â± 1.14 Âµs per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)` GPU: `3.93 ms Â± 225 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)`  System info (python version, jaxlib version, accelerator, etc.) CPU: ",2024-04-27T21:09:38Z,bug,open,0,4,https://github.com/jax-ml/jax/issues/20968,"To be extra clear, if you make `count_initial = 0` you see the same speed for sharding and for pmap. Only when this varies per shard does this slow down","Also, if you replace the body and do   you see the same speed, which indicates the second while loop is important to the slowdown","kidger you mentioned in https://github.com/patrickkidger/diffrax/issues/407 that you suspect this is within XLA, do you have any advice on how to approach that? I haven't investigated an XLA system this complex before. Even my reduced complexity example (shown below) yields XLA's that are not exceedingly readable (shown further below). Is there a goto issue/piece of XLA/jax documentation on identifying whether a bug is in jax vs XLA and how to spot it?  HloModule xla_computation_solve, entry_computation_layout={(f32[10,1]{1,0}, u32[10,2]{1,0})>(f32[10,1]{1,0})} clip.3 {   Arg_2.6 = s32[] parameter(2)   Arg_1.5 = s32[] parameter(1)   Arg_0.4 = s32[] parameter(0)   maximum.7 = s32[] maximum(Arg_1.5, Arg_0.4)   ROOT minimum.8 = s32[] minimum(Arg_2.6, maximum.7) } clip_0.9 {   Arg_2.12 = s32[] parameter(2)   Arg_1.11 = s32[] parameter(1)   Arg_0.10 = s32[] parameter(0)   maximum.13 = s32[] maximum(Arg_1.11, Arg_0.10)   ROOT minimum.14 = s32[] minimum(Arg_2.12, maximum.13) } clip_0.15 {   Arg_2.18 = s32[] parameter(2)   Arg_1.17 = s32[] parameter(1)   Arg_0.16 = s32[] parameter(0)   maximum.19 = s32[] maximum(Arg_1.17, Arg_0.16)   ROOT minimum.20 = s32[] minimum(Arg_2.18, maximum.19) } region_0.21 {   arg_tuple.22 = (s32[], s32[], u32[10,2]{1,0}, u32[10,2]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) parameter(0)   gettupleelement.23 = s32[] gettupleelement(arg_tuple.22), index=0   constant.33 = s32[] constant(1)   add.87 = s32[] add(gettupleelement.23, constant.33)   gettupleelement.24 = s32[] gettupleelement(arg_tuple.22), index=1   add.34 = s32[] add(gettupleelement.24, constant.33)   gettupleelement.25 = u32[10,2]{1,0} gettupleelement(arg_tuple.22), index=2   gettupleelement.26 = u32[10,2]{1,0} gettupleelement(arg_tuple.22), index=3   add.37 = u32[10,2]{1,0} add(gettupleelement.25, gettupleelement.26)   gettupleelement.30 = u32[4]{0} gettupleelement(arg_tuple.22), index=7   slice.35 = u32[1]{0} slice(gettupleelement.30), slice={[0:1]}   reshape.36 = u32[] reshape(slice.35)   broadcast.38 = u32[10,2]{1,0} broadcast(reshape.36), dimensions={}   shiftleft.39 = u32[10,2]{1,0} shiftleft(gettupleelement.26, broadcast.38)   constant.32 = u32[] constant(32)   subtract.40 = u32[] subtract(constant.32, reshape.36)   broadcast.41 = u32[10,2]{1,0} broadcast(subtract.40), dimensions={}   shiftrightlogical.42 = u32[10,2]{1,0} shiftrightlogical(gettupleelement.26, broadcast.41)   or.43 = u32[10,2]{1,0} or(shiftleft.39, shiftrightlogical.42)   xor.44 = u32[10,2]{1,0} xor(add.37, or.43)   add.47 = u32[10,2]{1,0} add(add.37, xor.44)   slice.45 = u32[1]{0} slice(gettupleelement.30), slice={[1:2]}   reshape.46 = u32[] reshape(slice.45)   broadcast.48 = u32[10,2]{1,0} broadcast(reshape.46), dimensions={}   shiftleft.49 = u32[10,2]{1,0} shiftleft(xor.44, broadcast.48)   subtract.50 = u32[] subtract(constant.32, reshape.46)   broadcast.51 = u32[10,2]{1,0} broadcast(subtract.50), dimensions={}   shiftrightlogical.52 = u32[10,2]{1,0} shiftrightlogical(xor.44, broadcast.51)   or.53 = u32[10,2]{1,0} or(shiftleft.49, shiftrightlogical.52)   xor.54 = u32[10,2]{1,0} xor(add.47, or.53)   add.57 = u32[10,2]{1,0} add(add.47, xor.54)   slice.55 = u32[1]{0} slice(gettupleelement.30), slice={[2:3]}   reshape.56 = u32[] reshape(slice.55)   broadcast.58 = u32[10,2]{1,0} broadcast(reshape.56), dimensions={}   shiftleft.59 = u32[10,2]{1,0} shiftleft(xor.54, broadcast.58)   subtract.60 = u32[] subtract(constant.32, reshape.56)   broadcast.61 = u32[10,2]{1,0} broadcast(subtract.60), dimensions={}   shiftrightlogical.62 = u32[10,2]{1,0} shiftrightlogical(xor.54, broadcast.61)   or.63 = u32[10,2]{1,0} or(shiftleft.59, shiftrightlogical.62)   xor.64 = u32[10,2]{1,0} xor(add.57, or.63)   add.67 = u32[10,2]{1,0} add(add.57, xor.64)   gettupleelement.27 = u32[10,1]{1,0} gettupleelement(arg_tuple.22), index=4   broadcast.75 = u32[10,1]{1,0} broadcast(gettupleelement.27), dimensions={0,1}   reshape.76 = u32[10]{0} reshape(broadcast.75)   broadcast.77 = u32[10,2]{1,0} broadcast(reshape.76), dimensions={0}   add.78 = u32[10,2]{1,0} add(add.67, broadcast.77)   slice.65 = u32[1]{0} slice(gettupleelement.30), slice={[3:4]}   reshape.66 = u32[] reshape(slice.65)   broadcast.68 = u32[10,2]{1,0} broadcast(reshape.66), dimensions={}   shiftleft.69 = u32[10,2]{1,0} shiftleft(xor.64, broadcast.68)   subtract.70 = u32[] subtract(constant.32, reshape.66)   broadcast.71 = u32[10,2]{1,0} broadcast(subtract.70), dimensions={}   shiftrightlogical.72 = u32[10,2]{1,0} shiftrightlogical(xor.64, broadcast.71)   or.73 = u32[10,2]{1,0} or(shiftleft.69, shiftrightlogical.72)   xor.74 = u32[10,2]{1,0} xor(add.67, or.73)   gettupleelement.28 = u32[10,1]{1,0} gettupleelement(arg_tuple.22), index=5   broadcast.79 = u32[10,1]{1,0} broadcast(gettupleelement.28), dimensions={0,1}   reshape.80 = u32[10]{0} reshape(broadcast.79)   broadcast.81 = u32[10,2]{1,0} broadcast(reshape.80), dimensions={0}   add.82 = u32[10,2]{1,0} add(xor.74, broadcast.81)   add.83 = s32[] add(gettupleelement.24, constant.33)   convert.84 = u32[] convert(add.83)   broadcast.85 = u32[10,2]{1,0} broadcast(convert.84), dimensions={}   add.86 = u32[10,2]{1,0} add(add.82, broadcast.85)   gettupleelement.29 = u32[10,1]{1,0} gettupleelement(arg_tuple.22), index=6   gettupleelement.31 = u32[4]{0} gettupleelement(arg_tuple.22), index=8   ROOT tuple.88 = (s32[], s32[], u32[10,2]{1,0}, u32[10,2]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) tuple(add.87, add.34, add.78, add.86, gettupleelement.28, gettupleelement.29, gettupleelement.27, gettupleelement.31, gettupleelement.30) } region_1.89 {   arg_tuple.90 = (s32[], s32[], u32[10,2]{1,0}, u32[10,2]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) parameter(0)   gettupleelement.92 = s32[] gettupleelement(arg_tuple.90), index=1   gettupleelement.93 = u32[10,2]{1,0} gettupleelement(arg_tuple.90), index=2   gettupleelement.94 = u32[10,2]{1,0} gettupleelement(arg_tuple.90), index=3   gettupleelement.95 = u32[10,1]{1,0} gettupleelement(arg_tuple.90), index=4   gettupleelement.96 = u32[10,1]{1,0} gettupleelement(arg_tuple.90), index=5   gettupleelement.97 = u32[10,1]{1,0} gettupleelement(arg_tuple.90), index=6   gettupleelement.98 = u32[4]{0} gettupleelement(arg_tuple.90), index=7   gettupleelement.99 = u32[4]{0} gettupleelement(arg_tuple.90), index=8   gettupleelement.91 = s32[] gettupleelement(arg_tuple.90), index=0   constant.100 = s32[] constant(5)   ROOT compare.101 = pred[] compare(gettupleelement.91, constant.100), direction=LT } _threefry_split.102 {   constant.106 = s32[] constant(0)   iota.109 = u32[4]{0} iota(), iota_dimension=0   slice.112 = u32[2]{0} slice(iota.109), slice={[0:2]}   reshape.114 = u32[1,2]{1,0} reshape(slice.112)   broadcast.118 = u32[1,2]{1,0} broadcast(reshape.114), dimensions={0,1}   reshape.119 = u32[2]{0} reshape(broadcast.118)   broadcast.120 = u32[10,2]{1,0} broadcast(reshape.119), dimensions={1}   Arg_0.103 = u32[10,2]{1,0} parameter(0)   slice.110 = u32[10,1]{1,0} slice(Arg_0.103), slice={[0:10], [0:1]}   broadcast.121 = u32[10,1]{1,0} broadcast(slice.110), dimensions={0,1}   reshape.122 = u32[10]{0} reshape(broadcast.121)   broadcast.123 = u32[10,2]{1,0} broadcast(reshape.122), dimensions={0}   add.124 = u32[10,2]{1,0} add(broadcast.120, broadcast.123)   slice.113 = u32[2]{0} slice(iota.109), slice={[2:4]}   reshape.115 = u32[1,2]{1,0} reshape(slice.113)   broadcast.125 = u32[1,2]{1,0} broadcast(reshape.115), dimensions={0,1}   reshape.126 = u32[2]{0} reshape(broadcast.125)   broadcast.127 = u32[10,2]{1,0} broadcast(reshape.126), dimensions={1}   slice.111 = u32[10,1]{1,0} slice(Arg_0.103), slice={[0:10], [1:2]}   broadcast.128 = u32[10,1]{1,0} broadcast(slice.111), dimensions={0,1}   reshape.129 = u32[10]{0} reshape(broadcast.128)   broadcast.130 = u32[10,2]{1,0} broadcast(reshape.129), dimensions={0}   add.131 = u32[10,2]{1,0} add(broadcast.127, broadcast.130)   xor.116 = u32[10,1]{1,0} xor(slice.110, slice.111)   constant.104 = u32[] constant(466688986)   broadcast.105 = u32[10,1]{1,0} broadcast(constant.104), dimensions={}   xor.117 = u32[10,1]{1,0} xor(xor.116, broadcast.105)   constant.108 = u32[4]{0} constant({13, 15, 26, 6})   constant.107 = u32[4]{0} constant({17, 29, 16, 24})   tuple.132 = (s32[], s32[], u32[10,2]{1,0}, u32[10,2]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) tuple(constant.106, constant.106, add.124, add.131, slice.111, xor.117, slice.110, constant.108, constant.107)   while.133 = (s32[], s32[], u32[10,2]{1,0}, u32[10,2]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) while(tuple.132), condition=region_1.89, body=region_0.21   gettupleelement.134 = s32[] gettupleelement(while.133), index=0   gettupleelement.135 = s32[] gettupleelement(while.133), index=1   gettupleelement.138 = u32[10,1]{1,0} gettupleelement(while.133), index=4   gettupleelement.139 = u32[10,1]{1,0} gettupleelement(while.133), index=5   gettupleelement.140 = u32[10,1]{1,0} gettupleelement(while.133), index=6   gettupleelement.141 = u32[4]{0} gettupleelement(while.133), index=7   gettupleelement.142 = u32[4]{0} gettupleelement(while.133), index=8   gettupleelement.136 = u32[10,2]{1,0} gettupleelement(while.133), index=2   gettupleelement.137 = u32[10,2]{1,0} gettupleelement(while.133), index=3   concatenate.143 = u32[10,4]{1,0} concatenate(gettupleelement.136, gettupleelement.137), dimensions={1}   ROOT reshape.144 = u32[10,2,2]{2,1,0} reshape(concatenate.143) } region_2.145 {   arg_tuple.146 = (s32[], s32[], u32[10,1]{1,0}, u32[10,1]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) parameter(0)   gettupleelement.147 = s32[] gettupleelement(arg_tuple.146), index=0   constant.157 = s32[] constant(1)   add.205 = s32[] add(gettupleelement.147, constant.157)   gettupleelement.148 = s32[] gettupleelement(arg_tuple.146), index=1   add.158 = s32[] add(gettupleelement.148, constant.157)   gettupleelement.149 = u32[10,1]{1,0} gettupleelement(arg_tuple.146), index=2   gettupleelement.150 = u32[10,1]{1,0} gettupleelement(arg_tuple.146), index=3   add.161 = u32[10,1]{1,0} add(gettupleelement.149, gettupleelement.150)   gettupleelement.154 = u32[4]{0} gettupleelement(arg_tuple.146), index=7   slice.159 = u32[1]{0} slice(gettupleelement.154), slice={[0:1]}   reshape.160 = u32[] reshape(slice.159)   broadcast.162 = u32[10,1]{1,0} broadcast(reshape.160), dimensions={}   shiftleft.163 = u32[10,1]{1,0} shiftleft(gettupleelement.150, broadcast.162)   constant.156 = u32[] constant(32)   subtract.164 = u32[] subtract(constant.156, reshape.160)   broadcast.165 = u32[10,1]{1,0} broadcast(subtract.164), dimensions={}   shiftrightlogical.166 = u32[10,1]{1,0} shiftrightlogical(gettupleelement.150, broadcast.165)   or.167 = u32[10,1]{1,0} or(shiftleft.163, shiftrightlogical.166)   xor.168 = u32[10,1]{1,0} xor(add.161, or.167)   add.171 = u32[10,1]{1,0} add(add.161, xor.168)   slice.169 = u32[1]{0} slice(gettupleelement.154), slice={[1:2]}   reshape.170 = u32[] reshape(slice.169)   broadcast.172 = u32[10,1]{1,0} broadcast(reshape.170), dimensions={}   shiftleft.173 = u32[10,1]{1,0} shiftleft(xor.168, broadcast.172)   subtract.174 = u32[] subtract(constant.156, reshape.170)   broadcast.175 = u32[10,1]{1,0} broadcast(subtract.174), dimensions={}   shiftrightlogical.176 = u32[10,1]{1,0} shiftrightlogical(xor.168, broadcast.175)   or.177 = u32[10,1]{1,0} or(shiftleft.173, shiftrightlogical.176)   xor.178 = u32[10,1]{1,0} xor(add.171, or.177)   add.181 = u32[10,1]{1,0} add(add.171, xor.178)   slice.179 = u32[1]{0} slice(gettupleelement.154), slice={[2:3]}   reshape.180 = u32[] reshape(slice.179)   broadcast.182 = u32[10,1]{1,0} broadcast(reshape.180), dimensions={}   shiftleft.183 = u32[10,1]{1,0} shiftleft(xor.178, broadcast.182)   subtract.184 = u32[] subtract(constant.156, reshape.180)   broadcast.185 = u32[10,1]{1,0} broadcast(subtract.184), dimensions={}   shiftrightlogical.186 = u32[10,1]{1,0} shiftrightlogical(xor.178, broadcast.185)   or.187 = u32[10,1]{1,0} or(shiftleft.183, shiftrightlogical.186)   xor.188 = u32[10,1]{1,0} xor(add.181, or.187)   add.191 = u32[10,1]{1,0} add(add.181, xor.188)   gettupleelement.151 = u32[10,1]{1,0} gettupleelement(arg_tuple.146), index=4   add.199 = u32[10,1]{1,0} add(add.191, gettupleelement.151)   slice.189 = u32[1]{0} slice(gettupleelement.154), slice={[3:4]}   reshape.190 = u32[] reshape(slice.189)   broadcast.192 = u32[10,1]{1,0} broadcast(reshape.190), dimensions={}   shiftleft.193 = u32[10,1]{1,0} shiftleft(xor.188, broadcast.192)   subtract.194 = u32[] subtract(constant.156, reshape.190)   broadcast.195 = u32[10,1]{1,0} broadcast(subtract.194), dimensions={}   shiftrightlogical.196 = u32[10,1]{1,0} shiftrightlogical(xor.188, broadcast.195)   or.197 = u32[10,1]{1,0} or(shiftleft.193, shiftrightlogical.196)   xor.198 = u32[10,1]{1,0} xor(add.191, or.197)   gettupleelement.152 = u32[10,1]{1,0} gettupleelement(arg_tuple.146), index=5   add.200 = u32[10,1]{1,0} add(xor.198, gettupleelement.152)   add.201 = s32[] add(gettupleelement.148, constant.157)   convert.202 = u32[] convert(add.201)   broadcast.203 = u32[10,1]{1,0} broadcast(convert.202), dimensions={}   add.204 = u32[10,1]{1,0} add(add.200, broadcast.203)   gettupleelement.153 = u32[10,1]{1,0} gettupleelement(arg_tuple.146), index=6   gettupleelement.155 = u32[4]{0} gettupleelement(arg_tuple.146), index=8   ROOT tuple.206 = (s32[], s32[], u32[10,1]{1,0}, u32[10,1]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) tuple(add.205, add.158, add.199, add.204, gettupleelement.152, gettupleelement.153, gettupleelement.151, gettupleelement.155, gettupleelement.154) } region_3.207 {   arg_tuple.208 = (s32[], s32[], u32[10,1]{1,0}, u32[10,1]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) parameter(0)   gettupleelement.210 = s32[] gettupleelement(arg_tuple.208), index=1   gettupleelement.211 = u32[10,1]{1,0} gettupleelement(arg_tuple.208), index=2   gettupleelement.212 = u32[10,1]{1,0} gettupleelement(arg_tuple.208), index=3   gettupleelement.213 = u32[10,1]{1,0} gettupleelement(arg_tuple.208), index=4   gettupleelement.214 = u32[10,1]{1,0} gettupleelement(arg_tuple.208), index=5   gettupleelement.215 = u32[10,1]{1,0} gettupleelement(arg_tuple.208), index=6   gettupleelement.216 = u32[4]{0} gettupleelement(arg_tuple.208), index=7   gettupleelement.217 = u32[4]{0} gettupleelement(arg_tuple.208), index=8   gettupleelement.209 = s32[] gettupleelement(arg_tuple.208), index=0   constant.218 = s32[] constant(5)   ROOT compare.219 = pred[] compare(gettupleelement.209, constant.218), direction=LT } region_4.220 {   arg_tuple.221 = (s32[], s32[], u32[10,1]{1,0}, u32[10,1]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) parameter(0)   gettupleelement.222 = s32[] gettupleelement(arg_tuple.221), index=0   constant.232 = s32[] constant(1)   add.280 = s32[] add(gettupleelement.222, constant.232)   gettupleelement.223 = s32[] gettupleelement(arg_tuple.221), index=1   add.233 = s32[] add(gettupleelement.223, constant.232)   gettupleelement.224 = u32[10,1]{1,0} gettupleelement(arg_tuple.221), index=2   gettupleelement.225 = u32[10,1]{1,0} gettupleelement(arg_tuple.221), index=3   add.236 = u32[10,1]{1,0} add(gettupleelement.224, gettupleelement.225)   gettupleelement.229 = u32[4]{0} gettupleelement(arg_tuple.221), index=7   slice.234 = u32[1]{0} slice(gettupleelement.229), slice={[0:1]}   reshape.235 = u32[] reshape(slice.234)   broadcast.237 = u32[10,1]{1,0} broadcast(reshape.235), dimensions={}   shiftleft.238 = u32[10,1]{1,0} shiftleft(gettupleelement.225, broadcast.237)   constant.231 = u32[] constant(32)   subtract.239 = u32[] subtract(constant.231, reshape.235)   broadcast.240 = u32[10,1]{1,0} broadcast(subtract.239), dimensions={}   shiftrightlogical.241 = u32[10,1]{1,0} shiftrightlogical(gettupleelement.225, broadcast.240)   or.242 = u32[10,1]{1,0} or(shiftleft.238, shiftrightlogical.241)   xor.243 = u32[10,1]{1,0} xor(add.236, or.242)   add.246 = u32[10,1]{1,0} add(add.236, xor.243)   slice.244 = u32[1]{0} slice(gettupleelement.229), slice={[1:2]}   reshape.245 = u32[] reshape(slice.244)   broadcast.247 = u32[10,1]{1,0} broadcast(reshape.245), dimensions={}   shiftleft.248 = u32[10,1]{1,0} shiftleft(xor.243, broadcast.247)   subtract.249 = u32[] subtract(constant.231, reshape.245)   broadcast.250 = u32[10,1]{1,0} broadcast(subtract.249), dimensions={}   shiftrightlogical.251 = u32[10,1]{1,0} shiftrightlogical(xor.243, broadcast.250)   or.252 = u32[10,1]{1,0} or(shiftleft.248, shiftrightlogical.251)   xor.253 = u32[10,1]{1,0} xor(add.246, or.252)   add.256 = u32[10,1]{1,0} add(add.246, xor.253)   slice.254 = u32[1]{0} slice(gettupleelement.229), slice={[2:3]}   reshape.255 = u32[] reshape(slice.254)   broadcast.257 = u32[10,1]{1,0} broadcast(reshape.255), dimensions={}   shiftleft.258 = u32[10,1]{1,0} shiftleft(xor.253, broadcast.257)   subtract.259 = u32[] subtract(constant.231, reshape.255)   broadcast.260 = u32[10,1]{1,0} broadcast(subtract.259), dimensions={}   shiftrightlogical.261 = u32[10,1]{1,0} shiftrightlogical(xor.253, broadcast.260)   or.262 = u32[10,1]{1,0} or(shiftleft.258, shiftrightlogical.261)   xor.263 = u32[10,1]{1,0} xor(add.256, or.262)   add.266 = u32[10,1]{1,0} add(add.256, xor.263)   gettupleelement.226 = u32[10,1]{1,0} gettupleelement(arg_tuple.221), index=4   add.274 = u32[10,1]{1,0} add(add.266, gettupleelement.226)   slice.264 = u32[1]{0} slice(gettupleelement.229), slice={[3:4]}   reshape.265 = u32[] reshape(slice.264)   broadcast.267 = u32[10,1]{1,0} broadcast(reshape.265), dimensions={}   shiftleft.268 = u32[10,1]{1,0} shiftleft(xor.263, broadcast.267)   subtract.269 = u32[] subtract(constant.231, reshape.265)   broadcast.270 = u32[10,1]{1,0} broadcast(subtract.269), dimensions={}   shiftrightlogical.271 = u32[10,1]{1,0} shiftrightlogical(xor.263, broadcast.270)   or.272 = u32[10,1]{1,0} or(shiftleft.268, shiftrightlogical.271)   xor.273 = u32[10,1]{1,0} xor(add.266, or.272)   gettupleelement.227 = u32[10,1]{1,0} gettupleelement(arg_tuple.221), index=5   add.275 = u32[10,1]{1,0} add(xor.273, gettupleelement.227)   add.276 = s32[] add(gettupleelement.223, constant.232)   convert.277 = u32[] convert(add.276)   broadcast.278 = u32[10,1]{1,0} broadcast(convert.277), dimensions={}   add.279 = u32[10,1]{1,0} add(add.275, broadcast.278)   gettupleelement.228 = u32[10,1]{1,0} gettupleelement(arg_tuple.221), index=6   gettupleelement.230 = u32[4]{0} gettupleelement(arg_tuple.221), index=8   ROOT tuple.281 = (s32[], s32[], u32[10,1]{1,0}, u32[10,1]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) tuple(add.280, add.233, add.274, add.279, gettupleelement.227, gettupleelement.228, gettupleelement.226, gettupleelement.230, gettupleelement.229) } region_5.282 {   arg_tuple.283 = (s32[], s32[], u32[10,1]{1,0}, u32[10,1]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) parameter(0)   gettupleelement.285 = s32[] gettupleelement(arg_tuple.283), index=1   gettupleelement.286 = u32[10,1]{1,0} gettupleelement(arg_tuple.283), index=2   gettupleelement.287 = u32[10,1]{1,0} gettupleelement(arg_tuple.283), index=3   gettupleelement.288 = u32[10,1]{1,0} gettupleelement(arg_tuple.283), index=4   gettupleelement.289 = u32[10,1]{1,0} gettupleelement(arg_tuple.283), index=5   gettupleelement.290 = u32[10,1]{1,0} gettupleelement(arg_tuple.283), index=6   gettupleelement.291 = u32[4]{0} gettupleelement(arg_tuple.283), index=7   gettupleelement.292 = u32[4]{0} gettupleelement(arg_tuple.283), index=8   gettupleelement.284 = s32[] gettupleelement(arg_tuple.283), index=0   constant.293 = s32[] constant(5)   ROOT compare.294 = pred[] compare(gettupleelement.284, constant.293), direction=LT } _randint.295 {   constant.303 = s32[] constant(0)   Arg_0.296 = u32[10,2]{1,0} parameter(0)   call.312 = u32[10,2,2]{2,1,0} call(Arg_0.296), to_apply=_threefry_split.102   slice.313 = u32[10,1,2]{2,1,0} slice(call.312), slice={[0:10], [0:1], [0:2]}   reshape.314 = u32[10,2]{1,0} reshape(slice.313)   slice.317 = u32[10,1]{1,0} slice(reshape.314), slice={[0:10], [0:1]}   slice.318 = u32[10,1]{1,0} slice(reshape.314), slice={[0:10], [1:2]}   xor.319 = u32[10,1]{1,0} xor(slice.317, slice.318)   constant.299 = u32[] constant(466688986)   broadcast.300 = u32[10,1]{1,0} broadcast(constant.299), dimensions={}   xor.320 = u32[10,1]{1,0} xor(xor.319, broadcast.300)   constant.305 = u32[4]{0} constant({13, 15, 26, 6})   constant.304 = u32[4]{0} constant({17, 29, 16, 24})   tuple.321 = (s32[], s32[], u32[10,1]{1,0}, u32[10,1]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) tuple(constant.303, constant.303, slice.317, slice.318, slice.318, xor.320, slice.317, constant.305, constant.304)   while.322 = (s32[], s32[], u32[10,1]{1,0}, u32[10,1]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) while(tuple.321), condition=region_3.207, body=region_2.145   gettupleelement.323 = s32[] gettupleelement(while.322), index=0   gettupleelement.324 = s32[] gettupleelement(while.322), index=1   gettupleelement.326 = u32[10,1]{1,0} gettupleelement(while.322), index=3   gettupleelement.327 = u32[10,1]{1,0} gettupleelement(while.322), index=4   gettupleelement.328 = u32[10,1]{1,0} gettupleelement(while.322), index=5   gettupleelement.329 = u32[10,1]{1,0} gettupleelement(while.322), index=6   gettupleelement.330 = u32[4]{0} gettupleelement(while.322), index=7   gettupleelement.331 = u32[4]{0} gettupleelement(while.322), index=8   slice.315 = u32[10,1,2]{2,1,0} slice(call.312), slice={[0:10], [1:2], [0:2]}   reshape.316 = u32[10,2]{1,0} reshape(slice.315)   slice.333 = u32[10,1]{1,0} slice(reshape.316), slice={[0:10], [0:1]}   slice.334 = u32[10,1]{1,0} slice(reshape.316), slice={[0:10], [1:2]}   xor.335 = u32[10,1]{1,0} xor(slice.333, slice.334)   xor.336 = u32[10,1]{1,0} xor(xor.335, broadcast.300)   tuple.337 = (s32[], s32[], u32[10,1]{1,0}, u32[10,1]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) tuple(constant.303, constant.303, slice.333, slice.334, slice.334, xor.336, slice.333, constant.305, constant.304)   while.338 = (s32[], s32[], u32[10,1]{1,0}, u32[10,1]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) while(tuple.337), condition=region_5.282, body=region_4.220   gettupleelement.339 = s32[] gettupleelement(while.338), index=0   gettupleelement.340 = s32[] gettupleelement(while.338), index=1   gettupleelement.342 = u32[10,1]{1,0} gettupleelement(while.338), index=3   gettupleelement.343 = u32[10,1]{1,0} gettupleelement(while.338), index=4   gettupleelement.344 = u32[10,1]{1,0} gettupleelement(while.338), index=5   gettupleelement.345 = u32[10,1]{1,0} gettupleelement(while.338), index=6   gettupleelement.346 = u32[4]{0} gettupleelement(while.338), index=7   gettupleelement.347 = u32[4]{0} gettupleelement(while.338), index=8   Arg_1.297 = s32[] parameter(1)   constant.307 = s32[] constant(2147483648)   constant.306 = s32[] constant(2147483647)   call.310 = s32[] call(Arg_1.297, constant.307, constant.306), to_apply=clip_0.9   broadcast.370 = s32[10]{0} broadcast(call.310), dimensions={}   gettupleelement.325 = u32[10,1]{1,0} gettupleelement(while.322), index=2   reshape.332 = u32[10]{0} reshape(gettupleelement.325)   Arg_2.298 = s32[] parameter(2)   call.308 = s32[] call(constant.306, constant.307, constant.306), to_apply=clip.3   compare.309 = pred[] compare(Arg_2.298, call.308), direction=GT   call.311 = s32[] call(Arg_2.298, constant.307, constant.306), to_apply=clip_0.15   compare.353 = pred[] compare(call.311, call.310), direction=GT   and.354 = pred[] and(compare.309, compare.353)   compare.351 = pred[] compare(call.311, call.310), direction=LE   constant.302 = u32[] constant(1)   subtract.349 = s32[] subtract(call.311, call.310)   convert.350 = u32[] convert(subtract.349)   select.352 = u32[] select(compare.351, constant.302, convert.350)   add.355 = u32[] add(select.352, constant.302)   select.356 = u32[] select(and.354, add.355, select.352)   broadcast.360 = u32[10]{0} broadcast(select.356), dimensions={}   remainder.361 = u32[10]{0} remainder(reshape.332, broadcast.360)   constant.301 = u32[] constant(65536)   remainder.357 = u32[] remainder(constant.301, select.356)   multiply.358 = u32[] multiply(remainder.357, remainder.357)   remainder.359 = u32[] remainder(multiply.358, select.356)   broadcast.362 = u32[10]{0} broadcast(remainder.359), dimensions={}   multiply.363 = u32[10]{0} multiply(remainder.361, broadcast.362)   gettupleelement.341 = u32[10,1]{1,0} gettupleelement(while.338), index=2   reshape.348 = u32[10]{0} reshape(gettupleelement.341)   broadcast.364 = u32[10]{0} broadcast(select.356), dimensions={}   remainder.365 = u32[10]{0} remainder(reshape.348, broadcast.364)   add.366 = u32[10]{0} add(multiply.363, remainder.365)   broadcast.367 = u32[10]{0} broadcast(select.356), dimensions={}   remainder.368 = u32[10]{0} remainder(add.366, broadcast.367)   convert.369 = s32[10]{0} convert(remainder.368)   ROOT add.371 = s32[10]{0} add(broadcast.370, convert.369) } region_7.372 {   arg_tuple.373 = (s32[], s32[], u32[10,1]{1,0}, u32[10,1]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) parameter(0)   gettupleelement.374 = s32[] gettupleelement(arg_tuple.373), index=0   constant.384 = s32[] constant(1)   add.432 = s32[] add(gettupleelement.374, constant.384)   gettupleelement.375 = s32[] gettupleelement(arg_tuple.373), index=1   add.385 = s32[] add(gettupleelement.375, constant.384)   gettupleelement.376 = u32[10,1]{1,0} gettupleelement(arg_tuple.373), index=2   gettupleelement.377 = u32[10,1]{1,0} gettupleelement(arg_tuple.373), index=3   add.388 = u32[10,1]{1,0} add(gettupleelement.376, gettupleelement.377)   gettupleelement.381 = u32[4]{0} gettupleelement(arg_tuple.373), index=7   slice.386 = u32[1]{0} slice(gettupleelement.381), slice={[0:1]}   reshape.387 = u32[] reshape(slice.386)   broadcast.389 = u32[10,1]{1,0} broadcast(reshape.387), dimensions={}   shiftleft.390 = u32[10,1]{1,0} shiftleft(gettupleelement.377, broadcast.389)   constant.383 = u32[] constant(32)   subtract.391 = u32[] subtract(constant.383, reshape.387)   broadcast.392 = u32[10,1]{1,0} broadcast(subtract.391), dimensions={}   shiftrightlogical.393 = u32[10,1]{1,0} shiftrightlogical(gettupleelement.377, broadcast.392)   or.394 = u32[10,1]{1,0} or(shiftleft.390, shiftrightlogical.393)   xor.395 = u32[10,1]{1,0} xor(add.388, or.394)   add.398 = u32[10,1]{1,0} add(add.388, xor.395)   slice.396 = u32[1]{0} slice(gettupleelement.381), slice={[1:2]}   reshape.397 = u32[] reshape(slice.396)   broadcast.399 = u32[10,1]{1,0} broadcast(reshape.397), dimensions={}   shiftleft.400 = u32[10,1]{1,0} shiftleft(xor.395, broadcast.399)   subtract.401 = u32[] subtract(constant.383, reshape.397)   broadcast.402 = u32[10,1]{1,0} broadcast(subtract.401), dimensions={}   shiftrightlogical.403 = u32[10,1]{1,0} shiftrightlogical(xor.395, broadcast.402)   or.404 = u32[10,1]{1,0} or(shiftleft.400, shiftrightlogical.403)   xor.405 = u32[10,1]{1,0} xor(add.398, or.404)   add.408 = u32[10,1]{1,0} add(add.398, xor.405)   slice.406 = u32[1]{0} slice(gettupleelement.381), slice={[2:3]}   reshape.407 = u32[] reshape(slice.406)   broadcast.409 = u32[10,1]{1,0} broadcast(reshape.407), dimensions={}   shiftleft.410 = u32[10,1]{1,0} shiftleft(xor.405, broadcast.409)   subtract.411 = u32[] subtract(constant.383, reshape.407)   broadcast.412 = u32[10,1]{1,0} broadcast(subtract.411), dimensions={}   shiftrightlogical.413 = u32[10,1]{1,0} shiftrightlogical(xor.405, broadcast.412)   or.414 = u32[10,1]{1,0} or(shiftleft.410, shiftrightlogical.413)   xor.415 = u32[10,1]{1,0} xor(add.408, or.414)   add.418 = u32[10,1]{1,0} add(add.408, xor.415)   gettupleelement.378 = u32[10,1]{1,0} gettupleelement(arg_tuple.373), index=4   add.426 = u32[10,1]{1,0} add(add.418, gettupleelement.378)   slice.416 = u32[1]{0} slice(gettupleelement.381), slice={[3:4]}   reshape.417 = u32[] reshape(slice.416)   broadcast.419 = u32[10,1]{1,0} broadcast(reshape.417), dimensions={}   shiftleft.420 = u32[10,1]{1,0} shiftleft(xor.415, broadcast.419)   subtract.421 = u32[] subtract(constant.383, reshape.417)   broadcast.422 = u32[10,1]{1,0} broadcast(subtract.421), dimensions={}   shiftrightlogical.423 = u32[10,1]{1,0} shiftrightlogical(xor.415, broadcast.422)   or.424 = u32[10,1]{1,0} or(shiftleft.420, shiftrightlogical.423)   xor.425 = u32[10,1]{1,0} xor(add.418, or.424)   gettupleelement.379 = u32[10,1]{1,0} gettupleelement(arg_tuple.373), index=5   add.427 = u32[10,1]{1,0} add(xor.425, gettupleelement.379)   add.428 = s32[] add(gettupleelement.375, constant.384)   convert.429 = u32[] convert(add.428)   broadcast.430 = u32[10,1]{1,0} broadcast(convert.429), dimensions={}   add.431 = u32[10,1]{1,0} add(add.427, broadcast.430)   gettupleelement.380 = u32[10,1]{1,0} gettupleelement(arg_tuple.373), index=6   gettupleelement.382 = u32[4]{0} gettupleelement(arg_tuple.373), index=8   ROOT tuple.433 = (s32[], s32[], u32[10,1]{1,0}, u32[10,1]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) tuple(add.432, add.385, add.426, add.431, gettupleelement.379, gettupleelement.380, gettupleelement.378, gettupleelement.382, gettupleelement.381) } region_8.434 {   arg_tuple.435 = (s32[], s32[], u32[10,1]{1,0}, u32[10,1]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) parameter(0)   gettupleelement.437 = s32[] gettupleelement(arg_tuple.435), index=1   gettupleelement.438 = u32[10,1]{1,0} gettupleelement(arg_tuple.435), index=2   gettupleelement.439 = u32[10,1]{1,0} gettupleelement(arg_tuple.435), index=3   gettupleelement.440 = u32[10,1]{1,0} gettupleelement(arg_tuple.435), index=4   gettupleelement.441 = u32[10,1]{1,0} gettupleelement(arg_tuple.435), index=5   gettupleelement.442 = u32[10,1]{1,0} gettupleelement(arg_tuple.435), index=6   gettupleelement.443 = u32[4]{0} gettupleelement(arg_tuple.435), index=7   gettupleelement.444 = u32[4]{0} gettupleelement(arg_tuple.435), index=8   gettupleelement.436 = s32[] gettupleelement(arg_tuple.435), index=0   constant.445 = s32[] constant(5)   ROOT compare.446 = pred[] compare(gettupleelement.436, constant.445), direction=LT } _uniform.447 {   constant.459 = s32[] constant(0)   Arg_0.448 = u32[10,2]{1,0} parameter(0)   slice.464 = u32[10,1]{1,0} slice(Arg_0.448), slice={[0:10], [0:1]}   slice.465 = u32[10,1]{1,0} slice(Arg_0.448), slice={[0:10], [1:2]}   xor.466 = u32[10,1]{1,0} xor(slice.464, slice.465)   constant.457 = u32[] constant(466688986)   broadcast.458 = u32[10,1]{1,0} broadcast(constant.457), dimensions={}   xor.467 = u32[10,1]{1,0} xor(xor.466, broadcast.458)   constant.461 = u32[4]{0} constant({13, 15, 26, 6})   constant.460 = u32[4]{0} constant({17, 29, 16, 24})   tuple.468 = (s32[], s32[], u32[10,1]{1,0}, u32[10,1]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) tuple(constant.459, constant.459, slice.464, slice.465, slice.465, xor.467, slice.464, constant.461, constant.460)   while.469 = (s32[], s32[], u32[10,1]{1,0}, u32[10,1]{1,0}, u32[10,1]{1,0}, /*index=5*/u32[10,1]{1,0}, u32[10,1]{1,0}, u32[4]{0}, u32[4]{0}) while(tuple.468), condition=region_8.434, body=region_7.372   gettupleelement.470 = s32[] gettupleelement(while.469), index=0   gettupleelement.471 = s32[] gettupleelement(while.469), index=1   gettupleelement.473 = u32[10,1]{1,0} gettupleelement(while.469), index=3   gettupleelement.474 = u32[10,1]{1,0} gettupleelement(while.469), index=4   gettupleelement.475 = u32[10,1]{1,0} gettupleelement(while.469), index=5   gettupleelement.476 = u32[10,1]{1,0} gettupleelement(while.469), index=6   gettupleelement.477 = u32[4]{0} gettupleelement(while.469), index=7   gettupleelement.478 = u32[4]{0} gettupleelement(while.469), index=8   Arg_1.449 = f32[] parameter(1)   reshape.494 = f32[1,1]{1,0} reshape(Arg_1.449)   broadcast.495 = f32[1,1]{1,0} broadcast(reshape.494), dimensions={0,1}   reshape.496 = f32[1]{0} reshape(broadcast.495)   broadcast.497 = f32[10,1]{1,0} broadcast(reshape.496), dimensions={1}   gettupleelement.472 = u32[10,1]{1,0} gettupleelement(while.469), index=2   constant.455 = u32[] constant(9)   broadcast.456 = u32[10,1]{1,0} broadcast(constant.455), dimensions={}   shiftrightlogical.479 = u32[10,1]{1,0} shiftrightlogical(gettupleelement.472, broadcast.456)   constant.453 = u32[] constant(1065353216)   broadcast.454 = u32[10,1]{1,0} broadcast(constant.453), dimensions={}   or.480 = u32[10,1]{1,0} or(shiftrightlogical.479, broadcast.454)   bitcastconvert.481 = f32[10,1]{1,0} bitcastconvert(or.480)   constant.451 = f32[] constant(1)   broadcast.452 = f32[10,1]{1,0} broadcast(constant.451), dimensions={}   subtract.482 = f32[10,1]{1,0} subtract(bitcastconvert.481, broadcast.452)   Arg_2.450 = f32[] parameter(2)   reshape.463 = f32[1]{0} reshape(Arg_2.450)   reshape.462 = f32[1]{0} reshape(Arg_1.449)   subtract.483 = f32[1]{0} subtract(reshape.463, reshape.462)   reshape.484 = f32[1,1]{1,0} reshape(subtract.483)   broadcast.485 = f32[1,1]{1,0} broadcast(reshape.484), dimensions={0,1}   reshape.486 = f32[1]{0} reshape(broadcast.485)   broadcast.487 = f32[10,1]{1,0} broadcast(reshape.486), dimensions={1}   multiply.488 = f32[10,1]{1,0} multiply(subtract.482, broadcast.487)   reshape.489 = f32[1,1]{1,0} reshape(Arg_1.449)   broadcast.490 = f32[1,1]{1,0} broadcast(reshape.489), dimensions={0,1}   reshape.491 = f32[1]{0} reshape(broadcast.490)   broadcast.492 = f32[10,1]{1,0} broadcast(reshape.491), dimensions={1}   add.493 = f32[10,1]{1,0} add(multiply.488, broadcast.492)   ROOT maximum.498 = f32[10,1]{1,0} maximum(broadcast.497, add.493) } region_9.499 {   arg_tuple.500 = (f32[10]{0}, f32[10,1]{1,0}) parameter(0)   gettupleelement.502 = f32[10,1]{1,0} gettupleelement(arg_tuple.500), index=1   reshape.511 = f32[10]{0} reshape(gettupleelement.502)   constant.503 = f32[] constant(2)   broadcast.504 = f32[10]{0} broadcast(constant.503), dimensions={}   compare.512 = pred[10]{0} compare(reshape.511, broadcast.504), direction=LT   gettupleelement.501 = f32[10]{0} gettupleelement(arg_tuple.500), index=0   constant.507 = f32[] constant(0.1)   broadcast.508 = f32[10]{0} broadcast(constant.507), dimensions={}   add.509 = f32[10]{0} add(gettupleelement.501, broadcast.508)   select.513 = f32[10]{0} select(compare.512, add.509, gettupleelement.501)   reshape.514 = pred[10,1]{1,0} reshape(compare.512)   constant.505 = f32[] constant(0.1)   broadcast.506 = f32[10,1]{1,0} broadcast(constant.505), dimensions={}   add.510 = f32[10,1]{1,0} add(gettupleelement.502, broadcast.506)   select.515 = f32[10,1]{1,0} select(reshape.514, add.510, gettupleelement.502)   ROOT tuple.516 = (f32[10]{0}, f32[10,1]{1,0}) tuple(select.513, select.515) } region_11.517 {   Arg_0.518 = pred[] parameter(0)   Arg_1.519 = pred[] parameter(1)   ROOT or.520 = pred[] or(Arg_0.518, Arg_1.519) } region_10.521 {   arg_tuple.522 = (f32[10]{0}, f32[10,1]{1,0}) parameter(0)   gettupleelement.523 = f32[10]{0} gettupleelement(arg_tuple.522), index=0   gettupleelement.524 = f32[10,1]{1,0} gettupleelement(arg_tuple.522), index=1   reshape.528 = f32[10]{0} reshape(gettupleelement.524)   constant.526 = f32[] constant(2)   broadcast.527 = f32[10]{0} broadcast(constant.526), dimensions={}   compare.529 = pred[10]{0} compare(reshape.528, broadcast.527), direction=LT   constant.525 = pred[] constant(false)   ROOT reduce.530 = pred[] reduce(compare.529, constant.525), dimensions={0}, to_apply=region_11.517 } region_6.531 {   arg_tuple.532 = (f32[10]{0}, f32[10,1]{1,0}, s32[10]{0}) parameter(0)   gettupleelement.535 = s32[10]{0} gettupleelement(arg_tuple.532), index=2   constant.536 = s32[] constant(5)   broadcast.537 = s32[10]{0} broadcast(constant.536), dimensions={}   compare.556 = pred[10]{0} compare(gettupleelement.535, broadcast.537), direction=LT   gettupleelement.533 = f32[10]{0} gettupleelement(arg_tuple.532), index=0   constant.542 = s32[] constant(32)   broadcast.543 = s32[10]{0} broadcast(constant.542), dimensions={}   shiftrightlogical.544 = s32[10]{0} shiftrightlogical(gettupleelement.535, broadcast.543)   convert.545 = u32[10]{0} convert(shiftrightlogical.544)   reshape.546 = u32[10,1]{1,0} reshape(convert.545)   convert.547 = u32[10]{0} convert(gettupleelement.535)   reshape.548 = u32[10,1]{1,0} reshape(convert.547)   concatenate.549 = u32[10,2]{1,0} concatenate(reshape.546, reshape.548), dimensions={1}   constant.541 = f32[] constant(0)   constant.540 = f32[] constant(1)   call.550 = f32[10,1]{1,0} call(concatenate.549, constant.541, constant.540), to_apply=_uniform.447   tuple.551 = (f32[10]{0}, f32[10,1]{1,0}) tuple(gettupleelement.533, call.550)   while.552 = (f32[10]{0}, f32[10,1]{1,0}) while(tuple.551), condition=region_10.521, body=region_9.499   gettupleelement.553 = f32[10]{0} gettupleelement(while.552), index=0   select.557 = f32[10]{0} select(compare.556, gettupleelement.553, gettupleelement.533)   reshape.558 = pred[10,1]{1,0} reshape(compare.556)   gettupleelement.554 = f32[10,1]{1,0} gettupleelement(while.552), index=1   gettupleelement.534 = f32[10,1]{1,0} gettupleelement(arg_tuple.532), index=1   select.559 = f32[10,1]{1,0} select(reshape.558, gettupleelement.554, gettupleelement.534)   constant.538 = s32[] constant(1)   broadcast.539 = s32[10]{0} broadcast(constant.538), dimensions={}   add.555 = s32[10]{0} add(gettupleelement.535, broadcast.539)   select.560 = s32[10]{0} select(compare.556, add.555, gettupleelement.535)   ROOT tuple.561 = (f32[10]{0}, f32[10,1]{1,0}, s32[10]{0}) tuple(select.557, select.559, select.560) } region_13.562 {   Arg_0.563 = pred[] parameter(0)   Arg_1.564 = pred[] parameter(1)   ROOT or.565 = pred[] or(Arg_0.563, Arg_1.564) } region_12.566 {   arg_tuple.567 = (f32[10]{0}, f32[10,1]{1,0}, s32[10]{0}) parameter(0)   gettupleelement.568 = f32[10]{0} gettupleelement(arg_tuple.567), index=0   gettupleelement.569 = f32[10,1]{1,0} gettupleelement(arg_tuple.567), index=1   gettupleelement.570 = s32[10]{0} gettupleelement(arg_tuple.567), index=2   constant.572 = s32[] constant(5)   broadcast.573 = s32[10]{0} broadcast(constant.572), dimensions={}   compare.574 = pred[10]{0} compare(gettupleelement.570, broadcast.573), direction=LT   constant.571 = pred[] constant(false)   ROOT reduce.575 = pred[] reduce(compare.574, constant.571), dimensions={0}, to_apply=region_13.562 } solve.576 {   constant.579 = f32[] constant(0)   broadcast.580 = f32[10]{0} broadcast(constant.579), dimensions={}   Arg_0.577 = f32[10,1]{1,0} parameter(0)   Arg_1.578 = u32[10,2]{1,0} parameter(1)   constant.581 = s32[] constant(2)   constant.582 = s32[] constant(2)   call.583 = s32[10]{0} call(Arg_1.578, constant.581, constant.582), to_apply=_randint.295   tuple.584 = (f32[10]{0}, f32[10,1]{1,0}, s32[10]{0}) tuple(broadcast.580, Arg_0.577, call.583)   while.585 = (f32[10]{0}, f32[10,1]{1,0}, s32[10]{0}) while(tuple.584), condition=region_12.566, body=region_6.531   gettupleelement.586 = f32[10]{0} gettupleelement(while.585), index=0   ROOT gettupleelement.587 = f32[10,1]{1,0} gettupleelement(while.585), index=1   gettupleelement.588 = s32[10]{0} gettupleelement(while.585), index=2 } ENTRY main.591 {   Arg_0.1 = f32[10,1]{1,0} parameter(0)   Arg_1.2 = u32[10,2]{1,0} parameter(1)   call.589 = f32[10,1]{1,0} call(Arg_0.1, Arg_1.2), to_apply=solve.576   ROOT tuple.590 = (f32[10,1]{1,0}) tuple(call.589) } ``` ",For performancerelated things like this it is usually in XLA. JAX is mostly at the mercy of whatever code XLA generates. Unfortunately the parallelism part of this isn't something I'm familiar with at all. I think  might know more? This one is out of my wheelhouse I'm afraid.
587,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(scipy.linalg.tril and .triu changed to sparse)ï¼Œ å†…å®¹æ˜¯ ( Description When using jax._src.scipy.linalg I get an error saying: `AttributeError: module 'scipy.linalg' has no attribute 'tril'` `AttributeError: module 'scipy.linalg' has no attribute 'triu'` It seems that scipy changed the function namespaces to `scipy.sparse.tril` and `scipy.sparse.triu`.  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,scipy.linalg.tril and .triu changed to sparse," Description When using jax._src.scipy.linalg I get an error saying: `AttributeError: module 'scipy.linalg' has no attribute 'tril'` `AttributeError: module 'scipy.linalg' has no attribute 'triu'` It seems that scipy changed the function namespaces to `scipy.sparse.tril` and `scipy.sparse.triu`.  System info (python version, jaxlib version, accelerator, etc.) ",2024-04-27T09:13:30Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/20965,Resolved when installing latest release 0.4.16.
1483,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Performance Issue Report: JAX Slower Than Autograd on GPU and CPU Setups)ï¼Œ å†…å®¹æ˜¯ ( Description **Introduction:** This report outlines a performance issue observed with JAX on both GPU and CPU hardware setups. The purpose of this report is to provide detailed feedback to the JAX development team to aid in identifying potential areas for optimization. **Observed Performance Issues:**  **GPU Performance:**    JAX is significantly slower than expected when compared to Autograd on identical tasks, showing a minimum of 5x slower performance on NVIDIA GPUs.  **CPU Performance:**    Similar underperformance is observed on Intel Core i7 CPUs, where JAX operations are markedly slower than those performed with Autograd. **Steps to Reproduce:** 1. Set up the environment with specified hardware and software versions. 2. Run benchmark tests including matrix operations, gradient calculations (ADAM). 3. Compare execution times of JAX and Autograd. **Expected Behavior:** JAX should exhibit comparable or better performance than Autograd given its design for highperformance machine learning tasks, especially on platforms supporting GPU acceleration. **Actual Behavior:** JAX underperforms significantly compared to Autograd across all tested hardware setups. **Attachments:**  **Benchmarking Scripts:**  ADAM (beta)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Performance Issue Report: JAX Slower Than Autograd on GPU and CPU Setups," Description **Introduction:** This report outlines a performance issue observed with JAX on both GPU and CPU hardware setups. The purpose of this report is to provide detailed feedback to the JAX development team to aid in identifying potential areas for optimization. **Observed Performance Issues:**  **GPU Performance:**    JAX is significantly slower than expected when compared to Autograd on identical tasks, showing a minimum of 5x slower performance on NVIDIA GPUs.  **CPU Performance:**    Similar underperformance is observed on Intel Core i7 CPUs, where JAX operations are markedly slower than those performed with Autograd. **Steps to Reproduce:** 1. Set up the environment with specified hardware and software versions. 2. Run benchmark tests including matrix operations, gradient calculations (ADAM). 3. Compare execution times of JAX and Autograd. **Expected Behavior:** JAX should exhibit comparable or better performance than Autograd given its design for highperformance machine learning tasks, especially on platforms supporting GPU acceleration. **Actual Behavior:** JAX underperforms significantly compared to Autograd across all tested hardware setups. **Attachments:**  **Benchmarking Scripts:**  ADAM (beta",2024-04-26T10:38:30Z,bug performance,open,0,4,https://github.com/jax-ml/jax/issues/20948,"What happens if you wrap the jax function in `jax.jit`? Also, can you include details on how you ran the benchmarks? Keep in mind these tips to make sure you're measuring what you think you're measuring when running benchmarks of JAX code: https://jax.readthedocs.io/en/latest/faq.htmlbenchmarkingjaxcode","Hi Jake,  I read the documentation you mentioned, I believe I haven't miss anything important, since my code is simple and trivial. 1. I wrapped the function using one of two ways:     The decorator: ``     Directly on the function: `jax.jit(f)` 2. I moved `x0` to the GPU as follows:     3. I ran identical code samples using `jnp` and `anp`. The `anp` version completed in under a second, while the `jnp` version has been running for over 10 minutes (I reduced the number of iteration to a minimum number to finish the test and take the screenshots, unlike `anp` which broke the loop upon meeting convergence criteria). 4. Here is ADAM with `time `:  I am using the `time` library for rough performance measurement. The function in question is simple, as described. !1 !2","When I try benchmarking your original function using `jax.jit`, I find that JAX is 4x faster than autograd on both CPU and GPU for inputs of size 1000  This is on a Colab CPU runtime, using the builtin `%timeit` magic function. On a Colab T4 GPU, the timings I get are:  If you could include your full endtoend benchmark script, including all imports, array definitions, function definitions, and function calls, I may be able to comment on why you are seeing different results.","Ah I think now I see, when I run your snippet I got this warning:  And got : `JAX Available devices: [CpuDevice(id=0)] ` When running:  But when I import `jaxlib` this warning disappears, and the performance drops to become almost equal to autograd (btw, never seen this warning before, maybe because I was importing jaxlib, but why really?)  and still :  `JAX Available devices: [CpuDevice(id=0)] ` This is my `nvcc version`:  And my `jax` and `jaxlib` versions are `0.4.26`"
1436,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Implements Ragged Dot API)ï¼Œ å†…å®¹æ˜¯ ( Background Ragged Dot is a specialized matrix multiplication operation that is commonly used in the context of Mixture of Experts (MoE) models. MoE models are a type of neural network architecture that consists of a collection of independent expert networks, each of which is responsible for processing a specific subset of the input data. In order to determine which expert should process a given input, a routing mechanism is employed. Ragged Dot plays a crucial role in this routing process. At the linear algebra level, Ragged Dot can be defined as follows: Given matrices, A (m x k), B (g x k x n), and G (g), where m is the number of input samples, k is the dimensionality of the input features, and g is the number of experts, the Ragged Dot operation produces a matrix C (m x n), where each row of C corresponds to the weighted sum of the corresponding row of A and the columns of B associated with the expert assigned to that input sample. More formally, the (i, j)th element of C is computed as follows:  where k ranges over the columns of B associated with the expert assigned to the ith input sample. The key characteristic of Ragged Dot is that the rows of A and slices of B in the 0th dimension (of size g) are group)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Implements Ragged Dot API," Background Ragged Dot is a specialized matrix multiplication operation that is commonly used in the context of Mixture of Experts (MoE) models. MoE models are a type of neural network architecture that consists of a collection of independent expert networks, each of which is responsible for processing a specific subset of the input data. In order to determine which expert should process a given input, a routing mechanism is employed. Ragged Dot plays a crucial role in this routing process. At the linear algebra level, Ragged Dot can be defined as follows: Given matrices, A (m x k), B (g x k x n), and G (g), where m is the number of input samples, k is the dimensionality of the input features, and g is the number of experts, the Ragged Dot operation produces a matrix C (m x n), where each row of C corresponds to the weighted sum of the corresponding row of A and the columns of B associated with the expert assigned to that input sample. More formally, the (i, j)th element of C is computed as follows:  where k ranges over the columns of B associated with the expert assigned to the ith input sample. The key characteristic of Ragged Dot is that the rows of A and slices of B in the 0th dimension (of size g) are group",2024-04-25T20:46:17Z,pull ready,closed,0,9,https://github.com/jax-ml/jax/issues/20940,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Assigning , who has thought a bit about how to support ragged operations more broadly in JAX.", Removed uses of numpy functions from lax.ragged_dot  Added _CompileAndCheck tests in addition to self._CheckAgainstNumpy.,PTAL.,Implemented the ragged_dot as JAX primitive.,Addressed all the comments and replaced _reduce_sum() by contracting dimensions in the dot_general() call. PTAL.,Applied Matt's suggestion. Thank you all for the feedback!,"Addressed Jake's feedback, thank you.",Looks great  the last thing we need before getting this submitted is to squash the changes into a single commit.
732,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Enable async dispatch on JAX CPU by setting 'jax_cpu_enable_async_dispatch' to be `True` by default.)ï¼Œ å†…å®¹æ˜¯ (Enable async dispatch on JAX CPU by setting 'jax_cpu_enable_async_dispatch' to be `True` by default. To prevent too much parallelism for nonparallel computations, we add a enqueue event to make sure next computation won't be enqueued until last one is done. In `~PyArray_Storage()`, we now release the python GIL then destroy the underlying buffer to prevent deadlock caused by interactions between argument donations and host callbacks on CPU backend.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Enable async dispatch on JAX CPU by setting 'jax_cpu_enable_async_dispatch' to be `True` by default.,"Enable async dispatch on JAX CPU by setting 'jax_cpu_enable_async_dispatch' to be `True` by default. To prevent too much parallelism for nonparallel computations, we add a enqueue event to make sure next computation won't be enqueued until last one is done. In `~PyArray_Storage()`, we now release the python GIL then destroy the underlying buffer to prevent deadlock caused by interactions between argument donations and host callbacks on CPU backend.",2024-04-25T19:27:49Z,,closed,0,0,https://github.com/jax-ml/jax/issues/20939
702,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Adding `tree_util.stack_leaves()` and `tree_util.unstack_leaves()`)ï¼Œ å†…å®¹æ˜¯ ( `stack_leaves`: Stack the leaves of one or more PyTrees along a new axis.  `unstack_leaves`: Unstack the leaves of a PyTree. References:  https://docs.lieselproject.org/en/v0.1.4/_modules/liesel/goose/pytree.htmlstack_leaves  https://gist.github.com/willwhitney/dd89cac6a5b771ccff18b06b33372c75?permalink_comment_id=4634557gistcomment4634557  https://github.com/ayaka14732/llama2jax/blob/ab33e1f15489daa8b9040389c77e486cd450e461/lib/tree_utils/__init__.py)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,Adding `tree_util.stack_leaves()` and `tree_util.unstack_leaves()`, `stack_leaves`: Stack the leaves of one or more PyTrees along a new axis.  `unstack_leaves`: Unstack the leaves of a PyTree. References:  https://docs.lieselproject.org/en/v0.1.4/_modules/liesel/goose/pytree.htmlstack_leaves  https://gist.github.com/willwhitney/dd89cac6a5b771ccff18b06b33372c75?permalink_comment_id=4634557gistcomment4634557  https://github.com/ayaka14732/llama2jax/blob/ab33e1f15489daa8b9040389c77e486cd450e461/lib/tree_utils/__init__.py,2024-04-25T13:36:30Z,enhancement,open,0,5,https://github.com/jax-ml/jax/issues/20934,"To be clear, are these the semantics you have in mind? ","> To be clear, are these the semantics you have in mind? Yes","For something like this, I'd probably lean toward recommending users implement what they need via existing API composability, rather than providing a new API for something that can already be pretty succinctly expressed. What do you think?",Maybe adding tree util cookbook would be useful?  ,"A pytree cookbook would be an interesting idea! This idea also came up in CC(Provide utilities for creating pytrees filled with random samples). , is that something you'd be interested in thinking about?"
710,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jax.numpy.insert returning incorrect results wen jitted on Metal)ï¼Œ å†…å®¹æ˜¯ ( Description (pasting from Apple Developer forum) The `jax.numpy.insert()` function returns an incorrect result (zeropadding the array) when compiled with `jax.jit`. When not `jit`ted, the results are correct.  MWE   Output * x2 (computed with the nonjitted function) is correct; x3 just has zeropadding instead of a column of 99  * The same code run on a nonmetal machine gives the correct results:   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jax.numpy.insert returning incorrect results wen jitted on Metal," Description (pasting from Apple Developer forum) The `jax.numpy.insert()` function returns an incorrect result (zeropadding the array) when compiled with `jax.jit`. When not `jit`ted, the results are correct.  MWE   Output * x2 (computed with the nonjitted function) is correct; x3 just has zeropadding instead of a column of 99  * The same code run on a nonmetal machine gives the correct results:   System info (python version, jaxlib version, accelerator, etc.) ",2024-04-24T16:53:56Z,bug Apple GPU (Metal) plugin,open,0,1,https://github.com/jax-ml/jax/issues/20918,It is reproducible. The jitted module is incorrectly optimized and we will look into the fix. 
784,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Pallas jax.lax.fori_loop over long inputs slows down)ï¼Œ å†…å®¹æ˜¯ ( Description Inside Pallas kernels, we often want a loop, and to speed up compilation, we typically use a scan function such as jax.lax.fori_loop. (For example, in the attention kernel example here.) As the length of the loop grows, fori_loop slows down execution substantially (relative to using a Python forloop). I put together a minimal script to isolate the issue, and running it on an A6000, saw a **23x slowdown** on long loops:  Here is the script that generated these results:   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Pallas jax.lax.fori_loop over long inputs slows down," Description Inside Pallas kernels, we often want a loop, and to speed up compilation, we typically use a scan function such as jax.lax.fori_loop. (For example, in the attention kernel example here.) As the length of the loop grows, fori_loop slows down execution substantially (relative to using a Python forloop). I put together a minimal script to isolate the issue, and running it on an A6000, saw a **23x slowdown** on long loops:  Here is the script that generated these results:   System info (python version, jaxlib version, accelerator, etc.) ",2024-04-24T05:16:28Z,bug pallas,open,0,1,https://github.com/jax-ml/jax/issues/20909,"I think the best explanation I found online is the following: > To elaborate on this, the reason GPU is so fast for vectorized operations is not that individual floating point operations are particularly fast (they're actually often slower than similar operations on a CPU!), but rather that it can very efficiently run many operations in parallel. For an operation like scan in which each step depends on the output of the previous, the sequence of operations as a whole cannot be parallelized. So you end up not taking advantage of any of the GPU's inherent parallelism, and the result is slow execution. > Contrast this to CPU, where individual floating point operations are relatively fast, but there is no so much inbuilt parallelism available. Because of this, scan does not incur as much of a performance penalty. I think this falls into the same bucket. Like my example, where the input of each layer depended on the output of the previous one (the carry), this is a pure sequential loop. There is probably a sweet spot with the unroll parameter where compilation times and loop times are optimal."
807,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix `jax.scipy.stats.poisson.logpmf` to emulate `scipy.stats.poisson.logpmf` for non-integer values of `k`)ï¼Œ å†…å®¹æ˜¯ (Inconsistency exists between `jax.scipy.stats.poisson.logpmf` and `scipy.stats.poisson.logpmf` for noninteger `k`. This pull request rectifies this behavior, enabling both functions `jax.scipy.stats.poisson.logpmf` and `jax.scipy.stats.poisson.pmf`  to emulate their corresponding `scipy` counterparts for any `k` value. Partly addresses CC(jax.scipy.stats has different behavior than scipy.stats when arguments are out of the support and when parameters lead to undefined densities) Current Behavior:  With this change: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Fix `jax.scipy.stats.poisson.logpmf` to emulate `scipy.stats.poisson.logpmf` for non-integer values of `k`,"Inconsistency exists between `jax.scipy.stats.poisson.logpmf` and `scipy.stats.poisson.logpmf` for noninteger `k`. This pull request rectifies this behavior, enabling both functions `jax.scipy.stats.poisson.logpmf` and `jax.scipy.stats.poisson.pmf`  to emulate their corresponding `scipy` counterparts for any `k` value. Partly addresses CC(jax.scipy.stats has different behavior than scipy.stats when arguments are out of the support and when parameters lead to undefined densities) Current Behavior:  With this change: ",2024-04-23T16:56:57Z,pull ready,closed,0,3,https://github.com/jax-ml/jax/issues/20891,Please fix the trailing whitespace (see the CI failures),Please also squash your changes into a single commit; see https://jax.readthedocs.io/en/latest/contributing.htmlsinglechangecommitsandpullrequests. Thanks!,Squashed all commits into single commit
340,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(In QDWH, eliminate one of the two triangular solves in the steps using Cholesky.)ï¼Œ å†…å®¹æ˜¯ (In QDWH, eliminate one of the two triangular solves in the steps using Cholesky.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"In QDWH, eliminate one of the two triangular solves in the steps using Cholesky.","In QDWH, eliminate one of the two triangular solves in the steps using Cholesky.",2024-04-23T01:06:33Z,,closed,0,1,https://github.com/jax-ml/jax/issues/20878,Closing Copybara created PR due to inactivity
1447,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorflow Building from Source Code)ï¼Œ å†…å®¹æ˜¯ ( Description I am trying to build Tensorflow from source because i want to run tensorflow using c++ and i am trying to download the GPU version So I ran ./Configure command, and choose the option to enable GPU Support I ran the bazel command given on the tensorflow Build From source on windows  https://www.tensorflow.org/install/source_windows and ran into the following error, What does it imply and how can i Solve it ERROR: An error occurred during the fetch of repository 'local_config_cuda':    Traceback (most recent call last):         File ""D:/tensorflow/tensorflow2.9.0/third_party/gpus/cuda_configure.bzl"", line 1401, column 38, in _cuda_autoconf_impl                 _create_local_cuda_repository(repository_ctx)         File ""D:/tensorflow/tensorflow2.9.0/third_party/gpus/cuda_configure.bzl"", line 1179, column 17, in _create_local_cuda_repository                 cc = find_cc(repository_ctx)         File ""D:/tensorflow/tensorflow2.9.0/third_party/gpus/cuda_configure.bzl"", line 216, column 34, in find_cc                 return _get_msvc_compiler(repository_ctx)         File ""D:/tensorflow/tensorflow2.9.0/third_party/gpus/cuda_configure.bzl"", line 133, column 26, in _get_msvc_compiler                 return fin)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Tensorflow Building from Source Code," Description I am trying to build Tensorflow from source because i want to run tensorflow using c++ and i am trying to download the GPU version So I ran ./Configure command, and choose the option to enable GPU Support I ran the bazel command given on the tensorflow Build From source on windows  https://www.tensorflow.org/install/source_windows and ran into the following error, What does it imply and how can i Solve it ERROR: An error occurred during the fetch of repository 'local_config_cuda':    Traceback (most recent call last):         File ""D:/tensorflow/tensorflow2.9.0/third_party/gpus/cuda_configure.bzl"", line 1401, column 38, in _cuda_autoconf_impl                 _create_local_cuda_repository(repository_ctx)         File ""D:/tensorflow/tensorflow2.9.0/third_party/gpus/cuda_configure.bzl"", line 1179, column 17, in _create_local_cuda_repository                 cc = find_cc(repository_ctx)         File ""D:/tensorflow/tensorflow2.9.0/third_party/gpus/cuda_configure.bzl"", line 216, column 34, in find_cc                 return _get_msvc_compiler(repository_ctx)         File ""D:/tensorflow/tensorflow2.9.0/third_party/gpus/cuda_configure.bzl"", line 133, column 26, in _get_msvc_compiler                 return fin",2024-04-22T21:54:38Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/20871,"This repository is for JAX, not TensorFlow. You'd need to file this issue on the TensorFlow github project."
1005,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Unbound axis names within scan within custom partitioning)ï¼Œ å†…å®¹æ˜¯ ( Description I encountered this problem while implementing a ring attention algorithm using custom_partitioning. A custom partitioning of jnp.sum using the following contrived partitioning implementation (see full self contained code at the end of this report) throws an error, saying the axis name cannot be found:  However, it works fine if the part_func does not contain a scan, such as:  This also works if shard_map is used to directly call part_func instead of via custom partitioning, but only if check_rep=False (check_rep=True throws this error, which seems like a different issue but I am unsure: `Scan carry input and output got mismatched replication types [None] and [{'x'}]`). Full code:   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Unbound axis names within scan within custom partitioning," Description I encountered this problem while implementing a ring attention algorithm using custom_partitioning. A custom partitioning of jnp.sum using the following contrived partitioning implementation (see full self contained code at the end of this report) throws an error, saying the axis name cannot be found:  However, it works fine if the part_func does not contain a scan, such as:  This also works if shard_map is used to directly call part_func instead of via custom partitioning, but only if check_rep=False (check_rep=True throws this error, which seems like a different issue but I am unsure: `Scan carry input and output got mismatched replication types [None] and [{'x'}]`). Full code:   System info (python version, jaxlib version, accelerator, etc.) ",2024-04-22T14:05:20Z,bug,closed,0,0,https://github.com/jax-ml/jax/issues/20864
281,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([jax:mosaic-gpu] FragmentedArray can do tiled load.)ï¼Œ å†…å®¹æ˜¯ ([jax:mosaicgpu] FragmentedArray can do tiled load.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[jax:mosaic-gpu] FragmentedArray can do tiled load.,[jax:mosaicgpu] FragmentedArray can do tiled load.,2024-04-18T19:01:52Z,,closed,0,1,https://github.com/jax-ml/jax/issues/20820,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
893,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Begin deprecation of implicit input conversion in FFT module)ï¼Œ å†…å®¹æ˜¯ (Towards CC(Update `jax.experimental.array_api` to v2023.12 API) Array API 2023 changelog.  Updates the `jax.numpy.fft` namespace to begin a deprecation of implicitly converted inputs  explicitly sets `fft` function domains to either `real floating` or `complex`, warning on implicit conversion (i.e. `fft(x: float32)`, `rfft(x: int32)`, etc.). Updates the `jax.experimental.array_api.fft` namespace with corresponding changes which directly raise `ValueError`, marked with deprecation once `jax.numpy.fft` is array API compliant. Adds corresponding tests for both modules. Updates dtype coverage in `tests/fft_test.py` to reflect new restricted domains.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Begin deprecation of implicit input conversion in FFT module,"Towards CC(Update `jax.experimental.array_api` to v2023.12 API) Array API 2023 changelog.  Updates the `jax.numpy.fft` namespace to begin a deprecation of implicitly converted inputs  explicitly sets `fft` function domains to either `real floating` or `complex`, warning on implicit conversion (i.e. `fft(x: float32)`, `rfft(x: int32)`, etc.). Updates the `jax.experimental.array_api.fft` namespace with corresponding changes which directly raise `ValueError`, marked with deprecation once `jax.numpy.fft` is array API compliant. Adds corresponding tests for both modules. Updates dtype coverage in `tests/fft_test.py` to reflect new restricted domains.",2024-04-18T17:29:44Z,,closed,0,4,https://github.com/jax-ml/jax/issues/20818,"I think this is a case where we should follow numpy rather than following the Array API to the letter. There are many places where the array API is more restrictive than NumPy, which we've been taking our cue from up until this point. Deprecating previously supported behavior when NumPy still supports it will lead to undue downstream churn, so I think we should avoid it. What do you think?","> I think this is a case where we should follow numpy rather than following the Array API to the letter. There are many places where the array API is more restrictive than NumPy, which we've been taking our cue from up until this point. Deprecating previously supported behavior when NumPy still supports it will lead to undue downstream churn, so I think we should avoid it. What do you think? I would generally agree, however there are a few benefits for this that make me prefer the domain restrictions. The domain restriction itself forces users to make an explicit decision regarding the transform they choose to use, pushing for using the most specific transform possible as opposed e.g. `fft` everything. Looking towards the scientific computing community, I think this by itself is a valuable aid, especially for folks that aren't familiar with the computational implications of `fft` vs say `rfft` or `hfft`. Also, pulling from  who provided a helpful summary of some of the discussion that went into the decision from the consortium, both in meetings and in PRs (e.g. this comment thread): > in the spec, we've sided against implicit upcasting from realtocomplex, as users should be clear about what/how they want to cast. While a realvalued array can be interpreted as a complex array having omitted imaginary components which are all zero, this doesn't have to be the case. The array could be imaginary components or interleaved real and imaginary, etc. Particularly for the FFT APIs, certain transforms are very explicitly C2C or C2R transforms. In which case, providing a realvalued array is not the intent. And ultimately afaik NumPy 2.0 _will_ be implementing the domain restrictions, as `numpy.array_api.fft` already does and they intend for the general `numpy` namespace to be properly array API compatible, so we're just getting ahead of them on this one. We don't need to complete the deprecation until they've fully adopted it anyways, so we won't be fully diverging. cc:  in case you have some opinions on this too","> And ultimately afaik NumPy 2.0 will be implementing the domain restrictions, Do you have a link to a NEP or other forum that discusses this?","Okay it looks like I had misinterpreted the word ""**should**"" in the array API spec to imply room for backwards compatibility _with the intent of deprecation_, given the changelog wording > `fft.fft`: **require** the input array to have a complexvalued floatingpoint data type however, that is not the case. As such, we are already compliant and this PR is not needed. Thanks for helping me catch that "
338,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Added int4 and uint4 to dtype-specific tests)ï¼Œ å†…å®¹æ˜¯ (I probably missed some cases, so this PR is really just the first step in making sure we have good *int4 coverage.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Added int4 and uint4 to dtype-specific tests,"I probably missed some cases, so this PR is really just the first step in making sure we have good *int4 coverage.",2024-04-18T14:22:43Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/20816
257,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Import etils.epath lazily.)ï¼Œ å†…å®¹æ˜¯ (Import etils.epath lazily. Reduces jax import time.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Import etils.epath lazily.,Import etils.epath lazily. Reduces jax import time.,2024-04-16T19:55:29Z,,closed,0,0,https://github.com/jax-ml/jax/issues/20786
452,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Update JAX official doc: point out that the device numbers are not in numerical order because of the underlying torus hardware topology.)ï¼Œ å†…å®¹æ˜¯ (Update JAX official doc: point out that the device numbers are not in numerical order because of the underlying torus hardware topology.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Update JAX official doc: point out that the device numbers are not in numerical order because of the underlying torus hardware topology.,Update JAX official doc: point out that the device numbers are not in numerical order because of the underlying torus hardware topology.,2024-04-16T18:08:28Z,,closed,0,0,https://github.com/jax-ml/jax/issues/20783
1385,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Latency Hiding Scheduler leads to x5 memory usage if used without jax.lax.scan)ï¼Œ å†…å®¹æ˜¯ ( Description Hi, we're training large (300B, 60 layers) mixture of experts transformer on a 1000+ GPU. We have some nonuniformity in layers so we can't use jax.lax.scan directly to stack layers together  instead, we just call each layer independently.  Model doesn't have completely random structure, it is like (3 layers with same structure, 1 with another) repeated 15 times (to achieve 60 layers in total) We would benefit a LOT from overlapping computations & communications but when we try to enable latency hiding scheduler `xla_gpu_enable_latency_hiding_scheduler`, this leads to increased in memory usage by a factor of 45 (from 50Gb per GPU to 200250Gb per GPU, which is completely unusable).  My guess is that compiler doesn't reuse buffers for async comms in this case for different layers. We've tested also variant with jax.lax.scan and uniform layers, it seemed to work okay from memory usage point of view  only 2025% overhead from latency hiding scheduler. Is this a known problem? Is these any workaround?  System info (python version, jaxlib version, accelerator, etc.) tested on 0.4.25/0.4.26, 1000+ H100 GPU)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Latency Hiding Scheduler leads to x5 memory usage if used without jax.lax.scan," Description Hi, we're training large (300B, 60 layers) mixture of experts transformer on a 1000+ GPU. We have some nonuniformity in layers so we can't use jax.lax.scan directly to stack layers together  instead, we just call each layer independently.  Model doesn't have completely random structure, it is like (3 layers with same structure, 1 with another) repeated 15 times (to achieve 60 layers in total) We would benefit a LOT from overlapping computations & communications but when we try to enable latency hiding scheduler `xla_gpu_enable_latency_hiding_scheduler`, this leads to increased in memory usage by a factor of 45 (from 50Gb per GPU to 200250Gb per GPU, which is completely unusable).  My guess is that compiler doesn't reuse buffers for async comms in this case for different layers. We've tested also variant with jax.lax.scan and uniform layers, it seemed to work okay from memory usage point of view  only 2025% overhead from latency hiding scheduler. Is this a known problem? Is these any workaround?  System info (python version, jaxlib version, accelerator, etc.) tested on 0.4.25/0.4.26, 1000+ H100 GPU",2024-04-15T13:26:26Z,bug NVIDIA GPU,open,0,4,https://github.com/jax-ml/jax/issues/20763,"Here is some toy repro tested on JAX 0.4.34     Latency hiding scheduler enabled, XLA_FLAGS: `xla_gpu_graph_level=0 xla_gpu_enable_triton_gemm=false xla_gpu_enable_command_buffer= xla_gpu_enable_latency_hiding_scheduler=true` SCAN=False (hits OOM):  SCAN=True:  Latency hiding scheduler disabled, XLA_FLAGS=`xla_gpu_graph_level=0 xla_gpu_enable_triton_gemm=false xla_gpu_enable_command_buffer=` SCAN=False:  SCAN=True "," Using JAX 0.4.35 `XLA_FLAGS=""xla_gpu_graph_level=0 xla_gpu_enable_triton_gemm=false xla_gpu_enable_command_buffer= ""` and `SCAN=False`, I'm seeing a failure.   Any chance you have other flags or env variables set? ", Can you please set `XLA_CLIENT_MEM_FRACTION=0.95` and use `xla_gpu_copy_insertion_use_region_analysis` in addition to your existing flags and report back if it resolves the issue?," `xla_gpu_memory_limit_slop_factor` flag could also help in this case. The default value is 95, so you can experiment with lower values (90, 80, 70, etc.). You can find more info about this flag at https://github.com/jaxml/jax/blob/main/docs/gpu_performance_tips.md. Let me know if you see any issues."
493,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Optimize `_create_copy_plan` in array.py)ï¼Œ å†…å®¹æ˜¯ (Optimize `_create_copy_plan` in array.py * `_get_device` is called from many tight loops, so it's worth avoiding unnecessary work as much as possible. * `_create_copy_plan` now uses sharding's `_internal_device_list` instead of querying the device of every shard in a loop.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Optimize `_create_copy_plan` in array.py,"Optimize `_create_copy_plan` in array.py * `_get_device` is called from many tight loops, so it's worth avoiding unnecessary work as much as possible. * `_create_copy_plan` now uses sharding's `_internal_device_list` instead of querying the device of every shard in a loop.",2024-04-12T19:36:48Z,,closed,0,0,https://github.com/jax-ml/jax/issues/20735
1474,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([callbacks] io_callback while_loop batching rule fix   ~(^.~!)z)ï¼Œ å†…å®¹æ˜¯ (~**Note to reviewer**: this PR currently contains the commit from CC([callbacks] io_callback batching rule accidentally called pure_callback), but once that goes in I'll rebase it away.~ When we batch a `while_loop(cond_fun, body_fun, init_val)`, two things can happen: If the predicate **is not** batched (because only some elements of `batched_init_val` are batched, and the predicate value doesn't depend on any of them), then we do something like this:  If the predicate **is** batched, we do something like this:  The latter poses a problem when side effects are present in `body_fun`, even if they are unordered: we're relying on our ability to run extra iterations of the loop body for elements of the batch that have already finished, and then just mask those updates off. That works great when body_fun is functionally pure, but not so great when someone might notice our side effects! The former actually has no issue for unordered effects. Before this PR, while_loop's batching rule would blow up whenever there were ordered or unordered IO effects in cond_fun or body_fun. This PR relaxes the check and allows the case where the body contains unordered effects and the predicate is unbatched. This came up in CC(Support)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[callbacks] io_callback while_loop batching rule fix   ~(^.~!)z,"~**Note to reviewer**: this PR currently contains the commit from CC([callbacks] io_callback batching rule accidentally called pure_callback), but once that goes in I'll rebase it away.~ When we batch a `while_loop(cond_fun, body_fun, init_val)`, two things can happen: If the predicate **is not** batched (because only some elements of `batched_init_val` are batched, and the predicate value doesn't depend on any of them), then we do something like this:  If the predicate **is** batched, we do something like this:  The latter poses a problem when side effects are present in `body_fun`, even if they are unordered: we're relying on our ability to run extra iterations of the loop body for elements of the batch that have already finished, and then just mask those updates off. That works great when body_fun is functionally pure, but not so great when someone might notice our side effects! The former actually has no issue for unordered effects. Before this PR, while_loop's batching rule would blow up whenever there were ordered or unordered IO effects in cond_fun or body_fun. This PR relaxes the check and allows the case where the body contains unordered effects and the predicate is unbatched. This came up in CC(Support",2024-04-12T04:15:30Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/20726
1469,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Let caller switch implementation of reduction after import)ï¼Œ å†…å®¹æ˜¯ (Let caller switch implementation of reduction after import Thank you to gnecula@ for adding the jax2tf_associative_scan_reductions flag and context: https://github.com/google/jax/pull/9189/commits/5bfe1852a4626680f3612beee7d1a8b7a66fc79c   For GPU, the specific implementation of `cumsum` can make the whopping difference between a latency in microseconds versus milliseconds! Before this change, adjusting the method of lowering `cumsum` via this scope has no effect:  ... because the cumsum method (and other reduce methods) have their implementations set when the `jax2tf` library is imported, ie when this line is called:  Thus, any future switches of the implementation (to, say associative scanning), even if they happen before the `jax2tf.convert` method executes, had no effect because methods such as `cumsum` had already been curried at import time. This change fixes that by varying the implementation based on the current value of `config.jax2tf_associative_scan_reductions`. We use existing tests to verify the continued correctness of this CL that affects latency. We add TPU to the list of devices to apply some limitations  One TPU unit test had suddenly failed because the scope now works: Even though TPUs use a d)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Let caller switch implementation of reduction after import,"Let caller switch implementation of reduction after import Thank you to gnecula@ for adding the jax2tf_associative_scan_reductions flag and context: https://github.com/google/jax/pull/9189/commits/5bfe1852a4626680f3612beee7d1a8b7a66fc79c   For GPU, the specific implementation of `cumsum` can make the whopping difference between a latency in microseconds versus milliseconds! Before this change, adjusting the method of lowering `cumsum` via this scope has no effect:  ... because the cumsum method (and other reduce methods) have their implementations set when the `jax2tf` library is imported, ie when this line is called:  Thus, any future switches of the implementation (to, say associative scanning), even if they happen before the `jax2tf.convert` method executes, had no effect because methods such as `cumsum` had already been curried at import time. This change fixes that by varying the implementation based on the current value of `config.jax2tf_associative_scan_reductions`. We use existing tests to verify the continued correctness of this CL that affects latency. We add TPU to the list of devices to apply some limitations  One TPU unit test had suddenly failed because the scope now works: Even though TPUs use a d",2024-04-12T02:08:26Z,,closed,0,0,https://github.com/jax-ml/jax/issues/20724
776,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jax.distributed.initialize() fails on jax==0.4.26)ï¼Œ å†…å®¹æ˜¯ ( Description The following example, works on jax==0.4.25, but yields a CUDA error on jax==0.4.26. It is being run on 4 processes with 1 GPU each (2 nodes x 2 GPUs), on a SLURM cluster.  **Failure with jax.distributed.initialize() for jax==0.4.26** (output appears 4 times because all process write to the same stdout file):  **Working/expected output on jax==0.4.24**:  **The SLURM jobfile:**   System info (python version, jaxlib version, accelerator, etc.) Output of jax.print_environment_info() **without calling jax.distributed.initialize()** )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jax.distributed.initialize() fails on jax==0.4.26," Description The following example, works on jax==0.4.25, but yields a CUDA error on jax==0.4.26. It is being run on 4 processes with 1 GPU each (2 nodes x 2 GPUs), on a SLURM cluster.  **Failure with jax.distributed.initialize() for jax==0.4.26** (output appears 4 times because all process write to the same stdout file):  **Working/expected output on jax==0.4.24**:  **The SLURM jobfile:**   System info (python version, jaxlib version, accelerator, etc.) Output of jax.print_environment_info() **without calling jax.distributed.initialize()** ",2024-04-11T12:23:46Z,bug,closed,1,3,https://github.com/jax-ml/jax/issues/20711,"I think this bug has already been fixed at head, but it made it into the last release because we don't have any way to test SLURM clusters in CI. Can you try with a nightly jaxlib and jax?","You are right, it is indeed already fixed on the nightly build. It works as expected using  **Thanks!**","Ran into the same issue without using SLURM, but multinode GPUs. Same fix worked with these too: > jax==0.4.27.dev20240415 > jaxlib==0.4.27.dev20240415+cuda12.cudnn89 Just dropping the install lines here in case anyone needed it like I did (got it from MaxText) "
636,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Automatic sharded parallelization of per-sample gradients fails on GPUs)ï¼Œ å†…å®¹æ˜¯ ( Description The following minimal example works when run on 2 CPUs (CUDA_VISBLE_DEVICES=""""), but fails when run on GPUs due to **Unexpected XLA sharding override**.  It appears to be specific to the combination of vmap and grad, and does not appear when using only vmap or only grad. In case of failure the output is:   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",dspy,Automatic sharded parallelization of per-sample gradients fails on GPUs," Description The following minimal example works when run on 2 CPUs (CUDA_VISBLE_DEVICES=""""), but fails when run on GPUs due to **Unexpected XLA sharding override**.  It appears to be specific to the combination of vmap and grad, and does not appear when using only vmap or only grad. In case of failure the output is:   System info (python version, jaxlib version, accelerator, etc.) ",2024-04-11T11:35:48Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/20710,I was able to repro this and have filed an internal bug. It should be fixed soon :),https://github.com/openxla/xla/pull/11483 should fix it!
735,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Provide output buffer for pure_callback result)ï¼Œ å†…å®¹æ˜¯ (Is there any value in supplying a pure_callback (one with a return value), a buffer into which to place it's output? Something like the code below. I have some science applications where I need to use bindings to underlying C++ code that require taking an output buffer into which to place the results. I currently need to create the array inside the callback function, but I suspect that JAX might already have some preallocated buffer waiting for the result anyways, and could provide that to the function. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Provide output buffer for pure_callback result,"Is there any value in supplying a pure_callback (one with a return value), a buffer into which to place it's output? Something like the code below. I have some science applications where I need to use bindings to underlying C++ code that require taking an output buffer into which to place the results. I currently need to create the array inside the callback function, but I suspect that JAX might already have some preallocated buffer waiting for the result anyways, and could provide that to the function. ",2024-04-10T20:50:08Z,enhancement,open,1,12,https://github.com/jax-ml/jax/issues/20701,"Also, is it safe to reuse the buffer of input values, e.g. like  EDIT: That would be a nope as it gives `jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Generated function failed: CpuCallback error: ValueError: output array is readonly`","There is a preallocated buffer on the XLA side, but it is not currently passed to the `pure_callback`.   should we have an API for this, wdyt?","Also, the main reason this would be helpful is because the output buffers for the science applications I'm working on are really large so if there is already one allocated by XLA it would save lots of memory to use that one.",  any update available for this?,"Hey , sorry for the silence. Here is a quick update *  is working on a proposal to allow ""write once"" mutation for `jax.Array`s which are currently immutable (and thus cannot be used for output buffers); * I'm exploring a parallel idea  `dlpack.callback`, a new callback API using DLPack capsules for both inputs and outputs.","Another quick update: I prototyped `dlpack.callback`, but after discussing it with a few JAX team members, I decided not to move forward with it as JAX has too many callback APIs already. Instead, the plan is to change existing callback APIs to support `mutable_results=`. I am waiting on a few changes in XLA FFI, but once they land, it should be fairly straightforward to implement this.","Hi , any news?","No news yet in a sense that none of my attempts landed. Hopefully, in the coming weeks :)", can you check out JAXbind which offers a way to specify both JVP and VJP for external callbacks? I argue that it should be also possible within JAX to specify both.," â€” JAX doesn't currently offer a public API for this (customizing multiple transforms for a single callable), but it's on our radar. I'd say that this is off topic for this issue thread, but feel free to open another with more info about your use cases for using both JVP and VJP for one callback!","Re original topic:  suggested passing the output buffer as an argument and donating it to the callback, i.e.  this will ensure that the runtime doesn't allocate a separate buffer for the output. Unfortunately, though, there isn't a great way for creating a mutable view of a `jax.Array`. I was hoping we can do `jax.Array` > DLPack > NumPy, but NumPy imports DLPack capsules as readonly.","Would donating the input avoid double allocating memory? I can imagine if it's something like: invoke pure callback > free input > allocate inside the pure callback > use that output array in JAX without reallocation. We're memory limited in this use case, `ducc.wgridder`."
759,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([key reuse] refactor & unify key reuse rule registration)ï¼Œ å†…å®¹æ˜¯ (This refactoring will pave the way for making key reuse checking part of the core dispatch path. There are two goals: 1. Move from having two tables of key reuse rules (static and dynamic) to having a single table: this will let us begin easily specifying key reuse signatures at the location where primitives are defined. 2. Move shared code into single utility functions that can be referenced from the main dispatch path: this will let us move away from the current approach which involves monkeypatching the impl rules.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[key reuse] refactor & unify key reuse rule registration,This refactoring will pave the way for making key reuse checking part of the core dispatch path. There are two goals: 1. Move from having two tables of key reuse rules (static and dynamic) to having a single table: this will let us begin easily specifying key reuse signatures at the location where primitives are defined. 2. Move shared code into single utility functions that can be referenced from the main dispatch path: this will let us move away from the current approach which involves monkeypatching the impl rules.,2024-04-10T17:52:46Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/20692
909,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(On jax-metal, updating multidimensional boolean arrays sometimes fails)ï¼Œ å†…å®¹æ˜¯ ( Description I ran into a rather surprising case involving 3D boolean arrays, which only seems to fail on jaxmetal. Correct behavior (CPU):  But on jaxmetal, I get:  After playing around with some inputs, the problem seems to occur for `.at[:, i]` and `.at[:, :, i]`, but `.at[i]` works fine. So, any dimension higher than 0 seems to have a bug in the scatter update algorithm for booleans. Is there some way I can help debug this? Is the jaxmetal code open source? If it is, then if you point me to build instructions, I can try to track down the bug.  System info (python version, jaxlib version, accelerator, etc.) On jaxmetal:  On CPU (correct behavior): )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"On jax-metal, updating multidimensional boolean arrays sometimes fails"," Description I ran into a rather surprising case involving 3D boolean arrays, which only seems to fail on jaxmetal. Correct behavior (CPU):  But on jaxmetal, I get:  After playing around with some inputs, the problem seems to occur for `.at[:, i]` and `.at[:, :, i]`, but `.at[i]` works fine. So, any dimension higher than 0 seems to have a bug in the scatter update algorithm for booleans. Is there some way I can help debug this? Is the jaxmetal code open source? If it is, then if you point me to build instructions, I can try to track down the bug.  System info (python version, jaxlib version, accelerator, etc.) On jaxmetal:  On CPU (correct behavior): ",2024-04-10T01:04:00Z,bug Apple GPU (Metal) plugin,open,0,2,https://github.com/jax-ml/jax/issues/20675,(Note that integers and floats seem to work fine; only `dtype=jnp.bool_` seems affected.),jaxmetal is not open sourced as of the time.  We'll look into the issue and update any change here. 
299,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(better unsupported indexing handling in lax_numpy.py)ï¼Œ å†…å®¹æ˜¯ (In replacement of CC(unsupported indexer reporting in lax_numpy)   )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,better unsupported indexing handling in lax_numpy.py,In replacement of CC(unsupported indexer reporting in lax_numpy)   ,2024-04-09T18:09:56Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/20666
1451,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump actions/upload-artifact from 3 to 4)ï¼Œ å†…å®¹æ˜¯ (Bumps actions/uploadartifact from 3 to 4.  Release notes Sourced from actions/uploadartifact's releases.  v4.0.0 What's Changed The release of uploadartifact and downloadartifact are major changes to the backend architecture of Artifacts. They have numerous performance and behavioral improvements. â„¹ï¸ However, this is a major update that includes breaking changes. Artifacts created with versions v3 and below are not compatible with the v4 actions. Uploads and downloads must use the same major actions versions. There are also key differences from previous versions that may require updates to your workflows. For more information, please see:  The changelog post. The README. The migration documentation. As well as the underlying npm package, @â€‹actions/artifact documentation.  New Contributors  @â€‹vmjoseph made their first contribution in actions/uploadartifact CC(Implement a scatterupdate operator.)  Full Changelog: https://github.com/actions/uploadartifact/compare/v3...v4.0.0 v3.1.3 What's Changed  chore(github): remove trailing whitespaces by @â€‹ljmf00 in actions/uploadartifact CC(skip some cases to satisfy internal tests) Bump @â€‹actions/artifact version to v1.1.2 by @â€‹bethanyj28 in actions/uploadartifact CC(Move ma)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bump actions/upload-artifact from 3 to 4,"Bumps actions/uploadartifact from 3 to 4.  Release notes Sourced from actions/uploadartifact's releases.  v4.0.0 What's Changed The release of uploadartifact and downloadartifact are major changes to the backend architecture of Artifacts. They have numerous performance and behavioral improvements. â„¹ï¸ However, this is a major update that includes breaking changes. Artifacts created with versions v3 and below are not compatible with the v4 actions. Uploads and downloads must use the same major actions versions. There are also key differences from previous versions that may require updates to your workflows. For more information, please see:  The changelog post. The README. The migration documentation. As well as the underlying npm package, @â€‹actions/artifact documentation.  New Contributors  @â€‹vmjoseph made their first contribution in actions/uploadartifact CC(Implement a scatterupdate operator.)  Full Changelog: https://github.com/actions/uploadartifact/compare/v3...v4.0.0 v3.1.3 What's Changed  chore(github): remove trailing whitespaces by @â€‹ljmf00 in actions/uploadartifact CC(skip some cases to satisfy internal tests) Bump @â€‹actions/artifact version to v1.1.2 by @â€‹bethanyj28 in actions/uploadartifact CC(Move ma",2024-04-08T17:42:57Z,dependencies github_actions,closed,0,2,https://github.com/jax-ml/jax/issues/20639,"Closing because the commit hash looks wrong. We already point to the v4.3.1 commit hash (https://github.com/actions/uploadartifact/commit/5d5d22a31266ced268874388b861e4b58bb5c2f3) it's unclear why the bot has chosen `ef09cdac3e2d3e60d8ccadda691f4f1cec5035cb`, as it doesn't correspond to any release tag.","OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
1456,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump actions/setup-python from 4.7.1 to 5.1.0)ï¼Œ å†…å®¹æ˜¯ (Bumps actions/setuppython from 4.7.1 to 5.1.0.  Release notes Sourced from actions/setuppython's releases.  v5.1.0 What's Changed  Leveraging the raw API to retrieve the versionmanifest, as it does not impose a rate limit and hence facilitates unrestricted consumption without the need for a token for Github Enterprise Servers by @â€‹Shegox in actions/setuppython CC(Generic support for JIT compilation with custom NumPy ops). Dependency updates by @â€‹dependabot and @â€‹HarithaVattikuti in actions/setuppython CC(JAX slow compared to numba for certain calculations) Documentation changes for version in README by @â€‹basnijholt in actions/setuppython CC(Scatter transpose rule doesnâ€™t handle symbolic zeros) Documentation changes for link in README by @â€‹ukd1 in actions/setuppython CC(vmap error with emtpy tuple) Documentation changes for link in Advanced Usage by @â€‹Jamim in actions/setuppython CC(Improve behavior of a number of math functions for extreme inputs.) Documentation changes for avoiding rate limit issues on GHES by @â€‹priyakinthali in actions/setuppython CC(IndexError: Indexing mode not yet supported)  New Contributors  @â€‹basnijholt made their first contribution in actions/setuppython CC(Scatter transpose rule doesnâ€™)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bump actions/setup-python from 4.7.1 to 5.1.0,"Bumps actions/setuppython from 4.7.1 to 5.1.0.  Release notes Sourced from actions/setuppython's releases.  v5.1.0 What's Changed  Leveraging the raw API to retrieve the versionmanifest, as it does not impose a rate limit and hence facilitates unrestricted consumption without the need for a token for Github Enterprise Servers by @â€‹Shegox in actions/setuppython CC(Generic support for JIT compilation with custom NumPy ops). Dependency updates by @â€‹dependabot and @â€‹HarithaVattikuti in actions/setuppython CC(JAX slow compared to numba for certain calculations) Documentation changes for version in README by @â€‹basnijholt in actions/setuppython CC(Scatter transpose rule doesnâ€™t handle symbolic zeros) Documentation changes for link in README by @â€‹ukd1 in actions/setuppython CC(vmap error with emtpy tuple) Documentation changes for link in Advanced Usage by @â€‹Jamim in actions/setuppython CC(Improve behavior of a number of math functions for extreme inputs.) Documentation changes for avoiding rate limit issues on GHES by @â€‹priyakinthali in actions/setuppython CC(IndexError: Indexing mode not yet supported)  New Contributors  @â€‹basnijholt made their first contribution in actions/setuppython CC(Scatter transpose rule doesnâ€™",2024-04-08T17:42:51Z,dependencies github_actions,closed,0,2,https://github.com/jax-ml/jax/issues/20638,Closing because the ratchet tag is wrong; I'll fix this manually.,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
796,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jax.debug.callback changes array type)ï¼Œ å†…å®¹æ˜¯ ( Description  For some reason, `debug.callback` rematerializes the array as a numpy array instead of `jaxlib.xla_extension.ArrayImpl` (the thing created by `jnp.asarray`).  This can cause problems if the array in the callback is mixed with jax arrays when passed to libraries that implement the Array API.  System info (python version, jaxlib version, accelerator, etc.) An NVIDIA GPU may be present on this machine, but a CUDAenabled jaxlib is not installed. Falling back to cpu. jax: 0.4.26 jaxlib: 0.4.26 numpy: 1.26.4 python: 3.11.8 (main, Feb 22 2024, 17:25:49) [GCC 11.4.0])è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jax.debug.callback changes array type," Description  For some reason, `debug.callback` rematerializes the array as a numpy array instead of `jaxlib.xla_extension.ArrayImpl` (the thing created by `jnp.asarray`).  This can cause problems if the array in the callback is mixed with jax arrays when passed to libraries that implement the Array API.  System info (python version, jaxlib version, accelerator, etc.) An NVIDIA GPU may be present on this machine, but a CUDAenabled jaxlib is not installed. Falling back to cpu. jax: 0.4.26 jaxlib: 0.4.26 numpy: 1.26.4 python: 3.11.8 (main, Feb 22 2024, 17:25:49) [GCC 11.4.0]",2024-04-08T02:13:34Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/20627,"Thanks for the report! This is working as intended: callback functions will convert JAX arrays to NumPy arrays, however this will change soon (see CC(jax.pure_callback and jax.experimental.io_callback now use jax.Arrays))"," Great to hear!   So, in an upcoming release then?   Feel free to close this if so :smile: ",Is `debug.callback` going to turn numpy arrays into Jax arrays?,"Yeah, `debug.callback` is now always called with `jax.Array`s. So, if you call it with a NumPy array, JAX will rewrap the buffer into its own array type.", That may cause array API problems if you push a numpy array in and expect it to work with other numpy arrays.  But this is a better default than before!,"True, but you can always do `np.asarray` in the callback and recover the NumPy array, right?","Yes, but I'd have to store the type somewhere.  It's not a problem for me now though.  If it becomes a problem, maybe I'll check back in :smile:"
447,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Segmentation Fault on JAX GPU)ï¼Œ å†…å®¹æ˜¯ ( Description Steps to reproduce:  and then running:  gives:  However, if I install the CPU version of JAX above:  everything is good. Any help would be greatly appreciated.  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Segmentation Fault on JAX GPU," Description Steps to reproduce:  and then running:  gives:  However, if I install the CPU version of JAX above:  everything is good. Any help would be greatly appreciated.  System info (python version, jaxlib version, accelerator, etc.) ",2024-04-07T03:24:52Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/20619,"I tried an older version:  and it seems to work, but with the following warning: "
1001,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Performance issue with 64bit on CPU)ï¼Œ å†…å®¹æ˜¯ ( Description Hi, I encountered a performance issue when enabling 64bit computations, and I would be glad if someone could help me here  is this a known issue, how do i fix it, etc. Below the example.  All 4 functions g1g4 produce the same result (for `x = 0.0`), but g2 takes significantly longer than all other options. Of course, in real use cases, the inputs are not zero. g1 / g2 and g3 / g4 are mathematically equivalent. I tried to keep this code chunk as small as possible without making it hard to understand what's going on. This code is used for finite element computation, so this seemingly random function g has actual meaning to it. I would appreciate any help!  System info (python version, jaxlib version, accelerator, etc.) python 3.12 jax 0.4.26 jaxlib 0.4.26 CPU (AMD))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Performance issue with 64bit on CPU," Description Hi, I encountered a performance issue when enabling 64bit computations, and I would be glad if someone could help me here  is this a known issue, how do i fix it, etc. Below the example.  All 4 functions g1g4 produce the same result (for `x = 0.0`), but g2 takes significantly longer than all other options. Of course, in real use cases, the inputs are not zero. g1 / g2 and g3 / g4 are mathematically equivalent. I tried to keep this code chunk as small as possible without making it hard to understand what's going on. This code is used for finite element computation, so this seemingly random function g has actual meaning to it. I would appreciate any help!  System info (python version, jaxlib version, accelerator, etc.) python 3.12 jax 0.4.26 jaxlib 0.4.26 CPU (AMD)",2024-04-06T19:04:24Z,performance XLA,closed,0,9,https://github.com/jax-ml/jax/issues/20616,"Is it possible that this is a RAM issue? It almost seems like it  with smaller arrays, even with x64 everything looks ok. But then again, I do not see (even with looking at the jaxpr) how the second option should be more expensive in terms of RAM than the others.","I think I tracked down the issue, and it looks like this even happens with 32bit numbers. Consider the following example, where the second function almost uses twice as much memory as the first one. The only thing different is the order of multiplication and dot product  as far as I know, this should not affect memory at all. Is there any way to check this? `","Thanks for the report. It looks like the compiler is able to recognize the constant expression in one case, but not in the other. I don't think I'd consider this a bug: we can't expect the compiler to be perfect, and a longenough sequence of operations may evade its rewrite rules. But we might raise this issue in http://github.com/openxla/xla: perhaps there's a way to update the compiler's heuristics to catch this case.","Thanks for the answer. I don't have any experience with XLA per se, so I wouldn't know where to start there. Also, I don't really know how to observe RAM usage of a compiled function reliably. Currently I think the problem is that in this function  the multiplication `x * d` is not performed inplace even though it would technically be possible. Do you know how it would be possible to confirm this?","I mean, it's just a suspicion. But why else would it need so much more memory than  But then again, I don't even know how to veryify that this memory problem is ""real"" and not just a ""on my machine"" problem.","We can see what the compiler is doing with these functions using ahead of time lowering. For example:  If you inspect the outputs, you see that the second function has a much larger ""bytes accessed"" in the cost analysis. I believe this is due to the compiler not having any heuristic to fuse the operations when they are performed in this order, but I'm not certain of that interpretation.","I think I understand the difference: the expensive allocation is the output of the `einsum`. In `h1`, the input to the einsum is an internal buffer (the output of `x * a`) and therefore can be overwritten. In `h2`, both inputs are userlevel buffers, and so neither are overwritable by default and the compiler must allocate a new large buffer for the output. You could probably address this by using `donate_argnums` to tell the compiler that one or more of the input buffers is overwriteable (but of course you would not be able to use those donated buffers in subsequent computations). If this operation is done as part of a larger program, then it's likely that the inputs will already be temporary buffers, and this microbenchmark will not be reflective of the overall performance of the larger program. Hope that's clear!","Closing, as I think this is working as expected.","Thanks for helping . I agree, _IF_ this is an issue, it's definitely not JAX, but whatever the compiler does. In the meantime I played around with this a bit and noticed two important things (Your comments have been very helpful): 1. In the code I posted in this comment https://github.com/google/jax/issues/20616issuecomment2041276158, there is no problem at all. Both functions need exactly the same amount of memory (I think, not entirely sure though). If you simply put a `del y` in between the function calls, everything works as expected. That's on me, sorry for that. 2. The original issue still persists, I'm having huge performance drops (which I still think has to do with memory usage) in my actual code, which is still reproducable with the original code I posted in this issue (in which the function `g2` needs almost twice as much memory as the other options). If I find the actual reason for my problem (and if it has something to do with jax), I'll probably just open a new issue, if that is ok."
885,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Efficient diag(JtJ))ï¼Œ å†…å®¹æ˜¯ (As best as I can tell, the most efficient way to get diag(JtJ) is to do jnp.square(primals).T @ jnp.square(cotangents), but this seems to be difficult to cleanly implement in JAX. I have an utterly terrible way of doing this by using a custom VJP:  Needless to say, this is rather ugly and fragile. So the question: Is there a good way to implement this with comparable efficiency? I did look at doing perexample gradients and then squaring and summing them, but that seems to be much less efficient: It builds the full outer product in memory before reducing. There doesn't seem to be an optimizer pass that will promote the square operation to the other side of the matrix multiply :()è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Efficient diag(JtJ),"As best as I can tell, the most efficient way to get diag(JtJ) is to do jnp.square(primals).T @ jnp.square(cotangents), but this seems to be difficult to cleanly implement in JAX. I have an utterly terrible way of doing this by using a custom VJP:  Needless to say, this is rather ugly and fragile. So the question: Is there a good way to implement this with comparable efficiency? I did look at doing perexample gradients and then squaring and summing them, but that seems to be much less efficient: It builds the full outer product in memory before reducing. There doesn't seem to be an optimizer pass that will promote the square operation to the other side of the matrix multiply :(",2024-04-05T02:43:52Z,enhancement,open,0,14,https://github.com/jax-ml/jax/issues/20600,"Thanks for the question! I'm not sure, but I wouldn't be surprised if we can't do the maximally efficient thing as a composition of just `jax.jvp` and `jax.vjp` (and `jax.vmap`), and instead we need to write a special structureexploiting recursion. By that I mean something like in CC(laplacian experiment) (and the libraries discussed in the comments which do a much better job with that, though I think the fundamental math is about the same), or like what people have done for computing log det Jacobians efficiently (like Oryx does), or what I believe this 'fast NTK' stuff does. Actually, that latter may be computing something similar to what you want, though they're usually after full JJ' matrices rather than diag(J'J) matrices. Indeed I think your rule for matmul is representing exactly such a structureexploiting rule: because the Jacobian of v > Av is simply A, and because diag(A'A) can be cleverly computed using a Hadamard product A.^2 (and summing), we can avoid ever having to do a matmul to compute the coefficients of diag(A'A).  I want to think about this more when I have more time, but wanted to plant that initial guess. I'm starting to think that there are several such special autodiff recursions of interest, and we should factor the JAX autodiff machinery to make those things easy to write (i.e. you just write the rules, then the library takes care of all the JAXinternals boilerplate). Then we could build more such things into JAX, and also make it easy to stand up user libraries. WDYT?","As you point out, we could play that trick with the whole Jacobian, or rather all the perexample gradients, and maybe we can't get better asymptotic FLOP costs than that. But with a special recursion we'll be doing the computation in the right order, and thus not OOMing for example, rather than relying on the compiler to clean up everything.","The special recursion absolutely sounds like something that would be very useful. I'll how to think about exactly what that might look like in the more general case. For the optimization side, it did occur to me that it's fairly natural. Computing the JtJ as:  This should compile to something like:  The matrix multiplication there has no contracting dimension, and the inputs are much smaller in size than the output, so lifting the square op to before the matrix mul should be a very easy optimization opportunity (lower FLOPS, lower memory BW, no precision issues as there's no additions). After that, it becomes:  At which point, removing the sum() is another zerorisk optimization: It's clearly just adding the contracting dimension to the dotgeneral. It saves much memory BW and has no impact on precision. So it might be the easier thing in the near term is to enable these optimizations? I'm not sure if they'd go in JAX or LAX (probably LAX?).  I absolutely agree it would be better not to rely on compiler magic, but these may be such clear wins that not doing them is a bug! :)",Good point about moving the square through the outer product. I see now what you meant in your first message. Is that the only optimization you need here?,"Is the conversion of the sum() into an additional contracting dimension for the preceeding dotgeneral an already existing optimization? If so then yes, lifting the square is the only one remaining.","Just checked, looks like merging the sum into the dotgeneral does happen:  gives  which looks like it has folded the sum into a contracting dimension. So yes, just lifting the square is enough to make the perexample gradient avenue efficient.","Hmm. Before I say that, the compile code for the current perexample computation is a little weird:  It's actually broadcasting out the primals and tangents before doing an elementwise multiplication, not doing a dotgeneral at all. That seems ... odd? Unless there's some downstream optimization, that's going to use more memory and more memory BW than doing the dotgeneral?","It would be good to check the jaxpr, but I think you can see from the name metadata that those ops started from dot_generals in the jaxpr. I'd want to know that the perexample gradient computation looks right in the jaxpr; then it's just a question of how XLA is deciding to lower it (which would be platformdependent, sizedependent, etc).","Yes, the jaxpr looks sane.  The square() at %20 needs to lift through the transpose at %19 as well as the dotgeneral at %18","I asked Blake the compiler guru and he said: > Seems like it is equivalent on reals but definitely not numerically identical so xla probably would not do it > It isn't unstable just different numerically > We don't know what the user is intending in general > So we avoid elementwise reassociation I'm following up a bit more, but it leaves us in an interesting spot: how would we solve this on the JAX side? Do we try to provide some mechanism to perform this optimization? Or instead of casting this as an optimization problem, do we instead work out some way to express the right computation well in the first place (along the lines of some 'special autodiff' recursion)?","I must admit it seems a little weird not to optimize this on the compiler side. This optimization reduces FLOP count and memory bandwidth by a factor equal to the batch size (e.g. 1/64th of the memBW for a batch size of 64). Maybe I'm misunderstanding the XLA goals? I take the general point about the elementwise reassociation, but in this case reassociating the square unlocks a huge decrease in the dotgeneral cost. That aside, it seems easy enough to optimize at the jaxpr level.  As far as I know, there's no hooks to add optimizations here? Ideally, this would be a callback from somewhere that takes and returns a Jaxpr. Maybe mlir.jaxpr_subcomp()? I just spent _way_ too long reading through chunks of the stages framework to see if I could mutate the Lowered() object before calling compile() on it, but it seems ... extremely fraught. It seems I can use jax.jit(...).lower(...)._hlo to get an ir.Module, and then get the assembly language from that as text, and then reparse it back into a new ir.Module and assign it to _hlo, and then call compile()! This _seems_ to work, but ... ??? ","I realized I'm actually confused about what you want to compute... When you write diag(JtJ), by J do you mean the Jacobian of scalarvalued function, like `apply` in your example? In that case, you'd be taking the diagonal elements of an outer product, i.e. you'd just be getting the elementwise square of gradients... Can you define J?","> Maybe I'm misunderstanding the XLA goals? Yeah, essentially XLA doesn't perform optimizations that would change numerical semantics too much, because that may be changing something the user wrote intentionally. You could imagine a compiler with a different philosophy, like ""optimize as much as possible, numerics be damned"", or a compiler that is even more conservative about numerics, or anything in between.","You're right, I'm saying diag(JtJ),  but it's a little illdefined here. What I want is $\mathbb{E}[\text{diag}(J^T J)]$, which is approximated by the perexample diag(JtJ) averaged over the batch. This is analogous to `jax.grad` returning $\mathbb{E}[J]$, being the perexample gradient averaged over the batch. NB: $\mathbb{E}[\text{diag}(J^T J)]$ is obviously distinct from $\text{diag}(\mathbb{E}[J]^T \mathbb{E}[J])$, which is what taking `diag(g.T @ g)` for `g = jax.grad(...)(...)` would give. Does that make sense?"
1097,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Call cuInit before querying compute capability)ï¼Œ å†…å®¹æ˜¯ (Since https://github.com/google/jax/commit/9fff9aeb69f586d36bdb6422f40b548aa41bd1e3 `jax.distributed.initialize` fails with the error  in some situations. Outside special cluster environments, JAX queries CUDA to find out how many GPUs are visible. Querying the device count correctly initialises CUDA, and all is well. However, at least on a Slurm clusters, the choice of which GPU a given distributed JAX process should use is based on Slurm environment variables, and in those environments JAX does **not** query the device count.  https://github.com/google/jax/commit/9fff9aeb69f586d36bdb6422f40b548aa41bd1e3 added a check that the selected GPUs have a minimum compute capability. This check assumes that CUDA is already initialised, which it is not in the Slurm case described above. This PR avoids the issue by calling cuInit before querying the compute capability.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Call cuInit before querying compute capability,"Since https://github.com/google/jax/commit/9fff9aeb69f586d36bdb6422f40b548aa41bd1e3 `jax.distributed.initialize` fails with the error  in some situations. Outside special cluster environments, JAX queries CUDA to find out how many GPUs are visible. Querying the device count correctly initialises CUDA, and all is well. However, at least on a Slurm clusters, the choice of which GPU a given distributed JAX process should use is based on Slurm environment variables, and in those environments JAX does **not** query the device count.  https://github.com/google/jax/commit/9fff9aeb69f586d36bdb6422f40b548aa41bd1e3 added a check that the selected GPUs have a minimum compute capability. This check assumes that CUDA is already initialised, which it is not in the Slurm case described above. This PR avoids the issue by calling cuInit before querying the compute capability.",2024-04-04T15:36:36Z,pull ready,closed,0,2,https://github.com/jax-ml/jax/issues/20588,Is `cuInit` idempotent? The docs do not say: https://docs.nvidia.com/cuda/cudadriverapi/group__CUDA__INITIALIZE.htmlgroup__CUDA__INITIALIZE,> Is `cuInit` idempotent? >  > The docs do not say: https://docs.nvidia.com/cuda/cudadriverapi/group__CUDA__INITIALIZE.htmlgroup__CUDA__INITIALIZE Yes. I made a request to document that.
1001,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(nan in hessian of linear function)ï¼Œ å†…å®¹æ˜¯ ( Description I am trying to vectorize a procedure, where I need to do the elementwise power of a large vector before getting its Hessian. If I manually compute the elementwise power of the vector in order to define the function of interest, I can use this code which gives the expected output:  However, if my function of interest takes the elementwise power as part of the definition, I get nan:  This is related to issue CC(Error in Automatic Differenciation for Functions with Powers of Zero) but different as the problem only arises with the Hessian (the Jacobian gives the expected output using both function definitions). Would you have any hint why this happens? Thank you!  System info (python version, jaxlib version, accelerator, etc.) jax version: 0.4.25 jaxlib version: 0.4.23)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,nan in hessian of linear function," Description I am trying to vectorize a procedure, where I need to do the elementwise power of a large vector before getting its Hessian. If I manually compute the elementwise power of the vector in order to define the function of interest, I can use this code which gives the expected output:  However, if my function of interest takes the elementwise power as part of the definition, I get nan:  This is related to issue CC(Error in Automatic Differenciation for Functions with Powers of Zero) but different as the problem only arises with the Hessian (the Jacobian gives the expected output using both function definitions). Would you have any hint why this happens? Thank you!  System info (python version, jaxlib version, accelerator, etc.) jax version: 0.4.25 jaxlib version: 0.4.23",2024-04-03T21:53:02Z,bug,open,0,4,https://github.com/jax-ml/jax/issues/20574,"Thanks for the report! I think you're corrrect that this is related to CC(Error in Automatic Differenciation for Functions with Powers of Zero). The reason it only affects the hessian and not the jacobian is because the hessian is a secondorder differentiation. Your function looks like this: $$ f(x) = x^1 $$ Differentiating once, you get this: $$ f^\prime(x) = x^0 $$ And differentiating a second time, you are differentiating $0^0$, which is illdefined for floating point exponent, and so we return NaN. Note that if you use a scalar integer exponent, the derivative is welldefined and the fix referenced in CC(Error in Automatic Differenciation for Functions with Powers of Zero) will apply to your hessian as well:  But for nonscalar, noninteger exponentiation, `nan` is the correct output because the value of the derivative is ambiguous (see the discussion in https://github.com/google/jax/issues/14397issuecomment1426386290).","Thank you for your reply, . I am afraid I do not understand why, in the second code snippet above,  is treated as a floating point exponent. I just checked and  has  so I would have expected JAX to treat it as an integer exponent accordingly. As the endgoal is to take the elementwise power of a large vector: I could of course take the scalar, integer exponent of each element in the vector instead of using  but I would expect the looping through all the elements in the vector to make the process rather inefficient. Is there no better way? Thank you again.","`x ** jnp.array([1])` lowers to `lax.pow`, which has floatpower semantics, while `x ** 1` lowers to `lax.integer_pow`, which has integerpower semantics. We could maybe specialize the autodiff rules of `pow_p` to be dtypedependent, but we haven't done that. , what do you think?","Thank you for following up, . : I would be happy to make it a pull request but I would most likely need some guidance given that I am not familiar with all the details of JAX yet. What would you suggest?"
344,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(test: work around issue with kstest in scipy>1.12)ï¼Œ å†…å®¹æ˜¯ (Works around the issue reported in https://github.com/scipy/scipy/issues/20386 Tested locally against scipy 1.13.0.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,test: work around issue with kstest in scipy>1.12,Works around the issue reported in https://github.com/scipy/scipy/issues/20386 Tested locally against scipy 1.13.0.,2024-04-03T18:18:30Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/20569
337,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jnp.geomspace: make complex behavior consistent with NumPy 2.0)ï¼Œ å†…å®¹æ˜¯ (Tested locally against NumPy 2.0.0rc1:  Related to https://github.com/numpy/numpy/issues/26195.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jnp.geomspace: make complex behavior consistent with NumPy 2.0,Tested locally against NumPy 2.0.0rc1:  Related to https://github.com/numpy/numpy/issues/26195.,2024-04-02T23:09:17Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/20552
306,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(test: skip complex geomspace test under numpy 2.0)ï¼Œ å†…å®¹æ˜¯ (We can revert this once https://github.com/numpy/numpy/issues/26195 is fixed.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,test: skip complex geomspace test under numpy 2.0,We can revert this once https://github.com/numpy/numpy/issues/26195 is fixed.,2024-04-02T19:29:30Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/20543
1485,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([callback] Allow external callbacks to return 64-bit values in 32-bit mode)ï¼Œ å†…å®¹æ˜¯ (Previously, prior to CC([callback] Fix io_callback for callbacks that return Python literals.), if the Python callback returned a Python literal (which is natively a 64bit value), and the `result_shape_dtypes` specified a 32bit expected returned value, we would just get garbage results. In CC([callback] Fix io_callback for callbacks that return Python literals.), I introduced an error in this situation. However, when trying to port the internal code that uses host_callback to `io_callback`, I am getting many instances of this error. The common scenario is a Python callback function that returns a Python scalar:  However, if the `f_host` were called directly JAX would canonicalize the value `42.` to a `float32` (when `jax_enable_x64` is not set). I do not think that it makes sense for `io_callback` to have stricter behavior that a direct call. In this PR we add a canonicalization step on the returned values of Python callbacks, which casts the values to 32bits if JAX is running in 32bit mode. Note that the above example should return an error in 64bit mode, because the actual returned value is a 64bit value but the declared expected value is `np.float32`. To avoid the error in both 64bit and 32bit mode, the pytho)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[callback] Allow external callbacks to return 64-bit values in 32-bit mode,"Previously, prior to CC([callback] Fix io_callback for callbacks that return Python literals.), if the Python callback returned a Python literal (which is natively a 64bit value), and the `result_shape_dtypes` specified a 32bit expected returned value, we would just get garbage results. In CC([callback] Fix io_callback for callbacks that return Python literals.), I introduced an error in this situation. However, when trying to port the internal code that uses host_callback to `io_callback`, I am getting many instances of this error. The common scenario is a Python callback function that returns a Python scalar:  However, if the `f_host` were called directly JAX would canonicalize the value `42.` to a `float32` (when `jax_enable_x64` is not set). I do not think that it makes sense for `io_callback` to have stricter behavior that a direct call. In this PR we add a canonicalization step on the returned values of Python callbacks, which casts the values to 32bits if JAX is running in 32bit mode. Note that the above example should return an error in 64bit mode, because the actual returned value is a 64bit value but the declared expected value is `np.float32`. To avoid the error in both 64bit and 32bit mode, the pytho",2024-04-02T14:12:43Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/20534
1456,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump actions/setup-python from 5.0.0 to 5.1.0)ï¼Œ å†…å®¹æ˜¯ (Bumps actions/setuppython from 5.0.0 to 5.1.0.  Release notes Sourced from actions/setuppython's releases.  v5.1.0 What's Changed  Leveraging the raw API to retrieve the versionmanifest, as it does not impose a rate limit and hence facilitates unrestricted consumption without the need for a token for Github Enterprise Servers by @â€‹Shegox in actions/setuppython CC(Generic support for JIT compilation with custom NumPy ops). Dependency updates by @â€‹dependabot and @â€‹HarithaVattikuti in actions/setuppython CC(JAX slow compared to numba for certain calculations) Documentation changes for version in README by @â€‹basnijholt in actions/setuppython CC(Scatter transpose rule doesnâ€™t handle symbolic zeros) Documentation changes for link in README by @â€‹ukd1 in actions/setuppython CC(vmap error with emtpy tuple) Documentation changes for link in Advanced Usage by @â€‹Jamim in actions/setuppython CC(Improve behavior of a number of math functions for extreme inputs.) Documentation changes for avoiding rate limit issues on GHES by @â€‹priyakinthali in actions/setuppython CC(IndexError: Indexing mode not yet supported)  New Contributors  @â€‹basnijholt made their first contribution in actions/setuppython CC(Scatter transpose rule doesnâ€™)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bump actions/setup-python from 5.0.0 to 5.1.0,"Bumps actions/setuppython from 5.0.0 to 5.1.0.  Release notes Sourced from actions/setuppython's releases.  v5.1.0 What's Changed  Leveraging the raw API to retrieve the versionmanifest, as it does not impose a rate limit and hence facilitates unrestricted consumption without the need for a token for Github Enterprise Servers by @â€‹Shegox in actions/setuppython CC(Generic support for JIT compilation with custom NumPy ops). Dependency updates by @â€‹dependabot and @â€‹HarithaVattikuti in actions/setuppython CC(JAX slow compared to numba for certain calculations) Documentation changes for version in README by @â€‹basnijholt in actions/setuppython CC(Scatter transpose rule doesnâ€™t handle symbolic zeros) Documentation changes for link in README by @â€‹ukd1 in actions/setuppython CC(vmap error with emtpy tuple) Documentation changes for link in Advanced Usage by @â€‹Jamim in actions/setuppython CC(Improve behavior of a number of math functions for extreme inputs.) Documentation changes for avoiding rate limit issues on GHES by @â€‹priyakinthali in actions/setuppython CC(IndexError: Indexing mode not yet supported)  New Contributors  @â€‹basnijholt made their first contribution in actions/setuppython CC(Scatter transpose rule doesnâ€™",2024-04-01T17:31:34Z,pull ready dependencies github_actions,closed,0,0,https://github.com/jax-ml/jax/issues/20520
882,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Issue installing JAX with Nvidia GPU: CUDA backend failed to initialize: Unable to load cuDNN.)ï¼Œ å†…å®¹æ˜¯ ( Description After installing JAX with Nvidia GPU using the recommended method here, essentially running:  doing something as simple as   will trigger an error like this, although I have a GPU on my node.  I was able to fix this problem by doing as suggested here: https://github.com/google/jax/issues/18027issuecomment2028488429 Although I am not sure, but I think there might be a bug in detecting the correct version of cuDNN when installing JAX using the recommended method.   System info (python version, jaxlib version, accelerator, etc.) Before the fix, here is the system info after jax installation )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Issue installing JAX with Nvidia GPU: CUDA backend failed to initialize: Unable to load cuDNN.," Description After installing JAX with Nvidia GPU using the recommended method here, essentially running:  doing something as simple as   will trigger an error like this, although I have a GPU on my node.  I was able to fix this problem by doing as suggested here: https://github.com/google/jax/issues/18027issuecomment2028488429 Although I am not sure, but I think there might be a bug in detecting the correct version of cuDNN when installing JAX using the recommended method.   System info (python version, jaxlib version, accelerator, etc.) Before the fix, here is the system info after jax installation ",2024-04-01T16:46:51Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/20519,"Yes, this is because CuDNN 9.0 was released and we didn't have a maximum version guard. We're making a new release soon that will fix the problem. Closing as duplicate of CC(Unable to correct CUDA vs. JAX version mismatch)."
1035,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Optimized code returns NaN, de-optimized does not)ï¼Œ å†…å®¹æ˜¯ ( Description I stumbled upon an error in JAX while trying to compute the gradient of a custom loss function. The error output states that jitoptimized code returns a `nan`, while a deoptimized function does not. There was a similar issue CC(jit(grad(f))(x) returns NaN, but grad(f)(x) evaluates fine), though occurring in a more complicated setting, so perhaps the underlying cause is not the same. Using `jax.config.update(""jax_enable_x64"", True)` does not fix it. A small reproducing sample:  Running the above results in  I wasn't able to make the error go away by using `jax.disable_jit`. Running  gives a warning  and then again the same error as before.  System info (python version, jaxlib version, accelerator, etc.) I'm running the above on a CPU. `import jax; jax.print_environment_info()` gives )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"Optimized code returns NaN, de-optimized does not"," Description I stumbled upon an error in JAX while trying to compute the gradient of a custom loss function. The error output states that jitoptimized code returns a `nan`, while a deoptimized function does not. There was a similar issue CC(jit(grad(f))(x) returns NaN, but grad(f)(x) evaluates fine), though occurring in a more complicated setting, so perhaps the underlying cause is not the same. Using `jax.config.update(""jax_enable_x64"", True)` does not fix it. A small reproducing sample:  Running the above results in  I wasn't able to make the error go away by using `jax.disable_jit`. Running  gives a warning  and then again the same error as before.  System info (python version, jaxlib version, accelerator, etc.) I'm running the above on a CPU. `import jax; jax.print_environment_info()` gives ",2024-04-01T16:37:50Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/20518,"I think it's likely due to taking the gradient of the norm when the argument is zero, see https://stackoverflow.com/questions/74864427/whydoesjaxgradlambdavjnplinalgnormvvjnpones2producenans The solution is usually to use the ""double where"" trick: https://jax.readthedocs.io/en/latest/faq.htmlgradientscontainnanwhereusingwhere","Good call :) It works if I replace `kernel` with the function below, using a ""double where"" trick. "
669,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([callback] Improve caching effectiveness in presence of callbacks.)ï¼Œ å†…å®¹æ˜¯ (Previously, the userprovided Python callback function was first flattened and then the result passed as a primitive parameter to the callback primitives. This means that two separate io_callback invocations with the same Python callable will generate different Jaxprs. To prevent this we defer the flattening to lowering time. I discovered this problem by trying to ensure that io_callback passes the host_callback_test.py.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[callback] Improve caching effectiveness in presence of callbacks.,"Previously, the userprovided Python callback function was first flattened and then the result passed as a primitive parameter to the callback primitives. This means that two separate io_callback invocations with the same Python callable will generate different Jaxprs. To prevent this we defer the flattening to lowering time. I discovered this problem by trying to ensure that io_callback passes the host_callback_test.py.",2024-04-01T09:31:54Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/20514
1545,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bazel not installed to executable path location by running python build/build.py --configure_only (as suggested in build instructions))ï¼Œ å†…å®¹æ˜¯ ( Description Hey, I'm  getting the tests up and running on a colab instance following the documentation presently published at https://jax.readthedocs.io/en/latest/developer.html It turns out that running  will output  and download bazel into the build/ directory of the source distribution but not put it on the path, so if you are using a system that doesn't have bazel installed already the remainder of the tutorial will not quite work unless you use the full path to the bazel executable or add it to the path manually.  Presumably some explicit instructions to install bazel should be added at this location.  System info (python version, jaxlib version, accelerator, etc.) Well I am actually seeing some sort of weird colab.research.google.com hiccup where the Jax module is being loaded successfully for introspection by the Jupyter/colab notebook interface but is somehow not being made available to the underlying ipython. This is completely unrelated to bazel not getting installed to the executable path by following along with the build instructions so I am filing this issue anyways.  If anybody can point me at colab's GitHub I'll post this part over there.  AttributeError                            Traceback (most)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bazel not installed to executable path location by running python build/build.py --configure_only (as suggested in build instructions)," Description Hey, I'm  getting the tests up and running on a colab instance following the documentation presently published at https://jax.readthedocs.io/en/latest/developer.html It turns out that running  will output  and download bazel into the build/ directory of the source distribution but not put it on the path, so if you are using a system that doesn't have bazel installed already the remainder of the tutorial will not quite work unless you use the full path to the bazel executable or add it to the path manually.  Presumably some explicit instructions to install bazel should be added at this location.  System info (python version, jaxlib version, accelerator, etc.) Well I am actually seeing some sort of weird colab.research.google.com hiccup where the Jax module is being loaded successfully for introspection by the Jupyter/colab notebook interface but is somehow not being made available to the underlying ipython. This is completely unrelated to bazel not getting installed to the executable path by following along with the build instructions so I am filing this issue anyways.  If anybody can point me at colab's GitHub I'll post this part over there.  AttributeError                            Traceback (most",2024-03-31T22:53:51Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/20512,"The documentation change to either mention this or provide a quick ""you need to install bazel with pip"" goes  https://github.com/google/jax/blob/main/docs/developer.md?plain=1L205 right here. I opened this issue by clicking through to here from the exact line source in GitHub, but as far as I can tell the actual line number information to make resolving it very easy disappears into the nether regions of the internet somewhere.  Presumably there's a place to open that as an issue on GitHub somewhere but I don't know where it is."
482,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`jax.lax.scan` function yield `ZeroDivisionError: integer division or modulo by zero` error when using unroll=0)ï¼Œ å†…å®¹æ˜¯ ( Description Here is the how to recover the error:  and here is the output error:   NOTE * Setting `unroll` to `1` works fine  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,`jax.lax.scan` function yield `ZeroDivisionError: integer division or modulo by zero` error when using unroll=0," Description Here is the how to recover the error:  and here is the output error:   NOTE * Setting `unroll` to `1` works fine  System info (python version, jaxlib version, accelerator, etc.) ",2024-03-28T18:52:02Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/20481,Thanks for the find! Did you intend on some behavior with unroll=0? Or will a better error work here?,I assume that 0 is False in this case so that the jax scan is completely unrolled.,"From the docs: > **unroll** (int | bool) â€“ optional positive int or bool specifying, in the underlying operation of the scan primitive, how many scan iterations to unroll within a single iteration of a loop. If an integer is provided, it determines how many unrolled loop iterations to run within a single rolled iteration of the loop. If a boolean is provided, it will determine if the loop is competely unrolled (i.e. unroll=True) or left completely unrolled (i.e. unroll=False). `0` is not `False` in this context, because the argument accepts both integer and boolean arguments with different documented behavior. If you want `False`, you should pass `False`. We should probably do more validation in the integer case, and raise a more informative error rather than assuming valid input.",This was fixed over in CC(Add more informative error message for invalid `unroll` parameter in `lax.scan`). Thanks for reporting!
1475,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([xla] hlo_computation: compact instructions' vector on Cleanup())ï¼Œ å†…å®¹æ˜¯ ([xla] hlo_computation: compact instructions' vector on Cleanup() tl;dr: this gives a 1.26x compilation time speedup for a large, dense model in XLA:GPU. The largest perf leaf seen in profiles of a large, dense model is related to computing the post order. Surprisingly, it is not the DFS itself what's most expensive; rather, most of the time is spent on scanning through HloComputation::Instructions() to identify DFS roots. The reason this scan becomes expensive as instructions are removed is that the vector holding HloInstructionInfo (introduced in cl/600130708  https://github.com/openxla/xla/commit/247280ab727) is not shrunk as it flows through the pipeline, making us having to walk through many deleted ""tombstone"" entries. Here is the histogram of  of tombstones encountered during post order computations for this model:  To ameliorate this, this CL shrinks the vector periodically, so far only between passes. This is done by running compaction on the vector during HloComputation::Cleanup(), which is called after every pass. The cost of compaction is made proportional to the number of deleted entries by swappingif neededeach tombstone with the rightmost (within the vector) nondeleted entry. This brings the number)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[xla] hlo_computation: compact instructions' vector on Cleanup(),"[xla] hlo_computation: compact instructions' vector on Cleanup() tl;dr: this gives a 1.26x compilation time speedup for a large, dense model in XLA:GPU. The largest perf leaf seen in profiles of a large, dense model is related to computing the post order. Surprisingly, it is not the DFS itself what's most expensive; rather, most of the time is spent on scanning through HloComputation::Instructions() to identify DFS roots. The reason this scan becomes expensive as instructions are removed is that the vector holding HloInstructionInfo (introduced in cl/600130708  https://github.com/openxla/xla/commit/247280ab727) is not shrunk as it flows through the pipeline, making us having to walk through many deleted ""tombstone"" entries. Here is the histogram of  of tombstones encountered during post order computations for this model:  To ameliorate this, this CL shrinks the vector periodically, so far only between passes. This is done by running compaction on the vector during HloComputation::Cleanup(), which is called after every pass. The cost of compaction is made proportional to the number of deleted entries by swappingif neededeach tombstone with the rightmost (within the vector) nondeleted entry. This brings the number",2024-03-28T15:04:51Z,,closed,0,0,https://github.com/jax-ml/jax/issues/20477
908,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jax.config.update(""jax_enable_x64"", True) does not work for joblib.)ï¼Œ å†…å®¹æ˜¯ ( Description Briefly, even I set jax.config.update(""jax_enable_x64"", True) ahead , joblib will work under jax.config.update(""jax_enable_x64"", False).    Joblib gives the float32 results, indicating jax.config.update(""jax_enable_x64"", True) not working.  However, if I set jax.config.update(""jax_enable_x64"",False), then it gives the same results as to joblib.   System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.22 jaxlib: 0.4.22 numpy:  1.25.0 python: 3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 $ nvidiasmi Thu Mar 28 02:00:11 2024        ++  ++)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"jax.config.update(""jax_enable_x64"", True) does not work for joblib."," Description Briefly, even I set jax.config.update(""jax_enable_x64"", True) ahead , joblib will work under jax.config.update(""jax_enable_x64"", False).    Joblib gives the float32 results, indicating jax.config.update(""jax_enable_x64"", True) not working.  However, if I set jax.config.update(""jax_enable_x64"",False), then it gives the same results as to joblib.   System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.22 jaxlib: 0.4.22 numpy:  1.25.0 python: 3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1 $ nvidiasmi Thu Mar 28 02:00:11 2024        ++  ++",2024-03-28T06:00:30Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/20473,"Well, this isn't really a bug in JAX: we didn't promise anything about `joblib`. That said, you can think of the `config` option like a global variable: it apparently isn't being set in the subprocesses `joblib` is using. Perhaps there's a way to do some extra initialization as part of the `joblib` subprocesses so you can set that config option? Or you could presumably set it as part of the function you are parallelizing, as the first thing you do. What do you think?","> Well, this isn't really a bug in JAX: we didn't promise anything about `joblib`. >  > That said, you can think of the `config` option like a global variable: it apparently isn't being set in the subprocesses `joblib` is using. Perhaps there's a way to do some extra initialization as part of the `joblib` subprocesses so you can set that config option? Or you could presumably set it as part of the function you are parallelizing, as the first thing you do. >  > What do you think? Yes, you are right. I just found that setting  can solve this issue. Thanks for pointing out!"
637,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Error: 'apple_common' value has no field or method 'multi_arch_split')ï¼Œ å†…å®¹æ˜¯ ( Description Hello, I am getting following error while building jax with cuda support.   apple_common.multi_arch_split seems to have been deleted with bazel 7. Is there anyway to build with bazel 7 https://www.buildbuddy.io/blog/whatsnewinbazel70/   System info (python version, jaxlib version, accelerator, etc.) endeavouros Python 3.11.8 bazel 7.1.1 cuda 12.4 cudnn 8.9.7.29 nccl 2.19.4)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Error: 'apple_common' value has no field or method 'multi_arch_split'," Description Hello, I am getting following error while building jax with cuda support.   apple_common.multi_arch_split seems to have been deleted with bazel 7. Is there anyway to build with bazel 7 https://www.buildbuddy.io/blog/whatsnewinbazel70/   System info (python version, jaxlib version, accelerator, etc.) endeavouros Python 3.11.8 bazel 7.1.1 cuda 12.4 cudnn 8.9.7.29 nccl 2.19.4",2024-03-27T11:35:40Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/20453,"We don't yet support building with Bazel 7. So this is mostly working as intended. You must use a compatible bazel version (6.1.1 IIRC right now, see `.bazelversion`)."
757,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`jax.scipy.special.ndtri` edge case behavior)ï¼Œ å†…å®¹æ˜¯ ( Description `jax.scipy.special.ndtri` returns positive infinity when the argument exceeds `1.0` and negative infinity when the argument is less than `0.0`.  It seems that it would be more appropriate to return NaN in these cases (as `scipy.special.ndtri`, `torch.special.ndtri`, etc. do).  This came up when adding support for JAX as an array API backend in SciPy. For more information, please see scipy/scipy CC([XLA:Python] Port several more modules to nanobind.).  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,`jax.scipy.special.ndtri` edge case behavior," Description `jax.scipy.special.ndtri` returns positive infinity when the argument exceeds `1.0` and negative infinity when the argument is less than `0.0`.  It seems that it would be more appropriate to return NaN in these cases (as `scipy.special.ndtri`, `torch.special.ndtri`, etc. do).  This came up when adding support for JAX as an array API backend in SciPy. For more information, please see scipy/scipy CC([XLA:Python] Port several more modules to nanobind.).  System info (python version, jaxlib version, accelerator, etc.) ",2024-03-26T04:42:29Z,bug,closed,0,0,https://github.com/jax-ml/jax/issues/20430
697,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`numpy.memmap.flush()` in `jax`)ï¼Œ å†…å®¹æ˜¯ (Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example. I'm implementing Stochastic Variational Inference algorithm of my model, during which, to avoid OOM, I try to use `jnp.load('*.npy', mmap_mode='r+')` load a huge batch parameter array from disk. However, when I attempt to do `flush()` on the loaded array I got `'ArrayImpl' object has no attribute 'flush'`.  Does `flush()` not be implemented in `jax`?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,`numpy.memmap.flush()` in `jax`,"Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example. I'm implementing Stochastic Variational Inference algorithm of my model, during which, to avoid OOM, I try to use `jnp.load('*.npy', mmap_mode='r+')` load a huge batch parameter array from disk. However, when I attempt to do `flush()` on the loaded array I got `'ArrayImpl' object has no attribute 'flush'`.  Does `flush()` not be implemented in `jax`?",2024-03-25T06:30:13Z,enhancement,closed,0,4,https://github.com/jax-ml/jax/issues/20418,"I found that using `numpy.memmap` load parameters then convert it to `jax.Array` seems feasible and efficient acceptable(My parameters are saved on a M.2 solid disk). After some update steps, I convert it back to `ndarray` and do `flush()`.","Yes. Certainly on CPU JAX can exchange buffers with NumPy zerocopy, so you can save an array by calling `np.asarray` on it and then using NumPy's facilities to do it. `jnp.load` and `jnp.save` are thin wrappers around the NumPy features. I'm not completely sure it makes sense for us to implement `flush` since our arrays are immutable.","> Yes. Certainly on CPU JAX can exchange buffers with NumPy zerocopy, so you can save an array by calling `np.asarray` on it and then using NumPy's facilities to do it. `jnp.load` and `jnp.save` are thin wrappers around the NumPy features. >  > I'm not completely sure it makes sense for us to implement `flush` since our arrays are immutable. Thanks for your reply. I mean is there some kind of `np.memmap` in `jax`?","> > Yes. Certainly on CPU JAX can exchange buffers with NumPy zerocopy, so you can save an array by calling `np.asarray` on it and then using NumPy's facilities to do it. `jnp.load` and `jnp.save` are thin wrappers around the NumPy features. > > I'm not completely sure it makes sense for us to implement `flush` since our arrays are immutable. >  > Thanks for your reply. I mean is there some kind of `np.memmap` in `jax`? I just finished the SVI algorithm and ran some experiments, in which I found there is no need to implement the socalled `jnp.memmap` because using `np.asarray` to convert parameters and then update the corresponding `np.memmap` object is efficient enough. It can save a lot of GPU memory while completing each step very fast. I think I can close this issue. Thank you!  "
661,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Pallas] Static argument yields `ValueError: safe_zip() argument 2 is shorter than argument 1`)ï¼Œ å†…å®¹æ˜¯ ( Description I am trying to write a very basic Pallas kernel with an additional static argument as input. However, I run into a ValueError: safe_zip() argument 2 is shorter than argument 1 whenever I actually use my static argument inside the dynamic slice. Here is the MWE:  Changing line 12 from  to  works as intended.  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[Pallas] Static argument yields `ValueError: safe_zip() argument 2 is shorter than argument 1`," Description I am trying to write a very basic Pallas kernel with an additional static argument as input. However, I run into a ValueError: safe_zip() argument 2 is shorter than argument 1 whenever I actually use my static argument inside the dynamic slice. Here is the MWE:  Changing line 12 from  to  works as intended.  System info (python version, jaxlib version, accelerator, etc.) ",2024-03-23T09:06:11Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/20406,Was a simple mistake with the `jax.jit`.
813,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jax.lax.scan tries to access empty array)ï¼Œ å†…å®¹æ˜¯ ( Description Apologies if I'm just very confused. When `jax.lax.scan(f, init, xs)` is fed an empty array as `xs` and `init`, it will still start the loop, and try to apply the scanning function`f` to the empty carry `init` and nonexistent `xs`. Since the carry itself is also empty, this will result in a crash.  Result:  The expected behavior is that scanning on an empty array `xs` should do nothing at all. It should not start the loop, should not try to apply the function, and should return the unmodified `init` as carry.  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",dspy,jax.lax.scan tries to access empty array," Description Apologies if I'm just very confused. When `jax.lax.scan(f, init, xs)` is fed an empty array as `xs` and `init`, it will still start the loop, and try to apply the scanning function`f` to the empty carry `init` and nonexistent `xs`. Since the carry itself is also empty, this will result in a crash.  Result:  The expected behavior is that scanning on an empty array `xs` should do nothing at all. It should not start the loop, should not try to apply the function, and should return the unmodified `init` as carry.  System info (python version, jaxlib version, accelerator, etc.) ",2024-03-22T19:13:46Z,better_errors,closed,0,11,https://github.com/jax-ml/jax/issues/20399,What would you expect from the code snippet?,"Since `myarray` is empty in this case, I would expect it to print `[]`, i.e. the final carry should be identical to `init` (because the scan call would do nothing). Most importantly I would expect it not to cause an error by trying to start a loop that should not be started. If the array is empty, I would expect it to behave exactly like `for _ in range(0)`. Note that if `myarray` is any nonempty array, then the snippet currently does the right thing, i.e. print `[0, 1, 2...]` with the same length as `myarray`.",Thanks for raising this and fo the thoughtful analysis! I agree.,"Ah, I just looked more carefully at the code in the OP, and I see the issue: we need to produce some scannedover output, and to figure out its type we need to be able to stage out the body function. But we can't stage out this body function because there's statically an outofbounds error, namely we know statically that `carry.at[j].set(j)` must fail. (Indeed we can see from reading the Python source that the scannedover output is just `None`, but JAX doesn't read source code; it just gets an opaque Python callable, and can do nothing but run it...) I think this is working as intended, and there's not much else we can do here other than raise a better error message.","I think the best thing to do is to recommend that you write the original code with a fori_loop, since you aren't using the scannedover inputs or outputs much:  That would've also crashed from running the body, but CC(if fori_loop statically takes zero trips, don't run body) makes it not run anymore. I realize that your example may have been a distilled toy example, and perhaps in your real one you do want to use scannedover inputs, or something. But still I think this is our best option, for the reasons mentioned above.","FWIW, the reason for this post is Craftax, a 2D Minecraftlike game implemented entirely in Jax. It has a lot of passages that use `jax.lax.scan` to loop over all enemies / mobs / plants, etc., like this:  If you try to set `max_zombies` to 0, this crashes (because `zombies.position` is an empty array but it still tries to access it). I've managed to make it work with a lot of `if` statements.  If this behaviour is not going to change, maybe it should be specified in the documentation that `scan` should not be applied to empty arrays, at least if the function accesses them?","How is checking if `length == 0` in a `fori_loop` different than checking if `len(xs)==0` in a `scan`? Is it the fact that a compiled `scan` does not recompile for different lengths of `xs`?  Sorry if it is a stupid question, but I was just wondering..."," super cool project! > How is checking if length == 0 in a fori_loop different than checking if len(xs)==0 in a scan? Is it the fact that a compiled scan does not recompile for different lengths of xs?  `scan` has to produce empty versions of the scannedover outputs, whereas a zerotripcount `fori_loop` just needs to return the initial carry you provided. That is, if you write `carry, ys = scan(f, carry, xs)` and `xs` have length 0, we still need to produce something for `ys`. To know what to produce, we need to know the output type of `f`: for example, if we had `f = lambda c, x: (c, x)` then we need `ys` to be an empty array that looks just like `xs`, but if we had `f = lambda c, x: (c, (x, x))` then we need `ys` to be a pair like `(xs, xs)`, or if we had `f = lambda c, x: (c, jnp.zeros(3, 'int32'))` then we need `ys` to be something like `jnp.zeros((0, 3), 'int32')`. (In general, 'the output type of `f`' means knowing its pytree structure and the shapes/dtypes of its leaves.) But `scan` is just given `f`, and has to infer its output type. The only thing we can do (robustly) with a Python callable is run it, so we run it and build a jaxpr for `f`, in the process discovering its output type. But what if `f` can't be run, e.g.  Then we can't infer its output type! That's the issue here: we have a loop body that can never run, for _any_ length of `xs` (basically like the `assert False` example), because the carry is (statically) empty and can never be indexed. If we didn't have an empty carry, but still had length0 xs, everything would've worked fine:  > If this behaviour is not going to change, maybe it should be specified in the documentation that scan should not be applied to empty arrays, at least if the function accesses them?   Well, the issue isn't exactly empty arrays or accessing them, as the example above shows; it's that the scan body can't always (i.e. statically) error, given the shapes of the arguments passed to scan, because we run the body for output type inference. That's the issue in the body function in the OP, but we'd have the same issue with a body function that just has an `assert False` at the top (and has nothing to do with empty arrays). We could indeed document that the body has to be runnable without error, if it's not already documented. > It has a lot of passages that use jax.lax.scan to loop over all enemies / mobs / plants, etc., like this:  For code that fits this pattern, I suggest `fori_loop` instead of scanning over a `jnp.arange`. That is, wherever you have  where you ignore the scannedover output anyway, you can instead write  and you'll get the behavior you want.  Another option we have is we could add an optional `out_type` argument to `scan`: if you provide the output type of the body, then we won't have to run it to infer the output type. Then you'd also get the behavior you want, i.e. you can write  and when xs has length 0 that would not error. But if your examples all fit the `fori_loop` pattern, it seems reasonable just to use that... WDYT?","Thanks for the explanation ! And just to be sure, if during compilation you where to check `len(xs)` and just set `ys` to `None` when it is 0, that would not work because that would create a representation of `f` that has different output pytrees depending on `xs`?  which is a no go  since e.g. `ys` shape does depend on `xs` length, but not its pytree structure which is found out by running `f`. Is that correct? Does recompilation happen when changing the length of `xs`? Thank you for your work and feel free to ignore this! :) ","Late comment, but this has come up several times in the past and we've always answered that it's working as intended; see e.g. https://github.com/google/jax/issues/3285 and in particular https://github.com/google/jax/issues/3285issuecomment638120124 We also have places in the code where we account for this fact; for example here https://github.com/google/jax/blob/81c3d51deff4e947f9c708ab4191540711f4f80c/jax/_src/lax/linalg.pyL1213L1214","> And just to be sure, if during compilation you where to check len(xs) and just set ys to None when it is 0, that would not work because that would create a representation of f that has different output pytrees depending on xs? That's right. > Does recompilation happen when changing the length of xs? Yes, any given program we generate and compile is for fixed shapes. I think we resolved things here well enough to close this issue. Please correct me if I'm mistaken! Thanks, all."
421,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Propagate sharding of inputs to full_like that are capable of carrying sharding as an attribute.)ï¼Œ å†…å®¹æ˜¯ (Propagate sharding of inputs to full_like that are capable of carrying sharding as an attribute. Fixes https://github.com/google/jax/issues/20390)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Propagate sharding of inputs to full_like that are capable of carrying sharding as an attribute.,Propagate sharding of inputs to full_like that are capable of carrying sharding as an attribute. Fixes https://github.com/google/jax/issues/20390,2024-03-22T15:47:38Z,,closed,0,0,https://github.com/jax-ml/jax/issues/20394
1486,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Deprecate jax.experimental.host_callback in favor of JAX external callbacks)ï¼Œ å†…å®¹æ˜¯ (We have marked the host_callback APIs deprecated on March 21, 2024 (JAX version 0.4.26). They will be removed in October 2024. Users should use instead the new JAX external callbacks.  Quick temporary migration As of October 1st, 2024 (JAX version 0.4.34) if you use the `jax.experimental.host_callback` APIs they will be implemented in terms of jax.experimental.io_callback. This is controlled by the configuration variable `jax_host_callback_legacy=False` (or the environment variable `JAX_HOST_CALLBACK_LEGACY=False`. For a very limited time, you can obtain the old behavior by setting the configuration variable to `True`. Very soon this configuration flag will be removed, so it is best to take the time to do the migration as explained below.  Real migration It is best to study the different flavors of JAX external callbacks to pick the right one for your use case. In general `io_callback(ordered=True)` will have more similar support to the existing `host_callback`. In general, you should replace calls to `id_tap` and `call` with `io_callback`, except when you need these calls to work under `vmap`, `grad`, `jvp`, `scan`, or `cond`, in which case you should use `jax.debug.callback`. Note that `jax.debug.callback` doe)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Deprecate jax.experimental.host_callback in favor of JAX external callbacks,"We have marked the host_callback APIs deprecated on March 21, 2024 (JAX version 0.4.26). They will be removed in October 2024. Users should use instead the new JAX external callbacks.  Quick temporary migration As of October 1st, 2024 (JAX version 0.4.34) if you use the `jax.experimental.host_callback` APIs they will be implemented in terms of jax.experimental.io_callback. This is controlled by the configuration variable `jax_host_callback_legacy=False` (or the environment variable `JAX_HOST_CALLBACK_LEGACY=False`. For a very limited time, you can obtain the old behavior by setting the configuration variable to `True`. Very soon this configuration flag will be removed, so it is best to take the time to do the migration as explained below.  Real migration It is best to study the different flavors of JAX external callbacks to pick the right one for your use case. In general `io_callback(ordered=True)` will have more similar support to the existing `host_callback`. In general, you should replace calls to `id_tap` and `call` with `io_callback`, except when you need these calls to work under `vmap`, `grad`, `jvp`, `scan`, or `cond`, in which case you should use `jax.debug.callback`. Note that `jax.debug.callback` doe",2024-03-22T06:28:08Z,enhancement,open,0,2,https://github.com/jax-ml/jax/issues/20385,"Could you give or reference an example of io_callback `result_shape_dtypes`?  res = io_callback(fn, result_shape_dtypes, arg). I just can't manage to get it properly. ","There's an example in the docs here: https://jax.readthedocs.io/en/latest/notebooks/external_callbacks.htmlexploringjaxexperimentaliocallback In this case, since the function returns a value that is the same shape and dtype as the input, we pass the input to `result_shape_dtypes`."
850,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Unexplained all-gather in batched array shuffling, crashes with shardmap)ï¼Œ å†…å®¹æ˜¯ ( Description I have an array, which is fully sharded along the first dimension. I'm trying to shuffle each row of the array using the code below. From what I understand, it should be an embarrassingly parallel operation, but XLA decides to insert an allgather in the end of it for some reason that I cannot understand. I also observe the same behavior if I replace `jax.random.permutation` with `jax.lax.top_k` Two questions: * What is this allgather doing there? * Is there a way to achieve what I want without any communications?   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"Unexplained all-gather in batched array shuffling, crashes with shardmap"," Description I have an array, which is fully sharded along the first dimension. I'm trying to shuffle each row of the array using the code below. From what I understand, it should be an embarrassingly parallel operation, but XLA decides to insert an allgather in the end of it for some reason that I cannot understand. I also observe the same behavior if I replace `jax.random.permutation` with `jax.lax.top_k` Two questions: * What is this allgather doing there? * Is there a way to achieve what I want without any communications?   System info (python version, jaxlib version, accelerator, etc.) ",2024-03-22T00:43:35Z,bug,open,0,2,https://github.com/jax-ml/jax/issues/20381,"Ok, I was able to work around it with shard_map: ","Ok, here is a fun fact: if I replace `jax.random.permutation` with `jax.lax.top_k` and leave `shard_map`, this code crashes with `Floating point exception` from native code (presumably XLA). Without shardmap it works fine except for undesired allgather."
1331,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Difference in output between jitted and non-jitted call)ï¼Œ å†…å®¹æ˜¯ ( Description Originally I posted this to the Flax repo(https://github.com/google/flax/issues/3777), but I think this is jax issue now. I have found that the results of the forward pass, differ considerably if the **apply** function is jitted or not.  System information  Ubuntu 23.10 x86_64  Flax, jax, jaxlib versions (obtain with `pip show flax jax jaxlib`: flax 0.8.2, jax 0.4.25, jaxlib 0.4.25+cuda12.cudnn89  Python version: v3.10.13  GPU/TPU model and memory:  NVIDIA GeForce RTX 3090  CUDA version (if applicable): 12.4  Problem you have encountered: When using the forward pass for a simple MLP the results are different with the jitted version.  What you expected to happen: That the results are the same.  Logs, error messages, etc:  Steps to reproduce:  this gives an error of 0.0005452320910990238 in my computer.  I found out that from `jaxlib==0.4.21` to `jaxlib==0.4.25` I get this error but with `jaxlib==0.4.20` there is no error. Interestingly if I use  'xla_gpu_triton_gemm_any=True ' there is no error as well.  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gemma,Difference in output between jitted and non-jitted call," Description Originally I posted this to the Flax repo(https://github.com/google/flax/issues/3777), but I think this is jax issue now. I have found that the results of the forward pass, differ considerably if the **apply** function is jitted or not.  System information  Ubuntu 23.10 x86_64  Flax, jax, jaxlib versions (obtain with `pip show flax jax jaxlib`: flax 0.8.2, jax 0.4.25, jaxlib 0.4.25+cuda12.cudnn89  Python version: v3.10.13  GPU/TPU model and memory:  NVIDIA GeForce RTX 3090  CUDA version (if applicable): 12.4  Problem you have encountered: When using the forward pass for a simple MLP the results are different with the jitted version.  What you expected to happen: That the results are the same.  Logs, error messages, etc:  Steps to reproduce:  this gives an error of 0.0005452320910990238 in my computer.  I found out that from `jaxlib==0.4.21` to `jaxlib==0.4.25` I get this error but with `jaxlib==0.4.20` there is no error. Interestingly if I use  'xla_gpu_triton_gemm_any=True ' there is no error as well.  System info (python version, jaxlib version, accelerator, etc.) ",2024-03-21T18:17:01Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/20371,"Thanks for the question! I think this is working as intended: especially on GPU, floating point numerics may not be exactly preserved under compilation. Indeed, floating point numerics may not even be preserved runtorun on GPU, unless you set some flags that significantly degrade performance! See https://github.com/google/jax/discussions/10674discussioncomment7214817 about nondeterminism. I'm not sure exactly why that flag would affect things, other than it's just changing the generated code. > differ considerably 0.0005452320910990238 does not seem surprising to me for f32, but if it's too much for your use case, maybe try a higher floating point precision (since presumably the compilerinduced numerical noise scales down with increased precision)? Or it may be that one can identify XLA:GPU flags that are best for ensuring jit invariance, though I'm not sure if those will be stable over time... WDYT?","Thanks for the prompt answer! I think that makes sense, but what I don't understand is why I get a different behaviour with `jaxlib==0.4.20` vs `jaxlib==0.4.21+`  or ~why the 'xla_gpu_triton_gemm_any=True ' seems to fix the problem as well~ (maybe related to the deterministic flag, however xla_gpu_deterministic_ops=true does not seem to fix the problem). Also if I run this in a collab notebook with 0.4.23 then I get no numerical error", is it possible that devicespecific behavior for `default_matmul_precision` could account for the difference across GPU types?," good point! Yeah I'm not sure, but different GPU types would also affect codegen in other ways.",Hi  The repro now produces a difference of 0.0 between jitted and nonjitted call when run on NVIDIA GeForce RTX 2060 in WSL with JAX 0.4.37. Please find the attached screenshots for reference. !image jax.print_environment_info(): !image Could you please verify if the issue still persists in your system and let us know. Thank you.,"Hi,  With versions >=0.4.37, the issue seems to be fixed. Thanks!"," Hi, with version 0.4.38 (also 0.5.0) there is still a difference `diff=0.0005837870994582772` on my system. Either `xla_gpu_triton_gemm_any=True` or `jax.config.update('jax_enable_x64', True)` resolves it, while `jax.default_matmul_precision('float32')` doesn't seem to work. It can be detrimental for scicomp when the error accumulates in a complex module. Strangely enough, `jax.config.update('jax_enable_x64', True)` seems to improve the inference speed while decreasing the training speed. System info: "
1542,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Use XLA operators instead of custom threefry2x32 kernel to generate random number, due to high memory costs of threefry2x32 kernel.)ï¼Œ å†…å®¹æ˜¯ ( Description It seems that Jax's default random number generation algorithm threefry2x32 consumes large amout of memory. From search results, the requirement of threefry2x32 is: To generate random values using threefry2x32 directly, you need to provide a counter array and a PRNG key. The counter array determines the shape of the output, while the PRNG key determines the specific random values generated The counter and key are arrays of unsigned 32bit integers. For threefry2x32, both the counter and key have 2 elements This means to generate a random tensor, custom kernel threefry2x32 requires 2X memory size of the output tensor. This will result in big issue about memory consumption, especially for embedding tensor, as embedding tensor is already very large, 2X memory consumption is very likely to cause OOM. An example of HLO dumps is attached.  module_0001.jit_init.sm_9.0_gpu_after_optimizations.txt Another solution is  to comment out this line of code in JAX: https://github.com/google/jax/blob/b6e985ffe7af6054a289613eb70ce31c6360cd76/jax/_src/prng.pyL1045 If you comment it out, JAX will use an HLOonly implementation of the prng, with no CUDA. The only reason the custom kernel exists is because of compile time,)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"Use XLA operators instead of custom threefry2x32 kernel to generate random number, due to high memory costs of threefry2x32 kernel."," Description It seems that Jax's default random number generation algorithm threefry2x32 consumes large amout of memory. From search results, the requirement of threefry2x32 is: To generate random values using threefry2x32 directly, you need to provide a counter array and a PRNG key. The counter array determines the shape of the output, while the PRNG key determines the specific random values generated The counter and key are arrays of unsigned 32bit integers. For threefry2x32, both the counter and key have 2 elements This means to generate a random tensor, custom kernel threefry2x32 requires 2X memory size of the output tensor. This will result in big issue about memory consumption, especially for embedding tensor, as embedding tensor is already very large, 2X memory consumption is very likely to cause OOM. An example of HLO dumps is attached.  module_0001.jit_init.sm_9.0_gpu_after_optimizations.txt Another solution is  to comment out this line of code in JAX: https://github.com/google/jax/blob/b6e985ffe7af6054a289613eb70ce31c6360cd76/jax/_src/prng.pyL1045 If you comment it out, JAX will use an HLOonly implementation of the prng, with no CUDA. The only reason the custom kernel exists is because of compile time,",2024-03-20T01:54:33Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/20328,Seems to be fixed by https://github.com/google/jax/commit/3f9540761e092772860cf7ca33a3cdca9ad40eb5.  Please reopen this issue if it's not.,"Yes, this should be covered by https://github.com/google/jax/commit/69878c4924f62a7b73d25c7721e5f07a75a96c18 and https://github.com/google/jax/commit/3f9540761e092772860cf7ca33a3cdca9ad40eb5 together. Thanks for noticing and closing."
1025,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jax.pure_callback and jax.experimental.io_callback now use jax.Arrays)ï¼Œ å†…å®¹æ˜¯ (jax.pure_callback and jax.experimental.io_callback now use jax.Arrays The motivation for this change is twofold * JAX APIs should use jax.Arrays. * Using jax.Arrays potentially allows keeping the data on device, instead   of always copying it to the host. Note that the version here still always   copies to the host. If this change breaks you, you can recover the old behavior by changing     jax.pure_callback(         f,         result_shape_dtypes,         *args,         **kwargs,     ) to     jax.pure_callback(         lambda *args: f(*jax.tree.map(np.asarray, args)),         result_shape_dtypes,         *args,         **kwargs,     ) so that the callback function is called with NumPy arrays as before. I will update the ""External callbacks"" tutorial in a follow up.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jax.pure_callback and jax.experimental.io_callback now use jax.Arrays,"jax.pure_callback and jax.experimental.io_callback now use jax.Arrays The motivation for this change is twofold * JAX APIs should use jax.Arrays. * Using jax.Arrays potentially allows keeping the data on device, instead   of always copying it to the host. Note that the version here still always   copies to the host. If this change breaks you, you can recover the old behavior by changing     jax.pure_callback(         f,         result_shape_dtypes,         *args,         **kwargs,     ) to     jax.pure_callback(         lambda *args: f(*jax.tree.map(np.asarray, args)),         result_shape_dtypes,         *args,         **kwargs,     ) so that the callback function is called with NumPy arrays as before. I will update the ""External callbacks"" tutorial in a follow up.",2024-03-19T20:24:00Z,,closed,0,0,https://github.com/jax-ml/jax/issues/20325
278,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jaxlib: Add `ifrt_proxy.pyi` to `build_wheel.py`.)ï¼Œ å†…å®¹æ˜¯ (jaxlib: Add `ifrt_proxy.pyi` to `build_wheel.py`.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jaxlib: Add `ifrt_proxy.pyi` to `build_wheel.py`.,jaxlib: Add `ifrt_proxy.pyi` to `build_wheel.py`.,2024-03-19T19:04:12Z,,closed,0,1,https://github.com/jax-ml/jax/issues/20324,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
286,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(doctest: avoid modifying global flag state)ï¼Œ å†…å®¹æ˜¯ (This was causing flaky tests due to changing order of execution.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,doctest: avoid modifying global flag state,This was causing flaky tests due to changing order of execution.,2024-03-19T18:28:05Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/20323
350,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([key reuse] add internal function_type_signature utility)ï¼Œ å†…å®¹æ˜¯ (I've found I often recreate this utility for local testing; it won't be public, but it's useful to have it around.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[key reuse] add internal function_type_signature utility,"I've found I often recreate this utility for local testing; it won't be public, but it's useful to have it around.",2024-03-19T17:47:18Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/20320
1252,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jax-metal: Memory leak on jit boundary, MPS)ï¼Œ å†…å®¹æ˜¯ ( Description Simply calling a  function with any kind of array input leaks memory in an noninsignificant way. CPU is okay, but device (MPS) exhibits the leak. Sample program:  Output on CPU:  Output on MPS:   System info (python version, jaxlib version, accelerator, etc.) Platform 'METAL' is experimental and not all JAX functionality may be correctly supported! 20240318 18:17:36.186791: W pjrt_plugin/src/mps_client.cc:563] WARNING: JAX Apple GPU support is experimental and not all JAX functionality is correctly supported! Metal device set to: Apple M3 Max systemMemory: 128.00 GB maxCacheSize: 48.00 GB jax:    0.4.25 jaxlib: 0.4.23 numpy:  1.26.4 python: 3.12.2  (main, Feb 27 2024, 12:57:28) [Clang 14.0.6 ] jax.devices (1 total, 1 local): [METAL(id=0)] process_count: 1 platform: uname_result(system='Darwin', node='JurajsMacBookPro.local', release='23.4.0', version='Darwin Kernel Version 23.4.0: Wed Feb 21 21:44:54 PST 2024; root:xnu10063.101.15~2/RELEASE_ARM64_T6031', machine='arm64') jaxmetal                 0.0.6)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"jax-metal: Memory leak on jit boundary, MPS"," Description Simply calling a  function with any kind of array input leaks memory in an noninsignificant way. CPU is okay, but device (MPS) exhibits the leak. Sample program:  Output on CPU:  Output on MPS:   System info (python version, jaxlib version, accelerator, etc.) Platform 'METAL' is experimental and not all JAX functionality may be correctly supported! 20240318 18:17:36.186791: W pjrt_plugin/src/mps_client.cc:563] WARNING: JAX Apple GPU support is experimental and not all JAX functionality is correctly supported! Metal device set to: Apple M3 Max systemMemory: 128.00 GB maxCacheSize: 48.00 GB jax:    0.4.25 jaxlib: 0.4.23 numpy:  1.26.4 python: 3.12.2  (main, Feb 27 2024, 12:57:28) [Clang 14.0.6 ] jax.devices (1 total, 1 local): [METAL(id=0)] process_count: 1 platform: uname_result(system='Darwin', node='JurajsMacBookPro.local', release='23.4.0', version='Darwin Kernel Version 23.4.0: Wed Feb 21 21:44:54 PST 2024; root:xnu10063.101.15~2/RELEASE_ARM64_T6031', machine='arm64') jaxmetal                 0.0.6",2024-03-18T16:18:58Z,bug Apple GPU (Metal) plugin,open,0,7,https://github.com/jax-ml/jax/issues/20296,"Can't even try with jaxmetal version 0.0.4 as recommented here: https://developer.apple.com/metal/jax/ Because pip says: > jaxmetal 0.0.4 depends on jaxlib==0.4.11 and > ERROR: Could not find a version that satisfies the requirement jaxlib==0.4.11 (from versions: 0.4.17, 0.4.18, 0.4.19, 0.4.20, 0.4.21, 0.4.22, 0.4.23, 0.4.24, 0.4.25) also not possible to go for the oldest available jaxlib 0.4.17: > The conflict is caused by: >   The user requested jaxlib==0.4.17 >    jaxmetal 0.0.4 depends on jaxlib==0.4.11 So we're in a bit of a pickle here as any longer/heavier training on the MPS will slow down dramatically as it swaps gigabytes of allocated (leaked) memory.","jaxmetal==0.0.5 with (its pip dependency) jaxlib==0.4.20 (which pulls jax0.4.20), on MLIR 1.0.0 (Sonoma 14.2.1)  also does exhibit the leak. ","Hi guys, would there be any outlook on this? It's a show stopper for me, and if nobody is free to have a look, maybe I could. What is the chance this is a bug in jaxmetal? (I don't think Apple's jaxmetal sources are public.) Last question  should this be raised in https://developer.apple.com/forums/tags/tensorflowmetal? (This page points to it.)",This is reproducible and we will take a look. ,"For anyone else having the same issue and being blocked on MPS due to it, have a look at the MLX framework which is similar to JAX and tailored specifically to Apple Silicon.", it is similar but also misses so much. have you found a good solution to MLX profiling?,"I am also experiencing the same issue. My program starts with 12 GB of memory usage, but after a few epochs, it increases to around 90 GB. At this point, GPU usage drops to 0%, and my code stops training altogether. Any updates or potential workarounds for this problem would be greatly appreciated. Platform 'METAL' is experimental and not all JAX functionality may be correctly supported! Metal device set to: Apple M3 Pro systemMemory: 18.00 GB maxCacheSize: 6.00 GB jax:    0.4.26 jaxlib: 0.4.26 numpy:  1.26.0 python: 3.11.7 (v3.11.7:fa7a6f2303, Dec  4 2023, 15:22:56) [Clang 13.0.0 (clang1300.0.29.30)] jax.devices (1 total, 1 local): [METAL(id=0)] process_count: 1 platform: uname_result(system='Darwin', node='AtasMacBookPro.fritz.box', release='24.0.0', version='Darwin Kernel Version 24.0.0: Tue Sep 24 23:37:25 PDT 2024; root:xnu11215.1.12~1/RELEASE_ARM64_T6030', machine='arm64') WARNING: All log messages before absl::InitializeLog() is called are written to STDERR W0000 00:00:1728661436.923585 7587819 mps_client.cc:510] WARNING: JAX Apple GPU support is experimental and not all JAX functionality is correctly supported! I0000 00:00:1728661436.937859 7587819 service.cc:145] XLA service 0x1143d2270 initialized for platform METAL (this does not guarantee that XLA will be used). Devices: I0000 00:00:1728661436.937877 7587819 service.cc:153]   StreamExecutor device (0): Metal,  I0000 00:00:1728661436.939592 7587819 mps_client.cc:406] Using Simple allocator. I0000 00:00:1728661436.939615 7587819 mps_client.cc:384] XLA backend will use up to 12884443136 bytes on device 0 for SimpleAllocator."
767,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Building from source fails with GCC 8.5 clang 16)ï¼Œ å†…å®¹æ˜¯ ( Description From version 0.4.24 (previous versions didn't have this problem) I am facing the following issue running `python build/build.py`  I know they are related to XLA but If I download the code related to the commit 12eee889e1f2ad41e27d7b0e970cb92d282d3ec5 (the one fixed for version 0.4.24) I can compile it with bazel 6.1.2 (`bazel build //xla/...`) without any problem and I am not sure where is the issue. Is there any kind of requirements about the build tools?  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Building from source fails with GCC 8.5 clang 16," Description From version 0.4.24 (previous versions didn't have this problem) I am facing the following issue running `python build/build.py`  I know they are related to XLA but If I download the code related to the commit 12eee889e1f2ad41e27d7b0e970cb92d282d3ec5 (the one fixed for version 0.4.24) I can compile it with bazel 6.1.2 (`bazel build //xla/...`) without any problem and I am not sure where is the issue. Is there any kind of requirements about the build tools?  System info (python version, jaxlib version, accelerator, etc.) ",2024-03-18T11:22:06Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/20290,My failure. I tried again to compile the XLA project from commit 12eee889e1f2ad41e27d7b0e970cb92d282d3ec5 and it failed. I've just opened the issue https://github.com/openxla/xla/issues/10643,"I'm going to close, because the XLA bug should suffice. No need to have both! (In general, XLA only does presubmit testing with clang, not gcc, so it's probably something that broke under gcc but not clang. Further, your gcc is quite old: v8.5 is 6 years old at this point.)"
1466,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(add a jax.Array analog that can contain extended dtypes)ï¼Œ å†…å®¹æ˜¯ ( What problem are you trying to solve? We want to be able to return arrays with arbitrary extended dtypes from jitted computations. That is, we want a `jax.Array`like type that works with extended dtypes. That will be useful for all of our alreadyknown applications of extended dtypes: 1. scale dtypes for AQT (though we hope to replace the need for these with other mechanisms) 2. dtypes with nonstandard tangent dtypes, like floats with different precisions or this AQT application 3. a replacement for float0 so we don't have to rely on a numpy 'void' dtype 4. a replacement for our `core.DArray` for handling bints (Creating a fully general extended dtype is an internal API, not a userfacing one. But we likely want to give users float0 arrays, or bint arrays, or give them ways to create nonstandard primaltangentdtype associations.) More concretely, we want the tests in this PR to pass. Currently, we can use arrays with arbitrary extended dtypes _within_ jitted computations (see e.g. these tests), but we can't return them because `jax.Array` doesn't allow extended dtypes. Moreover extending it would require editing its C++ implementation. And it's not even clear we should: it's nice that `jax.Array` models PjRt array)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,add a jax.Array analog that can contain extended dtypes," What problem are you trying to solve? We want to be able to return arrays with arbitrary extended dtypes from jitted computations. That is, we want a `jax.Array`like type that works with extended dtypes. That will be useful for all of our alreadyknown applications of extended dtypes: 1. scale dtypes for AQT (though we hope to replace the need for these with other mechanisms) 2. dtypes with nonstandard tangent dtypes, like floats with different precisions or this AQT application 3. a replacement for float0 so we don't have to rely on a numpy 'void' dtype 4. a replacement for our `core.DArray` for handling bints (Creating a fully general extended dtype is an internal API, not a userfacing one. But we likely want to give users float0 arrays, or bint arrays, or give them ways to create nonstandard primaltangentdtype associations.) More concretely, we want the tests in this PR to pass. Currently, we can use arrays with arbitrary extended dtypes _within_ jitted computations (see e.g. these tests), but we can't return them because `jax.Array` doesn't allow extended dtypes. Moreover extending it would require editing its C++ implementation. And it's not even clear we should: it's nice that `jax.Array` models PjRt array",2024-03-15T05:24:54Z,pull ready,closed,2,2,https://github.com/jax-ml/jax/issues/20266,"Looking ahead a bit: several unit tests that currently act on `PRNGKeyArray` are effectively earray tests more broadly. At some point after merging this, we should try to find and repurpose such tests for our earray fork/implementation (then work to get them passing).","> expose this via jax.experimental.earray (or similar) for now? Turns out we don't even need to expose it in this PR! That is, the only thing that ever needs to construct an EArray is the `global_sharded_result_handler` of an extended dtype rule set. So no new userfacing APIs here."
459,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`jnp.digitize` does not work in corner case with no bins)ï¼Œ å†…å®¹æ˜¯ ( Description `np.digitize(x, [])` is equivalent to `zeros_like(x, int)`. `jnp.digitize` blows up instead. The problem looks like a simple typo in the code.    System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,`jnp.digitize` does not work in corner case with no bins," Description `np.digitize(x, [])` is equivalent to `zeros_like(x, int)`. `jnp.digitize` blows up instead. The problem looks like a simple typo in the code.    System info (python version, jaxlib version, accelerator, etc.) ",2024-03-14T21:11:20Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/20256,Good catch! Are you interested in putting together a PR with the fix? If not one of the team members can take care of it. Thanks!,"ok, i'll do the pr"
1469,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([mutable-arrays] support closed-over mutable arrays in jit)ï¼Œ å†…å®¹æ˜¯ (This PR adds support for jitted functions closing over mutable arrays, like this:  Previously we only handled mutable arrays that were explicit arguments, like `def f(x_mut): ...`. Here are the main ingredients: 1. When we lower to an XLA computation, we don't want closedover MutableArrays to be treated as constants in the XLA computation; instead we want them to correspond to inputs (and outputs, with aliasing) so that the computation can be fed their current value (and return the new value) on each execution. To produce such an XLA computation, starting from a ClosedJaxpr with MutableArrays in its consts, we hoist out those MutableArray consts from the ClosedJaxpr and convert the corresponding jaxpr binders from constbinders to lambdabinders. (We also do some bookkeeping to handle the newlyintroduced inputs, e.g. triviallyextending the list of input shardings.) 2. Correspondingly, we pass along those MutableArray objects to the dispatch path, so that their buffers can be fed in on every dispatch. That is, where before this PR we had the dispatch logic take a `list[int  None]]` which passes along the extra arguments. 3. To handle the nested jit case, we ensure any MutableArrays closed over by inner jits get hoi)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[mutable-arrays] support closed-over mutable arrays in jit,"This PR adds support for jitted functions closing over mutable arrays, like this:  Previously we only handled mutable arrays that were explicit arguments, like `def f(x_mut): ...`. Here are the main ingredients: 1. When we lower to an XLA computation, we don't want closedover MutableArrays to be treated as constants in the XLA computation; instead we want them to correspond to inputs (and outputs, with aliasing) so that the computation can be fed their current value (and return the new value) on each execution. To produce such an XLA computation, starting from a ClosedJaxpr with MutableArrays in its consts, we hoist out those MutableArray consts from the ClosedJaxpr and convert the corresponding jaxpr binders from constbinders to lambdabinders. (We also do some bookkeeping to handle the newlyintroduced inputs, e.g. triviallyextending the list of input shardings.) 2. Correspondingly, we pass along those MutableArray objects to the dispatch path, so that their buffers can be fed in on every dispatch. That is, where before this PR we had the dispatch logic take a `list[int  None]]` which passes along the extra arguments. 3. To handle the nested jit case, we ensure any MutableArrays closed over by inner jits get hoi",2024-03-13T04:14:22Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/20218
681,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Slow host to GPU transfer with `device_put`)ï¼Œ å†…å®¹æ˜¯ ( Description I've observed very slow hosttogpu transfer speeds. I'm using the following benchmark script, which you may enjoy at your leisure.  On my A100 system it generates the following output.  Please let me know if I'm doing something wrong (likely) or if this to be expected! Many thanks for your assistance and time. Zac Update: modified the script to rerun the transfer several times.  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Slow host to GPU transfer with `device_put`," Description I've observed very slow hosttogpu transfer speeds. I'm using the following benchmark script, which you may enjoy at your leisure.  On my A100 system it generates the following output.  Please let me know if I'm doing something wrong (likely) or if this to be expected! Many thanks for your assistance and time. Zac Update: modified the script to rerun the transfer several times.  System info (python version, jaxlib version, accelerator, etc.) ",2024-03-12T22:27:35Z,bug NVIDIA GPU,closed,0,6,https://github.com/jax-ml/jax/issues/20209," Can you try this script? The first call is slower then others as JAX is compiling. So you must discard it. Also, on A100, you have half the perf you expected as your number is the sum on both direction. Here is the output I'm having on V100, so the PCI ratio isn't the right one as it should be lower.  That would still be only 22% efficient for V100. When I run under nsys, I see the comm is split into many part. I'm not sure if this is CUDA or XLA that trigger this behavoir. Are you only looking to understand or you need to speed this up?",I think this may have regressed in 0.4.25. Try 0.4.24? (Both jax and jaxlib.),"  When I run this repeatedly there is a slight speedup   3% is still unacceptable. However, for the development cycle I will often need to load a checkpoint at the start, and if that is slow then it slows everything down. Even if it is due to compilation, Jax should not require 15 seconds to compile a simple host to device transfer.   Under jax 0.4.24 it's a hair better, but nothing to write home about ","https://github.com/openxla/xla/pull/10528 will revert to the 0.4.24 speeds, which is something.", Did you fix the expected speed as discussed at https://github.com/google/jax/issues/20209issuecomment1995011832 ? I can take another look when the PR is merged.,"https://github.com/openxla/xla/pull/10629 should fix this issue, although I think we can do better still (next week!)"
1428,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(JAX-based gradient descent plateaus)ï¼Œ å†…å®¹æ˜¯ ( Description I'm writing my own implementation of some numerical solution papers that solves differential equations using machine learning. However, my implementation fails to converge for examples with slightly complex terms: for example, it can solve the one with exp(y), but it can't solve 4*(exp(y)+exp(y/2)). And when my implementation converges, it does so slower than reported results; what they achieve with 100 epochs I achieve with 1000. Now, perhaps there is a lot to explain in terms of this research to get us to the same page, but here I just want to know how JAX might have been causing the plateau here. I'm suspecting the stacking operation which happens in the first line of the function `N(w, x)`. There I'm just trying to represent a scalar by plugging it to first 5 Chebyshev polynomials, placing each result to a vector respective to the order of the polynomial and getting a 5dimensional representation.  GPU returns the same result but slower.  Result:   System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.23 jaxlib: 0.4.23 numpy:  1.25.2 python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,JAX-based gradient descent plateaus," Description I'm writing my own implementation of some numerical solution papers that solves differential equations using machine learning. However, my implementation fails to converge for examples with slightly complex terms: for example, it can solve the one with exp(y), but it can't solve 4*(exp(y)+exp(y/2)). And when my implementation converges, it does so slower than reported results; what they achieve with 100 epochs I achieve with 1000. Now, perhaps there is a lot to explain in terms of this research to get us to the same page, but here I just want to know how JAX might have been causing the plateau here. I'm suspecting the stacking operation which happens in the first line of the function `N(w, x)`. There I'm just trying to represent a scalar by plugging it to first 5 Chebyshev polynomials, placing each result to a vector respective to the order of the polynomial and getting a 5dimensional representation.  GPU returns the same result but slower.  Result:   System info (python version, jaxlib version, accelerator, etc.) jax:    0.4.23 jaxlib: 0.4.23 numpy:  1.25.2 python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] jax.devices (1 total, 1 local): [CpuDevice(id=0)] process_count: 1",2024-03-12T13:32:14Z,question,closed,0,11,https://github.com/jax-ml/jax/issues/20192,"If the problem is slow convergence, I would check the learning rate. `0.0000005` seems very small.","> If the problem is slow convergence, I would check the learning rate. `0.0000005` seems very small. Oh, I tried many learning rates, indeed I thought lr decay would solve it, but it didn't. Then I forgot to run with the largest appropriate one before sharing. Anyway, with .5 it converges to `error: 1278.272054` then it's stuck there.","Without digging further, my guess would be that you've found a local optimum.","> Without digging further, my guess would be that you've found a local optimum. Paper omits training details, so this is possible. I'd like to discuss here if this implementation of hardcoded representation is correct; does it hurt the backpropagation or is it fine? ","That formulation should be fine when it comes to autodiff. The only issue I see is that the recursive `chebyshev` implementation you're using is not as efficient as it could be: I suspect that the compiler optimizes it well enough, but it will generate a lot of duplicate statements and lead to slow compile times. It shouldn't affect the numerical results though.","(that said, I've not looked at the paper you're trying to implement, so I can't comment on whether what you're doing is similar to what it is doing)","Is the gradient norm approximately zero, i.e. do the iterates stop moving (in addition to the loss not progressing)? To rule out numerical approximation issues, you could try comparing to Autograd, since it is close to a dropin replacement (no vmap though) and it'd use NumPy for numerical operations. Can you link the paper in question?","Also, another thing I noticed: if you change the initial `params` state by using a different seed for `random.key`, you end up with a different answer (also with zero gradient). This strongly suggests that you have a multimodal likelihood space, and you're landing in a local rather than a global optimimum. One way around this would be to use more sophisticated optimization methods; see for example the jaxopt package.","> Is the gradient norm approximately zero, i.e. do the iterates stop moving (in addition to the loss not progressing)? >  > To rule out numerical approximation issues, you could try comparing to Autograd, since it is close to a dropin replacement (no vmap though) and it'd use NumPy for numerical operations. >  > Can you link the paper in question? Tuning the learning rate along training, for the above equation (loss function) I got `error: 70.535873 grad norm: 0.010273`, so yes, they vanish (Additional question here: I knew about learning rate decay, but I had never increased learning rates during training before, yet increasing it got me out of local optimums here. But if I had started with that learning rate, loss would return `nan`.  Any source on this?) This particular equation is not solved in the paper proposing this architecture, but I expect it to solve it, at least reach an error  Also, another thing I noticed: if you change the initial `params` state by using a different seed for `random.key`, you end up with a different answer (also with zero gradient). This strongly suggests that you have a multimodal likelihood space, and you're landing in a local rather than a global optimimum. One way around this would be to use more sophisticated optimization methods; see for example the jaxopt package. Few experiments with Adam resulted in same kind of behavior. And that one paper I mentioned, where it reported convergence in 100 epochs but my JAX implementation did after 1000, it reported using vanilla GD and so that's how I did it. I'll look into it though, thanks! ","> Additional question here: I knew about learning rate decay, but I had never increased learning rates during training before, yet increasing it got me out of local optimums here. But if I had started with that learning rate, loss would return nan. Any source on this? I think it depends on the problem at hand, but in some problems choosing good initializers is difficult and so if we ""warm up"" (ie increase) the step size at first we can get into better optimization regimes. I think that is common in deep learning: see e.g. this paper and references, though I don't know what the canonical references are. On the theory side, the work of Altschuler and Parrilo like here and here (and Altschuler's 2015 masters thesis under Parrilo) offer some explanation of nonmonotonic, notjustconstantordecaying step sizes. Should we keep this issue open, or close it until new questions arise? (Sorry, I'm just not sure if there are outstanding JAX questions at the moment!)","> > Additional question here: I knew about learning rate decay, but I had never increased learning rates during training before, yet increasing it got me out of local optimums here. But if I had started with that learning rate, loss would return nan. Any source on this? >  > I think it depends on the problem at hand, but in some problems choosing good initializers is difficult and so if we ""warm up"" (ie increase) the step size at first we can get into better optimization regimes. I think that is common in deep learning: see e.g. this paper and references, though I don't know what the canonical references are. On the theory side, the work of Altschuler and Parrilo like here and here (and Altschuler's 2015 masters thesis under Parrilo) offer some explanation of nonmonotonic, notjustconstantordecaying step sizes. >  > Should we keep this issue open, or close it until new questions arise? (Sorry, I'm just not sure if there are outstanding JAX questions at the moment!) I think we can close since there is no clear evidence that the problem is JAXrelated. Not sure if we must consider it ""completed"" though. Thanks for quick responses."
413,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Added a lowering rule for lax.sign_p and improved test coverage for binary ops)ï¼Œ å†…å®¹æ˜¯ (Added a lowering rule for lax.sign_p and improved test coverage for binary ops Closes CC(bug(pallas): Unimplemented primitive in Pallas GPU lowering: sign))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Added a lowering rule for lax.sign_p and improved test coverage for binary ops,Added a lowering rule for lax.sign_p and improved test coverage for binary ops Closes CC(bug(pallas): Unimplemented primitive in Pallas GPU lowering: sign),2024-03-12T13:10:29Z,,closed,0,0,https://github.com/jax-ml/jax/issues/20191
505,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fixed lowering of binary ops for signed dtypes)ï¼Œ å†…å®¹æ˜¯ (Fixed lowering of binary ops for signed dtypes All integers in Trition are signless, so we need to manually forward the signedness of the abstract values. I wonder if we should avoid relying on MLIR types altogether and change _cast and similar APIs to accept JAX dtypes instead?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Fixed lowering of binary ops for signed dtypes,"Fixed lowering of binary ops for signed dtypes All integers in Trition are signless, so we need to manually forward the signedness of the abstract values. I wonder if we should avoid relying on MLIR types altogether and change _cast and similar APIs to accept JAX dtypes instead?",2024-03-11T21:03:25Z,,closed,0,0,https://github.com/jax-ml/jax/issues/20180
234,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add int4 test to ArrayImpl.)ï¼Œ å†…å®¹æ˜¯ (Add int4 test to ArrayImpl.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add int4 test to ArrayImpl.,Add int4 test to ArrayImpl.,2024-03-11T20:11:28Z,,closed,0,0,https://github.com/jax-ml/jax/issues/20178
627,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add `where` argument to `argmax`, `argmin`, `ptp`, `cumsum`, `cumprod`)ï¼Œ å†…å®¹æ˜¯ (The following functions receive a `where` argument, which limits the reduction to a given boolean mask:  max  min  sum  mean  all  any  prod  var  std **Feature request:** Add a `where` argument to the following functions:  [ ] argmax  [ ] argmin  [ ] ptp  [ ] cumsum  [ ] cumprod Related:  https://github.com/numpy/numpy/issues/14371  https://github.com/numpy/numpy/pull/21625)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"Add `where` argument to `argmax`, `argmin`, `ptp`, `cumsum`, `cumprod`","The following functions receive a `where` argument, which limits the reduction to a given boolean mask:  max  min  sum  mean  all  any  prod  var  std **Feature request:** Add a `where` argument to the following functions:  [ ] argmax  [ ] argmin  [ ] ptp  [ ] cumsum  [ ] cumprod Related:  https://github.com/numpy/numpy/issues/14371  https://github.com/numpy/numpy/pull/21625",2024-03-11T20:03:12Z,enhancement,open,1,17,https://github.com/jax-ml/jax/issues/20177,"Hi  thanks for the request! We generally follow the NumPy API in `jax.numpy`, and as far as I can tell, numpy does not support a `where` argument for any of these functions."," Recalling what we did for jax.numpy.fill_diagonal with `inplace` (vs. numpy.fill_diagonal):  https://github.com/google/jax/issues/2680issuecomment1763273560  https://github.com/google/jax/pull/18180 I think that, since the API will be identical/compatible when this functionality is added to numpy (see the linked issue in the OP), JAX implementing it ahead of time should not cause any issues. Personally, I'd find it very helpful to have this functionality.","`fill_diagonal` is somewhat different: it's *impossible* to implement that in JAX without changing the API to avoid inplace modification. Adding additional functionality to existing APIs is a qualitatively different discussion. There is some upside in flexibility, but it has some downsides too, namely: 1. It increases the userfacing API surface that we need to support for the rest of time. 2. It makes `jax.numpy` harder to test, because we must also implement the ground truth version to test against. 3. If NumPy or the Array API eventually adds the same keyword with different semantics, it makes for an awkward deprecation (see e.g. the `arr.device` method deprecation that we're doing currently). 4.  It increases, if only marginally, the cognitive burden of switching between `jax.numpy` and original `numpy`. For those reason, I lean toward not adding these sorts of keywords until they are part of either the NumPy API or the Python Array API."," Thanks for your response! Is there a namespace for useful array functionality that has not yet been incorporated into NumPy? Perhaps `jax.lax` or some theoretical `jax.numpy_experimental`?  Somewhat offtopic, opinionated tangent: It seems detrimental to JAX's evolution to tie itself so strictly to NumPy. ğŸ˜” This tying has also caused issues in the past, when NumPy's way of doing things is awkward. Is there any existing theoretical discussion on the benefits, drawbacks, and future of JAX tying itself strictly to NumPy's API? This is perhaps a provocative opinion, and purely speculative at this point, but I'd love to see JAX liberate itself from NumPy's straightjacket. ğŸ™‚ Especially if it eclipses NumPy in terms of userbase (which I'm hoping and rooting for). For example, I find certain aspects of PyTorch's tensor API to be superior to NumPy's array API, from a usability perspective. It's also worth noting that NumPy itself currently breaks with the Python Array API.","We have a related discussion here: https://jax.readthedocs.io/en/latest/jep/18137numpyscipyscope.html, although it kind of takes as given that we won't implement things in `jax.numpy` or `jax.scipy` that are not in `numpy` or `scipy` respectively. JAX has plenty of APIs that exist outside of the numpy API, they are just not generally found in the `jax.numpy` namespace.","> For example, I find certain aspects of PyTorch's tensor API to be superior to NumPy's array API, from a usability perspective. Very interesting! Do you have examples in mind?"," One example is how PyTorch makes it easier to chain array operations. Consider the following:  Notice the ""backandforth"" or ""spiral"" pattern of the function calls. Compare it to  which is much easier to write and read: 1. It follows the logical order of the operations. 2. It avoids using (and importing) `jnp.`, which reduces code clutter. In my experience, this kind of pattern (with varying lengths) is common. Therefore, from a usability perspective, it would be convenient to attach more methods to JAX arrays themselves.","Thanks for elaboration. I've seen this kind of idea come up before in other contexts (e.g. https://github.com/numpy/numpy/issues/24081) I personally don't feel strongly about this (it would be convenient in some cases, but there are costs to that large an expansion of the API surface, both in terms of maintenance burden and cognitive overhead) but I'm happy to hear what others might think.","Thanks for the link. It's indeed very relevant. It also brings up another advantage I forgot to include: > With fluent API in common, users may more easily write libraryagnostic code. I saw this in practice here: https://github.com/cvxpy/cvxpy/issues/2237. Anecdotally, since it was mentioned in that thread, I've personally also found pandas's methodchaining API a pleasure to work with. Unfortunately, I think NumPy's devs are making the wrong call here. If I *had* to choose between standalone functions and methods, I'd choose the latter, due to the aforementioned advantages from the perspective of user experience. It's also easy to get a standalone function from the method (e.g. `sin = lambda x: x.sin()`), whereas the converse is not true.","I understand the choice the numpy devs have made. For what itâ€™s worth, I wouldnâ€™t suggest trying to reopen that discussion. Theyâ€™ve made their opinions pretty clear, and itâ€™s a very small team supporting a very large user community.","> I understand the choice the numpy devs have made. [...] Theyâ€™ve made their opinions pretty clear But do you agree with it? Does the user community agree with it? I won't try to steer NumPy's course on this design choice; it looks like that ship has sailed. I'm more interested in JAX's future. (Again, speaking theoretically at this point.) It might be wise to carefully survey the community for these kinds of impactful, longlasting design decisions. And to ensure surveyees have a good understanding of the issue, each option could be presented with a list of its pros and cons. Perhaps even an argument map. I'd also create a space for the community to discuss the options, and keep it open for a reasonably extended period of time before committing. Together, these could help the project establish a confident, communitywide consensus on the best direction for JAX's evolution. Any thoughts welcome. ğŸ™‚","> But do you agree with it? On the whole, yes I do. I understand the reason that you are advocating for methodbased access to all ufuncs and reductions: there are certain situations in which it could make for cleaner, more concise code. But I think the biggest danger to programming languages and DSLs in the long term is bloat in the API surface, and so it is prudent to be conservative when considering addition of new APIs or duplicate spellings of existing APIs. NumPy is quite conservative in this way, and I think that's inextricably tied to why it's remained a successful project for a quarter century. On the JAX side, we have benefitted from using numpy (and more recently the Python Array API) as a standard, because it lets us take the prudent stance without having to debate the individual merits of every single proposed API extension. I believe that is good for JAX as a whole. Is the JAX API perfect? No. Are we willing to change it? Yes, and we add features every day! But it would not be wise to approach changes like the ones proposed here without careful consideration of the costs as well as the benefits.","Just making a note of this here so I don't forget when https://github.com/numpy/numpy/issues/26336 is completed:  Once we add `where` to `argmax`, we can add `where` to `random.categorical` (which uses `argmax` internally), removing the need to manually mask logits with `jnp.inf`."," While this is being worked on from the NumPy side, in the meantime, would it be acceptable to add an optional mask argument to `lax.argmax`, which `numpy.argmax` uses internally? That way, as soon as NumPy adds the `where` argument, JAX can quickly do the same.",`lax` functions by design are moreorless a direct wrapper for the underlying XLA operation; I don't think it's appropriate to add masking support there when the XLA op doesn't implement it.,I didn't find an XLA op for argmax. Is the argmax reduction defined in JAX itself? I found the following chain of definitions: argmax > argmax_p > _compute_argminmax > _ArgMinMaxReducer.,`lax.argmax` is essentially a convenience wrapper that generates a single call to `lax.reduce`
493,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Pallas: transpose matmul throws segmentation fault)ï¼Œ å†…å®¹æ˜¯ ( Description Transposing a matrix and doing a matrix vector product produces `Segmentation fault (core dumped)` but multiplying by the non transposed matrix works fine. Here is a small reproducer:   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Pallas: transpose matmul throws segmentation fault," Description Transposing a matrix and doing a matrix vector product produces `Segmentation fault (core dumped)` but multiplying by the non transposed matrix works fine. Here is a small reproducer:   System info (python version, jaxlib version, accelerator, etc.) ",2024-03-09T14:53:42Z,bug pallas,closed,0,8,https://github.com/jax-ml/jax/issues/20152,"Thanks, this looks like a Triton bug.  I am able to reproduce this on A100 internally. The error message coming from one of the Triton passes is `getElemsPerThread is not supported for shared layout`. I will try to find a reproducer using Triton Python APIs.","For what is worth, I moved to pallas to see if I could avoid this other bug I had found on triton which also gave the error ",hmm. I thought the other user mentioned that it's solved using the latest triton.," can you check if the issue is still present in the Triton nightly? If yes, the next jaxlib release should have the upstream fixes as well.","That other issue is not solved by the Triton nightly (which, as per their instructions, I got from here: `pip install U indexurl https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/TritonNightly/pypi/simple/ tritonnightly`), because the Triton nightly build is an older version of Triton (2.1.0) that does not include 3D matmul functionality at all. It _is_ fixed by building Triton from source, at the most recent commit to main. Will that make it into the next jaxlib release?"," likely yes, assuming that version of Triton is integrated into the openxla/triton repository by the next release.",The nightly build has been broken for quite a while...sorry about that,I confirmed this does not crash with the latest Triton nightly. Closing.
679,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(JAX reports inaccurate error when trying to acquire already-owned TPU)ï¼Œ å†…å®¹æ˜¯ ( Description JAX doesn't nicely share TPUs with other frameworks (e.g. PyTorch/XLA, TF, etc). This is fine, but the error reported by JAX is misleading. It'd be preferable to actively check if JAX is unable to use the TPU due to multiple libraries using the TPU and report the issue to the user. Here's an example:  `jax.devices()` results in the following error:   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,JAX reports inaccurate error when trying to acquire already-owned TPU," Description JAX doesn't nicely share TPUs with other frameworks (e.g. PyTorch/XLA, TF, etc). This is fine, but the error reported by JAX is misleading. It'd be preferable to actively check if JAX is unable to use the TPU due to multiple libraries using the TPU and report the issue to the user. Here's an example:  `jax.devices()` results in the following error:   System info (python version, jaxlib version, accelerator, etc.) ",2024-03-08T23:12:56Z,bug,open,3,1,https://github.com/jax-ml/jax/issues/20148,"Hi  ,  I tested the provided example in a colab notebook connected to a v28 TPU runtime with JAX 0.5.0 version, it executed without any error. The gist is attached for your reference. Could you please verify in your setup with latest JAX version, let us know if the issue still persists? Thank you."
641,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Latest JAX `0.4.24` does not detect GPUs)ï¼Œ å†…å®¹æ˜¯ ( Description The latest JAX `0.4.24` does not detect GPUs, both using local cuda and pipinstalled cuda. The latest working version for me is `0.4.23`.  Reproduce `pip install ""jax[cuda12_pip]""==0.4.24 f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` With `python c ""import jax; jax.devices('gpu')""`, I get:   System info (python version, jaxlib version, accelerator, etc.) `nvcc version`  `nvidiasmi` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Latest JAX `0.4.24` does not detect GPUs," Description The latest JAX `0.4.24` does not detect GPUs, both using local cuda and pipinstalled cuda. The latest working version for me is `0.4.23`.  Reproduce `pip install ""jax[cuda12_pip]""==0.4.24 f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` With `python c ""import jax; jax.devices('gpu')""`, I get:   System info (python version, jaxlib version, accelerator, etc.) `nvcc version`  `nvidiasmi` ",2024-03-08T10:51:28Z,bug,open,0,12,https://github.com/jax-ml/jax/issues/20133,"The issue is in your error message:  You'll need to install cuBLAS version 12.3 or later when using the prebuilt wheels for jax v0.4.24, as those wheels are built against cuda 12.3. Since you used `jax[cuda12_pip]`, you should get compatible cuda automatically. My guess is that you have another CUDA installation on your system that is taking precedence. I would suggest figuring out where those other CUDA sources are installed, and either removing them, or ensuring that the pipinstalled CUDA sources take precedence in your path.","> Since you used jax[cuda12_pip], you should get compatible cuda automatically.  Yeah, that's why I thought the problem was not on my side somehow, thanks very much for the info. I would expect that `jax[cuda12_pip]` prioritises the CUDA installed from pip, though, no? Also, I didn't know which CUDA version was v0.4.24 targeting. Perhaps it would be useful to have the minimum CUDA version in https://github.com/google/jax/?tab=readmeovfileinstallation? It used to be there, IIRC.","`jax[cuda12_pip]` installs the correct CUDA sources in your Python `site_packages`. However, if you have other CUDA installations on your system, and your system is set up to load those other sources, they may be loaded before the ones installed with `pip`. There is nothing that JAX or pip can do about this: it is a property of your system. jax 0.4.24 targets CUDA 12.2, and jax 0.4.25 targets CUDA 12.3, if that helps.","Okay, great! Thanks, Jake!","> `jax[cuda12_pip]` installs the correct CUDA sources in your Python `site_packages`. > However, if you have other CUDA installations on your system, and your system is set up to load those other sources, they may be loaded before the ones installed with `pip`. There is nothing that JAX or pip can do about this: it is a property of your system. > jax 0.4.24 targets CUDA 12.2, and jax 0.4.25 targets CUDA 12.3, if that helps. That is it. Different from most other things in python venvs, cuda seem to add itself at the end of the path. So if you have a supercomputer with modules, strangely the system's cuda will take precedence.  Other stuff installed with pip in a venv will take over the ones from the modules, because venv adds them at the beginning of the python path, for example. Except for cuda. Unloading the cuda module forces it to find the one from site_packages and things work. Go figure.",Is there a proper way to check the location of the set of CUDA libraries that jax is loading? `TF_CPP_MIN_LOG_LEVEL=0` is not showing anything more than usual.,"Sorry, I have got to reopen this. Here's the issue. Different packages both install CUDA binaries (e.g., JAX and pytorch). These CUDA binaries have different versions. Using `jax[cuda12_pip]` or similar makes the two requirements conflict. Using `jax[cuda12_local]` and having a local version of CUDA (e.g., in a cluster) does not help because JAX prefers loading the binaries installed via `pip` by some other package. This happens despite setting `XLA_ARGS=""xla_gpu_cuda_data_dir=""`. Is there a way to better diagnose the issue? For example, printing the folder that JAX uses to load the CUDA binaries? (`TF_CPP_MIN_LOG_LEVEL=0` does not help). Is there any other way to force JAX loading from a different folder?"," It's currently not possible to override the search path, because it's set by an `RPATH` on the .so files in jaxlib. We could perhaps fix that with some cunning. However, note that JAX 0.4.26 relaxed its CUDA version constraints. You should be able to just use the same version of CUDA that Pytorch is using (12.1, last I checked).","Thank very much, . I resorted to that in the meanwhile but JAX v0.4.23 introduces breaking changes to the API. I could access those libraries and align them, but that's not a very general case.","Hey  still issues here. I migrated to the latest JAX version with `pip install ""jax[cuda12]""`. I removed all cuda path references in `LD_LIBRARY_PATH` and `PATH` itself, but for some reason JAX still tries to load an older version of the drivers. I get  with  and (empty)  My `nvidiasmi` shows:  But I guess JAX does not use this CUDA, but installs its own, correct? Where should I look at to address this?","> But I guess JAX does not use this CUDA, but installs its own, correct? With `pip install jax[cuda12]`, it will download its own CUDA; with `pip install jax[cuda12_local]`, it will use the local CUDA. See https://jax.readthedocs.io/en/latest/installation.htmlnvidiagpu for details. If you are seeing this error, and have both a locallyinstalled CUDA and a pipinstalled CUDA on your machine and have adjusted `LD_LIBRARY_PATH`, then I suspect the issue is crosstalk between your two CUDA installations.","Thanks, ! That's what I don't understand. Where could the locally installed CUDA be set as higher priority than the pipinstalled one, if `LD_LIBRARY_PATH` and `PATH` are clean? Do you have any idea?"
462,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Metal: failed assertion `New volume: 6 should match old volume: 24` for einsum ""ijk,kji->kij"")ï¼Œ å†…å®¹æ˜¯ ( Description distinct failure mode from CC(Metal: failed to legalize operation 'mhlo.dot_general' for einsum ""ijk,kji>k"")    System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"Metal: failed assertion `New volume: 6 should match old volume: 24` for einsum ""ijk,kji->kij"""," Description distinct failure mode from CC(Metal: failed to legalize operation 'mhlo.dot_general' for einsum ""ijk,kji>k"")    System info (python version, jaxlib version, accelerator, etc.) ",2024-03-07T06:53:13Z,bug Apple GPU (Metal) plugin,closed,0,2,https://github.com/jax-ml/jax/issues/20115,The issue will be fixed in the upcoming release.,Fixed in jaxmetal 0.0.6.
642,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Auto-labels 'gemma' on 'gemma' issues/PRs.)ï¼Œ å†…å®¹æ˜¯ (This workflow automatically identifies issues and pull requests (PRs) related to `Gemma`. It searches for the keyword `Gemma` (caseinsensitive) in both the title and description of the issue/PR. If a match is found, the workflow adds the label `Gemma` to the issue/PR. On first run, the workflow will try to create the label `Gemma`. If unsuccessful, the repository owner will need to create it manually.  label: `Gemma`)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gemma,Auto-labels 'gemma' on 'gemma' issues/PRs.,"This workflow automatically identifies issues and pull requests (PRs) related to `Gemma`. It searches for the keyword `Gemma` (caseinsensitive) in both the title and description of the issue/PR. If a match is found, the workflow adds the label `Gemma` to the issue/PR. On first run, the workflow will try to create the label `Gemma`. If unsuccessful, the repository owner will need to create it manually.  label: `Gemma`",2024-03-07T06:35:38Z,,open,0,0,https://github.com/jax-ml/jax/issues/20113
1411,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Improve documentation for multi-node/host training)ï¼Œ å†…å®¹æ˜¯ (As laid out in CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®), I feel that there is a need to have a minimal example for how to use the new sharding API with `device_put` for training in a multiprocess fashion, like a TPU Pod slice. So far, I've seen multiple different methods to accomplish the same which leverage ideas (like `pjit`) that are out of date and some that straight up don't work. For example, I was following this discussion where apparently `make_array_from_single_device_arrays` wasn't returning a sharded array for the host  but rather the global array which was `n` times as big (`n` is the number of hosts). This is a major pain point, as the API around parallelization should be topnotch considering scalability is a strong focus for the jax team. Rather, in practice the end user ends up wading through a sea of a variety of `xmap`/`pmap`/`sharding` and intermixing of both, some of which are deprecated and not recommended to be used. This seriously needs to be improved. I feel the sharding API is *really* well written for parallelizing across multipledevices on a single host, and it only took me 15 mins to integrate that. However, multinode/process **definitely** needs a minimal example.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Improve documentation for multi-node/host training,"As laid out in CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®), I feel that there is a need to have a minimal example for how to use the new sharding API with `device_put` for training in a multiprocess fashion, like a TPU Pod slice. So far, I've seen multiple different methods to accomplish the same which leverage ideas (like `pjit`) that are out of date and some that straight up don't work. For example, I was following this discussion where apparently `make_array_from_single_device_arrays` wasn't returning a sharded array for the host  but rather the global array which was `n` times as big (`n` is the number of hosts). This is a major pain point, as the API around parallelization should be topnotch considering scalability is a strong focus for the jax team. Rather, in practice the end user ends up wading through a sea of a variety of `xmap`/`pmap`/`sharding` and intermixing of both, some of which are deprecated and not recommended to be used. This seriously needs to be improved. I feel the sharding API is *really* well written for parallelizing across multipledevices on a single host, and it only took me 15 mins to integrate that. However, multinode/process **definitely** needs a minimal example.",2024-03-06T16:24:17Z,enhancement,open,4,3,https://github.com/jax-ml/jax/issues/20099,"I can't agree more! I'm also trying to write some multinode codes, but it's confusing when I want to stick to the style of automatic parallelization with multiple hosts. A minimum example will be extremely helpful. At least it can tell us what is the recommended way to implement parallelization. For now I can also spend a month implementing `make_array_from_single_device_arrays` everywhere in my codes, but I'm afraid it may be deprecated in the future, in which case my effort will be fully wasted.",You can also use `jax.make_array_from_process_local_data` to create global jax.Array from data on your hosts. ,"FWIW, I use scalax for multinode currently. however it does have a bit of a learning curve, but its pretty easy once you do figure it out. Rather disappointing that 3rd party libs have to put up support for something so fundamental as multinode parallelization. I feel it goes against the jax spirit. "
870,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(CPU: incorrect result from combo of arg donation, sharding constraint, and --xla_force_host_platform_device_count=2)ï¼Œ å†…å®¹æ˜¯ ( Description This test passes on GPU (1 or 2 GPUs), on CPU on MacOS M1 (with or without the XLA FLAG), and Linux CPU (without the flag) But with the `xla_force_host_platform_device_count=2` (or 8), on linux, on x64 CPU, the assertion fails with seemingly random garbage being returned instead. I've seen it on three different machines. Removing the donation or the sharding constraints, or the flag makes this test past. I've instantiated other versions of this bug, but I think (hope) they're all the same.    System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,"CPU: incorrect result from combo of arg donation, sharding constraint, and --xla_force_host_platform_device_count=2"," Description This test passes on GPU (1 or 2 GPUs), on CPU on MacOS M1 (with or without the XLA FLAG), and Linux CPU (without the flag) But with the `xla_force_host_platform_device_count=2` (or 8), on linux, on x64 CPU, the assertion fails with seemingly random garbage being returned instead. I've seen it on three different machines. Removing the donation or the sharding constraints, or the flag makes this test past. I've instantiated other versions of this bug, but I think (hope) they're all the same.    System info (python version, jaxlib version, accelerator, etc.) ",2024-03-05T23:30:07Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/20088,I can't seem to repro this in my environment:  Can you try with jax nightly on CPU?,Commands: https://jax.readthedocs.io/en/latest/installation.htmlnightlyinstallation,"yep, works fine. my bad."
1517,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`jax.experimental.attrs`: Spurious ""AssertionError: a jaxpr variable must be created only once per tracer"")ï¼Œ å†…å®¹æ˜¯ ( Description The following code results in an erroneous jax internal assertion > `AssertionError: a jaxpr variable must be created only once per tracer` The issue is a useafterfree due to `experimental.attrs` creating a Tracer but not adequately ensuring its lifetime.  (n.b. this is not code or a style I intend to use; I was just trying to experiment with `experimental.attrs` to wrap my head around it) Output on my laptop (`JAX_TRACEBACK_FILTERING=off` version attached)  The problem arises from these lines in `attrs.py`: https://github.com/google/jax/blob/f9e20d58754283de87b2ed35cc9df58bcdff2073/jax/experimental/attrs.pyL70L71 The `tracer` we create is referenced only by way of the `setattr` on the following line. If a future mutation to the tracked object drops that reference, the tracer will be freed, and its address (`id`) may be reused by a future object. If a new `Tracer` then happens to get allocated at the same address, the `tracer_to_var` map will still contain the old mapping and result in the observed problem. We can hackily test that theory by making sure to manually retain all the tracers inside of our reproducer; this version works reliably for me:   System info (python version, jaxlib version, acce)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"`jax.experimental.attrs`: Spurious ""AssertionError: a jaxpr variable must be created only once per tracer"""," Description The following code results in an erroneous jax internal assertion > `AssertionError: a jaxpr variable must be created only once per tracer` The issue is a useafterfree due to `experimental.attrs` creating a Tracer but not adequately ensuring its lifetime.  (n.b. this is not code or a style I intend to use; I was just trying to experiment with `experimental.attrs` to wrap my head around it) Output on my laptop (`JAX_TRACEBACK_FILTERING=off` version attached)  The problem arises from these lines in `attrs.py`: https://github.com/google/jax/blob/f9e20d58754283de87b2ed35cc9df58bcdff2073/jax/experimental/attrs.pyL70L71 The `tracer` we create is referenced only by way of the `setattr` on the following line. If a future mutation to the tracked object drops that reference, the tracer will be freed, and its address (`id`) may be reused by a future object. If a new `Tracer` then happens to get allocated at the same address, the `tracer_to_var` map will still contain the old mapping and result in the observed problem. We can hackily test that theory by making sure to manually retain all the tracers inside of our reproducer; this version works reliably for me:   System info (python version, jaxlib version, acce",2024-03-05T19:57:55Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/20082,"Brilliant, thank you for the clear diagnosis! I think we should probably persist all the Tracers we create during jaxpr tracing. That's usually what we do with this attribute on the builder, and we append to it when we make new tracers in the usual path, but as you point out we neglected to do it on the attrs path."
573,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(cusparse_build_version outputs version greater than latest cusparse version)ï¼Œ å†…å®¹æ˜¯ ( Description Hi There, I'm trying to run JAX on a fresh VM on cloud infrastructure. The error is on the first line when printing the environment info  This seems like a bug in the code that checks the cuSPARSE version because the latest version of cuSPARSE is `12.2.0.103`.  As seen on my system, I have this version: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,cusparse_build_version outputs version greater than latest cusparse version," Description Hi There, I'm trying to run JAX on a fresh VM on cloud infrastructure. The error is on the first line when printing the environment info  This seems like a bug in the code that checks the cuSPARSE version because the latest version of cuSPARSE is `12.2.0.103`.  As seen on my system, I have this version: ",2024-03-05T18:59:12Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/20079,"JAX's `cusparse_build_version` just returns `CUSPARSE_VERSION` as defined in the cusparse C headers *at the time that `jaxlib` is built*. The definition looks something like this depending on which CUDA version you're using:  This would become `cusparse_version = 12103`. `cusparse_version = 12200` would come from version `12.2.0`. It looks like you're using a jaxlib that was built against version `12.2.0`, but you have version `12.1.0` on your system. Does that help answer your question?","Ah, I see what you mean. I was confused because the `103` of `12103` happened to match the `103` of `12.2.0.103`. It turns out these are not correlated. So even though the latest version of cusparse I have through pip is `12.2.0.1031`, jax must be pulling the cusparse version from elsewhere. Likely I have cusparse `12.1.3.153` installed somewhere, though where I'm not sure yet. I'll go digging for that and see if I can remove it from the path. Confirmed. Cusparse was installed elsewhere. I removed it from LD_LIBRARY_PATH and all is good now. ","Great, glad you figured it out!"
393,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add copy argument to Array.__array__)ï¼Œ å†…å®¹æ˜¯ (This is used by NumPy 2.0, and will be required in a future numpy version (see https://github.com/numpy/numpy/issues/25941). Part of CC(Tracking issue: NumPy 2.0 Compatibility) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add copy argument to Array.__array__,"This is used by NumPy 2.0, and will be required in a future numpy version (see https://github.com/numpy/numpy/issues/25941). Part of CC(Tracking issue: NumPy 2.0 Compatibility) ",2024-03-05T17:32:47Z,kokoro:force-run pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/20077
1188,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Stochastic jaxlib.xla_extension.XlaRuntimeError with sine activation function on NVIDIA A100 (all algorithms tried for... failed.))ï¼Œ å†…å®¹æ˜¯ ( Description I sometimes, but not consistently, get the following jaxlib.xla_extension.XlaRuntimeError when training a neural network with sine activations in JAX on an NVIDIA A100 GPU:   The final part of the traceback is  **This error does however not occur when I use an NVIDIA GeForce RTX 3080 or NVIDIA GeForce RTX 3060.** Some accompanying logs are  The code for the Neural Network layer that seems to be at fault more or less (I'm giving a simplified version without inheritance etc.) comes down to  One of the networks that would sometimes give this error had an input size of 3, a linear output layer with output size 1, a hidden size of 384, and 6 layers total (including final linear layer and input layer). The value of w0 used was 16.  System info (python version, jaxlib version, accelerator, etc.)  The version of Equinox I use (in case it's important) is 0.11.3.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Stochastic jaxlib.xla_extension.XlaRuntimeError with sine activation function on NVIDIA A100 (all algorithms tried for... failed.)," Description I sometimes, but not consistently, get the following jaxlib.xla_extension.XlaRuntimeError when training a neural network with sine activations in JAX on an NVIDIA A100 GPU:   The final part of the traceback is  **This error does however not occur when I use an NVIDIA GeForce RTX 3080 or NVIDIA GeForce RTX 3060.** Some accompanying logs are  The code for the Neural Network layer that seems to be at fault more or less (I'm giving a simplified version without inheritance etc.) comes down to  One of the networks that would sometimes give this error had an input size of 3, a linear output layer with output size 1, a hidden size of 384, and 6 layers total (including final linear layer and input layer). The value of w0 used was 16.  System info (python version, jaxlib version, accelerator, etc.)  The version of Equinox I use (in case it's important) is 0.11.3.",2024-03-05T15:43:57Z,bug,open,1,5,https://github.com/jax-ml/jax/issues/20075,"I'm also seeing this error when using `sine` activations. However, it appears to only occur when `jitting` the function.",I'm running into the same issue when using sine activation function. Just wanted to check if you guys were able to find any solution to this? Thanks!,I used this repo instead of my network and no longer had the issue. Perhaps something to do with how the network was defined. ,"The problem disappeared for me after updating JAX at some point. No idea what changed, but I'm happy my code works ğŸ˜… Op do 5 sep 2024 om 18:42 schreef Mukesh Ghimire ***@***.***>: > I'm running into the same issue when using sine activation function. Just > wanted to check if you guys were able to find any solution to this? Thanks! > > â€” > Reply to this email directly, view it on GitHub > , or > unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >",Just curious if the models performed differently when you had the issue vs now? Appreciate you responding.  I'm still getting the error when the weight $w_0 = 30$ in the activation. 
695,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Pallas slicing makes Jupyter Kernel Crash)ï¼Œ å†…å®¹æ˜¯ ( Description Hi there, I am trying to experiment with slicing in Pallas and I noticed a strange behavior that I can't explain. The following code works fine.  But if I just change the boundaries of my slicing, my Jupyter kernel crashes:  !Screenshot from 20240304 102128 Any ideas and especially some guidelines when dealing with slicing?  System info (python version, jaxlib version, accelerator, etc.) jax=0.4.25 jaxlib=0.4.25+cuda11.cudnn86 jaxtyping=0.2.25 Python=3.11.8)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Pallas slicing makes Jupyter Kernel Crash," Description Hi there, I am trying to experiment with slicing in Pallas and I noticed a strange behavior that I can't explain. The following code works fine.  But if I just change the boundaries of my slicing, my Jupyter kernel crashes:  !Screenshot from 20240304 102128 Any ideas and especially some guidelines when dealing with slicing?  System info (python version, jaxlib version, accelerator, etc.) jax=0.4.25 jaxlib=0.4.25+cuda11.cudnn86 jaxtyping=0.2.25 Python=3.11.8",2024-03-04T09:23:10Z,bug,closed,0,8,https://github.com/jax-ml/jax/issues/20056,"Didn't have a chance to take a really close look but my guess is that you are slicing a (3,)sized array which Triton doesn't like (I think they have to be powersof2 shaped).","Interesting, isn't that like a massive downside when doing GPU operations, to only deal with powersof2 shaped arrays?","You can use masking to handle non powers of 2. Check out the `mask` and `other` argument to `pl.load`. if you want to operate on a, say, (5,)shaped array, you can load an (8,) sized block but use a mask that pads the last three elements.","Thank you very much, this makes more sense now!"," For some reason this also crashes the kernel, if you have any ideas why, it would be greatly appreciated.","`dynamic_slice` isn't supported as well, unfortunately.",I see so  `pl.load` is the only way to handle any sort of slicing with Pallas? ,"On GPU, yes."
821,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Matrix-vector multiply: ValueError: all dimensions of x and y must be >= 16 )ï¼Œ å†…å®¹æ˜¯ ( Description A simple matrixvector multiply Pallas implementation fails:  Error: ValueError: all dimensions of x and y must be >= 16  ...       dimension_numbers=(([1], [0]), ([], []))       preferred_element_type=float32     ] d e     c[:,:] < f   in () } Interestingly, changing the dimensions of y to (32, 16) or (32, 32) works fine, while (32, 8) still fails. There is no issue in the interpret=True mode. What exactly does the interpret mode do?  Thank you.   System info (python version, jaxlib version, accelerator, etc.) jaxlib=0.4.25 python=3.9 GPU=RTX4090)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,Matrix-vector multiply: ValueError: all dimensions of x and y must be >= 16 ," Description A simple matrixvector multiply Pallas implementation fails:  Error: ValueError: all dimensions of x and y must be >= 16  ...       dimension_numbers=(([1], [0]), ([], []))       preferred_element_type=float32     ] d e     c[:,:] < f   in () } Interestingly, changing the dimensions of y to (32, 16) or (32, 32) works fine, while (32, 8) still fails. There is no issue in the interpret=True mode. What exactly does the interpret mode do?  Thank you.   System info (python version, jaxlib version, accelerator, etc.) jaxlib=0.4.25 python=3.9 GPU=RTX4090",2024-03-02T17:18:11Z,question pallas,closed,0,2,https://github.com/jax-ml/jax/issues/20051,"This is a limitation of triton, not a pallas thing I think, see here: https://github.com/openai/triton/issues/1815","Thanks for the question! At first I thought we needed to improve the error, but actually ""all dimensions of x and y must be >=16"" seems pretty good! So I think we can close this issue, unless you have a specific recommendation for how we should improve the error message. > What exactly does the interpret mode do? Instead of lowering to Triton, it runs the computation in XLA:HLO. That means it doesn't have the same physical semantics as running in Triton, but it can be easier to debug."
506,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([mutable-arrays] move MutableArray, add eager, improve tests, fix bug)ï¼Œ å†…å®¹æ˜¯ (1. move MutableArray to core.py, and some handlers to their respective files 2. fix a bug in aliasing setup (it was just broken before, now better test coverage) 3. add eager support by enabling get_p, swap_p, and addupdate_p impls 4. improve tests slightly)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,"[mutable-arrays] move MutableArray, add eager, improve tests, fix bug","1. move MutableArray to core.py, and some handlers to their respective files 2. fix a bug in aliasing setup (it was just broken before, now better test coverage) 3. add eager support by enabling get_p, swap_p, and addupdate_p impls 4. improve tests slightly",2024-03-01T19:13:44Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/20044
1082,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Marking non-trainable / frozen parameters)ï¼Œ å†…å®¹æ˜¯ (Is there a recommended way for tagging an array as not trainable? Specifically in the case where it may not be known beforehand that we do not wish to train the parameter (i.e. so ``stop_gradient`` is not coded into the model). I am also aware that e.g. optax allows specifying which parameters are trainable, but in many cases it would be much simpler to tag the arrays in someway, rather than specifying the trainable parameters using optax. Option 1: make use of duck typing with ``__jax_array__``,  (from https://github.com/google/jax/issues/10065), this is experimental and seems to be at risk of being removed.  Option 2:  add an attribute to the arrays, and use this to partition the model e.g. using equinox  Option 2 seems like it will have some issues, e.g. losing the attributed when constructed with vmap. Is there a recommended way to achieve this?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Marking non-trainable / frozen parameters,"Is there a recommended way for tagging an array as not trainable? Specifically in the case where it may not be known beforehand that we do not wish to train the parameter (i.e. so ``stop_gradient`` is not coded into the model). I am also aware that e.g. optax allows specifying which parameters are trainable, but in many cases it would be much simpler to tag the arrays in someway, rather than specifying the trainable parameters using optax. Option 1: make use of duck typing with ``__jax_array__``,  (from https://github.com/google/jax/issues/10065), this is experimental and seems to be at risk of being removed.  Option 2:  add an attribute to the arrays, and use this to partition the model e.g. using equinox  Option 2 seems like it will have some issues, e.g. losing the attributed when constructed with vmap. Is there a recommended way to achieve this?",2024-02-28T17:36:40Z,enhancement,closed,0,3,https://github.com/jax-ml/jax/issues/20012,"JAX itself doesn't have any notion of model training, so the answer to your question would depend on what framework you're using. It looks like you're using `equinox`, so you may find more useful answers by asking at https://github.com/patrickkidger/equinox.","Ok thanks. I guess more broadly, is there a way to associate metadata with an array without duck typing? ","No, not really. The only supported way to associate metadata with an array would be using a pytree (see https://jax.readthedocs.io/en/latest/pytrees.html), but there's no way to make a pytree ducktype as an array (note that `__jax_array__`, despite existing in some places, is not meant as a public API and is not fully supported throughout the package)."
1041,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Pallas Tutorial outputs RESOURCE_EXHAUSTED)ï¼Œ å†…å®¹æ˜¯ ( Description Hi there, I am trying to learn Pallas by running the code bits from the official documentation from Jax but I am having trouble understanding why I'm short on GPU memory.  This code which is copy pasted from the documentation (except for the PRNGKey) is giving me the following error: `XlaRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 4194304, available: 101376` I saw that to manage memory allocation, we can set 3 environment variables to skip the part where Jax preallocates 75% of the memory, but it is still giving me the error. Any ideas on this? First time really diving into GPUs so I might not have all the theoretical concepts.  System info (python version, jaxlib version, accelerator, etc.) jax=0.4.25 jaxlib=0.4.25+cuda11.cudnn86 jaxtyping=0.2.25 python=3.11.8)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Pallas Tutorial outputs RESOURCE_EXHAUSTED," Description Hi there, I am trying to learn Pallas by running the code bits from the official documentation from Jax but I am having trouble understanding why I'm short on GPU memory.  This code which is copy pasted from the documentation (except for the PRNGKey) is giving me the following error: `XlaRuntimeError: RESOURCE_EXHAUSTED: Shared memory size limit exceeded: requested 4194304, available: 101376` I saw that to manage memory allocation, we can set 3 environment variables to skip the part where Jax preallocates 75% of the memory, but it is still giving me the error. Any ideas on this? First time really diving into GPUs so I might not have all the theoretical concepts.  System info (python version, jaxlib version, accelerator, etc.) jax=0.4.25 jaxlib=0.4.25+cuda11.cudnn86 jaxtyping=0.2.25 python=3.11.8",2024-02-28T09:54:54Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/20008,"Why did I close the issue? Just realised someone else was on the shared GPU, I ran the code with smaller matrices and it works fine."
611,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Vectorised operation on string arrays?)ï¼Œ å†…å®¹æ˜¯ (I know this has been discussed already (pandax), but maybe it comes back as LLMs usage grows. I have the following problem.   I have a big array of `jnp.int32` elements, for example, of shape (2000, 10, 100, 100).  Each id in the array can be mapped to its character `char(id)`, ideally.  However, JAX does not support string arrays.  Is there any way to vectorise/parallelise the computation?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,Vectorised operation on string arrays?,"I know this has been discussed already (pandax), but maybe it comes back as LLMs usage grows. I have the following problem.   I have a big array of `jnp.int32` elements, for example, of shape (2000, 10, 100, 100).  Each id in the array can be mapped to its character `char(id)`, ideally.  However, JAX does not support string arrays.  Is there any way to vectorise/parallelise the computation?",2024-02-28T09:41:11Z,enhancement,closed,0,1,https://github.com/jax-ml/jax/issues/20007,"Hi  unfortunately, since XLA does not have any support for string types, JAX cannot provide any vectorized string operations. There are a few options in this case: First, you could create a hostside mapping of integers to strings, and use JAX to manipulate the array of integers. This is effectively what the `pandax` experiment does under the hood. Once the integer manipulations are finished, you could map those integers back to strings on the host. This may or may not be possible for the particular string manipulations you have in mind. The only other option would be to do all your string manipulation on the host, for example using `pandas`. This is unlikely to change in the future, because string support is not on the roadmap for XLA. Hope that helps!"
780,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix unnecessary memory copies between GPU and CPU when `jax2tf.call_tf()` is used.)ï¼Œ å†…å®¹æ˜¯ (Fix unnecessary memory copies between GPU and CPU when `jax2tf.call_tf()` is used.  The root cause of the bug is that dtype lookups are incorrect because hashes behave differently between dtype instances and their types. Added comments to `jax.dlpack.SUPPORTED_DTYPES` about this.  Added unit test coverage.  Fixing this bug revealed a limitation of causing ""hosttodevice"" copy in the following two situations. See the details in the unit test comments.:    When the dtype is 'int32'.    When using PJRT C API runtime.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Fix unnecessary memory copies between GPU and CPU when `jax2tf.call_tf()` is used.,"Fix unnecessary memory copies between GPU and CPU when `jax2tf.call_tf()` is used.  The root cause of the bug is that dtype lookups are incorrect because hashes behave differently between dtype instances and their types. Added comments to `jax.dlpack.SUPPORTED_DTYPES` about this.  Added unit test coverage.  Fixing this bug revealed a limitation of causing ""hosttodevice"" copy in the following two situations. See the details in the unit test comments.:    When the dtype is 'int32'.    When using PJRT C API runtime.",2024-02-27T17:02:41Z,,closed,0,0,https://github.com/jax-ml/jax/issues/19996
755,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(XLA ""cannot remove instruction"" when compiling big MoE model )ï¼Œ å†…å®¹æ˜¯ ( Description Hi, i've implemented MoE transformer which is configurable by some hyperparams (number of experts, layers, hidden dim, etc.) and i'm training such model on GPU cluster (H100 GPUs) using model, expert and data parallelism. It works perfectly with smalltomedium sizes (125M, 1B, 7B backbones x 16 experts > 1B, 9B, 70B total params), but when i'm trying to train big model (30B backbone, 16 experts, 300B params) using exactly same code and i'm getting very strange error when compiling its train step: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"XLA ""cannot remove instruction"" when compiling big MoE model "," Description Hi, i've implemented MoE transformer which is configurable by some hyperparams (number of experts, layers, hidden dim, etc.) and i'm training such model on GPU cluster (H100 GPUs) using model, expert and data parallelism. It works perfectly with smalltomedium sizes (125M, 1B, 7B backbones x 16 experts > 1B, 9B, 70B total params), but when i'm trying to train big model (30B backbone, 16 experts, 300B params) using exactly same code and i'm getting very strange error when compiling its train step: ",2024-02-27T15:16:27Z,bug,closed,2,3,https://github.com/jax-ml/jax/issues/19994,https://github.com/openxla/xla/issues/10013,The problem is still present in JAX 0.4.25,Hasn't tested yet but it must have been fixed in  https://github.com/openxla/xla/issues/10013issuecomment2004969257
1442,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Errors when building on AMD GPU)ï¼Œ å†…å®¹æ˜¯ ( Description Hiï¼I am very interested in using STARRED (a python package designed for astronomical data processing with GPU support). However, it depends on jax and jaxlib. I followed the instructions from your documentation website and build with the following scripts: `python build/build.py enable_rocm rocm_path=/opt/rocm6.0.2   bazel_options=override_repository=xla=/home/phylmf/lib/xla ` I am sure the rocm is prepared well and I can use pytorch and stable diffusion with no troubles.  Anyway, the building process stopped and reports:  Anything from you will be appreciated! Thanks a lot in making jax/jaxlib available! Mingfeng Liu Nanjing Normal University  System info (python version, jaxlib version, accelerator, etc.) (base) phylmfastroPrecision3260:~$ rocminfo ROCk module is loaded =====================     HSA System Attributes     =====================     Runtime Version:         1.1 System Timestamp Freq.:  1000.000000MHz Sig. Max Wait Duration:  18446744073709551615 (0xFFFFFFFFFFFFFFFF) (timestamp count) Machine Model:           LARGE                               System Endianness:       LITTLE                              Mwaitx:                  DISABLED DMAbuf Support:          YES ==========        )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",agent,Errors when building on AMD GPU," Description Hiï¼I am very interested in using STARRED (a python package designed for astronomical data processing with GPU support). However, it depends on jax and jaxlib. I followed the instructions from your documentation website and build with the following scripts: `python build/build.py enable_rocm rocm_path=/opt/rocm6.0.2   bazel_options=override_repository=xla=/home/phylmf/lib/xla ` I am sure the rocm is prepared well and I can use pytorch and stable diffusion with no troubles.  Anyway, the building process stopped and reports:  Anything from you will be appreciated! Thanks a lot in making jax/jaxlib available! Mingfeng Liu Nanjing Normal University  System info (python version, jaxlib version, accelerator, etc.) (base) phylmfastroPrecision3260:~$ rocminfo ROCk module is loaded =====================     HSA System Attributes     =====================     Runtime Version:         1.1 System Timestamp Freq.:  1000.000000MHz Sig. Max Wait Duration:  18446744073709551615 (0xFFFFFFFFFFFFFFFF) (timestamp count) Machine Model:           LARGE                               System Endianness:       LITTLE                              Mwaitx:                  DISABLED DMAbuf Support:          YES ==========        ",2024-02-27T02:24:06Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/19989,"Hi, currently JAX on ROCM is supported only for MI Instinct GPUs. We are working to get support for Navi/Radeon in the near future."
1471,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Conditional array update on GPU using jnp.where vs fori_loop)ï¼Œ å†…å®¹æ˜¯ (**Description:**  Hi!  I am fairly new to using JAX. I have been trying to update 1 or more entries of a 1D array based on some condition inside a `jax.jit` and `jnp.vectorize` function. I managed to find a very fast way of doing this on CPU, however when I tested on GPU, it suffered from extreme slow down (around x1000). For more context, the way which seems to be fastest on CPU is similar to this code,  To be fair, only the `update` function is relevant to this issue, but I would like to include some other part of the code to show the architecture of my overall function. Since GPUs are not good at conditional branching, I changed my code to look like this,  I noticed that using `update` function with a fori_loop is really fast compared to `jnp.where` counterpart on CPU by 45 times. I would think they perform the same, but that is not the case. I can understand the slow down due to `jax.lax.cond` on GPU, but I was wondering **if there is a better way of implementing what I am trying to do?** And also, why is `jnp.where` with 3 arguments is slower than setting new values to an array? My guess is creating the copy of array instead of inplace update, but I am not sure. Some other requirements for my application:  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Conditional array update on GPU using jnp.where vs fori_loop,"**Description:**  Hi!  I am fairly new to using JAX. I have been trying to update 1 or more entries of a 1D array based on some condition inside a `jax.jit` and `jnp.vectorize` function. I managed to find a very fast way of doing this on CPU, however when I tested on GPU, it suffered from extreme slow down (around x1000). For more context, the way which seems to be fastest on CPU is similar to this code,  To be fair, only the `update` function is relevant to this issue, but I would like to include some other part of the code to show the architecture of my overall function. Since GPUs are not good at conditional branching, I changed my code to look like this,  I noticed that using `update` function with a fori_loop is really fast compared to `jnp.where` counterpart on CPU by 45 times. I would think they perform the same, but that is not the case. I can understand the slow down due to `jax.lax.cond` on GPU, but I was wondering **if there is a better way of implementing what I am trying to do?** And also, why is `jnp.where` with 3 arguments is slower than setting new values to an array? My guess is creating the copy of array instead of inplace update, but I am not sure. Some other requirements for my application:  ",2024-02-26T03:57:02Z,enhancement,closed,0,6,https://github.com/jax-ml/jax/issues/19972,"Thanks for the question! I think the issue you're running into is related to the execution model of loops (`fori_loop`, `while_loop`, and `scan`) on GPU. For GPU backends, each iteration effectively requires a kernel launch, so if you have very cheap iterations it can lead to a lot of overhead. On CPU, there is no such overhead. On GPU I'd suggest doing this kind of conditional update using `lax.select` or `jnp.where`, which is basically a more flexible wrapper to `lax.select`.","Thank you very much for the reply! Yes, on GPU jnp.where works way better than the fori_loop that basically does nothing for most of the iterations (if I was able to use ""continue"", maybe that would work but trueFun and falseFun have to return same type of things). A related question to your reply. Does every fori_loop step use a different core of the same GPU? So, basically, if each loop iteration is cheap, it is more efficient to use jnp.vectorize? Until now, I thought the problem was due to jax.lax.cond(). Thanks for the headsup. For people having similar slowdown on GPU, here is a very simple code for comparison,   For reference, same code on CPU resultsin, "," > For GPU backends, each iteration effectively requires a kernel launch Is this a limitation of XLA or a fundamental limitation of the hardware itself? (I also asked this here.)",I don't know,"Hi , I have a question regarding the efficiency of `jax.lax.select` vs `jnp.where` on GPU. Could you please clarify which one is more efficient for GPU computations? Thanks.",`jnp.where` lowers to `jax.lax.select`; the only difference is that `jnp.where` will do NumPystyle implicit rank and dtype promotion. So they should be essentially identical in terms of computational efficiency.
489,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TPU backend gets stuck )ï¼Œ å†…å®¹æ˜¯ ( Description  I have been trying multiple attempts to run a simple JAX script, but it keeps stuck. I am using TPU v432, on ubuntu 22.04 and Python 3.10. **jax_test.py**   System info (python version, jaxlib version, accelerator, etc.) Same behaviour on **jax.print_environment_info()**.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,TPU backend gets stuck ," Description  I have been trying multiple attempts to run a simple JAX script, but it keeps stuck. I am using TPU v432, on ubuntu 22.04 and Python 3.10. **jax_test.py**   System info (python version, jaxlib version, accelerator, etc.) Same behaviour on **jax.print_environment_info()**.",2024-02-25T20:48:14Z,bug,open,0,7,https://github.com/jax-ml/jax/issues/19971,"After a decent amount of time, the following exception is thrown: ",I've encountered the same issue when using TPU v432.,"According to the warning, you should run your code on all TPU hosts. As TPU v432 has 4 hosts, you should run the code simultaneously on all 4 hosts. You can refer to usingtpupodusingtpupod)  for help.","I had the same issue. However, when I switched to this code  it worked. I think the key is to use jax.distributed.initialize()","I've encountered this warning on v332 as well, but after some waiting, script was executed without any errors. Seems like it just takes some time sometimes.","To configure TPU devices and run commands across hosts for TPU Pods, you can now use tpux!",Same problem here.
557,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Colab tpu initialization in XLA and JAX fails)ï¼Œ å†…å®¹æ˜¯ ( Description Getting `cloud_tpu_init failed` using the google colab tpu runtime and torch xla package The torch_xla is installed with the command:  The imports used in notebook are:  Which shows the following error:   System info (python version, jaxlib version, accelerator, etc.) Tensorflow shows the TPU:   JAX is printing this: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Colab tpu initialization in XLA and JAX fails," Description Getting `cloud_tpu_init failed` using the google colab tpu runtime and torch xla package The torch_xla is installed with the command:  The imports used in notebook are:  Which shows the following error:   System info (python version, jaxlib version, accelerator, etc.) Tensorflow shows the TPU:   JAX is printing this: ",2024-02-24T14:43:41Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/19963,"I note that: a) that error comes from `torch_xla`, not JAX, and b) JAX, as of v0.4, does not support colab TPUs. (We *do* support Kaggle TPUs, if you want notebook with TPUs in it). v0.3.25 is very old and we won't be able to help debug issues with it. c) even notwithstanding the above, TPUs currently cannot be shared between multiple frameworks at the same time. If you've opened a TPU with torch or TensorFlow, you won't be able to use it from JAX and vice versa. Hope that helps! (Closing, since there's nothing we can do here.)",Thanks for looking into it and for pointers to `torch_xla` and Kaggle TPUs.
1448,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([attrs] add linearize and vjp support)ï¼Œ å†…å®¹æ˜¯ (There are two commits here: * 67572d30949df295b5b09c20247517c5658a401b is a small tweak to the implementation of `attrs.jvp` to be simpler (IMO, though I'm not sure yet...) so that we handle input perturbations at the traceable level and the inner transformations never need to worry about them (e.g. we don't create separate attr input tracers) * 2ce8c57b4124c26e96301c036ca323001439b3d5 adds `attrs.linearize` and `attrs.vjp` The plan is for all of these features to be incorporated into the normal `jax.jvp`, `jax.linearize`, and `jax.vjp`, but we're keeping them separate for now while we play with them. The signatures generalize `jax.linearize` and `jax.vjp`, like:  We're currently pretty inconsistent between using lists like `list[tuple[Object, str]]` and `list[tuple[Object, str, Array]]` vs sets and dicts like `set[tuple[Object, str]]` and `dict[tuple[Object, str], Array]`. For the latter we require the mutable objects of interest to use `object.__hash__` / `object.__eq__`. We're also being inconsistent about whether tangent/cotangent result dicts represent zeros by missing entries, or symbolic zeros, or dense zeros. We'll make these things consistent at some point. These APIs are general but pretty lowlevel. A )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[attrs] add linearize and vjp support,"There are two commits here: * 67572d30949df295b5b09c20247517c5658a401b is a small tweak to the implementation of `attrs.jvp` to be simpler (IMO, though I'm not sure yet...) so that we handle input perturbations at the traceable level and the inner transformations never need to worry about them (e.g. we don't create separate attr input tracers) * 2ce8c57b4124c26e96301c036ca323001439b3d5 adds `attrs.linearize` and `attrs.vjp` The plan is for all of these features to be incorporated into the normal `jax.jvp`, `jax.linearize`, and `jax.vjp`, but we're keeping them separate for now while we play with them. The signatures generalize `jax.linearize` and `jax.vjp`, like:  We're currently pretty inconsistent between using lists like `list[tuple[Object, str]]` and `list[tuple[Object, str, Array]]` vs sets and dicts like `set[tuple[Object, str]]` and `dict[tuple[Object, str], Array]`. For the latter we require the mutable objects of interest to use `object.__hash__` / `object.__eq__`. We're also being inconsistent about whether tangent/cotangent result dicts represent zeros by missing entries, or symbolic zeros, or dense zeros. We'll make these things consistent at some point. These APIs are general but pretty lowlevel. A ",2024-02-24T00:12:11Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/19960
511,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Cleanup: access tree utilities via jax.tree.*)ï¼Œ å†…å®¹æ˜¯ (Going forward, we plan to encourage downstream users to use `jax.tree.xxx` rather than `jax.tree_util.tree_xxx`, so we should probably lead by example in our own code. Done in `tests/` only for now, because we don't necessarily want to depend on toplevel `jax` submodules in `jax/_src`.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Cleanup: access tree utilities via jax.tree.*,"Going forward, we plan to encourage downstream users to use `jax.tree.xxx` rather than `jax.tree_util.tree_xxx`, so we should probably lead by example in our own code. Done in `tests/` only for now, because we don't necessarily want to depend on toplevel `jax` submodules in `jax/_src`.",2024-02-23T19:34:59Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/19958
557,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Strange behavior of jax.experimental.jet)ï¼Œ å†…å®¹æ˜¯ ( Description I was running the following code:  And get the following error:  However, when I change the function definition to   everything works. I'm not sure what could cause something like this; I guess multiplying by 1 creates a new node in the computational graph?   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Strange behavior of jax.experimental.jet," Description I was running the following code:  And get the following error:  However, when I change the function definition to   everything works. I'm not sure what could cause something like this; I guess multiplying by 1 creates a new node in the computational graph?   System info (python version, jaxlib version, accelerator, etc.) ",2024-02-23T15:54:43Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/19949
307,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Include build information in compilation cache key.)ï¼Œ å†…å®¹æ˜¯ (Include build information in compilation cache key. Testing: test workloads.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Include build information in compilation cache key.,Include build information in compilation cache key. Testing: test workloads.,2024-02-23T00:55:21Z,,closed,0,1,https://github.com/jax-ml/jax/issues/19939,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
458,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Promote `isclose` arguments to inexact dtype (fixes #19935).)ï¼Œ å†…å®¹æ˜¯ (It turns out that CC(`jax.numpy.isclose` differs from `numpy.isclose`) is easily fixed by promoting to inexact dtypes. This ensures `np.isclose(6, 10, rtol=0.5)` and `jnp.isclose(6, 10, rtol=0.5)` give the same result.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Promote `isclose` arguments to inexact dtype (fixes #19935).,"It turns out that CC(`jax.numpy.isclose` differs from `numpy.isclose`) is easily fixed by promoting to inexact dtypes. This ensures `np.isclose(6, 10, rtol=0.5)` and `jnp.isclose(6, 10, rtol=0.5)` give the same result.",2024-02-22T22:29:16Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/19936,"This is causing some downstream test failures for `isclose` called on key types:  We should probably fall back to equality checks when the following is true `dtypes.issubdtype(dtype, dtypes.extended)`."
691,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`jax.numpy.isclose` differs from `numpy.isclose`)ï¼Œ å†…å®¹æ˜¯ ( Description The result of `jax.numpy.isclose` and `numpy.isclose` differ because the former treats integers differently than floats. Specifically for ""exact"" dtypes, jax demands equality rather than being ""close"". The relevant line is https://github.com/google/jax/blob/8d6bb0197b65f7aa0439087d4fc4e34ccb88c509/jax/_src/numpy/lax_numpy.pyL974  Casting to a float first gives the expected result.   System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,`jax.numpy.isclose` differs from `numpy.isclose`," Description The result of `jax.numpy.isclose` and `numpy.isclose` differ because the former treats integers differently than floats. Specifically for ""exact"" dtypes, jax demands equality rather than being ""close"". The relevant line is https://github.com/google/jax/blob/8d6bb0197b65f7aa0439087d4fc4e34ccb88c509/jax/_src/numpy/lax_numpy.pyL974  Casting to a float first gives the expected result.   System info (python version, jaxlib version, accelerator, etc.) ",2024-02-22T22:20:15Z,bug,closed,0,0,https://github.com/jax-ml/jax/issues/19935
1198,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`jax.make_array_from_async_callback`)ï¼Œ å†…å®¹æ˜¯ (`jax.make_array_from_callback` does a sequential loop over devices attached to this host: https://github.com/google/jax/blob/ef40b85c8b2686f64bc9ca67de267a6b1a7935bb/jax/_src/array.pyL693. When fetching from remote storage with high latency, this sequential loop can become latencylimited rather than throughputlimited on the network connection. In that case, it's typically a latency improvement to be able to issue the network requests for all devices in parallel.  As a user, I do that manually from `jax.make_array_from_single_device_arrays`, but that's a lower level API. Instead, the ideal would be a new function `jax.make_array_from_async_callback` that takes an async callback. Besides changing the type signature, the only change in implementation would be to add `await asyncio.gather(*...)` to https://github.com/google/jax/blob/ef40b85c8b2686f64bc9ca67de267a6b1a7935bb/jax/_src/array.pyL693, making it:  Then `jax.make_array_from_callback` could wrap the async version.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,`jax.make_array_from_async_callback`,"`jax.make_array_from_callback` does a sequential loop over devices attached to this host: https://github.com/google/jax/blob/ef40b85c8b2686f64bc9ca67de267a6b1a7935bb/jax/_src/array.pyL693. When fetching from remote storage with high latency, this sequential loop can become latencylimited rather than throughputlimited on the network connection. In that case, it's typically a latency improvement to be able to issue the network requests for all devices in parallel.  As a user, I do that manually from `jax.make_array_from_single_device_arrays`, but that's a lower level API. Instead, the ideal would be a new function `jax.make_array_from_async_callback` that takes an async callback. Besides changing the type signature, the only change in implementation would be to add `await asyncio.gather(*...)` to https://github.com/google/jax/blob/ef40b85c8b2686f64bc9ca67de267a6b1a7935bb/jax/_src/array.pyL693, making it:  Then `jax.make_array_from_callback` could wrap the async version.",2024-02-22T01:59:12Z,enhancement,open,0,3,https://github.com/jax-ml/jax/issues/19919,Is this what you are looking for? https://github.com/google/jax/blob/main/jax/experimental/array_serialization/serialization.pyL67L78,"You can write a wrapper like this for your code base too! Or if the above utility is helpful and what you were looking for, I can expose that.",This seems quite close to CC(Wrapping a slow Python function in an asynchronous DeviceArray)
1489,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Memory issue when randomly initializing large parameters, sharding cannot help)ï¼Œ å†…å®¹æ˜¯ ( Description Consider the following code snippet:  Here I'm trying to initialize a very large dense layer. Despite the layer weights requiring only 32Gb of RAM (and I'm running on 80Gb H100), this code will fail because jax will try to simultaneously allocate quite a few buffers for RNG keys so that the total memory consumption is 112 Gb!  Is this intended? Do we really need to store these buffers in memory simultaneously to initialize the layer? In any case, we can try to fix the problem by sharding the layer over the available devices (8x80Gb H100, comment line 66 and uncomment line 67 in the code above). Interestingly, while this change reduces the size of the parameter tensor as intended, rng buffers are still being allocated in full!  This seems to be a bug: why is jax trying to materliaze the full rng tensor on each shard if it's not needed in full there? Finally, if I use all zeros initialization (uncomment line 22 in the code above), the issue goes away. So, to summarize, I have the following questions: * Is it expected that jax will try to simultaneously allocate so many rng buffers for weight initialization? * Why does the rng buffer allocation not respect sharding? * Are there any workarounds I can us)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"Memory issue when randomly initializing large parameters, sharding cannot help"," Description Consider the following code snippet:  Here I'm trying to initialize a very large dense layer. Despite the layer weights requiring only 32Gb of RAM (and I'm running on 80Gb H100), this code will fail because jax will try to simultaneously allocate quite a few buffers for RNG keys so that the total memory consumption is 112 Gb!  Is this intended? Do we really need to store these buffers in memory simultaneously to initialize the layer? In any case, we can try to fix the problem by sharding the layer over the available devices (8x80Gb H100, comment line 66 and uncomment line 67 in the code above). Interestingly, while this change reduces the size of the parameter tensor as intended, rng buffers are still being allocated in full!  This seems to be a bug: why is jax trying to materliaze the full rng tensor on each shard if it's not needed in full there? Finally, if I use all zeros initialization (uncomment line 22 in the code above), the issue goes away. So, to summarize, I have the following questions: * Is it expected that jax will try to simultaneously allocate so many rng buffers for weight initialization? * Why does the rng buffer allocation not respect sharding? * Are there any workarounds I can us",2024-02-20T17:07:03Z,bug,open,5,1,https://github.com/jax-ml/jax/issues/19893,Can confirm that shardingbased solution works if using `jax_default_prng_impl=rbg`
581,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Traced<ShapedArray(float32[])>with<JVPTrace(level=2/1)>)ï¼Œ å†…å®¹æ˜¯ ( Description  when I am printing the shape of a jax array, it is printing for the first batch, and then showing this message, what is this message exactly saying.  System info (python version, jaxlib version, accelerator, etc.) python = 3.11 jaxlibversion = 0.4.23 accelarator = gpu jax:    0.4.23 jaxlib: 0.4.23 numpy:  1.26.3 python: 3.11.7  ++)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Traced<ShapedArray(float32[])>with<JVPTrace(level=2/1)>," Description  when I am printing the shape of a jax array, it is printing for the first batch, and then showing this message, what is this message exactly saying.  System info (python version, jaxlib version, accelerator, etc.) python = 3.11 jaxlibversion = 0.4.23 accelarator = gpu jax:    0.4.23 jaxlib: 0.4.23 numpy:  1.26.3 python: 3.11.7  ++",2024-02-20T10:32:25Z,bug needs info,closed,0,2,https://github.com/jax-ml/jax/issues/19887,"Hi  thanks for the question, and sorry for the unclear error message, but I think we'll need more information in order to help you. What you printed above looks like the normal `repr` of a traced object within an autodiff transformation; for example:  Can you paste the code you were running (a [minimal reproducible example]() if possible) and the full error traceback if applicable? Also, it helps to put code and tracebacks between triple tick marks ( ````) to format them as code. Thanks!",Closing due to lack of activity here â€“ feel free to comment here or open another issue if you're still running into problems!
415,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Jax metal failed to install)ï¼Œ å†…å®¹æ˜¯ ( Description Using the instructions on the pip website the jax_metal failed to install    System info (python version, jaxlib version, accelerator, etc.) Macbook Air M2 Macos Sonoma 14.3.1 (23D60) Python 3.10)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Jax metal failed to install," Description Using the instructions on the pip website the jax_metal failed to install    System info (python version, jaxlib version, accelerator, etc.) Macbook Air M2 Macos Sonoma 14.3.1 (23D60) Python 3.10",2024-02-20T09:15:46Z,bug Apple GPU (Metal) plugin,open,0,8,https://github.com/jax-ml/jax/issues/19886,"Based on the packages, it is  AMD GPU? Could you try a venv with  python=3.9?",Reproduces on my m2 mac. with both py 3.10.6 and 3.9.13,Tried jax==0.4.11 jaxlib==0.4.11 jaxmetal==0.0.4  same thing,Haven't been able to reproduce the issue. The below config shows an installation and verification result:  ProductName:		macOS ProductVersion:		14.4   ,"Right, i think i was able to figure it out  in my case it was due python being `i386` arch and not `arm64`. After switching arch and installing native python, it worked. ","> Right, i think i was able to figure it out  in my case it was due python being `i386` arch and not `arm64`. After switching arch and installing native python, it worked. I have just tried to install following the instructions in the apple website (https://developer.apple.com/metal/jax/) and it failed. Same error than everyone here in a M2. How did you switched your native python3?  I have just ran the following code:  and the print out is: "," you switch in you CLI with `arch` command, then you install python afresh (it will be a different python) and go with jax m install instruct from apple.", thanks for the tip. It worked for me!
1484,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jax-metal: Failed assertion...expected element type f32 but received si32)ï¼Œ å†…å®¹æ˜¯ ( Description I set up a venv for my project using `jaxmetal`, but hit the following assertion error when I ran my otherwise functioning code:  The error does not come with a stack trace. Disabling jit for the entire script avoided the issue, but likewise didn't help with stack tracing. On a hunch, I found that the issue was somehow related to a noop Flax module I have that looks like this:  As noted in the comment, changing the shape for the unused param does fix the error. However, the assertion is **not** raised when the module's apply function is called. After stepping through linebyline with a debugger, I've found that it's raised much later, during the teardown of my jit'd training step function. Here is a schematic example of my code organization. This code **does not** reproduce the error, but is intended to show where the assertion gets raised in relation to the module's apply function:  Anyway, I'm not sure if this is a bug or intended behavior for Metal. I have a fix, but I would like to understand why that parameter can't be a zerosized array in my project code when it does work in this example code. Or maybe it's not related to that module? But then, why does changing that module fix the error? It se)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jax-metal: Failed assertion...expected element type f32 but received si32," Description I set up a venv for my project using `jaxmetal`, but hit the following assertion error when I ran my otherwise functioning code:  The error does not come with a stack trace. Disabling jit for the entire script avoided the issue, but likewise didn't help with stack tracing. On a hunch, I found that the issue was somehow related to a noop Flax module I have that looks like this:  As noted in the comment, changing the shape for the unused param does fix the error. However, the assertion is **not** raised when the module's apply function is called. After stepping through linebyline with a debugger, I've found that it's raised much later, during the teardown of my jit'd training step function. Here is a schematic example of my code organization. This code **does not** reproduce the error, but is intended to show where the assertion gets raised in relation to the module's apply function:  Anyway, I'm not sure if this is a bug or intended behavior for Metal. I have a fix, but I would like to understand why that parameter can't be a zerosized array in my project code when it does work in this example code. Or maybe it's not related to that module? But then, why does changing that module fix the error? It se",2024-02-16T02:43:42Z,bug Apple GPU (Metal) plugin,open,0,2,https://github.com/jax-ml/jax/issues/19841,The issue is not  reproducible.   Do you still see the same problem with the latest OS 14.4 and jaxmetal 0.0.6? ,"I just updated to macOS 14.4 and jaxmetal 0.0.6, and the issue does still occur if I pass `0` or `(0,)` to my module initializer, but not if I pass `(1,)`. As I say, the above code is a schematic that does not reproduce the error, and I'm unfortunately not in a position at the moment to start trimming my full code base down to a minimal reproducible example. It'll probably be at least a month or so before I'll have that sort of time.  I understand if you want to close the issue since it's not reproducible. I just wanted to make sure this occurrence was at least documented in case someone else has a similar issue."
708,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Problem with Pallas in JAX 0.4.24)ï¼Œ å†…å®¹æ˜¯ ( Description I trying to setup an environment for jax 0.4.24 When I try to running a pallas kernel (like an example), I get an error: `LLVM ERROR: Trying to register different dialects for the same namespace: builtin` Triton kernels work well  System info (python version, jaxlib version, accelerator, etc.) Tax 0.4.24, jaxlib 0.4.24, triton on commit https://github.com/openai/triton/commits/9f816a7b984ce20ec684866f9c8bb5ffd49e2500, jaxtriton on commit 3666738dd9fef74f702a82a3fa068d7e2bc80e2a)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Problem with Pallas in JAX 0.4.24," Description I trying to setup an environment for jax 0.4.24 When I try to running a pallas kernel (like an example), I get an error: `LLVM ERROR: Trying to register different dialects for the same namespace: builtin` Triton kernels work well  System info (python version, jaxlib version, accelerator, etc.) Tax 0.4.24, jaxlib 0.4.24, triton on commit https://github.com/openai/triton/commits/9f816a7b984ce20ec684866f9c8bb5ffd49e2500, jaxtriton on commit 3666738dd9fef74f702a82a3fa068d7e2bc80e2a",2024-02-15T14:18:04Z,bug pallas,closed,3,4,https://github.com/jax-ml/jax/issues/19825,"That error should have been fixed in https://github.com/google/jax/commit/5e2e609a9b8da90989a593575ad83eb2e43109e8, so you would need a version including that. If it suits your workflow, you could try the containers distributed as `ghcr.io/nvidia/jax:pallasYYYYMMDD` (or `ghcr.io/nvidia/jax:pallas` for the latest one).",Got it! Thank you for the answer,"Tried the latest one (20240214) and got en error:  As I understand, it rises because of the commit: https://github.com/openxla/triton/commit/f2d49c8fd08cef4adbfddd6e428d93f98576653f The flag `DISABLE_MMA_V3` was renamed to `ENABLE_MMA_V3` without fixing it in https://github.com/openxla/triton/blob/f2d49c8fd08cef4adbfddd6e428d93f98576653f/include/triton/Tools/Sys/GetEnv.hppL32L35",jaxlib 0.4.25 no longer has this issue. Closing as fixed.
509,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Incompatible CUDA and ptxas CUDA version)ï¼Œ å†…å®¹æ˜¯ ( Description Hey,  I installed the cuda version of jax for my cuda 12.2 using:  but I get this warning when trying to run the code:  It disables parallel execution and hence my code takes more time. What can I do to fix it?  System info (python version, jaxlib version, accelerator, etc.) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Incompatible CUDA and ptxas CUDA version," Description Hey,  I installed the cuda version of jax for my cuda 12.2 using:  but I get this warning when trying to run the code:  It disables parallel execution and hence my code takes more time. What can I do to fix it?  System info (python version, jaxlib version, accelerator, etc.) ",2024-02-15T08:52:11Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/19821,"I think the error message is selfexplanatory: ""You should update your NVIDIA driver or use the NVIDIAprovided CUDA forward compatibility packages."" However, if that's hard to do, don't worry about it, it just makes `jit` compilation slightly slower. Hope that helps!"
589,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(AttributeError message on jax.random.KeyArray)ï¼Œ å†…å®¹æ˜¯ ( Description Hello, I am working with a student in order to install a package that with jax dependencies. Our current version is jax 0.4.24 There is an AttributeError thrown for a call to jax.random.KeyArray. The full error message is below.   System info (python version, jaxlib version, accelerator, etc.) Python 3.9.18  Jax environment info   Conda environment )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,AttributeError message on jax.random.KeyArray," Description Hello, I am working with a student in order to install a package that with jax dependencies. Our current version is jax 0.4.24 There is an AttributeError thrown for a call to jax.random.KeyArray. The full error message is below.   System info (python version, jaxlib version, accelerator, etc.) Python 3.9.18  Jax environment info   Conda environment ",2024-02-14T19:48:00Z,question,closed,0,2,https://github.com/jax-ml/jax/issues/19812,"Hi  thanks for the question. `jax.random.KeyArray` was deprecated in jax v0.4.16, and removed in jax v0.4.24. It looks like the import is coming from `chex`, in which case your best course of action is probably to update `chex`. If that's not possible, you could install JAX version 0.4.23 or older. You can find information at those CHANGELOG links regarding how to replace other uses of the symbol. Please let me know if you have any further questions!",It seems like this is resolved  please let us know if you still have any questions about this!
641,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(sin/cos results are incorrect on pure imaginary inputs with large absolute value)ï¼Œ å†…å®¹æ˜¯ ( Description As reported in MPMath vs Jax sin, CPU, there exists regions in complex plane where `jax.numpy.sin` returns incorrect results. For example:  The expected result is `infj`. Also, MPMath vs Jax cos, CPU reports a similar issue in `jax.numpy.cos`:  The expected result is `inf+0j`  What jax/jaxlib version are you using? 0.4.23  Which accelerator(s) are you using? CPU/GPU)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,sin/cos results are incorrect on pure imaginary inputs with large absolute value," Description As reported in MPMath vs Jax sin, CPU, there exists regions in complex plane where `jax.numpy.sin` returns incorrect results. For example:  The expected result is `infj`. Also, MPMath vs Jax cos, CPU reports a similar issue in `jax.numpy.cos`:  The expected result is `inf+0j`  What jax/jaxlib version are you using? 0.4.23  Which accelerator(s) are you using? CPU/GPU",2024-02-12T14:28:16Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/19754,Fixed via CC(Fix complex sin and cos on inputs with small absolute value or large pure imaginary part) 
663,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Square on large complex(64) inputs return incorrect results)ï¼Œ å†…å®¹æ˜¯ ( Description As reported in MPMath vs Jax square, CPU and MPMath vs JAX square, CUDA, there exists regions in complex plane where `jax.numpy.square` returns incorrect results. For example:  The expected result is `inf  infj` as suggested by:  or  where the correct result is `infj`. Other samples problematic to jax square include:   What jax/jaxlib version are you using? 0.4.23  Which accelerator(s) are you using? CPU/GPU)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Square on large complex(64) inputs return incorrect results," Description As reported in MPMath vs Jax square, CPU and MPMath vs JAX square, CUDA, there exists regions in complex plane where `jax.numpy.square` returns incorrect results. For example:  The expected result is `inf  infj` as suggested by:  or  where the correct result is `infj`. Other samples problematic to jax square include:   What jax/jaxlib version are you using? 0.4.23  Which accelerator(s) are you using? CPU/GPU",2024-02-12T12:24:00Z,bug,closed,1,1,https://github.com/jax-ml/jax/issues/19751,This issue is fixed via https://github.com/jaxml/jax/pull/24874 . The current state of JAX `square` accuracy is summarized in https://github.com/pearu/functional_algorithms/issues/52.
1072,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Core-dump after upgrade)ï¼Œ å†…å®¹æ˜¯ ( Description Hi, I recently upgraded jax in my environment to v0.4.24, using CUDA 12.3 from outside the venv. After that, some operations started to yield strange core dumps like the following:  After the core dump, the Python REPL exited with SIGABRT. My system has ~800GB available disk space and around ~25GB available RAM, but since the error message is not about failing to write a file, I assume insufficient space not to be the cause of the issue. To investigate the issue, I cleared my environment to include nothing but the jax installation:  I would be grateful for any advice on what could have gone wrong. Thank you! Johannes  What jax/jaxlib version are you using? jax v0.4.24, jaxlib v0.4.24  Which accelerator(s) are you using? GPU  Additional system info? numpy v1.26.4, Python 3.12.1 installed with conda v23.10.0 on Arch Linux x86_64  NVIDIA GPU info )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Core-dump after upgrade," Description Hi, I recently upgraded jax in my environment to v0.4.24, using CUDA 12.3 from outside the venv. After that, some operations started to yield strange core dumps like the following:  After the core dump, the Python REPL exited with SIGABRT. My system has ~800GB available disk space and around ~25GB available RAM, but since the error message is not about failing to write a file, I assume insufficient space not to be the cause of the issue. To investigate the issue, I cleared my environment to include nothing but the jax installation:  I would be grateful for any advice on what could have gone wrong. Thank you! Johannes  What jax/jaxlib version are you using? jax v0.4.24, jaxlib v0.4.24  Which accelerator(s) are you using? GPU  Additional system info? numpy v1.26.4, Python 3.12.1 installed with conda v23.10.0 on Arch Linux x86_64  NVIDIA GPU info ",2024-02-09T12:13:07Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/19730,"This only happens when I use CUDA/cudnn from outside the venv. When using a pip install of jax, the error magically disappears.","This is a slightly less than graceful failure from XLA when it finds a bad ptxas version. The action item for us here would be to fail more gracefully (with a Python error, rather than a fatal crash). This fix on your side is to do as you say: you should use jax inside the virtualenv in which you installed it, because that way JAX will find a working copy of ptxas from the virtualenv (it's inside the `nvidianvcccu12` package, most likely).","Since I am using a Linux version with a rolling release update model, I believe the issue to be related to a version mismatch due to the recency of the packages, which fits your explanation. However, the core dumps vanished after updating my system (and rebooting). Thank you for your help!"
1242,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Issue with static fields in `in_axes` pytree structure in `jax.vmap`)ï¼Œ å†…å®¹æ˜¯ ( Description Hi, I am encountering an issue with the `in_axes` parameter when using `jax.vmap`. Specifically, I am passing `in_axes` as a pytree that mirrors the structure of the arguments for the function I am applying `vmap` to. However, due to certain applicationspecific requirements, the static fields (i.e., data elements that are bypassed by `tree_map`) within both pytrees do not match. This discrepancy leads to an error indicating a mismatch in the structures of the two pytrees, despite their relevant structures being identical. The error is resolved when I ensure that all static fields are identical across both pytrees. It appears that the root cause of this issue might be related to the `treedefs.node_data()` for `in_axes` and the corresponding values not matching. Below a MWE:  Results in:   What jax/jaxlib version are you using? jax v0.4.23, jaxlib v0.4.23+cuda12.cudnn89  Which accelerator(s) are you using? CPU/GPU  Additional system info? Ubuntu 20.04  NVIDIA GPU info )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Issue with static fields in `in_axes` pytree structure in `jax.vmap`," Description Hi, I am encountering an issue with the `in_axes` parameter when using `jax.vmap`. Specifically, I am passing `in_axes` as a pytree that mirrors the structure of the arguments for the function I am applying `vmap` to. However, due to certain applicationspecific requirements, the static fields (i.e., data elements that are bypassed by `tree_map`) within both pytrees do not match. This discrepancy leads to an error indicating a mismatch in the structures of the two pytrees, despite their relevant structures being identical. The error is resolved when I ensure that all static fields are identical across both pytrees. It appears that the root cause of this issue might be related to the `treedefs.node_data()` for `in_axes` and the corresponding values not matching. Below a MWE:  Results in:   What jax/jaxlib version are you using? jax v0.4.23, jaxlib v0.4.23+cuda12.cudnn89  Which accelerator(s) are you using? CPU/GPU  Additional system info? Ubuntu 20.04  NVIDIA GPU info ",2024-02-09T10:47:04Z,bug,open,0,4,https://github.com/jax-ml/jax/issues/19729,"Hi  this is moreorless working as expected, because pytree equivalence includes equivalence of static elements. Can you say more about why it's not possible to specify `in_axes` with an equivalent pytree to the input?","In our application, the construction of the `in_axes` and `val` pytrees occurs at distinct stages due to the nature of the codebase. Initially, when the `in_axes` pytree is assembled, some static data necessary for its completion is not yet available, leading us to fill these fields with `None` as placeholders. Only at a later stage, when we construct the `val` pytree, do we have access to all the requisite static data, allowing us to populate it accordingly. This sequential process results in a mismatch between the static elements of the `in_axes` and `val` pytrees. Adding to the complexity, the structure of the `in_axes` pytree does not mirror the `val` pytree exactly. Specifically, the `in_axes` pytree includes `None` leaves in places where the `val` pytree contains pytree nodes. This setup is designed so  that the `None` values in `in_axes` correspond to all leaves under those nodes in the `val` pytree. I did not anticipate the inclusion of static data as a factor for pytree equivalence, as my expectation was that this only concerned the ""nonstatic part"". Moreover, this equivalence is not strictly enforced, since `None` values for leafs where the `val` pytree has a deeper pytree structure is generally allowed. This expectation was partly due to my interpretation of the documentation on the `in_axes` parameter, which did not explicitly state the necessity for identical static data.","I don't entirely follow â€“ if `None` can be used in place of full subtrees, how could you possibly expect a generic `in_axis` to match a runtime pytree that may have more leaves than the specification? Overall, my recommendation would be to specify `in_axes` at a point where you actually have the data, and therefore know what the `in_axes` specification should be.","Using `None` or any `int` axis specification as leaves in `in_axes` where `val` contains full subtrees appears to function correctly (refer to the provided minimal working example). Looking at the internals, it seems this functionality is supported through this line, where the (partially incomplete) `in_axes` pytree is extended with the missing subtrees from `val`. This is also where the static data error comes from. This working is also discussed here.  Initially, it was not apparent that `in_axes` and `val` pytrees should mirror each other structurally, especially since ""prefix"" pytrees can define the `in_axes` parameter for whole subtrees. The error message, lacking details about the necessity for static data alignment, were confusing, leading to some time spent troubleshooting. While I still don't really understand why the static data of `in_axes` should exactly match that of `val`, a more informative error message that states this would already be helpful! In the example below I replace `in_axes.bar=None` such that `in_axes` has a None value in a place with `val` has a subtree. Internally, this line extends the None value to all leafs of the subtree. "
826,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Sharp edges around various collective ops, e.g. `axis_name=()` and negative indices)ï¼Œ å†…å®¹æ˜¯ ( Description Various semantics are a bit confusing. What should happen with degenerate axis names, e.g. `axis_name=()`? Right now behavior is inconsistent.  produces  How should we deal with negative indices? It works for `all_gather` but not `psum_scatter` or `all_to_all`.  produces  Obviously, I can add `if axis_name:` around my code or do `negative_index % len(x.shape)` but it's not ideal.  What jax/jaxlib version are you using? 0.4.24  Which accelerator(s) are you using? _No response_  Additional system info? _No response_  NVIDIA GPU info _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"Sharp edges around various collective ops, e.g. `axis_name=()` and negative indices"," Description Various semantics are a bit confusing. What should happen with degenerate axis names, e.g. `axis_name=()`? Right now behavior is inconsistent.  produces  How should we deal with negative indices? It works for `all_gather` but not `psum_scatter` or `all_to_all`.  produces  Obviously, I can add `if axis_name:` around my code or do `negative_index % len(x.shape)` but it's not ideal.  What jax/jaxlib version are you using? 0.4.24  Which accelerator(s) are you using? _No response_  Additional system info? _No response_  NVIDIA GPU info _No response_",2024-02-08T20:58:46Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/19720
716,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(""E1130: bad operand type for unary -: ArrayImpl (invalid-unary-operand-type)"" pylint errors in latest release (0.4.24))ï¼Œ å†…å®¹æ˜¯ ( Description Running pylint (`pylint test.py`) on this simple file  fails with the latest release of jax (0.4.24) :  The same code doesn't raise any error with jax 0.4.23. This has led to some tests failures in other packages such as optax  What jax/jaxlib version are you using? 0.4.24 0.4.24  Which accelerator(s) are you using? CPU  Additional system info? Tried on both linux and mac  NVIDIA GPU info _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"""E1130: bad operand type for unary -: ArrayImpl (invalid-unary-operand-type)"" pylint errors in latest release (0.4.24)", Description Running pylint (`pylint test.py`) on this simple file  fails with the latest release of jax (0.4.24) :  The same code doesn't raise any error with jax 0.4.23. This has led to some tests failures in other packages such as optax  What jax/jaxlib version are you using? 0.4.24 0.4.24  Which accelerator(s) are you using? CPU  Additional system info? Tried on both linux and mac  NVIDIA GPU info _No response_,2024-02-08T11:42:40Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/19713,"Hi  thanks for the report. Do you understand where pytype is inferring that `ones` returns `ArrayImpl`? The pyi file has not changed, and declares that `ones` returns `Array` unconditionally: https://github.com/google/jax/blob/4c505f8bac45517970149b13343988090fdaa920/jax/numpy/__init__.pyiL610L611 I just doublechecked that the pyi file is included in the v0.4.24 distribution, so I'm not sure why pytype would be ignoring it. Do you have any ideas?","Thanks Jake for the prompt reply. No, I don't understand why pylint is doing that inference. A couple of more observations: * changing `jnp.ones([])` to `jnp.negative(jnp.ones([]))` works fine (does not raise the error) * I don't think the error is specific to `jnp.ones`. In particular, `jnp.full([], 2.)` has the same issue","Chatting with , it seems this is coming from `pylint`, not `pytype`. `pylint` has some level of type checking builtin, but it seems that it ignores `pyi` files and thus will not work correctly with dynamicallydefined attributes. Here's a repro:  So in summary, this looks like a bug in `pylint` â€“ it's trying to do type checking without taking into account the interface files that we've defined for the sake of type checking. What do you think?",Thanks  for reporting this upstream! I'll close the issue here since it does seem more of an issue in pylint
400,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([JAX] Add an option subset_by_index that allows computing a contiguous subset of singular components from svd.)ï¼Œ å†…å®¹æ˜¯ ([JAX] Add an option subset_by_index that allows computing a contiguous subset of singular components from svd.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[JAX] Add an option subset_by_index that allows computing a contiguous subset of singular components from svd.,[JAX] Add an option subset_by_index that allows computing a contiguous subset of singular components from svd.,2024-02-08T00:28:51Z,,closed,0,1,https://github.com/jax-ml/jax/issues/19707,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
1473,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(shard_map *much* faster than pjit for simple data parallelism )ï¼Œ å†…å®¹æ˜¯ ( Description I'm trying to scale up some transformer training (currently at ~400m params), and as such I've been playing around with various ways to save memory and improve performance. On a whim, I tried replacing my `jax.jit(in_shardings=..., out_shardings=...)` setup for data parallelism with `jax.experimental.shard_map`, as so:  and I immediately saw a 2.8x (!) speedup. The reason why this is a problem is because I would like to move on to more advanced parallelism techniques (tensor parallel, fullysharded data parallel, etc) but it seems like it would be prohibitively difficult to write these manually using `shard_map`. However, if I continue using pjit's automatic partitioning, I worry that I'm leaving a bunch of performance on the table. I would think the automatic partitioner would be able to produce code with more or less equal performance in this very simple case. Here are the debugging steps I've tried so far:  I used `jax.debug.inspect_array_sharding` to look at the sharding of intermediate activations, and they all looked correct (fully sharded along the DP axis)   I added a bunch of sharding annotations anyway just to to make sure  I used `custom_vjp` to look at shardings during the backward pass, )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,shard_map *much* faster than pjit for simple data parallelism ," Description I'm trying to scale up some transformer training (currently at ~400m params), and as such I've been playing around with various ways to save memory and improve performance. On a whim, I tried replacing my `jax.jit(in_shardings=..., out_shardings=...)` setup for data parallelism with `jax.experimental.shard_map`, as so:  and I immediately saw a 2.8x (!) speedup. The reason why this is a problem is because I would like to move on to more advanced parallelism techniques (tensor parallel, fullysharded data parallel, etc) but it seems like it would be prohibitively difficult to write these manually using `shard_map`. However, if I continue using pjit's automatic partitioning, I worry that I'm leaving a bunch of performance on the table. I would think the automatic partitioner would be able to produce code with more or less equal performance in this very simple case. Here are the debugging steps I've tried so far:  I used `jax.debug.inspect_array_sharding` to look at the sharding of intermediate activations, and they all looked correct (fully sharded along the DP axis)   I added a bunch of sharding annotations anyway just to to make sure  I used `custom_vjp` to look at shardings during the backward pass, ",2024-02-04T22:51:42Z,bug,closed,0,12,https://github.com/jax-ml/jax/issues/19657,"Hi  I'm having trouble understanding your question. It sounds like you're comparing two implementations, but you've only shown us one implementation. Could you edit your question to show the code for both approaches?"," sorry, I've edited my question to hopefully make things more clear. The only difference between the two implementations is the addition of the 8 lines indicated (the shard_map itself and the corresponding pmean). ","Thanks! Assigning to , who might have some insights here.",Don't you need to `jnp.mean` for the jit version (without shard_map)?, The `jnp.mean` happens inside the loss function (a scalar is returned).,I don't see that loss function :) Can you create a minimal reproducer that we can run?,"Sure thing, here's my repro. With the shard_map version, I get 1.09 s/it, and with no shard_map, I get 2.95 s/it. This is on a v48 TPU VM.","Hey  sorry for the late reply, can you try with the latest jax and jaxlib version? (or better try with nightly) Also can you tell me what TPU you were using? TPUv4 but how many devices? ","This was a v48 VM (smallest you can get, I think). I no longer have easy access to TPUs, but I replicated the issue with `jax[cuda12]==0.4.33` on an 8xH100 DGX machine. With no shard map, I get `1.10s/it`, and with shard map, I get `1.70it/s`.","The culprit ended up being dropout, and the unpartitionable threefry algorithm (ref). Either removing dropout or setting `jax.config.update(""jax_threefry_partitionable"", True)` speeds up the pjit version to match the shard_map version.","Oof, sorry that caused so much pain. We've been meaning to switch it on by default, but haven't landed it since it requires updating a bunch of downstream google monorepo tests. ","No worries, looking forward for that to land! (I totally understand, because we've also had tests fail several times thanks to inconsistent RNG algorithms ğŸ˜… ). Luckily this did not actually affect our internal training runs, since we *were* using `jax_threefry_partitionable`  I was mainly concerned about this issue because I thought it was revealing some much larger misunderstanding of pjit/shard_map on my part, so it's actually a relief that it was something unrelated."
1325,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(CUDA driver not found after installing with cuda12_local wheel)ï¼Œ å†…å®¹æ˜¯ ( Description  The above output was encountered after installing JAX on WSL2 (Ubuntu 22.04) with the cuda12_local wheel.   Specifically, the command to install JAX was:   `pip3 install upgrade ""jax[cuda12_local]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` Prior to running the pip install command, I have installed the CUDA 12.3 and CuDNN 8.9.   CUDA was installed the standard WSL way (with driver on host Windows machine and rest of CUDA in the Linux system).   From the JAX installation page, I gather that the wheel installed is for CUDA 12.2 and CuDNN 8.9, but CUDA 12.3 should also work due to having the same major version and a newer minor version. I have checked that `/usr/local/cuda12.3/bin` is in PATH, and that `/usr/local/cuda12.3/lib64` is in LD_LIBRARY_PATH. Is this combination of CUDA, CuDNN, and JAX supported? Please advise on the recommended way to run JAX with GPU on WSL2.   Thanks!  What jax/jaxlib version are you using? 0.4.23, 0.4.23  Which accelerator(s) are you using? GPU  Additional system info? _No response_  NVIDIA GPU info )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,CUDA driver not found after installing with cuda12_local wheel," Description  The above output was encountered after installing JAX on WSL2 (Ubuntu 22.04) with the cuda12_local wheel.   Specifically, the command to install JAX was:   `pip3 install upgrade ""jax[cuda12_local]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` Prior to running the pip install command, I have installed the CUDA 12.3 and CuDNN 8.9.   CUDA was installed the standard WSL way (with driver on host Windows machine and rest of CUDA in the Linux system).   From the JAX installation page, I gather that the wheel installed is for CUDA 12.2 and CuDNN 8.9, but CUDA 12.3 should also work due to having the same major version and a newer minor version. I have checked that `/usr/local/cuda12.3/bin` is in PATH, and that `/usr/local/cuda12.3/lib64` is in LD_LIBRARY_PATH. Is this combination of CUDA, CuDNN, and JAX supported? Please advise on the recommended way to run JAX with GPU on WSL2.   Thanks!  What jax/jaxlib version are you using? 0.4.23, 0.4.23  Which accelerator(s) are you using? GPU  Additional system info? _No response_  NVIDIA GPU info ",2024-02-04T21:37:38Z,bug,open,1,2,https://github.com/jax-ml/jax/issues/19655,similiar issue with 12.5.  Error I got says  jax 0.4.28 does not provide the extra 'cuda12local',"Hi Zhao,  I have tested the issue with latest JAX cuda local version 0.4.33 on WSL with NVIDIA RTX A5000 GPU. I could not reproduce the issue. The `xla_bridge` was imported successfully. Please find the attached screeshot for reference: !image !image Could you please verify with latest JAX cuda local version, if the issue still exists? Thank you."
1291,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(.at[].set() is dangerous)ï¼Œ å†…å®¹æ˜¯ ( Description The inplace modification of arrays in jax is dangerous, and behavior of \.at[] under  breaks the outofplace (functional) paradigm. The documentation page carries a warning:  However, the different behavior under jit and nonjit may cause difficult to debug data issues.... Case in point  given a nontrivial permutation `idx`, this simple function will return a corrupted version of X (in my case there were nan's, luckily)... I wonder what other nondeterministic behavior usage of .at[].set() might be enabling in jax. I understand that there is a desire for performance optimization, however correctness should take precedence. Perhaps a warning could be printed if assigning an array to itself with permutation indexing.  What jax/jaxlib version are you using? 0.4.24.dev20240122+b512b576a (jax), 0.4.24.dev20240122 (jaxlib)  Which accelerator(s) are you using? GPU, ROCm  Additional system info? Linux login2 5.14.21150400.24.46_12.0.83cray_shasta_c CC(Python 3 compatibility issues) SMP Tue May 23 03:16:47 UTC 2023 (c6cda89) x86_64 x86_64 x86_64 GNU/Linux  AMD GPU info )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,.at[].set() is dangerous," Description The inplace modification of arrays in jax is dangerous, and behavior of \.at[] under  breaks the outofplace (functional) paradigm. The documentation page carries a warning:  However, the different behavior under jit and nonjit may cause difficult to debug data issues.... Case in point  given a nontrivial permutation `idx`, this simple function will return a corrupted version of X (in my case there were nan's, luckily)... I wonder what other nondeterministic behavior usage of .at[].set() might be enabling in jax. I understand that there is a desire for performance optimization, however correctness should take precedence. Perhaps a warning could be printed if assigning an array to itself with permutation indexing.  What jax/jaxlib version are you using? 0.4.24.dev20240122+b512b576a (jax), 0.4.24.dev20240122 (jaxlib)  Which accelerator(s) are you using? GPU, ROCm  Additional system info? Linux login2 5.14.21150400.24.46_12.0.83cray_shasta_c CC(Python 3 compatibility issues) SMP Tue May 23 03:16:47 UTC 2023 (c6cda89) x86_64 x86_64 x86_64 GNU/Linux  AMD GPU info ",2024-02-03T00:53:21Z,bug,open,1,6,https://github.com/jax-ml/jax/issues/19643,Update: I observed this in automatic SPMD (multi rank) calculations on sharded jax.Array's... but still trying to pin down the source of the data corruption (can't seem to be able to reproduce on single GPU),"Just to try and make what's going on here a bit clearer to me. First of all, doing `X.at[idx].set(...)`, where `idx` has repeated indices, is indeed undefined behaviour in JAX (regardless of what is being set). This is what the existing warning is about / is known / is considered fine. Are you saying that you've found a case in which `idx` does *not* have repeated indices, but for which specifically the pattern `X.at[idx].set(X)` produces the wrong result? If so, that's a bug.","> Just to try and make what's going on here a bit clearer to me. >  > First of all, doing `X.at[idx].set(...)`, where `idx` has repeated indices, is indeed undefined behaviour in JAX (regardless of what is being set). This is what the existing warning is about / is known / is considered fine. >  > Are you saying that you've found a case in which `idx` does _not_ have repeated indices, but for which specifically the pattern `X.at[idx].set(X)` produces the wrong result? If so, that's a bug. No, no ... I was just under the assumption that jax would do out of place transforms all the time and was surprised that it does inplace, but only under certain conditions ... (namely, jit)... so the sometimes inplace, sometimes out of place behavior seems confusing","Right. So logically, `.at[].set()` always operates outofplace. But physically, the XLA compiler (the backend to JAX) will attempt to do `.at[].set()` inplace where possible. Doing this is important for performance, to avoid lots of copies of the underlying data. You'll see similar behaviour in the compilers for almost all functional languages. Whether something happens inplace or outofplace shouldn't be observable from a user perspective, though. (Other than indirectly, through things like computation time or memory usage.) How is it that you think you're observing this distinction?"," Here's a code sample to elaborate on what Patrick is saying:  Basically it's just that JAX will avoid doing inplace operations on transformed function _arguments_ unless explicitly given permission to. (This includes constants which your function closes over). The inplace behavior will ideally happen for intermediate tensors which you don't have direct visibility into anyways, and you can optin to your input arguments getting clobbered with `donate_argnums`.","Hi  thanks for the report â€“ I recall in the past XLA fixed a similar bug, where something like `x.at[idx].set(x)` would return the wrong result under JIT, but I can't seem to find the previous issue. I tried briefly to reproduce this, but wasn't able to on a CPU or GPU runtime. Would you be able to put together a minimal example of inputs for which you're seeing this problematic behavior?"
530,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Import submodules from jax._src explicitly, instead of relying on import side-effects. It will lead to the missing x-refs in code search according to go/pywald-sawmill-analysis.)ï¼Œ å†…å®¹æ˜¯ (Import submodules from jax._src explicitly, instead of relying on import sideeffects. It will lead to the missing xrefs in code search according to go/pywaldsawmillanalysis.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"Import submodules from jax._src explicitly, instead of relying on import side-effects. It will lead to the missing x-refs in code search according to go/pywald-sawmill-analysis.","Import submodules from jax._src explicitly, instead of relying on import sideeffects. It will lead to the missing xrefs in code search according to go/pywaldsawmillanalysis.",2024-02-03T00:28:01Z,,closed,0,1,https://github.com/jax-ml/jax/issues/19642,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
319,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(More type annotations of Pallas primitives)ï¼Œ å†…å®¹æ˜¯ (I also noticed annoying errors on multiple_of so figured I'd annotate the rest of the primitives.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,More type annotations of Pallas primitives,I also noticed annoying errors on multiple_of so figured I'd annotate the rest of the primitives.,2024-02-01T00:50:22Z,pull ready,open,0,4,https://github.com/jax-ml/jax/issues/19608,Hi  can you squash your changes into a single commit? See https://jax.readthedocs.io/en/latest/contributing.htmlsinglechangecommitsandpullrequests. Thanks!,"Done, was being lazy and doing things from the Github UI. I squashed. I was just rebasing to get around the CI flake.",Thanks!,"This is leading to some pytype errors because `k_offset` below is annotated as an int, but is being passed the output of `program_id`: https://github.com/google/jax/blob/6e17bb3aa5aa9c612fd3942c0ae48c70cecba801/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_kernel.pyL585 I suspect the best fix there is to change this annotation to `int | Array`."
634,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([shmap] Support multiple axes for standard collectives in shard_map)ï¼Œ å†…å®¹æ˜¯ (Added test coverage and have tested several nontrivial use cases like collective matrix multiplication. Note that I don't actually see the `all_gather_invariant` anywhere in the code that is mentioned here: https://jax.readthedocs.io/en/latest/jep/17111shmaptranspose.htmltrackingdeviceinvarianceinavalsakaavalswithnamesrevived. Fixes CC([shmap] shard_map doesn't support multiple axes).)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[shmap] Support multiple axes for standard collectives in shard_map,Added test coverage and have tested several nontrivial use cases like collective matrix multiplication. Note that I don't actually see the `all_gather_invariant` anywhere in the code that is mentioned here: https://jax.readthedocs.io/en/latest/jep/17111shmaptranspose.htmltrackingdeviceinvarianceinavalsakaavalswithnamesrevived. Fixes CC([shmap] shard_map doesn't support multiple axes).,2024-01-31T17:14:46Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/19601
1365,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(gRPC error during multiprocess on Google Compute Engine instance)ï¼Œ å†…å®¹æ˜¯ ( Description I am trying to get a simple multiprocess script working on a Google Cloud instance with four T4s. I've gotten multiprocess working elsewhere, but something seems to be broken or incompatible with the GCE infra. Here is my test script:  It crashes during the final step due to the `psum`. (Removing the `psum` there is no crash.) This is the error message:  I've done what I can to debug this but no luck. I've tried multiple installs of mpirun, NCCL, both localcuda & pipcuda JAX installers, and adjusting firewall settings. I'm confused why the coordination step (during `.initialize()`) would work, only to fail later during the `.psum`.  I've also run the `nccltest` suite and that all seems to work fine.  What jax/jaxlib version are you using? jax v0.4.23, jaxlib v0.4.23  Which accelerator(s) are you using? GPU  Additional system info? I've tried both Python3.10 & 3.11. 1.26.3 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] uname_result(system='Linux', node='gpudev3', release='6.2.01019gcp', version=' CC(Typo)~22.04.1Ubuntu SMP Thu Nov 16 18:18:34 UTC 2023', machine='x86_64')  NVIDIA GPU info )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,gRPC error during multiprocess on Google Compute Engine instance," Description I am trying to get a simple multiprocess script working on a Google Cloud instance with four T4s. I've gotten multiprocess working elsewhere, but something seems to be broken or incompatible with the GCE infra. Here is my test script:  It crashes during the final step due to the `psum`. (Removing the `psum` there is no crash.) This is the error message:  I've done what I can to debug this but no luck. I've tried multiple installs of mpirun, NCCL, both localcuda & pipcuda JAX installers, and adjusting firewall settings. I'm confused why the coordination step (during `.initialize()`) would work, only to fail later during the `.psum`.  I've also run the `nccltest` suite and that all seems to work fine.  What jax/jaxlib version are you using? jax v0.4.23, jaxlib v0.4.23  Which accelerator(s) are you using? GPU  Additional system info? I've tried both Python3.10 & 3.11. 1.26.3 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] uname_result(system='Linux', node='gpudev3', release='6.2.01019gcp', version=' CC(Typo)~22.04.1Ubuntu SMP Thu Nov 16 18:18:34 UTC 2023', machine='x86_64')  NVIDIA GPU info ",2024-01-31T10:28:11Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/19596
1078,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Getting loss as `nan` with jit, but no `nan`s without jit)ï¼Œ å†…å®¹æ˜¯ ( Description Using keras 3 with JAX backend.  jit_compile=True * After few epochs of training properly, I started getting the loss as `nan`. (Trains properly without jit) * So I enabled `jax.config.update(""jax_debug_nans"", True)` and got the error after training a few epochs properly   jit_compile=False * The model runs perfectly fine and converges with no `nan`s (according to the error I think this is an Error from JAX and not Keras, so I posted the error here...)  What jax/jaxlib version are you using? 0.4.23 0.4.23  Which accelerator(s) are you using? GPU  Additional system info? > 1.26.2 3.11.7 (main, Dec  8 2023, 18:56:58) [GCC 11.4.0] uname_result(system='Linux', node='Enigma', release='6.5.015generic', version='15~22.04.1Ubuntu SMP PREEMPT_DYNAMIC Fri Jan 12 18:54:30 UTC 2', machine='x86_64')  NVIDIA GPU info _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,"Getting loss as `nan` with jit, but no `nan`s without jit"," Description Using keras 3 with JAX backend.  jit_compile=True * After few epochs of training properly, I started getting the loss as `nan`. (Trains properly without jit) * So I enabled `jax.config.update(""jax_debug_nans"", True)` and got the error after training a few epochs properly   jit_compile=False * The model runs perfectly fine and converges with no `nan`s (according to the error I think this is an Error from JAX and not Keras, so I posted the error here...)  What jax/jaxlib version are you using? 0.4.23 0.4.23  Which accelerator(s) are you using? GPU  Additional system info? > 1.26.2 3.11.7 (main, Dec  8 2023, 18:56:58) [GCC 11.4.0] uname_result(system='Linux', node='Enigma', release='6.5.015generic', version='15~22.04.1Ubuntu SMP PREEMPT_DYNAMIC Fri Jan 12 18:54:30 UTC 2', machine='x86_64')  NVIDIA GPU info _No response_",2024-01-31T10:19:54Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/19595,"Had a bug in my implementation of the model, now I corrected it, no `nan`s. Wonder why `nan` were coming only when `jit` was enabled though."
612,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(log1p results are incorrect on complex inputs with large absolute value)ï¼Œ å†…å®¹æ˜¯ ( Description As in the title. The results of evaluating log1p on a grid over complex plane are reported in NumPyvsJAX. As an example, consider a sample from the report for complex32 inputs using CPU:   What jax/jaxlib version are you using? 0.4.24.dev20240130+66308c30a  Which accelerator(s) are you using? CPU/GPU  Additional system info? 1.26.2 3.11.0  ++ ```)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,log1p results are incorrect on complex inputs with large absolute value," Description As in the title. The results of evaluating log1p on a grid over complex plane are reported in NumPyvsJAX. As an example, consider a sample from the report for complex32 inputs using CPU:   What jax/jaxlib version are you using? 0.4.24.dev20240130+66308c30a  Which accelerator(s) are you using? CPU/GPU  Additional system info? 1.26.2 3.11.0  ++ ```",2024-01-30T14:52:07Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/19573,It turns out that NumPy provided log1p has inaccuracy issues for inputs with small absolute values. A better illustration of log1p inaccuracies is obtained when using MPMath library as a reference. Here are some samples where JaX `log1py` returns incorrect results: ,It seems that this issue has been resolved in JAX version 0.4.27 on CPU/GPU with https://github.com/openxla/xla/pull/10503. I tested the mentioned code with JAX versions 0.4.27 on colab CPU and GPU. JAX now produces the results similar to MPMath.  Output:  Also tested for the values mentioned in the comment:  Output:  However the issue still exists on TPU. Attaching the gist for reference. Thank you,Hi   This issue appears resolved on TPU with JAX 0.4.38 and later.  log1p values are now similar between JAX and MpMath for these versions. Attaching the colab gist on TPU for reference. Thank you.,"Thanks,  for checking on the TPU platform. FTR, the origin of complex log1p fixes as well as log1p accuracy reports are available in https://github.com/pearu/functional_algorithms/issues/47issuecomment2563895024 ."
827,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(JAX jit fails with identifical pytree registered class instances)ï¼Œ å†…å®¹æ˜¯ ( Description I have encountered an issue with JAX's JIT compilation when passing two instances of a custom class registered as a Pytree node to a JITcompiled function. The function works as expected when called with the first instance but raises an error with the second instance, despite both instances being identical in data. This happens when x is set as auxiliary data. The smallest example that reproduces that issue:    What jax/jaxlib version are you using? 0.4.23  Which accelerator(s) are you using? GPU  Additional system info? _No response_  NVIDIA GPU info _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,JAX jit fails with identifical pytree registered class instances," Description I have encountered an issue with JAX's JIT compilation when passing two instances of a custom class registered as a Pytree node to a JITcompiled function. The function works as expected when called with the first instance but raises an error with the second instance, despite both instances being identical in data. This happens when x is set as auxiliary data. The smallest example that reproduces that issue:    What jax/jaxlib version are you using? 0.4.23  Which accelerator(s) are you using? GPU  Additional system info? _No response_  NVIDIA GPU info _No response_",2024-01-28T17:46:05Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/19547,"Hi  the issue here is that `aux_data` must contain hashable static entries, that can be evaluated for equality using normal `bool(a1 == a2)`. In your case, you've passed a nonstatic array to `aux_data`, and this cannot be hashed or evaluated for such equality:  To fix this, make sure during pytree flattening that you always pass array attributes to `children`, and never to `aux_data`."
728,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Unable to use JAX pmap with CPU cores)ï¼Œ å†…å®¹æ˜¯ ( Description I am trying to use JAX pmap but I am getting the error that XLA devices aren't visible  Here's my code     What jax/jaxlib version are you using? 0.4.13 0.4.13  Which accelerator(s) are you using? CPU/GPU  Additional system info? uname_result(system='Linux', node='thomaLenovoLegion515IMH05H', release='6.5.015generic', version=' CC(rename in_bdims, out_bdims > in_axes, out_axes)~22.04.1Ubuntu SMP PREEMPT_DYNAMIC Fri Jan 12 18:54:30 UTC 2', machine='x86_64', processor='x86_64')  NVIDIA GPU info )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Unable to use JAX pmap with CPU cores," Description I am trying to use JAX pmap but I am getting the error that XLA devices aren't visible  Here's my code     What jax/jaxlib version are you using? 0.4.13 0.4.13  Which accelerator(s) are you using? CPU/GPU  Additional system info? uname_result(system='Linux', node='thomaLenovoLegion515IMH05H', release='6.5.015generic', version=' CC(rename in_bdims, out_bdims > in_axes, out_axes)~22.04.1Ubuntu SMP PREEMPT_DYNAMIC Fri Jan 12 18:54:30 UTC 2', machine='x86_64', processor='x86_64')  NVIDIA GPU info ",2024-01-27T14:38:13Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/19543,It seems like this is a duplicate of CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) â€“ no need to ask this question multiple times. Thanks!,"My apologies. I thought that there was a bug. Thanks a lot for the help. On Sat, Jan 27, 2024 at 10:57â€¯AM Jake Vanderplas ***@***.***> wrote: > It seems like this is a duplicate of CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) >  â€“ no need to ask this > question multiple times. Thanks! > > â€” > Reply to this email directly, view it on GitHub > , or > unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >"
485,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(run time error: custom call 'xla.gpu.cublas.lt.matmul' failed)ï¼Œ å†…å®¹æ˜¯ ( Description XLA option:  Raise Error:   What jax/jaxlib version are you using? jax0.4.24.dev20240125+a6f26306b jaxlib0.4.24.dev20240125  Which accelerator(s) are you using? GPU  Additional system info? train with PaxML   NVIDIA GPU info ++  ++)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gemma,run time error: custom call 'xla.gpu.cublas.lt.matmul' failed, Description XLA option:  Raise Error:   What jax/jaxlib version are you using? jax0.4.24.dev20240125+a6f26306b jaxlib0.4.24.dev20240125  Which accelerator(s) are you using? GPU  Additional system info? train with PaxML   NVIDIA GPU info ++  ++,2024-01-26T19:46:41Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/19534,xla_gpu_enable_cublaslt cause this bug.
448,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Shorter errors for lax primitives)ï¼Œ å†…å®¹æ˜¯ (When I run snippets like `jax.lax.select` on incorrect shapes or dtypes, which should be an easy and fast check, I get absurdly long error messages. Could this be improved?  EDIT: jax version: 0.20.0 on Ubuntu WSL Python 3.10 Example, )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Shorter errors for lax primitives,"When I run snippets like `jax.lax.select` on incorrect shapes or dtypes, which should be an easy and fast check, I get absurdly long error messages. Could this be improved?  EDIT: jax version: 0.20.0 on Ubuntu WSL Python 3.10 Example, ",2024-01-26T12:03:19Z,enhancement better_errors,closed,0,6,https://github.com/jax-ml/jax/issues/19527,Thanks for the suggestion! Maybe we could add some of these abstract evaluation frames to the pieces hidden by `jax_traceback_filtering`.,"Actually, it looks like this is already covered. With `JAX_TRACEBACK_FILTERING='auto'` (the default value) I get this when running this script:  Can you check whether you're setting `JAX_TRACEBACK_FILTERING` to something different? It looks like you're running this in a notebook, so it's possible that you're running under a version of IPython that does not support filtering the traceback; see the logic here: https://github.com/google/jax/blob/f34bcc326ba882a500c9348d7992ae4f580bc4c3/jax/_src/traceback_util.pyL138L150","I experienced a similar traceback when using Colab (see below). , would it be possible for us to change that `JAX_TRACEBACK_FILTERING ` default setting for Colab? ",Thanks Paige â€“ it looks like things are working as expected there (note the`skipping hidden 19 frame` in your output). The traceback hiding code uses different mechanisms depending on whether it's being run in IPython or run in a script.," it looks like you may be using an older JAX version as well. These tracebacks have been improved in recent releases, you might try updating JAX to see if the error is more useful. Thanks!","Hey thanks for the quick reply! Yes forgot to state that my jax version is `0.20.0` I edited it in the post; I tested this in a IPython shell. I tried setting `os.environ['JAX_TRACEBACK_FILTERING']` to all shown options: `[""off"", ""tracebackhide"", ""remove_frames"", ""quiet_remove_frames"", ""auto""]`, but none of the options did anything.  At the moment v:0.20.0 is stable for my PC, on newer versions my GPU doesn't get detected by an unrelated error: cuBLAS version mismatch. But you're correct, when I tested this on another system where I do have jax v 0.23.0, the error message is automatically shortened. So I guess this is already fixed :). Thanks!"
655,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix `jax.lax.fori_loop(..., unroll=True)` with non-positive length)ï¼Œ å†…å®¹æ˜¯ (According to the docs: https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.fori_loop.html > As the Python version suggests, setting `upper <= lower` will produce no iterations. Negative or custom increments are not supported. This isn't supported by the underlying `scan` implementation: https://github.com/google/jax/blob/a6f26306b387ce082358e8e94fdcfd777773475f/jax/_src/lax/control_flow/loops.pyL1025)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"Fix `jax.lax.fori_loop(..., unroll=True)` with non-positive length","According to the docs: https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.fori_loop.html > As the Python version suggests, setting `upper <= lower` will produce no iterations. Negative or custom increments are not supported. This isn't supported by the underlying `scan` implementation: https://github.com/google/jax/blob/a6f26306b387ce082358e8e94fdcfd777773475f/jax/_src/lax/control_flow/loops.pyL1025",2024-01-25T16:48:41Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/19517,Removed usage of union types in the test that were causing Python 3.9 tests to fail.
495,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Remove unnecessary Array.register)ï¼Œ å†…å®¹æ˜¯ (This is unnecessary because `PRNGKeyArrayImpl` already has `jax.Array` in its hierarchy: https://github.com/google/jax/blob/cfb62501583b14face8e23d6a4f8a33950a8d64d/jax/_src/prng.pyL245 https://github.com/google/jax/blob/cfb62501583b14face8e23d6a4f8a33950a8d64d/jax/_src/prng.pyL150)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Remove unnecessary Array.register,This is unnecessary because `PRNGKeyArrayImpl` already has `jax.Array` in its hierarchy: https://github.com/google/jax/blob/cfb62501583b14face8e23d6a4f8a33950a8d64d/jax/_src/prng.pyL245 https://github.com/google/jax/blob/cfb62501583b14face8e23d6a4f8a33950a8d64d/jax/_src/prng.pyL150,2024-01-24T22:55:37Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/19509
574,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Can vmap vstack the outputs)ï¼Œ å†…å®¹æ˜¯ (I am trying to run ``vmap`` on different sized inputs (that have the same number of columns). For example  I have two functions, ``f1, f2``, that I want evaluated on x1 and x2 respectively through vmap. The following is **NOT** allowed by jax  This throws the error because vmap by default does not vstack the outputs. Is there a way to vstack the outputs? Thank you.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Can vmap vstack the outputs,"I am trying to run ``vmap`` on different sized inputs (that have the same number of columns). For example  I have two functions, ``f1, f2``, that I want evaluated on x1 and x2 respectively through vmap. The following is **NOT** allowed by jax  This throws the error because vmap by default does not vstack the outputs. Is there a way to vstack the outputs? Thank you.",2024-01-24T21:14:14Z,enhancement,closed,0,2,https://github.com/jax-ml/jax/issues/19506,"Hi, thanks for the question! `vmap` can stack outputs, but not for the data you provide in this example. The problem is that indices in `lax.switch` cannot be used to index into Python lists, because the indices are dynamic rather than static. You can address this by turning `xs` into an array rather than a list, but since JAX doesn't support ragged arrays, this requires making each of the entries the same shape. Once you do that, it should work (so long as you adjust the `in_axes` to not batch `xs` in your mapped function):  Is that along the lines of what you have in mind?","Hi Jake! First, thank you so much for your prompt reply and answer. I really appreciate the help. I now have a better understanding of ``vmap`` and ``lax.switch``. Unfortunately for my use case, the input shapes are different for each function along the first dimension. I will rethink my approach."
1497,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Computing Jacobian w.r.t. parameters used in lax.associative_scan is unexpectedly slow)ï¼Œ å†…å®¹æ˜¯ ( Description Thanks for considering this issue. My research requires me to compute the jacobian of a recurrent neural network with respect to a set of quantities. The minimal example below considers a linear recurrence $x_t = \lambda \odot x_{t1} + B u_t$ as found in recent deep statespace models such as S5. Here, $x_t, \lambda \in\mathbb{C}^n, u_t\in\mathbb{R}^m, B\in\mathbb{C}^{n\times m}$.   Problem Description Since recursion relations can be formulated as associative operators, they can be parallelized using `lax.associative_scan`.   My issue arises when using AD to compute the following quantities   $\frac{\partial x_t}{\partial x_0}$  $\frac{\partial x_t}{\partial u_s}$ for $s=1,\dots,t$  $\frac{\partial x_t}{\partial B}$  $\frac{\partial x_t}{\partial \lambda}$ I would expect these operations to   consume similar amounts of GPU time  parallelize with `associative_scan` as the forward pass is also parallelized When measuring the compute time on A100 (40GB), I find that particularly the derivative $\frac{\partial x_t}{\partial \lambda}$ is much slower than the other ones. The compute time of all derivatives increases linearly with sequence length (measured many different specifications, but the example bel)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Computing Jacobian w.r.t. parameters used in lax.associative_scan is unexpectedly slow," Description Thanks for considering this issue. My research requires me to compute the jacobian of a recurrent neural network with respect to a set of quantities. The minimal example below considers a linear recurrence $x_t = \lambda \odot x_{t1} + B u_t$ as found in recent deep statespace models such as S5. Here, $x_t, \lambda \in\mathbb{C}^n, u_t\in\mathbb{R}^m, B\in\mathbb{C}^{n\times m}$.   Problem Description Since recursion relations can be formulated as associative operators, they can be parallelized using `lax.associative_scan`.   My issue arises when using AD to compute the following quantities   $\frac{\partial x_t}{\partial x_0}$  $\frac{\partial x_t}{\partial u_s}$ for $s=1,\dots,t$  $\frac{\partial x_t}{\partial B}$  $\frac{\partial x_t}{\partial \lambda}$ I would expect these operations to   consume similar amounts of GPU time  parallelize with `associative_scan` as the forward pass is also parallelized When measuring the compute time on A100 (40GB), I find that particularly the derivative $\frac{\partial x_t}{\partial \lambda}$ is much slower than the other ones. The compute time of all derivatives increases linearly with sequence length (measured many different specifications, but the example bel",2024-01-24T16:59:13Z,bug,open,2,2,https://github.com/jax-ml/jax/issues/19498,"Just to mention, I am already happy about helpful comments how I can try to debug this myself. In the process of debugging, I'd like to inspect the XLA HLO results to see if it is compiled into a serial computation or if XLA recognizes the scan as a parallel operation. Therefore, I specify the flags   Is there a way to make XLA print human readable function names? Currently, the output reads for example `module_0012.jit__unnamed_function`. From the operations, I can guess which kernel this refers to, but it would make debugging much easier if the operations could be named.","Looking into the HLO output, I am quite sure about the correspondence between graphs and functions in the code. HLO for T=4 recurrence steps  Forward pass: module_0009.jit__unnamed_function_.sm_8.0_gpu_after_optimizations.pdf  $\frac{\partial h_4}{\partial \lambda}$ backward (VJP) module_0010.jit__unnamed_function_.sm_8.0_gpu_after_optimizations.pdf HLO for T=128 recurrence steps  Forward pass module_0009.jit__unnamed_function_.sm_8.0_gpu_after_optimizations.pdf  $\frac{\partial h_{128}}{\partial W}$ module_0010.jit__unnamed_function_.sm_8.0_gpu_after_optimizations.pdf  $\frac{\partial h_{128}}{\partial \lambda}$ module_0011.jit__unnamed_function_.sm_8.0_gpu_after_optimizations.pdf For T=128, the striking difference between $\frac{\partial h_{128}}{\partial W}$ and $\frac{\partial h_{128}}{\partial \lambda}$  with VJP is the much larger number of operations that seem to be required to compute the vjp."
1041,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Minor bug in jax.vjp? (difference w.r.t. jax.grad on scalar function))ï¼Œ å†…å®¹æ˜¯ ( Description Consider the following function: `f = lambda a : jax.numpy.multiply(a,a)`. I compute its gradient using not `grad`, but `vjp` (it is something I want, to be able to handle functions with multiple outputs). Given that `grad` is characterized in the doc as a restriction of `vjp`, the result should be the same. And yet, after applying the following code:  I obtain:  The result is correct if I use `grad` instead of `vjp`, or if the function `f` is defined as `jax.numpy.square`. This makes me think that the bug is related to the handling of broadcast (the fact that variable `a` is used twice inside the lambda).   What jax/jaxlib version are you using? 0.4.23 0.4.23  Which accelerator(s) are you using? CPU  Additional system info? Python 3.11.4  NVIDIA GPU info _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Minor bug in jax.vjp? (difference w.r.t. jax.grad on scalar function)," Description Consider the following function: `f = lambda a : jax.numpy.multiply(a,a)`. I compute its gradient using not `grad`, but `vjp` (it is something I want, to be able to handle functions with multiple outputs). Given that `grad` is characterized in the doc as a restriction of `vjp`, the result should be the same. And yet, after applying the following code:  I obtain:  The result is correct if I use `grad` instead of `vjp`, or if the function `f` is defined as `jax.numpy.square`. This makes me think that the bug is related to the handling of broadcast (the fact that variable `a` is used twice inside the lambda).   What jax/jaxlib version are you using? 0.4.23 0.4.23  Which accelerator(s) are you using? CPU  Additional system info? Python 3.11.4  NVIDIA GPU info _No response_",2024-01-23T09:49:23Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/19479,"The second value returned by `vjp` is a pullback function. If you return it, from a `jit`decorated function JAX *will* return it and treat it as opaque, ~but it would be an error to call it~. (*edit*: curiously in this case it does work, perhaps it does in general). See the documentation for an example of how to use it: https://jax.readthedocs.io/en/latest/_autosummary/jax.vjp.html Does that help?"
941,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Struggling with multiprocess in JAX)ï¼Œ å†…å®¹æ˜¯ ( Description I'm trying to compile a piece of code on multiple CPUs and evaluate it in parallel for nested sampling.  I'm not trying to parallelize individual instances of the code; I just want to control these separate processes from the same script so I know when I can stop sampling. I found I had to do a warmup call on each CPU in order to run the code in my nested sampling routine, otherwise it recompiled each time.  But I'm finding that the warmup call often stalls. Here's a snippet that reproduces the error:  This also has to be interrupted, giving   What jax/jaxlib version are you using? 0.4.23 0.4.23  Which accelerator(s) are you using? CPU  Additional system info? _No response_  NVIDIA GPU info _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Struggling with multiprocess in JAX," Description I'm trying to compile a piece of code on multiple CPUs and evaluate it in parallel for nested sampling.  I'm not trying to parallelize individual instances of the code; I just want to control these separate processes from the same script so I know when I can stop sampling. I found I had to do a warmup call on each CPU in order to run the code in my nested sampling routine, otherwise it recompiled each time.  But I'm finding that the warmup call often stalls. Here's a snippet that reproduces the error:  This also has to be interrupted, giving   What jax/jaxlib version are you using? 0.4.23 0.4.23  Which accelerator(s) are you using? CPU  Additional system info? _No response_  NVIDIA GPU info _No response_",2024-01-22T20:05:55Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/19467,"See, e.g., https://github.com/google/jax/issues/1805issuecomment561244991 JAX is incompatible with the `fork` strategy for `multiprocessing`. Does that fix the problem?","Yes, thanks!  Sorry, looks like I missed that when I was searching"
1038,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Incompatible CUDA versions installed from Pip)ï¼Œ å†…å®¹æ˜¯ ( Description When using the default install command:  it tries to install CUDA 12.3 libraries. However, the most recent stable version of PyTorch (2.1.2) is pinned to 12.1, and that's what I have on my machine. (I'm compiling flashattn, TransformerEngine and MSAMP on this machine after installing the main package dependencies, and the compilation appears to be sensitive to CUDA version.)  What jax/jaxlib version are you using? 0.4.23  Which accelerator(s) are you using? GPU, Nvidia A100  Additional system info? 1.24.4 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] uname_result(system='Linux', node='7e72bd4e01', release='5.15.091generic', version=' CC(Numpystyle indexed update support.)Ubuntu SMP Tue Nov 14 13:30:08 UTC 2023', machine='x86_64')  NVIDIA GPU info Mon Jan 22 11:32:43 2024        ++  ++)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Incompatible CUDA versions installed from Pip," Description When using the default install command:  it tries to install CUDA 12.3 libraries. However, the most recent stable version of PyTorch (2.1.2) is pinned to 12.1, and that's what I have on my machine. (I'm compiling flashattn, TransformerEngine and MSAMP on this machine after installing the main package dependencies, and the compilation appears to be sensitive to CUDA version.)  What jax/jaxlib version are you using? 0.4.23  Which accelerator(s) are you using? GPU, Nvidia A100  Additional system info? 1.24.4 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] uname_result(system='Linux', node='7e72bd4e01', release='5.15.091generic', version=' CC(Numpystyle indexed update support.)Ubuntu SMP Tue Nov 14 13:30:08 UTC 2023', machine='x86_64')  NVIDIA GPU info Mon Jan 22 11:32:43 2024        ++  ++",2024-01-22T19:33:06Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/19465,Hi  thanks for the question. You'll find some relevant info here: https://github.com/google/jax/issues/18032issuecomment1755389043 Our general approach is to provide two JAX CUDA builds: one at the most recent release (currently 12.3) and one older version that seeks to maintain compatibility with pytorch's requirements. Currently that is our cuda11 build.,"I'm going to close this, since it's essentially a duplicate of CC(JAX and TORCH). Thanks!",I'll note that the safest thing to do in general is use two separate venvs. Different frameworks have different version requirements. JAX tracks CUDA versions faster than PyTorch does.
1449,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump actions/cache from 3.3.3 to 4.0.0)ï¼Œ å†…å®¹æ˜¯ (Bumps actions/cache from 3.3.3 to 4.0.0.  Release notes Sourced from actions/cache's releases.  v4.0.0 What's Changed  Update action to node20 by @â€‹takost in actions/cache CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) feat: savealways flag by @â€‹tos in actions/cache CC(restore the behavior that Nones are pytrees)  New Contributors  @â€‹takost made their first contribution in actions/cache CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) @â€‹tos made their first contribution in actions/cache CC(restore the behavior that Nones are pytrees)  Full Changelog: https://github.com/actions/cache/compare/v3...v4.0.0    Changelog Sourced from actions/cache's changelog.  Releases 3.0.0  Updated minimum runner version support from node 12 &gt; node 16  3.0.1  Added support for caching from GHES 3.5. Fixed download issue for files &gt; 2GB during restore.  3.0.2  Added support for dynamic cache size cap on GHES.  3.0.3  Fixed avoiding empty cache save when no files are available for caching. (issue)  3.0.4  Fixed tar creation error while trying to create tar with path as ~/ home folder on ubuntulatest. (issue)  3.0.5  Removed error handling by consuming actions/cache 3.0 toolkit, Now cache server error handling will be done by toolkit. (PR)  3.0.6  Fixed  CC(Add a block_until_ready method to DeviceAr)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bump actions/cache from 3.3.3 to 4.0.0,"Bumps actions/cache from 3.3.3 to 4.0.0.  Release notes Sourced from actions/cache's releases.  v4.0.0 What's Changed  Update action to node20 by @â€‹takost in actions/cache CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) feat: savealways flag by @â€‹tos in actions/cache CC(restore the behavior that Nones are pytrees)  New Contributors  @â€‹takost made their first contribution in actions/cache CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) @â€‹tos made their first contribution in actions/cache CC(restore the behavior that Nones are pytrees)  Full Changelog: https://github.com/actions/cache/compare/v3...v4.0.0    Changelog Sourced from actions/cache's changelog.  Releases 3.0.0  Updated minimum runner version support from node 12 &gt; node 16  3.0.1  Added support for caching from GHES 3.5. Fixed download issue for files &gt; 2GB during restore.  3.0.2  Added support for dynamic cache size cap on GHES.  3.0.3  Fixed avoiding empty cache save when no files are available for caching. (issue)  3.0.4  Fixed tar creation error while trying to create tar with path as ~/ home folder on ubuntulatest. (issue)  3.0.5  Removed error handling by consuming actions/cache 3.0 toolkit, Now cache server error handling will be done by toolkit. (PR)  3.0.6  Fixed  CC(Add a block_until_ready method to DeviceAr",2024-01-22T17:51:43Z,dependencies github_actions,closed,0,2,https://github.com/jax-ml/jax/issues/19461,We need to adjust the ratchet comments â€“ I'll do this manually.,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
362,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Support for AMD/ROCm with Pallas)ï¼Œ å†…å®¹æ˜¯ (I have JAX and Triton installed. On trying the code below I get the following error: I assume this is due to XLA treating my GPU to be a NVIDIA GPU?  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Support for AMD/ROCm with Pallas,I have JAX and Triton installed. On trying the code below I get the following error: I assume this is due to XLA treating my GPU to be a NVIDIA GPU?  ,2024-01-22T13:18:06Z,enhancement contributions welcome AMD GPU,open,0,18,https://github.com/jax-ml/jax/issues/19453,I don't think we've ever tried this on AMD GPUs. Contributions welcome!  ?,   Yes we are working on upstreaming support for Pallas along with Triton and JaxTriton on AMD, thanks I did see that you had a fork for jaxtriton. Anything semi broken I can try right now? Also let me know if I can take up something particular and help. Thanks :),  Please try this docker image for now.  docker pull rocm/jaxbuild:rocm6.0.0jax0.4.20py3.10.0jax_triton,Thanks will do. Are there any binaries available. I am on LUMI and usually the step of converting from docker to singularity messes Jax containers for me. , I've tried above code with Jax image you provided and it failed with following error:  Any tips on how to run it? Thanks!,"  The above code runs totally fine for me in the container I provided. In your case, it's running on CPU.  By the way, AFAIK, pallas call will lower to cpu only if interpret mode is true https://github.com/google/jax/blob/main/jax/experimental/pallas/ops/attention.pyL179C5L179C14", it seems like in my case jax doesn't recognize gpu:  What's a correct way to run the docker container so jax can recognize AMD GPU available? I used docker command line from pytorch rocm tutorial. Thank you very much for helping!,> docker run it capadd=SYS_PTRACE securityopt seccomp=unconfined device=/dev/kfd device=/dev/dri groupadd video ipc=host shmsize 8G docker.io/rocm/jaxbuild:rocm6.0.0jax0.4.20py3.10.0jax_triton No issue with how you are running the docker. What kind of AMD GPU do you have? Do these commands work ok for you?  `rocmsmi` `rocminfo`, 2x 7900 XTX `rocmsmi`:  `rocminfo`: ,"  Unfortunately, JAX is currently not supported on Navi i.e. 7900 XTX. We are working on adding support for this platform and hopefully will have it supported soon. Thanks!", I couldn't get it work because our HPC has issues detecting GPUs inside singularity containers with JAX. Can you please share the exact branches/versions of the libs below so I can try building myself? Can you also point out if there is a minimum rocm version requirement for this to work? jaxlib jax jaxtriton triton ,>  I couldn't get it work because our HPC has issues detecting GPUs inside singularity containers with JAX. Can you please share the exact branches/versions of the libs below so I can try building myself? Can you also point out if there is a minimum rocm version requirement for this to work? >  > jaxlib jax jaxtriton triton   https://github.com/ROCm/jax/tree/rocmjaxlibv0.4.20rocm6.0jaxtriton https://github.com/ROCm/xla/tree/rocmjaxlibv0.4.20rocm6.0 https://github.com/ROCm/triton/tree/tritonjaxtriton https://github.com/rahulbatra85/jaxtriton/tree/jaxtritonrocm ROCm 5.7 or ROCm 6.0, thank you! I tried the setup proposed above and I run into this error on the line when I run MHA as shown at the start of the issue Any ideas what might be causing this? I am on a MI250x ,Further on making all the inputs bfloat16 I run into a different error: ,Bump when is this expected?,  Can you try these branches instead? https://github.com/ROCm/jax/tree/rocmjaxlibv0.4.24jaxtriton https://github.com/ROCm/xla/commits/rocmjaxlibv0.4.24jaxtriton/ https://github.com/rahulbatra85/jaxtriton/tree/jaxtritonrocm0.4.24 https://github.com/ROCm/triton/tree/jaxtritonrocm0.4.24, thanks for the update. I will try and get back to you. On our main server (lumi) we are unfortunately stuck with rocm 5.6.1. Is there any way to build these for for <rocm5.7  Thank you!
1005,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(sm_scale in flash_attention_tpu does not work like what it should)ï¼Œ å†…å®¹æ˜¯ ( Description hi. I'm trying to create a port for flash_attention and when I read the code I found out that (I guess I'm wrong) that the sm_scale is replace for attn_w / sqrt(head_dim)  but it's being multiplied instead of that so I thought the  calculated sm_scale should be  `2 ** (head_dim / 2)` and I set sm_scale to that but that simply wont work and even made predictions worst is it just  a bug or I'm not doing that right ? where sm_scale being used  https://github.com/google/jax/blob/main/jax/experimental/pallas/ops/tpu/flash_attention.pyL888 and why I thought this will work?  thanks for any help :)  What jax/jaxlib version are you using? 0.4.23  Which accelerator(s) are you using? TPU  Additional system info? Linux  NVIDIA GPU info _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,sm_scale in flash_attention_tpu does not work like what it should, Description hi. I'm trying to create a port for flash_attention and when I read the code I found out that (I guess I'm wrong) that the sm_scale is replace for attn_w / sqrt(head_dim)  but it's being multiplied instead of that so I thought the  calculated sm_scale should be  `2 ** (head_dim / 2)` and I set sm_scale to that but that simply wont work and even made predictions worst is it just  a bug or I'm not doing that right ? where sm_scale being used  https://github.com/google/jax/blob/main/jax/experimental/pallas/ops/tpu/flash_attention.pyL888 and why I thought this will work?  thanks for any help :)  What jax/jaxlib version are you using? 0.4.23  Which accelerator(s) are you using? TPU  Additional system info? Linux  NVIDIA GPU info _No response_,2024-01-20T16:54:22Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/19450,https://github.com/google/jax/blob/main/jax/experimental/pallas/ops/tpu/flash_attention.pyL832,You can set sm_scale to 1/sqrt(head_dim). I think that should work.
1284,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Linear solvers (jax.numpy.linalg.solve, jax.scipy.linalg.solve) bugged when allocating large memory)ï¼Œ å†…å®¹æ˜¯ ( Description Hi JAX team, I have noticed that there seems to be a bug in the linear solvers which seems to happen when taking up a significant part of GPU memory. Below is an example. I solve well conditioned linear systems ( A = identity, b =  all ones) with A are 10x10 matrices. In one test the number of matrices n is 10^6 (so the matrices A take a total of 0.4 GB), and in the other one the number of matrices is 10^7 (for a total memory of 4GB). Note that 63 GB are reserved by JAX on the GPU so this is well below the capacity, and the reported ""peak"" usage is 13 GB. This happens both with jax.numpy.linalg.solve and jax.scipy.linalg.solve   The output is as follows:  This same code works fine on CPU, and I have also tried on a different GPU with the same results. Thank you very much for all your great work!  What jax/jaxlib version are you using? jax v0.4.18; jaxlib v0.4.18  Which accelerator(s) are you using? GPU/CPU  Additional system info? Python 3.9.18, OS Linux  NVIDIA GPU info ++  ++)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,"Linear solvers (jax.numpy.linalg.solve, jax.scipy.linalg.solve) bugged when allocating large memory"," Description Hi JAX team, I have noticed that there seems to be a bug in the linear solvers which seems to happen when taking up a significant part of GPU memory. Below is an example. I solve well conditioned linear systems ( A = identity, b =  all ones) with A are 10x10 matrices. In one test the number of matrices n is 10^6 (so the matrices A take a total of 0.4 GB), and in the other one the number of matrices is 10^7 (for a total memory of 4GB). Note that 63 GB are reserved by JAX on the GPU so this is well below the capacity, and the reported ""peak"" usage is 13 GB. This happens both with jax.numpy.linalg.solve and jax.scipy.linalg.solve   The output is as follows:  This same code works fine on CPU, and I have also tried on a different GPU with the same results. Thank you very much for all your great work!  What jax/jaxlib version are you using? jax v0.4.18; jaxlib v0.4.18  Which accelerator(s) are you using? GPU/CPU  Additional system info? Python 3.9.18, OS Linux  NVIDIA GPU info ++  ++",2024-01-19T16:39:02Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/19431,"I have been making some tests about different batched solvers and decompositions, see below: The Cholesky (and LU probably), seems to be the main issue here. I tried to swap to a QR solver, but it is somehow 300 times slower than Cholesky for large batch size, and ~100 slower than SVD. SVD seems to be the most reliable, though I have seen it fail too.     import jax     import jax.numpy as jnp     device = jax.local_devices()[0]     print('on device:', device)     m = 10     import time     for n in [1e5, 1e6, 1e7]:         n = int(n)         A = jnp.repeat(jnp.identity(m)[None], n, axis = 0).block_until_ready()         print(""n="", n)         st_time = time.time()         U,S,Vh = jax.scipy.linalg.svd(A)         A2 = jax.lax.batch_matmul(U * S[...,None,:], Vh)         print(f""SVD error {jnp.mean(jnp.linalg.norm(A2  A, axis=(1, 2))/jnp.linalg.norm(A, axis=(1,2)))}, time = {time.time()  st_time} "")         st_time = time.time()         L = jax.scipy.linalg.cholesky(A)         A2 = jax.lax.batch_matmul(L, L.swapaxes(1,2)).block_until_ready()         print(f""Cholesky error {jnp.mean(jnp.linalg.norm(A2  A, axis=(1, 2))/jnp.linalg.norm(A, axis=(1,2)))}, time = {time.time()  st_time} "")         if n <= 1e6:             st_time = time.time()             Q,R = jnp.linalg.qr(A)             A2 = jax.lax.batch_matmul(Q,R).block_until_ready()             print(f""QR error {jnp.mean(jnp.linalg.norm(A2  A, axis=(1, 2))/jnp.linalg.norm(A, axis=(1,2)))}, time = {time.time()  st_time} "") Output:     on device: cuda:0     n= 100000     SVD error 0.0, time = 0.6100842952728271      Cholesky error 0.0, time = 0.15522980690002441      QR error 0.0, time = 3.7535462379455566      n= 1000000     SVD error 0.0, time = 0.5560173988342285      Cholesky error 0.0, time = 0.1310713291168213      QR error 0.0, time = 35.838584184646606      n= 10000000     SVD error 0.0, time = 2.056659460067749      Cholesky error nan, time = 0.27480244636535645  Note that JAX doesn't throw an error or warning  Here is a SVD based solver to use in the mean time, in case someone else needs a stopgap:     def solve_by_SVD(A,b):         U,S,Vh = jax.scipy.linalg.svd(A)         if b.ndim == A.ndim 1:             expand = True             b = b[...,None]         else:             expand = False         Uhb = jax.lax.batch_matmul(jnp.conj(U.swapaxes(1,2)),b)/ S[...,None]         x = jax.lax.batch_matmul(jnp.conj(Vh.swapaxes(1,2)),Uhb)         if expand:             x = x[...,0]         return x","As far as I can tell, the LU/Cholesky bugs are fixed by updating jax to 0.4.23 . See https://github.com/patrickkidger/lineax/issues/79issuecomment1929203311 for more info ","Well, fixed is fixed, I guess. We could dig into why, but it would mostly be of historical interest. Please reopen if it happens again!"
1112,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Jax with cuda  installation command not correct)ï¼Œ å†…å®¹æ˜¯ ( Description I'm installing jax following the command in README: `pip install U ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` However, this raises warning:  and the installed cuda Jax is not available with the failure below:  After checking the release website I tried to change the installation command to: `pip install U ""jax[cuda12]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` and it gave the correctly installed Jax  What jax/jaxlib version are you using? jax 0.4.23; jaxlib 0.4.23  Which accelerator(s) are you using? GPU  Additional system info? 1.26.3 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] uname_result(system='Linux', node='kellislab3.csail.mit.edu', release='5.15.086generic', version=' CC(add misc numpy ops (c.f. 70))Ubuntu SMP Wed Sep 20 08:23:49 UTC 2023', machine='x86_64')  NVIDIA GPU info )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Jax with cuda  installation command not correct," Description I'm installing jax following the command in README: `pip install U ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` However, this raises warning:  and the installed cuda Jax is not available with the failure below:  After checking the release website I tried to change the installation command to: `pip install U ""jax[cuda12]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` and it gave the correctly installed Jax  What jax/jaxlib version are you using? jax 0.4.23; jaxlib 0.4.23  Which accelerator(s) are you using? GPU  Additional system info? 1.26.3 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] uname_result(system='Linux', node='kellislab3.csail.mit.edu', release='5.15.086generic', version=' CC(add misc numpy ops (c.f. 70))Ubuntu SMP Wed Sep 20 08:23:49 UTC 2023', machine='x86_64')  NVIDIA GPU info ",2024-01-19T14:22:17Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/19430,"Hi, thanks for the report. JAX does specify the `cuda12_pip` extra, but not `cuda12pip`. Can you check whether you had a typo when you ran the installation command?","> Hi, thanks for the report. JAX does specify the `cuda12_pip` extra, but not `cuda12pip`. Can you check whether you had a typo when you ran the installation command? Thanks for reply. I checked the command and it is 'cuda12_pip'. However, it seems like then pip identify it as `cuda12pip`. This happens when I just created a new conda env with python=3.10",What version of `pip` do you have installed?,It looks like you're running into the issue discussed here: https://discuss.python.org/t/whatextrasnamesaretreatedasequalandwhy/7614,"many thanks for pointing this out. according to their discussion, this is fixed in PEP685, so i tried python 3.12 and it works!",Is there a resolution for python<3.12?,"I've never been able to reproduce this issue personally; I'm sure it arises with some special combination of Python version, pip version, and jax version. I'd try installing newer or older pip versions and see if that fixes things for you."
1003,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Memory leak when allocating arrays in a for loop)ï¼Œ å†…å®¹æ˜¯ ( Description Hi there,  The issue was surfaced in Numpyro initially: https://github.com/pyroppl/numpyro/issues/1699, it seems all memory is not properly released when allocating a new array in a for loop. One reproducible example:  Gives:  More examples are available here: https://github.com/pyroppl/numpyro/issues/1699issuecomment1877573525 Thanks!  What jax/jaxlib version are you using? 0.4.23, 0.4.23  Which accelerator(s) are you using? CPU  Additional system info? 1.26.0 3.10.12  (main, Jun 23 2023, 22:41:52) [Clang 15.0.7 ] uname_result(system='Darwin', node='ClementsMacBookPro2.local', release='23.2.0', version='Darwin Kernel Version 23.2.0: Wed Nov 15 21:53:18 PST 2023; root:xnu10002.61.3~2/RELEASE_ARM64_T6000', machine='arm64')  NVIDIA GPU info _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Memory leak when allocating arrays in a for loop," Description Hi there,  The issue was surfaced in Numpyro initially: https://github.com/pyroppl/numpyro/issues/1699, it seems all memory is not properly released when allocating a new array in a for loop. One reproducible example:  Gives:  More examples are available here: https://github.com/pyroppl/numpyro/issues/1699issuecomment1877573525 Thanks!  What jax/jaxlib version are you using? 0.4.23, 0.4.23  Which accelerator(s) are you using? CPU  Additional system info? 1.26.0 3.10.12  (main, Jun 23 2023, 22:41:52) [Clang 15.0.7 ] uname_result(system='Darwin', node='ClementsMacBookPro2.local', release='23.2.0', version='Darwin Kernel Version 23.2.0: Wed Nov 15 21:53:18 PST 2023; root:xnu10002.61.3~2/RELEASE_ARM64_T6000', machine='arm64')  NVIDIA GPU info _No response_",2024-01-19T13:27:31Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/19429,"The `Array` implementation uses a toplevel lrucaching function here: https://github.com/google/jax/blob/0b542ff585b30b24aceadd12a2335b0fa26b8209/jax/_src/array.pyL124 There may be other places with similar caches that I didn't go deep enough to dig up... but these would cause some amount of memory growth independent of the jaxspecific caching (e.g. anything that would be cleared by `clear_caches`) and a garbage collection would not free these LRU caches either. I also tried your test code without the JIT annotation and observed the same linear growth which makes me think that at least that is not the problem... So I experimented a bit. If you replace:  ... with:  You will still observe some memory growth, but the growth is more stable. For example, I tested by increasing the iteration count to 5000 (larger than the cache limit) and ran it with a fixed array size and got:  This is sort of the behavior you would expect from an LRU cache, so I would only be concerned if the memory growth is unbounded. You mentioned in the other ticket: > my model has varying input sizes and needs to reestimate all parameters regularly This would mean you are likely to have a lot of cache misses and would end up fully utilizing the cache space without benefiting much from the cached values.",It seems like this has been answered. Feel free to open another issue if you still have questions!
425,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([XLA:Python] Fail with an AttributeError if __cuda_array_interface__ is called on a sharded array.)ï¼Œ å†…å®¹æ˜¯ ([XLA:Python] Fail with an AttributeError if __cuda_array_interface__ is called on a sharded array. Fixes https://github.com/google/jax/issues/19134)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[XLA:Python] Fail with an AttributeError if __cuda_array_interface__ is called on a sharded array.,[XLA:Python] Fail with an AttributeError if __cuda_array_interface__ is called on a sharded array. Fixes https://github.com/google/jax/issues/19134,2024-01-19T12:16:10Z,,closed,0,0,https://github.com/jax-ml/jax/issues/19428
619,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(using jnp.nozero is not possible in jax.eval_shape)ï¼Œ å†…å®¹æ˜¯ ( Description im implementing mixtral model in jax and when i want to use jnp.nozero it causes error and i cant init the params and get shape here's how im using that   is there any recommendation or help that i can get ?  What jax/jaxlib version are you using? 0.4.20 JAX and JAXlib  Which accelerator(s) are you using? CPU/TPU  Additional system info? Linux  NVIDIA GPU info _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",mixtral,using jnp.nozero is not possible in jax.eval_shape, Description im implementing mixtral model in jax and when i want to use jnp.nozero it causes error and i cant init the params and get shape here's how im using that   is there any recommendation or help that i can get ?  What jax/jaxlib version are you using? 0.4.20 JAX and JAXlib  Which accelerator(s) are you using? CPU/TPU  Additional system info? Linux  NVIDIA GPU info _No response_,2024-01-19T09:53:19Z,bug,closed,0,8,https://github.com/jax-ml/jax/issues/19424,>  Description > im implementing mixtral model in jax and when i want to use jnp.nozero it causes error and i cant init the params and get shape >  > here's how im using that >  >  >  > is there any recommendation or help that i can get ? >  >  What jax/jaxlib version are you using? > 0.4.20 JAX and JAXlib >  >  Which accelerator(s) are you using? > CPU/TPU >  >  Additional system info? > Linux >  >  NVIDIA GPU info > _No response_ https://github.com/erfanzar/EasyDeL/blob/main/lib/python/EasyDel/modules/mixtral/modelling_mixtral_flax.pyL373,"Hi  the issue is that `jnp.nonzero` creates a dynamicallyshaped array (i.e. an array shape that depends on the values in the array passed to it), and thus is incompatible with `eval_shape`. You can address this by passing a static `size` argument to `nonzero`, in order to statiscally specify the size of the output array; for example:  If the array passed to `nonzero` has fewer nonzero entries than the specified `size`, the results will be padded with zeros. If it has more nonzero entries, they will be truncated. This static shape requirement is fundamental to the design of JAX transformations; for more discussion, see https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.htmldynamicshapes.",Yes i have tried that and noticed that with using size argument it will work correctly but it's no longer dynamic for the purpose that in using that," If you're trying to do megablocksstyle MoE layers, I don't know if you actually need/want to be computing the nonzeros for each expert. Instead, you know that each token gets assigned exactly k experts, so the number of token/expert pairs actually is statically knowable. The thing I haven't thought carefully about is whether you can actually implement the necessary blocksparse matmul efficiently in pure JAX. I suspect you can't do it memoryefficiently without a custom kernel but I'm not sure.",  guess an scan function does the job but anyway have you seen somebody implement this in jax or i should figure it out myself ," I've actually been experimenting with implementing the megablocks stuff. So far I've made the forward pass in pure JAX (so backwards will work as well), and I also have pallas kernels for the DSD and SDD matmuls done. There's still quite a lot to do, but I can prioritize getting something shareable uploaded if you're interested.",I would be more than happy to connect with you in that case but have you tried flax.linen.scan and conditioner to make it?,"I'm going to close this because the original question is answered; if you want to chat about ideas for how to implement megablocks MoE, perhaps a dedicated discussion would be a better place. Thanks!"
828,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Use jit's jaxpr creation function for eval_shape to maximize tracing cache hits.)ï¼Œ å†…å®¹æ˜¯ (Use jit's jaxpr creation function for eval_shape to maximize tracing cache hits. This comes up in LLM models, where we trace twice (one for eval_shape (usually the init function) and another during jit) when the output jaxpr is the same. This shouldn't happen and we should cache as much as possible.  The only caveat here is that in eval_shape the `traced_for` on `DebugInfo` is set to `jit`. But maybe it's ok to do that if we want to deprecate eval_shape for a AOT style method on `jax.jit` or have it be a thin wrapper around something like `jax.jit(f).eval_shape`)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,Use jit's jaxpr creation function for eval_shape to maximize tracing cache hits.,"Use jit's jaxpr creation function for eval_shape to maximize tracing cache hits. This comes up in LLM models, where we trace twice (one for eval_shape (usually the init function) and another during jit) when the output jaxpr is the same. This shouldn't happen and we should cache as much as possible.  The only caveat here is that in eval_shape the `traced_for` on `DebugInfo` is set to `jit`. But maybe it's ok to do that if we want to deprecate eval_shape for a AOT style method on `jax.jit` or have it be a thin wrapper around something like `jax.jit(f).eval_shape`",2024-01-18T17:31:22Z,,closed,0,0,https://github.com/jax-ml/jax/issues/19407
1469,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`jax.jvp` is tricky to use with non-differentiable inputs.)ï¼Œ å†…å®¹æ˜¯ ( Description This ticket is a followup of issue CC(jnp.zeros does not support float0 as a dtype) following a comment by  [[link]](https://github.com/google/jax/issues/4433issuecomment1894147442_). The following reproducer tries to compute the forward derivative of a simple function that has a differentiable and a nondifferentiable (scalar, in this case) input.  When the `bug` variable is set to zero at the top, the IMO most intuitive version of the code is used. This yields the error  It seems impossible to actually create a JAX array with that dtype, I tried many different things. Setting `bug=1` gives an example of those difficulties:  Setting `bug=2` finally works by creating such a `float0` array using NumPy (of all things!)  Anyways, this seems like a somewhat weird interface and is probably a bug. And it seems contradictory with 's comment: >  float0 arrays are purely a tracetime entity, and can only exist on the host (XLA does not have any equivalent of a float0 type).  I would also say that it's quite common for functions to accept nondifferentiable arguments and still expect `jax.jvp` to work (such variables are bound to occur somewhere in a pytree in more complex applications)  What jax/jaxlib version )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,`jax.jvp` is tricky to use with non-differentiable inputs.," Description This ticket is a followup of issue CC(jnp.zeros does not support float0 as a dtype) following a comment by  [[link]](https://github.com/google/jax/issues/4433issuecomment1894147442_). The following reproducer tries to compute the forward derivative of a simple function that has a differentiable and a nondifferentiable (scalar, in this case) input.  When the `bug` variable is set to zero at the top, the IMO most intuitive version of the code is used. This yields the error  It seems impossible to actually create a JAX array with that dtype, I tried many different things. Setting `bug=1` gives an example of those difficulties:  Setting `bug=2` finally works by creating such a `float0` array using NumPy (of all things!)  Anyways, this seems like a somewhat weird interface and is probably a bug. And it seems contradictory with 's comment: >  float0 arrays are purely a tracetime entity, and can only exist on the host (XLA does not have any equivalent of a float0 type).  I would also say that it's quite common for functions to accept nondifferentiable arguments and still expect `jax.jvp` to work (such variables are bound to occur somewhere in a pytree in more complex applications)  What jax/jaxlib version ",2024-01-16T20:05:32Z,bug,open,0,6,https://github.com/jax-ml/jax/issues/19386,"Thanks for the detail. I'm going to assign , because he has the most context on `float0`. Matt, maybe this is a good case for replacing `float0` with an implementation based on extended dtypes?","+1 on some kind of easy tracetime symbolic zero existing, but for unrelated reasons. I've had a few cases where constant folding of huge zero arrays leads to really long compile times, and I'm not aware of a nice pure JAX workaround atm.","Thanks for raising this. Yes we should expose a nice symboliczero dtype. However, for this `jvp` issue in the OP, I'd suggest writing it like this so you never have to make a `dy`: `out, tangents = jax.jvp(lambda x: f(x, y), (x,), (dx,))`.","Dear , thanks for looking into this, and for your great work on JAX. The partial evaluation workaround you suggested will work fine when the signature of the function is static and the nondiff. arguments nicely map to positional arguments.  My use case is too weird and generic: I am writing a differentiable bridge between JAX and another differentiable programming framework called Dr.Jit (drjit.wrap). The JVPed function has the signature `*args, **kwargs`, where any element could contain arbitrarily nested Pytrees including custom data structures where attempts to remove a field may cause problems. Having an officially supported symbolic zero would solve the issue. My current workaround is to create it with NumPy (the `bug=2` variant above), but I am unsure as to the longterm stability of that hack. Best, Wenzel","To handle the generic case you might be able to use equinox.filter_jvp which uses `None` as the appropriate tangent type. I wouldn't conflate this with symbolic zeros, though: I think choice of tangent dtype really is a separate thing. (What dtype would such a symbolic zero have, after all?) I'd also advocate against removing `float0`: this won't be a fun compatibility change, e.g. I have code that specialcases this is in a few spots and I would really prefer that not break. What I think *would* make sense though is for fixing various ergonomic issues, e.g. allowing for `float0` JAX arrays or adding a public `jax.unit = np.empty((), dtype=jax.float0)` that can be used.","> I'd also advocate against removing float0 Yes, sorry for not being clear. I'd want to fix the implementation and completeness of float0 (along the lines you described) while preserving the existing API."
1449,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bump actions/cache from 3.3.2 to 3.3.3)ï¼Œ å†…å®¹æ˜¯ (Bumps actions/cache from 3.3.2 to 3.3.3.  Release notes Sourced from actions/cache's releases.  v3.3.3 What's Changed  Cache v3.3.3 by @â€‹robherley in actions/cache CC(Temporarily disable test_jit_device_assignment.)  New Contributors  @â€‹robherley made their first contribution in actions/cache CC(Temporarily disable test_jit_device_assignment.)  Full Changelog: https://github.com/actions/cache/compare/v3...v3.3.3    Changelog Sourced from actions/cache's changelog.  Releases 3.0.0  Updated minimum runner version support from node 12 &gt; node 16  3.0.1  Added support for caching from GHES 3.5. Fixed download issue for files &gt; 2GB during restore.  3.0.2  Added support for dynamic cache size cap on GHES.  3.0.3  Fixed avoiding empty cache save when no files are available for caching. (issue)  3.0.4  Fixed tar creation error while trying to create tar with path as ~/ home folder on ubuntulatest. (issue)  3.0.5  Removed error handling by consuming actions/cache 3.0 toolkit, Now cache server error handling will be done by toolkit. (PR)  3.0.6  Fixed  CC(Add a block_until_ready method to DeviceArray.)  zstd d: no such file or directory error Fixed  CC(improve while_loop/fori_loop dtype error message)  cache doesn't )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bump actions/cache from 3.3.2 to 3.3.3,"Bumps actions/cache from 3.3.2 to 3.3.3.  Release notes Sourced from actions/cache's releases.  v3.3.3 What's Changed  Cache v3.3.3 by @â€‹robherley in actions/cache CC(Temporarily disable test_jit_device_assignment.)  New Contributors  @â€‹robherley made their first contribution in actions/cache CC(Temporarily disable test_jit_device_assignment.)  Full Changelog: https://github.com/actions/cache/compare/v3...v3.3.3    Changelog Sourced from actions/cache's changelog.  Releases 3.0.0  Updated minimum runner version support from node 12 &gt; node 16  3.0.1  Added support for caching from GHES 3.5. Fixed download issue for files &gt; 2GB during restore.  3.0.2  Added support for dynamic cache size cap on GHES.  3.0.3  Fixed avoiding empty cache save when no files are available for caching. (issue)  3.0.4  Fixed tar creation error while trying to create tar with path as ~/ home folder on ubuntulatest. (issue)  3.0.5  Removed error handling by consuming actions/cache 3.0 toolkit, Now cache server error handling will be done by toolkit. (PR)  3.0.6  Fixed  CC(Add a block_until_ready method to DeviceArray.)  zstd d: no such file or directory error Fixed  CC(improve while_loop/fori_loop dtype error message)  cache doesn't ",2024-01-15T18:02:32Z,pull ready dependencies github_actions,closed,0,0,https://github.com/jax-ml/jax/issues/19367
1033,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Pallas is very slow)ï¼Œ å†…å®¹æ˜¯ ( Description Hi, I am trying to write a simple cumulative sum program in Pallas and standard jax to compare their performance. However, it seems like Pallas is much slower.  When Pallas kernel is launching, there's a message saying `Removed command buffer support for CUBLAS as it's not supported with gpu toolkit version 12020 and driver version 12020. This might negatively impact performance.` I am wondering if this is the cause and how I could solve it. Thank you so much for your time and help!   Below is my program code:  And the output is   What jax/jaxlib version are you using? jax==0.4.24.dev20240104; jaxtriton==0.1.4; jaxlib==0.4.24.dev20240103+cuda12.cudnn89  Which accelerator(s) are you using? GPU  Additional system info? Python 3.10.13; Ubuntu 20.04;   NVIDIA GPU info  Cuda Toolkit: 12.3 cuDNN: 8.9.4 for CUDA 12.x)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Pallas is very slow," Description Hi, I am trying to write a simple cumulative sum program in Pallas and standard jax to compare their performance. However, it seems like Pallas is much slower.  When Pallas kernel is launching, there's a message saying `Removed command buffer support for CUBLAS as it's not supported with gpu toolkit version 12020 and driver version 12020. This might negatively impact performance.` I am wondering if this is the cause and how I could solve it. Thank you so much for your time and help!   Below is my program code:  And the output is   What jax/jaxlib version are you using? jax==0.4.24.dev20240104; jaxtriton==0.1.4; jaxlib==0.4.24.dev20240103+cuda12.cudnn89  Which accelerator(s) are you using? GPU  Additional system info? Python 3.10.13; Ubuntu 20.04;   NVIDIA GPU info  Cuda Toolkit: 12.3 cuDNN: 8.9.4 for CUDA 12.x",2024-01-13T04:03:09Z,bug pallas,open,0,15,https://github.com/jax-ml/jax/issues/19350,"Oh, it's my bad, I didn't jax.jit . **Please ignore my above message.** Once it's jitted, I test the below program  The output of the above `D = 1024` and `grid_size = 32` is reasonable:  **However, when `D = 768` and `grid_size = 32`, I got this error despite that 768 is divisible by 32:**  Could you please help me understand that?  Thank you very much!",I think blocks need to be powers of 2 sized.,Thank you very much for your kind reply. Another question is that it seems like Pallas has some issues handling **batched matrix multiplication**. A simple example below:  The error message is ,"To do a batched matmul, it often makes sense to make the batch dimension a parallel dimension in the grid. The local matmuls on each SM will be unbatched (which is that the TensorCores are capable of anyways). You can do this easily by using `jax.vmap` on an unbatched matmul kernel.",Thanks for your kind reply. It seems like the error occurs even for simple matmul (not batched):  The error message is ,Can you print the above exception? There should be a Triton exception as well,,The shapes you are using are too small. Try with bigger ones (like 32 at the very least).,"Thank you for your reply. I tried making `x` and `y` both have shape `(32,32)`, still got the below error. Making them `(64, 64)` or larger still has the similar error. ",You'll now need to have torch installed.,"Thank you very much! We are essentially trying to fuse cumsum and matmul, but have been constantly getting errors when assigning results to output buffer inside kernel. Our simple code is:  The error message is: ",Here it seems like you're trying to do a matmuls with a 1sized dimension. Either it needs to be padded or you should be doing a matrix vector product (multiply then reduce sum).,"I printed out the shape of variables inside the kernel function:  And the shape of `O_ref[i]` is the same as `z` (e.g., (1,1024)) in the for loop, but the assignment fails.","`z = X_ref[i] @ W_cumulative   shape: (1, 1024)` seems to be a matrix vector product, not a matrix multiplication right?","Hi, here the shape of X_ref[...] is [N//gs, 1, D], so X_ref[i] is of shape [1, D]. Thus the above is matrix multiplication. The simplified code we hope to test is as below, could you please help us understand why `O_ref[i] = z` fails? Thank you! "
947,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Improve jax.Array documentation)ï¼Œ å†…å®¹æ˜¯ (https://jax.readthedocs.io/en/latest/_autosummary/jax.Array.htmljax.Array Many of the methods and properties of `jax.Array` are not sufficiently documented. * I don't think `ArrayImpl` appears in the documentation at all. It at least needs an explanation. * For pretty much all methods, especially those that don't correspond to NumPy APIs, we should link to documentation that shows an example of usage, including a typical input and output. * Some of the documentation is very cryptic and needs more details. For example: > addressable_data(index)  List of addressable shards. What type of object is a shard? Does they come in any particular order? * Some attributes aren't documented as best I can tell, e.g., `device` and `devices`.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Improve jax.Array documentation,"https://jax.readthedocs.io/en/latest/_autosummary/jax.Array.htmljax.Array Many of the methods and properties of `jax.Array` are not sufficiently documented. * I don't think `ArrayImpl` appears in the documentation at all. It at least needs an explanation. * For pretty much all methods, especially those that don't correspond to NumPy APIs, we should link to documentation that shows an example of usage, including a typical input and output. * Some of the documentation is very cryptic and needs more details. For example: > addressable_data(index)  List of addressable shards. What type of object is a shard? Does they come in any particular order? * Some attributes aren't documented as best I can tell, e.g., `device` and `devices`.",2024-01-12T18:47:23Z,enhancement documentation,open,2,2,https://github.com/jax-ml/jax/issues/19342,"> I don't think `ArrayImpl` appears in the documentation at all This came up when we were creating the linked page. I recall  was a strong 1 on documenting `ArrayImpl`. Some implonly functions are not documented, becuase they don't exist on the base `Array` class. For the others, getting more detailed docs is mainly about changing how we declare `jax.Array` in the sphinx sources.","I think we should document the existence of `ArrayImpl` but not say a whole lot about it, pointing the user to `Array."
636,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([shape_poly] Protect shape_poly: rename to _shape_poly.py.)ï¼Œ å†…å®¹æ˜¯ ([shape_poly] Protect shape_poly: rename to _shape_poly.py. The public APIs can be accessed through `jax.experimental.export`. The shape_poly and serialization modules are still changing and I saw external references to various symbols in them, even protected ones. I have removed such references from the Google code base, and I want to take another step to discourage direct access to its symbols.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[shape_poly] Protect shape_poly: rename to _shape_poly.py.,"[shape_poly] Protect shape_poly: rename to _shape_poly.py. The public APIs can be accessed through `jax.experimental.export`. The shape_poly and serialization modules are still changing and I saw external references to various symbols in them, even protected ones. I have removed such references from the Google code base, and I want to take another step to discourage direct access to its symbols.",2024-01-12T10:11:34Z,,closed,0,0,https://github.com/jax-ml/jax/issues/19332
958,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(vmap conditional debug.print ignores conditioning)ï¼Œ å†…å®¹æ˜¯ ( Description I'm trying to debug a vmapped code and wanted to print a message when certain conditions are met, and notices some odd results. Here's a minimal reproducible example:  This prints out the debug print for all values of x_grid regardless if they meet the condition or not.  What jax/jaxlib version are you using? 0.4.23 0.4.23  Which accelerator(s) are you using? GPU  Additional system info? 1.26.2 3.10.13  (main, Oct 26 2023, 18:07:37) [GCC 12.3.0] uname_result(system='Linux', node='node2415', release='4.18.0348.el8.0.2.x86_64', version=' CC(Python 3 compatibility issues) SMP Sun Nov 14 00:51:12 UTC 2021', machine='x86_64')  NVIDIA GPU info NVIDIASMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,vmap conditional debug.print ignores conditioning," Description I'm trying to debug a vmapped code and wanted to print a message when certain conditions are met, and notices some odd results. Here's a minimal reproducible example:  This prints out the debug print for all values of x_grid regardless if they meet the condition or not.  What jax/jaxlib version are you using? 0.4.23 0.4.23  Which accelerator(s) are you using? GPU  Additional system info? 1.26.2 3.10.13  (main, Oct 26 2023, 18:07:37) [GCC 12.3.0] uname_result(system='Linux', node='node2415', release='4.18.0348.el8.0.2.x86_64', version=' CC(Python 3 compatibility issues) SMP Sun Nov 14 00:51:12 UTC 2021', machine='x86_64')  NVIDIA GPU info NVIDIASMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0",2024-01-11T15:39:48Z,question,closed,0,2,https://github.com/jax-ml/jax/issues/19314,"Hi, thanks for the question! This is behaving as expected: a vmapped `cond` becomes a `select` (as noted in the docstring) and a `select` executes both branches.",Thank you for the quick reply! I misunderstood what that part of the docstring meant! Sorry about the unnecessary issue report.
551,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Pallas: `jnp.indices` fails to lower to mosaic)ï¼Œ å†…å®¹æ˜¯ ( Description The following kernel fails with the error listed below:  Error:  Full reproducer   What jax/jaxlib version are you using? jax v0.4.24  Which accelerator(s) are you using? TPU  Additional system info? numpy version: 1.26.3, python version: 3.11.6, OS: gLinux (Debian 6.5.131rodete1)  NVIDIA GPU info _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Pallas: `jnp.indices` fails to lower to mosaic," Description The following kernel fails with the error listed below:  Error:  Full reproducer   What jax/jaxlib version are you using? jax v0.4.24  Which accelerator(s) are you using? TPU  Additional system info? numpy version: 1.26.3, python version: 3.11.6, OS: gLinux (Debian 6.5.131rodete1)  NVIDIA GPU info _No response_",2024-01-10T19:07:34Z,bug pallas,open,0,0,https://github.com/jax-ml/jax/issues/19291
837,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(pallas: improve indexing trace time)ï¼Œ å†…å®¹æ˜¯ (The problem here is that for values with large trace contexts, constructing a `ConcretizationTypeError` can be expensive, which leads to slow kernel compilation. We could maybe improve this by lazily constructing this string in `ConcretizationTypeError`, but it derives from `TypeError` which expects the message to be available at instantiation. Rather than trying to hack a lazy version of `TypeError`, I'm opting to just avoid the problem at the source. I've searched the rest of the code for `except ConcretizationTypeError` and found a few other instances of this, none of which are good candidates for a similar fix.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,pallas: improve indexing trace time,"The problem here is that for values with large trace contexts, constructing a `ConcretizationTypeError` can be expensive, which leads to slow kernel compilation. We could maybe improve this by lazily constructing this string in `ConcretizationTypeError`, but it derives from `TypeError` which expects the message to be available at instantiation. Rather than trying to hack a lazy version of `TypeError`, I'm opting to just avoid the problem at the source. I've searched the rest of the code for `except ConcretizationTypeError` and found a few other instances of this, none of which are good candidates for a similar fix.",2024-01-09T19:32:23Z,pull ready,closed,1,0,https://github.com/jax-ml/jax/issues/19272
934,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Some PRNG seeds causing zero gradients)ï¼Œ å†…å®¹æ˜¯ ( Description Hey, I have a minimal example here of an MLP (using flax) training script that, for some seeds, doesn't see any convergence (loss is constant and gradients are all zero). I've tried digging deeper but can't seem to figure out why this is happening I've included a couple of bad seeds, in which the issue occurs, and some random seeds where I don't see it. Also, changing the second dimension of the input from 16 to 17 mitigates the issue (line 82, I left a comment there).   What jax/jaxlib version are you using? jax==0.4.23, jaxlib==0.4.23+cuda11.cudnn86, jaxopt==0.8.1, flax==0.7.5  Which accelerator(s) are you using? GPU  Additional system info? Python 3.11.6, Ubuntu 20.04.6 LTS  NVIDIA GPU info )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Some PRNG seeds causing zero gradients," Description Hey, I have a minimal example here of an MLP (using flax) training script that, for some seeds, doesn't see any convergence (loss is constant and gradients are all zero). I've tried digging deeper but can't seem to figure out why this is happening I've included a couple of bad seeds, in which the issue occurs, and some random seeds where I don't see it. Also, changing the second dimension of the input from 16 to 17 mitigates the issue (line 82, I left a comment there).   What jax/jaxlib version are you using? jax==0.4.23, jaxlib==0.4.23+cuda11.cudnn86, jaxopt==0.8.1, flax==0.7.5  Which accelerator(s) are you using? GPU  Additional system info? Python 3.11.6, Ubuntu 20.04.6 LTS  NVIDIA GPU info ",2024-01-09T14:42:03Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/19268,I think the problem is that you apply relu to the output of the last layer. For the given seeds and input sizes the output is just < 0 and thus there is no gradient.,"I think  is right â€“ if you add `jax.debug.print(""Value of relu(x) is {x}"", x=x)` at the end of `MultiLayerPerceptron.__call__`, you can see that the result at each iteration is exactly zero.","Yep that's correct, thanks! It seems this was causing gradients to always be zero and no learning to occur. Removing the last relu or using leaky relu mitigated the issue"
944,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Missing ""Memory profile"" in Tensorboard when using CPU)ï¼Œ å†…å®¹æ˜¯ ( Description Hello,  I am trying to profile a script on both the CPU and GPU using Tensorboard (following the instructions in the docs).  When profiling on GPU, I can access the 'Memory profile' tab of Tensorboard, but when I change the device to CPU with the following command:   I can no longer see the profile output. Instead, I see the following error message:  !image I am particularly interested in measuring the peak memory use of the program so I would also be interested in knowing about any workarounds for obtaining this information.   What jax/jaxlib version are you using? jax v.0.4.23, jaxlib v.0.4.23  Which accelerator(s) are you using? CPU/GPU  Additional system info? WSL 2   NVIDIA GPU info )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"Missing ""Memory profile"" in Tensorboard when using CPU"," Description Hello,  I am trying to profile a script on both the CPU and GPU using Tensorboard (following the instructions in the docs).  When profiling on GPU, I can access the 'Memory profile' tab of Tensorboard, but when I change the device to CPU with the following command:   I can no longer see the profile output. Instead, I see the following error message:  !image I am particularly interested in measuring the peak memory use of the program so I would also be interested in knowing about any workarounds for obtaining this information.   What jax/jaxlib version are you using? jax v.0.4.23, jaxlib v.0.4.23  Which accelerator(s) are you using? CPU/GPU  Additional system info? WSL 2   NVIDIA GPU info ",2024-01-09T14:00:15Z,bug,open,1,1,https://github.com/jax-ml/jax/issues/19266,"Since your environment includes a mix of WSL 2 and GPU, also ensure that there are no compatibility issues related to running JAX and Tensorboard within the WSL 2 environment."
1401,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Error writing Perfetto trace to GCS)ï¼Œ å†…å®¹æ˜¯ ( Description I have a usecase where I'd like to collect a profiling trace without blocking, and save it somewhere nonemphemeral, like GCS, so that I can browse the trace later, even if the host VM is preempted. To avoid blocking, I am not creating a link to. Instead, I am saving the trace directly as described in the docs.  However, when I do , there is an error when calling , if LOGDIR is a Google Cloud Storage URI. For example, if , the stacktrace is:   Notably, the leading slash of the GCS URI is now missing, and a local path has been prepended, as if the provided logdir was a relative path. On the other hand, the nonperfetto traces are seemingly written without error to GCS when .  What jax/jaxlib version are you using? jax 0.4.23; jaxlib 0.4.23  Which accelerator(s) are you using? TPU v38  Additional system info? Numpy version: 1.23.5. Python version: 3.9.5 (default, Nov 23 2021, 15:27:38)  [GCC 9.3.0]. Platform uname: uname_result(system='Linux', node='t1vne84c95a6w0', release='5.13.01027gcp', version=' CC(Fix the bug in classifier example, batching_test and README)~20.04.1Ubuntu SMP Thu May 26 10:53:08 UTC 2022', machine='x86_64')  NVIDIA GPU info _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Error writing Perfetto trace to GCS," Description I have a usecase where I'd like to collect a profiling trace without blocking, and save it somewhere nonemphemeral, like GCS, so that I can browse the trace later, even if the host VM is preempted. To avoid blocking, I am not creating a link to. Instead, I am saving the trace directly as described in the docs.  However, when I do , there is an error when calling , if LOGDIR is a Google Cloud Storage URI. For example, if , the stacktrace is:   Notably, the leading slash of the GCS URI is now missing, and a local path has been prepended, as if the provided logdir was a relative path. On the other hand, the nonperfetto traces are seemingly written without error to GCS when .  What jax/jaxlib version are you using? jax 0.4.23; jaxlib 0.4.23  Which accelerator(s) are you using? TPU v38  Additional system info? Numpy version: 1.23.5. Python version: 3.9.5 (default, Nov 23 2021, 15:27:38)  [GCC 9.3.0]. Platform uname: uname_result(system='Linux', node='t1vne84c95a6w0', release='5.13.01027gcp', version=' CC(Fix the bug in classifier example, batching_test and README)~20.04.1Ubuntu SMP Thu May 26 10:53:08 UTC 2022', machine='x86_64')  NVIDIA GPU info _No response_",2024-01-09T10:50:26Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/19263,Closing this. I seem to be able to get a perfettoreadable trace written to GCS even when . 
1450,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tracking issue: NumPy 2.0 Compatibility)ï¼Œ å†…å®¹æ˜¯ (NumPy 2.0 is coming, and there are a number of things we need to do to ensure a smooth transition for users. This issue tracks these TODOs. Relevant NumPy issue is here: https://github.com/numpy/numpy/issues/24300. This has some overlap with CC(Tracking issue: support Array API), as NumPy is aiming for array API compatibility in v2.0.  [x] add an upstream nightly build (kokoro only, because of jaxlib build requirement)  [x] fix `np.ComplexWarning` references ( CC(Remove reference to np.ComplexWarning))  [x] new `sign` convention for complex entries ( CC(jnp.sign: use x/abs(x) for complex arguments))  [x] fix `jax.scipy.special.logsumexp` for new sign convention ( CC(logsumexp: use NumPy 2.0 convention for complex sign))  [x] New shape for `inverse_indices` in `jnp.unique` ( CC(jnp.unique: make return_inverse shape match NumPy 2.0))  [x] add new `jax.numpy` functions:    [x] `concat` CC([array api] add jax.numpy.concat)    [x] `isdtype` CC(Add jnp.isdtype function, following np.isdtype in NumPy 2.0)    [x]  `permute_dims` CC([array api] add jax.numpy.permute_dims function)    [x] `bitwise_invert` CC(array api: add jnp.bitwise_* aliases)    [x] `bitwise_left_shift` CC(array api: add jnp.bitwise_* aliases)    [x] `)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Tracking issue: NumPy 2.0 Compatibility,"NumPy 2.0 is coming, and there are a number of things we need to do to ensure a smooth transition for users. This issue tracks these TODOs. Relevant NumPy issue is here: https://github.com/numpy/numpy/issues/24300. This has some overlap with CC(Tracking issue: support Array API), as NumPy is aiming for array API compatibility in v2.0.  [x] add an upstream nightly build (kokoro only, because of jaxlib build requirement)  [x] fix `np.ComplexWarning` references ( CC(Remove reference to np.ComplexWarning))  [x] new `sign` convention for complex entries ( CC(jnp.sign: use x/abs(x) for complex arguments))  [x] fix `jax.scipy.special.logsumexp` for new sign convention ( CC(logsumexp: use NumPy 2.0 convention for complex sign))  [x] New shape for `inverse_indices` in `jnp.unique` ( CC(jnp.unique: make return_inverse shape match NumPy 2.0))  [x] add new `jax.numpy` functions:    [x] `concat` CC([array api] add jax.numpy.concat)    [x] `isdtype` CC(Add jnp.isdtype function, following np.isdtype in NumPy 2.0)    [x]  `permute_dims` CC([array api] add jax.numpy.permute_dims function)    [x] `bitwise_invert` CC(array api: add jnp.bitwise_* aliases)    [x] `bitwise_left_shift` CC(array api: add jnp.bitwise_* aliases)    [x] `",2024-01-08T18:36:24Z,enhancement,closed,0,2,https://github.com/jax-ml/jax/issues/19246,"We're in good shape here; the only failures as of today are fft bugs upstream (https://github.com/numpy/numpy/issues/25661 and https://github.com/numpy/numpy/issues/25679). These are being fixed in https://github.com/numpy/numpy/pull/25668 Beyond that, our main TODO is to update our builds to use the numpy 2.0 ABI once the release candidates are out.","I think we can declare this fixed. We've made a release with NumPy 2.0, which was the last main TODO."
845,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Pallas flash attention `causal` flag has no effect )ï¼Œ å†…å®¹æ˜¯ ( Description I am trying to use the pallas tpu flash_attention. It seems like setting the causal flag is not changing the output. I tested with the following code.  It gives the following output:   What jax/jaxlib version are you using? 0.4.21 0.4.21  Which accelerator(s) are you using? tpuv38  Additional system info? 1.26.2 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] uname_result(system='Linux', node='t1vn5b478b6dw0', release='5.19.01022gcp', version=' CC(attempt to centerjustify the jax logo in readme)~22.04.1Ubuntu SMP Sun Apr 23 09:51:08 UTC 2023', machine='x86_64')  NVIDIA GPU info _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Pallas flash attention `causal` flag has no effect ," Description I am trying to use the pallas tpu flash_attention. It seems like setting the causal flag is not changing the output. I tested with the following code.  It gives the following output:   What jax/jaxlib version are you using? 0.4.21 0.4.21  Which accelerator(s) are you using? tpuv38  Additional system info? 1.26.2 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] uname_result(system='Linux', node='t1vn5b478b6dw0', release='5.19.01022gcp', version=' CC(attempt to centerjustify the jax logo in readme)~22.04.1Ubuntu SMP Sun Apr 23 09:51:08 UTC 2023', machine='x86_64')  NVIDIA GPU info _No response_",2024-01-08T06:57:47Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/19237,Closing as it seems to be a precision caused artifact.
505,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add CUDA Array Interface consumer support)ï¼Œ å†…å®¹æ˜¯ (This PR adds CUDA Array Interface (versions 2 and 3) consumer support to JAX. In addition, the PR enables constructing JAX arrays from objects that implement dlpack provider support. Fixes CC(Support __cuda_array_interface__ on GPU )  Requires https://github.com/openxla/xla/pull/8237)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add CUDA Array Interface consumer support,"This PR adds CUDA Array Interface (versions 2 and 3) consumer support to JAX. In addition, the PR enables constructing JAX arrays from objects that implement dlpack provider support. Fixes CC(Support __cuda_array_interface__ on GPU )  Requires https://github.com/openxla/xla/pull/8237",2024-01-07T17:33:06Z,cla: yes pull ready NVIDIA GPU,closed,0,15,https://github.com/jax-ml/jax/issues/19233,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Thanks for the contribution â€“ it's not clear to me that `asarray` and `array` should transparently handle `dlpack` and other data interchange formats. For example, the Python Array API standard explicitly decided not to do this (see https://github.com/dataapis/arrayapi/pull/301 and linked issues). This suggests that we should stick with explicit data interchange functions like `jnp.from_dlpack`. What do you think?","> This suggests that we should stick with explicit data interchange functions like `jnp.from_dlpack`. What do you think?  Yes, I agree. I'll update the PR accordingly.","Similar to `from_dlpack`, I wonder if an explicit `from_cuda_interface` function would better achieve the goal here?","> Similar to `from_dlpack`, I wonder if an explicit `from_cuda_interface` function would better achieve the goal here? While arrayapi does not mention CUDA Array Interface nor NumPy Array Interface, these are kind of legacy buffer protocols and objects implementing the Buffer Protocol are allowed inputs to `asarray` according to the arrayapi. Here's a summary how the objects implementing CUDA Array Interface protocol are resolved elsewhere:  cupy resolves `__cuda_array_interface__` in `array` (`asarray` typically calls `array`). Btw, it has internal method `_array_from_cuda_array_interface` for that.  PyTorch resolves `__cuda_array_interface__` in `tensor` (analogue of `array`) and in `as_tensor` (analogue of `asarray`) and in `asarray`.  mpi4py resolves `__cuda_array_interface__` in `frombuffer` and `getbuffer`. Considering the above, my first choice would be to resolve CAI objects in the `array` function (as in this PR). However, the explicit `from_cai` or similar also makes sense as this is about importing a view of an existing buffer similar to `from_dlpack`. What do you think?","That sounds good, we can do it in `asarray`. Though it introduces some complexities, e.g. should lists of cudacompatible objects be treated as lists of arrays? That's possible currently with our handling of buffer protocol objects.","> Though it introduces some complexities, e.g. should lists of cudacompatible objects be treated as lists of arrays? That's possible currently with our handling of buffer protocol objects. Currently, jax, numpy, and cupy implement this using the following principle: if `input` to `asarray` is a native array object then `asarray([input])` is an array with `ndim` larger than `input.ndim`. (PyTorch does not support list of tensors as an input to `asarray`). Your concern corresponds to the case where `input` is not a native array object, say, it is anything that implements one of the data exchange protocols. Currently, all above mentioned libraries will raise an exception on `asarray([input])`:  numpy raises `TypeError` with a FutureWarning:     cupy raises `ValueError: Unsupported dtype object`  jax raises `TypeError: Cannot interpret '' as a data type` Considering the numpy FutureWarning, the answer might be affirmative, but notice that https://dataapis.org/arrayapi/latest/API_specification/generated/array_api.asarray.htmlarray_api.asarray does not support this nor does it support the current behavior where input list items are native array objects.","Thanks  before we proceed with the review, can you sign the CLA?","> Similar to `from_dlpack`, I wonder if an explicit `from_cuda_interface` function would better achieve the goal here? I'm not 100% sure about this one. DLPack should provide a superset of functionality of `__cuda_array_interface__` (and for CUDA specifically, the stream support in DLPack was improved based on the lessons of `__cuda_array_interface__`, so I'd consider the latter legacy and nudge any remaining libraries that don't have DLPack support that's on par with their `__cuda_array_interface__` implementation to fix that.  There's something to be said for this either way of course, given that not all libraries today have support that's fully equivalent between the two protocols. But there isn't too much traffic on gh1100 in 3+ years. And we have too many different protocols in Python that are overlapping  with DLPack being the most viable one longterm, as the only one that has multidevice support. `__cuda_array_interface__` doesn't even seem to support ROCm, so unless I'm missing something, it may not be needed longterm? Keeping its support inside another function (or multiple) makes it easier to evolve / phase out.","Circling back here: after chatting with  a bit, I think this is probably the right approach. We'll have to wait on https://github.com/openxla/xla/pull/8237 before we can do anything here, but once that's in let's plan to proceed with this review.","OK, looks good! Last thing: could you please squash the changes into a single commit? Thanks!","> OK, looks good! Last thing: could you please squash the changes into a single commit? Thanks! Done.","I guess before merging this PR, https://github.com/openxla/xla/pull/8237 needs to land soon.", I think we can move forward with this PR as https://github.com/openxla/xla/pull/8237 has landed,Sorry for the delay in mergind: we're running into some test failures when testing this against PJRT runtimes; I'm trying to debug internally.
1444,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Pallas] Error Lowering to Triton)ï¼Œ å†…å®¹æ˜¯ ( Description I'm attempting to test out taking gradients in a Pallas Kernel. Related to discussion 19184. My Pallas installation follows CC(Make Pallas/GPU easier to install). Here's my playground code:  When I use standard JAX, I can successfully get the gradient with respect to _w1_. But when I use the Pallas Kernel, I get the following error: ``` The above exception was the direct cause of the following exception: jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/juice5/scr5/yusun/data/karan/tttgpt/kernels/pallas_playground.py"", line 38, in      grad_w1 = gradient_kernel_test(x, w1, w2, y)   File ""/nlp/scr/yusun/miniconda3/envs/pallas/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py"", line 1621, in pallas_call_lowering     compilation_result = compile_jaxpr(   File ""/nlp/scr/yusun/miniconda3/envs/pallas/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py"", line 1564, in compile_jaxpr     lowering_result = lower_jaxpr_to_triton_module(   File ""/n)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,[Pallas] Error Lowering to Triton," Description I'm attempting to test out taking gradients in a Pallas Kernel. Related to discussion 19184. My Pallas installation follows CC(Make Pallas/GPU easier to install). Here's my playground code:  When I use standard JAX, I can successfully get the gradient with respect to _w1_. But when I use the Pallas Kernel, I get the following error: ``` The above exception was the direct cause of the following exception: jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/juice5/scr5/yusun/data/karan/tttgpt/kernels/pallas_playground.py"", line 38, in      grad_w1 = gradient_kernel_test(x, w1, w2, y)   File ""/nlp/scr/yusun/miniconda3/envs/pallas/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py"", line 1621, in pallas_call_lowering     compilation_result = compile_jaxpr(   File ""/nlp/scr/yusun/miniconda3/envs/pallas/lib/python3.10/sitepackages/jax/_src/pallas/triton/lowering.py"", line 1564, in compile_jaxpr     lowering_result = lower_jaxpr_to_triton_module(   File ""/n",2024-01-05T19:16:53Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/19222,"Could you try rerunning this with the latest jaxlib version (0.4.25)? Pallas should now do a slightly better job at reporting lowering errors on GPU. My guess is that the problem is that the issue is that block dimensions are not powers of 2, but you should get an error message saying that.",I will close the issue for now. Please reopen if you are able to reproduce with jaxlib 0.4.25+.
508,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(XLA_PYTHON_CLIENT_PREALLOCATE=false is not honored anymore)ï¼Œ å†…å®¹æ˜¯ ( Description Reproducer:  Output with recent JAX:  Working JAX version:  Not working for JAX>=0.4.21  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? GPU  Additional system info? Python 3.9, Ubuntu  NVIDIA GPU info _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,XLA_PYTHON_CLIENT_PREALLOCATE=false is not honored anymore," Description Reproducer:  Output with recent JAX:  Working JAX version:  Not working for JAX>=0.4.21  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? GPU  Additional system info? Python 3.9, Ubuntu  NVIDIA GPU info _No response_",2024-01-05T12:37:03Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/19213,Sorry for the breakage. This is already fixed (see https://github.com/google/jax/issues/19035) but needs a new release.,"That release has happened by now, so we should be good to close this."
1357,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(ImportError: cannot import name 'index_update' from 'jax.ops' (/home/home/.local/lib/python3.8/site-packages/jax/ops/__init__.py))ï¼Œ å†…å®¹æ˜¯ (Please:  [ ] Check for duplicate requests.  [ ] Describe your goal, and if possible provide a code snippet with a motivating example. Hi. I'm trying to run PRIEST, and the following link is about priest package https://github.com/fatemehrastgar/PRIEST The problem I'm having is I tried to run ""rosrun priest planner_holonomic.py"", but this error is happening homeB760MDS3HDDR4:~/priest_ws/src$ rosrun priest planner_holonomic.py Traceback (most recent call last):   File ""/home/home/priest_ws/src/PRIEST/priest/src/planner_holonomic.py"", line 21, in      import mpc_expert   File ""/home/home/priest_ws/src/PRIEST/priest/src/mpc_expert.py"", line 9, in      from jax.ops import index_update, index ImportError: cannot import name 'index_update' from 'jax.ops' (/home/home/.local/lib/python3.8/sitepackages/jax/ops/__init__.py) I guess it's a problem with version of jax and jaxlib, but to be honest due to my lacking skills, I have no idea which version should I have to use to run this package. What should I do to run this package?? Best regards :))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ImportError: cannot import name 'index_update' from 'jax.ops' (/home/home/.local/lib/python3.8/site-packages/jax/ops/__init__.py),"Please:  [ ] Check for duplicate requests.  [ ] Describe your goal, and if possible provide a code snippet with a motivating example. Hi. I'm trying to run PRIEST, and the following link is about priest package https://github.com/fatemehrastgar/PRIEST The problem I'm having is I tried to run ""rosrun priest planner_holonomic.py"", but this error is happening homeB760MDS3HDDR4:~/priest_ws/src$ rosrun priest planner_holonomic.py Traceback (most recent call last):   File ""/home/home/priest_ws/src/PRIEST/priest/src/planner_holonomic.py"", line 21, in      import mpc_expert   File ""/home/home/priest_ws/src/PRIEST/priest/src/mpc_expert.py"", line 9, in      from jax.ops import index_update, index ImportError: cannot import name 'index_update' from 'jax.ops' (/home/home/.local/lib/python3.8/sitepackages/jax/ops/__init__.py) I guess it's a problem with version of jax and jaxlib, but to be honest due to my lacking skills, I have no idea which version should I have to use to run this package. What should I do to run this package?? Best regards :)",2024-01-05T08:25:29Z,question,closed,0,2,https://github.com/jax-ml/jax/issues/19211,"`jax.ops.index_update` was deprecated in JAX v0.2.22, and removed in JAX v0.3.2. Instead of `jax.ops.index_update(x, i, y)`, you can use `x.at[i].set(y)`. If you are using another package that calls `index_update` so that you're unable to change the callsite, then you'll have to use `jax` version 0.3.1 or older:  You can find more information about installing older jaxlib versions at https://jax.readthedocs.io/en/latest/installation.htmlinstallingolderjaxlibwheels.",It looks like this has been resolved  thanks!
679,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([XLA:Python] Raise an AttributeError if __cuda_array_interface__ is called on various invalid buffers, rather than a RuntimeError.)ï¼Œ å†…å®¹æ˜¯ ([XLA:Python] Raise an AttributeError if __cuda_array_interface__ is called on various invalid buffers, rather than a RuntimeError. This makes hasattr(x, ""__cuda_array_interface__"") fail gracefully. In passing, also move the implementation into py_array.cc, and use an allowlist of supported types rather than a denylist. Fixes https://github.com/google/jax/issues/19134)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"[XLA:Python] Raise an AttributeError if __cuda_array_interface__ is called on various invalid buffers, rather than a RuntimeError.","[XLA:Python] Raise an AttributeError if __cuda_array_interface__ is called on various invalid buffers, rather than a RuntimeError. This makes hasattr(x, ""__cuda_array_interface__"") fail gracefully. In passing, also move the implementation into py_array.cc, and use an allowlist of supported types rather than a denylist. Fixes https://github.com/google/jax/issues/19134",2024-01-04T18:32:34Z,,closed,0,0,https://github.com/jax-ml/jax/issues/19199
648,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Need sequence parallel to improve compute strength and save HBM space when LLM training.)ï¼Œ å†…å®¹æ˜¯ (Feature sequence parallel has already support by Megatron and DeepSpeed for a long time. Input samples are not only separated at batch dim, but also at sequence length dim when using tensor parallelism. Paper:https://arxiv.org/pdf/2105.13120.pdf  Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,Need sequence parallel to improve compute strength and save HBM space when LLM training.,"Feature sequence parallel has already support by Megatron and DeepSpeed for a long time. Input samples are not only separated at batch dim, but also at sequence length dim when using tensor parallelism. Paper:https://arxiv.org/pdf/2105.13120.pdf  Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example.",2024-01-04T16:21:32Z,enhancement,open,0,0,https://github.com/jax-ml/jax/issues/19195
555,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(jnp.ndarray.item(): add args support)ï¼Œ å†…å®¹æ˜¯ (Addresses https://github.com/google/jax/pull/19181issuecomment1875833407 This implements the optional index arguments to `jnp.ndarray.item`, which previously were not implemented in JAX. It also makes `arr.item()` continue to work correctly independent of the deprecation in CC(Error for deprecated scalar conversions of nonscalar arrays).)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,jnp.ndarray.item(): add args support,"Addresses https://github.com/google/jax/pull/19181issuecomment1875833407 This implements the optional index arguments to `jnp.ndarray.item`, which previously were not implemented in JAX. It also makes `arr.item()` continue to work correctly independent of the deprecation in CC(Error for deprecated scalar conversions of nonscalar arrays).",2024-01-03T20:14:34Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/19182
410,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„jaxä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add the githash that the jaxlib was built at to __init__.py. This is to allow identifying the githash of nightlies.)ï¼Œ å†…å®¹æ˜¯ (Add the githash that the jaxlib was built at to __init__.py. This is to allow identifying the githash of nightlies.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add the githash that the jaxlib was built at to __init__.py. This is to allow identifying the githash of nightlies.,Add the githash that the jaxlib was built at to __init__.py. This is to allow identifying the githash of nightlies.,2024-01-02T20:56:09Z,,closed,0,0,https://github.com/jax-ml/jax/issues/19173
